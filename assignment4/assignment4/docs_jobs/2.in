<?xml version='1.0' encoding='utf8'?>
<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Electronic documents</title>
    <ns>14</ns>
    <id>699814</id>
    <revision>
      <id>546483581</id>
      <parentid>525385068</parentid>
      <timestamp>2013-03-23T06:27:20Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 14 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7210821]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="119" xml:space="preserve">[[Category:Documents]]
[[Category:Digital media]]
[[Category:Information retrieval]]
[[Category:Electronic publishing]]</text>
      <sha1>t1806qygmvdgbyf72c9lq6hqh776vz9</sha1>
    </revision>
  </page>
  <page>
    <title>Information needs</title>
    <ns>0</ns>
    <id>11016342</id>
    <revision>
      <id>758495817</id>
      <parentid>753282882</parentid>
      <timestamp>2017-01-05T20:02:38Z</timestamp>
      <contributor>
        <ip>72.239.0.15</ip>
      </contributor>
      <comment>Undid revision 753282882 by [[Special:Contributions/112.201.218.139|112.201.218.139]] ([[User talk:112.201.218.139|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5840" xml:space="preserve">information need is an uncertainty that arises in an individual, and which the individual is believed to be satisfied by information.{{more footnotes|date=June 2015}}
The concept '''Information need''' is seldom, if ever, mentioned in the general literature about [[needs]], but is a common term in the literature of [[information science]]. According to Hjørland (1997) it is closely related to the concept of [[relevance]]: If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. 

It is often understood as an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The ‘information’ and ‘need’ in ‘information need’ are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:
# The explanation of observed phenomena of information use or expressed need;
# The prediction of instances of information uses;
# The control and thereby improvement of the utilization of information manipulation of essentials conditions.

Information needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.

== Background ==

The concept of information needs was coined by an American information journalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (renamed Journal of the American Society of Information Science and Technology).

In this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.

According to Taylor, information need has four levels:
# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the “ideal question” — the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information
# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.
# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer’s doubts.
# The question as presented to the information system.

There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).

Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:
# When approached from the point of view of the scientist or technologists, these are studies of scientists’ communication behaviour;
# When approached from the point of view of any communication medium, they are use studies;
# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.

William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:
# The scientist within his culture.
# The scientist within a political system.
# The scientist within a membership group.
# The scientist within a reference group.
# The scientist within an invisible college.
# The scientist within a formal organization.
# The scientist within a work team.
# The scientist within his own head.
# The scientist within a legal/economical system.
# The scientist within a formal.

==See also==
* [[Information retrieval]]
* [[Needs]]

==References==
* Hjørland, Birger (1997). Information seeking and subject representation. An activity-theoretical approach to information science. Westport, CO: Greenwood Press. 
* Menzel, Herbert. “Information Needs and Uses in Science and Technology.” Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.
* Paisley, William J. “Information Needs and Uses.” Annual Review of Information Science and Technology, Vol.3, Encyclopædia Britannica, Inc. Chicago 1968, pp.1-30.
* Taylor, Robert S. “The Process of Asking Questions” American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.
* Wilson, T.D. “On User Studies and Information Needs.” Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15

[[Category:Information retrieval]]</text>
      <sha1>nn3h4km3zgr2fwj2pmpns1neer6x4b2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Citation indices</title>
    <ns>14</ns>
    <id>24447333</id>
    <revision>
      <id>679712371</id>
      <parentid>678206464</parentid>
      <timestamp>2015-09-06T09:12:40Z</timestamp>
      <contributor>
        <username>Colonies Chris</username>
        <id>577301</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="286" xml:space="preserve">{{Cat main|Citation index}}
{{distinguish|Category:Bibliographic databases and indexes|Category:Citation metrics}}

[[Category:Bibliometrics|Indices]]
[[Category:Reference works]]
[[Category:Indexes]]
[[Category:Information retrieval]]
[[Category:Bibliographic databases and indexes| ]]</text>
      <sha1>1xjzb686lb1dxkxr26wqn5gqxtu4hi7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directories</title>
    <ns>14</ns>
    <id>3119166</id>
    <revision>
      <id>637022163</id>
      <parentid>604574833</parentid>
      <timestamp>2014-12-07T14:10:02Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <comment>added to Information Retrieval category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="288" xml:space="preserve">A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.
{{Cat main|Directories}}
{{Commons cat|Directories}}

[[Category:Telephony]]
[[Category:Reference works]]
[[Category:Data management]]
[[Category:Information retrieval]]</text>
      <sha1>oqc4h7er045u1iygtnm663lkgnfom32</sha1>
    </revision>
  </page>
  <page>
    <title>Comprehensive Model of Information Seeking</title>
    <ns>0</ns>
    <id>45206870</id>
    <revision>
      <id>753673225</id>
      <parentid>708778578</parentid>
      <timestamp>2016-12-08T16:01:46Z</timestamp>
      <contributor>
        <ip>153.1.30.106</ip>
      </contributor>
      <comment>/* Antecdents */ fixed spelling mistake</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7574" xml:space="preserve">The '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].

The CMIS has been empirically tested in health and organizational contexts&lt;ref&gt;Johnson, J. D., &amp; Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women &amp; Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., &amp; Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. 
Science Communication, 16, 274-303.&lt;/ref&gt; The CMIS has inherent strengths for studying how people react to health problems such as cancer.&lt;ref name="auto"&gt;Johnson, J. D., Andrews, J. E. &amp; Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.&lt;/ref&gt; The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.

==Design==

[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]
The CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).&lt;ref name="auto"/&gt; There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person’s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer’s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.&lt;ref name="auto"/&gt;

==Antecedents==
The CMIS antecedents—demographics, personal experience, salience, and beliefs—are factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.&lt;ref&gt;Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.&lt;/ref&gt; In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.&lt;ref&gt;Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.&lt;/ref&gt;

==Information Carrier Characteristics==

The information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.&lt;ref name="auto"/&gt;

==Information Seeking Actions==

There are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.&lt;ref name="auto"/&gt;

==Stages in the CMIS==

A key concept from the CMIS is the notion of “stages,” or “cancer involvement”.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.

The first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.
 
The second stage is ''Purposive-Placid''. This is characterized by the question, “What can I do to prevent cancer?” Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.

The third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.

The fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.&lt;ref name="auto"/&gt;

== References ==
{{Research help|Med}}
{{Reflist}}



[[Category:Communication]]
[[Category:Information retrieval]]
[[Category:Health sciences]]</text>
      <sha1>02njhivw5b28hg6j444eqa77lp4mwqz</sha1>
    </revision>
  </page>
  <page>
    <title>Metadirectory</title>
    <ns>0</ns>
    <id>943530</id>
    <revision>
      <id>669335889</id>
      <parentid>669335659</parentid>
      <timestamp>2015-06-30T14:08:36Z</timestamp>
      <contributor>
        <username>Ronz</username>
        <id>7862</id>
      </contributor>
      <comment>/* Open source software */ no independent sources</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1180" xml:space="preserve">A '''metadirectory''' system provides for the flow of data between one or more [[directory service]]s and [[database]]s, in order to maintain synchronization of that data, and is an important part of [[identity management]] systems. The data being synchronized typically are collections of entries that contain user profiles and possibly authentication or policy information. Most metadirectory deployments synchronize data into at least one [[Lightweight Directory Access Protocol|LDAP]]-based directory server, to ensure that LDAP-based applications such as [[single sign-on]] and portal servers have access to recent data, even if the data is mastered in a non-LDAP data source.

Metadirectory products support filtering and transformation of data in transit.

Most [[identity management]] suites from commercial vendors include a metadirectory product, or a [[provisioning#User provisioning|user provisioning]] product.

== See also ==
* [[Virtual directory]]
* [[Identity correlation]]
* [[Microsoft Identity Integration Server]]
* [[Novell Identity Manager]]
* [[Critical Path, Inc.|Critical Path Metadirectory]]

[[Category:Directory services]]
[[Category:Data management]]</text>
      <sha1>g2r3einul3mmaoab7n9uv66xc7cd9ll</sha1>
    </revision>
  </page>
  <page>
    <title>Thomas write rule</title>
    <ns>0</ns>
    <id>217343</id>
    <revision>
      <id>732152254</id>
      <parentid>694446583</parentid>
      <timestamp>2016-07-30T00:18:26Z</timestamp>
      <contributor>
        <ip>197.215.241.230</ip>
      </contributor>
      <comment>Removed plural form of transaction in "For example a transaction..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2829" xml:space="preserve">In [[computer science]], particularly the field of [[database]]s, the '''Thomas write rule''' is a rule in [[timestamp-based concurrency control]].  It can be summarized as ''ignore outdated writes''.

It states that, if a more recent transaction has already written the value of an object, then a less recent transaction does not need perform its own write since it will eventually be overwritten by the more recent one.  

The Thomas write rule is applied in situations where a predefined '''logical''' order is assigned to transactions when they start.  For example a transaction might be assigned a monotonically increasing timestamp when it is created.  The rule prevents changes in the order in which the transactions are executed from creating different outputs: The outputs will always be consistent with the predefined logical order.

For example consider a database with 3 variables (A, B, C), and two atomic operations C := A (T1), and C := B (T2).  Each transaction involves a read (A or B), and a write (C).  The only conflict between these transactions is the write on C.  The following is one possible schedule for the operations of these transactions:

:&lt;math&gt;\begin{bmatrix}
T_1 &amp; T_2 \\
&amp; Read(A) \\
Read(B) &amp;   \\
 &amp;Write(C)   \\
Write(C) &amp;  \\
Commit &amp; \\
&amp; Commit \end{bmatrix} \Longleftrightarrow
\begin{bmatrix}
T_1 &amp; T_2 \\
&amp; Read(A) \\
Read(B) &amp; \\
&amp; Write(C) \\
 &amp; \\
Commit &amp; \\
&amp; Commit\\
\end{bmatrix}
&lt;/math&gt;

If (when the transactions are created) T1 is assigned a timestamp that precedes T2 (i.e., according to the logical order, T1 comes first), then only T2's write should be visible.  If, however, T1's write is executed after T2's write, then we need a way to detect this and discard the write.

One practical approach to this is to label each value with a write timestamp (WTS) that indicates the timestamp of the last transaction to modify the value.  Enforcing the Thomas write rule only requires checking to see if the write timestamp of the object is greater than the time stamp of the transaction performing a write.  If so, the write is discarded   

In the example above, if we call TS(T) the timestamp of transaction T, and WTS(O) the write timestamp of object O, then T2's write sets WTS(C) to TS(T2).  When T1 tries to write C, it sees that TS(T1) &lt; WTS(C), and discards the write.  If a third transaction T3 (with TS(T3) &gt; TS(T2)) were to then write to C, it would get TS(T3) &gt; WTS(C), and the write would be allowed.

==References==
*{{Cite journal | author=Robert H. Thomas | title=A majority consensus approach to concurrency control for multiple copy databases | journal=ACM Transactions on Database Systems | year=1979 | volume=4 | issue=2 | pages= 180–209 | doi=10.1145/320071.320076 }}

[[Category:Data management]]
[[Category:Transaction processing]]


{{compu-sci-stub}}</text>
      <sha1>6w3cax51sr45pdzipfbf9gxqkrvushu</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Metadata</title>
    <ns>14</ns>
    <id>2388558</id>
    <revision>
      <id>731303057</id>
      <parentid>544085047</parentid>
      <timestamp>2016-07-24T13:28:36Z</timestamp>
      <contributor>
        <username>Uanfala</username>
        <id>11049176</id>
      </contributor>
      <comment>+UDC classification</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="155" xml:space="preserve">{{Infobox library classification|CC = |DDC = |LCC = |UDC = 001.103.2}}
{{catdiffuse}}
{{Cat main|Metadata}}

[[Category:Data management]]
[[Category:Data]]</text>
      <sha1>jrht02f6061ji5glfhwz7a9t3j3ykkn</sha1>
    </revision>
  </page>
  <page>
    <title>Data Transformation Services</title>
    <ns>0</ns>
    <id>1605292</id>
    <revision>
      <id>722695616</id>
      <parentid>681827771</parentid>
      <timestamp>2016-05-29T17:14:54Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* References */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9193" xml:space="preserve">'''Data Transformation Services''', or '''DTS''', is a set of objects and utilities to allow the automation of [[extract, transform, load|extract, transform and load]] operations to or from a database. The objects are DTS packages and their components, and the utilities are called DTS tools. DTS was included with earlier versions of [[Microsoft SQL Server]], and was almost always used with SQL Server databases, although it could be used independently with other databases.

DTS allows data to be transformed and loaded from [[heterogeneous]] sources using [[OLE DB]], [[ODBC]], or text-only files, into any supported [[database]]. DTS can also allow automation of data import or transformation on a scheduled basis, and can perform additional functions such as [[File Transfer Protocol|FTPing]] files and executing external programs. In addition, DTS provides an alternative method of version control and backup for packages when used in conjunction with a version control system, such as [[Microsoft Visual SourceSafe]].
[[Image:DTS Designer screenshot.PNG|right|thumb|300px|Here a DTS package is edited with DTS Designer in [[Windows XP]].]] DTS has been superseded by [[SQL Server Integration Services]] in later releases of Microsoft SQL Server though there was some backwards compatibility and ability to run DTS packages in the new SSIS for a time.

__TOC__
{{clear}}

==History==

In SQL Server versions 6.5 and earlier, [[database administrators]] (DBAs) used [[SQL Server Transfer Manager]] and [[Bulk Copy Program]], included with SQL Server, to transfer data. These tools had significant shortcomings, and many{{quantify|date=May 2014}} DBAs used third-party tools such as [[Pervasive Data Integrator]] to transfer data more flexibly and easily. With the release of SQL Server 7 in 1998, "Data Transformation Services" was packaged with it to replace all these tools.

SQL Server 2000 expanded DTS functionality in several ways. It introduced new types of tasks, including the ability to [[File Transfer Protocol|FTP]] files, move databases or database components, and add messages into [[Microsoft Message Queuing|Microsoft Message Queue]]. DTS packages can be saved as a Visual Basic file in SQL Server 2000, and this can be expanded to save into any COM-compliant language. Microsoft also integrated packages into [[Windows 2000 security]] and made DTS tools more user-friendly;  tasks can accept input and output parameters.

DTS comes with all editions of SQL Server 7 and 2000, but was superseded by [[SQL Server Integration Services]] in the Microsoft SQL Server 2005 release in 2005.

== DTS packages ==
The DTS package is the fundamental logical component of DTS; every DTS object is a [[Information hiding|child component]] of the package. Packages are used whenever one modifies data using DTS. All the [[metadata]] about the data transformation is contained within the package. Packages can be saved directly in a SQL Server, or can be saved in the [[Microsoft Repository]] or in [[Component Object Model|COM]] files. SQL Server 2000 also allows a programmer to save packages in a [[Visual Basic]] or other language file (when stored to a VB file, the package is actually scripted—that is, a VB script is executed to dynamically create the package objects and its component objects).

A package can contain any number of [[ActiveX Data Objects|connection objects]], but does not have to contain any. These allow the package to read data from any [[OLE DB]]-compliant data source, and can be expanded to handle other sorts of data. The functionality of a package is organized into ''tasks'' and ''steps''.

A DTS Task is a discrete set of functionalities executed as a single step in a DTS package. Each task defines a work item to be performed as part of the data movement and data transformation process or as a job to be executed.

Data Transformation Services supplies a number of tasks that are part of the DTS [[object model]] and that can be accessed graphically through the DTS Designer or accessed programmatically. These tasks, which can be configured individually, cover a wide variety of data copying, data transformation and notification situations. For example, the following types of tasks represent some actions that you can perform by using [[Data transformation service|DTS]]: executing a single SQL statement, sending an email, and transferring a file with FTP.

A step within a DTS package describes the order in which tasks are run and the precedence constraints that describe what to do in the case damage or of failure. These steps can be executed sequentially or in parallel.

Packages can also contain [[global variable]]s which can be used throughout the package. SQL Server 2000 allows input and output parameters for tasks, greatly expanding the usefulness of global variables. DTS packages can be edited, password protected, scheduled for execution, and retrieved by version.

==DTS tools==
DTS tools packaged with SQL Server include the DTS wizards, DTS Designer, and DTS Programming Interfaces.

===DTS wizards===
The DTS [[Wizard (software)|wizards]] can be used to perform simple or common DTS tasks. These include the ''Import/Export Wizard'' and the ''Copy of Database Wizard''. They provide the simplest method of copying data between [[OLE DB]] data sources. There is a great deal of functionality that is not available by merely using a wizard. However, a package created with a wizard can be saved and later altered with one of the other DTS tools.

A ''Create Publishing Wizard'' is also available to schedule packages to run at certain times. This only works if [[SQL Server Agent]] is running; otherwise the package will be scheduled, but will not be executed.

===DTS Designer===
The DTS Designer is a [[graphical tool]] used to build complex DTS Packages with workflows and event-driven logic. DTS Designer can also be used to edit and customize DTS Packages created with the DTS wizard.

Each connection and task in DTS Designer is shown with a specific [[Icon (computing)|icon]]. These icons are joined with precedence constraints, which specify the order and requirements for tasks to be run. One task may run, for instance, only if another task succeeds (or fails). Other tasks may run concurrently.

The DTS Designer has been criticized for having unusual quirks and limitations, such as the inability to visually [[copy and paste]] multiple tasks at one time. Many of these shortcomings have been overcome in [[SQL Server Integration Services]], DTS's successor.

===DTS Query Designer===
A graphical tool used to build [[Information retrieval|queries]] in DTS.

===DTS Run Utility===
DTS Packages can be run from the command line using the DTSRUN Utility.&lt;BR/&gt;
The utility is invoked using the following  syntax: 
&lt;PRE&gt;
dtsrun /S server_name[\instance_name]
        { {/[~]U user_name [/[~]P password]} | /E }
    ]
    {    
        {/[~]N package_name }
        | {/[~]G package_guid_string}
        | {/[~]V package_version_guid_string}
    }
    [/[~]M package_password]
    [/[~]F filename]
    [/[~]R repository_database_name]
    [/A global_variable_name:typeid=value] 
    [/L log_file_name]
    [/W NT_event_log_completion_status]
    [/Z] [/!X] [/!D] [/!Y] [/!C]
]
&lt;/PRE&gt;

When passing in parameters which are mapped to Global Variables, you are required to include the typeid. This is rather difficult to find on the Microsoft site. Below are the TypeIds used in passing in these values.

{| class="wikitable sortable"
! Type !! typeid
|-
| Boolean
| align="right" | 11
|-
| Currency
| align="right" | 6
|-
| Date
| align="right" | 7
|-
| Decimal
| align="right" | 14
|-
| HRESULT
| align="right" | 25
|-
| Int
| align="right" | 22
|-
| Integer (1-byte)
| align="right" | 16
|-
| Integer (8-byte)
| align="right" | 20
|-
| Integer (small)
| align="right" | 2
|-
| LPWSTR
| align="right" | 31
|-
| Pointer
| align="right" | 26
|-
| Real (4-byte)
| align="right" | 4
|-
| Real (8-byte)
| align="right" | 5
|-
| String
| align="right" | 8
|-
| Unsigned int (1-byte)
| align="right" | 17
|-
| Unsigned int (2-byte)
| align="right" | 18
|-
| Unsigned int (4-byte)
| align="right" | 19
|-
| Unsigned int (1-byte)
| align="right" | 21
|-
| Unsigned int
| align="right" | 23
|}

==See also==

* [[OLAP]]
* [[Data warehouse]]
* [[Data mining]]
* [[SQL Server Integration Services]]
* [[Meta Data Services]]

==References==
* {{cite book |author1=Chaffin, Mark |author2=Knight, Brian |author3=Robinson, Todd | title=Professional SQL Server 2000 DTS | publisher=[[Wrox Press]] (Wiley Publishing, Inc.) | year=2003 | isbn=0-7645-4368-7}}

== External links ==
* [http://msdn2.microsoft.com/en-us/library/aa933484(SQL.80).aspx Microsoft SQL Server: Data Transformation Services (DTS)]
* [http://www.sqldts.com/ SQL DTS unique DTS information resource]
* [http://support.microsoft.com/kb/238912 Understanding Microsoft Repository]
* [http://pragmaticworks.com/Resources/webinars/Default.aspx DTS Videos &amp; Training]
* [http://www.softrus.org/dts/ DTS Documenter]

[[Category:Microsoft database software]]
[[Category:Data management]]
[[Category:Extract, transform, load tools]]
[[Category:Microsoft server technology]]</text>
      <sha1>ihca9ivm0abzvype6xbkju92cpudgdt</sha1>
    </revision>
  </page>
  <page>
    <title>Savepoint</title>
    <ns>0</ns>
    <id>1544409</id>
    <revision>
      <id>729770985</id>
      <parentid>677569226</parentid>
      <timestamp>2016-07-14T12:55:07Z</timestamp>
      <contributor>
        <ip>94.116.126.68</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1867" xml:space="preserve">{{For|save points in video games|Saved game}}
{{refimprove|date=September 2014}}

A '''savepoint''' is a way of implementing subtransactions (also known as [[nested transaction]]s) within a [[relational database management system]] by indicating a point within a [[database transaction|transaction]] that can be "[[Rollback (data management)|rolled back to]]" without affecting any work done in the transaction before the savepoint was created. Multiple savepoints can exist within a single transaction. Savepoints are useful for implementing complex error recovery in database applications. If an error occurs in the midst of a multiple-statement transaction, the application may be able to recover from the error (by rolling back to a savepoint) without needing to abort the entire transaction.

A savepoint can be declared by issuing a &lt;code&gt;SAVEPOINT ''name''&lt;/code&gt; statement. All changes made after a savepoint has been declared can be undone by issuing a &lt;code&gt;ROLLBACK TO SAVEPOINT ''name''&lt;/code&gt; command. Issuing &lt;code&gt;RELEASE SAVEPOINT ''name''&lt;/code&gt; will cause the named savepoint to be discarded, but will not otherwise affect anything. Issuing the commands &lt;code&gt;ROLLBACK&lt;/code&gt; or &lt;code&gt;COMMIT&lt;/code&gt; will also discard any savepoints created since the start of the main transaction.[http://docs.oracle.com/cd/B19306_01/appdev.102/b14261/savepoint_statement.htm]

Savepoints are supported in some form or other in database systems like [[PostgreSQL]], [[Oracle database|Oracle]], [[Microsoft SQL Server]], [[MySQL]], [[IBM DB2|DB2]], [[SQLite]] (since 3.6.8), [[Firebird (database server)|Firebird]], [[H2_(DBMS)|H2 Database Engine]], and [[Informix]] (since version 11.50xC3). Savepoints are also defined in the [[SQL#Interoperability_and_standardization|SQL standard]].

{{databases}}

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>fontevph04t7my196thmt8heaweitxv</sha1>
    </revision>
  </page>
  <page>
    <title>Data architecture</title>
    <ns>0</ns>
    <id>4071997</id>
    <revision>
      <id>747179067</id>
      <parentid>744295548</parentid>
      <timestamp>2016-10-31T21:56:24Z</timestamp>
      <contributor>
        <username>Rednblu</username>
        <id>15356</id>
      </contributor>
      <minor />
      <comment>/* Overview */  grammar</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10439" xml:space="preserve">{{Refimprove|date=November 2008}}
In [[information technology]], '''data architecture''' is composed of models, policies, rules or standards that govern which data is collected, and how it is stored, arranged, integrated, and put to use in data systems and in organizations.&lt;ref&gt;[http://www.businessdictionary.com/definition/data-architecture.html Business Dictionary - Data Architecture]&lt;/ref&gt;  Data is usually one of several [[architecture domain]]s that form the pillars of an [[enterprise architecture]] or [[solution architecture]].&lt;ref&gt;[http://www.learn.geekinterview.com/data-warehouse/data-architecture/what-is-data-architecture.html What is data architecture] GeekInterview, 2008-01-28, accessed 2011-04-28&lt;/ref&gt;

== Overview ==
A data architecture should{{POV statement|date=March 2013}} set data standards for all its data systems as a vision or a model of the eventual interactions between those data systems. [[Data integration]], for example, should be dependent upon data architecture standards since data integration requires data interactions between two or more data systems. A data architecture, in part, describes the [[data structure]]s used by a business and its computer [[applications software]]. Data architectures address data in storage and data in motion; descriptions of data stores, data groups and data items; and [[data mapping|mappings]] of those data artifacts to data qualities, applications, locations etc.

Essential to realizing the target state, Data Architecture describes how data is processed, stored, and utilized in an [[information system]]. It provides criteria for [[data processing]] operations so as to make it possible to design [[data flow]]s and also control the flow of data in the system.

The [[data architect]] is typically responsible for defining the target state, aligning during development and then following up to ensure enhancements are done in the spirit of the original blueprint.

During the definition of the target state, the Data Architecture breaks a subject down to the atomic level and then builds it back up to the desired form. The data architect breaks the subject down by going through 3 traditional architectural processes:
* Conceptual - represents all business entities.
* Logical - represents the logic of how entities are related.
* Physical - the realization of the data mechanisms for a specific type of functionality.

The "data" column of the [[Zachman Framework]] for enterprise architecture &amp;ndash;

{| border=1
|'''Layer''' || '''View''' || '''Data (What)''' || '''Stakeholder'''
|-
|1||'''Scope/Contextual''' || List of things and architectural standards&lt;ref&gt;[http://www.strins.com/data-architecture-standards.html Data Architecture Standards]&lt;/ref&gt; important to the business || Planner
|-
|2||'''Business Model/Conceptual'''  || Semantic model or [[Entity-relationship model|Conceptual]]/[http://tdan.com/the-enterprise-data-model/5205 Enterprise Data Model] || Owner
|-
|3||'''System Model/Logical''' || Enterprise/[[Logical data model|Logical Data Model]] || Designer
|-
|4||'''Technology Model/Physical''' || [[Physical data model|Physical Data Model]] || Builder
|-
|5||'''Detailed Representations'''  ||  Actual [[database]]s || Subcontractor
|}

In this second, broader sense, data architecture includes a complete analysis of the relationships among an organization's functions, available [[technologies]], and [[data type]]s.

Data architecture should be defined in the '''planning phase''' of the design of a new data processing and storage system. The major types and sources of data necessary to support an enterprise should be identified in a manner that is complete, consistent, and understandable. The primary requirement at this stage is to define all of the relevant '''data entities''', not to specify [[computer hardware]] items. A data entity is any real or abstracted thing about which an organization or individual wishes to store data.

== Physical data architecture ==
Physical data architecture of an information system is part of a [[Technology roadmapping|technology plan]]. As its name implies, the technology plan is focused on the actual tangible [[element (mathematics)|elements]] to be used in the implementation of the data architecture [[design]]. Physical data architecture encompasses database architecture. Database architecture is a [[Model (abstract)|schema]] of the actual database technology that will support the designed data architecture.

== Elements of data architecture ==
Certain elements must be defined during the design phase of the data architecture schema. For example, administrative structure that will be established in order to manage the data resources must be described. Also, the methodologies that will be employed to store the data must be defined. In addition, a description of the database technology to be employed must be generated, as well as a description of the processes that will manipulate the data. It is also important to design [[interface (computing)|interfaces]] to the data by other systems, as well as a design for the [[infrastructure]] that will support common data operations (i.e. emergency procedures, [[data import]]s, [[data backup]]s, external [[data transfer|transfers of data]]).

Without the guidance of a properly implemented data architecture design, common data operations might be implemented in different ways, rendering it difficult to understand and control the flow of data within such systems. This sort of fragmentation is highly undesirable due to the potential increased cost, and the data disconnects involved. These sorts of difficulties may be encountered with rapidly growing enterprises and also enterprises that service different lines of [[business]] (e.g. [[insurance]] [[Product (business)|products]]).

Properly executed, the data architecture phase of information system planning forces an organization to precisely specify and describe both internal and external information flows. These are patterns that the organization may not have previously taken the time to conceptualize. It is therefore possible at this stage to identify costly information shortfalls, disconnects between departments, and disconnects between organizational systems that may not have been evident before the data architecture analysis.&lt;ref&gt;{{cite book|last=Mittal|first=Prashant|title=Author|year=2009|publisher=Global India Publications|location=pg 256|isbn=978-93-8022-820-4|pages=314|url=https://books.google.com/books?id=BpkhYDj4tm0C&amp;dq=inauthor:%22PRASHANT+MITTAL%22&amp;source=gbs_navlinks_s}}&lt;/ref&gt;

== Constraints and influences ==
Various constraints and influences will have an effect on data architecture design. These include enterprise requirements, technology drivers, economics, business policies and data processing needs.

; Enterprise requirements: These will generally include such elements as economical and effective system expansion, acceptable performance levels (especially system access speed), [[Financial transaction|transaction]] reliability, and transparent [[data management]]. In addition, the [[Data conversion|conversion]] of raw data such as transaction [[Record (computer science)|records]] and [[image]] [[Computer file|files]] into more useful [[information]] forms through such features as [[data warehouse]]s is also a common organizational [[requirement]], since this enables managerial decision making and other organizational processes. One of the architecture techniques is the split between managing [[transaction data]] and (master) [[reference data]]. Another one is splitting [[Automatic identification and data capture|data capture systems]] from data retrieval systems (as done in a data warehouse).

; Technology drivers: These are usually suggested by the completed data architecture and database architecture designs. In addition, some technology drivers will derive from existing organizational integration frameworks and standards, organizational economics, and existing site resources (e.g. previously purchased [[software licensing]]).

; Economics: These are also important factors that must be considered during the data architecture phase. It is possible that some solutions, while optimal in principle, may not be potential candidates due to their cost. External factors such as the [[business cycle]], interest rates, market conditions, and legal considerations could all have an effect on decisions relevant to data architecture.

; Business policies: [[Business policies]] that also drive data architecture design include internal organizational policies, rules of [[regulatory agency|regulatory bodies]], professional standards, and applicable governmental [[laws]] that can vary by applicable [[government agency|agency]]. These policies and rules will help describe the manner in which enterprise wishes to process their data.

; Data processing needs: These include accurate and reproducible [[data transaction|transactions]] performed in high volumes, data warehousing for the support of management information systems (and potential [[data mining]]), repetitive periodic [[Data reporting|reporting]], ad hoc reporting, and support of various organizational initiatives as required (i.e. annual budgets, new [[Product (business)|product]] development).

== See also ==
* [[Enterprise Information Security Architecture]] - (EISA) positions data security in the enterprise information framework.
* [[FDIC Enterprise Architecture Framework]]
* [[Controlled vocabulary]]
* [[Information silo]]
* [[Disparate system]]
* [[Data Warehouse]]

== References ==
{{reflist}}

== Further reading ==
* Bass, L.; John, B.; &amp; Kates, J. (2001). ''Achieving Usability Through Software Architecture'', Carnegie Mellon University.
* Lewis, G.; Comella-Dorda, S.; Place, P.; Plakosh, D.; &amp; Seacord, R., (2001). ''Enterprise Information System Data Architecture Guide'' Carnegie Mellon University.
* Adleman, S.; Moss, L.; Abai, M. (2005). ''Data Strategy'' Addison-Wesley Professional.

== External links ==
{{commons category|Data architecture}}
* [http://www.sei.cmu.edu/library/abstracts/reports/01tr005.cfm Achieving Usability Through Software Architecture], sei.cmu.edu 2001
* [http://sunsite.uakom.sk/sunworldonline/swol-07-1998/swol-07-itarchitect.html The Logical Data Architecture], by Nirmal Baid

{{Data model}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:Enterprise architecture]]</text>
      <sha1>l50n91fv2xgkm9p26bn3y1g2nlco2c3</sha1>
    </revision>
  </page>
  <page>
    <title>World Wide Molecular Matrix</title>
    <ns>0</ns>
    <id>4205544</id>
    <revision>
      <id>745888372</id>
      <parentid>742207405</parentid>
      <timestamp>2016-10-23T23:54:32Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* top */ bold, LQ</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3056" xml:space="preserve">{{Refimprove|date=December 2010}}
{{No footnotes|date=December 2010}}

The '''World Wide Molecular Matrix''' ('''WWMM''') is an electronic [[Disciplinary repository|repository]] for unpublished chemical [[data]]. First proposed in 2002 by [[Peter Murray-Rust]] and his colleagues in the [[chemistry]] department at the [[University of Cambridge]] in the [[United Kingdom]], WWMM provides a free, easily searchable [[database]] for information about thousands of complicated [[molecules]], data that would otherwise remain inaccessible to [[scientists]].

Murray-Rust, a chemical [[cheminformatics|informatics]] specialist, has estimated that 80% of the results produced by chemists around the world is never published in [[scientific journals]]. Most of this data is not ground-breaking, yet it could conceivably be of use to scientists doing related projects—if they could access it. The WWMM was proposed as a solution to this problem. It would house the results of experiments on over 100,000 molecules in [[physical chemistry]], [[organic chemistry]], [[biochemistry]] and medicinal chemistry.

In other scientific fields, the need for a similar depository to house inaccessible information could be more acute. In a presentation at the "[[CERN]] Workshop on Innovations in Scholarly Communications ([[Open Archives Initiative|OAI4]])", Murray-Rust said that chemistry actually leads other fields in published data. He estimated that as much as 99% of the data in some scientific fields never reaches publication.{{Citation needed|date=December 2010|reason=This is not found in external link, OAI4 - Paine Ellsworth, ed.}}

Although scientific in nature, the WWMM is part of the broader [[Open Archives Initiative|open archives]] and [[open source]] movements, pushes to make more and more information freely available to any user via the [[Internet]] or [[www|World Wide Web]]. In his [[CERN]] presentation, Murray-Rust stated that the WWMM was a "response to the expense of [scientific] journals", and he asked the rhetorical question, "Can we win the war to make data open, or will it be absorbed into the [[publishing]] and pseudo-publishing world?" Murray-Rust and his colleagues are also responsible for the development of the Chemical Mark-up Language ([[Chemical Markup Language|CML]]), a variant of [[XML]] intended for [[chemists]].

==See also==
* [[Open Archives Initiative|The open archives initiative (OAI)]]
* [[Informatics (academic field)|The science of Informatics]]
* [[Chemical Markup Language|Chemical Mark-up language (CML)]]

==External links==
*[http://www.ch.cam.ac.uk/person/pm286 The home page of Dr. Peter Murray-Rust at the University of Cambridge]
*[http://www.escience.cam.ac.uk/projects/mi/mi_call.html The Cambridge Center for molecular informatics]
*[http://www.nesc.ac.uk/events/ahm2003/AHMCD/pdf/157.pdf An outline of the WWMM]
*[http://oai4.web.cern.ch/OAI4/ CERN Workshop on Innovations in Scholarly Communication (OAI4)]{{verify source|type=application to WWMM|date=December 2010}}

[[Category:Data management]]</text>
      <sha1>axgk4ac4n9jojdykcghzoxj4hf3340u</sha1>
    </revision>
  </page>
  <page>
    <title>IMS VDEX</title>
    <ns>0</ns>
    <id>5446251</id>
    <revision>
      <id>734815266</id>
      <parentid>671588320</parentid>
      <timestamp>2016-08-16T22:02:47Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>Removing link(s) to "IMS Global": rm redlink (deleted). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9168" xml:space="preserve">{{Multiple issues|
{{refimprove|date=December 2010}}
{{primary sources|date=December 2010}}
{{notability|date=December 2010}}
{{context|date=December 2010}}
}}

'''IMS VDEX''', which stands for '''IMS Vocabulary Definition Exchange''', is a mark-up language – or grammar – for [[Controlled vocabulary|controlled vocabularies]] developed by IMS Global as an open specification, with the Final Specification being approved in February 2004.

IMS VDEX allows the exchange and expression of simple machine-readable lists of human language terms, along with information that may assist a human in understanding the meaning of the various terms, i.e. a flat list of values, a hierarchical tree of values, a thesaurus, a taxonomy, a glossary or a dictionary.

Structural a vocabulary has an identifier, title and a list of terms. Each term has a unique key, titles and (optional) descriptions. A term may have nested terms, thus a hierarchical structure can be created. It is possible to define relationships between terms and add custom metadata to terms.

IMS VDEX support multilinguality. All values supposed to be read by a human, i.e. titles, can be defined in one or more languages.

== Purposes ==
VDEX was designed to supplement other IMS specifications and the IEEE LOM standard by giving additional semantic control to tool developers. IMS VDEX could be used for the following purposes. It is used in practice for other purposes as well.

* ''Interfaces providing pre-defined choices'' – providing radio buttons and drop-down menus for interfaces such as metadata editors or a repository browse tool, based on the vocabulary allowed in the metadata profile used
* ''Distributing vocabularies among many users'' – achieved by simple XML file sharing, or possibly a searchable [[Repository Open Service Interface Definition|repository]] or registry of vocabularies
* ''XML stylesheets used to select and generate different views'' – selecting an overview of an entire vocabulary as an [[HTML]] or [[PDF]] file, for example; providing scope notes for catalogues; or storing a glossary of terms which are called upon by hyperlinks within a document
* ''Validation of metadata instances'' – validated against an application profile, by comparison of the vocabulary terms used in certain metadata elements with those of the machine readable version of the vocabularies specified by the application profile.
* ''Controlled terms for other IMS specifications and IEEE LOM'' – both may contain elements where controlled terms should be used. These elements are often specified as being of a vocabulary data type, and a definition of the permitted terms and their usage may be expressed using VDEX.

== Technical details ==
[[Image:VDEX model.PNG|350px|right|thumb|simplified VDEX data model]]
The VDEX Information Model is represented in the diagram. A VDEX file describing a vocabulary comprises a number of information elements, most of which are relatively simple, such as a string representation of the default (human) language or a [[URI]] identifying the value domain (or vocabulary). Some of the elements are ‘containers’ – such as a ''term'' – that contain additional elements.

Elements may be required or optional, and in some cases, repeatable. Within a term, for example, a ''description'' and ''caption'' may be defined. Multiple language definitions can be used inside a description, by using a ''langstring'' element, where the description is paired with the language to be used. Additional elements within a term include ''media descriptors'', which are one or more media files to supplement a term’s description; and ''metadata'', which is used to describe the vocabulary further.

The ''relationship'' container defines a relationship between terms by identifying the two terms and the specifying type or relationship, such as a term being broader or narrower than another. The term used to specify the type of relationship may conform to the ISO standards for thesauri.

''Vocabulary identifiers'' are unique, persistent URIs, whereas term or relationship identifiers are locally unique strings. VDEX also allows for a ''default language'' and ''vocabulary name'' to be given, and for whether the ordering of terms within the vocabulary is significant (''order significance'') to be specified.

A ''profile type'' is specified to describe the type of vocabulary being expressed; different features of the VDEX model are permitted depending on the profile type, providing a common grammar for several classes of vocabulary. For example, it is possible, in some profile types, for terms to be contained within one another and be nested, which is suited to the expression of hierarchical vocabularies. Five profile types exist: ''lax'', ''thesaurus'', ''hierarchicalTokenTerms'', ‘glossaryOrDictionary’ and ''flatTokenTerms''. The lax profile is the least restrictive and offers the full VDEX model, whereas the flatTokenTerms profile is the most restrictive and lightweight.

VDEX also offers some scope for complex vocabularies, assuming the existence of a well-defined application profile (for exchange interoperability). Some examples are:
* ''Faceted schemes'' – faceted vocabularies are possible with the definition of appropriate relationships
* ''Multi-lingual thesauri'' – metadata could be used within a relationship to achieve multilingual thesauri
* ''Polyhierarchical taxonomies'' – can be expressed using the source/target value pairs in the relationship.

Identifiers in VDEX data should be persistent, unique, resolvable, transportable and URI-compliant. Specifically, vocabulary identifiers should be unique URIs, whereas term and relationship identifiers should be locally unique strings.

== Implementations ==
* [http://aloha2.netera.ca/ ALOHA Metadata Tagging Tool] — Java-based software project that can read IMS VDEX files.
* [http://www.ivimeds.org/news/demonstrator.html IVIMEDS 1G v1.0] – from The International Virtual Medical School – includes VDEX instances in curriculum maps. Partners can create their own maps in VDEX format and use these to help students search the repository.
* [http://www.elframework.org/projects/spws/view Skills Profiling Web Service] — project implemented and demonstrated use of a skills profiling web service using open standards in a medical context. IMS VDEX files were used in the representation of the SPWS hierarchy skills framework.
* [http://www.scottishdoctor.org/ Scottish Doctors] — project used VDEX as a format for expressing curricular outcome systems.
* [http://prs.heacademy.ac.uk/technical/vdex_scripts.html VDEX XSLT scripts] — developed by The Higher Education Academy Centre for Philosophical and Religious Studies to convert VDEX to XHTML and PostgreSQL .
* [http://www.icbl.hw.ac.uk/vdex VDEX Implementation Project] — carried out by the Institute for Computer Based Learning at Heriot-Watt University, with a primary objective of creating a tool for editing vocabularies in VDEX format. The project, which ended in January 2004, was based on the Public Draft (not the current Final Specification).
* [http://sourceforge.net/projects/vdex-j/ VDEX Java Binding] — implementation neutral Java interface for VDEX, as well as providing a default implementation of that interface, and XML marshalling functionality.
* [https://pypi.python.org/pypi/imsvdex imsvdex Python egg] —  API for VDEX XML-files. It is free software written in [[Python (programming language)|Python]].
* [http://plone.org/products/atvocabularymanager ATVocabularyManager] — addon for [[Plone]] CMS uses VDEX as a possible format to define vocabularies.
* [https://pypi.python.org/pypi/collective.vdexvocabulary collective.vdexvocabulary] — implements IMS VDEX as standard [[Zope]] vocabulary which can also be used in [[Plone]] CMS, written in [[Python (programming language)|Python]].
* [https://pypi.python.org/pypi/vdexcsv/ vdexcsv] — offers a commandline converter from [[Comma-separated values|CSV]] to VDEX. It is written in [[Python (programming language)|Python]].

== See also ==
*IMS Global
*[[Learning object metadata]]

==References==
#{{note|coillie}} Marc van Coillie [http://www.eife-l.org/publications/standards/interop/europasscv/europassCV-IMS-AP/usingvdex Using IMS VDEX for the EDS AP - EIfEL]
#{{note|sarasa}} Antonio Sarasa, Jose Manuel Canabal, Juan Carlos Sacristan, Raquel Jimenez [http://online-journals.org/i-jet/article/view/806 Using IMS VDEX in Agrega]

== External links ==
* [http://www.imsglobal.org/vdex IMS VDEX] — official resources by IMS global
* [http://wiki.cetis.ac.uk/What_is_IMS_VDEX What is IMS VDEX] — JISC CETIS
* [http://metadata.cetis.ac.uk/ CETIS Metadata and Digital Repository Special Interest Group (SIG)] — mailing list for those in UK Higher and Further Education  interested in creating, storing and serving educational metadata.

[[Category:Data management]]
[[Category:Educational technology standards]]
[[Category:Knowledge representation]]
[[Category:Library science]]
[[Category:Metadata]]
[[Category:Standards]]
[[Category:Standards organizations]]
[[Category:Technical communication]]</text>
      <sha1>ttrwfaqccd2ra89fkg7wveh0y0qlqzv</sha1>
    </revision>
  </page>
  <page>
    <title>Online complex processing</title>
    <ns>0</ns>
    <id>7577329</id>
    <revision>
      <id>544571927</id>
      <parentid>244255920</parentid>
      <timestamp>2013-03-16T06:46:44Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q4335184]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="446" xml:space="preserve">'''Online complex processing''' ('''OLCP''') is a class of realtime data processing involving complex queries, lengthy queries and/or simultaneous reads and writes to the same records.
{{software-stub}}
==Sources==
*http://www.pcmag.com/encyclopedia_term/0,2542,t=online+complex+processing&amp;i=48345,00.asp

==See also==
*[[Online transaction processing]] 
*[[OLAP]]
*[[Transaction processing]]


[[Category:Data management]]
[[Category:Databases]]</text>
      <sha1>gdd1yy6oy0v7xzucep5jhm78m9rhz5t</sha1>
    </revision>
  </page>
  <page>
    <title>Single source publishing</title>
    <ns>0</ns>
    <id>1227094</id>
    <revision>
      <id>746466339</id>
      <parentid>746465537</parentid>
      <timestamp>2016-10-27T15:45:35Z</timestamp>
      <contributor>
        <ip>80.195.137.224</ip>
      </contributor>
      <comment>/* List of single-source publishing tools */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13928" xml:space="preserve">{{redirect|Single source}}
'''Single source publishing''', also known as '''single sourcing publishing''', is a [[content management]] method which allows the same source [[Content (media)|content]] to be used across different forms of [[Media (communication)|media]] and more than one time.&lt;ref&gt;Kay Ethier, ''XML and FrameMaker'', pg. 19. [[New York City|New York]]: [[Apress]], 2004. ISBN 9781430207191&lt;/ref&gt;&lt;ref&gt;Lucas Walsh, "The Application of Single-Source Publishing to E-Government." Taken from ''Encyclopedia of Digital Government'', pg. 64. Eds. Ari-Veikko Anttiroiko and Matti Mälkiä. [[Hershey, Pennsylvania|Hershey]]: IGI Global, 2007. ISBN 9781591407904&lt;/ref&gt;&lt;ref&gt;[http://www.stylusstudio.com/single_source_publishing.html Single Source Publishing] at [[Stylus Studio]]. Copyright © 2005-2013 [[Progress Software]]. Accessed June 11, 2013.&lt;/ref&gt;&lt;ref name=petra&gt;[http://www.writersua.com/articles/singlesource/ Single Source Publishing with Flare]. Copyright © 2010 WritersUA. Published November 16, 2010; accessed June 11, 2013.&lt;/ref&gt; The labour-intensive and expensive work of [[Technical editing#Technical editing|editing]] need only be carried out once, on only one document;&lt;ref name=cms&gt;Barry Schaeffer, [http://www.cmswire.com/cms/information-management/single-source-publishing-creating-customized-output-015069.php Single Source Publishing: Creating Customized Output]. CMS Wire, 3 April 2012. Accessed 10 June 2013.&lt;/ref&gt; that source document can then be stored in one place and reused.&lt;ref&gt;[[Ann Rockley]] and Charles Cooper, [https://books.google.com/books?id=82X6jGY_dHMC&amp;pg=PT75&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CDoQ6AEwBg#v=onepage&amp;q=single%20source%20publishing&amp;f=false Managing Enterprise Content: A Unified Content Strategy], Chapter 5: Product content. 2nd ed. [[Berkeley, California|Berkeley]]: [[New Riders Press]], 2012. ISBN 9780132931649&lt;/ref&gt; This reduces the potential for error, as corrections are only made one time in the source document.&lt;ref&gt;Janet Mackenzie, ''The Editor's Companion'', pg. 92. [[Cambridge]]: [[Cambridge University Press]], 2011. ISBN 9781107402188&lt;/ref&gt;

The benefits of single source publishing primarily relate to the editor rather than the [[User (computing)|user]]. The user does benefit from consistent terminology and information, but this consistency is also a potential weakness of single source publishing if the content manager does not have an organized [[Conceptualization (information science)|conceptualization]].&lt;ref name=petra/&gt; Single-source publishing is sometimes used synonymously with '''multi-channel publishing''' though whether or not the two terms are synonymous is a matter of discussion.&lt;ref name=mek&gt;[http://www.mekon.com/index.php/pages/knowledge_zone/single-sourcing-multi-channel-publishing/technology_standards Single Source &amp; Multi-Channel Publishing]. © 2013 Mekon, accessed 23 June 2013.&lt;/ref&gt;

==Definition==
While there is a general definition of single source publishing, there is no single official delineation between single source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delinieation. Single source publishing is most often understood as the creation of one source document in [[Microsoft Word]] or [[Adobe FrameMaker]] and converting that document into different [[file format]]s or human [[language]]s (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.&lt;ref name=mek/&gt;

==History==
The origins of single-source publishing lie, indirectly, with the release of [[Windows 3.0]] in 1990.&lt;ref name=bob162&gt;Bob Boiko, [https://books.google.com/books?id=p6nUDn3ZaBoC&amp;pg=PA162&amp;dq=Single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=CUiqU6LEDaLV0QXd44CQDw&amp;ved=0CCoQ6AEwAzgK#v=onepage&amp;q=Single%20source%20publishing&amp;f=false Content Management Bible], pg. 162. [[Hoboken, New Jersey|Hoboken]]: [[John Wiley &amp; Sons]], 2005. ISBN 9780764583643&lt;/ref&gt; With the eclipsing of [[MS-DOS]] by [[graphical user interface]]s, help files went from being unreadable text along the bottom of the screen to hypertext systems such as [[WinHelp]]. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of [[software documentation]] did not simply move from being writers of traditional bound books to writers of [[electronic publishing]], but rather they became authors of central documents which could be reused multiple times across multiple formats.&lt;ref name=bob162/&gt;

The first single-source publishing project was started in 1993 by Cornelia Hofmann at [[Schneider Electric]] in [[Seligenstadt]], using software based on [[Interleaf]] to automatically create paper documentation in multiple languages based on a single original source file.&lt;ref&gt;[https://books.google.com/books?id=inCeft4AkXcC&amp;pg=PA65&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CCoQ6AEwAw#v=onepage&amp;q=single%20source%20publishing&amp;f=false Translating Into Success: Cutting-edge Strategies for Going Multilingual in a Global Age], pg. 227. Eds. Robert C. Sprung and Simone Jaroniec. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9789027231871&lt;/ref&gt;

[[XML]], developed during the mid- to late-1990s, was also significant to the development of single source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.&lt;ref&gt;Doug Wallace and Anthony Levinson, "The XML e-Learning Revolution: Is Your Production Model Holding You Back?" Taken from [https://books.google.com/books?id=4RK7tJ-OO3cC&amp;pg=PA65&amp;dq=Single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=CUiqU6LEDaLV0QXd44CQDw&amp;ved=0CEgQ6AEwCDgK#v=onepage&amp;q=Single%20source%20publishing&amp;f=false Best of The eLearning Guild's Learning Solutions: Articles from the eMagazine's First Five Years], pg. 63. Ed. Bill Brandon. Hoboken: John Wiley &amp; Sons, 2008. ISBN 9780470277157&lt;/ref&gt;

In the mid-1990s, several firms began creating and using single source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt &amp; Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor.  The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.
Ford, for example, was able to tag its single owner's manual files so that 12 model years could be generated via a resolution script running on the single completed file.  Pratt &amp; Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single source files, calling out the desired version at publication time.  World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.

Starting from the early 2000s, single source publishing was used with an increasing frequency in the field of [[technical translation]]. It is still regarded as the most efficient method of publishing the same material in different languages.&lt;ref&gt;Bert Esselink, "Localisation and translation." Taken from [https://books.google.com/books?id=a4W7lWgCqYoC&amp;pg=PA73&amp;dq=Single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=CUiqU6LEDaLV0QXd44CQDw&amp;ved=0CCUQ6AEwAjgK#v=onepage&amp;q=Single%20source%20publishing&amp;f=false Computers and Translation: A Translator's Guide], pg. 73. Ed. H. L. Somers. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2003. ISBN 9789027216403&lt;/ref&gt; Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method.&lt;ref&gt;Burt Esselink, ''A Practical Guide to Localization'', pg. 228. Volume 4 of Language international world directory. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9781588110060&lt;/ref&gt; [[Metadata]] could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.&lt;ref&gt;Cornelia Hofmann and Thorsten Mehnert, "Multilingual Information Management at Schneider Automation." Taken from ''Translating Into Success'', pg. 67.&lt;/ref&gt;

Although single source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single source publishing or render one's operations obsolete.&lt;ref name=cms/&gt;

==Criticism==
Single-source publishing has been criticized due to the quality of work, being compared to as the "conveyor belt assembly" of content creation by its critics.&lt;ref&gt;Mick Hiatt, [http://mashstream.com/mashups/the-myth-of-single-source-authoring/ The Myth of Single-Source Authoring]. Mashstream, November 18, 2009.&lt;/ref&gt; 

While heavily used in technical translation, there are risks of error in regard to [[Index (publishing)|indexing]]. While two words might be [[synonym]]s in English, they may not be synonyms in another language. In a document produced via single sourcing, however, the index will be translated automatically and the two words will be rendered as synonyms because they are in the [[Source language (translation)|source language]], while in the [[Target language (translation)|target language]] they are not.&lt;ref&gt;Nancy Mulvany, [https://books.google.com/books?id=G0Eqm8FbiTMC&amp;pg=PA312&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CEcQ6AEwCA#v=onepage&amp;q=single%20source%20publishing&amp;f=false Indexing Books], pg. 154. 2nd ed. [[Chicago]]: [[University of Chicago Press]], 2009. ISBN 9780226550176&lt;/ref&gt;

==See also==
* [[Content management]]
* [[Darwin Information Typing Architecture]]
* [[EPUB]]
* [[Markup language]]

===List of single-source publishing tools=== 
* [[Adobe FrameMaker]]&lt;ref&gt;Sarah S. O'Keefe, Sheila A. Loring, Terry Smith and Lydia K. Wong, [https://books.google.com/books?id=b-yEKgcQmN8C&amp;pg=PA6&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CE0Q6AEwCQ#v=onepage&amp;q=single%20source%20publishing&amp;f=false Publishing Fundamentals: Unstructured FrameMaker 8], pg. 6. Scriptorium Publishing, 2008. ISBN 9780970473349&lt;/ref&gt;
* [[Apache Cocoon]]
* [[Apache Forrest]]
* [[Altova]]
* [[Booktype]]
* [[DocBook XSL]]
* [[DITA Open Toolkit]]
* [[Help &amp; Manual]]
* [[MadCap Flare]]
* [[Oxygen XML Editor|Oxygen XML editor]]
* [[Scenari]]
* [[Sphinx (documentation generator)|Sphinx]]&lt;ref&gt;{{cite web | url = http://pythonic.pocoo.org/2008/3/21/sphinx-is-released | title = Sphinx is released! &amp;raquo; And now for something completely Pythonic... | publisher = Georg Brandl| work = And now for something completely Pythonic... | accessdate = 2011-04-03 }}&lt;/ref&gt;
* [[XPLM Publisher]]

==References==
{{reflist}}

==Further reading==
* {{cite book | last = Ament | first = Kurt | authorlink = | title = Single Sourcing: Building Modular Documentation | publisher = William Andrew | date = 2007-12-17 |  location = | pages = 245 | url = | doi = | id = | isbn = 0-8155-1491-3 }}
* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}
* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath | title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}
* {{cite book | last = Maler | first = Eve | authorlink = |author2=Jeanne El Andaloussi  | title = Developing SGML DTDs: From Text to Model to Markup | publisher = Prentice Hall PTR | date = 1995-12-15 | location = | pages = 560 | url = | doi = | id = | isbn = 0-13-309881-8 }} (the "bible" for Data Modeling)

==External links==
* [http://www.elkera.com/cms/articles/seminars_and_presentations/planning_a_single_source_publishing_application_for_business_documents/ Planning a Single Source Publishing Application for Business Documents] (A paper presented by Peter Meyer at OpenPublish, Sydney, on 29 July 2005)
* [https://www.tug.org/TUGboat/tb29-1/tb91sojka.pdf Single-source publishing in multiple formats for different output devices]
* [http://www.agilemodeling.com/essays/singleSourceInformation.htm Single Sourcing Information - An Agile Practice for Effective Documentation]
* [http://www.stcsig.org/ss Society for Technical Communication Single-sourcing Special Interest Group]
* [http://www.wisegeek.com/what-is-single-source-publishing.htm What Is Single Source Publishing?] at WiseGeek
* [http://www.technical-communication.org/topics/information-development.html tekom Europe] (Articles about Information Development and Single Source Publishing)

[[Category:Technical communication]]
[[Category:Computer file systems]]
[[Category:Data management]]</text>
      <sha1>lo7fy72f0vsj6ef57trf58lmzq5z09a</sha1>
    </revision>
  </page>
  <page>
    <title>Master data</title>
    <ns>0</ns>
    <id>7617930</id>
    <revision>
      <id>751501273</id>
      <parentid>747574872</parentid>
      <timestamp>2016-11-26T03:04:06Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http&amp;rarr;https for [[YouTube]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5129" xml:space="preserve">'''Master data''' represents the business objects which are agreed on and shared across the enterprise.&lt;ref&gt;[http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202010/Papers/2B1_EnterpriseMasterDataArchitecture.pdf "ENTERPRISE MASTER DATA ARCHITECTURE: DESIGN DECISIONS AND OPTIONS"], Boris Otto &amp; Alexander Schmidt, Institute of Information Management, University of St. Gallen&lt;/ref&gt; It  can cover relatively static reference data, [[Dynamic data|transactional]], [[Unstructured data|unstructured]], analytical, [[Hierarchical database model|hierarchical]] and [[Metadata|meta]] data.&lt;ref&gt;"What Is Master Data?", Roger Wolter and Kirk Haselden, Microsoft Corporation, [http://msdn.microsoft.com/en-us/library/bb190163.aspx "The What, Why, and How of Master Data Management"], November 2006&lt;/ref&gt; It is the primary focus of the [[Information Technology]] (IT) discipline of [[Master Data Management]] (MDM). 

While master data is often non-transactional in nature, it is not limited to non-transactional data, and often ''supports'' transactional processes and operations. For example, Master data may be about: customers, products, employees, materials, suppliers, and vendors, and it may also cover: sales, documents and aggregated sales.

==Types of master data ==

'''Reference Data''' is the set of permissible values to be used by other (master or transaction) data fields. Reference data normally changes slowly, reflecting changes in the modes of operation of the business, rather than changing in the normal course of business.

'''Master Data''' is a ''single source'' of common business data used across '''multiple''' systems, applications, and/or processes.

'''Enterprise Master Data''' is the ''single source'' of common business data used across '''all''' systems, applications, and processes for an entire enterprise (all departments, divisions, companies, and countries).

'''Market Master Data''' is the ''single source'' of common business data for an entire marketplace. Market master data is used among enterprises within the value chain. An example of Market Master Data is the UPC (Universal Product Code) found on consumer products.

Market Master Data is compatible with enterprise-specific and domain-specific systems, compliant with or linked to industry standards, and incorporated within market research analytics. Market master data also facilitates integration of multiple data sources and literally puts everyone in the market on the same page.

Excerpted from ''Master Data Management for Media: A Call to Action for Business Leaders in Marketing, Advertising, and the Media,'' a Microsoft White Paper by Scott Taylor and Robin Laylin, January 2010

==Master data and Master reference data==
Master data is also called '''Master reference data'''. This is to avoid confusion with the usage of the term Master data for '''[[original data]]''', like an original recording (see also: [[Master Tape]]). Master data is nothing but unique data, i.e., there are no duplicate values.{{Citation needed|date=January 2011}}

'''Material Master Data''' is a specific data set holding structured information about spare parts, raw materials and products within Enterprise Resource Planning (ERP) software. The data is held centrally and used across organisations.

'''Vendor Master''' refers to the centralised location of information pertinent to the Vendor. Often this will include the Legal entity name, Tax identification and contact information.

==Master Data Management==
{{main|Master Data Management}}
Curating and managing master data is key to ensuring master data quality. Analysis and reporting is greatly dependent on the quality of an organization's master data. Master data may either be stored in a central repository, sourced from one or more systems, or referenced centrally using an index. However, when it is used by several functional groups it may be distributed and redundantly stored in different applications across an organization and this copy data may be inconsistent (and if so, inaccurate).&lt;ref&gt;[http://blogs.gartner.com/andrew_white/2014/01/14/the-elephant-in-the-room-master-data-and-application-data/#comments "The Elephant in the Room – Master Data and Application Data"], Andrew White, Gartner, 14 January 2014&lt;/ref&gt; Thus Master Data should have an agreed-upon view that is shared across the organization. Care should be taken to properly version Master Data, if the need arises to modify it, to avoid issues with distributed copies.

==See also==
*[[Master data management]]
*[[Customer data integration]]
*[[Product information management]]
*[[Data governance]]

== External links==
{{reflist}}
* [http://www.stibosystems.com/Files/Billeder/Stibo%20Systems%20images/UK_Resource_Library_Images/What-is-Master-Data-Management_EN.png What is Master Data?]
* [http://www.semarchy.com/overview/what-is-master-data/ Semarchy: What is Master Data?]
* [http://www.orchestranetworks.com/rdm/ Managing Reference Data (RDM)]
* [https://www.youtube.com/watch?v=2tzVUqWAovg Aaron Zornes: Understanding Reference Data]

[[Category:Data management]]

[[fr:Données de référence]]</text>
      <sha1>58fvghjbhc8w5f4d6z0a46tfiilefll</sha1>
    </revision>
  </page>
  <page>
    <title>Long-running transaction</title>
    <ns>0</ns>
    <id>4984219</id>
    <revision>
      <id>719328322</id>
      <parentid>687016914</parentid>
      <timestamp>2016-05-09T01:06:34Z</timestamp>
      <contributor>
        <ip>2620:0:E50:100F:81EE:1CE1:86BD:52B8</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1346" xml:space="preserve">{{More sources|date=October 2015}}
'''Long-running transactions''' (also known as Saga transactions) are computer [[database transaction]]s that avoid [[lock (computer science)|locks]] on non-local resources, use compensation to handle failures, potentially aggregate smaller [[ACID]] transactions (also referred to as [[atomic transaction]]s), and typically use a coordinator to complete or abort the transaction. In contrast to [[rollback (data management)|rollback]] in ACID transactions, compensation restores the original state, or an equivalent, and is business-specific. For example, the compensating action for making a hotel reservation is canceling that reservation, possibly with a penalty.

A number of protocols have been specified for long-running transactions using Web services within business processes. OASIS Business Transaction Processing&lt;ref&gt;http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=business-transaction&lt;/ref&gt; and WS-CAF&lt;ref&gt;http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=ws-caf&lt;/ref&gt; are examples. These protocols use a coordinator to mediate the successful completion or use of compensation in a long-running transaction.

==See also==
*[[Optimistic concurrency control]]
*[[Long-lived transaction]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>mw56ofvdd8xr6hzqpl5n5x4rbzzf8az</sha1>
    </revision>
  </page>
  <page>
    <title>Data field</title>
    <ns>0</ns>
    <id>10082565</id>
    <revision>
      <id>650122753</id>
      <parentid>648635373</parentid>
      <timestamp>2015-03-06T09:23:09Z</timestamp>
      <contributor>
        <username>Nedrutland</username>
        <id>3556578</id>
      </contributor>
      <comment>Revert vandalism</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="463" xml:space="preserve">{{Unreferenced|date=March 2007}}
A '''data field''' is a place where you can store [[data]].  Commonly used to refer to a column in a [[database]] or a field in a [[Data entry clerk|data entry]] form or web form.

The field may contain data to be entered as well as data to be displayed.  

==See also== 
{{wiktionary|Data}}
*[[Data dictionary]]
*[[Data element]]
*[[Data acquisition]]
*[[Data hierarchy]]

[[Category:Data management]]

[[it:Campo (informatica)]]</text>
      <sha1>dwkpw0zfuvwc0bqrqmnmcu1v1i4zy84</sha1>
    </revision>
  </page>
  <page>
    <title>Scriptella</title>
    <ns>0</ns>
    <id>8303455</id>
    <revision>
      <id>712747548</id>
      <parentid>711485899</parentid>
      <timestamp>2016-03-30T22:14:23Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]] using [[Project:AWB|AWB]] (11971)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3665" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=March 2012}}
{{Notability|Products|date=March 2012}}
}}

{{Infobox Software
| name = Scriptella
| logo = [[File:Scriptella logo.png|160px|Scriptella logo]]
| latest_release_version = 1.1
| latest_release_date = 28 December 2012
| operating_system = [[Cross-platform]]
| genre = [[Extract transform load|ETL]], [[Data migration]] and [[SQL]].
| license = [[Apache Software License]]
| website = [http://scriptella.org http://scriptella.org]
}}

'''Scriptella''' is an open source [[Extract transform load|ETL (Extract-Transform-Load)]] and script execution tool written in Java. Its primary focus is simplicity. It doesn't require the user to learn another complex XML-based language to use it, but allows the use of SQL or another scripting language suitable for the data source to perform required transformations. Scriptella does not offer any [[graphical user interface]].

==Typical use==
* Database migration.
* Database creation/update scripts.
* Cross-database ETL operations, import/export.
* Alternative for Ant &lt;sql&gt; task.
* Automated database schema upgrade.

==Features==
* '''Simple XML syntax''' for scripts. Add dynamics to your existing SQL scripts by creating a thin wrapper XML file:&lt;source lang="xml"&gt;
      &lt;!DOCTYPE etl SYSTEM "http://scriptella.javaforge.com/dtd/etl.dtd"&gt;
      &lt;etl&gt;
          &lt;connection driver="$driver" url="$url" user="$user" password="$password"/&gt;
          &lt;script&gt;
              &lt;include href="PATH_TO_YOUR_SCRIPT.sql"/&gt;
              -- And/or directly insert SQL statements here
          &lt;/script&gt;
      &lt;/etl&gt;
&lt;/source&gt;
* Support for '''multiple datasources''' (or multiple connections to a single database) in an ETL file.
* Support for many useful '''[[Java Database Connectivity|JDBC]] features''', e.g. parameters in SQL including file blobs and JDBC escaping.
* '''Performance.''' Performance and low memory usage are one of the primary goals.
* Support for '''evaluated expressions and properties''' (JEXL syntax)
* Support for '''cross-database ETL scripts''' by using &lt;dialect&gt; elements
* '''Transactional execution'''
* '''Error handling''' via &lt;onerror&gt; elements
* '''Conditional scripts/queries execution''' (similar to Ant if/unless attributes but more powerful)
* '''Easy-to-Use''' as a standalone tool or Ant task. No deployment/installation required.
* '''Easy-To-Run''' ETL files directly from Java code.
* '''Built-in adapters for popular databases''' for a tight integration. Support for any database with JDBC/[[Open Database Connectivity|ODBC]] compliant driver.
* Service Provider Interface (SPI) for interoperability with non-JDBC DataSources and integration with scripting languages. Out of the box support for [[JSR 223|JSR 223 (Scripting for the Java Platform)]] compatible languages.
* Built-In [[Comma-separated values|CSV]], TEXT, [[XML]], [[Lightweight Directory Access Protocol|LDAP]], [[Apache Lucene|Lucene]], [[Apache Velocity|Velocity]], JEXL and Janino providers. Integration with [[Java EE]], [[Spring framework|Spring Framework]], [[Java Management Extensions|JMX]] and [[JNDI]] for enterprise ready scripts.

==External links==
* [http://scriptella.org Scriptella ETL Site]
* [https://github.com/scriptella/scriptella-etl GitHub Page]
* [http://groups.google.com/group/scriptella/ Discussion forum]
* [http://www.javaforge.com/proj/forum/browseForum.do?forum_id=3126 Discussion forum(deprecated)]
* [http://jroller.com/page/ejboy Scriptella ETL Author's Blog]
* {{Ohloh project|id=4526|name=Scriptella ETL}}

[[Category:Extract, transform, load tools]]
[[Category:Data warehousing products]]
[[Category:Data management]]


{{compu-stub}}</text>
      <sha1>l58xu5xyo58tdujaeuqu8ctd0wjr8oh</sha1>
    </revision>
  </page>
  <page>
    <title>Data classification (data management)</title>
    <ns>0</ns>
    <id>3743270</id>
    <revision>
      <id>755676697</id>
      <parentid>651075521</parentid>
      <timestamp>2016-12-19T14:55:23Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[data migration]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5581" xml:space="preserve">In the field of [[data management]], '''data classification''' as a part of [[Information Lifecycle Management]] (ILM) process can be defined as a tool for categorization of data to enable/help organization to effectively answer following questions:
*What [[data type]]s are available?
*Where are certain data located?
*What [[access level]]s are implemented?
*What protection level is implemented and does it adhere to [[compliance (regulation)|compliance]] regulations?
When implemented it provides a bridge between IT professionals and process or application owners. IT staff is informed about the data value and on the other hand management (usually application owners) understands better to what segment of data centre has to be invested to keep operations running effectively.  This can be of particular importance in risk management, legal discovery, and compliance with government regulations.
Data classification is typically a manual process; however, there are many tools from different vendors that can help gather information about the data.

==How to start process of data classification==
''Note that this classification structure is written from a Data Management perspective and therefore has a focus for text and text convertible binary data sources. Images, videos, and audio files are highly structured formats built for industry standard API's and do not readily fit within the classification scheme outlined below.''

First step is to evaluate and divide the various applications and data into their respective category as follows:
*Relational or Tabular data (around 15% of non audio/video data) 
**Generally describes  proprietary data which can be accessible only through application or [[application programming interfaces]] (API)
**Applications that produce structured data are usually database applications.
**This type of data usually brings complex procedures of data evaluation and migration between the storage tiers.
**To ensure adequate quality standards, the classification process has to be monitored by subject matter experts.
*Semi-structured or Poly-structured data (all other non audio/video data that does not conform to a system or platform defined Relational or Tabular form).
**Generally describes data files that have a dynamic or non-relational semantic structure (e.g. documents,XML,JSON,Device or System Log output,Sensor Output).
**Relatively simple process of data classification is criteria assignment.
**Simple process of [[data migration]] between assigned segments of predefined storage tiers.

Types of data classification - ''note that this designation is entirely orthogonal to the application centric designation outlined above. Regardless of structure inherited from application, data may be of the types below''

1. Geographical : i.e. according to area (supposing the rice production of a state or country etc.)
2. Chronological: i.e. according to time (sale of last 3 months)
3. Qualitative  : i.e. according to distinct categories. (E.g.: population on the basis of poor and rich)
4. Quantitative : i.e. according to magnitude(a) discrete and b)continuous

==Basic criteria for semi-structured or poly-structured data classification==
*Time criteria is the simplest and most commonly used where different type of data is evaluated by time of creation, time of access, time of update, etc.
*Metadata criteria as type, name, owner, location and so on can be used to create more advanced classification policy
*Content criteria which involve usage of advanced content classification algorithms are most advanced forms of unstructured data classification

''Note that any of these criteria may also apply to Tabular or Relational data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.''.

==Basic criteria for relational or Tabular data classification==
These criteria are usually initiated by application requirements such as:
*Disaster recovery and Business Continuity rules
*Data centre resources optimization and consolidation
*Hardware performance limitations and possible improvements by reorganization

''Note that any of these criteria may also apply to semi/poly structured data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.''

==Benefits of data classification==
Benefits of effective implementation of appropriate data classification can significantly improve ILM process and save data centre storage resources. If implemented systemically it can generate improvements in data centre performance and utilization. Data classification can also reduce costs and administration overhead. "Good enough" data classification can produce these results:
*Data compliance and easier [[risk management]]. Data are located where expected on predefined storage tier and "point in time"
*Simplification of data encryption because all data need not be encrypted. This saves valuable processor cycles and all related consecutiveness. 
*Data indexing to improve user access times
*Data protection is redefined where RTO ([[Recovery Time Objective]]) is improved.

==See also==
*[[Data classification (business intelligence)]]

==References==

* Josh Judd and Dan Kruger (2005), Principles of SAN Design. Infinity Publishing
* Stephen J. Bigelown (November 2005), SearchStorage.com, http://searchstorage.techtarget.com/news/article/0,289142,sid5_gci1139240,00.html

[[Category:Data management]]
[[Category:Information technology]]
[[Category:Regulations]]</text>
      <sha1>lv6nvshe1uqpd9kljl7bof5320qt67j</sha1>
    </revision>
  </page>
  <page>
    <title>Global concurrency control</title>
    <ns>0</ns>
    <id>12380968</id>
    <revision>
      <id>544873458</id>
      <parentid>461118993</parentid>
      <timestamp>2013-03-17T07:23:19Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q5570820]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2824" xml:space="preserve">{{POV|Commitment ordering|date=November 2011}}
'''Global concurrency control''' typically pertains to the [[concurrency control]] of a system comprising several components, each with its own concurrency control. The overall concurrency control of the whole system, the ''Global concurrency control'', is determined by the concurrency control of its components, [[Modular programming|module]]s. In this case also the term '''Modular concurrency control''' is used.

In many cases a system may be distributed over a communication network. In this case we deal with [[distributed concurrency control]] of the system, and the two terms sometimes overlap. However, distributed concurrency control typically relates to a case where the distributed system's components do not have each concurrency control of its own, but rather are involved with a concurrency control mechanism that spans several components in order to operate. For example, as typical in a [[distributed database]].

In ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') global concurrency control relates to the concurrency control of a ''multidatabase system'' (for example, a [[Federated database]]; other examples are [[Grid computing]] and [[Cloud computing]] environments). It deals with the properties of the ''global [[schedule (computer science)|schedule]]'', which is the unified schedule of the multidatabase system, comprising all the individual schedules of the [[database system]]s and possibly other [[transactional object]]s in the system. A major goal for global concurrency control is ''[[Global serializability]]'' (or ''Modular serializability''). The problem of achieving global serializability in a [[heterogeneous]] environment had been [[open problem|open]] for many years, until an effective solution based on [[Commitment ordering]] (CO) has been proposed (see [[Global serializability]]). Global concurrency control deals also with [[global serializability#Relaxing global serializability|relaxed]] forms of global serializability which compromise global serializability (and in many applications also correctness, and thus are avoided there). While local (to a database system) [[Serializability#Relaxing serializability|relaxed serializability methods]] compromise serializability for performance gain (utilized when the application allows), it is unclear that the various proposed relaxed global serializability methods provide any performance gain over CO, which guarantees global serializability.


==See also==
*[[Concurrency control]]
*[[Global serializability]]
*[[Commitment ordering]]
*[[Distributed concurrency control]]

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]</text>
      <sha1>6xf9m467sigpbh9kqq0ewf6tlbdzrxk</sha1>
    </revision>
  </page>
  <page>
    <title>Dynamic knowledge repository</title>
    <ns>0</ns>
    <id>1004008</id>
    <revision>
      <id>593025002</id>
      <parentid>571417452</parentid>
      <timestamp>2014-01-29T22:17:11Z</timestamp>
      <contributor>
        <username>HokeyMang</username>
        <id>20608926</id>
      </contributor>
      <minor />
      <comment>/* Definition */ Small grammatical improvements</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1908" xml:space="preserve">{{refimprove|date=August 2007}}

The '''dynamic knowledge repository''' ('''DKR''') is a concept developed by [[Douglas C. Engelbart]] as a primary strategic focus for allowing humans to address complex problems.{{when|date=September 2011}} Doug has proposed that a DKR will enable us to develop a collective [[IQ]] greater than any individual's IQ. References and discussion of Engelbart's DKR concept are available at the [[Doug Engelbart Institute]].&lt;ref&gt;{{cite web | url=http://www.dougengelbart.org/about/dkrs.html|title=About Dynamic Knowledge Repositories – an introduction | accessdate=September 15, 2011 | author=Christina Engelbart}}&lt;/ref&gt;

==Definition==
A knowledge repository is a computerized system that systematically captures, organizes and categorizes an organization's knowledge. The repository can be searched and data can be quickly retrieved.

The effective knowledge repositories include factual, conceptual, procedural and meta-cognitive techniques. The key features of knowledge repositories include communication forums.

A knowledge repository can take many forms to "contain" the knowledge it holds. A customer database is a knowledge repository of customer information and insights – or electronic explicit knowledge. A Library is a knowledge repository of books – physical explicit knowledge. A community of experts is a knowledge repository of tacit knowledge or experience. The nature of the repository only changes to contain/manage the type of knowledge it holds. A repository (as opposed to an archive) is designed to get knowledge out. It should therefore have some rules of structure, classification, taxonomy, record management, etc., to facilitate user engagement.

==References==
{{Reflist|2}}

== External links==
* [http://dougengelbart.org/ Doug Engelbart Institute]

[[Category:Knowledge representation]]
[[Category:Data management]]


{{compu-storage-stub}}</text>
      <sha1>iat2rpgw89p6fkxges2si1zdxavwc7z</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology-based data integration</title>
    <ns>0</ns>
    <id>11476249</id>
    <revision>
      <id>757143956</id>
      <parentid>749301573</parentid>
      <timestamp>2016-12-29T01:37:16Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>cap, punct., refpunct., rm items linked already, rm redlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6525" xml:space="preserve">'''Ontology-based data integration''' involves the use of [[ontology (computer science)|ontology]](s) to effectively combine data or information from multiple heterogeneous sources.&lt;ref name="wache"&gt;{{cite conference |author1=H. Wache |author2=T. Vögele |author3=U. Visser |author4=H. Stuckenschmidt |author5=G. Schuster |author6=H. Neumann |author7=S. Hübner | title=Ontology-Based Integration of Information A Survey of Existing Approaches | year=2001 | citeseerx = 10.1.1.142.4390 }}&lt;/ref&gt; It is one of the multiple [[data integration]] approaches and may be classified as Global-As-View (GAV).&lt;ref name="refone"&gt;{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | year=2002 | pages=243–246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf }}&lt;/ref&gt; The effectiveness of ontology based data integration is closely tied to the consistency and expressivity of the ontology used in the integration process.

==Background==
Data from multiple sources are characterized by multiple types of heterogeneity. The following hierarchy is often used:&lt;ref name="sheth"&gt;{{cite book | author=A.P. Sheth | title = Changing Focus on Interoperability in Information Systems: From System, Syntax, Structure to Semantics | booktitle=Interoperating Geographic Information Systems. M. F. Goodchild, M. J. Egenhofer, R. Fegeas, and C. A. Kottman (eds.), Kluwer Academic Publishers | year=1999 | pages=5–30 | url=http://lsdis.cs.uga.edu/library/download/S98-changing.pdf}}&lt;/ref&gt;&lt;ref&gt;[http://daks.ucdavis.edu/~ludaesch/Paper/AHM02/tutorial5.html AHM02 Tutorial 5: Data Integration and Mediation; Contributors: B. Ludaescher, I. Altintas, A. Gupta, M. Martone, R. Marciano, X. Qian]&lt;/ref&gt;
* [[Syntactic heterogeneity]]: is a result of differences in representation format of data
* Schematic or [[structural heterogeneity]]: the native model or structure to store data differ in data sources leading to structural heterogeneity. Schematic heterogeneity that particularly appears in structured databases is also an aspect of structural heterogeneity.&lt;ref name="sheth"/&gt;
* [[Semantic heterogeneity]]: differences in interpretation of the 'meaning' of data are source of semantic heterogeneity
* [[System heterogeneity]]: use of different [[operating system]], hardware platforms lead to system heterogeneity

[[ontology (computer science)|Ontologies]], as formal models of representation with explicitly defined concepts and named relationships linking them, are used to address the issue of [[semantic heterogeneity]] in data sources. In domains like [[bioinformatics]] and [[biomedicine]], the rapid development, adoption and public availability of ontologies [http://www.bioontology.org/repositories.html#obo] has made it possible for the [[data integration]] community to leverage them for [[semantic integration]] of data and information.

==The role of ontologies==

Ontologies enable the unambiguous identification of entities in heterogeneous information systems and assertion of applicable named relationships that connect these entities together. Specifically, ontologies play the following roles:

* Content Explication&lt;ref name="wache"/&gt;
The ontology enables accurate interpretation of data from multiple sources through the explicit definition of terms and relationships in the ontology.

* Query Model&lt;ref name="wache"/&gt;
In some systems like SIMS,&lt;ref name="arens"&gt;{{cite conference |author1=Y. Arens |author2=C. Hsu |author3=C.A. Knoblock | title=Query Processing in sims information mediator | year=1996 | url=http://www.isi.edu/integration/papers/arens98-agents.pdf}}&lt;/ref&gt; the query is formulated using the ontology as a global query schema.

* Verification&lt;ref name="wache"/&gt;
The ontology verifies the mappings used to integrate data from multiple sources. These mappings may either be user specified or generated by a system.

===Approaches using ontologies for data integration===
There are three main architectures that are implemented in ontology-based data integration applications,&lt;ref name="wache"/&gt; namely, 
;Single ontology approach: A single ontology is used as a global reference model in the system. This is the simplest approach as it can be simulated by other approaches.&lt;ref name="wache"/&gt; SIMS&lt;ref name="arens"/&gt;  a prominent example of this approach.  The Structured Knowledge Source Integration component of [[Cyc|Research Cyc]] is another prominent example of this approach.&lt;ref&gt;http://www.cyc.com/content/semantic-knowledge-source-integration&lt;/ref&gt;&lt;ref&gt;http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/2299&lt;/ref&gt; (Title = Harnessing Cyc to Answer Clinical Researchers' Ad Hoc Queries)

;Multiple ontologies: Multiple ontologies, each modeling an individual data source, are used in combination for integration. Though, this approach is more flexible than the single ontology approach, it requires creation of mappings between the multiple ontologies. Ontology mapping is a challenging issue and is focus of large number of research efforts in [[computer science]] [http://www.ontologymatching.org/]. The OBSERVER system&lt;ref name="mena"&gt;{{cite conference |author1=E. Mena |author2=V. Kashyap |author3=A. Sheth |author4=A. Illarramendi | title=OBSERVER: An Approach for Query Processing in Global Information Systems based on Interoperation across Pre-existing Ontologies | year=1996 | url=http://dit.unitn.it/~p2p/RelatedWork/Matching/MKSI96.pdf}}&lt;/ref&gt; is an example of this approach.

;Hybrid approaches: The hybrid approach involves the use of multiple ontologies that subscribe to a common, top-level vocabulary.&lt;ref name="goh"&gt;{{cite conference | author=Cheng Hian Goh | title=Representing and Reasoning about Semantic Conflicts in Heterogeneous Information Systems | year=1997 | url=http://context2.mit.edu/coin/publications/goh-thesis/goh-thesis.pdf}}&lt;/ref&gt;  The top-level vocabulary defines the basic terms of the domain. Thus, the hybrid approach makes it easier to use multiple ontologies for integration in presence of the common vocabulary.

==See also==
* [[Data mapping]]
* [[Enterprise application integration]]
* [[Enterprise information integration]]
* [[Ontology mapping]]
* [[Schema matching]]

==References==

&lt;references/&gt;

==External links==
*[http://sid.cps.unizar.es/OBSERVER/ OBSERVER home page]
*[http://www.cyc.com/content/semantic-knowledge-source-integration Cyc Semantic Knowledge Source Integration (SKSI)]

[[Category:Ontology (information science)]]
[[Category:Data management]]</text>
      <sha1>q65uvjpq2t7vmaw2kildrit3buhe3yh</sha1>
    </revision>
  </page>
  <page>
    <title>XLDB</title>
    <ns>0</ns>
    <id>14426202</id>
    <revision>
      <id>748738496</id>
      <parentid>738690280</parentid>
      <timestamp>2016-11-10T02:14:49Z</timestamp>
      <contributor>
        <username>Green Cardamom</username>
        <id>8931761</id>
      </contributor>
      <minor />
      <comment>Fix url error in cite template using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7605" xml:space="preserve">'''XLDB''' refers to '''eXtremely Large [[Database|Data Bases]]'''.  The definition of ''extremely large'' refers to data sets that are too big in terms of volume (too much), and/or velocity (too fast), and/of variety (too many places, too many formats) to be handled using conventional solutions.

== History ==

In October 2007 the XLDB experts gathered at [[SLAC]] for the [https://web.archive.org/web/20080417002612/http://www-conf.slac.stanford.edu/xldb07/ First Workshop on Extremely Large Databases]. As a result, the XLDB research community was formed. to meet rapidly growing demands, in addition to the original invitational workshop, an open conference, tutorials, and annual satellite events on different continents were added. The main event, held annually at Stanford gathers over 300 technically savvy attendees. XLDB is one of the premier database events catered towards both academic and industrial communities.

== Goals ==

The main goals of this community include:&lt;ref&gt;{{ cite web | url=http://www-conf.slac.stanford.edu/xldb09/docs/xldb09_welcomeTalk.ppt | year=2009 | last=Becla| first=Jacek | title=XLDB 3 Welcome | accessdate=2009-08-29 }}&lt;/ref&gt;

* Identify trends, commonalities and major roadblocks related to building extremely large databases
* Bridge the gap between users trying to build extremely large databases and database solution providers worldwide
* Facilitate development and growth of practical technologies for extremely large data stores

== XLDB Community ==
As of 2013, the community consisted of about a thousand members including:
# Scientists who develop, use, or plan to develop or use XLDB for their research, from laboratories.
# Commercial users of XLDB.
# Providers of database products, including commercial vendors and representatives from open source database communities.
# Academic database researchers.

== XLDB Conferences, Workshops and Tutorials ==
The community meets annually at [[Stanford]] where the main event is held each fall, usually in September. These who live too far from California to attend have the opportunity to attend satellite events, organized annually around May/June either in [[Asia]] or in [[Europe]].

A detailed report is produced after each workshop.

{| class="wikitable"
|-
! Year
! Place
! Link
! Report
! Comments
|-
| 2015
| [[Stanford]]
| [https://web.archive.org/web/20150521105100/http://www-conf.slac.stanford.edu/xldb2015/]
|
| 8th XLDB Conference
|-
| 2014
| [http://www.on.br/ Observatório Nacional], [[Rio_de_Janeiro]]
| [https://web.archive.org/web/20150219081443/http://xldb-rio2014.linea.gov.br/]
|
| Satellite XLDB Workshop in South America
|-
| 2014
| [[Stony_Brook_University]]
| [https://web.archive.org/web/20150521052839/http://www3.cs.stonybrook.edu/~xldb/]
|
| XLDB-Healthcare Workshop
|-
| 2013
| [[Stanford]]
| [https://conf-slac.stanford.edu/xldb-2013/]
|
| 7th XLDB Conference
|-
| 2013
| [[CERN]], [[Geneva]]/[[Switzerland]]
| [http://xldb-europe-workshop-2013.web.cern.ch/]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
|
| Satellite XLDB Workshop in Europe
|-
| 2012
| [[Stanford]]
| [http://www-conf.slac.stanford.edu/xldb2012/]
| [http://www.jstage.jst.go.jp/article/dsj/12/0/12_12_023/_pdf]
| 6th XLDB Conference, Workshop &amp; Tutorials
|-
| 2012
| [[Beijing]], [[China]]
| [https://web.archive.org/web/20120708164351/http://idke.ruc.edu.cn/xldb/www.xldb-asia.org/home.html]
| [http://www.xldb.org/wp-content/uploads/2012/09/XLDBAsia2012Report.pdf]
| Satellite XLDB Conference in Asia
|-
| 2011
| [[SLAC]]
| [https://web.archive.org/web/20110426125951/http://www-conf.slac.stanford.edu/xldb2011/]
| [http://www.jstage.jst.go.jp/article/dsj/11/0/37/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 5th XLDB Conference and Workshop
|-
| 2011
| [[Edinburgh]], [[UK]]
| [https://web.archive.org/web/20160303221547/http://xldb.eu/xldb_europe_2011/]
| not available
| Satellite XLDB Workshop in Europe
|-
| 2010
| [[SLAC]]
| [https://web.archive.org/web/20110727234052/http://www-conf.slac.stanford.edu/xldb2010/]
| [http://www.jstage.jst.go.jp/article/dsj/9/0/9_MR1/_article]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 4th XLDB Conference and Workshop
|-
| 2009
| [[Lyon]], [[France]]
| [https://web.archive.org/web/20110727234623/http://www-conf.slac.stanford.edu/xldb2009/]
| [http://www.jstage.jst.go.jp/article/dsj/8/0/MR1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 3rd XLDB Workshop
|-
| 2008
| [[SLAC]]
| [https://web.archive.org/web/20110727234818/http://www-conf.slac.stanford.edu/xldb2008/]
| [http://www.jstage.jst.go.jp/article/dsj/7/0/196/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 2nd XLDB Workshop
|-
| 2007
| [[SLAC]]
| [https://web.archive.org/web/20110727235121/http://www-conf.slac.stanford.edu/xldb2007/]
| [http://www.jstage.jst.go.jp/article/dsj/7/0/1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 1st XLDB Workshop
|}

== Tangible results ==

The XLDB events led to initiating the effort of building a new open source, science database, [https://web.archive.org/web/20090220121225/http://scidb.org/ SciDB].&lt;ref&gt;{{ cite web | url=http://www.jstage.jst.go.jp/article/dsj/7/0/88/_pdf  | year=2008 | last=Becla| first=Jacek | title=Report from the SciDB Workshop |accessdate=2008-09-29}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

The XLDB organizers started defining a [http://www.xldb.org/science-benchmark/ science benchmark] for scientific data management systems called SS-DB.

At [http://xldb.org/2012|XLDB 2012]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} the XLDB organizers announced that two major databases that support arrays as first-class objects ([[MonetDB]] SciQL and [[SciDB]]) have formed a working group in conjunction with XLDB. This working group is proposing a common syntax (provisionally named “ArrayQL”) for manipulating arrays, including array creation and query.

== References ==
{{reflist}}

== Further reading ==
* Pavlo A., Paulson E., Rasin A., Abadi D. J., Dewitt D. J., Madden S., and Stonebraker M., ''A Comparison of Approaches to Large-Scale Data Analysis," Proceedings of the 2009 ACM SIGMOD, http://web.archive.org/web/20090611174944/http://database.cs.brown.edu:80/sigmod09/benchmarks-sigmod09.pdf
* Becla, J., et al. 2006, ''Designing a multi-petabyte database for LSST,'' http://arxiv.org/abs/cs/0604112
* Becla, J., &amp; Wang, D. L. 2005, ''Lessons Learned from Managing a Petabyte'', downloaded from http://web.archive.org/web/20110604223735/http://www.slac.stanford.edu/pubs/slacpubs/10750/slac-pub-10963.pdf on 2007-11-25.
* Bell, G., Gray, J., &amp; Szalay, A. 2005, ''Petascale computations systems: Balanced cyberinfrastructure in a data-centric world,'' http://arxiv.org/abs/cs/0701165
* Duellmann, D. 1999, ''Petabyte Databases'', ACM SIGMOD Record, vol. 28, p. 506, http://web.archive.org/web/20071012015357/http://www.sigmod.org/sigmod/record/issues/9906/index.html#TutorialSessions.
* Hanushevsky, A., &amp; Nowak, M. 1999, ''Pursuit of a Scalable High Performance Multi-Petabyte Database'', 16th IEEE Symposium on Mass Storage Systems, pp. 169–175, http://citeseer.ist.psu.edu/217883.html.
* Shiers, J., ''Building Very Large, Distributed Object Databases'', downloaded from http://web.archive.org/web/20070915101842/http://wwwasd.web.cern.ch:80/wwwasd/cernlib/rd45/papers/dbprog.html on 2007-11-25.

[[Category:Types of databases]]
[[Category:Data management]]</text>
      <sha1>nz60v4y81ouc3hcxgbesd2q0ejvpg8t</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data structures</title>
    <ns>14</ns>
    <id>691150</id>
    <revision>
      <id>721795166</id>
      <parentid>721793566</parentid>
      <timestamp>2016-05-24T02:53:35Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="703" xml:space="preserve">{{catdiffuse}}

{{Commons cat|Data structures}}
{{Category see also|Data types}}

In [[computer science]], a '''[[data structure]]''' is a way of storing [[data]] in a computer so that it can be used efficiently. Often a carefully chosen data structure will allow a more efficient [[algorithm]] to be used. The choice of the data structure must begin from the choice of an [[abstract data structure]].

{{Cat main|Data structures}}

== See also ==

* [[:Category:Abstract data types]]
* [[:Category:Hashing]]
* [[:Category:Computer file formats]]

[[Category:Algorithms and data structures]]
[[Category:Computer data|Structures]]
[[Category:Data management|Structures]]
[[Category:Computer programming]]</text>
      <sha1>emihnvooajnzusv07pkmismdrynjzv5</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Databases</title>
    <ns>14</ns>
    <id>2276471</id>
    <revision>
      <id>761378468</id>
      <parentid>732428061</parentid>
      <timestamp>2017-01-22T17:37:50Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* top */Cleaning up... using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="309" xml:space="preserve">{{Commonscat|Databases}}
{{distinguish|Category:Database software}}

:*'''[[Database]]s'''

{{clr}}
::{{Cat main|Database}}
:::::::{{cat see also|Digital libraries}}

{{catdiffuse}}

{{Database}}
{{Databases}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:Information retrieval systems]]</text>
      <sha1>drsbmwcdombsffa1mp31jztjri4m4x1</sha1>
    </revision>
  </page>
  <page>
    <title>Virtual facility</title>
    <ns>0</ns>
    <id>13444144</id>
    <revision>
      <id>650168862</id>
      <parentid>597578169</parentid>
      <timestamp>2015-03-06T17:13:15Z</timestamp>
      <contributor>
        <username>Fraulein451</username>
        <id>16206019</id>
      </contributor>
      <comment>/* References */ added reflist</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3277" xml:space="preserve">{{Advert|article|date=May 2013}}
[[Image:VirtualFacility.jpg|thumb|250px|A Virtual Facility snapshot created with 6SigmaDC software.|right]]
A '''Virtual Facility''' (VF) is a highly realistic digital representation of a [[data center]] (primarily). The term virtual in Virtual Facility refers to the use of the word as in [[Virtual reality|Virtual Reality]] rather than the abstraction of computer resources as in [[platform virtualization]]. The VF mirrors the characteristics of the physical facility over time and allows modeling all relevant characteristics of a physical data center with a high degree of precision. 

==VF Model includes==

* Three-dimensional physical facility layout
* Network connectivity of facility equipment
* Full inventory of facility equipment, including electronics and electrical systems such as [[Power distribution unit|Power Distribution Units]] (PDU’s) and [[Uninterruptible power supplies|Uninterruptible Power Supplies]] (UPS’s)
* Full air conditioning system (ACU’s) and controls within the room

The term Virtual Facility was introduced by Future Facilities, a data centre design consultancy focused on delivering Design and Operational solutions to address the emerging environmental problems facing the modern Mission Critical Facility (MCF). The concept is in essence a convergence of the fields of [[Virtual reality|Virtual Reality]] (VR), [[Computer simulation|Computer Simulation]] and [[Expert systems|Expert Systems]], applied to the specific domain of facilities.

The VF type of computer simulation allows detailed analysis and prototyping of air flow in the data center by making use of [[Computational fluid dynamics|Computational Fluid Dynamics]] (CFD) techniques. This in turn allows the air flow and temperatures of the facility to be analyzed visually ([[Scientific visualization|Scientific Visualisation]]) and numerically to study and predict what will happen in the real facility. The importance of scientific methods in design of mission critical facilities has become a necessity, since the performance gains predicted by [[Moore’s law|Moore's Law]] go hand in hand with a rise in power and heat dissipated by equipment. Rules of thumb have proven to be no longer adequate.

==VF design purposes==

* Green field design
* Asset management
* Troubleshooting existing data centers
* Making existing data centers more resilient
* Making existing data centers more energy efficient
* Cost prediction
* Staff training
* Capacity planning
* Load growth management
 
The VF is now being employed by many large organizations as a way of virtually assessing a situation before having to spend huge sums of money trying to solve a problem in the real facility.

It is essential to know whether adding new equipment or changing equipment will cause a logistical or thermal problem.  The VF allows the designer or operator to assess the best course of action and gives in depth understanding on unintuive behaviours.

==References==
{{reflist}}
* {{Citation
  | last = Seymour
  | first = Mark
  | title = Virtual Data Centre Design. A blueprint for success
  | url=http://www.futurefacilities.com/newsarticles/articles/commerzbankarticlesummerZDTjournal.pdf
  | accessdate = 2007-09-26}}

[[Category:Data management]]</text>
      <sha1>b2iwp7wbtpckn3dvms3k82vdhjxd55i</sha1>
    </revision>
  </page>
  <page>
    <title>Integration competency center</title>
    <ns>0</ns>
    <id>15014956</id>
    <revision>
      <id>753839619</id>
      <parentid>599128956</parentid>
      <timestamp>2016-12-09T14:05:49Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[data profiling]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12804" xml:space="preserve">{{ad|date=March 2014}}
{{peacock|date=March 2014}}
The '''integration competency center''' (ICC), sometimes referred to as an '''integration center of excellence''' (COE), is a [[shared services|shared service]] function within an organization, particularly large corporate enterprises as well as public sector institutions, for performing methodical [[data integration]], [[system integration]] or [[enterprise application integration]]. 

[[Data integration]] allows companies to access their enterprise data and functions, fragmented across disparate systems, in order to create a combined, accurate, and consistent view of their core information as well as process assets and leverage them across the enterprise to drive business decisions and operations.  System integration is the bringing together of component subsystems into one system and ensuring that they function together effectively. Enterprise application integration enables efficient information exchanges and business process automation across separate computer applications in a [[Cohesion (computer science)|cohesive]] fashion.

== Overview ==
The term may be better understood by examining each of the three words that comprise the acronym. '''Integration''' refers to the objective of the ICC to take a holistic perspective and optimize certain qualities such as cost efficiency, organizational agility and effectiveness, operational risk, customer (internal or external) experience, etc. across multiple functional groups. '''Competency''' refers to the expertise, knowledge or capability that the ICC offers as services.  '''Center''' means that the service is managed or coordinated from a common (central) point independent from the functional areas that it supports. 

Large organizations are usually sub-divided into functional areas such as marketing, sales, distribution, finance, human resources to name just a few. These functional groups have separate operations and are ''vertically integrated'' and are therefore sometimes referred to as "silos" or "stovepipes".  From an organizational perspective, an ICC is a group of people with special skills, who are centrally coordinated, and offer services to accomplish a mission that requires separate functional areas to work together. 

Key objectives of an ICC are:

* Lead and support enterprise integration (data, system and process) projects with the cooperation/coordination of subject matter experts
* Promote Enterprise integration as a formal discipline. For example, data integration will include [[data warehousing]], [[data migration]], [[data quality]] management, data integration for [[service oriented architecture]] deployments, and [[data synchronization]]. Similar system integration will include common messaging services, business service virtualization etc.
* Develop staff specialists in integration processes and operations and leverage their expertise company-wide
* Assess and select integration technology and tools from the marketplace
* Manage integration pilots and projects across the organization
* Optimize integration investments across the enterprise level
* Leverage economies of scale for the integration tools portfolio at enterprise level

ICCs allow companies to:
* Optimize scarce resources by combining integration skills, resources, and processes into one group
* Reduce project delivery times and development and maintenance costs through effectiveness and efficiency
* Improve ROI through creation and reuse of enterprise assets like source definitions, application interfaces, and codified business rules
* Decrease duplication of integration related effort across the enterprise
* Build on past successes instead of reinventing the wheel with each project
* Lower total technology cost of ownership by leveraging technology investments across multiple projects

An ICC may be a temporary group in support of a [[program management|program]] or a permanent part of the organization.  Furthermore, ICC’s can be established at various scales or levels; within a division of a company, at the enterprise level, or across multiple companies in a supply chain.

== History ==
The term "integration competency center" and its acronym ICC was popularized by Roy Schulte of [[Gartner]] in a series of articles and conference presentations beginning in 2001 with ''The Integration Competency Center'' {{citation missing|date=February 2013}}&lt;!--[Ref SPA-14-0456] mostly useless, cannot be found anywhere--&gt;. He picked up the term from one of his colleagues, Gary Long, who found some of his clients using it (they took the established term “competency center” and applied it to integration). Prior to that (from 1997 to 2001) Gartner had been referring to it as the ''central integration team''. The concept itself (even before it was given a label) goes back to 1996 in one of Gartner’s first reports on integration. {{citation missing|date=February 2013}}

A major milestone was the publication in 2005 of the first book on the topic: ''Integration Competency Center: An Implementation Methodology''&lt;ref name="ICC"&gt;John G. Schmidt and David Lyle (2005), ''Integration Competency Center, An Implementation Methodology,'' ISBN 0-9769163-0-4&lt;/ref&gt; by ''John G. Schmidt'' and ''David Lyle''. The book introduced five ICC organizational models and explored the people, process and technology dimensions of ICC’s.  Several reviews of the book can be found at [http://blogs.ittoolbox.com/eai/business/archives/soa-competency-center-5731  IT Toolbox] and at [http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304 Amazon]. The concept of integration as a competency in the IT domain has now survived for over 10 years and appears to be picking up momentum and broad-based acceptance. 

These days, ICC’s are often called, integration center of excellence, SOA center of excellence, the data management center of excellence and other variants. The most advanced ICC's are using [[Lean Integration]] practices to optimize end-to-end processes and to drive continuous improvements. Universities are also beginning to include integration topics in their MBA programs and computer science curricula. For example, The College of Information Sciences and Technology at Penn State University has established a [http://ist.psu.edu/facultyresearch/facilities/eii/  Enterprise Informatics and Integration Center] with the following mission:

"''The Enterprise Informatics and Integration Center (EI²) will actively engage industry, non-profit, and government agency leaders to address critical issues in enterprise processes, knowledge management, and decision making.''"

== Operating models ==
There are a number of ways an ICC can be organized and a wide range of responsibilities with which it can be chartered. The ICC book&lt;ref name="ICC"/&gt; introduced five ICC organizational models and explored the people, process and technology dimensions of ICCs. They include:

=== Best practices ICC ===
The primary function of this ICC model is to document best practices. It does not include a central support or development team to implement those standards across projects, and probably not metadata either. To implement a best practices ICC, companies need a flexible development environment that supports diverse teams and that enables the team to enhance and extend existing systems and processes. Such a team might be a subset of an existing enterprise architecture capability and generally consists of a small number of staff (1-5).

=== Standard services ICC ===
A standard services ICC provides the same knowledge leverage as a best practices ICC, but enforces technical consistency in software development and hardware choices. A standard services ICC focuses on processes, including standardizing and enforcing naming conventions, establishing metadata standards, instituting change management procedures, and providing standards training. This type of ICC also reviews emerging technologies, selects vendors, and manages hardware and software systems.  This style of ICC is often tightly linked with the enterprise architecture team and may be slightly larger than a typical best practices ICC.

=== Shared services ICC ===
A shared services ICC provides a supported technical environment and services ranging from development support all the way through to a help desk for projects in production. This type of ICC is significantly more complex than a best practices or Standard Services model. It establishes processes for knowledge management, including product training, standards enforcement, technology benchmarking, and metadata management, and it facilitates impact analysis, software quality, and effective use of developer resources across projects. The organizational structure of a Shared Services ICC is sometimes referred to as a hybrid or federated model which often includes a small central coordinating team plus dotted-line reporting relationships with multiple distributed teams.

=== Central services ICC ===
A central services ICC controls integration across the enterprise. It carries out the same processes as the other models, but in addition usually has its own budget and a charge-back methodology. It also offers more support for development projects, providing management, development resources, [[data profiling]], data quality, and unit testing. Because a central services ICC is more involved in development activities than the other models, it requires a production operator and a data integration developer.  The staff in a central services ICC does not necessarily need to be a central location and may be distributed geographically; the important distinction is that the staffs have a solid-line reporting relationship to the ICC Director.  The size of these teams can vary and may be as large as 10%-15% of the IT staff in an organization.

=== Self service ICC ===
The self-service ICC represents the highest level of maturity in an organization.  The ICC itself may be almost invisible in that its functions are so ingrained in the day-to-day systems development life-cycle and its operations are so tightly integrated with the infrastructure that it may require only small central team to sustain itself.  This ICC model achieves both a highly efficient operation and provides an environment where independent development and innovation can flourish. This goal is achieved by strict enforcement of a set of application integration standards through automated processes enabled by tools and systems.

== Key challenges ==
ICC as a concept is fairly simple. It is embodiment of the IT management best practices to deliver shared services. However, being an organizational concept, it is far more challenging to implement in practice than the conceptual view because every organization has different DNA and it takes specific personalization/customization effort for ICC that makes the ICC initiative successful. Here are some of the common challenges in ICC establishment journey:
* Change management in terms of technology, processes, organization structure
* Ability of the organization to deal with the pace and quantum of change
* Alignment of stakeholders and process owners for ICC strategy
* Inappropriate ownership level for ICC program and lack of senior management sponsorship
* Highly tactical focus and business program level constraints
* Ignoring foundation elements and jumping to implementation directly
* Inappropriate funding

These issues are important to consider when embarking on the ICC investment since the last leg of the implementation of ICC that's what matters most. Intellectual definition of ICC that is not implemented in the organisation has no real value for the enterprise.

== See also ==
* [[Lean Integration]]

== References ==
&lt;references/&gt;

== Further reading ==
* Maurizio Lenzerini (2002). "Data Integration: A Theoretical Perspective". PODS 2002: 243-246.

== External links ==
* Integration Competency Center book: (http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304)
* [[Informatica]] ICC Blog: (http://blogs.informatica.com/perspectives/category/data-integration/integration-competency-centers/)
* Gartner paper  (http://www.ebizq.net/topics/tech_in_biz/features/5360.html)
* Integration Consortium: (http://www.integrationconsortium.org)
* Infosys ICC Blogs (http://www.infosysblogs.com/bpm-eai/integration_competency_center_icc)
* ICC Handbook (http://www.unthink.fi/Global/PDF/ICC-Handbook.pdf)
* Integration Warstories - article about avoiding ICC pitfalls (http://integrationwarstories.com/2013/10/25/avoiding-pitfalls-of-integration-competency-centers/)

[[Category:Data management]]
[[Category:Software development philosophies]]
[[Category:Information technology]]</text>
      <sha1>k0ccn6uzfeeyybw3nxjjnv4tnc5mm27</sha1>
    </revision>
  </page>
  <page>
    <title>Transaction data</title>
    <ns>0</ns>
    <id>15348791</id>
    <revision>
      <id>748126272</id>
      <parentid>748124215</parentid>
      <timestamp>2016-11-06T13:57:40Z</timestamp>
      <contributor>
        <ip>41.221.99.238</ip>
      </contributor>
      <comment>/* Data Warehousing */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1351" xml:space="preserve">{{Unreferenced|date=July 2010}}

'''Transaction data''' are data describing an event (the change as a result of a [[Transaction processing|transaction]]) and is usually described with verbs. Transaction data always has a time dimension, a numerical value and refers to one or more objects (i.e. the [[reference data]]).

Typical transactions are:
* Financial: orders, invoices, payments
* Work: Plans, activity records
* [[Logistics]]: Deliveries, storage records, travel records, etc.

Typical [[transaction processing system]]s (systems generating transactions) are [[SAP ERP|SAP]] and [[Oracle Financials]].

==Records management==
{{Main|Records management}}
Recording and retaining transactions is called [[records management]]. The record of the transaction is stored in a place where the [[wikt:retention|retention]] can be guaranteed and where data are archived/removed following a [[retention period]]. The format of the transaction can be data (to be stored in a database), but it can also be a document.

==Data Warehousing==
Transaction data can be summarised in a [[Data warehouse]], which helps accessibility and analysis of the data.

==See also==
* [[Data modeling]]
* [[Data architecture]]
* [[Information Lifecycle Management]]
* [[reference data]]

[[Category:Data management]]
[[Category:Transaction processing]]


{{compsci-stub}}</text>
      <sha1>r02veuowjchqvqamcwsedntxf6yfi1j</sha1>
    </revision>
  </page>
  <page>
    <title>Information architecture</title>
    <ns>0</ns>
    <id>185945</id>
    <revision>
      <id>762880615</id>
      <parentid>762880536</parentid>
      <timestamp>2017-01-31T05:57:44Z</timestamp>
      <contributor>
        <username>JueLinLi</username>
        <id>29760810</id>
      </contributor>
      <comment>Reverted to revision 762078237 by [[Special:Contributions/Swpb|Swpb]] ([[User talk:Swpb|talk]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10201" xml:space="preserve">{{Information science}}
'''Information architecture''' ('''IA''') is the structural design of shared [[information]] environments; the art and science of organizing and labelling [[website]]s, [[intranet]]s, [[online communities]] and [[software]] to support usability and findability; and an emerging [[community of practice]] focused on bringing principles of [[design]] and [[architecture]] to the digital landscape.&lt;ref name = "What"&gt;{{Cite journal | title = What is IA? | publisher = Information Architecture Institute | url = http://www.iainstitute.org/documents/learn/What_is_IA.pdf | format = [[PDF]] | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt;  Typically, it involves a [[Scientific modelling|model]] or [[concept]] of [[information]] that is used and applied to activities which require explicit details of complex [[information system]]s. These activities include [[library]] systems and [[database]] development.

Information architecture is considered to have been founded by [[Richard Saul Wurman]].&lt;ref name = "Richard Saul Wurman, Cooper-Hewitt"&gt;{{cite web|title=Richard Saul Wurman awarded for Lifetime Achievement|url=http://wurman.com/rsw/|publisher=Smithsonian Cooper-Hewitt, National Design Museum|accessdate=19 April 2014}}&lt;/ref&gt; Today there is a growing network of active IA specialists who constitute the [[Information Architecture Institute]].&lt;ref&gt;{{Cite journal | title = Join the IA Network | publisher = Information Architecture Institute | url = http://www.iainstitute.org/en/network/ | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt;

==Definition==
''Information architecture'' has somewhat different meanings in different branches of [[Information system|IS]] or [[Information technology|IT]]:
# The structural design of shared information environments.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}
# The art and science of organizing and labeling web sites, intranets, online communities, and software to support [[findability]] and [[usability]].&lt;ref name="What"/&gt;&lt;ref&gt;Morville&amp;nbsp;&amp;amp; Rosenfeld (2007). p.&amp;nbsp;4. "The art and science of shaping information products and experienced to support usability and findability."&lt;/ref&gt;
# An emerging [[community of practice]] focused on bringing principles of design and architecture to the digital landscape.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}&lt;ref&gt;Resmini, A. &amp; Rosati, L. (2012). A Brief History of Information Architecture. ''Journal of Information Architecture''. Vol. 3, No. 2. [Available at http://journalofia.org/volume3/issue2/03-resmini/]. Originally published in Resmini, A. &amp; Rosati L. (2011). ''Pervasive Information Architecture''. Morgan Kauffman. (Edited by the authors).&lt;/ref&gt;
# The combination of organization, labeling, search and navigation systems within websites and intranets.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}
# Extracting required parameters/data of Engineering Designs in the process of creating a knowledge-base linking different systems and standards.  
# A subset of [[data architecture]] where usable data (a.k.a. information) is constructed in and designed or arranged in a fashion most useful or empirically holistic to the users of this data.
# The practice of organizing the information / content / functionality of a web site so that it presents the best user experience it can, with information and services being easily usable and findable (as applied to web design and development).&lt;ref&gt;{{Cite web|url=https://developer.mozilla.org/en-US/docs/Glossary/Information_architecture|title=Information Architecture|last=|first=|date=|website=|publisher=Mozilla Developer Network|access-date=}}&lt;/ref&gt;

=== Debate ===
The difficulty in establishing a common definition for "information architecture" arises partly from the term's existence in multiple fields.  In the field of [[systems design]], for example, information architecture is a component of [[enterprise architecture]] that deals with the information component when describing the structure of an enterprise.

While the definition of information architecture is relatively well-established in the field of systems design, it is much more debatable within the context of online information systems (i.e., websites). Andrew Dillon refers to the latter as the "big IA–little IA debate".&lt;ref&gt;{{Cite journal | last = Dillon | first = A | year = 2002 | title = Information Architecture in JASIST: Just where did we come from? | journal = Journal of the American Society for Information Science and Technology | volume = 53 | pages = 821–23 | issue = 10 | doi = 10.1002/asi.10090 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt; In the little IA view, information architecture is essentially the application of [[information science]] to [[web design]] which considers, for example, issues of classification and information retrieval. In the big IA view, information architecture involves more than just the organization of a website; it also factors in [[user experience]], thereby considering [[usability]] issues of [[information design]].

==Information architect==
About the term '''information architect''' [[Richard Saul Wurman]] wrote: "I mean architect as used in the words ''architect of foreign policy''. I mean architect as in the creating of systemic, structural, and orderly principles to make something work — the thoughtful making of either artifact, or idea, or policy that informs because it is clear."&lt;ref&gt;Wurman, "Introduction", in: ''Information Architects'' (1997). p. 16.&lt;/ref&gt;

==Notable people in information architecture==

===Pioneers===
*[[Richard Saul Wurman]]
*[[Peter Morville]]
*[[Louis Rosenfeld]]

===First generation===
*Jorge Arango
*[[Jesse James Garrett]]
*[[Adam Greenfield]]
*[[Peter Merholz]]
*[[Eric Reiss]]
*[[Donna Spencer]]
*[[Christina Wodtke]]

===Second generation===
*[[Abby Covert]]
*[[Andrew Hinton]]
*[[Dan Klyn]]
*[[Andrea Resmini]]

===Influencers===
*[[David Weinberger]]

== See also ==
{{Div col}}
* [[Applications architecture]]
* [[Card sorting]]
* [[Chief experience officer]]
* [[Content management]]
* [[Content strategy]]
* [[Controlled vocabulary]]
* [[Data management]]
* [[Data presentation architecture]]
* [[Digital humanities]]
* [[Ecological interface design]]
* [[Enterprise information security architecture]]
* [[Faceted classification]]
* [[Human factors and ergonomics]]
* [[Informatics]]
* [[Interaction design]]
* [[Process architecture]]
* [[Site map]]
* [[Social information architecture]]
* [[Tree testing]]
* [[User experience design]]
* {{section link|Visualization (graphics)|Knowledge visualization}}
* [[Wayfinding]]
* [[Web graph]]
* [[Web literacy]] (Infrastructure)
{{Div col end}}

== References ==
{{reflist}}

== Bibliography ==
* {{Cite book | editor-last1 = Wurman | editor-first1 = Richard Saul | editor1-link = Richard Saul Wurman | isbn = 1-888-00138-0 | url = http://www.amazon.com/dp/1888001380 | year = 1997 | title = Information Architects | edition = 1st | publisher = Graphis Inc. }}
* {{Cite book | last2 = Rosenfeld | first2 = Louis | author2-link = Lou Rosenfeld | last1 = Morville | first1 = Peter | author1-link = Peter Morville | isbn = 0-596-52734-9 | url = https://books.google.com/books?id=2d2Ry2hZc2MC&amp;printsec=frontcover&amp;dq=information+architecture#v=onepage&amp;q&amp;f=false | year = 2007 | title = Information architecture for the World Wide Web | edition = 3rd|publisher = O'Reilly &amp; Associates | place = Sebastopol, CA | ref=harv}}
* {{Cite book | last1 = Brown | first1 = Peter  | isbn = 0-471-48679-5 | url = http://www.amazon.com/dp/0471486795 | year = 2003 | title = Information Architecture with XML | edition = 1st | publisher = John Wiley &amp; Sons Ltd. }}
* {{Cite book | last1 = Wodtke | first1 = Christina | author1-link = Christina Wodtke | isbn = 0-321-60080-0 | url = https://books.google.com/books?id=Tp40QFGCU2sC | year = 2009 | title = Information Architecture - Blueprints for the Web | edition = 2nd | publisher = New Riders }}
* {{Cite book | last1 = Resmini | first1 = Andrea | last2 = Rosati | first2 = Luca | isbn = 0-123-82094-4 | url = https://books.google.com/books?id=ntWc13nSiNkC | year = 2011 | title = Pervasive Information Architecture - Designing Cross-channel User Experiences | edition = 1st | publisher = Morgan Kauffman }}

== Further reading ==
* {{cite book|author1=Wei Ding|author2=Xia Lin|title=Information Architecture: The Design and Integration of Information Spaces|url=https://books.google.com/books?id=-wy3RhKoWWQC|date= 15 May 2009 | publisher=Morgan &amp; Claypool |isbn=978-1-59829-959-5}}
* {{cite book|author1=Sue Batley|title=Information Architecture for Information Professionals|url=https://books.google.com/books?id=6g0PAQAAMAAJ|date=January 2007| publisher=Woodhead Publishing |isbn=978-1-84334-233-5}}
* {{cite book|author1=Earl Morrogh|title=Information Architecture: An Emerging 21st Century Profession
|url=https://books.google.com/books?id=JzlmQgAACAAJ&amp;dq|year=2003| publisher=Prentice Hall |isbn=9780130967466}}
* {{cite book|author1=Peter Van Dijck|title=Information Architecture for Designers: Structuring Websites for Business Success |url=https://books.google.com/books?id=Wy2sb0r_udYC&amp;dq|date=August 1, 2003| publisher=Rotovision|isbn=9782880467319}}
* {{cite book|author1=Alan Gilchrist|author2=Barry Mahon|title=Information Architecture: Designing Information Environments for Purpose|url=https://books.google.com/books?id=akxqAAAAMAAJ&amp;q|year=2004| publisher=Facet|isbn=9781856044875}}

{{Semantic Web}}

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Information architects]]
[[Category:Information governance]]
[[Category:Information science]]
[[Category:Information technology management]]
[[Category:Information technology]]
[[Category:Records management]]
[[Category:Technical communication]]
[[Category:Information architecture| ]]</text>
      <sha1>01t37r1ti7pg69hj4e5y3ygagws1ff8</sha1>
    </revision>
  </page>
  <page>
    <title>Long-lived transaction</title>
    <ns>0</ns>
    <id>17866900</id>
    <revision>
      <id>721205323</id>
      <parentid>687016731</parentid>
      <timestamp>2016-05-20T09:48:43Z</timestamp>
      <contributor>
        <username>R'n'B</username>
        <id>2300502</id>
      </contributor>
      <minor />
      <comment>Disambiguating links to [[Versioning]] (link changed to [[Version control]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1596" xml:space="preserve">{{Multiple issues|
{{Orphan|date=February 2009}}
{{No sources|date=October 2015}}
}}

A '''long-lived transaction''' is a [[Database transaction|transaction]] that spans multiple database transactions. The transaction is considered "long-lived" because its boundaries must, by necessity of business logic, extend past a single database transaction. A long-lived transaction can be thought of as a sequence of database transactions grouped to achieve a single atomic result.

A common example is a multi-step sequence of requests and responses of an interaction with a user through a web client.

A long-lived transaction creates challenges of [[concurrency control]] and [[scalability]].

A chief strategy in designing long-lived transactions is [[optimistic concurrency control]] with [[Version control|versioning]].

So much research work related to these long lived transactions was carried out by several professors from the Oxford University and Michigan State University and the Central University of Hyderabad. Dr. James from the Oxford University created several hypotheses for long-lived transactions. Dr Copperfield of the Michigan State University was regarded highly for his contributions in this field. Dr A B Sagar of Hyderabad Central University has also done very creative work in relating long-lived transactions with financial transactions in Microfinance.

However the study is not complete and is still open to challenges and research issues.

==See also==
*[[Long-running transaction]]

[[Category:Data management]]
[[Category:Transaction processing]]


{{software-eng-stub}}</text>
      <sha1>sqm2cvhumht8rgrrscfgeiz0aji8z0j</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Semantic Web</title>
    <ns>14</ns>
    <id>18014783</id>
    <revision>
      <id>601308308</id>
      <parentid>594005753</parentid>
      <timestamp>2014-03-26T05:41:52Z</timestamp>
      <contributor>
        <ip>99.65.176.161</ip>
      </contributor>
      <comment>del cat WWW.  Internet ages is a top level cat and having pages in both top level and one level down is redundant.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="244" xml:space="preserve">{{Cat main|Semantic Web}}
{{Commons cat|Semantic Web}}

[[Category:Internet ages|Web 3]]
[[Category:World Wide Web Consortium]] &lt;!-- the Semantic Web is a major W3C activity --&gt;
[[Category:Knowledge representation]]
[[Category:Data management]]</text>
      <sha1>mkcln7p7q7j6y333y8creg62oizg40t</sha1>
    </revision>
  </page>
  <page>
    <title>Schema crosswalk</title>
    <ns>0</ns>
    <id>18048026</id>
    <revision>
      <id>760224308</id>
      <parentid>746499655</parentid>
      <timestamp>2017-01-15T18:34:36Z</timestamp>
      <contributor>
        <username>Capvespre</username>
        <id>25212216</id>
      </contributor>
      <minor />
      <comment>#1Lib1Ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7635" xml:space="preserve">A '''schema crosswalk''' is a table that shows equivalent elements (or "fields") in more than one [[database schema]]. It maps the elements in one schema to the equivalent elements in another schema.

Crosswalk tables are often employed within or in parallel to [[enterprise systems]], especially when multiple systems are interfaced or when the system includes [[legacy system]] data. In the context of Interfaces, they function as a sort of internal [[Extract, Transform, Load|ETL]] mechanism.

For example, this is a [[metadata]] crosswalk from [[MARC standards|MARC]] to [[Dublin Core]]:

&lt;center&gt;
{| class="wikitable"
|-
! MARC field
! 
! Dublin Core element
|-
| $260c (Date of publication, distribution, etc.)
| →
| Date.Created
|-
| 522 (Geographic Coverage Note)
| →
| Coverage.Spatial
|-
| $300a (Physical Description)
| →
| Format.Extent
|}
&lt;/center&gt;

Crosswalks show people where to put the data from one scheme into a different scheme. They are often used by libraries, archives, museums, and other cultural institutions to translate data to or from [[MARC standards|MARC]], [[Dublin Core]], [[Text Encoding Initiative|TEI]], and other metadata schemes.  For example, say an archive has a MARC record in their catalog describing a manuscript.  If the archive makes a digital copy of that manuscript and wants to display it on the web along with the information from the catalog, it will have to translate the data from the MARC catalog record into a different format such as [[Metadata Object Description Schema|MODS]] that is viewable in a webpage.  Because MARC has different fields than MODS, decisions must be made about where to put the data into MODS. This type of "translating" from one format to another is often called "metadata mapping" or "field mapping," and is related to "[[data mapping]]", and "[[Semantic mapper|semantic mapping]]".

Crosswalks also have several technical capabilities.  They help databases using different metadata schemes to share information. They help metadata harvesters create union catalogs.  They enable search engines to search multiple databases simultaneously with a single query.

== Challenges for crosswalks ==

One of the biggest challenges for crosswalks is that no two metadata schemes are 100% equivalent.  One scheme may have a field that doesn't exist in another scheme, or it may have a field that is split into two different fields in another scheme; this is why you often lose data when mapping from a complex scheme to a simpler one.  For example, when mapping from MARC to Simple Dublin Core, you lose the distinction between types of titles:

&lt;center&gt;
{| class="wikitable"
|-
! MARC field
! 
! Dublin Core element
|-
| 210 Abbreviated Title
| →
| Title
|-
| 222 Key Title
| →
| Title
|-
| 240 Uniform Title
| → 
| Title
|-
| 242 Translated Title
| →
| Title
|-
| 245 Title Statement
| →
| Title
|-
| 246 Variant Title
| →
| Title
|}
&lt;/center&gt;

Simple Dublin Core only has one single "Title" element so all of the different types of MARC titles get lumped together without any further distinctions.  This is called "many-to-one" mapping. This is also why, once you've translated these titles into Simple Dublin Core you can't translate them back into MARC.  Once they're Simple Dublin Core you've lost the MARC information about what types of titles they are so when you map from Simple Dublin Core back to MARC, all the data in the "Title" element maps to the basic MARC 245 Title Statement field.&lt;ref&gt;[http://www.loc.gov/marc/dccross.html "Dublin Core to MARC Crosswalk,"] Network Development and MARC Standards Office, Library of Congress&lt;/ref&gt;

&lt;center&gt;
{| class="wikitable"
|-
! Dublin Core element
!
! MARC field
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|}
&lt;/center&gt;

This is why crosswalks are said to be "lateral" (one-way) mappings from one scheme to another.  Separate crosswalks would be required to map from scheme A to scheme B and from scheme B to scheme A.&lt;ref&gt;{{Cite book|title=Metadata fundamentals for all librarians|last=Caplan|first=Priscilla|publisher=American Library Association|year=2003|isbn=0838908470|location=Chicago|pages=39|quote=|via=}}&lt;/ref&gt;

===Difficulties in mapping===
Other mapping problems arise when:

*One scheme has one element that needs to be split up with different parts of it placed in multiple other elements in the second scheme ("one-to-many" mapping)
*One scheme allows an element to be repeated more than once while another only allows that element to appear once with multiple terms in it
*Schemes have different data formats (e.g. ''John Doe'' or ''Doe, John'')
*An element in one scheme is indexed but the equivalent element in the other scheme is not
*Schemes may use different controlled vocabularies
*Schemes change their standards over time

Some of these problems are simply not fixable. As Karen Coyle says in "''Crosswalking Citation Metadata: The University of California's Experience,''"

&lt;blockquote&gt;"The more metadata experience we have, the more it becomes clear that metadata perfection is not attainable, and anyone who attempts it will be sorely disappointed.  When metadata is crosswalked between two or more unrelated sources, there will be data elements that cannot be reconciled in an ideal manner.  The key to a successful metadata crosswalk is intelligent flexibility.  It is essential to focus on the important goals and be willing to compromise in order to reach a practical conclusion to projects."&lt;ref&gt;&lt;u&gt;in&lt;/u&gt; "Metadata in Practice" Diane I. Hillmann and Elaine L. Westbrooks, eds., American Library Association, Chicago, 2004, p. 91.&lt;/ref&gt;&lt;/blockquote&gt;

==Examples==

MARC to Dublin Core (Library of Congress) 
http://loc.gov/marc/marc2dc.html

Dublin Core to MARC21 (Library of Congress) 
http://www.loc.gov/marc/dccross.html

Dublin Core to UNIMARC (UKOLN)
http://www.ukoln.ac.uk/metadata/interoperability/dc_unimarc.html

TEI to and from MARC
http://purl.oclc.org/NET/teiinlibraries

FGDC to USMARC (Alexandria) 
http://www.alexandria.ucsb.edu/public-documents/metadata/fgdc2marc.html

ONIX to MARC21 (LC) 
http://www.loc.gov/marc/onix2marc.html

VRA to MARC (Indiana University) 
http://php.indiana.edu/%7Efryp/marcmap.html

Metadata Mappings (MIT Library)
http://web.archive.org/web/20080720134522/http://libraries.mit.edu/guides/subjects/metadata/mappings.html

Mapping Between Metadata formats (UKOLN) 
http://www.ukoln.ac.uk/metadata/interoperability/

International Metadata Standard Mappings (Academia Sinica) 
http://www.sinica.edu.tw/%7Emetadata/standard/mapping-foreign_eng.htm

JATS to MARC
http://webservices.itcs.umich.edu/mediawiki/jats/index.php/JATS-to-MARC_mapping

== See also ==
* [[Meta tag]]
* [[Metadata]]
* [[Database]]

==References==
 {{Reflist}}

==External links==
* [http://www.oclc.org/research/researchworks/schematrans/default.htm "Metadata Crosswalk Depository" (SchemaTrans)](OCLC)
* [http://www.ukoln.ac.uk/metadata/interoperability "Mapping Between Metadata Formats"] (UKOLN)
* [http://www.getty.edu/research/conducting_research/standards/intrometadata/path.html "Crosswalks the Path to Universal Access?"] (Getty)
* [http://www.dlib.org/dlib/june06/chan/06chan.html "Metadata Interoperability and Standardization - A Study of Methodology Part I"] (D-Lib)

[[Category:Data management]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata]]
[[Category:Technical communication]]</text>
      <sha1>sxjzi8iw00fcxqqojkeu14hl24y0flc</sha1>
    </revision>
  </page>
  <page>
    <title>British Oceanographic Data Centre</title>
    <ns>0</ns>
    <id>19075690</id>
    <revision>
      <id>737252362</id>
      <parentid>736857128</parentid>
      <timestamp>2016-09-01T16:29:26Z</timestamp>
      <contributor>
        <username>ArmbrustBot</username>
        <id>18663209</id>
      </contributor>
      <minor />
      <comment>/* External links */re-categorisation per [[WP:CFDS|CFDS]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8523" xml:space="preserve">{{Infobox organization
|name         = British Oceanographic Data Centre
|image        = Bodc logo.jpg
|image_border = 
|size         = 150px
|alt          = British Oceanographic Data Centre
|caption      = 
|map          =
|msize        = 
|malt         = 
|mcaption     = 
|abbreviation = 
|motto        = 
|formation    = 1969
|extinction   = 
|type         = 
|status       = 
|purpose      = 
|headquarters = 
|location     = [[Liverpool]], [[UK]]&lt;br&gt;
[[UK postcodes|L3 5DA]]
|region_served =
|membership   = 
|language     =
|leader_title = Head of BODC
|leader_name  = Dr Graham Allen
|main_organ   = 
|parent_organization = [[Natural Environment Research Council]] (NERC)
|affiliations = 
|num_staff    = approx. 500
|num_volunteers =
|budget       = 
|website      = {{URL|http://www.bodc.ac.uk/}}
|remarks      =
}}
The '''British Oceanographic Data Centre''' ('''BODC''') is a national facility for looking after and distributing [[data]] about the [[marine (ocean)|marine]] environment. BODC is the designated marine science data centre for the [[UK]] and part of the [[Natural Environment Research Council]] (NERC). The centre provides a resource for science, education and industry, as well as the general public. BODC is hosted by the [[National Oceanography Centre]] (NOC) — primarily at its facility in [[Liverpool]], with small number of its staff in [[Southampton]].

[[File:Bidston Observatory.jpg|250px|right|thumb|Bidston Observatory, home of BODC from 1975 to 2004.]]
[[File:Joseph Proudman Building.jpg|250px|right|thumb|Joseph Proudman Building, Liverpool.]]

== History ==
The origins of BODC go back to 1969 when NERC created the '''British Oceanographic Data Service''' ('''BODS'''). Located at the National Institute of [[Oceanography]], [[Wormley, Surrey|Wormley]] in [[Surrey]], its purpose was to: 
* Act as the UK's National Oceanographic Data Centre
* Participate in the international exchange of data as part of the [[Intergovernmental Oceanographic Commission]] (IOC) network of national data centres

In 1975 BODS was transferred to [[Bidston]] Observatory on the [[Wirral Peninsula|Wirral]], near Liverpool, as part of the newly formed Institute of Oceanographic Sciences. The following year BODS became the Marine Information and Advisory Service (MIAS)[http://www.soton.ac.uk/library/about/nol/mias.html]. Its primary activity was to manage the data collected from weather ships, [[oil rigs]] and [[Data Buoys|data buoys]].
The data banking component of MIAS was restructured to form BODC in April 1989. Its mission was to 'operate as a world-class data centre in support of UK marine science'. BODC pioneered a start to finish approach to marine data management. This involved:
* Assisting in the collection of data at sea
* Quality control of data
* Assembling the data for use by the scientists
* The publication of data sets on [[CD-ROM]]
In December 2004, BODC moved to the purpose-built [[Joseph Proudman]] Building on the campus of the [[University of Liverpool]]. A small number of its staff are based in the [[National Oceanography Centre]] (NOC), Southampton.

== Aims ==
* Work alongside scientists during marine research projects
* Provide quality control and archiving of oceanographic data
* Maintain an online source of information and improve public access to marine data
* Provide innovative marine data products

== National role ==
[[File:Current meter inventory.jpg|250px|right|thumb|BODC [[current meter]] data holdings from around the UK.]]
BODC is one of six designated data centres that manage NERC's environmental data and has a number of national roles and responsibilities:
* Performing data management for NERC-funded marine projects
* Maintaining and developing its archive of marine data, the '''National Oceanographic Database''' ('''NODB''')
* Managing, checking and archiving data from [[tide gauge]]s around the UK coast for the [[UK National Tide Gauge Network|National Tide Gauge Network]], which aims to obtain high quality [[tidal]] information and to provide warning of possible flooding of coastal areas around the British Isles. This  is part of the [[National Tidal and Sea Level Facility|National Tidal &amp; Sea Level Facility]] (NTSLF)
* Hosting the Marine Environmental Data and Information Network ([http://www.oceannet.org/ MEDIN])
* Working in partnership with other NERC marine research centres:
** [[British Antarctic Survey]] (BAS)
** [[National Oceanography Centre]] (NOC), Liverpool, formerly [[Proudman Oceanographic Laboratory]] (POL)
** [[National Oceanography Centre]] (NOC), Southampton
** [[Plymouth Marine Laboratory]] (PML)
** [[Scottish Association for Marine Science]] (SAMS)
** [[Sea Mammal Research Unit]] (SMRU)

== International role ==
BODC's international roles and responsibilities include:
* Contributing to the [[International Council for the Exploration of the Sea]] (ICES) Marine Data Management
* Creating, maintaining and publishing the [[General Bathymetric Chart of the Oceans]] (GEBCO) Digital Atlas
* BODC is one of over 60 national oceanographic data centres that form part of the IOC [[International Oceanographic Data and Information Exchange]] (IODE)

==Projects and initiatives==
The following are a selection of the projects that BODC is or has been involved with:
:[[Image:RAPID mooring.JPG|250px|right|thumb|Servicing of a RAPID mooring.]]
* '''Atlantic Meridional Transect (AMT)'''
:The AMT programme [http://www.bodc.ac.uk/projects/uk/amt/] undertook a twice yearly [[transect]] between the UK and the [[Falkland Islands]] to study the factors determining the [[ecological]] and [[biogeochemical]] variability in the [[planktonic]] [[ecosystems]].
* '''Autosub Under Ice (AUI)'''
:The AUI programme [http://www.bodc.ac.uk/projects/uk/aui/] investigated the role of sub-ice shelf processes in the [[climate]] system. The marine environment beneath floating [[ice shelves]] was explored using Autosub, an [[Autonomous_underwater_vehicle|AUV]].
* '''Marine Productivity (MarProd)'''
:MarProd [http://www.bodc.ac.uk/projects/uk/marprod/] helped to develop coupled [[Computer simulation|models]] and observation systems for the [[pelagic]] ecosystem, with emphasis on the physical factors affecting [[zooplankton]] dynamics.
* '''Rapid Climate Change (RAPID)'''
:The RAPID programme [http://www.bodc.ac.uk/projects/uk/rapid/] aimed to improve understanding of the causes of sudden changes in the Earth's climate.
* '''Ocean Margin Exchange (OMEX)'''
:The OMEX project [http://www.bodc.ac.uk/projects/european/omex/] studied, measured and modelled the physical, chemical and biological processes and fluxes at the ocean margin - the interface between the open [[Atlantic ocean]] and the European [[continental shelf]].
* '''SeaDataNet'''
:[[SeaDataNet]] [http://www.bodc.ac.uk/projects/european/seadatanet/] aims to develop a [[standardised]], distributed system providing transparent access to marine data sets and data products from countries in and around [[Europe]].
*'''System of Industry Metocean data for the Offshore and Research Communities (SIMORC)'''
:SIMORC [http://www.bodc.ac.uk/projects/european/simorc/] aimed to create a central index and database of [[metocean]] data sets collected globally by the oil and gas industry.
*'''Vocabulary Server'''
:BODC operates the NERC Vocabulary Server Web Service [http://www.bodc.ac.uk/products/web_services/vocab/], which provides access to [[Controlled_vocabulary|controlled vocabularies]] of relevance to the scientific community.

== External links ==
* [http://www.bodc.ac.uk BODC homepage]
* [http://www.bodc.ac.uk/about/news_and_events/ BODC News and events]
* [http://www.nerc.ac.uk Natural Environment Research Council (NERC) homepage]
* [http://www.nerc.ac.uk/research/sites/data/ NERC Data centres]
* [http://ndg.nerc.ac.uk/discovery NERC Data Discovery Service]
* [http://www.ntslf.org/ National Tidal and Sea Level Facility (NTSLF)]

{{coord|53|24|27.5|N|2|58|8.2|W|type:landmark|display=title}}

[[Category:Oceanographic organizations]]
[[Category:Scientific organisations based in the United Kingdom]]
[[Category:Data management]]
[[Category:Oceanography]]
[[Category:Marine biology]]
[[Category:Marine geology]]
[[Category:Environmental science]]
[[Category:Environment of the United Kingdom]]
[[Category:Public bodies and task forces of the United Kingdom government]]
[[Category:1969 establishments in the United Kingdom]]
[[Category:Scientific organizations established in 1969]]
[[Category:Organisations based in Liverpool]]</text>
      <sha1>ckekksjb207xl82b60r8buh9db5jv83</sha1>
    </revision>
  </page>
  <page>
    <title>SIGMOD Edgar F. Codd Innovations Award</title>
    <ns>0</ns>
    <id>18927887</id>
    <revision>
      <id>723952493</id>
      <parentid>718924736</parentid>
      <timestamp>2016-06-06T07:32:52Z</timestamp>
      <contributor>
        <ip>84.171.123.58</ip>
      </contributor>
      <comment>Added 2016 award details</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2151" xml:space="preserve">The [[Association for Computing Machinery|ACM]] '''[[SIGMOD]] [[Edgar F. Codd]] Innovations Award''' is a lifetime research achievement award given by the ACM Special Interest Group on Management of Data, at its yearly flagship conference (also called SIGMOD). According to its homepage, it is given "for innovative and highly significant contributions of enduring value to the development, understanding, or use of database systems and databases".&lt;ref&gt;http://www.sigmod.org/sigmod-awards&lt;/ref&gt; The award has been given since 1992.

== Recipients ==
{| class="wikitable"
! Year
! Name
|-
|2016
|[[Gerhard Weikum]]
|-
| 2015
| [[Laura M. Haas]]&lt;ref&gt;[http://www.sigmod.org/all-news/dr.-laura-haas-is-the-recipient-of-the-2015-sigmod-edgar-f.-codd-innovation-award Dr. Laura Haas is the recipient of the 2015 SIGMOD Edgar F. Codd Innovation Award], [[SIGMOD]], retrieved 2015-06-21.&lt;/ref&gt;
|-
| 2014
| [[Martin L. Kersten]]&lt;ref&gt;{{cite web |url=https://www.cwi.nl/news/2014/international-innovation-award-big-data-research-martin-kersten |title=International innovation award to Martin Kersten |date=26 June 2014 |website=CWI Amsterdam |accessdate=26 June 2014}}&lt;/ref&gt;
|-
| 2013
| [[Stefano Ceri]]
|-
| 2012
| Bruce Lindsay
|-
| 2011
| [[Surajit Chaudhuri]]
|-
| 2010
| [http://www.hpl.hp.com/people/umesh_dayal/ Umeshwar Dayal]
|-
| 2009
| [[Masaru Kitsuregawa]]
|-
| 2008
| [[Moshe Y. Vardi]]
|-
| 2007
| [[Jennifer Widom]]
|-
| 2006
| [[Jeffrey D. Ullman]]
|-
| 2005
| Michael Carey
|-
| 2004
| [[Ronald Fagin]]
|-
| 2003
| [[Don Chamberlin]]
|-
| 2002
| [[Patricia Selinger]]
|-
| 2001
| [[Rudolf Bayer]]
|-
| 2000
| [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]]
|-
| 1999
| [[Hector Garcia-Molina]]
|-
| 1998
| [[Serge Abiteboul]]
|-
| 1997
| [[David Maier]]
|-
| 1996
| [[C. Mohan]]
|-
| 1995
| [[David DeWitt]]
|-
| 1994
| [[Phil Bernstein|Philip Bernstein]]
|-
| 1993
| [[Jim Gray (computer scientist)|Jim Gray]]
|-
| 1992
| [[Michael Stonebraker]]
|}

== References ==
&lt;references/&gt;

[[Category:Association for Computing Machinery]]
[[Category:Awards established in 1992]]
[[Category:Computer science awards]]
[[Category:Data management]]</text>
      <sha1>gyhxxudu8tb7v1jfnr60itvanlff1x3</sha1>
    </revision>
  </page>
  <page>
    <title>UI data binding</title>
    <ns>0</ns>
    <id>18644154</id>
    <revision>
      <id>753190483</id>
      <parentid>753187205</parentid>
      <timestamp>2016-12-05T18:45:26Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Cn}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2309" xml:space="preserve">{{Refimprove|date=February 2015}}

'''UI data binding''' is a [[Design pattern (computer science)|software design pattern]] to simplify development of [[GUI]] applications.  UI [[data binding]] binds UI elements to an application [[domain model]]. Most frameworks employ the [[Observer pattern]] as the underlying binding mechanism.  To work efficiently, UI data binding has to address [[Data validation|input validation]] and data type mapping.

A ''bound control'' is a [[GUI widget|widget]] whose value is tied or [[data binding|bound]] to a field in a [[recordset]] (e.g., a [[column (database)|column]] in a [[row (database)|row]] of a [[table (database)|table]]).  Changes made to data within the control are automatically saved to the database when the control's exit [[event trigger]]s.

== Data binding frameworks and tools ==

=== [[Embarcadero Delphi|Delphi]] ===
* [[DSharp (data binding)|DSharp]] 3rd party Data Binding tool{{cn|date=December 2016}}
* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd party Visual Data Binding tool

=== Java ===
* [[JFace]] Data Binding
* [[JavaFX]] Property&lt;ref&gt;https://docs.oracle.com/javafx/2/binding/jfxpub-binding.htm&lt;/ref&gt;

=== .NET ===
* [[Windows Forms]] data binding overview
* [[Windows Presentation Foundation|WPF]] data binding overview
* Unity 3D data binding framework (available in modifications for NGUI, iGUI and EZGUI libraries){{cn|date=December 2016}}

=== JavaScript ===
* [[AngularJS]]
* [[Backbone.js]]
* [[Ember.js]]
* Datum.js&lt;ref&gt;{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}&lt;/ref&gt;
* [[knockout.js]]
* [[Meteor (web framework)|Meteor]], via its ''Blaze'' live update engine&lt;ref&gt;{{cite web|title=Meteor Blaze|url=https://www.meteor.com/blaze|quote=Meteor Blaze is a powerful library for creating live-updating user interfaces. Blaze fulfills the same purpose as Angular, Backbone, Ember, React, Polymer, or Knockout, but is much easier to use. We built it because we thought that other libraries made user interface programming unnecessarily difficult and confusing.}}&lt;/ref&gt;
* [[OpenUI5]]
* [[React (JavaScript library)|React]]

==See also==
*[[Data binding]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Software design patterns]]


{{compu-prog-stub}}
{{database-stub}}</text>
      <sha1>1iptk4z55om8ou4ij93r3p24awfm7og</sha1>
    </revision>
  </page>
  <page>
    <title>Write–read conflict</title>
    <ns>0</ns>
    <id>217741</id>
    <revision>
      <id>731667482</id>
      <parentid>543696786</parentid>
      <timestamp>2016-07-26T19:37:21Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>cap, punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1238" xml:space="preserve">In [[computer science]], in the field of [[database]]s, '''write–read conflict''', also known as '''reading uncommitted data''', is a computational anomaly associated with interleaved execution of transactions.

Given a schedule S

:&lt;math&gt;S = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;  \\
W(A) &amp; \\
 &amp; R(A) \\
 &amp; W(A)\\
 &amp; R(B) \\
 &amp; W(B) \\
 &amp; Com. \\
R(B) &amp; \\
W(B) &amp; \\
Com. &amp; \end{bmatrix}&lt;/math&gt;

T2 could read a database object A, modified by T1 which hasn't committed. This is a '''''dirty read'''''.

T1 may write some value into A which makes the database inconsistent.  It is possible that interleaved execution can expose this inconsistency and lead to inconsistent final database state, violating [[ACID]] rules.

[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T2 out from performing a Read/Write on A.  Note however that [[Strict two-phase locking|Strict 2PL]] can have a number of drawbacks, such as the possibility of [[deadlock]]s.

== See also ==

* [[Concurrency control]]
* [[Read–write conflict]]
* [[Write–write conflict]]

==References==
{{reflist}}
{{Unreferenced|date=August 2009}}

{{DEFAULTSORT:Write-Read Conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>eogg863188kbw0nrr910ydw1nnu6fz2</sha1>
    </revision>
  </page>
  <page>
    <title>Consumer relationship system</title>
    <ns>0</ns>
    <id>21395468</id>
    <revision>
      <id>740274277</id>
      <parentid>740274095</parentid>
      <timestamp>2016-09-20T02:36:19Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* See also */ add link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2013" xml:space="preserve">'''Consumer relationship systems''' ('''CRS''') are specialized [[customer relationship management]] (CRM) [[software]] applications that are used to handle a company's dealings with its customers.&lt;ref name ="Insight44-50"&gt;[http://www.nxtbook.com/nxtbooks/cmp/cmi_200709/index.php ICMI Customer Management Insight Magazine, September 2007, pp 44–50], Retrieved 11 January 2012&lt;/ref&gt;

Current consumer relationship systems integrate the [[software]] with telephone and call recording systems as well as with corporate systems for input and reporting. Customers can provide input from the company's website directly into the CRS. These systems are popular because they can deliver the 'voice of the consumer' that contributes to product quality improvement and that ultimately increases corporate profits.&lt;ref name ="Insight44-50" /&gt;

Consumer relationship systems that provide automated support as well as advanced systems may have [[artificial intelligence]] (AI) interfaces that can extract and analyse [[data]] collected, or handle basic questions and complaints.&lt;ref&gt;{{cite web|last1=Smith| first1=S.E.|title= What is Consumer Relationship System? |date= |publisher= WiseGeek.net|url= http://www.wisegeek.net/what-is-consumer-relationship-system.htm|accessdate=1 February 2013}}&lt;/ref&gt;

==History==
The first CRS was developed in the 1980s. In 1981 Michael Wilke and Robert Thornton founded Wilke/Thornton, Inc in [[Columbus, Ohio]], to develop new CRS software.&lt;ref&gt;[http://www.wilke-thornton.com/WTI/Pages/products.html Wilke/Thornton, Inc Products] Retrieved 11 January 2012&lt;/ref&gt;

==See also==
* [[ECRM]]
* [[Business intelligence]]
* [[Customer experience]]
* [[Customer intelligence]]
* [[Customer service]] – contains ISO standards
* [[Customer value maximization]]
* [[Enterprise relationship management]] (ERM)
* [[Sales force management system]]
* [[Sales intelligence]]
* [[Sales process engineering]]

== References ==
{{reflist}}

[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>hvxaglisjcm466weacsx1d6csoatnpl</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic warehousing</title>
    <ns>0</ns>
    <id>20420236</id>
    <revision>
      <id>730312823</id>
      <parentid>679779238</parentid>
      <timestamp>2016-07-18T06:21:02Z</timestamp>
      <contributor>
        <username>DocWatson42</username>
        <id>38455</id>
      </contributor>
      <minor />
      <comment>Made two minor corrections.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6202" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=November 2008}}
{{confusing|date=August 2009}}
{{cleanup-rewrite|date=August 2009}}
}}

In [[data management]],  '''semantic warehousing''' is a methodology of digitalized text data using similar functions to [[Data warehousing]] (DW), such as ETL([[Extract, transform, load]]), ODS([[Operational data store]]), and MODEL. [[Value (computer science)|Key value]] operation is less useful for the digitalized text. Semantic warehousing is different from DW in that semantic information base from text(semantic) data.

Semantic warehousing is different from search engine in that semantic information base from text data is stored in the database.([[DBMS]])

Though data is most important word in computing era, it can not explain human knowledge well yet.
Data(numeric data) is key element of computing systems for certain organization (especially companies, enterprises), but no performance oriented organization needs something to gather and use knowledge or human feeling.
Semantic warehousing will be equally or more important than data warehousing in the future.

==Definition==
Semantic warehousing is a conceptual and functional term meaning to gather from a source, semantically defining and providing information from digitalized text type data.

==Background==

Data warehousing (DW) is popular these days. Gathering data from systems that generate transactions, data warehouses become a base of [[information]]. Key of data warehouse is a model (called [[datamart]]) and that model is made up of dimensions(key) and measures(value). Users get information from the models by doing certain operations. [[Online analytical processing]] (OLAP) is most the important operation for the users to get information from the DW models. Handling dimensions with pivoting, drilling, slice &amp; dice operations users get numeric values like sales amounts, growth rates, etc.
Various areas of this world defined and appeared on the World Wide Web(Internet), eager to present their contents in a semantic way. 
Briefly speaking semantic warehousing has datawarehousing boby and search head and ontology features.

Data warehousing contributed to companies' business values and lots of solutions and tools are commercially successful. Analysis of internal data delivers a certain level of business values, on the contrary to this Semantic warehousing environment has not yet matured. Capacity of social data is increasing rapidly and various efforts of finding value from that data are made widely known as Big data, etc. Semantic warehousing can be the mainstream of treat data and intelligence of social world in the future though it is defined with other keywords.

At the Big data era, semantic processing is going to become major IT process. Semantic warehousing is digital infra of Intelligence.

== Practices ==

'''▣ Medical area (Clinical Information)'''

Some hospital implement semantic warehousing for [[clinic]]al information (SWCI). Medical information is now knowledge network level. [[UMLS]] define semantic knowledge network of medical language. Currently medical information stored in database and not fully used for clinic. Semantic warehousing is next stage of digitalized medical information.

SWCI is a name of conceptual system of clinical information.&lt;br /&gt;
Named by Juhan Kim (SNUH, [[Seoul National University Hospital]]) and Bohyon Hwang, YongChan Keum in 2008.

Defined architecture on SWCI ;&lt;br /&gt;
1. Semantic-oriented cleansing&lt;br /&gt;
2. Semantic-oriented meta management&lt;br /&gt;
3. Clinical(Medical) knowledge basement&lt;br /&gt;
4. Semantic-oriented user intelligence

'''▣ Intelligence Area'''

At the point of Big data usage, intelligence reporting can be valuable results.

1. Source information&lt;br /&gt;
2. Manage intelligence &amp; Semantic data&lt;br /&gt;
3. Intelligence service &amp; use

http://www.globalintelligence.kr/gibigdata/

== Connected area ==

- [[Big data]] &lt;br /&gt;
- [[Semantic web]] &lt;br /&gt;
- [[Ontology]] &lt;br /&gt;
- [[Knowledge]] &lt;br /&gt;
- [[Medical]] and [[healthcare]] : EMR [[Electronic medical record|(Electronic Medical Record)]], EHR [[Electronic health record|(Electronic Health Record)]]&lt;br /&gt;
- [[Data warehouse]] &lt;br /&gt;
- AI ([[artificial intelligence]])

== References ==
*[http://www.snubi.org/ '''BI''' Laboratory of Seoul National University Hospital]
*Smith, Barry Kumar, Anand and Schulze-Kremer, Steffen (2004) [http://ontology.buffalo.edu/medo/UMLS_SN.pdf Revising the UMLS Semantic Network], in M. Fieschi, et al. (eds.), Medinfo 2004, Amsterdam: IOS Press, 1700.
* Foundations of Data Warehouse Quality :
  Data Quality article mentioning that semantically rich DW.
  http://www.cs.brown.edu/courses/cs227/Papers/Projects/iq97_dwq.pdf
* An Integrative and Uniform Model for Metadata Management in Data Warehousing Environment.
  Semantic metadata and technical metadata.
  http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-19/paper12.pdf

* Effective Query Expansion using Condensed UMLS Metathesaurus for Medical Information Retrieval
http://www.e-hir.org/journal/view.html?uid=201&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

* A Study of Effective Unified Medical Language System Concept Indexing in Radiology Reports
http://www.e-hir.org/journal/view.html?uid=226&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

* Developing a Reference Terminology Model for Health Care Using an Object-Oriented Approach
http://www.e-hir.org/journal/view.html?uid=311&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

* UMLS(Unified Medical Language System)의 증상용어와 국내의무기록에서 사용되는 증상용어와의 비교연구
http://www.e-hir.org/journal/view.html?uid=922&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

[[Category:Data management]]</text>
      <sha1>4r2u156v40k16ohs9jx8aipmkx1yu2n</sha1>
    </revision>
  </page>
  <page>
    <title>Hybrid array</title>
    <ns>0</ns>
    <id>24278544</id>
    <revision>
      <id>746062178</id>
      <parentid>743727892</parentid>
      <timestamp>2016-10-25T01:51:02Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6956" xml:space="preserve">A '''hybrid array''' is a form of [[hierarchical storage management]] that combines [[hard disk drive]]s (HDDs) with [[solid-state drive]]s (SSDs) for [[I/O]] speed improvements.

Hybrid storage arrays aim to mitigate the ever increasing price-performance gap between HDDs and [[DRAM]] by adding a non-volatile flash level to the [[memory hierarchy]].&lt;ref name="MicheloniMarelli2012"&gt;{{cite book|author1=Rino Micheloni|author2=Alessia Marelli|author3=Kam Eshghi|title=Inside Solid State Drives (SSDs)|url=https://books.google.com/books?id=S8xRtkF7hUkC&amp;pg=PA62|year=2012|publisher=Springer|isbn=978-94-007-5145-3|page=62}}&lt;/ref&gt; Hybrid arrays thus aim to lower the cost per I/O, compared to using only SSDs for storage.  Hybrid architectures can be as simple as involving a single SSD [[Cache (computing)|cache]] for desktop or laptop computers, or can be more complex as configurations for [[data center]]s and [[cloud computing]].

== Implementations ==
&lt;!-- please only add products covered in [[WP:SECONDARY]] sources at adequate depth --&gt;
Some commercial products for building hybrid arrays include:
* [[Adaptec]] demonstrated the MaxIQ series in 2009.&lt;ref&gt;{{cite web |author=Charlie Demerjian |url= http://semiaccurate.com/2009/09/09/adaptecs-maxiq-caches-raids-ssds/ |title= Adaptec's MaxIQ caches RAIDs with SSDs |publisher= SemiAccurate |date= September 9, 2009 |accessdate= October 10, 2016 }}&lt;/ref&gt;
* Apple's [[Fusion Drive]]
*  [[Linux]] software includes [[bcache]], [[dm-cache]], and [[Flashcache]] (and its fork EnhanceIO).
* Condusive's [[ExpressCache]] is marketed for laptops.
* [[EMC Corporation]] VFcache was announced in 2012.&lt;ref&gt;{{cite web |last= Larry Dignan |url= http://www.zdnet.com/blog/btl/emc-unveils-vfcache-targets-fusion-io/68657 |title=EMC unveils VFCache, targets Fusion-io |publisher= ZDNet |work= Between the Lines |date= February 5, 2012 |accessdate= October 10, 2016 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= One day later: EMC declares war on all-flash array, server flash card rivals: Rolls out XtremIO array, renamed VFCache |date= March 5, 2013 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ |accessdate= October 10, 2016 }}&lt;/ref&gt;
* [[Fusion-io]] acquired ioTurbine in 2011,&lt;ref name="io"&gt;{{cite web |url= http://www.theregister.co.uk/2013/06/25/fusionio_spins_ioturbine_faster/ |title=Fusion-io spins up ioTurbine, enhances server flash caching |work= The Register |accessdate= October 10, 2016 }}&lt;/ref&gt; and the product line it acquired by buying NexGen in 2013.&lt;ref&gt;{{cite web|url=http://www.theregister.co.uk/2013/04/24/fusion_io_nexgen/|title=Fusion-io buys NexGen|work=theregister.co.uk |accessdate=2015-03-26}}&lt;/ref&gt;
* [[Hitachi]] Accelerated Flash Storage (HAFS) used together with the Hitachi Dynamic Tiering software&lt;ref&gt;{{citation |url=http://www.computerweekly.com/feature/Big-storage-turns-the-tide-in-the-hybrid-flash-array-market |title=Big storage turns the tide in the hybrid flash array market |work=[[Computer Weekly]] |date=September 2013 |accessdate=2015-03-26}}&lt;/ref&gt;
* [[IBM]] Flash Cache Storage Accelerator (FCSA) server software&lt;ref&gt;{{cite web|author=The SSD Guy |url=http://thessdguy.com/ibm-adds-server-side-caching/ |title=IBM Adds Server-Side Caching |publisher=The SSD Guy |date=2013-08-20 |accessdate=2013-12-23}}&lt;/ref&gt;
* Intel's [[Smart Response Technology]] for desktop
* Intel's [[Cache Acceleration Software]] for servers and workstations 
* [[LSI Corporation|LSI]] CacheCade software for their controllers&lt;ref&gt;{{cite web|url=http://www.storagereview.com/lsi_megaraid_cachecade_pro_20_review|title=LSI MegaRAID CacheCade Pro 2.0 Review |accessdate=2015-03-26 |work=storagereview.com}}&lt;/ref&gt;
* [[Marvell Technology Group|Marvell]]'s HyperDuo controllers&lt;ref&gt;{{cite web|url=http://www.cnet.com/8301-32254_1-20027657-283.html|title=Hands-on with the Marvell HyperDuo hybrid storage controller |accessdate=2015-03-26 |publisher=CBS Interactive|work=CNET}}&lt;/ref&gt;
* Microsoft's [[Automated Tiering]] (since Windows 2012 R2)
* [[NetApp]]'s Flash Cache, Flash Pool, Flash Accel&lt;ref&gt;{{cite web|url=http://www.theregister.co.uk/2012/08/21/netapp_server_flash/|title=NetApp: Flash as a STORAGE tier? You must be joking |accessdate=2015-03-26 |work=theregister.co.uk}}&lt;/ref&gt;
* [[Oracle Corporation]] markets products such as [[Exadata]] Smart Cache Flash, and the FS1 flash storage system.&lt;ref&gt;{{Cite news |title= Oracle crashes all-flash bash: Behold, our hybrid FS1 arrays: Mutant flash/disk box a pillar of storage: It's axiomatic |date= September 30, 2014 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2014/09/30/the_fs1_pillar_of_oracle_arrays_storage/ |accessdate= October 10, 2016 }}&lt;/ref&gt;
* Microsoft [[ReadyBoost]] allows personal computers to [[USB flash drive]]s as cache.
* Nvelo DataPlex SSD caching software was announced in 2011,&lt;ref&gt;{{cite web |url= http://www.thessdreview.com/our-reviews/nvelo-dataplex-ssd-caching-software-review-seven-msata-ssds-prove-an-amazing-concept/  |title= NVELO Dataplex SSD Caching Software Review - Seven mSATA SSDs Prove An Amazing Concept |work= The SSD Review |date= December 4, 2011 |author= Les Tokar |accessdate= October 10, 2016 }}&lt;/ref&gt; and was acquired by [[Samsung]] in 2012.&lt;ref&gt;{{cite web |url= http://www.anandtech.com/show/6518/samsung-acquires-ssd-caching-company-nvelo |title=Samsung Acquires SSD Caching Company NVELO |publisher=AnandTech |author= Kristian Vättö |date= December 16, 2012 |accessdate= October 10, 2016 }}&lt;/ref&gt;
* [[SanDisk]] FlashSoft for Windows, Linux, and [[vSphere]]&lt;ref name="io"/&gt;
* Products are offered by vendors like AMI [[StorTrends]],&lt;ref&gt;{{cite web|author=Ian Barker |url=http://betanews.com/2014/01/27/ami-stortrends-3500i-offers-high-performance-storage-for-smaller-enterprises/ |title=AMI StorTrends 3500i offers high performance storage for smaller enterprises |publisher=BetaNews |date=2014-01-27 |accessdate=2014-10-17}}&lt;/ref&gt; [[Tegile Systems]], [[Reduxio]], and [[Tintri]].&lt;ref&gt;{{cite web|url=http://www.theregister.co.uk/2013/04/09/blind_spot/ |accessdate=2015-03-26 |title=Mutant array upstarts feast on EMC, NetApp's leavings|work= The Register }}&lt;/ref&gt;
* [[ZFS]] using hybrid storage pools, are used for example in some Oracle Corporation products.&lt;ref&gt;{{cite web|url=http://www.enterprisestorageforum.com/san-nas-storage/oracles-flash-friendly-sun-zfs-storage-is-ready-for-new-sparcs.html|title=Oracle's Flash-Friendly Sun ZFS Storage Is Ready for New SPARCs|date=3 April 2013 |accessdate=2015-03-26 |work=enterprisestorageforum.com}}&lt;/ref&gt;

== See also ==
* [[Hybrid drive]]{{spaced ndash}} built-in flash cache, handled by firmware
* [[Automated tiered storage]]{{spaced ndash}} another name for hierarchical storage management
* The "[[five-minute rule]]" for caching

== References ==
{{Reflist|30em}}

[[Category:Data management]]
[[Category:Solid-state caching]]
[[Category:Memory management software]]</text>
      <sha1>3wme3jsmip0axzrum680jbqneer1l5r</sha1>
    </revision>
  </page>
  <page>
    <title>Modular concurrency control</title>
    <ns>0</ns>
    <id>24906259</id>
    <redirect title="Global concurrency control" />
    <revision>
      <id>323027606</id>
      <parentid>323024786</parentid>
      <timestamp>2009-10-31T01:36:44Z</timestamp>
      <contributor>
        <username>Comps</username>
        <id>1071011</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="191" xml:space="preserve">#REDIRECT [[Global concurrency control]]

Modular concurrency control

[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]</text>
      <sha1>n91o2igtd1n2mt1vgecy84cffvdoy24</sha1>
    </revision>
  </page>
  <page>
    <title>Operational system</title>
    <ns>0</ns>
    <id>14190268</id>
    <revision>
      <id>700186629</id>
      <parentid>621313214</parentid>
      <timestamp>2016-01-16T23:31:02Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Synonyms */ caps</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1369" xml:space="preserve">{{distinguish|Operating system}}
An '''operational system''' is a term used in [[data warehousing]] to refer to a system that is used to process the day-to-day transactions of an organization. These systems are designed in a manner that processing of day-to-day transactions is performed efficiently and the integrity of the transactional data is preserved.
== Synonyms ==
Sometimes operational systems are referred to as [[operational database]]s, [[transaction processing system]]s, or [[online transaction processing]] systems (OLTP). However, the use of the last two terms as synonyms may be confusing, because operational systems can be [[batch processing]] systems as well.

Any enterprise must necessarily maintain a lot of data about its operation. This is its "operational data".

{| class="wikitable" border="1"
|-
! Organization
! Probably
|-
| Manufacturing Company 
| Product data
|-
| Bank
| Account Data
|-
| Hospital
| Patient Data
|-
| University
| Student Data
|-
| Government Department
| Planning data
|}

==See also==
* [[Operating system]] (OS)
* [[Data warehouse#Data warehouses versus operational systems|Data warehouses versus operational systems]]

{{database-stub}}

{{DEFAULTSORT:Operational System}}
[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Business intelligence]]</text>
      <sha1>dq3z1auop60p3e1q76xio2bddfsfczz</sha1>
    </revision>
  </page>
  <page>
    <title>Commit (data management)</title>
    <ns>0</ns>
    <id>1626958</id>
    <revision>
      <id>717532767</id>
      <parentid>665913704</parentid>
      <timestamp>2016-04-28T08:07:46Z</timestamp>
      <contributor>
        <username>Intgr</username>
        <id>246230</id>
      </contributor>
      <minor />
      <comment>/* See also */ Simplify link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1422" xml:space="preserve">{{For|the revision control concept|Commit (revision control)}}
{{Unreferenced|date=May 2014}}

In [[computer science]] and [[data management]], a '''commit''' is the making of a set of tentative changes permanent. A popular usage is at the end of a [[database transaction|transaction]]. A ''commit'' is an act of committing.

==Data management==
A &lt;code&gt;[[COMMIT (SQL)|COMMIT]]&lt;/code&gt; statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a &lt;code&gt;[[Begin work (SQL)|BEGIN WORK]]&lt;/code&gt; statement, one or more SQL statements, and then the &lt;code&gt;COMMIT&lt;/code&gt; statement. Alternatively, a &lt;code&gt;[[Rollback (data management)|ROLLBACK]]&lt;/code&gt; statement can be issued, which undoes all the work performed since &lt;code&gt;BEGIN WORK&lt;/code&gt; was issued. A &lt;code&gt;COMMIT&lt;/code&gt; statement will also release any existing [[savepoint]]s that may be in use.

In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].

==See also==
* [[Commit (version control)]]
* [[Atomic commit]]
* [[Two-phase commit protocol]]
* [[Three-phase commit protocol]]

{{databases}}

{{DEFAULTSORT:Commit (Data Management)}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Transaction processing]]

{{comp-sci-stub}}</text>
      <sha1>3equq4wiy404bzljriv29vxvska1yvw</sha1>
    </revision>
  </page>
  <page>
    <title>Database transaction</title>
    <ns>0</ns>
    <id>233953</id>
    <revision>
      <id>735112713</id>
      <parentid>735112652</parentid>
      <timestamp>2016-08-18T17:51:28Z</timestamp>
      <contributor>
        <username>ThePlatypusofDoom</username>
        <id>28032930</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/97.78.143.201|97.78.143.201]] ([[User talk:97.78.143.201|talk]]) to last revision by HakanIST. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9458" xml:space="preserve">{{Refimprove|date=August 2010}}

A '''transaction''' symbolizes a unit of work performed within a [[database management system]] (or similar system) against a database, and treated in a coherent and reliable way independent of other transactions. A transaction generally represents any change in database. Transactions in a database environment have two main purposes:

# To provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failure, when execution stops (completely or partially) and many operations upon a database remain uncompleted, with unclear status.
# To provide isolation between programs accessing a database concurrently. If this isolation is not provided, the programs' outcomes are possibly erroneous.

A database transaction, by definition, must be [[Atomicity (database systems)|atomic]], [[Consistency (database systems)|consistent]], [[Isolation (database systems)|isolated]] and [[Durability (database systems)|durable]].&lt;ref&gt;[http://msdn.microsoft.com/en-us/library/aa366402(VS.85).aspx A transaction is a group of operations that are atomic, consistent, isolated, and durable (ACID).]&lt;/ref&gt; Database practitioners often refer to these properties of database transactions using the acronym [[ACID]].

Transactions provide an "all-or-nothing" proposition, stating that each work-unit performed in a database must either complete in its entirety or have no effect whatsoever. Further, the system must isolate each transaction from other transactions, results must conform to existing constraints in the database, and transactions that complete successfully must get written to durable storage.

==Purpose==
[[Database]]s and other data stores which treat the [[data integrity|integrity]] of data as paramount often include the ability to handle transactions to maintain the integrity of data. A single transaction consists of one or more independent units of work, each reading and/or writing information to a database or other data store. When this happens it is often important to ensure that all such processing leaves the database or data store in a consistent state.

Examples from [[Double-entry bookkeeping system|double-entry accounting systems]] often illustrate the concept of transactions. In double-entry accounting every debit requires the recording of an associated credit. If one writes a check for $100 to buy groceries, a transactional double-entry accounting system must record the following two entries to cover the single transaction:

# Debit $100 to Groceries Expense Account
# Credit $100 to Checking Account

A transactional system would make both entries pass or both entries would fail. By treating the recording of multiple entries as an atomic transactional unit of work the system maintains the integrity of the data recorded. In other words, nobody ends up with a situation in which a debit is recorded but no associated credit is recorded, or vice versa.

==Transactional databases==
A '''transactional database''' is a [[DBMS]] where write transactions on the database are able to be rolled back if they are not completed properly (e.g. due to power or connectivity loss).

Most {{As of|2008|alt=modern}}  [[relational database management system]]s fall into the category of databases that support transactions.

In a database system a transaction might consist of one or more data-manipulation statements and queries, each reading and/or writing information in the database. Users of [[database system]]s consider [[Data consistency|consistency]] and [[data integrity|integrity]] of data as highly important. A simple transaction is usually issued to the database system in a language like [[Structured Query Language|SQL]] wrapped in a transaction, using a pattern similar to the following:

# Begin the transaction
# Execute a set of data manipulations and/or queries
# If no errors occur then commit the transaction and end it
# If errors occur then rollback the transaction and end it

If no errors occurred during the execution of the transaction then the system commits the transaction. A transaction commit operation applies all data manipulations within the scope of the transaction and persists the results to the database. If an error occurs during the transaction, or if the user specifies a [[Rollback (data management)|rollback]] operation, the data manipulations within the transaction are not persisted to the database. In no case can a partial transaction be committed to the database since that would leave the database in an inconsistent state.

Internally, multi-user databases store and process transactions, often by using a transaction [[identifier|ID]] or XID.

There are multiple varying ways for transactions to be implemented other than the simple way documented above. [[Nested transaction]]s, for example, are transactions which contain statements within them that start new transactions (i.e. sub-transactions). ''Multi-level transactions'' are a variant of nested transactions where the sub-transactions take place at different levels of a layered system architecture (e.g., with one operation at the database-engine level, one operation at the operating-system level) &lt;ref&gt;Beeri, C., Bernstein, P.A., and Goodman, N. A model for concurrency in nested transactions systems. Journal of the ACM, 36(1):230-269, 1989&lt;/ref&gt; Another type of transaction is the [[compensating transaction]].

===In SQL===
Transactions are available in most SQL database implementations, though with varying levels of robustness. (MySQL, for example, does not support transactions in the [[MyISAM]] storage engine, which was its default storage engine before version 5.5.)

A transaction is typically started using the command &lt;code&gt;BEGIN&lt;/code&gt; (although the SQL standard specifies &lt;code&gt;START TRANSACTION&lt;/code&gt;). When the system processes a &lt;code&gt;[[Commit (SQL)|COMMIT]]&lt;/code&gt; statement, the transaction ends with successful completion.  A &lt;code&gt;[[Rollback (data management)|ROLLBACK]]&lt;/code&gt; statement can also end the transaction, undoing any work performed since &lt;code&gt;BEGIN TRANSACTION&lt;/code&gt;. If [[autocommit]] was disabled using &lt;code&gt;START TRANSACTION&lt;/code&gt;, autocommit will also be re-enabled at the transaction's end.

One can set the [[Isolation (database systems)|isolation level]] for individual transactional operations as well as globally. At the READ COMMITTED level, the result of any work done after a transaction has commenced, but before it has ended, will remain invisible to other database-users until it has ended. At the lowest level (READ UNCOMMITTED), which may occasionally be used to ensure high concurrency, such changes will be visible.

==Object databases==
Relational databases traditionally comprise tables with fixed size fields and thus records. Object databases comprise variable sized [[Binary large object|blobs]] (possibly incorporating a [[mime-type]] or [[Serializable (databases)|serialized]]). The fundamental similarity though is the start and the [[Commit (data management)|commit]] or [[Rollback (data management)|rollback]].

After starting a transaction, database records or objects are locked, either read-only or read-write. Actual reads and writes can then occur. Once the user (and application) is happy, any changes are committed or rolled-back [[Atomicity (database systems)|atomically]], such that at the end of the transaction there is no [[Consistency (database systems)|inconsistency]].

==Distributed transactions==
Database systems implement [[distributed transaction]]s as transactions against multiple applications or hosts. A distributed transaction enforces the ACID properties over multiple systems or data stores, and might include systems such as databases, file systems, messaging systems, and other applications. In a distributed transaction a coordinating service ensures that all parts of the transaction are applied to all relevant systems. As with database and other transactions, if any part of the transaction fails, the entire transaction is rolled back across all affected systems.

==Transactional filesystems==
The [[Namesys]] [[Reiser4]] filesystem for [[Linux]]&lt;ref&gt;[http://namesys.com/v4/v4.html#committing namesys.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; supports transactions, and as of [[Microsoft]] [[Windows Vista]], the Microsoft [[NTFS]] filesystem&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/library/default.asp?url=/library/en-us/fileio/fs/portal.asp|title=MSDN Library|publisher=|accessdate=16 October 2014}} {{dead link|date=May 2014}}&lt;/ref&gt; supports [[distributed transaction]]s across networks.

==See also==
* [[Concurrency control]]

==References==
{{reflist}}

==Further reading==
* &lt;cite id=Bern2009&gt;[[Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4 &lt;/cite&gt;
* Gerhard Weikum, Gottfried Vossen (2001), ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, ISBN 1-55860-508-8

==External links==
* [[c2:TransactionProcessing]]
* https://docs.oracle.com/database/121/CNCPT/transact.htm#CNCPT016
* https://docs.oracle.com/cd/B28359_01/server.111/b28318/transact.htm

{{Databases}}

{{DEFAULTSORT:Database Transaction}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>mcyw5huf8xuouqlezj0t6hcrm0jgx62</sha1>
    </revision>
  </page>
  <page>
    <title>Durability (database systems)</title>
    <ns>0</ns>
    <id>245944</id>
    <revision>
      <id>617369164</id>
      <parentid>596543787</parentid>
      <timestamp>2014-07-17T20:53:29Z</timestamp>
      <contributor>
        <ip>73.183.64.164</ip>
      </contributor>
      <comment>change link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1240" xml:space="preserve">{{Unreferenced|date=December 2009}}
In [[database system]]s, '''durability''' is the [[ACID]] property which guarantees that [[database transaction|transaction]]s that have committed will survive permanently. 
For example, if a flight booking reports that a seat has successfully been booked, then the seat will remain booked even if the system crashes.

Durability can be achieved by flushing the transaction's log records to [[non-volatile storage]] before acknowledging commitment.

In [[distributed transaction]]s, all participating servers must coordinate before commit can be acknowledged. This is usually done by a [[two-phase commit protocol]].

Many DBMSs implement durability by writing transactions into a [[transaction log]] that can be reprocessed to recreate the system state right before any later failure. A transaction is deemed committed only after it is entered in the log.

==See also==
* [[Atomicity (database systems)|Atomicity]]
* [[Consistency (database systems)|Consistency]]
* [[Isolation (database systems)|Isolation]]
* [[Relational database management system]]

{{DEFAULTSORT:Durability (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]


{{Compu-sci-stub}}
{{Database-stub}}</text>
      <sha1>qw9bdp9zmxukd37b2t1h6xv6wfsxogi</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Structured storage</title>
    <ns>14</ns>
    <id>25067249</id>
    <revision>
      <id>548068555</id>
      <parentid>389257360</parentid>
      <timestamp>2013-04-01T00:36:50Z</timestamp>
      <contributor>
        <username>Jerome Charles Potts</username>
        <id>562899</id>
      </contributor>
      <comment>removed [[Category:Databases]] using [[WP:HC|HotCat]] entries in here are not necessarily databases</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="61" xml:space="preserve">{{Cat main|Structured storage}}

[[Category:Data management]]</text>
      <sha1>iihgl772os0xqjp9pex5h3mbfuecwl0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Open data</title>
    <ns>14</ns>
    <id>25128034</id>
    <revision>
      <id>641372190</id>
      <parentid>628514013</parentid>
      <timestamp>2015-01-07T07:49:27Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="99" xml:space="preserve">{{cat main}}
[[Category:Open content]]
[[Category:Free software|Data]]
[[Category:Data management]]</text>
      <sha1>jm9rcjkacqe4shupx9drt15yq9mwsqj</sha1>
    </revision>
  </page>
  <page>
    <title>Archive site</title>
    <ns>0</ns>
    <id>2643012</id>
    <revision>
      <id>732076298</id>
      <parentid>708346259</parentid>
      <timestamp>2016-07-29T12:55:37Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <comment>some [[WP:COPYEDIT|copy-editing]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4032" xml:space="preserve">{{refimprove|date=January 2016}}

In [[web archiving]], an '''archive site''' is a [[website]] that stores information on webpages from the past for anyone to view.

==Common techniques==
Two common techniques for archiving web sites are using a [[web crawler]] or soliciting user submissions:

# '''Using a [[web crawler]]''': By using a web crawler (e.g., the [[Internet Archive]]) the service will not depend on an active community for its content, and thereby can build a larger database faster. However, web crawlers are only able to index and archive information the public has chosen to post to the Internet, or that is available to be crawled, as web site developers and system administrators have the ability to block web crawlers from accessing [certain] web pages (using a [[Robots Exclusion Standard|robots.txt]]).
# '''User submissions''': While it can be difficult to start user submissions services due to potentially low rates of user submission, this system can yield some of the best results. By crawling web pages one is only able to obtain the information the public has chosen to post online; however, potential content providers may not bother to post certain information, assuming no one would be interested in it, because they lack a proper venue in which to post it, or because of copyright concerns.&lt;ref&gt;{{cite news|url=http://dlib.org/dlib/march12/niu/03niu1.html|journal=D-Lib Magazine| date=March–April 2012 |
 volume =18| number =3/4| title=An Overview of Web Archiving |author=Jinfang Niu,  University of South Florida|doi=10.1045/march2012-niu1}}&lt;/ref&gt; However, users who see someone wants their information may be more apt to submit it.

==Examples==

===Google Groups===
On February 12, 2001, [[Google]] acquired the [[usenet]] discussion group archives from [[Deja.com]] and turned it into their [[Google Groups]] service.&lt;ref&gt;{{cite web |title=Google Acquires Usenet Discussion Service and Significant Assets from Deja.com |work= |date=February 12, 2001 |url=https://googlepress.blogspot.com/2001/02/google-acquires-usenet-discussion.html }}&lt;/ref&gt; They allow users to search old discussions with Google's search technology, while still allowing users to post to the [[mailing list]]s.

===Internet Archive===
The [[Internet Archive]] is building a compendium of websites and [[digital media]]. Starting in 1996, the Archive has been employing a web crawler to build up their database. It is one of the best known archive sites.

===NBCUniversal Archives===
[[NBCUniversal Archives]] offer access to exclusive content from [[NBCUniversal]] and its subsidiaries. Their NBCUniversal Archives website provides easy viewing of past and recent news clips, and it is a prime example of a news archive.&lt;ref&gt;[http://www.nbcuniversalarchives.com/nbcuni/home.do NBCUniversal Archives]&lt;/ref&gt;

===Nextpoint===
[[Nextpoint]] offers an automated [[Cloud computing|cloud]]-based, [[Software as a service|SaaS]] for marketing, compliance, and litigation related needs including electronic discovery.

===PANDORA Archive===
PANDORA ([[Pandora Archive]]), founded in 1996 by the National Library of [[Australia]], stands for Preserving and Accessing Networked Documentary Resources of Australia, which encapsulates their mission. They provide a long-term catalog of select online publications and web sites authored by Australians or that are of an Australian topic. They employ their PANDAS (PANDORA Digital Archiving System) when building their catalog.

===textfiles.com===
[[textfiles.com]] is a large library of old text files maintained by [[Jason Scott Sadofsky]]. Its mission is to archive the old documents that had floated around the [[bulletin board systems]] (BBS) of his youth and to document other people's experiences on the bulletin board systems.

==See also==
* [[Internet Archive]]
* [[Pandora Archive]]
* [[WebCite]]
* [[Web archiving]]

== References ==
{{reflist}}

{{DEFAULTSORT:Archive Site}}
[[Category:Data management]]
[[Category:Online archives]]
[[Category:Web archiving initiatives]]</text>
      <sha1>3d1m8pwnum0r4ds48m5p7yom7p3zssz</sha1>
    </revision>
  </page>
  <page>
    <title>Virtual directory</title>
    <ns>0</ns>
    <id>943527</id>
    <revision>
      <id>675909033</id>
      <parentid>616993640</parentid>
      <timestamp>2015-08-13T13:36:42Z</timestamp>
      <contributor>
        <ip>50.205.49.193</ip>
      </contributor>
      <comment>/* Capabilities */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9367" xml:space="preserve">In [[computing]], the term '''virtual directory''' has a couple of meanings. It may simply designate (for example in [[Internet Information Services|IIS]]) a [[Folder (computing)|folder]] which appears in a [[Path (computing)|path]] but which is not actually a subfolder of the preceding folder in the path. However, this article will discuss the term in the context of [[directory service]]s and [[identity management]].

A virtual directory or '''virtual directory server''' in this context is a software layer that delivers a single access point for [[identity management]] applications and service platforms. A virtual directory operates as a high-performance, lightweight abstraction layer that resides between client applications and disparate types of identity-data repositories, such as proprietary and standard directories, databases, web services, and applications.

A virtual directory receives queries and directs them to the appropriate data sources by abstracting and virtualizing data. The virtual directory integrates identity data from multiple heterogeneous data stores and presents it as though it were coming from one source. This ability to reach into disparate repositories makes virtual directory technology ideal for consolidating data stored in a distributed environment. 

{{As of | 2011}}, virtual directory servers most commonly use the [[Lightweight Directory Access Protocol|LDAP]] protocol, but more sophisticated virtual directories can also support [[SQL]] as well as [[Directory Services Markup Language|DSML]] and [[Service Provisioning Markup Language|SPML]].

Industry experts have heralded the importance of the virtual directory in modernizing the identity infrastructure. According to Dave Kearns of Network World, "Virtualization is hot and a virtual directory is the building block, or foundation, you should be looking at for your next identity management project."&lt;ref&gt;{{cite web | url=http://www.networkworld.com/article/2305608/access-control/virtual-directory-finally-gains-recognition.html | title=Virtual directory finally gains recognition | publisher=NetworkWorld | date=7 August 2006 | accessdate=14 July 2014 | author=Kearns, Dave}}&lt;/ref&gt; In addition, Gartner analyst, Bob Blakley&lt;ref&gt;The Emerging Architecture of Identity Management, Bob Blakley, April 16, 2010.&lt;/ref&gt; said that virtual directories are playing an increasingly vital role. In his report, “The Emerging Architecture of Identity Management,” Blakley wrote: “In the first phase, production of identities will be separated from consumption of identities through the introduction of a virtual directory interface.”

==Capabilities==
Virtual directories can have some or all of the following capabilities:&lt;ref&gt;{{cite web|url=http://optimalidm.com/resources/blog/virtual-directory-server-2/|title=An Introduction To Virtual Directories|publisher=Optimal Idm|accessdate=15 July 2014}}&lt;/ref&gt;
* Aggregate identity data across sources to create a single point of access.
* Create high-availability for authoritative data stores.
* Act as identity firewall by preventing [[denial-of-service attack]]s on the primary data stores through an additional virtual layer.
* Support a common searchable namespace for centralized authentication.
* Present a unified virtual view of user information stored across multiple systems.
* Delegate authentication to backend sources through source-specific security means.
* Virtualize data sources to support migration from legacy data stores without modifying the applications that rely on them.
* Enrich identities with attributes pulled from multiple data stores, based on a link between user entries. 

Some advanced identity virtualization platforms can also:
* Enable application-specific, customized views of identity data without violating internal or external regulations governing identity data. Reveal contextual relationships between objects through hierarchical directory structures.
* Develop advanced correlation across diverse sources using correlation rules. 
* Build a global user identity by correlating unique user accounts across various data stores, and enrich identities with attributes pulled from multiple data stores, based on a link between user entries. 
* Enable constant data refresh for real-time updates through a persistent cache.

==Advantages ==
Virtual directories:
* Enable faster deployment because users do not need to add and sync additional application-specific data sources 
* Leverage existing identity infrastructure and security investments to deploy new services 
* Deliver high availability of data sources 
* Provide application-specific views of identity data which can help avoid the need to develop a master enterprise schema
* Allow a single view of identity data without violating internal or external regulations governing identity data
* Act as identity firewalls by preventing denial-of-service attacks on the primary data-stores and providing further security on access to sensitive data
* Can reflect changes made to authoritative sources in real-time
* Present a unified virtual view of user information from multiple systems so that it appears to reside in a single system
* Can secure all backend storage locations with a single security policy

==Disadvantages==
An original disadvantage is public perception of "push &amp; pull technologies" which is the general classification of "virtual directories" depending on the nature of their deployment. Virtual directories were initially designed and later deployed with "push technologies" in mind, which also contravened with [[privacy laws of the United States]]. This is no longer the case. There are, however, other disadvantages in the current technologies.

* The classical virtual directory based on proxy cannot modify underlying data structures or create new views based on the relationships of data from across multiple systems. So if an application requires a different structure, such as a flattened list of identities, or a deeper hierarchy for delegated administration, a virtual directory is limited. 
* Many virtual directories cannot correlate same-users across multiple diverse sources in the case of duplicate users
* Virtual directories without advanced caching technologies cannot scale to heterogeneous, high-volume environments.

==Sample terminology==
{{Overly detailed|section=yes|date=July 2014}}
* Unify metadata: Extract schemas from the local data source, map them to a common format, and link the same identities from different data silos based on a unique identifier.
* Namespace joining: Create a single large directory by bringing multiple directories together at the namespace level. For instance, if one directory has the namespace "ou=internal,dc=domain,dc=com" and a second directory has the namespace "ou=external,dc=domain,dc=com," then creating a virtual directory with both namespaces is an example of namespace joining.
* Identity joining: Enrich identities with attributes pulled from multiple data stores, based on a link between user entries.  For instance if the user joeuser exists in a directory as "cn=joeuser,ou=users" and in a database with a username of "joeuser" then the "joeuser" identity can be constructed from both the directory and the database.
* Data remapping: The translation of data inside of the virtual directory. For instance, mapping “uid” to “samaccountname,” so a client application that only supports a standard LDAP-compliant data source is able to search an Active Directory namespace, as well.
* Query routing: Route requests based on certain criteria, such as “write operations going to a master, while read operations are forwarded to replicas.”
* Identity routing: Virtual directories may support the routing of requests based on certain criteria (such as write operations going to a master while read operations being forwarded to replicas).
* Authoritative source: A "virtualized" data repository, such as a directory or database, that the virtual directory can trust for user data.
* Server groups: Group one or more servers containing the same data and functionality. A typical implementation is the multi-master, multi-replica environment in which replicas process "read" requests and are in one server group, while masters process "write" requests and are in another, so that servers are grouped by their response to external stimuli, even though all share the same data.

==Use cases==
The following are sample use cases of virtual directories:
* Integrating multiple directory namespaces to create a central enterprise directory.
* Supporting infrastructure integrations after mergers and acquisitions. 
* Centralizing identity storage across the infrastructure, making identity information available to applications through various protocols (including LDAP, JDBC, and web services). 
* Creating a single access point for [[Web Access Management|web access management]] (WAM) tools. 
* Enabling web [[single sign-on]] (SSO) across varied sources or domains.
* Supporting role-based, fine-grained authorization policies
* Enabling authentication across different security domains using each domain’s specific credential checking method.
* Improving secure access to information both inside and outside of the firewall.

==References==
&lt;references/&gt;

{{DEFAULTSORT:Virtual Directory}}
[[Category:Data management]]</text>
      <sha1>t7yrvn3k9mgt0mg713nbsgozkimougk</sha1>
    </revision>
  </page>
  <page>
    <title>Linear medium</title>
    <ns>0</ns>
    <id>676562</id>
    <revision>
      <id>691101967</id>
      <parentid>374241380</parentid>
      <timestamp>2015-11-17T17:47:19Z</timestamp>
      <contributor>
        <username>Aasasd</username>
        <id>22263090</id>
      </contributor>
      <comment>/* See also */ redirects to 'Sequential access'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1340" xml:space="preserve">{{Unreferenced|date=December 2009}}
A '''linear medium''' is any medium which is intended to be written to or accessed in a [[linear]] fashion, literally meaning ''in a line''. 

This means that the information is written to or read from the medium in a given order, so for example a book containing a [[novel]] is intended to be read from front to back, beginning to end, and is therefore a linear medium. It may be written in the same way, but would not necessarily need to be, to be considered a linear medium. 
A book containing an [[encyclopedia]] however is a non-linear medium, as it is not necessary for the articles to be accessed (or written) in any particular order. Even though both non-linear and linear mediums have perimeters to which they are restricted, linear mediums have a set path of how to get from point A to point B, whereas non-linear mediums do not. 

Examples in technology are a pre-recorded [[videocassette]] which is usually accessed one item after another, compared with a pre-recorded [[DVD]] which can be accessed in any order.

==Types of linear medium==
* [[Scroll]]
* [[Magnetic tape data storage]]
* [[Paper tape]]
* [[Photographic film]]
* [[Novel|story book]]s
* [[Compact cassette]]s

==See also==
*[[Sequential access]] 
*[[Random access]]

{{DEFAULTSORT:Linear Medium}}
[[Category:Data management]]</text>
      <sha1>c1l13yttu7gdgdlvpa4nchfp8tgc3b9</sha1>
    </revision>
  </page>
  <page>
    <title>White pages schema</title>
    <ns>0</ns>
    <id>1462050</id>
    <revision>
      <id>718116231</id>
      <parentid>688509110</parentid>
      <timestamp>2016-05-01T16:45:22Z</timestamp>
      <contributor>
        <username>Wiae</username>
        <id>3495083</id>
      </contributor>
      <minor />
      <comment>to log into -&gt; to log in to</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3746" xml:space="preserve">{{Unreferenced|date=December 2009}}
A '''white pages schema''' is a [[data model]], specifically a [[logical schema]], for organizing the data contained in entries in a [[directory service]], database, or application, such as an address book.  In a white pages directory, each entry typically represents an individual [[user (computing)|person that makes use of]] network resources, such as by receiving email or having an account to log in to a system.
In some environments, the schema may also include the representation of organizational divisions, roles, groups, and devices.  The term is derived from the [[white pages]], the listing of individuals in a [[telephone directory]], typically sorted by the individual's home location (e.g. city) and then by
their name.

While many [[Postal Telephone and Telegraph|telephone service providers]] have for decades published a list of their [[subscriber]]s in a [[telephone directory]], and similarly corporations published a list of their employees in an internal directory, it was not until the rise of [[electronic mail]] systems that a requirement for standards for the electronic exchange of [[subscriber]] information between different systems appeared.

A white pages schema typically defines, for each real-world object being represented:

* what attributes of that object are to be represented in the entry for that object
* what relationships of that object to other objects are to be represented
* how is the entry to be named in a [[Directory Information Tree|DIT]]
* how an entry is to be located by a client searching for it
* how similar entries are to be distinguished
* how are entries to be ordered when displayed in a list

One of the earliest attempts to standardize a white pages schema for electronic mail use was in [[X.520]] and [[X.521]], part of the [[X.500]] specifications,
that was derived from the addressing requirements of [[X.400]] and defined a [[Directory Information Tree]] that mirrored the international telephone system, with entries representing residential and organizational subscribers.  This evolved into the [[Lightweight Directory Access Protocol]] standard schema in RFC 2256.  One of the most widely deployed white pages schemas used in LDAP
for representing individuals in an organizational context is '''inetOrgPerson''', defined in RFC 2798, although versions of [[Active Directory]] require a different object class, '''User'''.  Many large organizations have
also defined their own white pages schemas for their employees or customers, as part of their [[Identity management]] architecture.  Converting between data bases and directories using different schemas is often the
function of a [[Metadirectory]], and data interchange standards such as [[Common Indexing Protocol]].

Some early directory deployments suffered due to poor design choices in their white pages schema, such as:

* attributes used for naming purposes were non-unique in large environments (such as a person's common name)
* attributes used for naming purposes were likely to change (such as surnames)
* attributes were included which could lead to [[Identity theft]], such as a [[Social security number]]
* users were required during [[provisioning]] to choose attributes which are unique but still memorable to them

Numerous other proposed schemas exist, both as standalone definitions suitable for use with general purpose
directories, or as embedded into network protocols.

Examples of other generic white pages schemas include [[vCard]], defined in RFC 2426, and [[FOAF (software)|FOAF]].

==See also==
* [[Recognition of human individuals]]

{{DEFAULTSORT:White Pages Schema}}
[[Category:Data modeling]]
[[Category:Data management]]
[[Category:Identity management]]</text>
      <sha1>qvjp24it97x8zxlzne09jkz6v0e94zp</sha1>
    </revision>
  </page>
  <page>
    <title>Data auditing</title>
    <ns>0</ns>
    <id>6890125</id>
    <revision>
      <id>746124190</id>
      <parentid>737583262</parentid>
      <timestamp>2016-10-25T11:34:13Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/73.254.190.33|73.254.190.33]] ([[User talk:73.254.190.33|talk]]) to last revision by Addbot. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="416" xml:space="preserve">{{Unreferenced|date=December 2009}}
'''Data auditing''' is the process of conducting a data audit to assess how company's data is fit for given purpose.  This involves [[data profiling|profiling]] the data and assessing the impact of [[data quality|poor quality data]] on the organization's performance and profits.

{{DEFAULTSORT:Data Auditing}}
{{Tech-stub}}

[[Category:Data management]]
[[Category:Data quality]]</text>
      <sha1>3rv7ibk1b3gv2peq2shcumpkl2s8b19</sha1>
    </revision>
  </page>
  <page>
    <title>Automated tiered storage</title>
    <ns>0</ns>
    <id>5732700</id>
    <revision>
      <id>755676226</id>
      <parentid>733446991</parentid>
      <timestamp>2016-12-19T14:51:23Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[data migration]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6859" xml:space="preserve">'''Automated tiered storage''' (also '''automated storage tiering''') is the automated progression or demotion of data across different tiers (types) of storage devices and media. The movement of data takes place in an automated way with the help of a software or embedded firmware and is assigned to the related media according to performance and capacity requirements. More advanced implementations include the ability to define rules and policies that dictate if and when data can be moved between the tiers, and in many cases provides the ability to pin data to tiers permanently or for specific periods of time. Implementations vary, but are classed into two broad categories: pure software based implementations that run on general purpose processors supporting most forms of general purpose storage media and embedded automated tiered storage controlled by firmware as part of a closed embedded storage system such as a SAN disk array. Software Defined Storage architectures commonly include a component of tiered storage as part of their primary functions.

In the most general definition, Automated Tiered Storage is a form of Hierarchical Storage Management. However, the term automated tiered storage has emerged to accommodate newer forms of real-time performance optimized [[data migration]] driven by the proliferation of solid state disks and storage class memory. Furthermore, where traditional HSM systems act on files and move data between storage tiers in a batch, scheduled like fashion, automated storage tiered systems are capable of operating at sub-file level both in batch and real-time modes. In the case of the latter, data is moved almost as soon as it enters the storage system or relocated based on its activity levels within seconds of data being accessed, whereas more traditional tiering tends to operate on an hourly, daily or even weekly schedule. Some more background on the relative differences between HSM, ILM and automated tiered storage is available at SNIA web site.&lt;ref&gt;http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf&lt;/ref&gt; A general comparison of different approaches can also be found in this 'comparison article on auto tiered storage'[http://searchstorage.techtarget.co.uk/feature/Automated-storage-tiering-product-comparison].

==OS and Software Based Automated Tiered Storage==
Most server oriented software automated tiered storage vendors offer tiering as a component of a general storage virtualization stack offering, an example being [[Microsoft]] with their Tiered Storage Spaces.&lt;ref&gt;https://redmondmag.com/articles/2013/08/30/windows-storage-tiering.aspx?m=1&lt;/ref&gt; However, automated tiering is now becoming a common part of industry standard operating systems such as Linux and Microsoft Windows, and in the case of consumer PCs, Apple OSX with its Fusion Drive.&lt;ref&gt;[http://www.apple.com/imac/performance/ "Apple iMac Performance Website"] October 24, 2012.&lt;/ref&gt; This solution allowed a single SSD and hard disk drive to be combined into a single automated tiered storage drive that ensured that the most frequently accessed data was stored on the SSD portion of the virtual disk. A more OS agnostic version was introduced by Enmotus which supports real-time tiering with its FuzeDrive product for Linux and Windows operating systems, extending support to storage class memory offerings such as NVDIMM and NVRAM devices.&lt;ref&gt;http://cdn2.hubspot.net/hub/486631/file-2586107985-pdf/PDFs/20111129_S2-102_Mills.pdf?t=1447892865729&lt;/ref&gt;

==SAN Based Tiered Storage==
An example of automated tiered storage in a hardware storage array is a feature called Data Progression from Compellent Technologies. Data Progression has the capability to transparently move blocks of data between different drive types and RAID groups such as RAID 10 and RAID 5. The blocks are part of the "same virtual volume even as they span different RAID groups and drive types.  Compellent can do this because they keep metadata about every block -- which allows them to keep track of each block and its associations.".&lt;ref&gt;[http://blogs.computerworld.com/compellent_ilm  Tony Asaro, Computerworld. "Compellent-Intelligent Tiered Storage."] January 19, 2009.&lt;/ref&gt; Another strong example of SAN based tiering is DotHill's Autonomous Tiered Storage which moves data between tiers of storage within the SAN disk array with decisions made every few seconds".&lt;ref&gt;[https://www.dothill.com/solutions/tiered-data-storage/ "Hybrid Data Storage Solution with SSD and HDD Tiers"]&lt;/ref&gt;

== Automated Tiered Storage vs. SSD Caching ==
While tiering solutions and caching may look the same on the surface, the fundamental differences lie in the way the faster storage is utilized and the algorithms used to detect and accelerate frequently accessed data. SSD caching operates much like SRAM-DRAM caches do i.e. they make a copy of frequently accessed blocks of data, for example in 4K cache page sizes, and store the copy in the SSD and use this copy instead of the original data source on the slower backend storage. Every time a storage IO occurs, the caching software look to see if a copy of this data already exists using a variety of algorithms and service the host request from the SSD if it is found. The SSD is used in this case as a lookaside device as it is not part of the primary storage. While some good caching algorithms can demonstrate native SSD performance on reads and short bursts of writes, caching typically operates well below the maximum sustainable rate of the underlying SSD devices as overhead CPU cycles are introduced during the host IO commands that increasingly impact performance as the amount of data cached grows. Tiering on the other hand operates very differently. Using the specific case of SSDs, once data is identified as frequently used, the identified blocks of data are moved in the background to the SSD and not copied as the SSD is being utilized as a primary storage tier, not a look aside copy area. When the data is subsequently accessed, the IOs occur at or near the native performance of the SSDs as there area are few if any CPU cycles needed to do the simpler virtual to physical addressing translations.&lt;ref&gt;[http://searchsolidstatestorage.techtarget.com/tip/Tiering-vs-caching-in-flash-based-storage-systems] "Tiering vs. caching in flash-based storage systems"&lt;/ref&gt;

== See also ==
* [[Hierarchical storage management]]
* [[Tiered storage]]

== References ==
* Russ Taddiken – Senior Storage Architect (2006). Automating Data Movement Between Storage Tiers. Retrieved from the UW Records Management Web site: http://www.compellent.com/
&lt;references/&gt;

== External links ==
* http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf

[[Category:Data management]]</text>
      <sha1>558tmm1uhtfsnglm1ynaxyh5q84guoq</sha1>
    </revision>
  </page>
  <page>
    <title>Jenks natural breaks optimization</title>
    <ns>0</ns>
    <id>25397242</id>
    <revision>
      <id>739548450</id>
      <parentid>669097782</parentid>
      <timestamp>2016-09-15T10:02:07Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>clean up, [[WP:AWB/T|typo(s) fixed]]: 37 year → 37-year using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6033" xml:space="preserve">The '''Jenks optimization method''', also called the '''Jenks natural breaks classification method''', is a [[data clustering]] method designed to determine the best arrangement of values into different classes. This is done by seeking to minimize each class’s average deviation from the class mean, while maximizing each class’s deviation from the means of the other groups. In other words, the method seeks to reduce the [[variance]] within classes and maximize the variance between classes.&lt;ref name="Jenks"&gt;Jenks, George F. 1967. "The Data Model Concept in Statistical Mapping", International Yearbook of Cartography 7: 186–190.&lt;/ref&gt;&lt;ref name="McMaster"&gt;McMaster, Robert, "In Memoriam: George F. Jenks (1916–1996)". Cartography and Geographic Information Science. 24(1) p.56-59.&lt;/ref&gt;

==History==

=== George Jenks ===
George Frederick Jenks was a 20th Century American [[cartography|cartographer]]. Graduating with his Ph.D. in agricultural geography from [[Syracuse University]] in 1947, Jenks began his career under the tutelage of [[Richard Edes Harrison|Richard Harrison]], cartographer for [[Time (magazine)|TIME]] and Fortune magazine.&lt;ref name="McMaster2"&gt;McMaster, Robert and McMaster, Susanna. 2002. “A History of Twentieth-Century American Academic Cartography”, Cartography and Geographic Information Science. 29(3) p.312-315.&lt;/ref&gt; He joined the faculty of the [[University of Kansas]] in 1949 and began to build the cartography program. During his 37-year tenure at KU, Jenks developed the Cartography program into one of three programs renowned for their graduate education in the field; the others being the [[University of Wisconsin]] and the [[University of Washington]]. Much of his time was spent developing and promoting improved cartographic training techniques and programs. He also spent significant time investigating three-dimensional maps, eye-movement research, [[thematic map]] communication, and [[geostatistics]].&lt;ref name="McMaster" /&gt;&lt;ref name="McMaster2" /&gt;&lt;ref name="CSUN"&gt;CSUN Cartography Specialty Group, [http://www.csun.edu/~hfgeg003/csg/winter97.html Winter 1997 Newsletter]&lt;/ref&gt;

===Development===
Jenks was a cartographer by profession. His work with [[statistics]] grew out of a desire to make [[choropleth map]]s more visually accurate for the viewer. In his paper, ''The Data Model Concept in Statistical Mapping'', he claims that by visualizing data in a three dimensional model cartographers could devise a “systematic and rational method for preparing choroplethic maps”.&lt;ref name="Jenks" /&gt; Jenks used the analogy of a “blanket of error” to describe the need to use elements other than the mean to generalize data. The three dimensional models were created to help Jenks visualize the difference between data classes. His aim was to generalize the data using as few planes as possible and maintain a constant “blanket of error”.

==Method==
The method requires an iterative process. That is, calculations must be repeated using different breaks in the dataset to determine which set of breaks has the smallest in-class [[variance]]. The process is started by dividing the ordered data into groups. Initial group divisions can be arbitrary. There are four steps that must be repeated:
#Calculate the sum of squared deviations between classes (SDBC).
#Calculate the sum of squared deviations from the array mean (SDAM).
#Subtract the SDBC from the SDAM (SDAM-SDBC). This equals the sum of the squared deviations from the class means (SDCM).
#After inspecting each of the SDBC, a decision is made to move one unit from the class with the largest SDBC toward the class with the lowest SDBC.

New class deviations are then calculated, and the process is repeated until the sum of the within class deviations reaches a minimal value.&lt;ref name="Jenks" /&gt;&lt;ref name="ESRI"&gt;ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&amp;d=26442 What is the Jenks Optimization method]&lt;/ref&gt;

Alternatively, all break combinations may be examined, SDCM calculated for each combination, and the combination with the lowest SDCM selected. Since all break combinations are examined, this guarantees that the one with the lowest SDCM is found.

Finally, the GVF statistic (goodness of variance fit) is calculated. GVF is defined as (SDAM - SDCM) / SDAM. GVF ranges from 0 (worst fit) to 1 (perfect fit).

==Uses==

{{main article|Choropleth map}}
Jenks’ goal in developing this method was to create a map that was absolutely accurate, in terms of the representation of data’s spatial attributes. By following this process, Jenks claims, the “blanket of error” can be uniformly distributed across the mapped surface. He developed this with the intention of using relatively few data classes, less than seven, because that was the limit when using monochromatic shading on a choroplethic map.&lt;ref name="Jenks" /&gt;

==Alternative methods==
{{Main article|Cluster analysis}}

Other methods of data classification include [[Head/tail Breaks]], Natural Breaks (without Jenks Optimization), Equal Interval, Quantile, and Standard Deviation.

==See also==
* [[k-means clustering]], a generalization for multivariate data (Jenks natural breaks optimization seems to be one dimensional k-means&lt;ref&gt;[http://www.quantdec.com/SYSEN597/GTKAV/section1/chapter_9.htm]&lt;/ref&gt;).

==References==
{{Reflist}}

==External links==
* ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&amp;d=26442 What is the Jenks Optimization method]
* Volunteered Geographic Information, Daniel Lewis, [http://danieljlewis.org/2010/06/07/jenks-natural-breaks-algorithm-in-python/ Jenks Natural Breaks Algorithm with an implementation in python]
* Object Vision wiki, [http://wiki.objectvision.nl/index.php/Fisher%27s_Natural_Breaks_Classification Fisher's Natural Breaks Classification, a O(k*n*log(n)) algorithm]
* [http://www.ehdp.com/vitalnet/breaks-1.htm What is Jenks Natural Breaks?]

[[Category:Data management]]
[[Category:Cartography]]</text>
      <sha1>rde0ox4vb9d8y79l5w6nwisoi41v9ll</sha1>
    </revision>
  </page>
  <page>
    <title>Data validation and reconciliation</title>
    <ns>0</ns>
    <id>26683958</id>
    <revision>
      <id>740318391</id>
      <parentid>717520199</parentid>
      <timestamp>2016-09-20T09:32:41Z</timestamp>
      <contributor>
        <username>Nicolaufg</username>
        <id>27551214</id>
      </contributor>
      <minor />
      <comment>A sentence regarding the effect of "error remediation" seemed to be truncated. I added a couple of words that I think do match what should be there.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26483" xml:space="preserve">'''Industrial process data validation and reconciliation''', or more briefly, '''data validation and reconciliation (DVR)''', is a technology that uses process information and mathematical methods in order to automatically correct measurements in industrial processes. The use of DVR allows for extracting accurate and reliable information about the state of industry processes from raw measurement data and produces a single consistent set of data representing the most likely process operation.

==Models, data and measurement errors==
Industrial processes, for example chemical or thermodynamic processes in chemical plants, refineries, oil or gas production sites, or power plants, are often represented by two fundamental means:
# Models that express the general structure of the processes,
# Data that reflects the state of the processes at a given point in time.
Models can have different levels of detail, for example one can incorporate simple mass or compound conservation balances, or more advanced thermodynamic models including energy conservation laws. Mathematically the model can be expressed by a [[nonlinear system|nonlinear system of equations]] &lt;math&gt;F(y)=0\,&lt;/math&gt; in the variables &lt;math&gt;y=(y_1,\ldots,y_n)&lt;/math&gt;, which incorporates all the above-mentioned system constraints (for example the mass or heat balances around a unit). A variable could be the temperature or the pressure at a certain place in the plant.

===Error types===
&lt;gallery caption="Random and systematic errors" widths="300%" perrow="2" align="right"&gt;
File:Normal_no_bias.jpg|Normally distributed measurements without bias.
File:Normal_with_bias.jpg|Normally distributed measurements with bias.
&lt;/gallery&gt;
Data originates typically from [[measurements]] taken at different places throughout the industrial site, for example temperature, pressure, volumetric flow rate measurements etc. To understand the basic principles of DVR, it is important to first recognize that plant measurements are never 100% correct, i.e. raw measurement &lt;math&gt;y\,&lt;/math&gt; is not a solution of the nonlinear system &lt;math&gt;F(y)=0\,\!&lt;/math&gt;. When using measurements without correction to generate plant balances, it is common to have incoherencies. [[Observational error|Measurement errors]] can be categorized into two basic types:
# [[random error]]s due to intrinsic [[sensor]] [[accuracy]] and
# [[systematic errors]] (or gross errors) due to sensor [[calibration]] or faulty data transmission.

[[Random error]]s means that the measurement &lt;math&gt;y\,\!&lt;/math&gt; is a [[random variable]] with [[mean]] &lt;math&gt;y^*\,\!&lt;/math&gt;, where &lt;math&gt;y^*\,\!&lt;/math&gt; is the true value that is typically not known. A [[systematic error]] on the other hand is characterized by a measurement &lt;math&gt;y\,\!&lt;/math&gt; which is a random variable with [[mean]] &lt;math&gt;\bar{y}\,\!&lt;/math&gt;, which is not equal to the true value &lt;math&gt;y^*\,&lt;/math&gt;.  For ease in deriving and implementing an optimal estimation solution, and based on arguments that errors are the sum of many factors (so that the [[Central limit theorem]] has some effect), data reconciliation assumes these errors are [[normal distribution|normally distributed]].   

Other sources of errors when calculating plant balances include process faults such as leaks, unmodeled heat losses, incorrect physical properties or other physical parameters used in equations, and incorrect structure such as unmodeled bypass lines.  Other errors include unmodeled plant dynamics such as holdup changes, and other instabilities in plant operations that violate steady state (algebraic) models.  Additional dynamic errors arise when measurements and samples are not taken at the same time, especially lab analyses.  

The normal practice of using time averages for the data input partly reduces the dynamic problems.  However, that does not completely resolve timing inconsistencies for infrequently-sampled data like lab analyses.  

This use of average values, like a [[moving average]], acts as a [[low-pass filter]], so high frequency noise is mostly eliminated.   The result is that, in practice, data reconciliation is mainly making adjustments to correct systematic errors like biases.

===Necessity of removing measurement errors===
ISA-95 is the international standard for the integration of enterprise and control systems&lt;ref&gt;[http://www.isa-95.com/ "ISA-95: the international standard for the integration of enterprise and control systems"]. isa-95.com.&lt;/ref&gt; It asserts that:
&lt;blockquote&gt;Data reconciliation is a serious issue for enterprise-control integration. The data have to be valid to be useful for the enterprise system. The data must often be determined from physical measurements that have associated error factors. This must usually be converted into exact values for the enterprise system. This conversion may require manual, or intelligent reconciliation of the converted values [...].

Systems must be set up to ensure that accurate data are sent to production and from production. Inadvertent operator or clerical errors may result in too much production, too little production, the wrong production, incorrect inventory, or missing inventory.&lt;/blockquote&gt;

==History==
DVR has become more and more important due to industrial processes that are becoming more and more complex. DVR started in the early 1960s with applications aiming at closing [[mass balance|material balances]] in production processes where raw measurements were available for all [[variable (mathematics)|variables]].&lt;ref&gt;D.R. Kuehn, H. Davidson, ''Computer Control II. Mathematics of Control'', Chem. Eng. Process 57: 44–47, 1961.&lt;/ref&gt; At the same time the problem of [[systematic error|gross error]] identification and elimination has been presented.&lt;ref&gt;V. Vaclavek, ''Studies on System Engineering I. On the Application of the Calculus of the Observations of Calculations of Chemical Engineering Balances'', Coll. Czech Chem. Commun 34: 3653, 1968.&lt;/ref&gt; In the late 1960s and 1970s unmeasured variables were taken into account in the data reconciliation process.,&lt;ref&gt;V. Vaclavek, M. Loucka, ''Selection of Measurements Necessary to Achieve Multicomponent Mass Balances in Chemical Plant'', Chem. Eng. Sci. 31: 1199–1205, 1976.&lt;/ref&gt;&lt;ref name="Mah-Stanley-Downing-1976"&gt;[http://gregstanleyandassociates.com/ReconciliationRectificationProcessData-1976.pdf R.S.H. Mah, G.M. Stanley, D.W. Downing, ''Reconciliation and Rectification of Process Flow and Inventory Data'', Ind. &amp; Eng. Chem. Proc. Des. Dev. 15: 175–183, 1976.]&lt;/ref&gt; DVR also became more mature by considering general nonlinear equation systems coming from thermodynamic models.,&lt;ref&gt;J.C. Knepper, J.W. Gorman, ''Statistical Analysis of Constrained Data Sets'', AiChE Journal 26: 260–164, 1961.&lt;/ref&gt;
,&lt;ref name="Stanley-Mah-1977"&gt;[http://gregstanleyandassociates.com/AIChEJ-1977-EstimationInProcessNetworks.pdf G.M. Stanley and R.S.H. Mah, ''Estimation of Flows and Temperatures in Process Networks'', AIChE Journal 23: 642–650, 1977.]&lt;/ref&gt;
&lt;ref&gt;P. Joris, B. Kalitventzeff, ''Process measurements analysis and validation'', Proc. CEF’87: Use Comput. Chem. Eng., Italy, 41–46, 1987.&lt;/ref&gt; Quasi steady state dynamics for filtering and simultaneous parameter estimation over time were introduced in 1977 by Stanley and Mah.&lt;ref name="Stanley-Mah-1977"/&gt;  Dynamic DVR was formulated as a nonlinear optimization problem by Liebman et al. in 1992.&lt;ref&gt;M.J. Liebman, T.F. Edgar, L.S. Lasdon, ''Efficient Data Reconciliation and Estimation for Dynamic Processes Using Nonlinear Programming Techniques'', Computers Chem. Eng. 16: 963–986, 1992.&lt;/ref&gt;

==Data reconciliation==
Data reconciliation is a technique that targets at correcting measurement errors that are due to measurement noise, i.e. [[random error]]s. From a statistical point of view the main assumption is that no [[systematic errors]] exist in the set of measurements, since they may bias the reconciliation results and reduce the robustness of the reconciliation.

Given &lt;math&gt;n&lt;/math&gt; measurements &lt;math&gt;y_i&lt;/math&gt;, data reconciliation can mathematically be expressed as an [[optimization problem]] of the following form:

&lt;math&gt; \begin{align}
 \min_{x,y^*} &amp; \sum_{i=1}^n\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2 \\
\text{subject to  }      &amp; F(x,y^*)=0 \\
&amp; y_\min \le y^*\le y_\max\\
&amp; x_\min \le x\le x_\max,
\end{align}\,\!
&lt;/math&gt;

where
&lt;math&gt;y_i^*\,\!&lt;/math&gt; is the reconciled value of the &lt;math&gt;i&lt;/math&gt;-th measurement (&lt;math&gt;i=1,\ldots,n\,\!&lt;/math&gt;), &lt;math&gt;y_i\,\!&lt;/math&gt; is the measured value of the &lt;math&gt;i&lt;/math&gt;-th measurement (&lt;math&gt;i=1,\ldots,n\,\!&lt;/math&gt;), &lt;math&gt;x_j\,\!&lt;/math&gt; is the &lt;math&gt;j&lt;/math&gt;-th unmeasured variable (&lt;math&gt;j=1,\ldots,m\,\!&lt;/math&gt;),  and &lt;math&gt;\sigma_i\,\!&lt;/math&gt; is the standard deviation of the &lt;math&gt;i&lt;/math&gt;-th measurement (&lt;math&gt;i=1,\ldots,n\,\!&lt;/math&gt;),
&lt;math&gt;F(x,y^*)=0\,\!&lt;/math&gt; are the &lt;math&gt;p\,\!&lt;/math&gt; process equality constraints and
&lt;math&gt;x_{\min}, x_{\max}, y_{\min}, y_{\max}\,\!&lt;/math&gt; are the bounds on the measured and unmeasured variables.

The term &lt;math&gt;\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2\,\!&lt;/math&gt; is called the ''penalty'' of measurement ''i''. The objective function is the sum of the penalties, which will be denoted in the following by &lt;math&gt;f(y^*)=\sum_{i=1}^n\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2&lt;/math&gt;.

In other words, one wants to minimize the overall correction (measured in the least squares term) that is needed in order to satisfy the [[constraint (mathematics)|system constraints]]. Additionally, each least squares term is weighted by the [[standard deviation]]  of the corresponding measurement.

===Redundancy===
&lt;gallery caption="Sensor and topological redundancy" heights="150px" widths="225px" perrow="2" align="right"&gt;
File:sensor_red.jpg|Sensor redundancy arising from multiple sensors of the same quantity at the same time at the same place.
File:topological_red.jpg|Topological redundancy arising from model information, using the mass conservation constraint &lt;math&gt;a=b+c\,\!&lt;/math&gt;, for example one can calculate &lt;math&gt;c\,\!&lt;/math&gt;, when &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt; are known.
&lt;/gallery&gt;
Data reconciliation relies strongly on the concept of redundancy to correct the measurements as little as possible in order to satisfy the process constraints.  Here, redundancy is defined differently from [[Redundancy (information theory)|redundancy in information theory]].  Instead, redundancy arises from combining sensor data with the model (algebraic constraints), sometimes more specifically called "spatial redundancy",&lt;ref name="Stanley-Mah-1977"/&gt; "analytical redundancy", or "topological redundancy". 

Redundancy can be due to [[redundancy (engineering)|sensor redundancy]], where sensors are duplicated in order to have more than one measurement of the same quantity. Redundancy also arises when a single variable can be estimated in several independent ways from separate sets of measurements at a given time or time averaging period, using the algebraic constraints.  

Redundancy is linked to  the concept of [[observability]].  A variable (or system) is observable if the models and sensor measurements can be used to uniquely determine its value (system state).  A sensor is redundant if its removal causes no loss of observability.   Rigorous definitions of observability, calculability, and redundancy, along with criteria for determining it, were established by Stanley and Mah,&lt;ref name="Stanley-Mah-1981a"&gt;
[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981a-ObservabilityRedundancy.pdf Stanley G.M. and Mah, R.S.H., "Observability and Redundancy in Process Data Estimation, Chem. Engng. Sci. 36, 259 (1981)]&lt;/ref&gt; for these cases with set constraints such as algebraic equations and inequalities.    Next, we illustrate some special cases:

Topological redundancy  is intimately linked with the [[degrees of freedom (physics and chemistry)|degrees of freedom]] (&lt;math&gt;dof\,\!&lt;/math&gt;) of a mathematical system,&lt;ref name="vdi"&gt;VDI-Gesellschaft Energie und Umwelt, "Guidelines - VDI 2048 Blatt 1 - Uncertainties of measurements at acceptance tests for energy conversion and power plants - Fundamentals", ''[http://www.vdi.de/401.0.html Association of German Engineers]'', 2000.&lt;/ref&gt; i.e. the minimum number of pieces of information (i.e. measurements) that are required in order to calculate all of the system variables. For instance, in the example above the flow conservation requires that &lt;math&gt;a=b+c\,&lt;/math&gt;.  One needs to know the value of two of the 3 variables in order to calculate the third one. The degrees of freedom for the model in that case is equal to 2.  At least 2 measurements are needed to estimate all the variables, and 3 would be needed for redundancy.

When speaking about topological redundancy we have to distinguish between measured and unmeasured variables. In the following let us denote by &lt;math&gt;x\,\!&lt;/math&gt; the unmeasured variables and &lt;math&gt;y\,\!&lt;/math&gt; the measured variables. Then the system of the process constraints becomes &lt;math&gt;F(x,y)=0\,\!&lt;/math&gt;, which is a nonlinear system in &lt;math&gt;y\,\!&lt;/math&gt; and &lt;math&gt;x\,\!&lt;/math&gt;.
If the system &lt;math&gt;F(x,y)=0\,\!&lt;/math&gt; is calculable with the &lt;math&gt;n\,&lt;/math&gt; measurements given, then the level of topological redundancy is defined as &lt;math&gt;red= n - dof\,\!&lt;/math&gt;, i.e. the number of additional measurements that are at hand on top of those measurements which are required in order to just calculate the system. Another way of viewing the level of redundancy is to use the definition of &lt;math&gt;dof\,&lt;/math&gt;, which is the difference between the number of variables (measured and unmeasured) and the number of equations. Then one gets

:&lt;math&gt;\begin{align}
red= n - dof = n-(n+m-p) = p-m,
\end{align}&lt;/math&gt;

i.e. the redundancy is the difference between the number of equations &lt;math&gt;p\,&lt;/math&gt; and the number of unmeasured variables &lt;math&gt;m\,&lt;/math&gt;. The level of total redundancy is the sum of sensor redundancy and topological redundancy. We speak of positive redundancy if the system is calculable and the total redundancy is positive. One can see that the level of topological redundancy merely depends on the number of equations (the more equations the higher the redundancy) and the number of unmeasured variables (the more unmeasured variables, the lower the redundancy) and not on the number of measured variables. 

Simple counts of variables, equations, and measurements are inadequate for many systems, breaking down for several reasons: (a) Portions of a system might have redundancy, while others do not, and some portions might not even be possible to calculate, and  (b) Nonlinearities can lead to different conclusions at different operating points.  As an example, consider the following system with 4 streams and 2 units.

====Example of calculable and non-calculable systems====
&lt;gallery caption="Calculable and non-calculable systems" heights="150px" widths="225px" perrow="2" align="right"&gt;
File:calculable_system.jpg|Calculable system, from &lt;math&gt;d\,\!&lt;/math&gt; one can compute &lt;math&gt;c\,\!&lt;/math&gt;, and knowing &lt;math&gt;a\,\!&lt;/math&gt; yields &lt;math&gt;b\,\!&lt;/math&gt;.
File:uncalculable_system.jpg|non-calculable system, knowing &lt;math&gt;c\,\!&lt;/math&gt; does not give information about &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt;.
&lt;/gallery&gt;

We incorporate only flow conservation constraints and obtain &lt;math&gt;a+b=c\,\!&lt;/math&gt; and &lt;math&gt;c=d\,\!&lt;/math&gt;.  It is possible that the system &lt;math&gt;F(x,y)=0\,\!&lt;/math&gt; is not calculable, even though &lt;math&gt;p-m\ge 0\,\!&lt;/math&gt;.

If we have measurements for &lt;math&gt;c\,\!&lt;/math&gt; and &lt;math&gt;d\,\!&lt;/math&gt;, but not for &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt;, then the system cannot be calculated (knowing &lt;math&gt;c\,\!&lt;/math&gt; does not give information about &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt;). On the other hand, if &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;c\,\!&lt;/math&gt; are known, but not &lt;math&gt;b\,\!&lt;/math&gt; and &lt;math&gt;d\,\!&lt;/math&gt;, then the system can be calculated.

In 1981, observability and redundancy criteria were proven for these sorts of flow networks involving only mass and energy balance constraints.&lt;ref name="Stanley-Mah-1981b"&gt;[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981b-ObservabilityRedundancyProcessNetworks.pdf Stanley G.M., and Mah R.S.H., "Observability and Redundancy Classification in Process Networks", Chem. Engng. Sci. 36, 1941 (1981) ]&lt;/ref&gt;  After combining all the plant inputs and outputs into an "environment node",  loss of observability corresponds to cycles of unmeasured streams.  That is seen in the second case above, where streams a and b are in a cycle of unmeasured streams.  Redundancy classification follows, by testing for a path of unmeasured streams, since that would lead to an unmeasured cycle if the measurement was removed.  Measurements c and d are redundant in the second case above, even though part of the system is unobservable.

===Benefits===
Redundancy can be used as a source of information to cross-check and correct the measurements &lt;math&gt;y\,\!&lt;/math&gt; and increase their accuracy and precision: on the one hand they reconciled Further, the data reconciliation problem presented above also includes unmeasured variables &lt;math&gt;x\,\!&lt;/math&gt;. Based on information redundancy, estimates for these unmeasured variables can be calculated along with their accuracies. In industrial processes these unmeasured variables that data reconciliation provides are referred to as [[soft sensor]]s or virtual sensors, where hardware sensors are not installed.

==Data validation==
Data validation denotes all validation and verification actions before and after the reconciliation step.

===Data filtering===
Data filtering denotes the process of treating measured data such that the values become meaningful and lie within the range of expected values. Data filtering is necessary before the reconciliation process in order to increase robustness of the reconciliation step. There are several ways of data filtering, for example taking the [[average]] of several measured values over a well-defined time period.

===Result validation===
Result validation is the set of validation or verification actions taken after the reconciliation process and it takes into account measured and unmeasured variables as well as reconciled values. Result validation covers, but is not limited to, penalty analysis for determining the reliability of the reconciliation, or bound checks to ensure that the reconciled values lie in a certain range, e.g. the temperature has to be within some reasonable bounds.

===Gross error detection===
Result validation may include statistical tests to validate the reliability of the reconciled values, by checking whether [[systematic error|gross errors]] exist in the set of measured values. These tests can be for example
* the chi square test (global test)
* the individual test.

If no gross errors exist in the set of measured values, then each penalty term in the objective function is a [[normal distribution|random variable]] that is normally distributed with mean equal to 0 and variance equal to 1. By consequence, the objective function is a random variable which follows a [[chi-square distribution]], since it is the sum of the square of normally distributed random variables. Comparing the value of the objective function &lt;math&gt;f(y^*)\,\!&lt;/math&gt; with a given [[percentile]] &lt;math&gt;P_{\alpha}\,&lt;/math&gt; of the probability density function of a chi-square distribution (e.g. the 95th percentile for a 95% confidence) gives an indication of whether a gross error exists: If &lt;math&gt;f(y^*)\le P_{95}&lt;/math&gt;, then no gross errors exist with 95% probability. The chi square test gives only a rough indication about the existence of gross errors, and it is easy to conduct: one only has to compare the value of the objective function with the critical value of the chi square distribution.

The individual test compares each penalty term in the objective function with the critical values of the normal distribution. If the &lt;math&gt;i&lt;/math&gt;-th penalty term is outside the 95% confidence interval of the normal distribution, then there is reason to believe that this measurement has a gross error.

==Advanced data validation and reconciliation==
Advanced data validation and reconciliation (DVR) is an integrated approach of combining data reconciliation and data validation techniques, which is characterized by
* complex models incorporating besides mass balances also thermodynamics, momentum balances, equilibria constraints, hydrodynamics etc.
* gross error remediation techniques to ensure meaningfulness of the reconciled values,
* robust algorithms for solving the reconciliation problem.

===Thermodynamic models===
Simple models include mass balances only. When adding thermodynamic constraints such as [[First law of thermodynamics|energy balances]] to the model, its scope and the level of [[Data redundancy|redundancy]] increases. Indeed, as we have seen above, the level of redundancy is defined as &lt;math&gt;p-m&lt;/math&gt;, where &lt;math&gt;p&lt;/math&gt; is the number of equations. Including energy balances means adding equations to the system, which results in a higher level of redundancy (provided that enough measurements are available, or equivalently, not too many variables are unmeasured).

===Gross error remediation===
[[image:scheme reconciliation.jpg|thumb|350px|The workflow of an advanced data validation and reconciliation process.]]
Gross errors are measurement systematic errors that may [[bias]] the reconciliation results. Therefore it is important to identify and eliminate these gross errors from the reconciliation process. After the reconciliation [[statistical tests]] can be applied that indicate whether or not a gross error does exist somewhere in the set of measurements. These techniques of gross error remediation are based on two concepts:
* gross error elimination
* gross error relaxation.
Gross error elimination determines one measurement that is biased by a systematic error and discards this measurement from the data set. The determination of the measurement to be discarded is based on different kinds of penalty terms that express how much the measured values deviate from the reconciled values. Once the gross errors are detected they are discarded from the measurements and the reconciliation can be done without these faulty measurements that spoil the reconciliation process. If needed, the elimination is repeated until no gross error exists in the set of measurements.

Gross error relaxation targets at relaxing the estimate for the uncertainty of suspicious measurements so that the reconciled value is in the 95% confidence interval. Relaxation typically finds application when it is not possible to determine which measurement around one unit is responsible for the gross error (equivalence of gross errors). Then measurement uncertainties of the measurements involved are increased.

It is important to note that the remediation of gross errors reduces the quality of the reconciliation, either the redundancy decreases (elimination) or the uncertainty of the measured data increases (relaxation). Therefore it can only be applied when the initial level of redundancy is high enough to ensure that the data reconciliation can still be done (see Section 2,&lt;ref name="vdi" /&gt;).

===Workflow===
Advanced DVR solutions offer an integration of the techniques mentioned above:
# data acquisition from data historian, data base or manual inputs
# data validation and filtering of raw measurements
# data reconciliation of filtered measurements
# result verification
#* range check
#* gross error remediation (and go back to step 3)
# result storage (raw measurements together with reconciled values)
The result of an advanced DVR procedure is a coherent set of validated and reconciled process data.

==Applications==
DVR finds application mainly in industry sectors where either measurements are not accurate or even non-existing, like for example in the [[upstream (fossil-fuel industry)|upstream sector]] where [[flow measurement|flow meters]] are difficult or expensive to position (see &lt;ref&gt;P. Delava, E. Maréchal, B. Vrielynck, B. Kalitventzeff (1999), ''Modelling of a Crude Oil Distillation Unit in Term of Data Reconciliation with ASTM or TBP Curves as Direct Input – Application : Crude Oil Preheating Train'', Proceedings of ESCAPE-9 conference, Budapest, May 31-June 2, 1999, supplementary volume, p. 17-20.&lt;/ref&gt;); or where accurate data is of high importance, for example for security reasons in [[nuclear power plants]] (see &lt;ref&gt;M. Langenstein, J. Jansky, B. Laipple (2004), ''Finding Megawatts in nuclear power plants with process data validation'', Proceedings of ICONE12, Arlington, USA, April 25–29, 2004.&lt;/ref&gt;). Another field of application is [[Performance test (assessment)|performance and process monitoring]] (see &lt;ref&gt;Th. Amand, G. Heyen, B. Kalitventzeff, ''Plant Monitoring and Fault Detection: Synergy between Data Reconciliation and Principal Component Analysis'', Comp. and Chem, Eng. 25, p. 501-507, 2001.&lt;/ref&gt;) in oil refining or in the chemical industry.

As DVR enables to calculate estimates even for unmeasured variables in a reliable way, the German Engineering Society (VDI Gesellschaft Energie und Umwelt) has accepted the technology of DVR as a means to replace expensive sensors in the nuclear power industry (see VDI norm 2048,&lt;ref name="vdi" /&gt;).

==See also==
* [[Process simulation]]
* [[Pinch analysis]]
* [[Industrial processes]]
* [[Chemical engineering]]

==References==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes --&gt;
{{Reflist}}

* Alexander, Dave, Tannar, Dave &amp; Wasik, Larry "Mill Information System uses Dynamic Data Reconciliation for Accurate Energy Accounting" TAPPI Fall Conference 2007.[http://www.tappi.org/Downloads/Conference-Papers/2007/07EPE/07epe87.aspx]
* Rankin, J. &amp; Wasik, L. "Dynamic Data Reconciliation of Batch Pulping Processes (for On-Line Prediction)" PAPTAC Spring Conference 2009.
* S. Narasimhan, C. Jordache, ''Data reconciliation and gross error detection: an intelligent use of process data'', Golf Publishing Company, Houston, 2000.
* V. Veverka, F. Madron, 'Material and Energy Balancing in the Process Industries'', Elsevier Science BV, Amsterdam, 1997.
* J. Romagnoli, M.C. Sanchez, ''Data processing and reconciliation for chemical process operations'', Academic Press, 2000.


{{DEFAULTSORT:Data Validation And Reconciliation}}
[[Category:Data management]]</text>
      <sha1>twch10bd30efz88548j53hih0yl6m8c</sha1>
    </revision>
  </page>
  <page>
    <title>Storage area network</title>
    <ns>0</ns>
    <id>20444608</id>
    <revision>
      <id>753620028</id>
      <parentid>753516739</parentid>
      <timestamp>2016-12-08T06:57:07Z</timestamp>
      <contributor>
        <username>GünniX</username>
        <id>237572</id>
      </contributor>
      <comment>Undid revision 753516739 by [[Special:Contributions/203.123.36.183|203.123.36.183]] ([[User talk:203.123.36.183|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16606" xml:space="preserve">{{Distinguish|Network-attached storage}}
{{Use dmy dates|date=February 2013}}
{{Area networks}}

A '''storage area network''' ('''SAN''') &lt;ref&gt;{{cite web |url=http://cctvinstitute.co.uk/storage-area-network/|title=Storage Area Network by Noor Ul Mushtaq }}&lt;/ref&gt; is a network which provides access to consolidated, [[Block device|block level data storage]]. SANs are primarily used to enhance storage devices, such as [[disk array]]s, [[tape library|tape libraries]], and [[optical jukebox]]es, accessible to [[Server (computing)|server]]s so that the devices appear to the [[operating system]] as [[Direct-attached storage|locally attached devices]]. A SAN typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.

A SAN does not provide file abstraction, only block-level operations. However, [[file systems]] built on top of SANs do provide file-level access, and are known as [[shared-disk file system]]s.

== Storage ==
{{Refimprove section|date=February 2014}}
Historically, [[data centers]] first created "islands" of [[SCSI]] [[disk array]]s as [[direct-attached storage]] (DAS), each dedicated to an application, often visible as a number of "virtual hard drives" addressed as [[Logical Unit Number]]s (LUNs).&lt;ref&gt;{{cite web |url=http://www.novell.com/documentation/oes/stor_ovw/?page=/documentation/oes/stor_ovw/data/ami6rr0.html |title=Novel Doc: OES 1 - Direct Attached Storage Solutions}}&lt;/ref&gt; Essentially, a SAN consolidates such storage islands together using a high-speed network.

Operating systems maintain their own [[file system]]s on their own dedicated, non-shared LUNs, as though they were local to themselves. If multiple systems were simply to attempt to share a LUN, these would interfere with each other and quickly corrupt the data. Any planned sharing of data on different computers within a LUN requires software, such as [[SAN file system]]s or [[clustered computing]].

Despite such issues, SANs help to increase storage capacity utilization, since multiple servers consolidate their private storage space onto the disk arrays.
Common uses of a SAN include provision of transactionally accessed data that require high-speed [[block device|block-level access]] to the hard drives such as email servers, databases, and high usage file servers.

===SAN compared to NAS===
[[Network-attached storage]] (NAS) was designed independently of SAN systems. In both a NAS and SAN, the various computers in a network, such as individual users' desktop computers and dedicated servers running applications ("[[application server]]s"), can share a more centralized collection of storage devices via a network connection such as a [[local area network]] (LAN).

Concentrating the storage on one or more NAS servers or in a SAN instead of placing storage devices on each application server allows application server configurations to be optimized for running their applications instead of also storing all the related data and moves the storage management task to the NAS or SAN system. Both NAS and SAN have the potential to reduce the amount of excess storage that must be purchased and provisioned as spare space. In a DAS-only architecture, each computer must be provisioned with enough excess storage to ensure that the computer does not run out of space at an untimely moment. In a DAS architecture the spare storage on one computer cannot be utilized by another. With a NAS or SAN architecture, where storage is shared across the needs of multiple computers, one normally provisions a pool of shared spare storage that will serve the peak needs of the connected computers, which typically is less than the total amount of spare storage that would be needed if individual storage devices were dedicated to each computer.

In a NAS the storage devices are directly connected to a file server that makes the storage available at a file-level to the other computers. In a SAN, the storage is made available at a lower "block-level", leaving file system concerns to the "client" side. SAN protocols include [[Fibre Channel]], [[iSCSI]], [[ATA over Ethernet]] (AoE) and [[HyperSCSI]]. One way to loosely conceptualize the difference between a NAS and a SAN is that NAS appears to the client OS (operating system) as a file server (the client can map network drives to shares on that server) whereas a disk available through a SAN still appears to the client OS as a disk, visible in disk and volume management utilities (along with client's local disks), and available to be formatted with a file system and mounted.

One drawback to both the NAS and SAN architecture is that the connection between the various CPUs and the storage units are no longer dedicated high-speed busses tailored to the needs of storage access. Instead the CPUs use the LAN to communicate, potentially creating bandwidth as well as performance bottlenecks. Additional data security considerations are also required for NAS and SAN setups, as information is being transmitted via a network that potentially includes design flaws, security exploits and other vulnerabilities that may not exist in a DAS setup.

While it is possible to use the NAS or SAN approach to eliminate all storage at user or application computers, typically those computers still have some local Direct Attached Storage for the operating system, various program files and related temporary files used for a variety of purposes, including [[cache (computing)|caching]] content locally.

To understand their differences, a comparison of SAN, DAS and NAS architectures may be helpful.&lt;ref name="eval"&gt;{{cite web |title= Storage Architectures: DAS, SAN, NAS, iSCSI SAN |work= Marketing web site |url=  http://www.evaluatorgroup.com/document/storage-architectures/ |publisher= Evaluator Group |archivedate= September 17, 2016 |archiveurl= https://web.archive.org/web/20160917144751/http://www.evaluatorgroup.com/document/storage-architectures/ |accessdate= November 10, 2016 }}&lt;/ref&gt;

===SAN-NAS hybrid===
[[Image:Compingles3.png|right|thumb|260px|Hybrid using SAN, [[Direct-attached storage|DAS]] and NAS technologies.]]
Despite their differences, SAN and NAS are not mutually exclusive, and may be combined as a SAN-NAS hybrid, offering both file-level protocols (NAS) and block-level protocols (SAN) from the same system. An example of this is [[Openfiler]], a free software product running on Linux-based systems. A shared disk file system can also be run on top of a SAN to provide filesystem services.

== Benefits ==
Sharing storage usually simplifies storage administration and adds flexibility since cables and storage devices do not have to be physically moved to shift storage from one server to another.

Other benefits include the ability to allow servers to boot from the SAN itself. This allows for a quick and easy replacement of faulty servers since the SAN can be reconfigured so that a replacement server can use the [[Logical Unit Number|LUN]] of the faulty server. While this area of technology is still new, many view it as being the future of the enterprise datacenter.&lt;ref&gt;{{cite web | title=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | url=http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx | work=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | date=31 October 2008 | accessdate=2010-01-28}}&lt;/ref&gt;

SANs also tend to enable more effective [[disaster recovery]] processes. A SAN could span a distant location containing a secondary storage array. This enables [[storage replication]] either implemented by [[disk array controller]]s, by server software, or by specialized SAN devices. Since IP [[Wide area network|WAN]]s are often the least costly method of long-distance transport, the [[Fibre Channel over IP]] (FCIP) and iSCSI protocols have been developed to allow SAN extension over IP networks. The traditional physical SCSI layer could support only a few meters of distance - not nearly enough to ensure business continuance in a disaster.

The economic consolidation of disk arrays has accelerated the advancement of several features including I/O caching, [[Snapshot (computer storage)|snapshotting]], and volume cloning ([[Business Continuance Volumes]] or BCVs).

==Network types==
Most storage networks use the [[SCSI]] protocol for communication between servers and disk drive devices. A mapping layer to other protocols is used to form a network:

* [[ATA over Ethernet|ATA over Ethernet (AoE)]], mapping of [[AT Attachment|ATA]] over [[Ethernet]]
* [[Fibre Channel Protocol]] (FCP), the most prominent one, is a mapping of SCSI over [[Fibre Channel]]
* [[Fibre Channel over Ethernet]] (FCoE)
* [[ESCON]] over Fibre Channel ([[FICON]]), used by [[mainframe computer]]s
* [[HyperSCSI]], mapping of SCSI over Ethernet
* [[iFCP]]&lt;ref&gt;{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=IPstorage |title=TechEncyclopedia: IP Storage |accessdate=2007-12-09}}&lt;/ref&gt; or [[SANoIP]]&lt;ref&gt;{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=SANoIP |title=TechEncyclopedia: SANoIP |accessdate=2007-12-09}}&lt;/ref&gt; mapping of FCP over IP
* [[iSCSI]], mapping of SCSI over [[TCP/IP]]
* [[iSCSI Extensions for RDMA]] (iSER), mapping of iSCSI over [[InfiniBand]]

Storage networks may also be built using [[Serial attached SCSI|SAS]] and [[Serial ATA|SATA]] technologies. SAS evolved from SCSI direct-attached storage. SATA evolved from [[Parallel ATA|IDE]] direct-attached storage. SAS and SATA devices can be networked using [[Serial attached SCSI#SAS expanders|SAS Expanders]].

Examples of stacked protocols using SCSI:

{| class="wikitable" style="text-align:center"
| colspan="5" | Applications
|-
| colspan="5" | [[SCSI]] Layer
|-
| rowspan="4" | [[Fibre Channel Protocol|FCP]]
| rowspan="3" | [[Fibre Channel Protocol|FCP]]
| [[Fibre Channel Protocol|FCP]]
| [[Fibre Channel Protocol|FCP]]
| rowspan="2" | [[iSCSI]]
|-
| [[Fibre Channel over IP|FCIP]]
| [[Internet Fibre Channel Protocol|iFCP]]
|-
| colspan="3" | [[Internet Protocol|TCP]]
|-
| [[Fibre Channel over Ethernet|FCoE]]
| colspan="3" | [[Internet Protocol|IP]]
|-
| [[Fibre Channel|FC]]
| colspan="4" | [[Ethernet]]
|}

== SAN infrastructure ==
[[Image:ML-QLOGICNFCCONN.JPG|thumb| [[Qlogic]] SAN-[[Fibre Channel switch|switch]] with optical [[Fibre Channel]] [[Electrical connector|connectors]] installed.]]
SANs often use a [[Fibre Channel fabric]] topology, an infrastructure specially designed to handle storage communications. It provides faster and more reliable access than higher-level protocols used in [[Network-attached storage|NAS]]. A fabric is similar in concept to a [[network segment]] in a local area network. A typical Fibre Channel SAN fabric is made up of a number of [[Fibre Channel switch]]es.

Many SAN equipment vendors also offer some form of Fibre Channel routing, and these can allow data to cross between different fabrics without merging them. These offerings use proprietary protocol elements, and the top-level architectures being promoted are radically different.
For example, they might map Fibre Channel traffic over IP or over [[Synchronous optical networking|SONET/SDH]].

== Compatibility ==
One of the early problems with Fibre Channel SANs was that the switches and other hardware from different manufacturers were not compatible. Although the basic storage protocol FCP was standard, some of the higher-level functions did not interoperate well. Similarly, many host operating systems would react badly to other operating systems sharing the same fabric.{{Citation needed|date=October 2010}}.

== In media and entertainment ==
[[Video editing]] systems require very high data transfer rates and very low latency.
SANs in media and entertainment are often referred to as serverless due to the nature of the configuration which places the video workflow (ingest, editing, playout) desktop clients directly on the SAN rather than attaching to servers. Control of data flow is managed by a distributed file system such as StorNext by Quantum.&lt;ref&gt;{{cite web|url=http://www.quantum.com/products/software/stornext/index.aspx |title=StorNext Storage Manager - High-speed file sharing, Data Management and Digital Archiving Software |publisher=Quantum.com |date= |accessdate=2013-07-08}}&lt;/ref&gt;

Per-node bandwidth usage control, sometimes referred to as [[quality of service]] (QoS), is especially important in video editing as it ensures fair and prioritized bandwidth usage across the network.

==Storage virtualization==
{{main|Storage virtualization}}
[[Storage virtualization]] is the process of abstracting logical storage from physical storage. The physical storage resources are aggregated into storage pools, from which the logical storage is created. It presents to the user a logical space for data storage and transparently handles the process of mapping it to the physical location, a concept called [[location transparency]]. This is implemented in modern disk arrays, often using vendor proprietary technology. However, the goal of storage virtualization is to group multiple disk arrays from different vendors, scattered over a network, into a single storage device.  The single storage device can then be managed uniformly. {{Citation needed|date=September 2011}}

==Quality of service==
SAN Storage QoS enables the desired storage performance to be calculated and maintained for network customers accessing the device.
Some factors that affect SAN QoS are:

*[[Bandwidth (computing)|Bandwidth]] – The rate of data throughput available on the system.
*[[Latency (engineering)|Latency]] – The time delay for a read/write operation to execute.
*Queue depth – The number of outstanding operations waiting to execute to the underlying disks (traditional or [[solid-date drive]]s).

QoS can be impacted in a SAN storage system by unexpected increase in data traffic (usage spike) from one network user that can cause performance to decrease for other users on the same network. This can be known as the “noisy neighbor effect.” When QoS services are enabled in a SAN storage system, the “noisy neighbor effect” can be prevented and network storage performance can be accurately predicted.

Using SAN storage QoS is in contrast to using disk over-provisioning in a SAN environment. Over-provisioning can be used to provide additional capacity to compensate for peak network traffic loads. However, where network loads are not predictable, over-provisioning can eventually cause all bandwidth to be fully consumed and latency to increase significantly resulting in SAN performance degradation.

== See also ==
* [[ATA over Ethernet]] (AoE)
* [[Direct-attached storage]] (DAS)
* [[Disk array]]
* [[Fibre Channel]]
* [[Fibre Channel over Ethernet]]
* [[File Area Network]]
* [[Host Bus Adapter]] (HBA)
* [[iSCSI]]
* [[iSCSI Extensions for RDMA]]
* [[List of networked storage hardware platforms]]
* [[List of storage area network management systems]]
* [[Massive array of idle disks]] (MAID)
* [[Network-attached storage]] (NAS)
* [[Redundant array of independent disks]] (RAID)
* [[SCSI RDMA Protocol]] (SRP)
* [[Storage Management Initiative – Specification]] — (SMI-S)
* [[Storage hypervisor]]
* [[Storage Resource Management]] (SRM)
* [[Storage virtualization]]
* [[System area network]]

==References==
{{More footnotes|date=June 2008}}
&lt;references/&gt;

==External links==
&lt;!-- ATTENTION! Please do not add links without discussion and consensus on the talk page. Undiscussed links will be removed. --&gt;
* [https://www.redbooks.ibm.com/redbooks/pdfs/sg245470.pdf Introduction to Storage Area Networks Exhaustive Introduction into SAN, [[IBM Redbooks|IBM Redbook]]]
* [http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx SAN vs. DAS: A Cost Analysis of Storage in the Enterprise]
* [http://searchstorage.techtarget.co.uk/generic/0,295582,sid181_gci1516893,00.html SAS and SATA, solid-state storage lower data center power consumption]
* [https://www.youtube.com/playlist?list=PLivYD7W2z2HMGGRIwRoRcqLL4HMpR1dIe SAN NAS Videos]
* [http://www.storageareanetworkinfo.blogspot.com.ar/ Storage Area Network Info]

&lt;!--Interwikies--&gt;

{{Authority control}}

{{DEFAULTSORT:Storage Area Network}}
[[Category:Data management]]
[[Category:Telecommunications engineering]]
[[Category:Storage area networks| ]]</text>
      <sha1>mrvn4qp149psc8frps2dbd1v11ihsm3</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic integration</title>
    <ns>0</ns>
    <id>4381551</id>
    <revision>
      <id>757142095</id>
      <parentid>733703850</parentid>
      <timestamp>2016-12-29T01:17:14Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Applications and Methods */ cap, punct.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3407" xml:space="preserve">{{Unreferenced|date=October 2013}}
'''Semantic integration''' is the process of interrelating information from diverse sources, for example calendars and to do lists, email archives, presence information (physical, psychological, and social), documents of all sorts, contacts (including [[social graph]]s), search results, and advertising and marketing relevance derived from them. In this regard, [[semantics]] focuses on the organization of and action upon [[information]] by acting as an intermediary between heterogeneous data sources, which may conflict not only by structure but also context or value.

==Applications and methods==

In [[enterprise application integration]] (EAI), semantic integration can facilitate or even automate the communication between computer systems using [[metadata publishing]]. Metadata publishing potentially offers the ability to automatically link [[ontology (computer science)|ontologies]]. One approach to (semi-)automated ontology mapping requires the definition of a semantic distance or its inverse, [[semantic similarity]] and appropriate rules. Other approaches include so-called ''lexical methods'', as well as methodologies that rely on exploiting the structures of the ontologies.  For explicitly stating similarity/equality, there exist special properties or relationships in most ontology languages. [[Web Ontology Language|OWL]], for example has "owl:equivalentClass", "owl:equivalentProperty" and "owl:sameAs".

Eventually system designs may see the advent of composable architectures where published semantic-based interfaces are joined together to enable new and meaningful capabilities{{Citation needed|date=February 2014}}. These could predominately be described by means of design-time declarative specifications, that could ultimately be rendered and executed at run-time{{Citation needed|date=February 2014}}.

Semantic integration can also be used to facilitate design-time activities of interface design and mapping. In this model, semantics are only explicitly applied to design and the run-time systems work at the [[syntax]] level{{Citation needed|date=February 2014}}. This  "early semantic binding" approach can improve overall system performance while retaining the benefits of semantic driven design{{Citation needed|date=February 2014}}.

==Examples==

The [[Pacific Symposium on Biocomputing]] has been a venue for the popularization of the ontology mapping task in the biomedical domain, and a number of papers on the subject can be found in its proceedings.

==See also==
* [[Data integration]]
* [[Dataspaces]]
* [[Enterprise integration]]
* [[Ontology-based data integration]]
* [[Ontology matching]]
* [[Semantic heterogeneity]]
* [[Semantic translation]]
* [[Semantic unification]]

== References ==
{{Reflist|2}}

==External links==
*[https://web.archive.org/web/20070811204850/http://zapthink.com/report.html?id=ZapFlash-08082003 Semantic Integration: Loosely Coupling the Meaning of Data]
*[http://drops.dagstuhl.de/opus/volltexte/2005/40/ Ontology Mapping: The State of the Art] (2005 paper)
*[http://arxiv.org/ftp/arxiv/papers/0901/0901.4934.pdf 2010 paper by Carl Hewitt]
*[http://wwwhome.portavita.nl/~yeb/ooi.pdf OpenCyc to Oracle Interface]

{{Semantic Web}}

{{DEFAULTSORT:Semantic Integration}}
[[Category:Ontology (information science)]]
[[Category:Data management]]
[[Category:Semantics]]
[[Category:Bioinformatics]]</text>
      <sha1>adkrua5pgsihmag7zx6uj04dzj2krac</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft Query</title>
    <ns>0</ns>
    <id>684176</id>
    <revision>
      <id>750706969</id>
      <parentid>750706811</parentid>
      <timestamp>2016-11-21T10:50:33Z</timestamp>
      <contributor>
        <ip>62.252.8.106</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1541" xml:space="preserve">{{Refimprove|date=December 2009}}

{{wikibooks|Structured Query Language}}
{{wikibooks|SQL dialects reference}}

'''Microsoft Query''' is a visual method of creating [[database query|database queries]] using examples based on a text string, the name of a [[document]] or a list of documents. The QBE system converts the user input into a formal database query using [[SQL|Structured Query Language]] (SQL) on the backend, allowing the user to perform powerful searches without having to explicitly compose them in SQL, and without even needing to know SQL. It is derived from Moshé M. Zloof's original [[Query by Example]] (QBE) implemented in the mid-1970s at [[IBM]]'s Research Centre in [[Yorktown, New York]].&lt;ref&gt;Zloof, M. M., [http://dx.doi.org/10.1147/sj.164.0324 Query-by-Example: A data base language]&lt;/ref&gt;

In the context of [[Microsoft Access]], QBE is used for introducing students to database querying, and as a user-friendly [[database management system]] for small businesses.

[[Microsoft Excel]] allows results of QBE queries to be embedded in spreadsheets.&lt;ref&gt;[https://support.office.com/en-us/article/Use-Microsoft-Query-to-retrieve-external-data-42a2ea18-44d9-40b3-9c38-4c62f252da2e Use Microsoft Query to retrieve external data]&lt;/ref&gt;

==See also==
*[[Query by Example]]
*[[Microsoft Access]]
*[[Microsoft SQL Server]]

==References==
{{Reflist}}

{{DEFAULTSORT:Microsoft Query By Example}}
[[Category:Data management]]
[[Category:Microsoft database software]]


{{Microsoft-software-stub}}
{{database-software-stub}}</text>
      <sha1>hk44rthjmzd9x7mdmuvmip02wmbxias</sha1>
    </revision>
  </page>
  <page>
    <title>Signed overpunch</title>
    <ns>0</ns>
    <id>16284001</id>
    <revision>
      <id>688112677</id>
      <parentid>662925342</parentid>
      <timestamp>2015-10-29T18:43:29Z</timestamp>
      <contributor>
        <username>SoledadKabocha</username>
        <id>16861812</id>
      </contributor>
      <minor />
      <comment>/* top */ fix anchor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1553" xml:space="preserve">{{Refimprove|date=March 2008}}
A '''signed overpunch''' is a code used to store the [[Sign (mathematics)|sign]] of a number by changing the last digit. It is used in [[COBOL]], especially when using [[EBCDIC]]. Its purpose is to save a character that would otherwise be used by the sign digit.&lt;ref name="EncycArk"&gt;{{Cite web|url=http://www.3480-3590-data-conversion.com/article-signed-fields.html |title=Tech Talk, COBOL Tutorials, EBCDIC to ASCII Conversion of Signed Fields |accessdate=2008-03-15}}&lt;/ref&gt;  The code is derived from the [[punched card#IBM 80-column punched card formats and character codes|Hollerith Punched Card Code]], where both a digit and a sign can be entered in the same card column.

==The codes==
{| class="wikitable" style="text-align:center"
! Code !! Digit !! Sign
|-
| } || 0 || &amp;minus;
|-
| J || 1 || &amp;minus;
|-
| K || 2 || &amp;minus;
|-
| L || 3 || &amp;minus;
|-
| M || 4 || &amp;minus;
|-
| N || 5 || &amp;minus;
|-
| O || 6 || &amp;minus;
|-
| P || 7 || &amp;minus;
|-
| Q || 8 || &amp;minus;
|-
| R || 9 || &amp;minus;
|-
| { || 0 || +
|-
| A || 1 || +
|-
| B || 2 || +
|-
| C || 3 || +
|-
| D || 4 || +
|-
| E || 5 || +
|-
| F || 6 || +
|-
| G || 7 || +
|-
| H || 8 || +
|-
| I || 9 || +
|}

==Examples==
10} is -100&lt;BR&gt;
45A is 451

Decimal points are usually implied and not explicitly stated in the text. Using numbers with two decimal digits:

1000} is -100.00

==References==
{{Reflist}}

{{DEFAULTSORT:Signed Overpunch}}
[[Category:Computer programming]]
[[Category:Punched card]]
[[Category:Data management]]
[[Category:History of software]]</text>
      <sha1>9alf6rgcmb2q45vkbbggl49bj4i8jik</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise manufacturing intelligence</title>
    <ns>0</ns>
    <id>10612614</id>
    <revision>
      <id>681177748</id>
      <parentid>567398012</parentid>
      <timestamp>2015-09-15T17:07:15Z</timestamp>
      <contributor>
        <ip>50.177.205.50</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1731" xml:space="preserve">{{unreferenced|date=February 2013}}
'''Enterprise manufacturing intelligence (EMI),''' or simply manufacturing intelligence (MI), is a term which applies to software used to bring a corporation's manufacturing-related data together from many sources for the purposes of reporting, analysis, visual summaries, and passing data between enterprise-level and plant-floor systems. As data is combined from multiple sources, it can be given a new structure or context that will help users find what they need regardless of where it came from.  The primary goal is to turn large amounts of manufacturing data into real knowledge and drive business results based on that knowledge.{{reflist}}

== Core functions ==

[[AMR Research]] has identified five core functions every Enterprise Manufacturing Intelligence application should possess:

* '''Aggregation:''' Making available data from many sources, most often databases.
* '''Contextualization:''' Providing a structure, or model, for the data that will help users find what they need.  Usually a folder tree utilizing a hierarchy such as the [[ISA-95]] standard.
* '''Analysis:''' Enabling users to analyze data across sources and especially across production sites.  This often includes the ability for true ''ad hoc'' reporting.
* '''Visualization:''' Providing tools to create visual summaries of the data to alert decision makers and call attention to the most important information of the moment.  The most common visualization tool is the [[Dashboard (business)|dashboard.]]
* '''Propagation:''' Automating the transfer of data from the plant-floor up to enterprise-level systems or vice versa.

{{DEFAULTSORT:Enterprise Manufacturing Intelligence}}
[[Category:Data management]]</text>
      <sha1>cxmso4m7611javhijtave4kis8thei5</sha1>
    </revision>
  </page>
  <page>
    <title>Learning object</title>
    <ns>0</ns>
    <id>18126</id>
    <revision>
      <id>744464451</id>
      <parentid>736540516</parentid>
      <timestamp>2016-10-15T11:27:59Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12467" xml:space="preserve">A '''learning object''' is "a collection of content items, practice items, and assessment items that are combined based on a single learning objective".&lt;ref&gt;{{citation|last=Cisco Systems|title=Reusable information object strategy|url=http://www.cisco.com/warp/public/779/ibs/solutions/learning/whitepapers/el_cisco_rio.pdf}}&lt;/ref&gt;  The term is credited to Wayne Hodgins, and dates from a working group in 1994 bearing the name.&lt;ref&gt;{{citation|last=Gerard|first=R.W.|title="Shaping the mind: Computers in education", In N. A. Sciences, Applied Science and Technological Progress|url=https://books.google.com/books?id=BTcrAAAAYAAJ|year=1967}}&lt;/ref&gt; The concept encompassed by 'Learning Objects' is known by numerous other terms, including: content objects, chunks, educational objects, information objects, intelligent objects, knowledge bits, knowledge objects, learning components, media objects, reusable curriculum components, nuggets, reusable information objects, reusable learning objects, testable reusable units of cognition, training components, and units of learning.

The core idea of the use of learning objects is characterized by the following: discoverability, reusability, and interoperability. To support discoverability, learning objects are described by Learning Object Metadata, formalized as IEEE 1484.12 [[Learning object metadata]].&lt;ref&gt;[http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf IEEE 1484.12 Learning Object Metadata]&lt;/ref&gt; To support reusability, the IMS Consortium proposed a series of specifications such as the IMS [[Content package]]. And to support interoperability, the U.S. military's [[Advanced Distributed Learning]] organization created the [[Sharable Content Object Reference Model]].&lt;ref&gt;[http://legacy.adlnet.gov/Technologies/scorm/SCORMSDocuments/2004%204th%20Edition/Overview.aspx SCORM 2004 4th Edition Version 1.1 Overview]&lt;/ref&gt; Learning objects were designed in order to reduce the cost of learning, standardize learning content, and to enable the use and reuse of learning content by learning management systems.&lt;ref&gt;[http://www.irrodl.org/index.php/irrodl/article/view/32/378|Stephen Downes|Learning Objects: Resources For Distance Education Worldwide|The International Review of Research in Open and Distributed Learning|Volume 2 Number 1 2001|Athabasca University Press]&lt;/ref&gt;

==Definitions==
The [[Institute of Electrical and Electronics Engineers]] (IEEE) defines a learning object as "any entity, digital or non-digital, that may be used for learning, education or training".&lt;ref&gt;{{Harvnb|Learning Technology Standards Committee|2002|p=45}}&lt;/ref&gt;

Chiappe defined Learning Objects as: "A digital self-contained and reusable entity, with a clear educational purpose, with at least three internal and editable components: content, learning activities and elements of context. The learning objects must have an external structure of information to facilitate their identification, storage and retrieval: the metadata."&lt;ref&gt;{{Harvnb|Chiappe|Segovia|Rincon|2007|p=8}}.&lt;/ref&gt;

The following definitions focus on the relation between learning object and digital media.  RLO-CETL, a British inter-university Learning Objects Center, defines "reusable learning objects" as "web-based interactive chunks of e-learning designed to explain a stand-alone learning objective".&lt;ref&gt;{{citation|chapter= Learning Objects|url=http://www.rlo-cetl.ac.uk/joomla/index.php?option=com_content&amp;task=view&amp;id=235&amp;Itemid=28|title=RLO-CETL: Reusable Learning Objects|accessdate=2008-04-29}}.&lt;/ref&gt; Daniel Rehak and Robin Mason define it as "a digitized entity which can be used, reused or referenced during technology supported learning".&lt;ref&gt;http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf&lt;/ref&gt;&lt;ref&gt;{{Harvnb|Rehak|Mason|2003|p=}}&lt;/ref&gt;

Adapting a definition from the Wisconsin Online Resource Center, Robert J. Beck suggests that learning objects have the following key characteristics:

* Learning objects are a new way of thinking about learning content. Traditionally, content comes in a several hour chunk.  Learning objects are much smaller units of learning, typically ranging from 2 minutes to 15 minutes.
* Are self-contained – each learning object can be taken independently
* Are reusable – a single learning object may be used in multiple contexts for multiple purposes
* Can be aggregated – learning objects can be grouped into larger collections of content, including traditional course structures
* Are tagged with metadata – every learning object has descriptive information allowing it to be easily found by a search&lt;ref name="beck"&gt;{{citation|last=Beck|first=Robert J.|chapter=What Are Learning Objects?|url=http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 | title=Learning Objects|publisher=Center for International Education, University of Wisconsin-Milwaukee|accessdate=2008-04-29}}&lt;/ref&gt;

== Components ==
The following is a list of some of the types of information that may be included in a learning object and its metadata:
* General Course Descriptive Data, including: course identifiers, language of content (English, Spanish, etc.), subject area (Maths, Reading, etc.), descriptive text, descriptive keywords
* Life Cycle, including: version, status
* Instructional Content, including: text, web pages, images, sound, video
* Glossary of Terms, including: terms, definition, acronyms
* Quizzes and Assessments, including: questions, answers
* Rights, including: cost, copyrights, restrictions on Use
* Relationships to Other Courses, including prerequisite courses
* Educational Level, including: grade level, age range, typical learning time, and difficulty. [IEEE 1484.12.1:2002]
*Typology as defined by Churchill (2007): presentation, practice, simulation, conceptual models, information, and contextual representation &lt;ref name="Churchill"&gt;Churchill, D. (2007). Towards a useful classification of learning objects. ''Educational Technology Research &amp; Development, 55(5)'', 479-497.&lt;/ref&gt;

==Metadata==
One of the key issues in using learning objects is their identification by search engines or content management systems.{{Citation needed|date=April 2008}}  This is usually facilitated by assigning descriptive [[learning object metadata]]. Just as a book in a library has a record in the [[card catalog]], learning objects must also be tagged with metadata.  The most important pieces of metadata typically associated with a learning object include:
# '''objective:''' The educational objective the learning object is instructing
# '''prerequisites:''' The list of skills (typically represented as objectives) which the learner must know before viewing the learning object
# '''topic:''' Typically represented in a taxonomy, the topic the learning object is instructing
# '''interactivity:''' The [[Interaction Model]] of the learning object.
# '''technology requirements:''' The required system requirements to view the learning object.

==Mutability==

A mutated learning object is, according to Michael Shaw, a learning object that has been "re-purposed and/or re-engineered, changed or simply re-used in some way different from its original intended design". Shaw also introduces the term "contextual learning object", to describe a learning object that has been "designed to have specific meaning and purpose to an intended learner".&lt;ref&gt;{{Harvnb|Shaw|2003}}&lt;/ref&gt;

==Portability==
Before any institution invests a great deal of time and energy into building high-quality e-learning content (which can cost over $10,000 per classroom hour),&lt;ref&gt;Rumble, Greville. 2001. The Cost and Costing of Networked Learning. Journal of Asynchronous Learning Networks, Volume 5, Issue 2.&lt;/ref&gt; it needs to consider how this content can be easily loaded into a [[Learning Management System]]. It is possible for example, to package learning objects with [[SCORM]] specification and load it in [[Moodle]] Learning Management System or [[Desire2Learn]] Learning Environment.

If all of the properties of a course can be precisely defined in a common format, the content can be serialized into a standard format such as [[XML]] and loaded into other systems.  When it is considered that some e-learning courses need to include video, mathematical equations using [[MathML]], chemistry equations using [[Chemical Markup Language|CML]] and other complex structures, the issues become very complex, especially if the systems needs to understand and validate each structure and then place it correctly in a database.{{Citation needed|date=April 2008}}

==Criticism==
In 2001, David Wiley criticized learning object theory in his paper, [https://web.archive.org/web/20041019162710/http:/rclt.usu.edu/whitepapers/paradox.html The Reusability Paradox] which is [http://www.darcynorman.net/2003/08/21/addressing-the-reusability-paradox/ summarized by D'Arcy Norman] as, ''If a learning object is useful in a particular context, by definition it is not reusable in a different context. If a learning object is reusable in many contexts, it isn’t particularly useful in any.'' 
In [http://www.learningspaces.org/papers/objections.html Three Objections to Learning Objects and E-learning Standards], Norm Friesen, Canada Research Chair in E-Learning Practices at Thompson Rivers University, points out that the word ''neutrality'' in itself implies ''a state or position that is antithetical ... to pedagogy and teaching.''

== See also ==
* [[Intelligent tutoring system]]
* [[North Carolina Learning Object Repository (NCLOR)]]
* [[Serious games]]

==References==
{{reflist|30em}}

==Further reading==
*{{citation|last=Beck|first=Robert J.|title="What Are Learning Objects?", Learning Objects, Center for International Education, University of Wisconsin-Milwaukee, |url= http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 |year= 2009|accessdate= 2009-10-23}}.
*{{citation|last=Learning Technology Standards Committee|title=Draft Standard for Learning Object Metadata. IEEE Standard 1484.12.1|place=New York|publisher=Institute of Electrical and Electronics Engineers|year=2002|url=http://ltsc.ieee.org/wg12/files/LOM_1484_12_1_v1_Final_Draft.pdf| format=PDF| accessdate=2008-04-29}}.
*{{citation|last1=Rehak |first1=Daniel R.|first2=Robin |last2=Mason|chapter=Engaging with the Learning Object Economy|editor-first=Allison|editor-last=Littlejohn|title=Reusing Online Resources: A Sustainable Approach to E-Learning|place= London|publisher= Kogan Page| year=2003| pages=22–30| isbn=978-0-7494-3949-1}}.
*{{citation|last=Shaw|first=Michael|chapter=(Contextual and Mutated) Learning Objects in the Context of Design, Learning and (Re)Use| url=http://www.shawmultimedia.com/edtech_oct_03.html|title=Teaching and Learning with Technology|date=October 2003|accessdate=2008-04-29}}
*{{citation|first1=Andrés Chiappe |last1=Laverde |first2=Yasbley Segovia |last2=Cifuentes |first3=Helda Yadira Rincón |last3=Rodríguez|chapter=Toward an instructional design model based on learning objects|editor-first=Springer|editor-last=Boston|title=Educational Technology Research and Development |place=Boston|year= 2007|pages=671–81|publisher=Springer US |doi=10.1007/s11423-007-9059-0 |issue=6 |volume=55 |issn=1042-1629|id=(Print) {{ISSN|1556-6501}} (Online) |url= http://www.springerlink.com/content/u84w63873vq77h2h/?p=41be7fbeef9648ee9b554f1835112005&amp;pi=6|accessdate=2008-08-21}} Spanish Draf: [http://andreschiappe.blogspot.com/2007/09/que-es-un-objeto-de-aprendizaje-what-is.html ''Blog de Andrés Chiappe - Objetos de Aprendizaje''].
*{{citation|last=Northrup|first=Pamela|title=Learning Objects for Instruction: Design and Evaluation |place=USA|publisher=Information Science Publishing |year=2007| format=Book}}.
*{{citation|last1=Hunt|first1=John P.|last2=Bernard|first2=Robert|title="An XML-based information architecture for learning content", IBM developerWorks, |url= http://www.ibm.com/developerworks/xml/library/x-dita9a |year= 2005|accessdate= 2005-08-05}}.
*Churchill, D. (2007). Towards a useful classification of learning objects.  ''Educational Technology Research &amp; Development, 55(5)'', 479-497.
Innayah: Creating An Audio Script with Learning Object, unpublished, 2013.

==External links==
* The [http://www4.uwm.edu/cie/learning_objects.cfm?gid=55 Learning Objects] at Milwaukee's Center for International Education.

{{DEFAULTSORT:Learning Object}}
[[Category:Data management]]
[[Category:Educational materials]]
[[Category:Educational technology]]</text>
      <sha1>f4m0pq449v401dcwkl251j3i06otqkt</sha1>
    </revision>
  </page>
  <page>
    <title>Category:NoSQL</title>
    <ns>14</ns>
    <id>29590205</id>
    <revision>
      <id>727132764</id>
      <parentid>546038600</parentid>
      <timestamp>2016-06-26T22:26:27Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="104" xml:space="preserve">{{Cat main|NoSQL}}

[[Category:Databases]] 
[[Category:Data management]]
[[Category:Structured storage]]</text>
      <sha1>rs6jlybdre5hu6v3i3ejr76k9xe4iwt</sha1>
    </revision>
  </page>
  <page>
    <title>Database schema</title>
    <ns>0</ns>
    <id>345937</id>
    <revision>
      <id>762638767</id>
      <parentid>762216814</parentid>
      <timestamp>2017-01-29T23:07:02Z</timestamp>
      <contributor>
        <ip>70.184.214.35</ip>
      </contributor>
      <comment>Alphabetized the categories.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9494" xml:space="preserve">A '''database schema''' ({{IPAc-en|ˈ|s|k|i|.|m|ə}} {{respell|SKEE|mə}}) of a [[database system]] is its structure described in a [[formal language]] supported by the [[database management system]] (DBMS). The term "schema" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of [[relational databases]]). The formal definition of a [[database]] schema is a set of formulas (sentences) called [[integrity constraints]] imposed on a database.{{citation needed|date=January 2016}} These integrity constraints ensure compatibility between parts of the schema. All constraints are expressible in the same language. A database can be considered a structure in realization of the [[database language]].&lt;ref name="source1" /&gt; The states of a created [[conceptual schema]] are transformed into an [[Explicit and implicit methods|explicit mapping]], the database schema. This describes how real-world entities are modeled in the database.

"A database schema specifies, based on the [[database administrator]]'s knowledge of possible applications, the facts that can enter the database, or those of interest to the possible [[end-user]]s."&lt;ref name="source3"/&gt; The notion of a database schema plays the same role as the notion of theory in [[predicate calculus]]. A model of this "theory" closely corresponds to a database, which can be seen at any instant of time as a [[mathematical object]]. Thus a schema can contain formulas representing [[Data integrity#Types of integrity constraints|integrity constraints]] specifically for an application and the constraints specifically for a type of database, all expressed in the same database language.&lt;ref name="source1" /&gt; In a [[relational database]], the schema defines the [[Table (database)|tables]], [[Field (computer science)|fields]], [[Relational model|relationship]]s, [[View (database)|view]]s, [[Index (database)|index]]es, [[Software package (installation)|package]]s, [[stored procedure|procedure]]s, [[subroutine|function]]s, [[Queue (data structure)|queue]]s, [[Database trigger|trigger]]s, [[Data type|type]]s, [[sequence]]s, [[materialized view]]s, [[Synonym (database)|synonym]]s, [[database link]]s, [[Directory (file systems)|directories]], [[XML schema]]s, and other elements.

A database generally stores its schema in a [[data dictionary]]. Although a schema is defined in text database language, the term is often used to refer to a graphical depiction of the database structure. In other words, schema is the structure of the database that defines the objects in the database.

In an [[Oracle Database]] system, the term "schema" has a slightly different [[connotation]].

==Ideal requirements for schema integration==
{{See also|Database normalization}}

The requirements listed below influence the detailed structure of schemas that are produced. Certain applications will not require that all of these conditions are met, but these four requirements are the most ideal.

; Overlap preservation
: Each of the overlapping elements specified in the input mapping is also in a database schema relation.&lt;ref name="source2" /&gt;

; Extended overlap preservation
: Source-specific elements that are associated with a source’s overlapping elements are passed through to the database schema.&lt;ref name="source2" /&gt;

; Normalization
: Independent entities and relationships in the source data should not be grouped together in the same relation in the database schema. In particular, source specific schema elements should not be grouped with overlapping schema elements, if the grouping co-locates independent entities or relationships.&lt;ref name="source2" /&gt;

; Minimality
: If any elements of the database schema are dropped then the database schema is not ideal.&lt;ref name="source2" /&gt;

==Example of two schema integrations==
Suppose we want a mediated (database) schema to integrate two travel databases, Go-travel and Ok-travel.

'''&lt;code&gt;Go-travel&lt;/code&gt;''' has two relations:
&lt;syntaxhighlight lang="text"&gt;
Go-flight(f-num, time, meal(yes/no))
Go-price(f-num, date, price)
&lt;/syntaxhighlight&gt;
(&lt;code&gt;f-num&lt;/code&gt; being the flight number)

'''&lt;code&gt;Ok-travel&lt;/code&gt;''' has just one relation:
&lt;syntaxhighlight lang="text"&gt;
Ok-flight(f-num, date, time, price, nonstop(yes/no))
&lt;/syntaxhighlight&gt;

The overlapping information in Ok-travel’s and Go-travel’s schemas could be represented in a mediated schema:&lt;ref name="source2" /&gt;
&lt;syntaxhighlight lang="text"&gt;
Flight(f-num, date, time, price)
&lt;/syntaxhighlight&gt;

== Oracle database specificity ==
In the context of [[Oracle database]]s, a '''schema object''' is a logical [[Database storage structures|data storage structure]].&lt;ref&gt;
{{cite book
|first1= Lance |last1= Ashdown
|first2= Tom  |last2= Kyte
|others= ''et al''.
|title= Oracle Database Concepts 11g Release 2 (11.2)
|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111
|accessdate= 2010-04-14 |date=February 2010
|publisher= Oracle Corporation
|quote= A database schema is a logical container for data structures, called schema objects. Examples of schema objects are tables and indexes. 
}}
&lt;/ref&gt;

An Oracle database associates a separate schema with each database '''user'''.&lt;ref&gt;
{{cite book
|title= Oracle Database Concepts 10g Release 2 (10.2)Part Number B14220-02 
|url= http://docs.oracle.com/cd/B19306_01/server.102/b14220/schema.htm
|accessdate= 2012-11-26
|quote= A schema is a collection of logical structures of data, or schema objects. A schema is owned by a database user and has the same name as that user. Each user owns a single schema. Schema objects can be created and manipulated with SQL. 
}}&lt;/ref&gt;
A schema comprises a collection of schema objects. Examples of schema objects include:

* [[Table (database)|tables]]
* [[View (database)|views]]
* [[sequence]]s
* [[Synonym (database)|synonyms]]
* [[Index (database)|indexes]]
* clusters
* database links
* [[Snapshot (computer storage)|snapshot]]s
* [[stored procedure|procedure]]s
* functions
* packages

On the other hand, non-schema objects may include:&lt;ref&gt;{{cite book
|first1= Lance
|last1= Ashdown
|author1-link=
|first2= Tom
|last2= Kyte
|author2-link=
|others= et al.
|title= Oracle Database Concepts 11g Release 2 (11.2)
|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111
|accessdate= 2010-04-14
|date=February 2010
|publisher= Oracle Corporation
|location=
|isbn=
|quote= Other types of objects are also stored in the database and can be created and manipulated with SQL statements but are not contained in a schema. These objects include database users, roles, contexts, and directory objects.
}}&lt;/ref&gt;

* users
* roles
* contexts
* directory objects

Schema objects do not have a one-to-one correspondence to physical files on disk that store their information. However, [[Oracle database]]s store schema objects logically within a [[tablespace]] of the database. The data of each object is physically contained in one or more of the tablespace's [[datafile]]s. For some objects (such as tables, indexes, and clusters) a [[database administrator]] can specify how much disk space the Oracle [[RDBMS]] allocates for the object within the tablespace's datafiles.

There is no necessary relationship between schemas and tablespaces: a tablespace can contain objects from different schemas, and the objects for a single schema can reside in different tablespaces.

== See also ==
{{too many see alsos|date=July 2013}}
* [[Core architecture data model]] (CADM)
* [[Data definition language]] (DDL)
* [[Database design]]
* [[Data dictionary]]
* [[Data element]]
* [[Data modeling]]
* [[Data mapping]]
* [[Database integrity]]
* [[Entity–relationship model]]
* [[Knowledge representation and reasoning]]
* [[Object-role modeling]]
* [[Relational algebra]]
* [[Schema matching]]
* [[Three schema approach]]

==References==
{{reflist|refs=
&lt;ref name="source1"&gt;{{cite journal |last=Rybinski |first=H. |year=1987|title=On First-Order-Logic Databases |journal=ACM Transactions on Database Systems |volume=12 |issue=3 |pages=325–349 |doi= 10.1145/27629.27630}}&lt;/ref&gt;
&lt;ref name="source2"&gt;{{cite journal |last= Pottinger |first=P. |last2=Berstein |first2=P. |year=2008 |title= Schema merging and mapping creation for relational sources |journal= Proceedings of the 11th international conference on Extending database technology: Advances in database technology (EDBT '08) |publisher=ACM |location= New York, NY |pages=73–84 |doi= 10.1145/1353343.1353357}}&lt;/ref&gt;
&lt;ref name="source3"&gt;{{cite journal |last= Imielinski |first=T. |last2=Lipski |first2=W. |year=1982 |title=A systematic approach to relational database theory |journal= Proceedings of the 1982 ACM SIGMOD international conference on Management of data (SIGMOD '82) |publisher=ACM |location= New York, NY |pages=8–14 |DOI= 10.1145/582353.582356}}&lt;/ref&gt;
}}

== External links ==
* [https://weblogs.asp.net/scottgu/Tip_2F00_Trick_3A00_-Online-Database-Schema-Samples-Library Tip/Trick: Online Database Schema Samples Library]
* [http://web.archive.org/web/20081217074637/http://msdn.microsoft.com/en-us/library/bb187299%28SQL.80%29.aspx Database Schema Samples]
* [http://web.archive.org/web/20080828210315/http://ciobriefings.com/Publications/WhitePapers/DesigningtheStarSchemaDatabase/tabid/101/Default.aspx Designing the Star Schema Database]

{{DEFAULTSORT:Database Schema}}
[[Category:Data management]]
[[Category:Data modeling]]</text>
      <sha1>866h7bmchhywb54g5v0vjrf7o8g4s4o</sha1>
    </revision>
  </page>
  <page>
    <title>Dashboard (business)</title>
    <ns>0</ns>
    <id>4166591</id>
    <revision>
      <id>745181429</id>
      <parentid>744632161</parentid>
      <timestamp>2016-10-19T18:44:04Z</timestamp>
      <contributor>
        <ip>180.242.28.51</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10076" xml:space="preserve">[[File:3 Dashboards.JPG|thumb|200px|Business Dashboards.]]

'''Dashboards''' often provide at-a-glance views of KPIs ([[key performance indicators]])  relevant to a particular objective or business process (e.g. [[sales]], [[marketing]], [[human resources]], or [[Production (economics)|production]]).&lt;ref&gt;Michael Alexander and John Walkenbach, ''Excel Dashboards and Reports'' (Wiley, 2010)&lt;/ref&gt; In real-world terms, "dashboard" is another name for "progress report" or "report."

Often, the "dashboard" is displayed on a web page that is linked to a database which allows the report to be constantly updated. For example, a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured, or number of failed quality inspections per hour. Similarly, a human resources dashboard may show numbers related to staff recruitment, retention and composition, for example number of open positions, or average days or cost per recruitment.&lt;ref name="Briggs"&gt;{{cite web|url=http://www.targetdashboard.com/site/Dashboard-Best-Practice/Management-Report-and-Dashboard-best-practice-index.aspx|title=Management Reports &amp; Dashboard Best Practice|last=Briggs|first=Jonathan|publisher=Target Dashboard|accessdate=18 February 2013}}&lt;/ref&gt;

The term dashboard originates from the [[automobile]] [[dashboard]] where drivers   monitor the major functions at a glance via the instrument cluster. 

==Benefits==
Digital dashboards allow managers to monitor the contribution of the various departments in their organization. To gauge exactly how well an organization is performing overall, digital dashboards allow you to capture and report specific data points from each department within the organization, thus providing a "snapshot" of performance.

Benefits of using digital dashboards include:&lt;ref name="Briggs" /&gt;
*Visual presentation of performance measures
*Ability to identify and correct negative trends
*Measure efficiencies/inefficiencies
*Ability to generate detailed reports showing new trends
*Ability to make more informed decisions based on collected [[business intelligence]]
*Align strategies and organizational goals
*Saves time compared to running multiple reports
*Gain total visibility of all systems instantly
*Quick identification of data outliers and correlations

==Classification==
Dashboards can be broken down according to role and are either [[strategic]], analytical, operational, or informational.&lt;ref&gt;Steven Few, ''Information Dashboard Design: The Effective Visual Communication of Data'' (O'Reilly, 2006)&lt;/ref&gt; Strategic dashboards support managers at any level in an organization, and provide the quick overview that decision makers need to monitor the health and opportunities of the business. Dashboards of this type focus on high level measures of performance, and forecasts. Strategic dashboards benefit from static snapshots of data (daily, weekly, monthly, and quarterly) that are not constantly changing from one moment to the next. Dashboards for analytical purposes often include more context, comparisons, and history, along with subtler performance evaluators. Analytical dashboards typically support interactions with the data, such as drilling down into the underlying details. Dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment's notice.

==Types of dashboards==

Digital dashboards may be laid out to track the flows inherent in the business processes that they monitor. Graphically, users may see the high-level processes and then [[data drilling|drill down]] into low level data. This level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives.

Three main types of digital dashboard dominate the market today: stand alone software applications, web-browser based applications, and desktop applications also known as [[desktop widgets]]. The last are driven by a [[Software widget|widget engine]].

Specialized dashboards may track all corporate functions. Examples include [[human resources]], [[Recruitment|recruiting]], [[sales]], [[Business operations|operations]], [[security]], [[information technology]], [[project management]], [[customer relationship management]] and many more departmental dashboards. For a smaller organization like a startup a compact startup scorecard dashboard tracks important activities across lot of domains ranging from social media to sales.{{cn|date=September 2016}}

Digital dashboard projects involve business units as the driver and the information technology department as the enabler. The success of digital dashboard projects often depends on the [[measurement|metrics]] that were chosen for monitoring. [[Key performance indicator]]s, [[balanced scorecard]]s, and sales performance figures are some of the content appropriate on business dashboards.

==Dashboards and scoreboards==
Balanced Scoreboards and Dashboards have been linked together as if they were interchangeable. However, although both visually display critical information, the difference is in the format: Scoreboards can open the quality of an operation while dashboards provide calculated direction. 
A balanced scoreboard has what they called a "prescriptive" format. It should always contain these components (Active Strategy)...
*Perspectives – groupings of high level strategic areas
*Objectives – verb-noun phrases pulled from a strategy plan
*Measures – also called Metric or Key Performance Indicators (KPIs)
*Spotlight Indicators – red, yellow, or green symbols that provide an at-a-glance view of a measure’s performance.
Each of these sections ensures that a Balanced Scorecard is essentially connected to the businesses critical strategic needs.

The design of a dashboard is more loosely defined.  Dashboards are usually a series of graphics, charts, gauges and other visual indicators that can be monitored and interpreted.  Even when there is a strategic link, on a dashboard, it may not be noticed as such since objectives are not normally present on dashboards.  However, dashboards can be customized to link their graphs and charts to strategic objectives.&lt;ref&gt;ZSL Inc., ''Dashboards Vs Scorecards – An Insight'' ZSL Inc. (2006)&lt;/ref&gt;

==Design==
Digital dashboard technology is available "out-of-the-box" from many software providers. Some companies however continue to do in-house development and maintenance of dashboard applications. For example, [[GE Aviation]] has developed a proprietary software/portal called "Digital Cockpit" to monitor the trends in aircraft spare parts business.

A good information design will clearly communicate key information to users and makes supporting information easily accessible.&lt;ref&gt;Stacey Barr, ''7 Small Business Dashboard Design Dos and Don'ts'' (Barr, 2010)&lt;/ref&gt;

==Assessing the quality of dashboards==
There are four key elements to a good dashboard:.&lt;ref&gt;Victoria Hetherington, ''Dashboard Demystified: What is a Dashboard?'' (Hetherington, 2009)&lt;/ref&gt;
# Simple, communicates easily
# Minimum distractions...it could cause confusion
# Supports organized business with meaning and useful data
# Applies human visual perception to visual presentation of information

==History==

The idea of digital dashboards followed the study of [[decision support system]]s in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s in the form of [[Executive Information Systems]] (EISs). Due to problems primarily with data refreshing and handling, it was soon realized that the approach wasn’t practical as information was often incomplete, unreliable, and spread across too many disparate sources.&lt;ref&gt;Steven Few, ''Information Dashboard Design: The Effective Visual Communication of Data'' (O'Reilly, 2006)&lt;/ref&gt; Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and [[online analytical processing]] (OLAP) allowed dashboards to function adequately.{{fact|date=August 2014}} Despite the availability of enabling technologies, the dashboard use didn't become popular until later in that decade, with the rise of [[key performance indicators]] (KPIs), and the introduction of Robert S. Kaplan and David P. Norton's [[Balanced Scorecard]].&lt;ref&gt;[[Wayne W. Eckerson]], ''Performance Dashboards: Measuring, Monitoring, and Managing Your Business'' (Wiley , 2010)&lt;/ref&gt; In the late 1990s, [[Microsoft]] promoted a concept known as the [[Digital Nervous System]] and "digital dashboards" were described as being one leg of that concept.&lt;ref&gt;{{cite web | url=http://www.kmworld.com/Articles/News/Breaking-News/Microsoft-refines-Digital-Dashboard-concept--12189.aspx | title=Microsoft refines Digital Dashboard concept | accessdate=2009-06-09}}&lt;/ref&gt; Today, the use of dashboards forms an important part of Business Performance Management (BPM).

==See also==
* [[Business activity monitoring]]
* [[Complex event processing]]
* [[Corporate performance management]]
* [[Data presentation architecture]]
* [[Enterprise manufacturing intelligence]]
* [[Event stream processing]]
* [[Infographic|Information graphics]]
* [[Information design]]
* [[Scientific visualization]]

==References==
{{reflist}}

==Further reading==
* {{cite book
  |title=Information Dashboard Design
  |last=Few   |first=Stephen
  |publisher=O'Reilly
  |isbn=978-0-596-10016-2
  |date=2006
}}

* {{cite book
  |title=Performance Dashboards: Measuring, Monitoring, and Managing Your Business
  |last=Eckerson|first=Wayne W |author-link=
  |publisher=John Wiley &amp; Sons
  |isbn=978-0-471-77863-9
  |date=2006
}}

{{Data warehouse}}

{{DEFAULTSORT:Dashboard (Business)}}
{{Use dmy dates|date=April 2011}}
[[Category:Business terms]]
[[Category:Computing terminology]]
[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Business software]]
[[Category:Information systems]]</text>
      <sha1>41gzuccbc1s7trmkoddo1ciw267ng96</sha1>
    </revision>
  </page>
  <page>
    <title>Bitmap index</title>
    <ns>0</ns>
    <id>2017214</id>
    <revision>
      <id>750350473</id>
      <parentid>747607300</parentid>
      <timestamp>2016-11-19T05:01:29Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21156" xml:space="preserve">A '''bitmap index''' is a special kind of [[Index (database)|database index]] that uses [[Bit array|bitmap]]s.

Bitmap indexes have traditionally been considered to work well for ''low-cardinality columns'', which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use [[bit array]]s (commonly called bitmaps) and answer queries by performing [[bitwise operation|bitwise logical operation]]s on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional [[B-tree]] indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for [[online transaction processing]] applications.

Some researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.&lt;ref name="sharma"&gt;[http://www.oracle.com/technetwork/articles/sharma-indexes-093638.html Bitmap Index vs. B-tree Index: Which and When?], Vivek Sharma, Oracle Technical Network.&lt;/ref&gt;

Bitmap indexes are also useful in [[data warehousing]] applications for joining a large [[fact table]] to smaller [[dimension table]]s such as those arranged in a [[star schema]].

Bitmap based representation can also be used for representing a data structure which is labeled and directed attributed multigraph, used for queries in [[graph databases]].&lt;code&gt;[http://www.researchgate.net/publication/236593640_Efficient_graph_management_based_on_bitmap_indices Efficient graph management based on bitmap indices]&lt;/code&gt; article shows how bitmap index representation can be used to manage large dataset(billions of data points) and answer queries related to graph efficiently.

==Example==
Continuing the internet access example, a bitmap index may be logically viewed as follows:
{| class="wikitable" style="text-align:center; float:left"
|-
!rowspan=2| Identifier
!rowspan=2| HasInternet
!colspan=2| Bitmaps
|-
!Y !! N
|-
|1 || Yes || 1|| 0
|-
|2 || No || 0 || 1
|-
|3 || No || 0 || 1
|-
|4 || Unspecified || 0 || 0
|-
|5 || Yes || 1 || 0
|}

On the left, [[Identifier]] refers to the unique number assigned to each resident, HasInternet is the data to be indexed, the content of the bitmap index is shown as two columns under the heading ''bitmaps''. Each column in the left illustration is a ''bitmap'' in the bitmap index. In this case, there are two such bitmaps, one for "has internet" ''Yes'' and one for "has internet" ''No''. It is easy to see that each bit in bitmap ''Y'' shows whether a particular row refers to a person who has internet access. This is the simplest form of bitmap index. Most columns will have more distinct values. For example, the sales amount is likely to have a much larger number of distinct values. Variations on the bitmap index can effectively index this data as well. We briefly review three such variations.

Note: Many of the references cited here are reviewed at ([[#JohnWu2007|John Wu (2007)]]).&lt;ref&gt;{{cite web|ref=JohnWu2007|author=John Wu |year=2007 |title=Annotated References on Bitmap Index |url=http://www.cs.umn.edu/~kewu/annotated.html}}&lt;/ref&gt; For those who might be interested in experimenting with some of the ideas mentioned here, many of them are implemented in open source software such as FastBit,&lt;ref&gt;[http://codeforge.lbl.gov/projects/fastbit/ FastBit]&lt;/ref&gt; the Lemur Bitmap Index C++ Library,&lt;ref&gt;[https://code.google.com/p/lemurbitmapindex/ Lemur Bitmap Index C++ Library]&lt;/ref&gt; the Roaring Bitmap Java library,&lt;ref&gt;[http://roaringbitmap.org/ Roaring bitmaps]&lt;/ref&gt;  the [[Apache Hive]] Data Warehouse system and [[LucidDB]].

{{Clear}}

==Compression==
Software can [[data compression|compress]] each bitmap in a bitmap index to save spaces.  There has been considerable amount of work on this subject.&lt;ref&gt;{{cite book |author=T. Johnson |editor1=Malcolm P. Atkinson |editor2=[[Maria Orłowska|Maria E. Orlowska]] |editor3=Patrick Valduriez |editor4=Stanley B. Zdonik |editor5=Michael L. Brodie | title = VLDB'99, Proceedings of 25th International Conference on Very Large Data Bases, September 7–10, 1999, Edinburgh, Scotland, UK | publisher = Morgan Kaufmann | year = 1999 | isbn = 1-55860-615-7 | chapter =Performance Measurements of Compressed Bitmap Indices | pages=278–89 | url=http://www.vldb.org/conf/1999/P29.pdf }}&lt;/ref&gt;&lt;ref&gt;{{cite web |vauthors=Wu K, Otoo E, Shoshani A | title=On the performance of bitmap indices for high cardinality attributes | date=March 5, 2004 | url=http://www.osti.gov/energycitations/servlets/purl/822860-LOzkmz/native/822860.pdf }}&lt;/ref&gt;
Though there are exceptions such as Roaring bitmaps,&lt;ref name=roaring&gt;{{Cite journal | last1 = Chambi | first1 = S. | last2 = Lemire | first2 = D. | last3 = Kaser | first3 = O. | last4 = Godin | first4 = R.  | title = Better bitmap performance with Roaring bitmaps | doi = 10.1002/spe.2325 | journal = Software: Practice &amp; Experience | volume = 46 | pages = 5 | year = 2016 | pmid =  | pmc = }}&lt;/ref&gt; Bitmap compression algorithms typically employ [[run-length encoding]], such as the Byte-aligned Bitmap Code,&lt;ref&gt;{{US Patent|5363098|Byte aligned data compression}}&lt;/ref&gt; the Word-Aligned Hybrid code,&lt;ref&gt;{{US Patent|6831575|Word aligned bitmap compression method, data structure, and apparatus}}&lt;/ref&gt; the Partitioned Word-Aligned Hybrid (PWAH) compression,&lt;ref&gt;{{cite conference |url=http://dl.acm.org/citation.cfm?doid=1989323.1989419 |title=A memory efficient reachability data structure through bit vector compression | last1=van Schaik | first1=Sebastiaan |last2=de Moor |first2=Oege |year=2011 |publisher=ACM |booktitle=Proceedings of the 2011 international conference on Management of data |pages=913–924 |location=Athens, Greece |doi=10.1145/1989323.1989419 |conference=SIGMOD '11 |isbn=978-1-4503-0661-4 }}&lt;/ref&gt; the Position List Word Aligned Hybrid,&lt;ref name="doi_10.1145/1739041.1739071"&gt;{{cite book | chapter = Position list word aligned hybrid: optimizing space and performance for compressed bitmaps |vauthors=Deliège F, Pedersen TB |editor1=Ioana Manolescu |editor2=Stefano Spaccapietra |editor3=Jens Teubner |editor4=Masaru Kitsuregawa |editor5=Alain Leger |editor6=Felix Naumann |editor7=Anastasia Ailamaki |editor8=Fatma Ozcan | title = EDBT '10, Proceedings of the 13th International Conference on Extending Database Technology | publisher = ACM | location = New York, NY, USA | year = 2010 | pages = 228–39 | isbn = 978-1-60558-945-9 | doi = 10.1145/1739041.1739071 | url = http://alpha.uhasselt.be/icdt/edbticdt2010proc/edbt/papers/p0228-Deliege.pdf }}&lt;/ref&gt; the Compressed Adaptive Index (COMPAX),&lt;ref name="autogenerated1382"&gt;{{cite journal|author1=F. Fusco |author2=M. Stoecklin |author3=M. Vlachos |title=NET-FLi: on-the-fly compression, archiving and indexing of streaming network traffic |date=September 2010 | volume = 3 | issue = 1–2 | pages = 1382–93 | journal=Proc. VLDB Endow | url=http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/I01.pdf }}&lt;/ref&gt; Enhanced Word-Aligned Hybrid (EWAH) &lt;ref name=ewah&gt;{{Cite journal | last1 = Lemire | first1 = D. | last2 = Kaser | first2 = O. | last3 = Aouiche | first3 = K. | title = Sorting improves word-aligned bitmap indexes | doi = 10.1016/j.datak.2009.08.006 | journal = Data &amp; Knowledge Engineering | volume = 69 | pages = 3 | year = 2010 | pmid =  | pmc = }}&lt;/ref&gt; and the COmpressed 'N' Composable Integer SEt.&lt;ref&gt;[http://ricerca.mat.uniroma3.it/users/colanton/concise.html Concise: Compressed 'n' Composable Integer Set] {{webarchive |url=https://web.archive.org/web/20110528033714/http://ricerca.mat.uniroma3.it/users/colanton/concise.html |date=May 28, 2011 }}&lt;/ref&gt;&lt;ref name="doi_10.1016/j.ipl.2010.05.018" /&gt; These compression methods require very little effort to compress and decompress. More importantly, bitmaps compressed with BBC, WAH, COMPAX, PLWAH, EWAH and CONCISE can directly participate in [[bitwise operation]]s without decompression. This gives them considerable advantages over generic compression techniques such as [[LZ77]]. BBC compression and its derivatives are used in a commercial [[database management system]]. BBC is effective in both reducing index sizes and maintaining [[database query|query]] performance. BBC encodes the bitmaps in [[bytes]], while WAH encodes in words, better matching current [[CPU]]s. "On both synthetic data and real application data, the new word aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC."&lt;ref&gt;{{cite book |vauthors=Wu K, Otoo EJ, Shoshani A |editor1=Henrique Paques |editor2=Ling Liu |editor3=David Grossman | chapter =A Performance comparison of bitmap indexes | year=2001 | title = CIKM '01 Proceedings of the tenth international conference on Information and knowledge management | publisher = ACM | location = New York, NY, USA | pages = 559–61 | isbn = 1-58113-436-3 | doi = 10.1145/502585.502689 | url = http://crd.lbl.gov/~kewu/ps/LBNL-48975.pdf }}&lt;/ref&gt; PLWAH bitmaps were reported to take 50% of the storage space consumed by WAH bitmaps and offer up to 20% faster performance on [[logical operation]]s.&lt;ref name="doi_10.1145/1739041.1739071" /&gt; Similar considerations can be done for CONCISE &lt;ref name="doi_10.1016/j.ipl.2010.05.018"&gt;{{cite journal |vauthors=Colantonio A, Di Pietro R | title=Concise: Compressed 'n' Composable Integer Set | journal = Information Processing Letters | volume = 110 | issue = 16 | date = 31 July 2010 | doi = 10.1016/j.ipl.2010.05.018 | url = http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf |pages=644–50 }}&lt;/ref&gt; and Enhanced Word-Aligned Hybrid.&lt;ref name="ewah"/&gt;

The performance of schemes such as BBC, WAH, PLWAH, EWAH, COMPAX and CONCISE is dependent on the order of the rows. A simple lexicographical sort can divide the index size by 9 and make indexes several times faster.&lt;ref&gt;{{cite journal|author1=D. Lemire |author2=O. Kaser |author3=K. Aouiche |title=Sorting improves word-aligned bitmap indexes |journal=Data &amp; Knowledge Engineering | volume=69 | issue=1 |date=January 2010 |arxiv=0901.3751 | doi = 10.1016/j.datak.2009.08.006|pages=3–28 }}&lt;/ref&gt; The larger the table, the more important it is to sort the rows. Reshuffling techniques have also been proposed to achieve the same results of sorting when indexing streaming data.&lt;ref name="autogenerated1382"/&gt;

==Encoding==
Basic bitmap indexes use one bitmap for each distinct value. It is possible to reduce the number of bitmaps used by using a different encoding method.&lt;ref name="autogenerated355"&gt;{{cite book |chapter=Bitmap index design and evaluation |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1998 | title = Proceedings of the 1998 ACM SIGMOD international conference on Management of data (SIGMOD '98) |editor1=Ashutosh Tiwary |editor2=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 355–6 | doi=10.1145/276304.276336 | url = http://www.comp.nus.edu.sg/~chancy/sigmod98.pdf }}&lt;/ref&gt;&lt;ref&gt;{{cite book |chapter=An efficient bitmap encoding scheme for selection queries |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1999 | title = Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD '99) | publisher = ACM | location = New York, NY, USA | pages = 215–26 | doi = 10.1145/304182.304201 | url = http://www.ist.temple.edu/~vucetic/cis616spring2005/papers/P4%20p215-chan.pdf }}&lt;/ref&gt; For example, it is possible to encode C distinct values using log(C) bitmaps with binary encoding.&lt;ref&gt;{{cite journal |author1=P. E. O'Neil  |author2=D. Quass |lastauthoramp=yes | chapter = Improved Query Performance with Variant Indexes | title = Proceedings of the 1997 ACM SIGMOD international conference on Management of data (SIGMOD '97) | year = 1997 |editor1=Joan M. Peckman |editor2=Sudha Ram |editor3=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 38–49| doi=10.1145/253260.253268 }}&lt;/ref&gt;

This reduces the number of bitmaps, further saving space, but to answer any query, most of the bitmaps have to be accessed. This makes it potentially not as effective as scanning a vertical projection of the base data, also known as a [[materialized view]] or projection index. Finding the optimal encoding method that balances (arbitrary) query performance, index size and index maintenance remains a challenge.

Without considering compression, Chan and Ioannidis analyzed a class of multi-component encoding methods and came to the conclusion that two-component encoding sits at the kink of the performance vs. index size curve and therefore represents the best trade-off between index size and query performance.&lt;ref name="autogenerated355"/&gt;

==Binning==
For high-cardinality columns, it is useful to bin the values, where each bin covers multiple values and build the bitmaps to represent the values in each bin. This approach reduces the number of bitmaps used regardless of encoding method.&lt;ref&gt;{{cite book |chapter = Space efficient bitmap indexing| title= Proceedings of the ninth international conference on Information and knowledge management (CIKM '00) | year=2000 | author =N. Koudas | publisher = ACM | location = New York, NY, USA | pages = 194–201 | doi=10.1145/354756.354819 }}&lt;/ref&gt; However, binned indexes can only answer some queries without examining the base data. For example, if a bin covers the range from 0.1 to 0.2, then when the user asks for all values less than 0.15, all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0.15. The process of checking the base data is known as the candidate check. In most cases, the time used by the candidate check is significantly longer than the time needed to work with the bitmap index. Therefore, binned indexes exhibit irregular performance. They can be very fast for some queries, but much slower if the query does not exactly match a bin.

==History==
The concept of bitmap index was first introduced by Professor Israel Spiegler and Rafi Maayan in their research "Storage and Retrieval Considerations of Binary Data Bases", published in 1985.&lt;ref&gt;{{cite journal | title = Storage and retrieval considerations of binary data bases | author1 = Spiegler I | author2 = Maayan R | journal = Information Processing and Management: an International Journal | volume = 21 | issue = 3 | year = 1985 | doi = 10.1016/0306-4573(85)90108-6 | pages = 233–54   }}&lt;/ref&gt; The first commercial database product to implement a bitmap index was Computer Corporation of America's [[Model 204]]. [[Patrick O'Neil]] published a paper about this implementation in 1987.&lt;ref name="model204"&gt;{{cite conference | last = O'Neil | first = Patrick | title = Model 204 Architecture and Performance | booktitle = Proceedings of the 2nd International Workshop on High Performance Transaction Systems | pages = 40–59 | publisher = Springer-Verlag | year = 1987 | location = London, UK | editor = Dieter Gawlick |editor2=Mark N. Haynie |editor3=Andreas Reuter (Eds.) }}&lt;/ref&gt; This implementation is a hybrid between the basic bitmap index (without compression) and the list of Row Identifiers (RID-list). Overall, the index is organized as a [[B+tree]]. When the column cardinality is low, each leaf node of the B-tree would contain long list of RIDs. In this case, it requires less space to represent the RID-lists as bitmaps. Since each bitmap represents one distinct value, this is the basic bitmap index. As the column cardinality increases, each bitmap becomes sparse and it may take more disk space to store the bitmaps than to store the same content as RID-lists. In this case, it switches to use the RID-lists, which makes it a [[B+tree]] index.&lt;ref&gt;{{cite conference | title=Bit-sliced index arithmetic | booktitle= Proceedings of the 2001 ACM SIGMOD international conference on Management of data (SIGMOD '01) | year = 2001 |author1=D. Rinfret, P. O'Neil  |author2=E. O'Neil  |lastauthoramp=yes | editor = Timos Sellis | publisher = ACM | location = New York, NY, USA | pages = 47–57 | doi = 10.1145/375663.375669 }}&lt;/ref&gt;&lt;ref&gt;{{cite conference |author1=E. O'Neil |author2=P. O'Neil |author3=K. Wu | title = Bitmap Index Design Choices and Their Performance Implications | booktitle = 11th International Database Engineering and Applications Symposium (IDEAS 2007) | year = 2007 | pages = 72–84 | url=http://crd.lbl.gov/~kewu/ps/LBNL-62756.pdf | isbn = 0-7695-2947-X | doi = 10.1109/IDEAS.2007.19 }}&lt;/ref&gt;

==In-memory bitmaps==
One of the strongest reasons for using bitmap indexes is that the intermediate results produced from them are also bitmaps and can be efficiently reused in further operations to answer more complex queries. Many programming languages support this as a bit array data structure. For example, Java has the &lt;code&gt;[http://download.oracle.com/javase/6/docs/api/java/util/BitSet.html BitSet]&lt;/code&gt; class.

Some database systems that do not offer persistent bitmap indexes use bitmaps internally to speed up query processing. For example, [[PostgreSQL]] versions 8.1 and later implement a "bitmap index scan" optimization to speed up arbitrarily complex [[logical operation]]s between available indexes on a single table.

For tables with many columns, the total number of distinct indexes to satisfy all possible queries (with equality filtering conditions on either of the fields) grows very fast, being defined by this formula:

:&lt;math&gt; \mathbf{C}_n^\left [ \frac{n}{2} \right ] \equiv \frac{n!}{\left(n-\left [ \frac{n}{2} \right ]\right)! \left [ \frac{n}{2} \right ]!}&lt;/math&gt;.&lt;ref&gt;{{cite web|author=Alex Bolenok|date=2009-05-09|title=Creating indexes|url=http://explainextended.com/2009/05/09/creating-indexes/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Egor Timoshenko|title=On minimal collections of indexes|url=http://explainextended.com/files/index-en.pdf }}&lt;/ref&gt;

A bitmap index scan combines expressions on different indexes, thus requiring only one index per column to support all possible queries on a table.

Applying this access strategy to B-tree indexes can also combine range queries on multiple columns. In this approach, a temporary in-memory bitmap is created with one [[bit]] for each row in the table (1 [[MiB]] can thus store over 8 million entries). Next, the results from each index are combined into the bitmap using [[bitwise operation]]s. After all conditions are evaluated, the bitmap contains a "1" for rows that matched the expression. Finally, the bitmap is traversed and matching rows are retrieved. In addition to efficiently combining indexes, this also improves [[locality of reference]] of table accesses, because all rows are fetched sequentially from the main table.&lt;ref&gt;{{cite web |author=Tom Lane |date=2005-12-26 |title=Re: Bitmap indexes etc. |publisher=PostgreSQL mailing lists |url=http://archives.postgresql.org/pgsql-performance/2005-12/msg00623.php |accessdate=2007-04-06 }}&lt;/ref&gt; The internal bitmap is discarded after the query. If there are too many rows in the table to use 1 bit per row, a "lossy" bitmap is created instead, with a single bit per disk page. In this case, the bitmap is just used to determine which pages to fetch; the filter criteria are then applied to all rows in matching pages.

==References==
;Notes
{{Reflist|30em}}

;Bibliography
*{{Cite journal|last=O'Connell|first=S.|year=2005|title=Advanced Databases Course Notes|location=[[Southampton]]|publisher=[[University of Southampton]]|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}
*{{Cite journal|last1=O'Neil|first1=P.|last2=O'Neil|first2=E.|year=2001|title=Database Principles, Programming, and Performance|location=[[San Francisco]]|publisher=[[Morgan Kaufmann Publishers]]|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}
*{{Cite journal|last1=Zaker|first1=M.|last2=Phon-Amnuaisuk|first2=S.|last3=Haw|first3=S.C.|year=2008|issue=2|volume=2|title=An Adequate Design for Large Data Warehouse Systems: Bitmap index versus B-tree index|journal=[[International Journal of Computers and Communications]]|url=http://www.universitypress.org.uk/journals/cc/cc-21.pdf|accessdate=2010-01-07|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}

{{DEFAULTSORT:Bitmap Index}}
[[Category:Bit data structures|Index]]
[[Category:Data management]]
[[Category:Database index techniques]]</text>
      <sha1>7wvz3zma0weqneiyc50cuqiphi2ghb4</sha1>
    </revision>
  </page>
  <page>
    <title>Very large database</title>
    <ns>0</ns>
    <id>30864622</id>
    <revision>
      <id>639584728</id>
      <parentid>624218067</parentid>
      <timestamp>2014-12-25T14:10:11Z</timestamp>
      <contributor>
        <username>Henrikdv</username>
        <id>3703541</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="731" xml:space="preserve">{{About|Large size databases|International Conference on Very Large Databases|VLDB}}

A '''very large database''', or '''VLDB''', is a database that contains an extremely high number of [[tuple]]s (database rows), or occupies an extremely large physical [[filesystem]] storage space. The most common definition of VLDB is a database that occupies more than 1 [[terabyte]] or contains several billion rows, although naturally this definition changes over time.{{Citation needed|date=March 2013}}

Very large databases are often, but not necessarily, a core component in [[big data]] analysis.

==References==
{{reflist}}

{{Database}}

{{DEFAULTSORT:Very Large Database}}
[[Category:Data management]]
[[Category:Types of databases]]</text>
      <sha1>ol46byttqtrrvj2w9q4o4towimegqjq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data analysis</title>
    <ns>14</ns>
    <id>14482748</id>
    <revision>
      <id>720925070</id>
      <parentid>711443396</parentid>
      <timestamp>2016-05-18T19:52:47Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>moved [[Category:Statistics]] one level down</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="122" xml:space="preserve">{{Commons category|Data analysis}}
{{cat main|Data analysis}}

[[Category:Analysis]]
[[Category:Data management|Analysis]]</text>
      <sha1>7z3xmbvynnpo01zsqbuqodxmv6zwzgk</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data quality</title>
    <ns>14</ns>
    <id>31206312</id>
    <revision>
      <id>547255735</id>
      <parentid>532024561</parentid>
      <timestamp>2013-03-27T13:07:26Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363880]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="195" xml:space="preserve">{{Cat main|Data quality}}
:''See also:'' [[:category:Data security]] ([[data loss]] prevention is in fact an assurance of data quality)


[[Category:Data management|Quality]]
[[Category:Quality]]</text>
      <sha1>08tzbporo203mz2h4zql4uxr5zgrlfh</sha1>
    </revision>
  </page>
  <page>
    <title>State transition network</title>
    <ns>0</ns>
    <id>31261582</id>
    <revision>
      <id>635507085</id>
      <parentid>583937780</parentid>
      <timestamp>2014-11-26T13:04:49Z</timestamp>
      <contributor>
        <username>Brirush</username>
        <id>17368641</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="708" xml:space="preserve">{{unreferenced|date=March 2011}}

A '''state transition network''' is a [[diagram]] that is developed from a set of data and charts the [[data flow|flow of data]] from particular data points (called states or nodes) to the next in a probabilistic manner.

==Use==
State transition networks are used in both [[academic]] and [[industry|industrial]] fields. 

==Examples==
State transition networks are a general construct, with more specific examples being augmented transition networks, recursive transition networks, and augmented recursive networks, among others.
==See also==
* [[State transition system]]
* [[Markov network]]
* [[History monoid]]

==References==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>opltjpqygpr4itqiszrg6cqrpy26tmo</sha1>
    </revision>
  </page>
  <page>
    <title>Log trigger</title>
    <ns>0</ns>
    <id>31397529</id>
    <revision>
      <id>760393642</id>
      <parentid>760393573</parentid>
      <timestamp>2017-01-16T18:42:58Z</timestamp>
      <contributor>
        <ip>187.39.108.160</ip>
      </contributor>
      <comment>/* Alternatives */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16705" xml:space="preserve">In [[relational database]]s, the '''Log trigger''' or '''History trigger''' is a mechanism for automatic recording of information about changes inserting or/and updating or/and deleting [[Row (database)|rows]] in a [[Table (database)|database table]].

It is a particular technique for [[Change data capture|change data capturing]], and in [[data warehousing]] for dealing with [[slowly changing dimension]]s.

== Definition ==

Suppose there is a [[Table (database)|table]] which we want to audit. This [[Table (database)|table]] contains the following [[Column (database)|columns]]:

&lt;code&gt;Column1, Column2, ..., Columnn&lt;/code&gt;

The [[Column (database)|column]] &lt;code&gt;Column1&lt;/code&gt; is assumed to be the [[primary key]].

These [[Column (database)|columns]] are defined to have the following types:

&lt;code&gt;Type1, Type2, ..., Typen&lt;/code&gt;

The '''Log Trigger''' works writing the changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) on the [[Table (database)|table]] in another, '''history table''', defined as following:

&lt;syntaxhighlight lang="sql"&gt;
CREATE TABLE HistoryTable (
   Column1   Type1,
   Column2   Type2,
      :        :
   Columnn   Typen,

   StartDate DATETIME,
   EndDate   DATETIME
)
&lt;/syntaxhighlight&gt;

As shown above, this new [[Table (database)|table]] contains the same [[Column (database)|columns]] as the original [[Table (database)|table]], and additionally two new [[Column (database)|columns]] of type &lt;code&gt;DATETIME&lt;/code&gt;: &lt;code&gt;StartDate&lt;/code&gt; and &lt;code&gt;EndDate&lt;/code&gt;. This is known as [[Tuple-versioning|tuple versioning]]. These two additional [[Column (database)|columns]] define a period of time of "validity" of the data associated with a specified entity (the entity of the [[primary key]]), or in other words, it stores how the data were in the period of time between the &lt;code&gt;StartDate&lt;/code&gt; (included) and &lt;code&gt;EndDate&lt;/code&gt; (not included).

For each entity (distinct [[primary key]]) on the original [[Table (database)|table]], the following structure is created in the history [[Table (database)|table]]. Data is shown as example.

[[File:example log trigger.png|center|example]]

Notice that if they are shown chronologically the &lt;code&gt;EndDate&lt;/code&gt; [[Column (database)|column]] of any [[Row (database)|row]] is exactly the &lt;code&gt;StartDate&lt;/code&gt; of its successor (if any). It does not mean that both [[Row (database)|rows]] are common to that point in time, since -by definition- the value of &lt;code&gt;EndDate&lt;/code&gt; is not included.

There are two variants of the '''Log trigger''', depending how the old values (DELETE, UPDATE) and new values (INSERT, UPDATE) are exposed to the trigger (it is RDBMS dependent):

'''Old and new values as fields of a record data structure'''

&lt;syntaxhighlight lang="sql"&gt;
CREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS
DECLARE @Now DATETIME
SET @Now = GETDATE()

/* deleting section */

UPDATE HistoryTable
   SET EndDate = @Now
 WHERE EndDate IS NULL
   AND Column1 = OLD.Column1

/* inserting section */

INSERT INTO HistoryTable (Column1, Column2, ...,Columnn, StartDate, EndDate) 
VALUES (NEW.Column1, NEW.Column2, ..., NEW.Columnn, @Now, NULL)
&lt;/syntaxhighlight&gt;

'''Old and new values as rows of virtual tables'''

&lt;syntaxhighlight lang="sql"&gt;
CREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS
DECLARE @Now DATETIME
SET @Now = GETDATE()

/* deleting section */

UPDATE HistoryTable
   SET EndDate = @Now
  FROM HistoryTable, DELETED
 WHERE HistoryTable.Column1 = DELETED.Column1
   AND HistoryTable.EndDate IS NULL

/* inserting section */

INSERT INTO HistoryTable
       (Column1, Column2, ..., Columnn, StartDate, EndDate)
SELECT (Column1, Column2, ..., Columnn, @Now, NULL)
  FROM INSERTED
&lt;/syntaxhighlight&gt;

=== Compatibility notes ===

* The function &lt;code&gt;GetDate()&lt;/code&gt; is used to get the system date and time, a specific [[Relational database management system|RDBMS]] could either use another function name, or get this information by another way.
* Several [[Relational database management system|RDBMS]] (DB2, MySQL) do not support that the same trigger can be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]). In such a case a trigger must be created for each operation; For an [[Insert (SQL)|INSERT]] operation only the ''inserting section'' must be specified, for a [[Delete (SQL)|DELETE]] operation only the ''deleting section'' must be specified, and for an [[Update (SQL)|UPDATE]] operation both sections must be present, just as it is shown above (the ''deleting section'' first, then the ''inserting section''), because an [[Update (SQL)|UPDATE]] operation is logically represented as a [[Delete (SQL)|DELETE]] operation followed by an [[Insert (SQL)|INSERT]] operation.
* In the code shown, the record data structure containing the old and new values are called &lt;code&gt;OLD&lt;/code&gt; and &lt;code&gt;NEW&lt;/code&gt;. On a specific [[Relational database management system|RDBMS]] they could have different names.
* In the code shown, the virtual tables are called &lt;code&gt;DELETED&lt;/code&gt; and &lt;code&gt;INSERTED&lt;/code&gt;. On a specific [[Relational database management system|RDBMS]] they could have different names. Another [[Relational database management system|RDBMS]] (DB2) even let the name of these logical tables be specified.
* In the code shown, comments are in C/C++ style, they could not be supported by a specific [[Relational database management system|RDBMS]], or a different syntax should be used.
* Several [[Relational database management system|RDBMS]] require that the body of the trigger is enclosed between &lt;code&gt;BEGIN&lt;/code&gt; and &lt;code&gt;END&lt;/code&gt; keywords.

=== [[Data warehousing]] ===

According with the [[slowly changing dimension]] management methodologies, The '''log trigger''' falls into the following:

* [[Slowly changing dimension#Type 2|Type 2]] ([[Tuple-versioning|tuple versioning]] variant)
* [[Slowly changing dimension#Type 4|Type 4]] (use of history tables)

== Implementation in common [[RDBMS]] ==

=== [[IBM DB2]]&lt;ref&gt;"Database Fundamentals" by Nareej Sharma et al. (First Edition, Copyright IBM Corp. 2010)&lt;/ref&gt; ===

* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.
* The old and new values are exposed as fields of a record data structures. The names of these records can be defined, in this example they are named as &lt;code&gt;O&lt;/code&gt; for old values and &lt;code&gt;N&lt;/code&gt; for new values.

&lt;syntaxhighlight lang="sql"&gt;
-- Trigger for INSERT
CREATE TRIGGER Database.TableInsert AFTER INSERT ON Database.OriginalTable
REFERENCING NEW AS N
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);
END;

-- Trigger for DELETE
CREATE TRIGGER Database.TableDelete AFTER DELETE ON Database.OriginalTable
REFERENCING OLD AS O
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   UPDATE Database.HistoryTable
      SET EndDate = Now
    WHERE Column1 = O.Column1
      AND EndDate IS NULL;
END;

-- Trigger for UPDATE
CREATE TRIGGER Database.TableUpdate AFTER UPDATE ON Database.OriginalTable
REFERENCING NEW AS N OLD AS O
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   UPDATE Database.HistoryTable
      SET EndDate = Now
    WHERE Column1 = O.Column1
      AND EndDate IS NULL;

   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);
END;
&lt;/syntaxhighlight&gt;

=== [[Microsoft SQL Server]]&lt;ref&gt;"Microsoft SQL Server 2008 - Database Development" by Thobias Thernström et al. (Microsoft Press, 2009)&lt;/ref&gt; ===

* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.
* Old and new values as rows of virtual tables named &lt;code&gt;DELETED&lt;/code&gt; and &lt;code&gt;INSERTED&lt;/code&gt;.

&lt;syntaxhighlight lang="sql"&gt;
CREATE TRIGGER TableTrigger ON OriginalTable FOR DELETE, INSERT, UPDATE AS

DECLARE @NOW DATETIME
SET @NOW = CURRENT_TIMESTAMP

UPDATE HistoryTable
   SET EndDate = @now
  FROM HistoryTable, DELETED
 WHERE HistoryTable.ColumnID = DELETED.ColumnID
   AND HistoryTable.EndDate IS NULL

INSERT INTO HistoryTable (ColumnID, Column2, ..., Columnn, StartDate, EndDate)
SELECT ColumnID, Column2, ..., Columnn, @NOW, NULL
  FROM INSERTED
&lt;/syntaxhighlight&gt;

=== [[MySQL]] ===

* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.
* The old and new values are exposed as fields of a record data structures called &lt;code&gt;Old&lt;/code&gt; and &lt;code&gt;New&lt;/code&gt;.

&lt;syntaxhighlight lang="sql"&gt;
DELIMITER $$

/* Trigger  for INSERT */
CREATE TRIGGER HistoryTableInsert AFTER INSERT ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();
    
   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);
END;

/* Trigger for DELETE */
CREATE TRIGGER HistoryTableDelete AFTER DELETE ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();
    
   UPDATE HistoryTable
      SET EndDate = N
    WHERE Column1 = OLD.Column1
      AND EndDate IS NULL;
END;

/* Trigger for UPDATE */
CREATE TRIGGER HistoryTableUpdate AFTER UPDATE ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();

   UPDATE HistoryTable
      SET EndDate = N
    WHERE Column1 = OLD.Column1
      AND EndDate IS NULL;

   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);
END;
&lt;/syntaxhighlight&gt;

=== [[Oracle Database|Oracle]] ===

* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.
* The old and new values are exposed as fields of a record data structures called &lt;code&gt;:OLD&lt;/code&gt; and &lt;code&gt;:NEW&lt;/code&gt;.
* It is necessary to test the nullity of the fields of the &lt;code&gt;:NEW&lt;/code&gt; record that define the [[primary key]] (when a [[Delete (SQL)|DELETE]] operation is performed), in order to avoid the insertion of a new row with null values in all columns.

&lt;syntaxhighlight lang="sql"&gt;
CREATE OR REPLACE TRIGGER TableTrigger
AFTER INSERT OR UPDATE OR DELETE ON OriginalTable
FOR EACH ROW
DECLARE Now TIMESTAMP;
BEGIN
   SELECT CURRENT_TIMESTAMP INTO Now FROM Dual;

   UPDATE HistoryTable
      SET EndDate = Now
    WHERE EndDate IS NULL
      AND Column1 = :OLD.Column1;

   IF :NEW.Column1 IS NOT NULL THEN
      INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate) 
      VALUES (:NEW.Column1, :NEW.Column2, ..., :NEW.Columnn, Now, NULL);
   END IF;
END;
&lt;/syntaxhighlight&gt;

== Historic information ==

Typically, [[Database dump|database backups]] are used to store and retrieve historic information. A [[Database dump|database backup]] is a security mechanism, more than an effective way to retrieve ready-to-use historic information.

A (full) [[Database dump|database backup]] is only a snapshot of the data in specific points of time, so we could know the information of each snapshot, but we can know nothing between them. Information in [[Database dump|database backups]] is discrete in time.

Using the '''log trigger''' the information we can know is not discrete but continuous, we can know the exact state of the information in any point of time, only limited to the granularity of time provided with the &lt;code&gt;DATETIME&lt;/code&gt; data type of the [[Relational database management system|RDBMS]] used.

== Advantages ==

* It is simple.
* It is not a commercial product, it works with available features in common [[Relational database management system|RDBMS]].
* It is automatic, once it is created, it works with no further human intervention.
* It is not required to have good knowledge about the tables of the database, or the data model.
* Changes in current programming are not required.
* Changes in the current [[Table (database)|tables]] are not required, because log data of any [[Table (database)|table]] is stored in a different one.
* It works for both programmed and ad hoc statements.
* Only changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) are registered, so the growing rate of the history tables are proportional to the changes.
* It is not necessary to apply the trigger to all the tables on database, it can be applied to certain [[Table (database)|tables]], or certain [[Column (database)|columns]] of a [[Table (database)|table]].

== Disadvantages ==

* It does not automatically store information about the user producing the changes (information system user, not database user). This information might be provided explicitly. It could be enforced in information systems, but not in ad hoc queries.

== Examples of use ==

=== Getting the current version of a table ===

&lt;syntaxhighlight lang="sql"&gt;
SELECT Column1, Column2, ..., Columnn
  FROM HistoryTable
 WHERE EndDate IS NULL
&lt;/syntaxhighlight&gt;

It should return the same resultset of the whole original [[Table (database)|table]].

=== Getting the version of a table in a certain point of time ===

Suppose the &lt;code&gt;@DATE&lt;/code&gt; variable contains the point or time of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT  Column1, Column2, ..., Columnn
  FROM  HistoryTable
 WHERE  @Date &gt;= StartDate
   AND (@Date &lt; EndDate OR EndDate IS NULL)
&lt;/syntaxhighlight&gt;

=== Getting the information of an entity in a certain point of time ===

Suppose the &lt;code&gt;@DATE&lt;/code&gt; variable contains the point or time of interest, and the &lt;code&gt;@KEY&lt;/code&gt; variable contains the [[primary key]] of the entity of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT  Column1, Column2, ..., Columnn
  FROM  HistoryTable
 WHERE  Column1 = @Key
   AND  @Date &gt;= StartDate
   AND (@Date &lt;  EndDate OR EndDate IS NULL)
&lt;/syntaxhighlight&gt;

=== Getting the history of an entity ===

Suppose the &lt;code&gt;@KEY&lt;/code&gt; variable contains the [[primary key]] of the entity of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT Column1, Column2, ..., Columnn, StartDate, EndDate
  FROM HistoryTable
 WHERE Column1 = @Key
 ORDER BY StartDate
&lt;/syntaxhighlight&gt;

=== Getting when and how an entity was created ===

Suppose the &lt;code&gt;@KEY&lt;/code&gt; variable contains the [[primary key]] of the entity of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT H2.Column1, H2.Column2, ..., H2.Columnn, H2.StartDate
  FROM HistoryTable AS H2 LEFT OUTER JOIN HistoryTable AS H1
    ON H2.Column1 = H1.Column1
   AND H2.Column1 = @Key
   AND H2.StartDate = H1.EndDate
 WHERE H2.EndDate IS NULL
&lt;/syntaxhighlight&gt;

== Immutability of [[primary key]]s ==

Since the trigger requires that [[primary key]] being the same throughout time, it is desirable to either ensure or maximize its immutability, if a [[primary key]] changed its value, the entity it represents would break its own history.

There are several options to achieve or maximize the [[primary key]] immutability:

* Use of a [[Surrogate Key|surrogate key]] as a [[primary key]]. Since there is no reason to change a value with no meaning other than identity and uniqueness, it would never change.
* Use of an immutable [[natural key]] as a [[primary key]]. In a good database design, a [[natural key]] which can change should not be considered as a "real" [[primary key]].
* Use of a mutable [[natural key]] as a [[primary key]] (it is widely discouraged) where changes are propagated in every place where it is a [[foreign key]]. In such a case, the history table should be also affected.

=== Alternatives ===

Sometimes the [[Slowly changing dimension]] is used as a method, this diagram is an example:
[[File:Scd model.png|frame|right|Scd model]]

== See also ==

* [[RDBMS|Relational database]]
* [[Primary key]]
* [[Natural key]]
* [[Surrogate key]]
* [[Change data capture]]
* [[Slowly changing dimension]]
* [[Tuple-versioning|Tuple versioning]]

== Notes ==

The Log trigger was written by [[Laurence Ruiz Ugalde|Laurence R. Ugalde]] to automatically generate history of transactional databases.

==References==
&lt;references /&gt;

{{DEFAULTSORT:Log Trigger}}
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Data warehousing]]</text>
      <sha1>7pj4vao46yek0q2jq42gjf272potvdf</sha1>
    </revision>
  </page>
  <page>
    <title>Data deduplication</title>
    <ns>0</ns>
    <id>17174890</id>
    <revision>
      <id>759377096</id>
      <parentid>756132583</parentid>
      <timestamp>2017-01-10T20:20:41Z</timestamp>
      <contributor>
        <username>Ost316</username>
        <id>6289403</id>
      </contributor>
      <minor />
      <comment>[[WP:AWB]] cleanup, [[WP:AWB/T|typo(s) fixed]]: For example → For example, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20100" xml:space="preserve">{{multiple issues|
{{original research|date=February 2011}}
{{More footnotes|date=September 2009}}
}}

In [[computing]], '''data deduplication''' is a specialized [[data compression]] technique for eliminating duplicate copies of repeating data. Related and somewhat synonymous terms are '''intelligent (data) compression''' and '''[[single-instance storage|single-instance (data) storage]]'''. This technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent. In the deduplication process, unique chunks of data, or byte patterns, are identified and stored during a process of analysis. As the analysis continues, other chunks are compared to the stored copy and whenever a match occurs, the redundant chunk is replaced with a small reference that points to the stored chunk. Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced.&lt;ref&gt;"[http://www.druva.com/blog/2009/01/09/understanding-data-deduplication/ Understanding Data Deduplication]" Druva, 2009. Retrieved 2013-2-13&lt;/ref&gt;

This type of deduplication is different from that performed by standard file-compression tools, such as [[LZ77 and LZ78]]. Whereas these tools identify short repeated substrings inside individual files, the intent of storage-based data deduplication is to inspect large volumes of data and identify large sections – such as entire files or large sections of files – that are identical, in order to store only one copy of it. This copy may be additionally compressed by single-file compression techniques. For example, a typical email system might contain 100 instances of the same 1 MB ([[megabyte]]) file attachment. Each time the [[email]] platform is backed up, all 100 instances of the attachment are saved, requiring 100 MB storage space. With data deduplication, only one instance of the attachment is actually stored; the subsequent instances are referenced back to the saved copy for deduplication ratio of roughly 100 to 1.

==Benefits==
* Storage-based data deduplication reduces the amount of storage needed for a given set of files. It is most effective in applications where many copies of very similar or even identical data are stored on a single disk—a surprisingly common scenario. In the case of data backups, which routinely are performed to protect against data loss, most data in a given backup remain unchanged from the previous backup. Common backup systems try to exploit this by omitting (or [[hard link]]ing) files that haven't changed or storing [[Data differencing|differences]] between files.  Neither approach captures all redundancies, however. Hard-linking does not help with large files that have only changed in small ways, such as an email database;  differences only find redundancies in adjacent versions of a single file (consider a section that was deleted and later added in again, or a logo image included in many documents).
* Network data deduplication is used to reduce the number of bytes that must be transferred between endpoints, which can reduce the amount of bandwidth required. See [[WAN optimization]] for more information.
* Virtual servers benefit from deduplication because it allows nominally separate system files for each virtual server to be coalesced into a single storage space. At the same time, if a given server customizes a file, deduplication will not change the files on the other servers—something that alternatives like hard links or shared disks do not offer.  Backing up or making duplicate copies of virtual environments is similarly improved.

==Deduplication overview==
Deduplication may occur "in-line", as data is flowing, or "post-process" after it has been written.

===Post-process deduplication===
With post-process deduplication, new data is first stored on the storage device and then a process at a later time will [[analysis|analyze]] the data looking for duplication. The benefit is that there is no need to wait for the hash calculations and lookup to be completed before storing the data, thereby ensuring that store performance is not degraded. Implementations offering policy-based operation can give users the ability to defer optimization on "active" files, or to process files based on type and location. One potential drawback is that duplicate data may be unnecessarily stored for a short time, which can be problematic if the system is nearing full capacity.

===In-line deduplication===
Alternatively, deduplication hash calculations can be done in real-time as data enters the target device. If the storage system identifies a block which it has already stored, only a reference to the existing block is stored, rather than the whole new block.

The advantage of in-line deduplication over post-process deduplication is that it requires less storage, since duplicate data is never stored.  On the negative side, it is frequently argued{{by whom|date=August 2016}} that because hash calculations and lookups take so long, [[data ingestion]] can be slower, thereby reducing the backup throughput of the device.  However, certain vendors with in-line deduplication have demonstrated equipment with similar performance to their post-process deduplication counterparts{{according to whom|date=April 2015}}.

Data coming in is stored into "lining space" before it hits real storage blocks. On SSD disks lining space is provided using [[Non-volatile random-access memory|NVRAM]] which is not cost efficient{{according to whom|date=August 2016}}.

Post-process and in-line deduplication methods are often heavily debated.&lt;ref&gt;{{cite web|url=http://www.backupcentral.com/content/view/134/47/ |title=In-line or post-process de-duplication? (updated 6-08) |publisher=Backup Central |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091206035054/http://www.backupcentral.com:80/content/view/134/47 |archivedate=2009-12-06 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://searchdatabackup.techtarget.com/tip/0,289483,sid187_gci1315295,00.html |title=Inline vs. post-processing deduplication appliances |publisher=Searchdatabackup.techtarget.com |date= |accessdate=2009-10-16}}&lt;/ref&gt;

===Data formats===
[[SNIA Dictionary]] identifies two methods:
* content-agnostic data deduplication - a data deduplication method that does not require awareness of specific application data formats. 
* content-aware data deduplication - a data deduplication method that leverages knowledge of specific application data formats.

===Source versus target deduplication===
Another way to classify data deduplication methods is according to where they occur. Deduplication occurring close to where data is created, is often referred to{{according to whom|date=August 2016}} as "source deduplication". When it occurs near where the data is stored, it is commonly called "target deduplication".

* Source deduplication ensures that data on the data source is deduplicated.  This generally takes place directly within a file system.&lt;ref&gt;{{cite web|url=http://www.microsoft.com/windowsserver2008/en/us/WSS08/SIS.aspx |title=Windows Server 2008: Windows Storage Server 2008 |publisher=Microsoft.com |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091004073508/http://www.microsoft.com:80/windowsserver2008/en/us/WSS08/SIS.aspx |archivedate=2009-10-04 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.netapp.com/us/products/platform-os/dedupe.html |title=Products - Platform OS |publisher=NetApp |date= |accessdate=2009-10-16}}&lt;/ref&gt;  The file system will periodically scan new files creating hashes and compare them to hashes of existing files.   When files with same hashes are found then the file copy is removed and the new file points to the old file.  Unlike [[hard links]] however, duplicated files are considered to be separate entities and if one of the duplicated files is later modified, then using a system called [[copy-on-write]] a copy of that file or changed block is created.  The deduplication process is transparent to the users and backup applications.  Backing up a deduplicated file system will often cause duplication to occur resulting in the backups being bigger than the source data.
* Target deduplication is the process of removing duplicates when the data was not generated at that location.  Example of this would be a server connected to a SAN/NAS, The SAN/NAS would be a target for the server (Target deduplication).  The server is not aware of any deduplication, the server is also the point of data generation.

A second example would be backup. If you have a backup system with deduplication. Generally this will be a backup store such as a data repository or a [[virtual tape library]].

===Deduplication methods===
One of the most common forms of data deduplication implementations works by comparing chunks of data to detect duplicates. For that to happen, each chunk of data is assigned an identification, calculated by the software, typically using cryptographic hash functions. In many implementations, the assumption is made that if the identification is identical, the data is identical, even though this cannot be true in all cases due to the [[pigeonhole principle]]; other implementations do not assume that two blocks of data with the same identifier are identical, but actually verify that data with the same identification is identical.&lt;ref&gt;An example of an implementation that checks for identity rather than assuming it is described in [http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PG01&amp;s1=shnelvar&amp;OS=shnelvar&amp;RS=shnelvar "US Patent application # 20090307251"].&lt;/ref&gt; If the software either assumes that a given identification already exists in the deduplication namespace or actually verifies the identity of the two blocks of data, depending on the implementation, then it will replace that duplicate chunk with a link.

Once the data has been deduplicated, upon read back of the file, wherever a link is found, the system simply replaces that link with the referenced data chunk. The deduplication process is intended to be transparent to end users and applications.

Commercial deduplication implementations differ by their chunking methods and architectures.

* Chunking.  In some systems, chunks are defined by physical layer constraints (e.g. 4KB block size in [[Write Anywhere File Layout|WAFL]]). In some systems only complete files are compared, which is called [[single-instance storage]] or SIS. The most intelligent (but CPU intensive) method to chunking is generally considered to be sliding-block. In sliding block, a window is passed along the file stream to seek out more naturally occurring internal file boundaries.
* Client backup deduplication. This is the process where the deduplication hash calculations are initially created on the source (client) machines.  Files that have identical hashes to files already in the target device are not sent, the target device just creates appropriate internal links to reference the duplicated data.  The benefit of this is that it avoids data being unnecessarily sent across the network thereby reducing traffic load.
* Primary storage and secondary storage. By definition, primary storage systems are designed for optimal performance, rather than lowest possible cost.  The design criteria for these systems is to increase performance, at the expense of other considerations.  Moreover, primary storage systems are much less tolerant of any operation that can negatively impact performance.  Also by definition, secondary storage systems contain primarily duplicate, or secondary copies of data.  These copies of data are typically not used for actual production operations and as a result are more tolerant of some performance degradation, in exchange for increased efficiency.

To date, data deduplication has predominantly been used with secondary storage systems.  The reasons for this are two-fold.  First, data deduplication requires overhead to discover and remove the duplicate data.  In primary storage systems, this overhead may impact performance.  The second reason why deduplication is applied to secondary data, is that secondary data tends to have more duplicate data.  Backup application in particular commonly generate significant portions of duplicate data over time.

Data deduplication has been deployed successfully with primary storage in some cases where the system design does not require significant overhead, or impact performance.

==Drawbacks and concerns==
Whenever data is transformed, concerns arise about potential loss of data.  By definition, data deduplication systems store data differently from how it was written.  As a result, users are concerned with the integrity of their data.  The various methods of deduplicating data all employ slightly different techniques.  However, the integrity of the data will ultimately depend upon the design of the deduplicating system, and the quality used to implement the algorithms.  As the technology has matured over the past decade, the integrity of most of the major products has been well proven .{{citation needed|date=November 2012}}

One method for deduplicating data relies on the use of [[cryptographic hash function]]s to identify duplicate segments of data. If two different pieces of information generate the same hash value, this is known as a [[collision (computer science)|collision]].  The probability of a collision depends upon the hash function used, and although the probabilities are small, they are always non zero. Thus, the concern arises that [[data corruption]] can occur if a [[hash collision]] occurs, and additional means of verification are not used to verify whether there is a difference in data, or not. Both in-line and post-process architectures may offer bit-for-bit validation of original data for guaranteed data integrity.&lt;ref&gt;{{citation |url=http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ |title=Data Deduplication - Why, When, Where and How |publisher=Evaluator Group |accessdate=2011-07-05}}&lt;/ref&gt; The hash functions used include standards such as [[SHA-1]], [[SHA-256]] and others.

The computational resource intensity of the process can be a drawback of data deduplication.  However, this is rarely an issue for stand-alone devices or appliances, as the computation is completely offloaded from other systems.  This can be an issue when the deduplication is embedded within devices providing other services. To improve performance, many systems utilize both weak and strong hashes.  Weak hashes are much faster to calculate but there is a greater risk of a hash collision.  Systems that utilize weak hashes will subsequently calculate a strong hash and will use it as the determining factor to whether it is actually the same data or not. Note that the system overhead associated with calculating and looking up hash values is primarily a function of the deduplication workflow. The reconstitution of files does not require this processing and any incremental performance penalty associated with re-assembly of data chunks is unlikely to impact application performance.

Another area of concern with deduplication is the related effect on [[Snapshot (computer storage)|snapshots]], [[backup]], and [[archival]], especially where deduplication is applied against primary storage (for example inside a [[Network-attached storage|NAS]] filer).{{elucidate|date=December 2011}} Reading files out of a storage device causes full reconstitution of the files (also known as rehydration), so any secondary copy of the data set is likely to be larger than the primary copy. In terms of snapshots, if a file is snapshotted prior to deduplication, the post-deduplication snapshot will preserve the entire original file. This means that although storage capacity for primary file copies will shrink, capacity required for snapshots may expand dramatically.

Another concern is the effect of compression and encryption. Although deduplication is a version of compression, it works in tension with traditional compression. Deduplication achieves better efficiency against smaller data chunks, whereas compression achieves better efficiency against larger chunks. The goal of encryption is to eliminate any discernible patterns in the data. Thus encrypted data cannot be deduplicated, even though the underlying data may be redundant. Deduplication ultimately reduces redundancy.  If this was not expected and planned for, this may ruin the underlying reliability of the system.  (Compare this, for example, to the [[LOCKSS]] storage architecture that achieves reliability through multiple copies of data.)

Scaling has also been a challenge for deduplication systems because ideally, the scope of deduplication needs to be shared across storage devices. If there are multiple disk backup devices in an infrastructure with discrete deduplication, then space efficiency is adversely affected. A deduplication shared across devices preserves space efficiency, but is technically challenging from a reliability and performance perspective.{{citation needed|date=December 2011}}

Although not a shortcoming of data deduplication, there have been data breaches{{citation needed|date=August 2016}} when insufficient security and access validation procedures are used with large repositories of deduplicated data.  In some systems, as typical with cloud storage{{citation needed|date=August 2016}}, an attacker can retrieve data owned by others by knowing or guessing the hash value of the desired data.&lt;ref&gt;{{cite journal |title=A Cloud You Can Trust |publisher=[[IEEE]] |work=[[IEEE Spectrum]] |accessdate=2011-12-21 |url=http://spectrum.ieee.org/computing/networks/a-cloud-you-can-trust |author1=CHRISTIAN CACHIN |author2=MATTHIAS SCHUNTER |date=December 2011}}&lt;/ref&gt;

==See also==
* [[Capacity optimization]]
* [[Cloud storage]]
* [[Single-instance storage]]
* [[Content-addressable storage]]
* [[Delta encoding]]
* [[Linked data]]
* [[Pointer (computer programming)|Pointer]]
* [[Record linkage]]
* [[Identity resolution]]
* [[Convergent encryption]]

==References==
{{Reflist|30em}}

==External links==
* Biggar, Heidi(2007.12.11). [http://wayback.archive.org/web/20120325005645/http://www.infostor.com/webcast/display_webcast.cfm?ID=540 WebCast: The Data Deduplication Effect]
* Fellows, Russ(Evaluator Group, Inc.) [http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ Data Deduplication, why when where and how?]
* [http://wayback.archive.org/web/20120328022229/http://www.tacoma.washington.edu/tech/docs/research/gradresearch/MSpiz.pdf Using Latent Semantic Indexing for Data Deduplication].
* [http://www.forbes.com/2009/08/08/exagrid-storage-data-technology-cio-network-tape.html A Better Way to Store Data].
* [http://www.eweek.com/c/a/Database/What-Is-the-Difference-Between-Data-Deduplication-File-Deduplication-and-Data-Compression What Is the Difference Between Data Deduplication, File Deduplication, and Data Compression?] - Database from eWeek
* [http://www.snia.org/forums/dmf/programs/data_protect_init/ddsrsig/ SNIA DDSR SIG] * * [http://wayback.archive.org/web/20120322084240/http://www.snia.org/forums/dmf/knowledge/white_papers_and_reports/Understanding_Data_Deduplication_Ratios-20080718.pdf Understanding Data Deduplication Ratios]
* [http://public.dhe.ibm.com/common/ssi/ecm/en/tsu12345usen/TSU12345USEN.PDF Data Footprint Reduction Technology Whitepaper]
* [http://www.itnext.in/content/doing-more-less.html Doing More with Less by Jatinder Singh]
* [http://www.sersc.org/journals/IJSIA/vol7_no5_2013/38.pdf Byte Index Chunking Algorithm for Data Deduplication]

{{DEFAULTSORT:Data Deduplication}}
[[Category:Data management]]
[[Category:Data compression]]</text>
      <sha1>8hs8rkno35ofy3oln87wd031l04c08q</sha1>
    </revision>
  </page>
  <page>
    <title>Information governance</title>
    <ns>0</ns>
    <id>22723009</id>
    <revision>
      <id>756925029</id>
      <parentid>754590586</parentid>
      <timestamp>2016-12-27T18:10:17Z</timestamp>
      <contributor>
        <ip>217.35.252.69</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13911" xml:space="preserve">{{Governance}}

'''Information governance''', or '''IG''', is the set of multi-disciplinary structures, policies, procedures, processes and controls implemented to manage information at an enterprise level, supporting an organization's immediate and future regulatory, legal, risk, environmental and operational requirements. Information governance should determine the balance point between two potentially divergent organizational goals: extracting value from information and reducing the potential risk of information. Information governance reduces organizational risk in the fields of compliance, operational transparency, and reducing expenditures associated with e-discovery and litigation response. An organization can establish a consistent and logical framework for employees to handle data through their information governance policies and procedures. These policies guide proper behavior regarding how organizations and their employees handle electronically stored information ([[Electronically stored information (Federal Rules of Civil Procedure)|ESI]]).&lt;ref&gt;{{cite web|url=http://blogs.gartner.com/debra_logan/2010/01/11/what-is-information-governance-and-why-is-it-so-hard/|title=What is Information Governance? And Why is it So Hard? - Debra Logan|date=11 January 2010|publisher=}}&lt;/ref&gt;&lt;ref&gt;[Kooper, M., Maes, R., and Roos Lindgreen, E. (2011). On the governance of information: Introducing a new concept of governance to support the management of information. International Journal of Information Management, 31(3), 195-200]&lt;/ref&gt;

Information governance encompasses more than traditional [[records management]].  It incorporates [[information security]] and protection, compliance, [[data governance]], [[electronic discovery]], [[risk management]], privacy, data storage and archiving, [[knowledge management]], business operations and management, audit, analytics, IT management, [[master data management]], [[enterprise architecture]], [[business intelligence]], [[big data]], [[data science]], and finance.&lt;ref&gt;{{cite web|url=http://iginitiative.com/igi-publishes-2014-annual-report/|title=IGI PUBLISHES 2014 ANNUAL REPORT - Information Governance Initiative|date=11 August 2014|publisher=}}&lt;/ref&gt;

==History==

===Records management===
Records management deals with the creation, retention and storage and disposition of records.  A record can either be a physical, tangible object, or digital information such as a database, application data, and e-mail.  The [[records life-cycle|lifecycle]] was historically viewed as the point of creation to the eventual disposal of a record.  As data generation exploded in recent decades, and regulations and compliance issues increased, traditional records management failed to keep pace.  A more comprehensive platform for managing records and information became necessary to address all phases of the lifecycle, which led to the advent of information governance.&lt;ref&gt;http://www.arma.org/pdf/WhatIsRIM.pdf&lt;/ref&gt;

In 2003 the Department of Health in England introduced the concept of broad-based information governance into the National Health Service, publishing version 1 of an online performance assessment tool with supporting guidance. The NHS IG Toolkit&lt;ref&gt;{{cite web|url=https://www.igt.hscic.gov.uk/|title=Home|publisher=}}&lt;/ref&gt; is now used by over 30,000 NHS and partner organisations, supported by an e-learning platform with some 650,000 users.

In 2008, [[ARMA International]] introduced the Generally Accepted Recordkeeping Principles®, or "The Principles"&lt;ref&gt;{{cite web|url=http://www.arma.org/principles|title=Generally Accepted Recordkeeping Principles|publisher=}}&lt;/ref&gt; and the subsequent "The Principles" Information Governance Maturity Model.&lt;ref&gt;http://www.arma.org/principles/metrics.cfm&lt;/ref&gt; "The Principles" identify the critical hallmarks of information governance. As such, they apply to all sizes of organizations, in all types of industries, and in both the private and public sectors. Multi-national organizations can also use "The Principles" to establish consistent practices across a variety of business units. ARMA International recognized that a clear statement of "Generally Accepted Recordkeeping Principles®" ("The Principles") would guide:

* CEOs in determining how to protect their organizations in the use of information assets;
* Legislators in crafting legislation meant to hold organizations accountable; and
* Records management professionals in designing comprehensive and effective records management programs.

Information governance goes beyond retention and disposition to include privacy, access controls, and other compliance issues.  In electronic discovery, or e-discovery, relevant data in the form of [[electronically stored information]] is searched for by attorneys and placed on [[legal hold]].  IG includes consideration of how this data is held and controlled for e-discovery, and also provides a platform for defensible disposition and compliance.  Additionally, [[metadata]] often accompanies electronically stored data and can be of great value to the enterprise if stored and managed correctly.

With all of these additional considerations that go beyond traditional records management, IG emerged as a platform for organizations to define policies at the enterprise level, across multiple jurisdictions.  IG then also provides for the enforcement of these policies into the various repositories of information, data, and records.

A coalition of organizations known as Electronic Discovery Reference Model (EDRM), which was founded in 2005 to address issues related to electronic discovery and information governance, subsequently developed, as one of its projects, a resource called the Information Governance Reference Model (IGRM).&lt;ref&gt;{{cite web|author=EDRM|url=http://www.edrm.net/what-is-edrm|title=About EDRM|accessdate=2015-01-21}}&lt;/ref&gt; In 2011, EDRM, in collaboration with ARMA International, published a white paper that describes ''How the Information Governance Reference Model (IGRM) Complements ARMA International’s Generally Accepted Recordkeeping Principles ("The Principles")''&lt;ref&gt;{{cite book|last=White Paper|title=How the Information Governance Reference Model (IGRM)Complements ARMA International’s Generally Accepted Recordkeeping Principles|year=2011|publisher=EDRM and ARMA International|pages=15|url=http://www.edrm.net/wp-content/uploads/downloads/2011/12/White-Paper-EDRM-Information-Governance-Reference-Model-IGRM-and-ARMAs-GARP-Principles-12-7-2011.pdf|editor-last=Ledergerber|editor-first=Marcus}}&lt;/ref&gt;  The IGRM illustrates the relationship between key stakeholders and the Information Lifecycle and highlights the transparency required to enable effective governance IGRM v3.0 Update: Privacy &amp; Security Officers As Stakeholders.&lt;ref&gt;[http://www.edrm.net/download/all_projects/igrm/The-Final..-IGRM_v3.0Update-Whitepaper_Oct_2012.pdf IGRM v3.0 Update: Privacy &amp; Security Officers As Stakeholders]&lt;/ref&gt;

Universities and professional associations started to develop information governance training and education programmes. In 2010, Dr Elizabeth Lomas (who had been aligning RM with information security, assurance and risk management models throughout the 2000s) authored distance learning materials for Information Governance modules delivered internationally through Northumbria University. ARMA subsequently started to deliver an Information Governance certification. These initiatives have now been picked up by other Universities, e.g. San Jose State University offers a graduate certificate in information governance, information assurance, and cyber security, and has also incorporated a required course in information governance as part of their 100% online Master of Archives and Records Administration&lt;ref&gt;[http://ischool.sjsu.edu/programs/master-archives-records-administration-mara]]&lt;/ref&gt; (MARA) degree program.

In 2014, [[John Wiley &amp; Sons]] published the first textbook on information governance, "Information Governance: Concepts, Strategies, and Best Practices"&lt;ref&gt;{{cite book|url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118218302.html|title=Information Governance: Concepts, Strategies, and Best Practices|isbn=978-1-118-21830-3|date=April 2014|publisher=John Wiley &amp; Sons}}&lt;/ref&gt; by [[Robert Smallwood]]. Also in 2014, The Information Governance Conference,&lt;ref&gt;Information Governance Conference [http://www.infogovcon.com InfoGovCon.com]&lt;/ref&gt; an annual conference on information governance best practices began and the Information Governance Model&lt;ref&gt;Information Governance Model [http://www.infogovmodel.com InfoGovModel.com]&lt;/ref&gt; was launched at the inaugural event, it is now in use at over 1000 organizations worldwide.

===Organizational structure===
In the past, records managers owned records management, perhaps within a compliance department at an enterprise.  In order to address the broader issues surrounding records management, several other key stakeholders must be involved.  Legal, IT, and Compliance tend to be the departments that touch information governance the most, though certainly other departments might seek representation.  Many enterprises create information governance committees to ensure that all necessary constituents are represented and that all relevant issues are addressed.&lt;ref&gt;{{cite web|url=http://www.law.com/jsp/cc/PubArticleFriendlyCC.jsp?id=1202533945005|title=From the Experts: Information Governance and Its Impact on Litigation|publisher=}}&lt;/ref&gt;

===Tools===
To address retention and disposition, Records Management and Enterprise Content Management applications were developed.  Sometimes detached search engines or homegrown policy definition tools were created.  These were often employed at a departmental or divisional level; rarely were tools used across the enterprise.  While these tools were used to define policies, they lacked the ability to enforce those policies.  Monitoring for compliance with policies was increasingly challenging. Since information governance addresses so much more than traditional records management, several software solutions have emerged to include the vast array of issues facing records managers.

Other available tools include:
* ARMA International [[Www.arma.org/nextlevel|Next Level Information Governance Assessment]] ( Based upon the Generally Accepted Recordkeeping Principles)
* ARMA Generally Accepted Recordkeeping Principles&lt;ref&gt;ARMA International, [http://www.arma.org/r2/generally-accepted-br-recordkeeping-principles "The Principles"], ''ARMA International''&lt;/ref&gt;
* EDRM Information Governance Reference Model&lt;ref&gt;EDRM, [http://www.edrm.net/projects/igrm "Information Governance Reference Model"], ''EDRM''&lt;/ref&gt;
* Information Coalition Information Governance Model&lt;ref&gt;Information Coalition, [http://infocoalition.com/resources/models-methodologies/information-governance-model-infogovmodel "The Information Governance Model"], ''Information Coalition''&lt;/ref&gt;
* NHS Information Governance Toolkit&lt;ref&gt;NHS, [https://www.igt.hscic.gov.uk/ "NHS Information Governance Toolkit"], ''NHS''&lt;/ref&gt;

===Laws and regulations===
Key to IG are the regulations and laws that help to define corporate policies.  Some of these regulations include:
*The Foreign Account Tax Compliance Act, or [[Foreign Account Tax Compliance Act|FATCA]]&lt;ref&gt;{{cite web|url=http://www.irs.gov/businesses/corporations/article/0,,id=236667,00.html|title=Foreign Account Tax Compliance Act|publisher=}}&lt;/ref&gt;
*Payment Card Industry Data Security Standard, or [[Payment Card Industry Data Security Standard|PCI Compliance]]&lt;ref&gt;{{cite web|url=https://www.pcisecuritystandards.org/|title=Official PCI Security Standards Council Site - Verify PCI Compliance, Download Data Security and Credit Card Security Standards|publisher=}}&lt;/ref&gt;
*Health Insurance Portability and Accountability Act, or [[Health Insurance Portability and Accountability Act|HIPAA]]&lt;ref&gt;{{cite web|url=http://www.hhs.gov/hipaa/|title=Health Information Privacy|date=26 August 2015|publisher=}}&lt;/ref&gt;
*Financial Services Modernization Act of 1999, or [[Gramm–Leach–Bliley Act|GLBA]]&lt;ref&gt;{{cite web|url=https://www.congress.gov/bill/106th-congress/senate-bill/900|title=S.900 - Gramm-Leach-Bliley Act}}&lt;/ref&gt;
*Sarbanes–Oxley Act of 2002, or [[Sarbanes–Oxley|Sarbox or SOX]]&lt;ref&gt;{{cite web|url=https://www.sec.gov/about/laws/soa2002.pdf|title=Sarbanes–Oxley Act of 2002}}&lt;/ref&gt;
*[[Federal Rules of Civil Procedure]]

===Guidelines===
*[[MoReq2]]&lt;ref&gt;{{cite web|url=http://www.moreq2.eu/|title=Home - MoReq2|publisher=}}&lt;/ref&gt;
*MoReq2010&lt;ref&gt;{{cite web|url=http://moreq2010.eu/|title=Account Suspended|publisher=}}&lt;/ref&gt;
*[[ISO 15489 Information and documentation -- Records management|ISO 15489 Information and Documentation - Records Management]]&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=31908|title=ISO 15489-1:2001 - Information and documentation -- Records management -- Part 1: General|publisher=}}&lt;/ref&gt;
*DoD 5015.2, or [[Design Criteria Standard for Electronic Records Management Software Applications]]&lt;ref&gt;{{cite web|url=http://www.archives.gov/records-mgmt/initiatives/dod-standard-5015-2.html|title=DoD Standard 5015.2|publisher=}}&lt;/ref&gt;

==See also==
*[[Data defined storage]]
* [[Data governance]]
*[[Electronic discovery]]
*[[Enterprise content management]]
*[[Information management]]
*[[Information technology governance]]
*[[Knowledge management]]
*[[National archives]]
*[[Records management]]

==References==
{{reflist|30em}}

==External links==
* [http://www.epa.gov/records/what/quest1.htm EPA 10 Reasons for RM]
* [http://www.druva.com/resources/analyst-reports/governance-takes-central-role-enterprises-shift-to-mobile/]

[[Category:Information governance|*]]
[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Public records]]
[[Category:Data management]]</text>
      <sha1>03aoheqiswrw3e7vlnyfqs003oan6pu</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data mapping</title>
    <ns>14</ns>
    <id>34275960</id>
    <revision>
      <id>469506322</id>
      <timestamp>2012-01-04T13:38:11Z</timestamp>
      <contributor>
        <username>Danim</username>
        <id>14026077</id>
      </contributor>
      <comment>create</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="54" xml:space="preserve">{{catmain|Data mapping}}

[[Category:Data management]]</text>
      <sha1>pk0rhcmjsxd3drsxztdtsdy5r3rkotm</sha1>
    </revision>
  </page>
  <page>
    <title>ADO.NET</title>
    <ns>0</ns>
    <id>1434840</id>
    <revision>
      <id>737015454</id>
      <parentid>735899502</parentid>
      <timestamp>2016-08-31T07:24:37Z</timestamp>
      <contributor>
        <username>Louisvdw</username>
        <id>1011282</id>
      </contributor>
      <minor />
      <comment>Add missing "it" to sentence</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4527" xml:space="preserve">{{No footnotes|date=March 2009}}
{{Infobox software
|
| operating system       = [[Microsoft Windows]]
| genre                  = [[Software framework]]
| license                = [[Proprietary software]] ([[Base Class Library|BCL]] portion under [[MIT license]]; source code under [[Ms-RSL]])
| website                = {{url|http://msdn2.microsoft.com/en-us/library/aa286484.aspx}}
| Nonprofit              =
}}
'''ADO.NET''' is a data access technology from the [[Microsoft]] [[.NET Framework]] which provides communication between relational and non-relational systems through a common set of components.
ADO.NET is a set of computer software components that programmers can use to access data and data services from the database. It is a part of the [[Base Class Library|base class library]] that is included with the Microsoft .NET Framework. It is commonly used by programmers to access and modify data stored in [[Relational DBMS|relational database systems]], though it can also access data in non-relational sources. ADO.NET is sometimes considered an evolution of [[ActiveX Data Objects]] (ADO) technology, but was changed so extensively that it can be considered an entirely new product.

== Architecture ==
{{Main|ADO.NET data provider}}
[[Image:DotNet3.0.svg|thumb|right|240px|This [[technology]] forms a part of [[.NET Framework 3.0]] (having been part of the framework since version 1.0)]]

ADO.NET is conceptually divided into ''[[ADO.NET consumer|consumers]]'' and ''[[ADO.NET provider|data providers]]''. The consumers are the applications that need access to the data, and the providers are the software components that implement the interface and thereby provide the data to the consumer.

Functionality exists in [[Microsoft Visual Studio|Visual Studio]] IDE to create specialized subclasses of the DataSet classes for a particular database schema, allowing convenient access to each field through strongly typed [[Property (programming)|properties]]. This helps catch more programming errors at compile-time and enhances the IDE's [[Intellisense]] feature.

== O/R Mapping ==
{{main|Object-relational mapping}}

=== Entity Framework ===
{{main|Entity Framework}}

Entity Framework (EF) is an open source object-relational mapping (ORM) framework for ADO.NET, part of .NET Framework. It is a set of technologies in ADO.NET that support the development of data-oriented software applications. Architects and developers of data-oriented applications have typically struggled with the need to achieve two very different objectives. The Entity Framework enables developers to work with data in the form of domain-specific objects and properties, such as customers and customer addresses, without having to concern themselves with the underlying database tables and columns where this data is stored. With the Entity Framework, developers can work at a higher level of abstraction when they deal with data, and can create and maintain data-oriented applications with less code than in traditional applications.

=== LINQ to SQL ===
{{main|LINQ to SQL}}

LINQ to SQL (formerly called DLINQ) allows [[LINQ]] to be used to query Microsoft SQL Server databases, including SQL Server Compact databases. Since SQL Server data may reside on a remote server, and because SQL Server has its own query engine, it does not use the query engine of LINQ. Instead, it converts a LINQ query to a SQL query that is then sent to SQL Server for processing. However, since SQL Server stores the data as relational data and LINQ works with data encapsulated in objects, the two representations must be mapped to one another. For this reason, LINQ to SQL also defines a mapping framework. The mapping is done by defining classes that correspond to the tables in the database, and containing all or a certain subset of the columns in the table as data members.

==See also==
* [[Comparison of ADO and ADO.NET]]

==External links==
;ADO.NET
* [http://msdn2.microsoft.com/en-us/library/aa286484.aspx ADO.NET Overview on MSDN]
* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO Programmer]
* [http://www.devlist.com/ConnectionStringsPage.aspx ADO.NET Connection Strings]

{{.NET Framework}}
{{Microsoft APIs}}{{Windows-software-stub}}{{Microsoft-stub}}{{Programming-software-stub}}

{{DEFAULTSORT:Ado.Net}}
[[Category:Data management]]
[[Category:.NET Framework terminology]]
[[Category:Microsoft application programming interfaces]]
[[Category:SQL data access]]
[[Category:ADO.NET Data Access technologies]]</text>
      <sha1>0oqg6svxujfk78ztvwql823cusaybtg</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic lab notebook</title>
    <ns>0</ns>
    <id>1616185</id>
    <revision>
      <id>750728389</id>
      <parentid>741454683</parentid>
      <timestamp>2016-11-21T14:08:46Z</timestamp>
      <contributor>
        <username>Deb</username>
        <id>1219</id>
      </contributor>
      <comment>/* See also */ remove red link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10621" xml:space="preserve">An '''electronic lab notebook''' (also known as electronic laboratory notebook, or ELN) is a [[computer program]] designed to replace paper [[lab notebook|laboratory notebook]]s.  Lab notebooks in general are used by [[scientist]]s, [[engineer]]s, and [[technician]]s to document [[research]], [[experiment]]s, and procedures performed in a laboratory.  A lab notebook is often maintained to be a [[legal document]] and may be used in a [[court of law]] as [[evidence (law)|evidence]].  Similar to an [[inventor's notebook]], the lab notebook is also often referred to in [[patent]] prosecution and [[intellectual property]] [[litigation]].

Electronic lab notebooks are a fairly new technology and offer many benefits to the user as well as organizations. For example: electronic lab notebooks are easier to search upon, simplify data copying and backups, and support collaboration amongst many users.&lt;ref&gt;{{cite conference |
title = A Collaborative Electronic Notebook |
first = James | 
last = Myers |author2=Elena Mendoza |author3=Bonnie Hoopes |
journal = Proceedings of the IASTED International Conference on Internet and Multimedia Systems and Applications |
year = 2001 
}}&lt;/ref&gt;  
ELNs can have fine-grained access controls, and can be more secure than their paper counterparts.&lt;ref&gt;{{
cite conference | 
last= Myers | 
first = James | 
year = 2003 | 
journal = Proceedings of the 2003 International Symposium On Collaborative Technologies and Systems | 
title = Collaborative Electronic Notebooks as Electronic Records:Design Issues for the Secure Electronic Laboratory Notebook (ELN) | 
url = http://collaboratory.emsl.pnl.gov/resources/publications/papers/seceln(final1)1-22Nov.pdf
}}&lt;/ref&gt;  They also allow the direct incorporation of data from instruments, replacing the practice of printing out data to be stapled into a paper notebook.&lt;ref&gt;{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541–543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}&lt;/ref&gt;

==Types==
ELNs can be divided into two categories:

* "Specific ELNs" contain features designed to work with specific applications, scientific instrumentation or data types.
* "[[Cross-disciplinary]] ELNs" or "Generic ELNs" are designed to support access to all data and information that needs to be recorded in a lab notebook.

Solutions range from specialized programs designed from the ground up for use as an ELN, to modifications or direct use of more general programs.  Examples of using more general software include using [[OpenWetWare]], a [[MediaWiki]] install (running the same software that Wikipedia uses), as an ELN, or the use of general note taking software such as OneNote as an ELN.&lt;ref&gt;{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541–543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}.&lt;/ref&gt;

ELN's come in many different forms. They can be standalone programs, use a client-server model, or be entirely web-based. Some use a lab-notebook approach, others resemble a blog.

A good many variations on the "ELN" acronym have appeared.&lt;ref&gt;{{Cite web|url=http://cerf-notebook.com/articles/eln-glossary/|title=Lab Notebook (ELN) Glossary - CERF|date=2016-02-16|language=en-US|access-date=2016-08-20}}&lt;/ref&gt; Differences between systems with different names are often subtle, with considerable functional overlap between them. Examples include "ERN" (Electronic Research Notebook), "ERMS" (Electronic Resource (or Research or Records) Management System (or Software) and SDMS (Scientific Data (or Document) Management System (or Software). Ultimately, these types of systems all strive to do the same thing: Capture, record, centralize and protect scientific data in a way that is highly searchable, historically accurate, and legally stringent, and which also promotes secure collaboration, greater efficiency, reduced mistakes and lowered total research costs.

==Objectives==
A good electronic laboratory notebook should offer a secure environment to protect the integrity of both data and process, whilst also affording the flexibility to adopt new processes or changes to existing processes without recourse to further software development. The package architecture should be a modular design, so as to offer the benefit of minimizing validation costs of any subsequent changes that you may wish to make in the future as your needs change.

A good electronic laboratory notebook should be an "out of the box" solution that, as standard, has fully configurable forms to comply with the requirements of regulated analytical groups through to a sophisticated ELN for inclusion of structures, spectra, chromatograms, pictures, text, etc. where a preconfigured form is less appropriate. All data within the system may be stored in a database (e.g. MySQL, MS-SQL, Oracle) and be fully searchable. The system should enable data to be collected, stored and retrieved through any combination of forms or ELN that best meets the requirements of the user.

The application should enable secure forms to be generated that accept laboratory data input via PCs and/or laptops / palmtops, and should be directly linked to electronic devices such as laboratory balances, pH meters, etc.  Networked or wireless communications should be accommodated for by the package which will allow data to be interrogated, tabulated, checked, approved, stored and archived to comply with the latest regulatory guidance and legislation.  A system should also include a scheduling option for routine procedures such as equipment qualification and study related timelines. It should include configurable qualification requirements to automatically verify that instruments have been cleaned and calibrated within a specified time period, that reagents have been quality-checked and have not expired, and that workers are trained and authorized to use the equipment and perform the procedures.

==Regulatory and legal aspects==
The laboratory accreditation criteria found in the [[ISO 17025]] standard needs to be considered for the protection and computer backup of electronic records. These criteria can be found specifically in clause 4.13.1.4 of the standard.&lt;ref&gt;"ISO/IEC 17025:2005 - General Requirements for the Competence of Testing and Calibration Laboratories." ISO - International Organization for Standardization. Web. 16 Nov. 2011. &lt;http://www.iso.org/iso/Catalogue_detail?csnumber=39883&gt;.&lt;/ref&gt;

Electronic lab notebooks used for development or research in regulated industries, such as medical devices or pharmaceuticals, are expected to comply with FDA regulations related to software validation.  The purpose of the regulations is to ensure the integrity of the entries in terms of time, authorship, and content.  Unlike ELNs for patent protection, FDA is not concerned with patent interference proceedings, but is concerned with avoidance of falsification.  Typical provisions related to software validation are included in the medical device regulations at 21 CFR 820 (et seq.)&lt;ref&gt;United States. Food and Drug Administration. Department of Health and Human Resources. 1 Food and Drugs - Subchapter H Medical Devices - Part 820 System RegCode of Federal Regulations - Title 2ulation. FDA.gov, 7 Oct. 1996. Web. &lt;http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=820&gt;.&lt;/ref&gt; and [[Title 21 CFR Part 11]].&lt;ref&gt;United States. Food and Drug Administration. Department of Health and Human Resources. Code of Federal Regulations - Title 21 Part 11 Electronic Records; Electronic Signatures. FDA.gov. Authority: 21 U.S.C. 321-393; 42 U.S.C. 262., 20 Mar. 1997. Web. 16 Nov. 2011. &lt;http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=11&gt;.&lt;/ref&gt;  Essentially, the requirements are that the software has been designed and implemented to be suitable for its intended purposes.  Evidence to show that this is the case is often provided by a Software Requirements Specification (SRS) setting forth the intended uses and the needs that the ELN will meet; one or more testing protocols that, when followed, demonstrate that the ELN meets the requirements of the specification and that the requirements are satisfied under worst-case conditions.  Security, audit trails, prevention of unauthorized changes without substantial collusion of otherwise independent personnel (i.e., those having no interest in the content of the ELN such as independent quality unit personnel) and similar tests are fundamental.  Finally, one or more reports demonstrating the results of the testing in accordance with the predefined protocols are required prior to release of the ELN software for use.  If the reports show that the software failed to satisfy any of the SRS requirements, then corrective and preventive action ("CAPA") must be undertaken and documented.  Such CAPA may extend to minor software revisions, or changes in architecture or major revisions.  CAPA activities need to be documented as well.

Aside from the requirements to follow such steps for regulated industry, such an approach is generally a good practice in terms of development and release of any software to assure its quality and fitness for use.  There are standards related to software development and testing that can be applied (see ref.).

==See also==
* [[List of ELN software packages]]	
* [[Data management]]
* [[Laboratory informatics]]
* [[Scientific management]]

==References==
{{reflist}}

== Further reading ==
* {{Cite journal 
| last1 = Taylor | first1 = K. T. 
| title = The status of electronic laboratory notebooks for chemistry and biology 
| journal = Current opinion in drug discovery &amp; development 
| volume = 9 
| issue = 3 
| pages = 348–353 
| year = 2006 
| pmid = 16729731
}}
* {{Cite journal | last1 = Rubacha | first1 = M. | last2 = Rattan | first2 = A. K. | last3 = Hosselet | first3 = S. C. | doi = 10.1016/j.jala.2009.01.002 | title = A Review of Electronic Laboratory Notebooks Available in the Market Today | journal = Journal of Laboratory Automation | volume = 16 | issue = 1 | pages = 90–98 | year = 2011 | pmid =  21609689| pmc = }}

{{DEFAULTSORT:Electronic Lab Notebook}}
[[Category:Electronic lab notebook]]
[[Category:Research]]
[[Category:Science software]]
[[Category:Scientific documents]]
[[Category:Notebooks]]
[[Category:Electronic documents]]
[[Category:Data management]]
[[Category:Content management systems]]
[[Category:Data management software]]</text>
      <sha1>h0szlanaajetcxu8axsdg9p45m6ist4</sha1>
    </revision>
  </page>
  <page>
    <title>Linked data</title>
    <ns>0</ns>
    <id>11174052</id>
    <revision>
      <id>751161017</id>
      <parentid>751036988</parentid>
      <timestamp>2016-11-23T19:37:02Z</timestamp>
      <contributor>
        <username>Denny</username>
        <id>10969</id>
      </contributor>
      <comment>Undid revision 751036988 by [[Special:Contributions/109.193.94.192|109.193.94.192]] ([[User talk:109.193.94.192|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14182" xml:space="preserve">In [[computing]], '''linked data''' (often capitalized as '''Linked Data''') is a method of publishing structured data so that it can be interlinked and become more useful through [[semantic query|semantic queries]]. It builds upon standard [[World Wide Web|Web]] technologies such as [[Hypertext Transfer Protocol|HTTP]], [[Resource Description Framework|RDF]] and [[uniform resource identifier|URIs]], but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.&lt;ref name=linkeddatastorysofar&gt;{{Cite journal |url=http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf
 |title=Linked Data&amp;mdash;The Story So Far |last=Bizer |first=Christian |last2=Heath |first2=Tom |last3=Berners-Lee
 |first3=Tim |author3-link=Tim Berners-Lee |year=2009 |accessdate=2010-12-18 |doi=10.4018/jswis.2009081901 |issn=1552-6283
 |journal=International Journal on Semantic Web and Information Systems |volume=5 |issue=3 |pages=1–22}} Solving Semantic Interoperability Conflicts in Cross–Border E–Government Services.&lt;/ref&gt;

[[Tim Berners-Lee]], director of the [[World Wide Web Consortium]] (W3C), coined the term in a 2006 design note about the [[Semantic Web]] project.&lt;ref name=DesignIssues&gt;{{cite web |url=http://www.w3.org/DesignIssues/LinkedData.html
 |title=Linked Data |work=Design Issues |author=Tim Berners-Lee |authorlink=Tim Berners-Lee |date=2006-07-27
 |publisher=[[W3C]] |accessdate=2010-12-18}}&lt;/ref&gt;

== Principles ==
Tim Berners-Lee outlined four principles of linked data in his "Linked Data" note of 2006,&lt;ref name=DesignIssues/&gt; paraphrased along the following lines:

&lt;blockquote&gt;
# Use [[uniform resource identifier|URIs]] to name (identify) things.
# Use [[Hypertext Transfer Protocol|HTTP]] URIs so that these things can be looked up (interpreted, "dereferenced").
# Provide useful information about what a name identifies when it's looked up, using open standards such as [[Resource Description Framework|RDF]], [[SPARQL]], etc.
# Refer to other things using their HTTP URI-based names when publishing data on the Web.
&lt;/blockquote&gt;

Tim Berners-Lee gave a presentation on linked data at the [[TED (conference)|TED]] 2009 conference.&lt;ref&gt;{{cite web |url=http://www.ted.com/talks/tim_berners_lee_on_the_next_web.html |title=Tim Berners-Lee on the next Web}}&lt;/ref&gt;  In it, he restated the linked data principles as three "extremely simple" rules:

&lt;blockquote&gt;
# All kinds of conceptual things, they have names now that start with HTTP.
# If I take one of these HTTP names and I look it up...I will get back some data in a standard format which is kind of useful data that somebody might like to know about that thing, about that event.
# When I get back that information it's not just got somebody's height and weight and when they were born, its got relationships. And when it has relationships, whenever it expresses a relationship then the other thing that it's related to is given one of those names that starts with HTTP.
&lt;/blockquote&gt;

== Components ==
* [[Uniform resource identifier|URI]]s
* [[HTTP]]
* [[Structured data]] using [[controlled vocabulary]] terms and dataset definitions expressed in [[Resource Description Framework]] [[serialization]] formats such as [[RDFa]], [[RDF/XML]], [[Notation 3|N3]], [[Turtle (syntax)|Turtle]], or [[JSON-LD]]
* [[Linked Data Platform]]

==Linked open data==
'''Linked open data''' is linked data that is [[open content]].&lt;ref&gt;{{cite web|url=http://linkeddata.org/faq|title=Frequently Asked Questions (FAQs) - Linked Data - Connect Distributed Data across the Web|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.coar-repositories.org/activities/repository-observatory/second-edition-linked-open-data/7-things-you-should-know-about-open-data/|title=COAR »   7 things you should know about…Linked Data|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://openorg.ecs.soton.ac.uk/wiki/Linked_Data_Basics_for_Techies#Open_Linked_Data|title=Linked Data Basics for Techies|publisher=}}&lt;/ref&gt; Tim Berners-Lee gives the clearest definition of linked open data in differentiation with linked data. {{Quote |text=Linked Open Data (LOD) is Linked Data which is released under an open licence, which does not impede its reuse for free. |author=Tim Berners-Lee |title=Linked Data&lt;ref name=DesignIssues /&gt;&lt;ref&gt;{{cite web|url=http://5stardata.info/en|title=5 Star Open Data}}&lt;/ref&gt;}} Large linked open data sets include [[DBpedia]] and [[Freebase]].

=== History ===

The term "linked open data" has been in use since at least February 2007, when the "Linking Open Data" mailing list&lt;ref&gt;{{cite web|url=http://lists.w3.org/Archives/Public/public-lod/|title=public-lod@w3.org Mail Archives|publisher=}}&lt;/ref&gt; was created.&lt;ref&gt;{{cite web|url=http://www.w3.org/wiki/SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|title=SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|publisher=}}&lt;/ref&gt; The mailing list was initially hosted by the [[SIMILE]] project&lt;ref&gt;{{cite web|url=http://simile.mit.edu/mail.html|title=SIMILE Project - Mailing Lists|publisher=}}&lt;/ref&gt; at the [[Massachusetts Institute of Technology]].

=== Linking Open Data community project ===
[[File:LOD Cloud 2014.svg|thumb|400px|The above diagram shows which Linking Open Data datasets are connected, as of August 2014.  This was produced by the 
Linked Open Data Cloud project, which was started in 2007.  Some sets may include copyrighted data which is freely available.&lt;ref&gt;Linking open data cloud diagram 2014, by Max Schmachtenberg, Christian Bizer, Anja Jentzsch and Richard Cyganiak. http://lod-cloud.net/&lt;/ref&gt;]]

The goal of the W3C Semantic Web Education and Outreach group's Linking Open Data community project is to extend the Web with a [[Knowledge commons|data commons]] by publishing various [[open knowledge|open]] [[dataset]]s as RDF on the Web and by setting [[Resource Description Framework|RDF]] links between data items from different data sources. In October 2007, datasets consisted of over two billion [[RDF triples]], which were interlinked by over two million RDF links.&lt;ref&gt;[http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data]&lt;/ref&gt;&lt;ref&gt;{{cite book |last1=Fensel |first1=Dieter |last2=Facca |first2= Federico Michele |last3=Simperl |first3=Elena |last4=Ioan |first4=Toma |title=Semantic Web Services |year=2011 |publisher=Springer|isbn=3642191924 |pages=99}}&lt;/ref&gt;  By September 2011 this had grown to 31 billion RDF triples, interlinked by around 504 million RDF links.  A detailed statistical breakdown was published in 2014.&lt;ref&gt;http://linkeddatacatalog.dws.informatik.uni-mannheim.de/state/&lt;/ref&gt;

=== European Union projects ===
There are a number of European Union projects{{Definition|date=June 2013}} involving linked data. These include the linked open data around the clock (LATC) project,&lt;ref&gt;[http://latc-project.eu/ Linked open data around the clock (LATC)]&lt;/ref&gt; the PlanetData project,&lt;ref&gt;[http://planet-data.eu/ PlanetData]&lt;/ref&gt; the DaPaaS (Data-and-Platform-as-a-Service) project,&lt;ref&gt;[http://project.dapaas.eu/ DaPaaS]&lt;/ref&gt; and  the Linked Open Data 2 (LOD2) project.&lt;ref&gt;[http://lod2.eu/ Linking Open Data 2 (LOD2)]&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&amp;ACTION=D&amp;CAT=PROJ&amp;RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects – LOD2 |date=2010-04-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://static.lod2.eu/Deliverables/LOD2_D12.5.1_Project_Fact_Sheet_Version.pdf |title=LOD2 Project Fact Sheet – Project Summary |date=2010-09-01 |accessdate=2010-12-18}}&lt;/ref&gt; Data linking is one of the main goals of the [[EU Open Data Portal]], which makes available thousands of datasets for anyone to reuse and link.

=== Datasets ===

* [[DBpedia]] – a dataset containing extracted data from Wikipedia; it contains about 3.4 million concepts described by 1 billion [[Semantic triple|triples]], including abstracts in 11 different languages
* [[FOAF (software)|FOAF]] – a dataset describing persons, their properties and relationships
* [[GeoNames]] provides RDF descriptions of more than {{formatnum:7500000}} geographical features worldwide.
* [[UMBEL]] – a lightweight reference structure of {{formatnum:20000}} subject concept classes and their relationships derived from [[OpenCyc]], which can act as binding classes to external data; also has links to 1.5 million named entities from DBpedia and [[YAGO (ontology)|YAGO]]
* [[Wikidata]] – a collaboratively-created linked dataset that acts as central storage for the structured data of its [[Wikimedia]] sister projects

=== Dataset instance and class relationships ===
Clickable diagrams that show the individual datasets and their relationships within the DBpedia-spawned LOD cloud (as shown by the figures to the right) are available.&lt;ref&gt;[http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-07-14.html Instance relationships amongst datasets]&lt;/ref&gt;&lt;ref&gt;[http://web.archive.org/web/20110828103804/http://umbel.org/sites/umbel.org/lod/lod_constellation.html Class relationships amongst datasets]&lt;/ref&gt;

==See also==
* [[Authority control]] – about ''controlled headings'' in library catalogs
* [[Citation analysis]] – for citations between scholarly articles
* [[Hyperdata]]
* [[Linked data page]]
* [[Network model]] – an older type of database management system
* [[Schema.org]]
* [[Web Ontology Language]]

== References ==
{{reflist|30em}}

== Further reading ==
{{ref begin|2}}
* Ahmet Soylu, Felix Mödritscher, and Patrick De Causmaecker. 2012. [http://www.ahmetsoylu.com/wp-content/uploads/2013/10/soylu_ICAE2012.pdf “Ubiquitous Web Navigation through Harvesting Embedded Semantic Data: A Mobile Scenario.”] Integrated Computer-Aided Engineering 19 (1): 93–109.
* ''[http://linkeddatabook.com/book Linked Data: Evolving the Web into a Global Data Space]'' (2011) by Tom Heath and Christian Bizer, Synthesis Lectures on the Semantic Web: Theory and Technology, Morgan &amp; Claypool &lt;!-- note this resources supersedes the tutorial [http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/ How to publish Linked Data on the Web] by Bizer, Cyganiak, and Heath --&gt;
* [http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/LinkedDataTutorial/ How to Publish Linked Data on the Web], by Chris Bizer, Richard Cyganiak and Tom Heath, Linked Data Tutorial at Freie Universität Berlin, Germany, 27 July 2007.
* [http://www.scientificamerican.com/article.cfm?id=berners-lee-linked-data The Web Turns 20: Linked Data Gives People Power], part 1 of 4, by Mark Fischetti, ''[[Scientific American]]'' 2010 October 23
* [http://knoesis.wright.edu/library/publications/linkedai2010_submission_13.pdf Linked Data Is Merely More Data] – Prateek Jain, Pascal Hitzler, Peter Z. Yeh, Kunal Verma, and Amit P. Sheth. In: Dan Brickley, Vinay K. Chaudhri, Harry Halpin, and Deborah McGuinness: ''Linked Data Meets Artificial Intelligence''. Technical Report SS-10-07, AAAI Press, Menlo Park, California, 2010, pp.&amp;nbsp;82–86.
* [http://knoesis.org/library/resource.php?id=1718 Moving beyond sameAs with PLATO: Partonomy detection for Linked Data] – Prateek Jain, Pascal Hitzler, Kunal Verma, Peter Z. Yeh, Amit Sheth. In:  Proceedings of the 23rd ACM Hypertext and Social Media conference (HT 2012), Milwaukee, WI, USA, June 25–28, 2012.
* Freitas, André, Edward Curry, João Gabriel Oliveira, and Sean O’Riain. 2012. [http://www.edwardcurry.org/publications/freitas_IC_12.pdf “Querying Heterogeneous Datasets on the Linked Data Web: Challenges, Approaches, and Trends.”] IEEE Internet Computing 16 (1): 24–33.
* [http://www2008.org/papers/pdf/p1265-bizer.pdf Linked Data on the Web] – Chris Bizer, Tom Heath, [[Kingsley Uyi Idehen]], [[Tim Berners-Lee]]. In Proceedings WWW2008, Beijing, China
* [http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkingOpenData.pdf Interlinking Open Data on the Web] – Chris Bizer, Tom Heath, Danny Ayers, Yves Raimond. In Proceedings Poster Track, ESWC2007, Innsbruck, Austria
* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data] – Prateek Jain, Pascal Hitzler, Amit Sheth, Kunal Verma, Peter Z. Yeh. In proceedings of the 9th International Semantic Web Conference, ISWC 2010, Shanghai, China
* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3121711/ Linked open drug data for pharmaceutical research and development] - J Cheminform. 2011; 3: 19. Samwald, Jentzsch, Bouton, Kallesøe, Willighagen, Hajagos, Marshall, Prud'hommeaux, Hassenzadeh, Pichler, and Stephens (May 2011)
* [http://www.community-of-knowledge.de/beitrag/the-hype-the-hope-and-the-lod2-soeren-auer-engaged-in-the-next-generation-lod/ Interview with Sören Auer, head of the LOD2 project about the continuation of LOD2 in 2011], June 2011
* [http://www.semantic-web.at/LOD-TheEssentials.pdf Linked Open Data: The Essentials] - Florian Bauer and Martin Kaltenböck (January 2012)
* [http://semanticweb.com/the-flap-of-a-butterfly-wing_b26808 The Flap of a Butterfly Wing] - semanticweb.com Richard Wallis (February 2012)
{{ref end}}

== External links ==
* [http://www.w3.org/wiki/LinkedData LinkedData] at the W3C Wiki
* [http://linkeddata.org LinkedData.org]
* [http://virtuoso.openlinksw.com/white-papers/ OpenLink Software white papers]
* [http://demo.openlinksw.com/Demo/customers/CustomerID/ALFKI%23this Data from Northwind SQL schema as linked data], use case demo
* [http://nomisma.org/ Linked data for the discipline of numismatics], use case demo
* [http://en.lodlive.it Interactive LOD demo]
* [http://americanartcollaborative.org/ American Art Collaborative], consortium of US art museums committed to establishing a critical mass of linked open data on American art

{{Semantic Web}}
{{Open data navbox}}

{{Authority control}}

[[Category:Cloud standards]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
[[Category:Hypermedia]]
[[Category:Internet terminology]]
[[Category:Open data]]
[[Category:World Wide Web]]
[[Category:Semantic Web]]</text>
      <sha1>37fwszmf1rok7oe9e13vku7016ksi06</sha1>
    </revision>
  </page>
  <page>
    <title>Asset Description Metadata Schema</title>
    <ns>0</ns>
    <id>35676267</id>
    <revision>
      <id>745248841</id>
      <parentid>695226264</parentid>
      <timestamp>2016-10-20T02:47:20Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead. #IABot (v1.2.5)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6174" xml:space="preserve">[[File:ADMSmodelv1.png|thumb|300px|ADMS UML model version 1.00]]

The '''Asset Description Metadata Schema''' ('''ADMS''') is a common [[metadata]] vocabulary to describe standards, so-called interoperability assets, on the Web.

Used in concert with [[Web Syndication|web syndication technology]] ADMS helps people make sense of the complex multi-publisher environment around standards and in particular the ones which are semantic assets such as [[ontologies]], [[data model]]s, [[Data dictionary|data dictionaries]], code lists, [[XML]] and [[Resource Description Framework|RDF]] schemas. In spite of their importance, standards are not easily discoverable on the web via search engines because [[metadata]] about them is seldom available. Navigating on the websites of the different publishers of standards is not efficient either.

==Key terminology==
A '''semantic asset''' is a specific type of standard which involves:

 highly reusable metadata
 (e.g. xml schemata, generic data models)
 and/or reference data
 (e.g. code lists, taxonomies, dictionaries, vocabularies)

Organisations use semantic assets to share information and knowledge (within themselves and with others). Semantic assets are usually very valuable and reusable elements for the development of Information Systems, in particular, as part of machine-to-machine interfaces. As enablers to interoperable information exchange, semantic assets are usually created, published and maintained by standardisation bodies. Nonetheless, ICT projects and groups of experts also create such assets. There are therefore many publishers of semantic assets with different degrees of formalism.

==What is ADMS==
ADMS&lt;ref name="ADMS"&gt;[http://joinup.ec.europa.eu/asset/adms/home], ADMS homepage on Joinup&lt;/ref&gt; is a standardised metadata vocabulary created by the [[European Union|EU]]'s Interoperability Solutions for European Public Administrations (ISA) Programme&lt;ref name="ISA"&gt;[http://ec.europa.eu/isa/], Interoperability Solutions for European Public Administrations (ISA) Programme&lt;/ref&gt; of the [[European Commission]] to help publishers of standards document what their standards are about (their name, their status, theme, version, etc.) and where they can be found on the Web. ADMS descriptions can then be published on different websites while the standard itself remains on the website of its publisher (i.e. syndication of content). ADMS embraces the multi-publisher environment and, at the same time, it provides the means for the creation of aggregated catalogues of standards and single points of access to them based on ADMS descriptions. The Commission will offer a single point of access to standards described using ADMS via its collaborative platform, Joinup.&lt;ref name="Joinup"&gt;[https://joinup.ec.europa.eu/], Link to Joinup&lt;/ref&gt; The Federation&lt;ref name="Federation"&gt;[https://joinup.ec.europa.eu/elibrary/document/adms-enabled-federation-semantic-asset-repositories-brochure], Link to the brochure of the Federation of Semantic Asset Repositories&lt;/ref&gt; service will increase the visibility of standards described with ADMS on the web. This will also stimulate their reuse by Pan-European initiatives.

==ADMS Working Group==
More than 43 people of 20 EU Member States as well as from the US and Australia have participated in the [https://joinup.ec.europa.eu/asset/adms/document/adms-working-group ADMS Working Group]. Most of them were experts from standardisation bodies, research centres and the EU Commission. The working group used a methodology based on [[W3C]]’s processes and methods.&lt;ref name="CoreVocsPM"&gt;{{cite web|url=http://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |title=Archived copy |accessdate=2012-04-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130430164940/https://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |archivedate=2013-04-30 |df= }}, Link to ISA's Core Vocs methodology&lt;/ref&gt;

==How to download ADMS==
ADMS version 1 was officially released in April 2012.&lt;ref name="ADMSrelease"&gt;[https://joinup.ec.europa.eu/news/adms-v100-officially-released], ADMS V1 release announcement&lt;/ref&gt; Version 1.00 of ADMS is available for download on Joinup:&lt;ref name="Joinup"/&gt;
https://joinup.ec.europa.eu/asset/adms/release/100&lt;ref name="ADMS1"&gt;[https://joinup.ec.europa.eu/], Link to ADMS v1&lt;/ref&gt;

ADMS is offered under ISA's Open Metadata Licence v1.1&lt;ref name="OpenMetadataLicense"&gt;[https://joinup.ec.europa.eu/category/licence/isa-open-metadata-licence-v11], ISA Open Metadata Licence v1.1&lt;/ref&gt;

==Related work==
The ADMS specification reuses existing [[metadata]] vocabularies and core vocabularies including:
* The [[Dublin Core]] Metadata Element Set (DCMES)&lt;ref&gt;http://dublincore.org/documents/dces/&lt;/ref&gt;
* The [[Data Catalog Vocabulary]] (DCAT) &lt;ref&gt;http://www.w3.org/TR/vocab-dcat/&lt;/ref&gt;
* The [[FOAF (software)|Friend of a Friend (FOAF) Ontology]]
* The [[vCard]] Ontology &lt;ref&gt;[http://www.w3.org/TR/vcard-rdf/], Representing vCard Objects in RDF&lt;/ref&gt;

==The future of ADMS==
ADMS v1.00 will be contributed to&lt;ref name="ADMScontributed"&gt;[https://joinup.ec.europa.eu/asset/adms/topic/adms-public-review-key-specifications-interoperability-developed-eus-isa-programme-], Announcement that key specifications for interoperability developed by the EU's ISA Programme will become W3C standards&lt;/ref&gt; W3C’s Government Linked Data (GLD) Working Group.&lt;ref name="W3C_GLD"&gt;[http://www.w3.org/2011/gld/wiki/Main_Page], W3 Government Linked Data (GLD) Working Group&lt;/ref&gt; This means that ADMS will be published by the GLD Working Group as First Public Working Drafts for further consultation within the context of the typical W3C standardization process. The desired outcome of that process will be the publication of ADMS as a W3C Recommendation available under W3C's Royalty-Free License.

The ADMS RDFS Vocabulary already has a w3.org namespace: [http://www.w3.org/ns/adms http://www.w3.org/ns/adms#].

==References==
{{reflist|30em}}

[[Category:Data management]]
[[Category:Metadata]]
[[Category:Technical communication]]</text>
      <sha1>olprxnd5keb39hxtue8jzyxopq13j7l</sha1>
    </revision>
  </page>
  <page>
    <title>Clone (database)</title>
    <ns>0</ns>
    <id>8586147</id>
    <revision>
      <id>674652410</id>
      <parentid>643822482</parentid>
      <timestamp>2015-08-05T08:42:26Z</timestamp>
      <contributor>
        <ip>180.214.240.254</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="776" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=December 2006}}
{{orphan|date=February 2009}}
}}

A '''database clone ''' is a complete and separate copy of a database system that includes the business data , the [[DBMS]] software and any other application  tiers that make up the environment. Cloning is a different kind of operation to [[Data replication|replication]] and [[backup]]s in that the cloned environment is both fully functional and separate in its own right. Additionally the cloned environment may be modified at its inception due to configuration changes or data subsetting.

The cloning refers to the replication of the server in order to have a backup, to upgrade the environment.

{{DEFAULTSORT:Clone (Database)}}
[[Category:Data management]]
[[Category:Databases]]</text>
      <sha1>002v73ur29c2req9sx9hsw6t5rv42xi</sha1>
    </revision>
  </page>
  <page>
    <title>NewSQL</title>
    <ns>0</ns>
    <id>37256799</id>
    <revision>
      <id>752623030</id>
      <parentid>751922209</parentid>
      <timestamp>2016-12-02T09:10:38Z</timestamp>
      <contributor>
        <username>Queenypingcap</username>
        <id>29541323</id>
      </contributor>
      <comment>/* New architectures */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7702" xml:space="preserve">'''NewSQL''' is a class of modern [[relational database management system|relational]] [[database management system]]s that seek to provide the same scalable performance of [[NoSQL]] systems for [[online transaction processing]] (OLTP) read-write workloads while still maintaining the [[ACID]] guarantees of a traditional database system.&lt;ref name="aslett2012"&gt;
{{cite web 
| url = http://cs.brown.edu/courses/cs227/archives/2012/papers/newsql/aslett-newsql.pdf
| title = How Will The Database Incumbents Respond To NoSQL And NewSQL?
| first = Matthew
| last = Aslett
| publisher = 451 Group
| publication-date = 2011-04-04
| year = 2011
| accessdate = 2012-07-06
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext
| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps
| first = Michael
| last = Stonebraker
| publisher = Communications of the ACM Blog
| publication-date = 2011-06-16
| accessdate  = 2012-07-06
}}
&lt;/ref&gt;&lt;ref name="highscalability"&gt;
{{cite web 
| url = http://highscalability.com/blog/2012/9/24/google-spanners-most-surprising-revelation-nosql-is-out-and.html
| title = Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In
| first = Todd
| last = Hoff
| publication-date = 2012-09-24
| accessdate  = 2012-10-07
}}
&lt;/ref&gt;

== History ==
The term was first used by 451 Group analyst Matthew Aslett in a 2011 research paper discussing the rise of new database systems as challengers to established vendors.&lt;ref name="aslett2010" /&gt; Many enterprise systems that handle high-profile data (e.g., financial and order processing systems) also need to be able to scale but are unable to use NoSQL solutions because they cannot give up strong transactional and consistency requirements.&lt;ref name="aslett2010"&gt;{{cite web 
| url = http://blogs.the451group.com/information_management/2011/04/06/what-we-talk-about-when-we-talk-about-newsql/
| title = What we talk about when we talk about NewSQL
| first = Matthew
| last = Aslett
| publisher = 451 Group
| publication-date = 2011-04-06
| year = 2010
| accessdate = 2012-10-07
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://berlinbuzzwords.de/sessions/keynote-0
| title = Building Spanner
| first = Alex
| last = Lloyd
| publisher = Berlin Buzzwords
| publication-date = 2012-06-05
| year = 2012
| accessdate = 2012-10-07
}}&lt;/ref&gt; The only options previously available for these organizations were to either purchase a more powerful single-node machine or develop custom middleware that distributes queries over traditional DBMS nodes. Both approaches are prohibitively expensive and thus are not an option for many. Thus, in this paper, Aslett discusses how NewSQL upstarts are poised to challenge the supremacy of commercial vendors, in particular [[Oracle Database|Oracle]].

== Systems ==
Although NewSQL systems vary greatly in their internal architectures, the two distinguishing features common amongst them is that they all support the [[Relational model|relational data model]] and use [[SQL]] as their primary interface.&lt;ref&gt;{{Cite journal | last1 = Cattell | first1 = R. | title = Scalable SQL and NoSQL data stores | doi = 10.1145/1978915.1978919 | journal = ACM SIGMOD Record | volume = 39 | issue = 4 | pages = 12 | year = 2011 | url = http://cattell.net/datastores/Datastores.pdf| pmid =  | pmc = }}&lt;/ref&gt;
The applications targeted by these NewSQL systems are characterized as having a large number of transactions that (1) are short-lived (i.e., no user stalls), (2) touch a small subset of data using index lookups (i.e., no full table scans or large distributed joins), and (3) are repetitive (i.e. executing the same queries with different inputs).&lt;ref&gt;
{{cite conference
| authorlink = Michael Stonebraker
| first = Mike | last = Stonebraker
| title = The end of an architectural era: (it's time for a complete rewrite
| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases
| location = Vienna, Austria
| year = 2007
| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf
| format = PDF |display-authors=etal}}&lt;/ref&gt; These NewSQL systems achieve high performance and scalability by eschewing much of the legacy architecture of the original [[IBM System R]] design, such as heavyweight [[Algorithms for Recovery and Isolation Exploiting Semantics|recovery]] or [[concurrency control]] algorithms.&lt;ref&gt;{{Cite journal | last1 = Stonebraker | first1 = M. | last2 = Cattell | first2 = R. | doi = 10.1145/1953122.1953144 | title = 10 rules for scalable performance in 'simple operation' datastores | journal = Communications of the ACM | volume = 54 | issue = 6 | pages = 72 | year = 2011 | pmid =  | pmc = }}&lt;/ref&gt; One of the first known NewSQL systems is the [[H-Store]] [[Parallel database|parallel database system]].&lt;ref&gt;
{{cite web 
| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/
| title = Is H-Store the future of database management systems?
| first = Matthew
| last = Aslett
| year = 2008
| publication-date = 2008-03-04
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055
| title = H-Store: Complete destruction of the old DBMS order?
| first = Larry
| last = Dignan
| year = 2008
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

NewSQL systems can be loosely grouped into three categories:
&lt;ref&gt;
{{cite web 
| url = http://www.linuxforu.com/2012/01/newsql-handle-big-data/
| title = NewSQL - The New Way to Handle Big Data
| first =  Prasanna
| last = Venkatesh
| year = 2012
| publication-date = 2012-01-30
| accessdate  = 2012-10-07
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.scalebase.com/the-story-of-newsql/
| title = The NewSQL Market Breakdown
| first = Doron
| last = Levari
| year = 2011
| accessdate  = 2012-04-08
}}
&lt;/ref&gt;

=== New architectures ===
The first type of NewSQL systems are completely new database platforms. These are designed to operate in a distributed cluster of [[Shared nothing architecture|shared-nothing]] nodes, in which each node owns a subset of the data. These databases are often written from scratch with a distributed architecture in mind, and include components such as distributed concurrency control, flow control, and distributed query processing. Example systems in this category are [[Google Spanner]], [[Clustrix]], [[VoltDB]], [[MemSQL]], [[Pivotal Labs|Pivotal]]'s GemFire XD, [[SAP HANA]],&lt;ref&gt;{{cite web|title=SAP HANA|url=http://www.sap.com/pc/tech/data-management/software/extreme-transaction-oltp/index.html|publisher=SAP|accessdate=17 July 2014}}&lt;/ref&gt; [[NuoDB]], [[TiDB]], and [[Trafodion]].&lt;ref&gt;
{{cite web 
| url = http://www.trafodion.org
| title = Trafodion: Transactional SQL-on-HBase
| year = 2014
}}
&lt;/ref&gt;

=== SQL engines ===
The second category are highly optimized [[Database engine|storage engines]] for [[SQL]]. These systems provide the same programming interface as SQL, but scale better than built-in engines, such as [[InnoDB]]. Examples of these new storage engines include [[MySQL Cluster]], [[Infobright]], [[TokuDB]] and the now defunct [[InfiniDB]].

=== Transparent sharding ===
These systems provide a [[Shard (database architecture)|sharding]] [[middleware]] layer to automatically split databases across multiple nodes. [[ScaleBase]] is an example of this type of system.

==See also==
* [[Transaction processing]]
* [[Partition (database)]]

== References ==
{{Reflist|30em}}

{{Databases}}

&lt;!--Categories--&gt;
[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:NewSQL]]</text>
      <sha1>50i33yp6ocxgtqyr9rrz0velbwf4x32</sha1>
    </revision>
  </page>
  <page>
    <title>Category:NewSQL</title>
    <ns>14</ns>
    <id>37256859</id>
    <revision>
      <id>516566229</id>
      <timestamp>2012-10-08T02:19:05Z</timestamp>
      <contributor>
        <username>Apavlo</username>
        <id>1709507</id>
      </contributor>
      <comment>Creating category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="73" xml:space="preserve">{{Cat main|NewSQL}}

[[Category:Databases]] 
[[Category:Data management]]</text>
      <sha1>t8at130tz96tqtxji3yh5gulgbjq3a8</sha1>
    </revision>
  </page>
  <page>
    <title>Data set (IBM mainframe)</title>
    <ns>0</ns>
    <id>1042727</id>
    <revision>
      <id>703227916</id>
      <parentid>703227879</parentid>
      <timestamp>2016-02-04T07:15:08Z</timestamp>
      <contributor>
        <username>DMacks</username>
        <id>712163</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/14.139.120.66|14.139.120.66]] ([[User talk:14.139.120.66|talk]]) to last version by Deeday-UK</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6698" xml:space="preserve">{{about|mainframe computer file|a general meaning in computing field|Data set}}
{{Refimprove|date=September 2014}}

In the context of [[IBM]] [[mainframe computer]]s, a '''data set''' (IBM preferred) or  '''dataset''' is a [[computer file]] having a [[record-oriented file|record organization]]. Use of this term began with [[OS/360]] and is still used by its successors, including the current [[z/OS]]. Documentation for these systems historically preferred this term rather than ''[[computer file|file]]''.

A data set is typically stored on a [[direct access storage device]] (DASD) or [[magnetic tape]], however unit record devices, such as punch card readers, card punch, and line printers can provide input/output (I/O) for a data set (file).&lt;ref&gt;http://publib.boulder.ibm.com/infocenter/zvm/v5r4/index.jsp?topic=/com.ibm.zvm.v54.hcpa7/hcse7b3050.htm&lt;/ref&gt;

Data sets are not unstructured streams of [[byte]]s, but rather are organized in various logical record and block structures determined by the &lt;code&gt;DSORG&lt;/code&gt; (data set organization), &lt;code&gt;RECFM&lt;/code&gt; (record format), and other parameters. These parameters are specified at the time of the data set allocation (creation), for example with [[Job Control Language]] &lt;code&gt;DD&lt;/code&gt; statements. Inside a job they are stored in the [[Data Control Block]] (DCB), which is a data structure used to access data sets, for example using [[access method]]s.

==Data set organization==
For OS/360, the DCB's DSORG parameter specifies how the data set is organized. It may be physically sequential ("PS"), indexed sequential ("IS"), partitioned ("PO"), or Direct Access ("DA"). Data sets on tape may only be DSORG=PS. The choice of organization depends on how the data is to be accessed, and in particular, how it is to be updated.

Programmers utilize various [[access method]]s (such as [[Queued Sequential Access Method|QSAM]] or [[VSAM]]) in programs for reading and writing data sets. Access method depends on the given data set organization.

==Record format (RECFM)==
Regardless of organization, the physical structure of each record is essentially the same, and is uniform throughout the data set. This is specified in the DCB &lt;code&gt;RECFM&lt;/code&gt; parameter. &lt;code&gt;RECFM=F&lt;/code&gt; means that the records are of fixed length, specified via the &lt;code&gt;LRECL&lt;/code&gt; parameter, and &lt;code&gt;RECFM=V&lt;/code&gt; specifies a variable-length record. V records when stored on media are prefixed by a Record Descriptor Word (RDW) containing the integer length of the record in bytes. With &lt;code&gt;RECFM=FB&lt;/code&gt; and &lt;code&gt;RECFM=VB&lt;/code&gt;, multiple logical records are grouped together into a single [[Block (data storage)|physical block]] on tape or disk. FB and VB are &lt;code&gt;fixed-blocked&lt;/code&gt;, and &lt;code&gt;variable-blocked&lt;/code&gt;, respectively. The &lt;code&gt;BLKSIZE&lt;/code&gt; parameter specifies the maximum length of the block. &lt;code&gt;RECFM=FBS&lt;/code&gt; could be also specified, meaning &lt;code&gt;fixed-blocked standard&lt;/code&gt;, meaning all the blocks except the last one were required to be in full &lt;code&gt;BLKSIZE&lt;/code&gt; length. &lt;code&gt;RECFM=VBS&lt;/code&gt;, or &lt;code&gt;variable-blocked spanned&lt;/code&gt;, means a logical record could be spanned across two or more blocks, with flags in the RDW indicating whether a record segment is continued into the next block and/or was continued from the previous one.

This mechanism eliminates the need for using any "delimiter" byte value to separate records. Thus data can be of any type, including binary integers, floating point, or characters, without introducing a false end-of-record condition. The data set is an abstraction of a collection of records, in contrast to files as unstructured streams of bytes.

=={{anchor|Partitioned datasets}}Partitioned data sets==
A '''partitioned data set''' ('''PDS''') is a data set containing multiple ''members'', each of which holds a separate sub-data set, similar to a [[directory (file systems)|directory]] in other types of [[file system]]s. This type of data set is often used to hold executable programs (''load modules''), source program libraries (especially Assembler macro definitions), and [[Job Control Language]]. A PDS may be compared to a [[ZIP (file format)|Zip]] file or [[COM Structured Storage]].

A Partitioned Data Set can only allocate on a single volume with the maximum size of 65535 tracks.

Besides members, a PDS consists also of their directory. Each member can be accessed directly using the directory structure. Once a member is located, the data stored in that member is handled in the same manner as a PS (sequential) data set.

Whenever a member is deleted, the space it occupied is unusable for storing other data. Likewise, if a member is re-written, it is stored in a new spot at the back of the PDS and leaves wasted “dead” space in the middle. The only way to recover “dead” space is to perform frequent file compression, that moves all members to the front of the data space and leaves free usable space at the back.  (Note that in modern parlance, this kind of operation might be called [[defragmentation]] or [[garbage collection (computer science)|garbage collection]]; [[data compression]] nowadays refers to a different, more complicated concept.)  PDS files can only reside on disk in order to use the directory structure to access individual members, not on tape. They are most often used for storing multiple JCL files, utility control statements and executable modules.

An improvement of this scheme is a Partitioned Data Set Extended (PDSE or PDS/E, sometimes just ''libraries'') introduced with [[MVS/XA]] system.

PDS/E structure is similar to PDS and is used to store the same types of data. However, PDS/E files have a better directory structure which does not require pre-allocation of directory blocks when the PDS/E is defined (and therefore does not run out of directory blocks if not enough were specified). Also, PDS/E automatically stores members in such a way that compression operation is not needed to reclaim "dead" space. PDS/E files can only reside on disk in order to use the directory structure to access individual members.

==See also==
* [[Volume table of contents]] (VTOC), a structure describing data sets stored on the disk
* [[Distributed Data Management Architecture]]

==References==
{{Reflist}}
* [http://publib-b.boulder.ibm.com/Redbooks.nsf/RedbookAbstracts/sg246366.html Introduction to the New Mainframe: z/OS Basics], Ch. 5, "Working with data sets", March 29, 2011. ISBN 0738435341

{{Mainframe I/O access methods}}

{{DEFAULTSORT:Data Set (IBM Mainframe)}}
[[Category:Data management]]
[[Category:IBM mainframe operating systems]]
[[Category:Computer file systems]]
[[Category:Computer files]]</text>
      <sha1>pn0k1cl4n1a1pejkb9wab7btxi3d2k7</sha1>
    </revision>
  </page>
  <page>
    <title>Consistency (database systems)</title>
    <ns>0</ns>
    <id>1140830</id>
    <revision>
      <id>762789251</id>
      <parentid>762780282</parentid>
      <timestamp>2017-01-30T19:25:29Z</timestamp>
      <contributor>
        <username>Andy Dingley</username>
        <id>3606755</id>
      </contributor>
      <comment>rv unsourced fragment</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4025" xml:space="preserve">{{merge from|Data consistency|date=November 2014}}{{About||Consistency in distributed systems as defined in the CAP Theorem|CAP theorem}}

'''Consistency''' in [[database systems]] refers to the requirement that any given [[database transaction]] must change affected data only in allowed ways. Any data written to the database must be valid according to all defined rules, including [[Integrity constraints|constraints]], [[Cascading rollback|cascades]], [[Database trigger|triggers]], and any combination thereof.  This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any programming errors cannot result in the violation of any defined rules.

==As an ACID guarantee==
Consistency is one of the four guarantees that define [[ACID]] [[database transaction|transactions]]; however, significant ambiguity exists about the nature of this guarantee. It is defined variously as:
* The guarantee that any transactions started in the future necessarily see the effects of other transactions committed in the past&lt;ref name="CAP Theorem Paper"&gt;http://webpages.cs.luc.edu/~pld/353/gilbert_lynch_brewer_proof.pdf "Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services"&lt;/ref&gt;&lt;ref name="Ports et al"&gt;{{cite journal | url=http://drkp.net/papers/txcache-osdi10.pdf | title=Transactional Consistency and Automatic Management in an Application Data Cache |author1=Ports, D.R.K |author2=Clements, A.T |author3=Zhang, I |author4=Madden, S |author5=Liskov, B. | journal=MIT CSAIL}}&lt;/ref&gt;
* The guarantee that [[Relational database#Constraints|database constraints]] are not violated, particularly once a transaction commits&lt;ref name="Haerder &amp; Reuter"&gt;{{cite journal | url=http://www.minet.uni-jena.de/dbis/lehre/ws2005/dbs1/HaerderReuter83.pdf | title=Principles of Transaction-Oriented Database Recovery |author1=Haerder, T |author2=Reuter, A. | journal=Computing Surveys |date=December 1983  | volume=15 | issue=4 | pages=287–317}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://databases.about.com/od/specificproducts/a/acid.htm|title=The ACID Model|author=Mike Chapple|work=About}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/aa480356.aspx|title=ACID properties|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.techopedia.com/definition/23949/atomicity-consistency-isolation-durability-acid|title=What is ACID in Databases? - Definition from Techopedia|author=Cory Janssen|work=Techopedia.com}}&lt;/ref&gt;
* The guarantee that operations in transactions are performed accurately, correctly, and with validity, with respect to application semantics&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=27614|title=ISO/IEC 10026-1:1998 - Information technology -- Open Systems Interconnection -- Distributed Transaction Processing -- Part 1: OSI TP Model|publisher=}}&lt;/ref&gt;

As these various definitions are not mutually exclusive, it is possible to design a system that guarantees "consistency" in every sense of the word, as most [[relational database management system]]s in common use today arguably do.

==As a CAP trade-off==

The [[CAP theorem]] is based on three trade-offs, one of which is "atomic consistency" (shortened to "consistency" for the acronym), about which the authors note, "Discussing atomic consistency is somewhat different than talking about an ACID database, as database consistency refers to transactions, while atomic consistency refers only to a property of a single request/response operation sequence. And it has a different meaning than the Atomic in ACID, as it subsumes the database notions of both Atomic and Consistent."&lt;ref name="CAP Theorem Paper" /&gt;

==See also==
* [[Consistency model]]
* [[CAP theorem]]
* [[Eventual consistency]]

==References==
{{reflist}}

{{DEFAULTSORT:Consistency (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>qch9svk3ngj6av0asrkl6x4fzwhkwzk</sha1>
    </revision>
  </page>
  <page>
    <title>SQL/PSM</title>
    <ns>0</ns>
    <id>11665200</id>
    <revision>
      <id>746720088</id>
      <parentid>712337419</parentid>
      <timestamp>2016-10-29T03:32:26Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5455" xml:space="preserve">{{infobox programming language
| name                   = SQL/PSM
| logo                   =
| paradigm               = [[multi-paradigm programming language|Multi-paradigm]]
| year                   = 1996
| designer               =
| developer              =
| latest_release_version = [[SQL:2011]]
| latest_release_date    = 
| latest_preview_version = 
| latest_preview_date    = 
| turing-complete        = Yes
| typing                 = 
| implementations        = [[MySQL]] &lt;br /&gt;IBM's [[SQL PL]]
| influenced_by          = [[Ada (programming language)|Ada]]&lt;ref&gt;{{Citation | url = http://ocelot.ca/blog/blog/2015/01/15/stored-procedures-critiques-and-defences/ | title = Stored Procedures: critiques and defences | year = 2015 | first1 = Peter | last1 = Gulutzan }}&lt;/ref&gt;
| influenced             = 
| operating_system       = [[Cross-platform|Cross-platform (multi-platform)]]
| license                =
| website                = 
| file_ext               =
| dialects               =
| wikibooks              = 
}}

'''SQL/PSM''' ([[SQL]]/Persistent Stored Modules) is an [[ISO standard]] mainly defining an extension of SQL with a [[procedural language]] for use in [[stored procedure]]s. Initially published in 1996 as an extension of [[SQL-92]] (ISO/IEC 9075-4:1996, a version sometimes called PSM-96 or even SQL-92/PSM&lt;ref&gt;{{Cite journal | last1 = Eisenberg | first1 = A. | title = New standard for stored procedures in SQL | doi = 10.1145/245882.245907 | journal = ACM SIGMOD Record | volume = 25 | issue = 4 | pages = 81-88| year = 1996 | pmid =  | pmc = }}&lt;/ref&gt;), SQL/PSM was later incorporated into the multi-part [[SQL:1999]] standard, and has been part 4 of that standard since then, most recently in [[SQL:2011]].  The SQL:1999 part 4 covered less than the original PSM-96 because the SQL statements for defining, managing, and invoking routines were actually incorporated into part 2 SQL/Foundation, leaving only the procedural language itself as SQL/PSM.&lt;ref&gt;{{cite book| first1 =Jim | last1 = Melton | first2 =Alan R | last2 = Simon | title = SQL: 1999|year=2002| publisher = Morgan Kaufmann|isbn= 978-1-55860-456-8 | pages = 541–42}}&lt;/ref&gt; The SQL/PSM facilities are still optional as far as the SQL standard is concerned; most of them are grouped in Features P001-P008.

SQL/PSM standardizes syntax and semantics for [[control flow]], [[exception handling]] (called "condition handling" in SQL/PSM), local variables, assignment of expressions to variables and parameters, and (procedural) use of [[Cursor (databases)|cursors]]. It also defines an information schema ([[metadata]]) for stored procedures.  SQL/PSM is one language in which [[Method (computer programming) |methods]] for the SQL:1999 [[structured type]]s can be defined.  The other is Java, via [[SQL/JRT]].

In practice [[MySQL]]'s procedural language and IBM's [[SQL PL]] (used in DB2) are closest to the SQL/PSM standard.&lt;ref name = "HarrisonFeuerstein2008"&gt;{{cite book | first1 = Guy | last1 = Harrison| first2 = Steven | last2 = Feuerstein|title=MySQL Stored Procedure Programming|url= https://books.google.com/books?id=YpeP0ok0cO4C&amp;pg=PT75 | year=2008|publisher=O'Reilly |isbn = 978-0-596-10089-6 |page= 49}}&lt;/ref&gt; 

SQL/PSM resembles and inspired by [[PL/SQL]], as well as [[PL/pgSQL]], so they are similar languages.  With [[PostgreSQL]] v9 some SQL/PSM features, like overloading of SQL-invoked functions and procedures&lt;ref&gt;{{Citation | publisher = PostgreSQL | title = SQL standard features | edition = 9 | contribution-url = http://www.postgresql.org/docs/9.0/static/features-sql-standard.html | contribution = feature T322}}.&lt;/ref&gt; are now supported.  A [[PostgreSQL]] addon implements SQL/PSM&lt;ref&gt;{{Citation | url = https://github.com/okbob/plpsm0 | format = git | type = repository | title = plpsm0}}.&lt;/ref&gt;&lt;ref&gt;{{Citation | publisher = PostgreSQL | url = http://www.postgresql.org/message-id/1305291347.14548.13.camel@jara.office.nic.cz | date = May 2011 | title = Announce}}.&lt;/ref&gt;&lt;ref&gt;[http://www.postgresql.org/message-id/CAFj8pRDWFdcjNSnwQB_3j1-rMO6b8=TmLTNBvDCSpRrOW2Dfeg@mail.gmail.com 2012-2's Proposal PL/pgPSM announce]&lt;/ref&gt;&lt;ref&gt;{{Citation | title = SQL/PSM | format = wiki | url = http://postgres.cz/wiki/SQL/PSM_Manual | publisher = PostgreSQL | type = manual | year = 2008}}.&lt;/ref&gt; (alongside its own procedural language), although it is not part of the core product.&lt;ref&gt;{{Citation | contribution-url = http://www.postgresql.org/docs/9.2/static/features.html | publisher = PostgreSQL | title = Documentation | edition = 9.2 | contribution = SQL Conformance}}.&lt;/ref&gt;

==See also==
The following implementations adopt the standard, but they are not 100% compatible to SQL/PSM:

[[Open source]]:
* [[HSQLDB]] stored procedures and functions&lt;ref name="SQL/PSM routines"&gt;http://hsqldb.org/doc/2.0/guide/sqlroutines-chapt.html#src_psm_routines&lt;/ref&gt;
* [[MySQL]] stored procedures &lt;ref name="HarrisonFeuerstein2008"/&gt;
* [[PostgreSQL]] [[PL/pgSQL]]

Proprietary:
* Oracle [[PL/SQL]]
* Microsoft and Sybase [[Transact-SQL]]

==References==
{{reflist}}

==Further reading==
* Jim Melton, ''Understanding SQL's Stored Procedures: A Complete Guide to SQL/PSM'', Morgan Kaufmann Publishers, 1998, ISBN 1-55860-461-8

{{SQL}}

__NOTOC__

{{DEFAULTSORT:SQL PSM}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Data-centric programming languages]]
[[Category:Programming languages created in 1996]]


{{compu-lang-stub}}
{{database-stub}}</text>
      <sha1>23s3pf4l2kcfnf6rtrynjw02n3t9dlj</sha1>
    </revision>
  </page>
  <page>
    <title>Approximate inference</title>
    <ns>0</ns>
    <id>39019965</id>
    <revision>
      <id>603934296</id>
      <parentid>564359964</parentid>
      <timestamp>2014-04-12T22:03:41Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>fixed [[Help:CS1 errors#bad_date|CS1 errors: dates]] to meet [[MOS:DATEFORMAT]] (also [[WP:AWB/GF|General fixes]]) using [[Project:AWB|AWB]] (10069)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1158" xml:space="preserve">'''Approximate inference''' methods make it possible to learn realistic models from [[big data]] by trading off computation time for accuracy, when exact learning and [[inference]] are [[computationally intractable]].

==Major methods classes ==

*[[Variational Bayesian method]]s 
*[[Expectation propagation]]
*[[Markov random field]]s 
*[[Bayesian network]]s
**[[Variational message passing]]
*loopy and generalized [[belief propagation]] 
&lt;ref&gt;{{cite journal|url=http://academic.research.microsoft.com/Paper/14666.aspx|title=Approximate Inference and Constrained Optimization|journal=Uncertainty in Artificial Intelligence - UAI|pages=313–320|year=2003}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://mlg.eng.cam.ac.uk/zoubin/approx.html|title=Approximate Inference|accessdate=2013-07-15}}&lt;/ref&gt;

==See also==
*[[Statistical inference]]
*[[fuzzy logic]]
*[[data mining]]

==References==
{{reflist}}

==External links==
*{{cite web|url=http://videolectures.net/mlss09uk_minka_ai/|title=Machine Learning Summer School (MLSS), Cambridge 2009, Approximate Inference|author= Tom Minka, Microsoft Research|date=Nov 2, 2009|type=video lecture}}

[[Category:Data management]]</text>
      <sha1>4mc17z7p7molc85ufvs8evuhpxr0hfy</sha1>
    </revision>
  </page>
  <page>
    <title>System of record</title>
    <ns>0</ns>
    <id>2100046</id>
    <revision>
      <id>753419366</id>
      <parentid>740941382</parentid>
      <timestamp>2016-12-07T01:36:37Z</timestamp>
      <contributor>
        <username>Mgibby5</username>
        <id>17240541</id>
      </contributor>
      <minor />
      <comment>grammar, caps.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3114" xml:space="preserve">A '''system of record''' (SOR) or '''Source System of Record''' (SSoR) is a [[Data Management|data management]] term for an [[information]] storage system (commonly implemented on a [[computer system]]) that is the authoritative data source for a given [[data element]] or piece of information. The need to identify systems of record can become acute in organizations where [[management information system]]s have been built by taking output data from multiple source systems, re-processing this data, and then re-presenting the result for a new business use.

In these cases, multiple information systems may disagree about the same piece of information. These disagreements may stem from semantic differences, differences in opinion, use of different sources, differences in the timing of the [[Extract, transform, load|ETL]] extracts that create the data they report against, or may simply be the result of bugs. 

The [[integrity]] and [[validity]] of any data set is open to question when there is no [[tracing (software)|traceable]] connection to a good source, such as a known System of Record. Where the integrity of the data is vital, if there is an agreed system of record, the data element must either be linked to, or extracted directly from it. In other cases, the provenance and estimated data quality should be documented. 

The "system of record" approach is a good fit for environments where both:
* there is a single authority over all data consumers, and 
* all consumers have similar needs 
In diverse environments, one instead needs to support the presence of multiple opinions. Consumers may accept different authorities or may differ on what constitutes an authoritative source -- researchers may prefer carefully vetted data, while tactical military systems may require the most recent credible report.

==See also==
* [[Single Source of Truth]] practice of using one source for a particular data element
* [[Privacy Act of 1974]] United States law including requirement for agencies to publish System Of Records Notices (SORN) in the [[Federal Register]] to identify the system and describe the use of individuals data.
* [[Master Data Management]] defining the handling of master data
* [[Systems of Engagement]] — more decentralized systems that incorporate technologies which encourage peer interactions

==References==
* {{cite web |title=The System of Record in the Global Data Warehouse | url=http://www.information-management.com/issues/20030501/6645-1.html  |publisher = Information Management |accessdate=2007-12-18 |author=[[Bill Inmon]]  |date=May 2003 |work= }}
* {{cite web 
| title = The Golden Copy 
| url = http://adam.goucher.ca/?p=72
| last = Goucher
| first = Adam
| date = 2006-04-26
| accessdate = 2013-04-30}}
* {{cite web
| title = The Move from Systems of Record to Systems Of Engagement
| url = http://www.forbes.com/sites/joshbersin/2012/08/16/the-move-from-systems-of-record-to-systems-of-engagement/
| last = Bersin
| first = Josh
| date = 2012-08-16
| accessdate = 2013-04-30}}


[[Category:Information systems]]
[[Category:Data management]]


{{compu-stub}}</text>
      <sha1>ck4sf64teutbm27qeiwzv8vw98mqc6n</sha1>
    </revision>
  </page>
  <page>
    <title>Single source of truth</title>
    <ns>0</ns>
    <id>6831362</id>
    <revision>
      <id>757107301</id>
      <parentid>733619064</parentid>
      <timestamp>2016-12-28T21:06:08Z</timestamp>
      <contributor>
        <ip>69.4.118.251</ip>
      </contributor>
      <comment>SPoT is NOT the same thing as SSoT.  SPOT is where sources merge to create a single point of truth and a POINT of Truth isn't always a or the SOURCE.  May be out dated but not sure there is really a SSOT these days since our enterprises are so diverse.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7081" xml:space="preserve">{{Unreferenced|date=December 2009}}

In [[information systems]] design and theory, '''single source of truth''' ('''SSOT'''), is the practice of structuring information models and associated [[Database schema|schemata]] such that every data element is stored exactly once (e.g., in no more than a single row of a single table).  Any possible linkages to this data element (possibly in other areas of the relational schema or even in distant [[federated database|federated databases]]) are by [[Reference (computer science)|reference]] only.  Because all other locations of the data just refer back to the primary "source of truth" location, updates to the data element in the primary location propagate to the entire system without the possibility of a duplicate value somewhere being forgotten.

Deployment of an SSOT architecture is becoming increasingly important in enterprise settings where incorrectly linked duplicate or de-normalized data elements (a direct consequence of intentional or unintentional [[denormalization]] of any explicit data model) poses a risk for retrieval of outdated, and therefore incorrect, information.  A common example would be the [[electronic health record]], where it is imperative to accurately validate patient identity against a single referential repository, which serves as the SSOT.  Duplicate representations of data within the enterprise would be implemented by the use of [[pointer (computer programming)|pointer]]s rather than duplicate database tables, rows, or cells.  This ensures that data updates to elements in the authoritative location are comprehensively distributed to all [[federated database]] constituencies in the larger overall enterprise architecture.{{fact|date=July 2012}}

SSOT systems provide data that is authentic, relevant, and referable.&lt;ref&gt;IBM Smarter Planet - Operational risk management for financial services[http://www.ibm.com/smarterplanet/za/en/banking_technology/nextsteps/solution/Z017038Z16405R75.html]&lt;/ref&gt;

==Implementation==
The "ideal" implementation of SSOT as described above is rarely possible in most enterprises. This is because many organisations have multiple information systems, each of which needs access to data relating to the same entities (e.g., customer). Often these systems are purchased "off-the-shelf" from vendors and cannot be modified in non-trivial ways. Each of these various systems therefore needs to store its own version of common data or entities, and therefore each system must retain its own copy of a record (hence immediately violating the SSOT approach defined above). For example, an ERP (enterprise resource planning) system (such as SAP or Oracle e-Business Suite) may store a customer record; the CRM (customer relationship management) system also needs a copy of the customer record (or part of it) and the warehouse despatch system might also need a copy of some or all of the customer data (e.g., shipping address). In cases where vendors do not support such modifications, it is not always possible to replace these records with pointers to the SSOT.

For organisations (with more than one information system) wishing to implement a Single Source of Truth (without modifying all but one master system to store pointers to other systems for all entities), three supporting technologies are commonly used:{{fact|date=July 2012}}

*[[Enterprise service bus]] (ESB)
*[[Master data management]] (MDM)
*[[Data warehouse]] (DW)

===Enterprise service bus (ESB)===
An enterprise service bus (ESB) allows any number of systems in an organisation to receive updates of data that has changed in another system. To implement a Single Source of Truth, a single source system of correct data for any entity must be identified. Changes to this entity (creates, updates, and deletes) are then published via the ESB; other systems which need to retain a copy of that data subscribe to this update, and update their own records accordingly. For any given entity, the master source must be identified (sometimes called the Golden Record). It should be noted that any given system could publish (be the source of truth for) information on a particular entity (e.g., customer) and also subscribe to updates from another system for information on some other entity (e.g., product).{{fact|date=July 2012}}

An alternative approach is point-to-point data updates, but these become exponentially more expensive to maintain as the number of systems increases, and this approach is increasingly out of favour as an IT architecture.{{fact|date=July 2012}}

===Master data management (MDM)===
An MDM system can act as the source of truth for any given entity that might not necessarily have an alternative "source of truth" in another system. Typically the MDM acts as a hub for multiple systems, many of which could allow (be the source of truth for) updates to different aspects of information on a given entity. For example, the CRM system may be the "source of truth" for most aspects of the customer, and is updated by a call centre operator. However, a customer may (for example) also update their address via a customer service web site, with a different back-end database from the CRM system. The MDM application receives updates from multiple sources, acts as a broker to determine which updates are to be regarded as authoritative (the Golden Record) and then syndicates this updated data to all subscribing systems. The MDM application normally requires an ESB to syndicate its data to multiple subscribing systems.&lt;ref&gt;BAYT Job Site - June 2014[http://www.bayt.com/en/specialties/q/7370/what-are-the-top-business-processes-and-applications-that-need-master-data-management/]&lt;/ref&gt;
[[Customer Data Integration]] (CDI), as a common application of Master Data Management, is sometimes abbreviated CDI-MDM.{{fact|date=July 2012}}

===Data warehouse (DW)===
While the primary purpose of a data warehouse is to support reporting and analysis of data that has been combined from multiple sources, the fact that such data has been combined (according to business logic embedded in the [[Extract, transform, load|data transformation and integration processes]]) means that the data warehouse is often used as a ''de facto'' SSOT. Generally, however, the data available from the data warehouse is not used to update other systems; rather the DW becomes the "single source of truth" for reporting to multiple stakeholders. In this context, the Data Warehouse is more correctly referred to as a "[[single version of the truth]]" since other versions of the truth exist in its operational data sources (no data originates in the DW;  it is simply a reporting mechanism for data loaded from operational systems).{{fact|date=July 2012}}

==See also==
*[[Don't repeat yourself]] (DRY)
*[[SOLID (object-oriented design)]]
*[[Database normalization]]
*[[Single version of the truth]]
*[[System of record]]

==References==
{{reflist}}

==External links==


{{DEFAULTSORT:Single Source Of Truth}}
[[Category:Data modeling]]
[[Category:Database normalization]]
[[Category:Data management]]</text>
      <sha1>2p4drvlotdg4yulxms33xotftpxjne3</sha1>
    </revision>
  </page>
  <page>
    <title>Data room</title>
    <ns>0</ns>
    <id>1216068</id>
    <revision>
      <id>755124661</id>
      <parentid>755124485</parentid>
      <timestamp>2016-12-16T10:27:39Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <minor />
      <comment>Refspam</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2918" xml:space="preserve">{{Refimprove|date=March 2015}}
'''Data rooms''' are spaces used for housing data, usually of a secure or privileged nature. They can be physical data rooms, [[virtual data room]]s, or [[data centers]].&lt;ref&gt;{{cite web|title=Data Room (entry)|url=http://financial-dictionary.thefreedictionary.com/Data+room|website=Financial Dictionary, The Free Dictionary by Farlex}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Data Room (entry)|url=http://www.nasdaq.com/investing/glossary/d/data-room|website=Nasdaq}}&lt;/ref&gt; They are used for a variety of purposes, including data storage, document exchange, file sharing, financial transactions, legal transactions, and more.

In mergers and acquisitions, the traditional data room will literally be a physically secure continually monitored room, normally in the vendor’s offices (or those of their lawyers), which the bidders and their advisers will visit in order to inspect and report on the various documents and other data made available. Often only one bidder at a time will be allowed to enter and if new documents or new versions of documents are required these will have to be brought in by [[courier]] as [[hardcopy]]. Teams involved in large [[due diligence]] processes will typically have to be flown in from many regions or countries and remain available throughout the process.  Such teams often comprise a number of experts in different fields and so the overall cost of keeping such groups on call near to the data room is often extremely high.  Combating the significant cost of physical datarooms is the [[virtual data room]], which provides for the secure, online dissemination of confidential information.

A [[virtual data room]] (VDR) is essentially a website with limited controlled access (using a secure log-on supplied by the vendor/authority which can be disabled at any time by the vendor/authority if a bidder withdraws) to which the bidders and their advisers are given access. Much of the information released will be confidential and restrictions should be applied to the viewers' ability to release this to third parties by forwarding, copying or printing. [[Digital rights management]] is sometimes applied to control information.

Detailed auditing must be provided for legal reasons so that a record is kept of who has seen which version of each document.

Data rooms are commonly used by [[legal]], [[accounting]], [[investment banking]] and [[private equity]] companies performing [[mergers and acquisitions]], [[fundraising]], [[insolvency]], [[corporate restructuring]], and joint ventures including bio-technology and tender processes.

==References==
{{Reflist}}
* [http://www.imaa-institute.org/docs/kummer-sliskovic_do%20virtual%20data%20rooms%20add%20value%20to%20the%20mergers%20and%20acquisitions%20process.pdf A report about the advantages and disadvantages of virtual vs. physical data rooms]

{{DEFAULTSORT:Data Room}}
[[Category:Data management]]</text>
      <sha1>otte3oxkfxd6vf4onb4jxb34ny6f274</sha1>
    </revision>
  </page>
  <page>
    <title>Data thinking</title>
    <ns>0</ns>
    <id>40598793</id>
    <revision>
      <id>751623617</id>
      <parentid>584787059</parentid>
      <timestamp>2016-11-26T21:48:57Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="647" xml:space="preserve">{{unreferenced|date=September 2013}}
'''Data thinking''' is the generic mental pattern observed during the processes of picking a subject to start with, identifying its parts or components, organizing and describing them in an informative fashion that is relevant to what motivated and initiated the whole processes.

The term was created by Mario Faria and Rogerio Panigassi in 2013 when they were writing a book about data science, [[data analysis|data analytics]], data management and how data practitioners were able to achieve their goals.

Mario Faria is one of the first [[Chief Data Officer]]s in the world.



[[Category:Data management]]</text>
      <sha1>5k54s6sjbsvor48c51ocx3cdx3n1fre</sha1>
    </revision>
  </page>
  <page>
    <title>ISO/IEC JTC 1/SC 32</title>
    <ns>0</ns>
    <id>41418778</id>
    <revision>
      <id>671602052</id>
      <parentid>671601888</parentid>
      <timestamp>2015-07-15T19:37:22Z</timestamp>
      <contributor>
        <username>Katieburkhardt</username>
        <id>25402509</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14915" xml:space="preserve">'''ISO/IEC JTC 1/SC 32 Data management and interchange''' is a [[standardization]] subcommittee of the Joint Technical Committee [[ISO/IEC JTC1|ISO/IEC JTC 1]] of the [[International Organization for Standardization]] (ISO) and the [[International Electrotechnical Commission]] (IEC), which develops and facilitates standards within the field of data management and interchange. The international [[Secretariat (administrative office)|secretariat]] of ISO/IEC JTC 1/SC 32 is the [[American National Standards Institute]] (ANSI) located in the United States.&lt;ref name=countries&gt;{{cite web| title=ISO/IEC JTC 1/SC 32 - Data management and interchange| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee_participation.htm?commid=45342| author=ISO| accessdate=2013-10-03}}&lt;/ref&gt;

==History==
ISO/IEC JTC 1/SC 32 was formed in 1997, as a combination of the following three ISO/IEC JTC 1 subgroups: ISO/IEC JTC 1/SC 21/WG 3, Database; ISO/IEC JTC 1/SC 14, Data elements; and ISO/IEC JTC 1/SC 30, Open-edi. The new subcommittee was established with the intention of developing, and facilitating the development of, standards for data management within local and distributed information system environments.&lt;ref name=briefing&gt;{{cite news| title=Information technology: ISO/IEC JTC 1/SC 32, Data Management and Interchange| type=Briefings| author1=Cannan, Stephen| author2=Melton, Jim| journal=ISO Bulletin| date=January 2000| url=http://jtc1sc32.org/doc/N0601-0650/32N0607.pdf| pages=3–4| volume=31| issue=1}}&lt;/ref&gt; ISO/IEC JTC 1/SC 32 was originally made up of five working groups (WGs), though ISO/IEC JTC 1/SC 32/WG 5, Database access and interchange, was disbanded in March 2002.&lt;ref&gt;{{cite report| type=Business Plan Draft| title=Draft Business Plan for ISO/IEC JTC 1/SC32, Data Management and Interchange| author=Mann, Douglas| accessdate=2013-10-04| url=http://jtc1sc32.org/doc/N0751-0800/32N0783.pdf| date=2002-04-04|page=4}}&lt;/ref&gt; The four other original working groups of the subcommittee are currently active, although the title of ISO/IEC JTC 1/SC 32/WG 1 was changed from Open-edi to its current title, e-Business.&lt;ref name=briefing/&gt;

==Scope==
The scope of ISO/IEC JTC 1/SC 32 is “Standards for data management within and among local and distributed information systems environments. SC 32 provides enabling technologies to promote harmonization of data management facilities across sector-specific areas. Specifically, SC32 standards include:”&lt;ref name=business2012&gt;{{cite report| type=Business Plan| url=http://jtc1info.org/wp-content/uploads/2013/03/SC-32-Business-Plan-2012.pdf| accessdate=2013-10-03| author=Melton, Jim| date=2012-10-02| title=Business Plan for JTC1/SC32: 2012-2013}}&lt;/ref&gt;
* Reference models and frameworks for the coordination of existing and emerging standards
* Definition of data domains, data types, and data structures, and their associated semantics
* Languages, services, and protocols for persistent storage, concurrent access and concurrent update, and interchange of data
* Methods, languages, services, and protocols to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce

==Structure==
ISO/IEC JTC 1/SC 32 is made up of four active working groups, each of which carries out specific tasks in standards development within the field of data management and interchange. As a response to changing standardization needs, working groups of ISO/IEC JTC 1/SC 32 can be disbanded if their area of work is no longer applicable, or established if new working areas arise. The focus of each working group is described in the group’s terms of reference. Active working groups of ISO/IEC JTC 1/SC 32 are:&lt;ref name=business2012/&gt;&lt;ref&gt;{{cite web| title=ISO/IEC JTC 1/SC 32 Data management and interchange| author=ISO| accessdate=2013-10-03| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342}}&lt;/ref&gt;
{| class="wikitable" width="60%"
! width="20%" | Working Group
! width="40%" | Working Area
|-
| ISO/IEC JTC 1/SC 32/WG 1 || [[Electronic business|e-Business]]
|-
|ISO/IEC JTC 1/SC 32/WG 2	|| [[Metadata]]
|-
|ISO/IEC JTC 1/SC 32/WG 3	|| [[Database#Database languages|Database languages]]
|-
|ISO/IEC JTC 1/SC 32/WG 4 || [[SQL]] Multimedia and application packages
|-
|}

==Collaborations==
ISO/IEC JTC 1/SC 32 works in close collaboration with a number of other organizations or subcommittees, both internal and external to ISO or IEC, in order to avoid conflicting or duplicative work. Organizations internal to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:&lt;ref name=business2012/&gt;&lt;ref&gt;{{cite web| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342| title=ISO/IEC JTC 1/SC 32 Data management and interchange| accessdate=2013-10-03| author=ISO}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=SC32 Liaison Organizations| accessdate=2013-10-03| author=ISO/IEC JTC 1/SC 32| url=http://jtc1sc32.org/}}&lt;/ref&gt;
* [[ISO/IEC JTC 1/SC 7]], Software and systems engineering
* [[ISO/IEC JTC 1/SC 25]], Interconnection of information technology equipment
* [[ISO/IEC JTC 1/SC 38]], Cloud Computing and Distributed Platforms
* ISO/TC 12, Quantities and units
* [[ISO/TC 37]], Terminology and other language and content resources 
* ISO/TC 37/SC 2, Terminographical and lexicographical working methods
* ISO/TC 37/SC 3, Systems to manage terminology, knowledge and content
* ISO/TC 37/SC 4, Language resource management
* ISO/TC 46/SC 4, Technical interoperability
* ISO/TC 46/SC 11, Archives/records management
* ISO/TC 68/SC 2, Financial Services, security
* ISO/TC 127, Earth-moving machinery
* ISO/TC 154, Processes, data elements and documents in commerce, industry and administration
* ISO/TC 184, Automation systems and integration
* [[ISO TC 184/SC 4|ISO/TC 184/SC 4]], Industrial data
* ISO/TC 204, Intelligent transport systems
* [[ISO/TC 211]], Geographic information/Geomatics
* [[ISO/TC 215]], Health informatics
* ISO/TC 232, Learning services outside formal education

Some organizations external to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:
* [[Confédération Internationale des Sociétés d´Auteurs et Compositeurs|International Confederation of Societies of Authors and Composers]] (CISAC)
* [[Dublin Core Metadata Initiative]] (DCMI)
* [[EUROSTAT]]
* [[International Telecommunications Satellite Organization]] (ITSO)
* [[International Telecommunication Union|ITU]]
* [[Infoterm]]
* [[Object Management Group]] (OMG)
* [[Society for Worldwide Interbank Financial Telecommunication]] (SWIFT)
* [[UN/CEFACT]]
* [[United Nations Economic Commission for Europe]] (UNECE)
* [[World Meteorological Organization]] (WMO)
* [[W3C]]

==Member countries==
Countries pay a fee to ISO to be members of subcommittees.&lt;ref&gt;{{cite manual| url=http://www.iso.org/iso/iso_membership_manual_2012.pdf| pages=-18| chapter=III. What Help Can I Get from the ISO Central Secretariat?| title=ISO Membership Manual| author=ISO| date=June 2012| accessdate=2013-07-12| publisher=ISO}}&lt;/ref&gt;

The 14 "P" (participating) members of ISO/IEC JTC 1/SC 32 are: Canada, China, Czech Republic, Côte d'Ivoire, Egypt, Finland, Germany, India, Japan, Republic of Korea, Portugal, Russian Federation, United Kingdom, and United States.&lt;ref name=countries/&gt;

The 22 "O" (observing) members of ISO/IEC JTC 1/SC 32 are: Australia, Austria, Belgium, Bosnia and Herzegovina, France, Ghana, Hungary, Iceland, Indonesia, Islamic Republic of Iran, Ireland, Italy, Kazakhstan, Luxembourg, Netherlands, Norway, Poland, Romania, Serbia, Spain, Switzerland, and Turkey.

==Published standards==
ISO/IEC JTC 1/SC 32 standards are meant to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce.&lt;ref name=briefing/&gt; ISO/IEC JTC 1/SC 32 currently has 74 published standards within the field of data management and interchange, including:&lt;ref&gt;{{cite web| title=ISO/IEC JTC 1/SC 32| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_tc_browse.htm?commid=45342&amp;published=on| accessdate=2013-10-03| author=ISO}}&lt;/ref&gt;&lt;ref&gt;{{cite web| publisher=ISO| url=http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html| title=Freely Available Standards| accessdate=2013-09-26}}&lt;/ref&gt;
{| class="wikitable sortable" width="100%"
! data-sort-type="number" width="14%" | ISO/IEC Standard
! width="29%" | Title
! width="6%" | Status
! width="49%" | Description
! width= "2%" | WG
|-
|data-sort-value="14662"|ISO/IEC 14662 [http://standards.iso.org/ittf/licence.html free] || Information technology – Open-edi reference model || Published (2010) || Specifies the framework for coordinating the integration of existing International Standards and the development of future International Standards for the interworking of Open-edi parties through Open-edi&lt;ref&gt;{{cite journal| title=ISO/IEC 14662| date=2010-02-15| author=ISO| edition=3| page=1}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=ISO/IEC 14662:2010| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55290| date=2010-02-02| author=ISO}}&lt;/ref&gt; || 1
|-
|data-sort-value="15944"|ISO/IEC 15944-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Business Operational View – Part 1: Operational aspects of Open-edi for implementation || Published (2011) || Allows constraints, including legal requirements, commercial and/or international trade and contract terms, public policy, and laws and regulations, to be defined and integrated into Open-edi through the business operational view (BOV)&lt;ref&gt;{{cite journal| title=ISO/IEC 15944-1| date=2011-08-01| author=ISO| edition=2| page=1}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=ISO/IEC 15944-1:2011| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55289| date=2011-07-21| author=ISO}}&lt;/ref&gt; || 1
|-
|data-sort-value="11179"|[[ISO/IEC 11179]]-3 [http://standards.iso.org/ittf/licence.html free] || Information technology – [[metadata registry|Metadata registries]] (MDR) – Part 3: Registry metamodel and basic attributes || Published (2013) || Specifies the structure of a metadata registry in the form of a conceptual data model and specifies basic attributes which are required to describe metadata items&lt;ref&gt;{{cite journal| page=1| date=2003-02-15| title=ISO/IEC 11179-3| author=ISO| edition=2}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=ISO/IEC 11179-3:2013| accessdate=2013-10-03| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=50340| date=2013-02-12| author=ISO}}&lt;/ref&gt; || 2
|-
|data-sort-value="20943"| ISO/IEC TR 20943-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Procedures for achieving metadata registry content consistency – Part 1: Data elements || Published (2003) || “Describes a set of procedures for the consistent registration of data elements and their attributes in a registry.”&lt;ref&gt;{{cite journal| title=ISO/IEC TR 20943-1| author=ISO| page=1| date=2003-08-01| edition=1}}&lt;/ref&gt;&lt;ref&gt;{{cite web| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=34343| date=2012-12-19| title=ISO/IEC TR 20943-1:2003| author=ISO}}&lt;/ref&gt; || 2
|-
|data-sort-value="20944"| ISO/IEC 20944-1 || Information technology – Metadata Registries Interoperability and Bindings (MDR-IB) – Part 1: Framework, common vocabulary, and common provisions for conformance || Published (2013) || Contains the overview, framework, common vocabulary, and common provisions for conformance for the ISO/IEC 20944 series, which provides the bindings and their interoperability for MDRs&lt;ref&gt;{{cite web| author=ISO| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=51914| date=2013-01-08| accessdate=2013-10-04| title=ISO/IEC 20944-1:2013}}&lt;/ref&gt; || 2
|-
|data-sort-value="19502"|ISO/IEC 19502 || Information technology – [[Meta-Object Facility|Meta Object Facility (MOF)]] || Published (2005) || Defines a metamodel using MOF, and a set of interfaces using Open Distributed Processing (ODP) that can be used to define and manipulate a set of interoperable metamodels and their corresponding models&lt;ref&gt;{{cite web| title=ISO/IEC 19502:2005| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32621| date=2011-03-17| author=ISO}}&lt;/ref&gt; || 2
|-
|data-sort-value="19773"|ISO/IEC 19773 || Information technology – Metadata Registries (MDR) modules || Published (2011) || Specifies small modules of data to be used or reused in applications&lt;ref&gt;{{cite web| url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=41769| date=2011-09-01| accessdate=2013-10-04| author=ISO| title=ISO/IEC 19773:2011}}&lt;/ref&gt; || 2
|-
|data-sort-value="09075"|ISO/IEC 9075-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Database languages – [[SQL#Standardization|SQL]] – Part 1: Framework (SQL/Framework) || Published (2011) || Defines the conceptual framework to specify the grammar of SQL and the result of processing statements in that language by an SQL-implementation&lt;ref&gt;{{cite journal| title=ISO/IEC 9075-1| page=1| date=2008-07-15| edition=3| author=ISO}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53681| date=2013-02-04| accessdate=2013-10-04| author=ISO| title=ISO/IEC 9075-1:2011}}&lt;/ref&gt; || 3
|-
|data-sort-value="13249"|ISO/IEC 13249-3 || Information technology – Database languages – SQL multimedia and application packages – Part 3: Spatial || Published (2011) || “Defines spatial user-defined types, routines, and schemas for generic spatial data handling.”&lt;ref&gt;{{cite web| date=2011-08-22| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53698| accessdate=2013-10-04| author=ISO| title=ISO/IEC 13249-3:2011}}&lt;/ref&gt; || 4
|-
|}

==See also==
* [[ISO/IEC JTC1]]
* [[List of International Organization for Standardization standards|List of ISO standards]]
* [[American National Standards Institute]]
* [[International Organization for Standardization]]
* [[International Electrotechnical Commission]]

==References==
{{Reflist|2}}

==External links==
* [http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342 ISO/IEC JTC 1/SC 32 page at ISO]

{{DEFAULTSORT:ISO IEC JTC1 SC32}}
[[Category:ISO/IEC JTC1 subcommittees|#032]]
[[Category:Standards organizations]]
[[Category:Data management]]
[[Category:Data interchange standards]]</text>
      <sha1>akoaiv8ccfnfkxv9v49ctbfr8vswa0b</sha1>
    </revision>
  </page>
  <page>
    <title>Small data</title>
    <ns>0</ns>
    <id>41530851</id>
    <revision>
      <id>755120538</id>
      <parentid>743759064</parentid>
      <timestamp>2016-12-16T09:47:00Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Definition */clean up; http&amp;rarr;https for [[The Guardian]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4793" xml:space="preserve">
==Introduction==
The term small data did not exist before the word big data which came into use in the 1990's. What we once called data is now called small data.  Small data was not made useless by the advent of big data. Most data we see in our life is small data, and it should not be overlooked in any field. 

This article helps to define small data and to give examples in marketing and recruiting to help in understanding.

==Definition==
'''Small data''' (''sm’aē’āll DH(ə)ta'') is data that is 'small' enough for human comprehension.&lt;ref&gt;{{cite news|author=Rufus Pollock |url=https://www.theguardian.com/news/datablog/2013/apr/25/forget-big-data-small-data-revolution |title=Forget big data, small data is the real revolution &amp;#124; News |newspaper=[[The Guardian]] |date= |accessdate=2016-10-02}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://jwork.org/main/node/18 |title="Small data". Never heard this term? |website=jWork.ORG |date= |accessdate=2016-10-02}}&lt;/ref&gt; It is data in a volume and format that makes it accessible, informative and actionable.&lt;ref&gt;{{cite web|url=http://whatis.techtarget.com/definition/small-data |title=What is small data? - Definition from WhatIs.com |website=Whatis.techtarget.com |date=2016-08-18 |accessdate=2016-10-02}}&lt;/ref&gt;

The term "big data" is about machines and "small data" is about people.&lt;ref&gt;{{cite web|author=Eric Lundquist |url=http://www.eweek.com/enterprise-apps/small-data-analysis-the-next-big-thing-advocates-assert.html/ |title='Small Data' Analysis the Next Big Thing, Advocates Assert |website=Eweek.com |date=2013-09-10 |accessdate=2016-10-02}}&lt;/ref&gt; This is to say that eye witness observations or five pieces of related data could be small data. Small data is what we used to think of as data. The only way to comprehend [[Big data]] is to reduce the data into small, visually-appealing objects representing various aspects of large data sets (such as
[[histogram]], [[chart]]s, and scatter plots). So sometimes big data is simplified to be like small data. 

A formal definition of small data has been proposed by Allen Bonde, VP of Innovation at [[Actuate Corporation|Actuate]]: "Small data connects people with timely, meaningful insights (derived from big data and/or “local” sources), organized and packaged – often visually – to be accessible, understandable, and actionable for everyday tasks."&lt;ref&gt;{{cite web|url=http://smalldatagroup.com/2013/10/18/defining-small-data/ |title=Defining Small Data |publisher=Small Data Group |date= |accessdate=2016-10-02}}&lt;/ref&gt;
 
Another definition of '''small data''' is:
* The small set of specific attributes produced by the [[Internet of Things]]. These are typically a small set of sensor data such as temperature, wind speed, vibration and status.&lt;ref&gt;{{cite web|author= |url=http://www.forbes.com/sites/mikekavis/2015/02/25/forget-big-data-small-data-is-driving-the-internet-of-things/#4a72ffad661b |title=Forget Big Data - Small Data Is Driving The Internet Of Things |website=Forbes.com |date= |accessdate=2016-10-02}}&lt;/ref&gt;


==Some Examples of Uses in Business==

===Marketing===

Bonde has written extensively about the topic for Forbes,&lt;ref&gt;{{cite web|author= |url=http://www.forbes.com/sites/markfidelman/2012/10/30/these-smart-social-apps-bring-big-data-down-to-size/ |title=These Smart, Social Apps Bring Big Data Down to Size |website=Forbes.com |date= |accessdate=2016-10-02}}&lt;/ref&gt; Direct Marketing News,&lt;ref&gt;{{cite web|url=http://www.dmnews.com/why-small-data-is-the-next-big-thing-for-marketers/article/308376/ |title=Why Small Data Is the Next Big Thing for Marketers - DMN |website=Dmnews.com |date=2013-08-22 |accessdate=2016-10-02}}&lt;/ref&gt; CMO.com&lt;ref&gt;{{cite web|last=Bonde |first=Allen |url=http://www.cmo.com/features/articles/2013/11/20/think_small_time_for.html |title=Think Small: Time For Marketers To Move Beyond The Big Data Hype |website=Cmo.com |date=2013-12-12 |accessdate=2016-10-02}}&lt;/ref&gt; and other publications.  

According to Martin Lindstrom, in his book, [[Small Data: The Tiny Clues that Uncover Huge Trends|Small Data:]] "{In customer research, small data is} Seemingly insignificant behavioral observations containing very specific attributes pointing towards an unmet customer need. Small data is the foundation for break through ideas or completely new ways to turnaround brands."&lt;ref&gt;{{cite web|url=https://www.martinlindstrom.com/small-data/ |title=Small Data - Martin Lindstrom - Bestselling Author |publisher=Martin Lindstrom |date= |accessdate=2016-10-02}}&lt;/ref&gt;

==Conclusion==
Small Data is what we used to call data. The hype about Big Data should not cause us to look down on Small Data. Small Data's practicality and depth of insight is often better than big data.

==References==
{{Reflist}}

[[Category:Data management]]</text>
      <sha1>g8630n1o1obzla4jd773zzjyf5rge7f</sha1>
    </revision>
  </page>
  <page>
    <title>Quality of Data (QoD)</title>
    <ns>0</ns>
    <id>42426440</id>
    <revision>
      <id>723788746</id>
      <parentid>699755417</parentid>
      <timestamp>2016-06-05T07:19:03Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3791" xml:space="preserve">{{Orphan|date=January 2016}}

'''Quality-of-Data (QoD)''' is a designation coined by L. Veiga, that specifies and describes the required Quality of Service of a distributed storage system from the Consistency point of view of its data.
. It can be used to support [[Big data|Big Data]] management frameworks, Workflow management, and HPC systems (mainly for data replication and consistency). It takes into account data semantics, namely Time interval of data freshness, Sequence of tolerable number of outstanding versions of the data read before refresh, and Value divergence allowed before displaying it. Initially it was based in a model from an existing research work regarding vector-field Consistency,&lt;ref&gt;{{cite conference |author1=Nuno Santos |author2=Luís Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~lveiga/msc-08-09/vfc-middleware-07.pdf | format=PDF}}&lt;/ref&gt; awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007 and later enhanced for increased scalability and fault-tolerance.&lt;ref&gt;{{cite conference |author1=Luís Veiga |author2=André Negrão |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency | booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}&lt;/ref&gt;

This consistency model has been successfully applied and proven in Big Data key/value store [[Apache HBase]],&lt;ref group="nb"&gt;url=https://hbase.apache.org&lt;/ref&gt; initially designed as a [[middleware]]&lt;ref&gt;{{cite conference |author1=Sergio Estéves |author2=João Silva |author3=Luís Veiga  |last-author-amp=yes | year=2013 | title=Quality-of-service for consistency of data geo-replication in cloud computing  | booktitle = Euro-Par 2012 Parallel Processing. Springer Berlin Heidelberg, 2012. 285-297 | url=http://www.gsd.inesc-id.pt/~sesteves/papers/vfc3-europar12.pdf | format=PDF}}&lt;/ref&gt; module seating between clusters from separate data centres. The HBase-QoD coupling &lt;ref&gt;{{cite conference |author1=Álvaro García-Recuero |author2=Sergio Estéves |author3=Luís Veiga | year=2013 | title=Quality-of-Data for Consistency Levels in Geo-replicated Cloud Data Stores  | booktitle = IEEE CloudCom 2013 | url=http://www.inesc-id.pt/ficheiros/publicacoes/9253.pdf | format=PDF}}&lt;/ref&gt; minimises bandwidth usage and optimises resources allocation during replication achieving the desired consistency level at a more fine-grained level.

QoD is defined by the three-dimensions of vector k=(θ,σ,ν), but with a broader view of the issue, applicable also to large-scale data management techniques in regards to their timely delivery.&lt;ref group="nb"&gt;&lt;sub&gt;url=http://www-01.ibm.com/software/data/quality/&lt;/sub&gt;&lt;/ref&gt;

== Other Descriptions ==

Quality-of-Data should not be confused with other definitions for Data Quality such as
&lt;ref&gt;{{cite conference |author1=Richard Y. Wang | year= 1992 | title=Toward quality data : an attribute-based approach | booktitle=Decision Support Systems 13, MIT  | url=http://web.mit.edu/tdqm/www/tdqmpub/Toward%20Quality%20Data.pdf | format=PDF}}&lt;/ref&gt;
&lt;ref&gt;{{cite conference |author1=George A. Mihaila |author2=Louiqa Raschid |author3=María-Esther Vidal | year= 2000 | title=Using Quality of Data Metadata for Source Selection and Ranking | booktitle =  | url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.9361 | format=}}&lt;/ref&gt;
- Completeness
- Validity
- Accuracy

== Notes ==
&lt;references group="nb"/&gt;

== References ==
&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>195rmbcklpswptyhucqsoplelwz34ue</sha1>
    </revision>
  </page>
  <page>
    <title>Geospatial metadata</title>
    <ns>0</ns>
    <id>7419799</id>
    <revision>
      <id>737970273</id>
      <parentid>737969570</parentid>
      <timestamp>2016-09-06T04:16:41Z</timestamp>
      <contributor>
        <username>Annbeaumaris</username>
        <id>29111955</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15596" xml:space="preserve">'''Geospatial metadata''' (also '''geographic metadata''', or simply  '''metadata''' when used in a geographic context) is a type of [[metadata]] that is applicable to objects that have an explicit or implicit [[Geography|geographic]] extent, i.e. are associated with some position on the surface of the [[globe]]. Such objects may be stored in a [[geographic information system]] (GIS) or may simply be documents, data-sets, images or other objects, services, or related items that exist in some other native environment but whose features may be appropriate to describe in a (geographic) metadata catalog (may also be known as a data directory, data inventory, etc.).

==Definition==
I'''SO 19115:2013 "Geographic Information - Metadata'''"&lt;ref name=":0"&gt;{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:19115:-1:ed-1:v1:en|title=ISO 19115-1:2014(en)|last=International Organization for Standardization|first=|date=2014-04-01|website=ISO|publisher=|access-date=2016-04-01}}&lt;/ref&gt; from [[ISO/TC 211]], the industry standard for geospatial metadata, describes its scope as follows:

"''[This standard] provides information about the identification, the extent, the quality, the spatial and temporal aspects, the content, the spatial reference, the portrayal, distribution, and other properties of digital geographic data and services"&lt;ref name=":0" /&gt;''

ISO 19115:2013 also provides for non-digital mediums: ''"[t]hough this part of [https://www.iso.org/obp/ui/#iso:std:iso:19115:en ISO 19115] is applicable to digital data and services, its principles can be extended to many other types of resources such as maps, charts, and textual documents as well as non-geographic data.''" &lt;ref name=":0" /&gt;

'''The U.S. Federal Geographic Data Committee (FGDC)''' describes geospatial metadata as follows:

"''A metadata record is a file of information, usually presented as an XML document, which captures the basic characteristics of a data or information resource. It represents the who, what, when, where, why and how of the resource. Geospatial metadata commonly document geographic digital data such as Geographic Information System (GIS) files, geospatial databases, and earth imagery but can also be used to document geospatial resources including data catalogs, mapping applications, data models and related websites. Metadata records include core library catalog elements such as Title, Abstract, and Publication Data; geographic elements such as Geographic Extent and Projection Information; and database elements such as Attribute Label Definitions and Attribute Domain Values.''" &lt;ref&gt;{{Cite web|url=http://www.fgdc.gov/metadata|title=Geospatial Metadata — Federal Geographic Data Committee|website=www.fgdc.gov|access-date=2016-04-01}}&lt;/ref&gt;

==History==
The growing appreciation of the value of geospatial metadata through the 1980s and 1990s led to the development of a number of initiatives to collect metadata according to a variety of formats either within agencies, communities of practice, or countries/groups of countries. For example, [[NASA]]'s "DIF" metadata format was developed during an Earth Science and Applications Data Systems Workshop in 1987,&lt;ref&gt;[http://gcmd.nasa.gov/User/difguide/whatisadif.html Gene Major and Lola Olsen: "A short history of the DIF". On GCMD website, visited 16 October 2006]&lt;/ref&gt; and formally approved for adoption in 1988. Similarly, the U.S. FGDC developed its geospatial metadata standard over the period 1992–1994.&lt;ref&gt;[http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Libraries Guide: "Federal Geographic Data Committee (FGDC) Metadata". On MIT Libraries website, visited 16 October 2006]
&lt;/ref&gt; The Spatial Information Council of Australia and New Zealand (ANZLIC),&lt;ref&gt;
{{cite web
| url         = http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf
| title       = ANZLIC Metadata Profile Guidelines version 1.2 July 2011
| year        = 2011
| publisher   = ANZLIC
| accessdate  = 2011-04-11
| quote       = ANZLIC[:] The Spatial Information Council of Australia and New Zealand (formerly known as the Australia New Zealand Land Information Council)
}}
&lt;/ref&gt; a combined body representing spatial data interests in Australia and New Zealand, released version 1 of its "metadata guidelines" in 1996.&lt;ref&gt;[http://anzlic.gov.au/resources/anzlic-metadata-profile ANZLIC Metadata Guidelines: Core metadata elements for geographic data in Australia and New Zealand, Version 2 (February 2001)]&lt;/ref&gt; [[ISO/TC 211]] undertook the task of harmonizing the range of formal and ''de facto'' standards over the approximate period 1999–2002, resulting in the release of '''ISO 19115''' "'''Geographic Information - Metadata'''" in 2003 and a subsequent revision in 2013. {{As of | 2011}} individual countries, communities of practice, agencies, etc. have started re-casting their previously used metadata standards as "profiles" or recommended subsets of ISO 19115, occasionally with the inclusion of additional metadata elements as formal extensions to the ISO standard. The growth in popularity of Internet technologies and data formats, such as [[Extensible Markup Language]] (XML) during the 1990s led to the development of mechanisms for exchanging geographic metadata on the [[World Wide Web|web]]. In 2004, the [[Open Geospatial Consortium]] released the current version (3.1) of [[Geography Markup Language]] (GML), an XML grammar for expressing geospatial features and corresponding metadata. With the growth of the [[Semantic Web]] in the 2000s, the geospatial community has begun to develop [[Ontology (computer science)|ontologies]] for representing semantic geospatial metadata. Some examples include the [http://www.ordnancesurvey.co.uk/oswebsite/ontology/ Hydrology and Administrative ontologies] developed by the [[Ordnance Survey]] in the [[United Kingdom]].

==ISO 19115: Geographic information - Metadata==
ISO 19115 is a standard of the International Organization for Standardization (ISO).&lt;ref&gt;ISO 19115 Geographic Information - Metadata. International Organization for Standardization (ISO), Geneva, 2003&lt;/ref&gt; The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series). ISO 19115 and its parts define how to describe geographical information and associated services, including contents, spatial-temporal purchases, data quality, access and rights to use.

The objective of this International Standard is to provide a clear procedure for the description of digital geographic data-sets so that users will be able to determine whether the data in a holding will be of use to them and how to access the data. By establishing a common set of metadata terminology, definitions and extension procedures, this standard promotes the proper use and effective retrieval of geographic data.&lt;ref&gt;{{cite web|title=ISO 19115 Metadata Factsheet|url=http://www.isotc211.org/Outreach/Overview/Factsheet_19115.pdf|publisher=AG Outreach|accessdate=2012-11-22}}&lt;/ref&gt;

ISO 19115 was revised in 2013 to accommodate growing use of the internet for metadata management, as well as add many new categories of metadata elements (referred to as codelists) and the ability to limit the extent of metadata use temporally or by user.&lt;ref&gt;{{Cite web|url=https://wiki.earthdata.nasa.gov/display/NASAISO/NASA+Metadata+and+the+New+ISO+19115-1+Capabilities|title=NASA Metadata and the New ISO 19115-1 Capabilities - NASA ISO for EOSDIS - Earthdata Wiki|website=wiki.earthdata.nasa.gov|access-date=2016-04-01}}&lt;/ref&gt;

{{Expand section|date=June 2012}}

==ISO 19139 Geographic information Metadata XML schema implementation==
ISO 19139:2012 &lt;ref&gt;{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:ts:19139:-2:ed-1:v1:en|title=ISO/TS 19139-2:2012(en)|last=International Organization for Standardization|first=|date=2012-12-15|website=ISO|publisher=|access-date=2016-04-01}}&lt;/ref&gt; provides the XML implementation schema for ISO 19115 specifying the metadata record format and may be used to describe, validate, and exchange geospatial metadata prepared in XML.&lt;ref&gt;[http://marinemetadata.org/references/iso19139 "ISO 19139 Geographic information Metadata XML schema implementation"], Marine Metadata Interoperability Project&lt;/ref&gt;

The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series), and provides a spatial metadata XML (spatial metadata eXtensible Mark-up Language (smXML)) encoding, an XML schema implementation derived from ISO 19115, Geographic information – Metadata. The metadata includes information about the identification, constraint, extent, quality, spatial and temporal reference, distribution, lineage, and maintenance of the digital geographic data-set.

{{Expand section|date=June 2012}}

==Metadata directories==
Also known as metadata catalogues or data directories.

(need discussion of, and subsections on GCMD, FGDC metadata gateway, ASDD, European and Canadian initiatives, etc. etc.)
* [http://gisinventory.net GIS Inventory] – National GIS Inventory System which is maintained by the US-based [[National States Geographic Information Council|National States Geographic Information Council (NSGIC)]] as a tool for the entire US GIS Community. Its primary purpose is to track data availability and the status of geographic information system (GIS) implementation in state and local governments to aid the planning and building of statewide spatial data infrastructures (SSDI). The Random Access Metadata for Online Nationwide Assessment (RAMONA) database is a critical component of the GIS Inventory. RAMONA moves its FGDC-compliant metadata (CSDGM Standard) for each data layer to a web folder and a Catalog Service for the Web (CSW) that can be harvested by Federal programs and others. This provides far greater opportunities for discovery of user information. The GIS Inventory website was originally created in 2006 by NSGIC under award NA04NOS4730011 from the Coastal Services Center, National Oceanic and Atmospheric Administration, U.S. Department of Commerce. The Department of Homeland Security has been the principal funding source since 2008 and they supported the development of the Version 5 during 2011/2012 under Order Number HSHQDC-11-P-00177. The Federal Emergency Management Agency and National Oceanic and Atmospheric Administration have provided additional resources to maintain and improve the GIS Inventory. Some US Federal programs require submission of CSDGM-Compliant Metadata for data created under grants and contracts that they issue. The GIS Inventory provides a very simple interface to create the required Metadata. 
* [http://gcmd.nasa.gov GCMD] - Global Change Master Directory's goal is to enable users to locate and obtain access to Earth science data sets and services relevant to global change and Earth science research. The GCMD database holds more than 20,000 descriptions of Earth science data sets and services covering all aspects of Earth and environmental sciences.
* [http://earthdata.nasa.gov/echo ECHO] - The EOS Clearing House (ECHO) is a spatial and temporal metadata registry, service registry, and order broker. It allows users to more efficiently search and access data and services through the [http://reverb.earthdata.nasa.gov/echo Reverb Client] or Application Programmer Interfaces (APIs). ECHO stores metadata from a variety of science disciplines and domains, totalling over 3400 Earth science data sets and over 118&amp;nbsp;million granule records.
* [http://www.gogeo.ac.uk/gogeo/ GoGeo] - GoGeo is a service run by [[EDINA]] (University of Edinburgh) and is supported by [[Jisc]]. GoGeo allows users to conduct geographically targeted searches to discover geospatial datasets. GoGeo searches many data portals from the HE and FE community and beyond. GoGeo also allows users to create standards compliant metadata through its Geodoc metadata editor.

==Geospatial metadata tools==
There are many commercial GIS or geospatial products that support metadata viewing and editing on GIS resources. For example, [[ESRI]]'s [[ArcGIS]] Desktop, [[SOCET GXP]], [[Autodesk]]'s AutoCAD Map 3D 2008, [[Arcitecta]]'s [[Mediaflux]] and [[Intergraph]]'s [[GeoMedia]] support geospatial metadata extensively.

[http://gisinventory.net GIS Inventory] is a free web-based tool that provides a very simple interface to create geospatial metadata. Participants create a profile and document their data layers through a survey-style interface. The GIS Inventory produces metadata that is compliant with the Federal Content Standard for Digital Geospatial Metadata (CSDGM). The GIS Inventory is also capably of ingesting already completed metadata through document upload and web server connectivity. Through the GIS Inventory web services, metadata are automatically shared with US Federal agencies.

[http://geonetwork-opensource.org GeoNetwork opensource] is a comprehensive [[Free and Open Source Software]] solution to manage and publish geospatial metadata and services based on international metadata and catalog standards. The software is part of the [[Open Source Geospatial Foundation]]'s software stack.

[http://geocat.net/bridge GeoCat Bridge] allows to edit, validate and directly publish metadata from [[ArcGIS]] Desktop to [http://geonetwork-opensource.org GeoNetwork] (and generic CSW catalogs) and publishes data as map services on [http://geoserver.org GeoServer]. Several metadata profiles are supported.

[[pycsw]] is an OGC CSW server implementation written in Python. pycsw fully implements the OpenGIS Catalogue Service Implementation Specification ([[Catalog Service for the Web|Catalogue Service for the Web]]). The project is certified OGC Compliant, and is an OGC Reference Implementation.

[http://catmdedit.sourceforge.net/ CATMDEdit]
terraCatalog
ArcCatalog
ArcGIS Server Portal
[http://geonetwork-opensource.org GeoNetwork opensource]
[http://www.conterra.de/en/products/sdi/terracatalog/index.shtm IME]
[http://www.intelec.ca/html/en/technologies/m3cat.html M3CAT MetaD]
[http://www.gigateway.org.uk/metadata/metagenie.html MetaGenie]
Parcs Canada Metadata Editor
Mapit/CADit
NOKIS Editor

{{Expand section|date=June 2008}}

==References==
&lt;references/&gt;
[http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf ANZLIC Metadata Profile Version 1.2 (viewed July 2011)]

==External links==
*[http://www.fgdc.gov/metadata FGDC metadata page]
*[http://gcmd.nasa.gov/ Global Change Master Directory(GCMD)]
*[http://wiki.milcord.com/wiki/Geospatial_Exploitation_of_Motion_Imagery Geospatial Exploitation of Motion Imagery] is a geospatially aware and integrated Intelligent Video Surveillance (IVS) software system targeted at real-time and forensic video analytic and mining applications that require low-resolution detection, tracking, and classification of moving objects (people and vehicles) in outdoor, wide-area scenes.
*[http://www.iso.org/iso/en/CatalogueDetailPage.CatalogueDetail?CSNUMBER=26020 ISO 19115:2003 Geographic information -- Metadata]
*[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32557 Geographic information -- Metadata -- XML schema implementation ]
*[http://www.earthdatamodels.org/designs/metadata_BGS.html EarthDataModels design for Metadata] is a logical data model and physical implementation of a Spatial Metadata Database, based on ISO19115 and is INSPIRE compliant.

{{use dmy dates|date=January 2011}}

[[Category:Data management]]
[[Category:Metadata]]
[[Category:Geographic data and information]]</text>
      <sha1>sdc2fvo47quks8hzqgsqn4696r8uuyo</sha1>
    </revision>
  </page>
  <page>
    <title>Vinelink.com</title>
    <ns>0</ns>
    <id>42814178</id>
    <revision>
      <id>760897181</id>
      <parentid>665032048</parentid>
      <timestamp>2017-01-19T18:29:24Z</timestamp>
      <contributor>
        <username>Kimberley Murry</username>
        <id>30142502</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3292" xml:space="preserve">'''''Vinelink.com''''' (VINE) is a national website in the [[United States]] that allows victims of crime, and the general public, to track the movements of prisoners held by the various [[US states|states]] and [[Territories of the United States|territories]]. The first four letters in the websites name, "vine", are an acronym for "Victim Information and Notification Everyday". Vinelink.com displays information, based on the information provided by the various states' departments of correction and other law enforcement agencies, on whether an inmate is in custody, has been released, has been granted parole or probation, or has escaped from custody. In some cases, the website will reveal whether a defendant has been granted parole or probation, but then subsequently violated conditions of their release and become a fugitive.&lt;ref&gt;{{cite web | url=http://www.correct.state.ak.us/probation-parole/vine|title=Automated Victim Notification System (VINE)|publisher=[[Alaska Department of Corrections]]|accessdate=2014-05-20}}&lt;/ref&gt; Information provided on Vinelink.com represents [[metadata]], in that the website lists a defendants custody status; but does not list what the individual is charged with, their criminal history, or the amount of their bail, if applicable.

[[Internet]] users accessing the Vinelink.com website choose from a map of states and provinces within the United States where they wish to perform a search for an inmate. The user may then search for an individual using the inmate or parolees name, or by entering the inmates specific department of corrections inmate number, if known. When the inmates custody status changes, users who have registered to be notified of such changes will be notified via email, phone or both.&lt;ref&gt;{{cite web | url=http://www.doj.state.or.us/victims/pages/vine.aspx|title=Victim Information and Notification Everyday (VINE)|publisher=[[Oregon Department of Justice]]|date=2013-02-03| accessdate=2014-05-20}}&lt;/ref&gt; This information is currently released upon request, without the website requesting reasons for the users search or requiring payment, as [[public records]] available to the [[general public]].

Inmate information is available for most states, and for [[Puerto Rico]], on the website. The states of [[Arizona]], [[Georgia (U.S. state)|Georgia]], [[Massachusetts]], [[Montana]], [[New Hampshire]] and [[West Virginia]] provide very limited information on the site. The states of [[Kansas]], [[Maine]] and [[South Dakota]] do not participate in the VINE system.&lt;ref&gt;{{cite web | url=http://www.theledger.com/article/20130203/news/130209793| title=Who to Call: VINElink|publisher=[[The Ledger]]|date=2010| accessdate=2014-05-20}}&lt;/ref&gt; The website does not provide data on prisoners detained by the [[United States federal government]].

==References==
==External links==
* [http://www.vinelink.com official website] 

==See also==
Visit [http://www.bop.gov/inmateloc/ the official Federal Bureau of Prisons website] to search for inmates being held by the [[United States Federal Bureau of Prisons]]

[[Category:Data management]]
[[Category:Government services web portals in the United States]]
[[Category:Law enforcement websites]]
[[Category:Metadata]]
[[Category:Public records]]


{{website-stub}}
{{Crime-stub}}</text>
      <sha1>0qvcm5rcrs9w3qxupwbuzfh879i9f8l</sha1>
    </revision>
  </page>
  <page>
    <title>Head/tail Breaks</title>
    <ns>0</ns>
    <id>42933069</id>
    <revision>
      <id>760464914</id>
      <parentid>760429490</parentid>
      <timestamp>2017-01-17T04:02:37Z</timestamp>
      <contributor>
        <ip>82.152.154.105</ip>
      </contributor>
      <comment>/* Color rendering DEM */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14361" xml:space="preserve">{{Multiple issues|
{{technical|date=June 2014}}
{{COI|date=July 2014}}
}}

[[File:Patterns1024Cities2.jpg|thumb|500px|1024 cities that follow exactly Zipf's law, which implies that the first largest city is size 1, the second largest city is size 1/2, the third largest city is size 1/3, ... and the smallest city is size 1/1024. The left pattern is produced by head/tail breaks, while the right one by natural breaks, also known as [[Jenks natural breaks optimization]].]]
'''Head/tail breaks''' is a new [[clustering algorithm]] scheme for data with a heavy-tailed distribution such as [[power laws]] and [[lognormal distributions]]. The heavy-tailed distribution can be simply referred to the scaling pattern of far more small things than large ones, or alternatively numerous smallest, a very few largest, and some in between the smallest and largest. The classification is done through dividing things into large (or called the head) and small (or called the tail) things around the arithmetic mean or average, and then recursively going on for the division process for the large things or the head until the notion of far more small things than large ones is no longer valid, or with more or less similar things left only.&lt;ref name="Jiang1"&gt;Jiang, Bin (2013a). "Head/tail breaks: A new classification scheme for data with a heavy-tailed distribution", ''The Professional Geographer'', 65 (3), 482 – 494.&lt;/ref&gt; Head/tail breaks is not just for classification, but also for visualization of big data by keeping the head, since the head is self-similar to the whole. Head/tail breaks can be applied not only to vector data such as points, lines and polygons, but also to raster data like digital elevation model (DEM). 

==Motivation==
The head/tail breaks is mainly motivated by inability of conventional classification methods such as equal intervals, quantiles, geometric progressions, standard deviation, and natural breaks - commonly known as [[Jenks natural breaks optimization]] for revealing the underlying scaling pattern of far more small things than large ones. Note that the notion of far more small things than large one is not only referred to geometric property, but also to topological and semantic properties. In this connection, the notion should be interpreted as far more unpopular (or less-connected) things than popular (or well-connected) ones, or far more meaningless things than meaningful ones.

==Method==
Given some variable X that demonstrates a heavy-tailed distribution, there are far more small x than large ones. Take the average of all xi, and obtain the first mean m1. Then calculate the second mean for those xi greater than m1, and obtain m2. In the same recursive way, we can get m3 depending on whether the ending condition of no longer far more small x than large ones is met. For simplicity, we assume there are three means, m1, m2, and m3. This classification leads to four classes: [minimum, m1], (m1, m2], (m2, m3], (m3, maximum]. In general, it can be represented as a recursive function as follows:

     Recursive function '''Head/tail Breaks''':
     Break the input data (around mean or average) into the head and the tail;  
     // the head for data values greater the mean
     // the tail for data values less the mean
     while (head &lt;= 40%):
         '''Head/tail Breaks'''(head);
     End Function

The resulting number of classes is referred to as ht-index, an alternative index to [[fractal dimension]] for characterizing complexity of fractals or geographic features: the higher the ht-index, the more complex the fractals.&lt;ref name="Jiang2"&gt;Jiang, Bin and Yin Junjun (2014). "Ht-index for quantifying the fractal or scaling structure of geographic features", ''Annals of the Association of American Geographers'', 104(3), 530–541.&lt;/ref&gt; Recently, a more sensitive ht-index, namely CRG-index,&lt;ref&gt;{{Cite journal|title = CRG Index: A more sensitive ht-index for enabling dynamic views of geographic features|url = http://dx.doi.org/10.1080/00330124.2015.1099448|journal = The Professional Geographer|date = 2015-12-09|issn = 0033-0124|pages = 1–13|volume = 0|issue = 0|doi = 10.1080/00330124.2015.1099448|first = Peichao|last = Gao|first2 = Zhao|last2 = Liu|first3 = Meihui|last3 = Xie|first4 = Kun|last4 = Tian|first5 = Gang|last5 = Liu}}&lt;/ref&gt; has been developed, and it is able to capture slight changes which ht-index is unable to. Thus while ht-index is an integer, CRG-index is a real number. A PostgreSQL function for calculating ht-index can be found here.&lt;ref&gt;{{Cite journal|title = A PostgreSQL function for calculating the ht-index|url = https://www.researchgate.net/publication/287533541_A_PostgreSQL_function_for_calculating_the_ht-index?channel=doi&amp;linkId=56777e5b08aebcdda0e962fe&amp;showFulltext=true|date = 2015-01-01|doi = 10.13140/RG.2.1.3041.0324|first = Peichao Gao|last = Kun Tian}}&lt;/ref&gt;

=== Threshold or its sensitivity ===
The criterion to stop the iterative classification process using the head/tail breaks method is that the remaining data (i.e., the head part) are not heavy-tailed, or simply, the head part is no longer a minority (i.e., the proportion of the head part is no longer less than a threshold such as 40%). This threshold is suggested to be 40% by Jiang et al. (2013),&lt;ref name="Jiang3" /&gt; just as the codes above (i.e., head &lt;= 40%). But sometimes a larger threshold, for example 50% or more, can be used, as Jiang and Yin (2014)&lt;ref name="Jiang2" /&gt; noted in another article: "this condition can be relaxed for many geographic features, such as 50 percent or even more". However, all heads' percentage on average must be smaller than 40% (or 41, 42%), indicating far more small things than large ones. This sensitivity issue deserves further research in the future.

=== Rank-size plot and RA index ===
A good tool to display the scaling pattern, or the heavy-tailed distribution, is the rank-size plot, which is a scatter plot to display a set of values according to their ranks. With this tool, a new index &lt;ref&gt;{{Cite journal|last=Gao|first=Peichao|last2=Liu|first2=Zhao|last3=Tian|first3=Kun|last4=Liu|first4=Gang|date=2016-03-10|title=Characterizing Trafﬁc Conditions from the Perspective of Spatial-Temporal Heterogeneity|url=http://www.mdpi.com/2220-9964/5/3/34|journal=ISPRS International Journal of Geo-Information|language=en|volume=5|issue=3|pages=34|doi=10.3390/ijgi5030034}}&lt;/ref&gt; termed as the ratio of areas (RA) in a rank-size plot was defined to characterize the scaling pattern. The RA index has been successfully used in the estimation of traffic conditions. However, it should be noted that the RA index can only be used as a complementary method to the ht-index, because it is ineffective to capture the scaling structure of geographic features.

==Applications==
Instead of more or less similar things, there are far more small things than large ones surrounding us. Given the ubiquity of the scaling pattern, head/tail breaks is found to be of use to statistical mapping, map generalization, cognitive mapping and even perception of beauty
.&lt;ref name="Jiang3"&gt;Jiang, Bin, Liu, Xintao and Jia, Tao (2013). "Scaling of geographic space as a universal rule for map generalization", ''Annals of the Association of American Geographers'', 103(4), 844 – 855.&lt;/ref&gt;&lt;ref name="Jiang4"&gt;Jiang, Bin (2013b). "The image of the city out of the underlying scaling of city artifacts or locations", ''Annals of the Association of American Geographers'', 103(6), 1552-1566.&lt;/ref&gt;&lt;ref name="Jiang5"&gt;Jiang, Bin and Sui, Daniel (2014). "A new kind of beauty out of the underlying scaling of geographic space", ''The Professional Geographer'', 66(4), 676–686&lt;/ref&gt; It helps visualize big data, since big data are likely to show the scaling property of far more small things than large ones. The visualization strategy is to recursively drop out the tail parts until the head parts are clear or visible enough.&lt;ref name="Jiang6"&gt;Jiang, Bin (2015). "Head/tail breaks for visualization of city structure and dynamics", ''Cities'', 43, 69 - 77.&lt;/ref&gt; In addition, it helps delineate cities or natural cities to be more precise from various geographic information such as street networks, social media geolocation data, and nighttime images.

=== Characterizing the imbalance ===
As the head/tail breaks method can be used iteratively to obtain head parts of a data set, this method actually captures the underlying hierarchy of the data set. For example, if we divide the array (19, 8, 7, 6, 2, 1, 1, 1, 0) with the head/tail breaks method, we can get two head parts, i.e., the first head part (19, 8, 7, 6) and the second head part (19). These two head parts as well as the original array form a three-level hierarchy:

the 1st level (19),

the 2nd level (19, 8, 7, 6), and

the 3rd level (19, 8, 7, 6, 2, 1, 1, 1, 0).

The number of levels of the above-mentioned hierarchy is actually a characterization of the imbalance of the example array, and this number of levels has been termed as the ht-index.&lt;ref name="Jiang2" /&gt; With the ht-index, we are able to compare degrees of imbalance of two data sets. For example, the ht-index of the example array (19, 8, 7, 6, 2, 1, 1, 1, 0) is 3, and the ht-index of another array (19, 8, 8, 8, 8, 8, 8, 8, 8) is 2. Therefore, the degree of imbalance of the former array is higher than that of the latter array.
[[File:Natural_cities_of_Germany,_created_from_points_of_interest.jpg|thumb|250px|right|The left panel pattern contains 50,000 natural cities, which can be put into 7 hierarchical levels. It looks like a hair ball. Instead of showing all the 7 hierarchical levels, we show 4 top levels, by dropping out 3 low levels. Now with the right panel, the scaling pattern of far more small cities than large ones emerges. It is important to note that the right pattern (or the remaining part after dropping out the tails) is self-similar to the whole (or the left pattern). Thus the right pattern reflects the underlying structure of the left one, and enables us to see the whole.]]
[[File:Headtail breaks of American DEM.jpg|thumb|250px|The scaling pattern of US terrain surface is distorted by the natural breaks, but revealed by the head/tail breaks.]]

=== Delineating natural cities ===
The term ‘natural cities’ refers to the human settlements or human activities in general on Earth’s surface that are naturally or objectively defined and delineated from massive geographic information based on head/tail division rule, a non-recursive form of head/tail breaks.&lt;ref name="Jiang7"&gt;Jiang, Bin and Miao, Yufan (2015). "The evolution of natural cities from the perspective of location-based social media", ''The Professional Geographer'', 67(2), 295 - 306.&lt;/ref&gt;&lt;ref name="Long"&gt;Long, Ying (2016). "Redefining Chinese city system with emerging new data", ''Applied Geography'', 75, 36 - 48.&lt;/ref&gt;  Such geographic information could be from various sources, such as massive street junctions &lt;ref name="Long"/&gt; and street ends, a massive number of street blocks, nighttime imagery and social media users’ locations etc. Distinctive from conventional cities, the adjective ‘natural’ could be explained not only by the sources of natural cities, but also by the approach to derive them. Natural cities are derived from a meaningful cutoff averaged from a massive amount of units extracted from geographic information.&lt;ref name="Jiang6"/&gt; Those units vary according to different kinds of geographic information, for example the units could be area units for the street blocks and pixel values for the nighttime images. A '''[http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d natural cities model]''' has been created using ArcGIS model builder,&lt;ref name="Ren"&gt;Ren, Zheng (2016). "Natural cities model in ArcGIS", ''http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d''.&lt;/ref&gt; it follows the same process of deriving natural cities from location-based social media,&lt;ref name="Jiang7"/&gt; namely, building up huge triangular irregular network (TIN) based on the point features (street nodes in this case) and regarding the triangles which are smaller than a mean value as the natural cities.

=== Color rendering DEM ===
Current color renderings for DEM or density map are essentially based on conventional classifications such as natural breaks or equal intervals, so they disproportionately exaggerate high elevations or high densities. As a matter of fact, there are not so many high elevations or high-density locations.&lt;ref name="Jiang8"&gt;Jiang, Bin (2015). "Geospatial analysis requires a different way of thinking: The problem of spatial heterogeneity", ''GeoJournal'', 80(1), 1-13.&lt;/ref&gt; It was found that coloring based head/tail breaks is more favorable than those by other classifications &lt;ref name="Wu"&gt;Wu, Jou-Hsuan (2015). "Examining the new kind of beauty using the human being as a measuring instrument", ''http://www.diva-portal.org/smash/get/diva2:805296/FULLTEXT01.pdf''.&lt;/ref&gt;

== Software implementations ==
The following implementations are available under [[Free and open-source software|Free/Open Source Software]] licenses.
* '''[https://github.com/digmaa/HeadTailBreaks HT calculator]''': a winform application for obtaining related metrics of head/tail breaks applying on a single data array.
* '''[http://jsfiddle.net/mhkeller/5yATK/ HT in JavaScript]''': a JavaScript implementation for applying head/tail breaks on a single data array.
* '''[http://fromto.hig.se/~bjg/axwoman/ HT Mapping tool]''': a function in the free plug-in Axwoman 6.3 to [[ArcMap]] 10.2 that conducts geo-data symbolization automatically based on the head/tail breaks classification.
* '''[https://github.com/chad-m/head_tail_breaks_algorithm HT in Python]''': Python and JavaScript code for the head/tail breaks algorithm. It's works great for choropleth map coloring.

==References==
{{one author|date=June 2014}}
{{Reflist}}

==Further reading==
{{Further reading cleanup|date=June 2014}}
* Lin, Yue (2013), A comparison study on natural and head/tail breaks involving digital elevation models. http://www.diva-portal.org/smash/get/diva2:658963/FULLTEXT02.pdf
* Wu, Jou-Hsuan (2015), The mirror of the self test: http://sharon19891101.wix.com/mirror-of-the-self

{{DEFAULTSORT:Head tail Breaks}}
[[Category:Data management]]
[[Category:Cartography]]</text>
      <sha1>seruqmirs4txlpsha8vh7lwmsavjhei</sha1>
    </revision>
  </page>
  <page>
    <title>Data security</title>
    <ns>0</ns>
    <id>1157832</id>
    <revision>
      <id>760636811</id>
      <parentid>757511165</parentid>
      <timestamp>2017-01-18T04:52:15Z</timestamp>
      <contributor>
        <username>Jennica</username>
        <id>623801</id>
      </contributor>
      <minor />
      <comment>removed 500px using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7847" xml:space="preserve">{{Refimprove|date=February 2012}}
'''Data security''' means protecting data, such as a database, from destructive forces and from the unwanted actions of unauthorized users.&lt;ref&gt;Summers, G. (2004). Data and databases.  In: Koehne, H Developing Databases with Access: Nelson Australia Pty Limited. p4-5.&lt;/ref&gt;

== Data security technologies ==

=== Disk encryption ===

[[Disk encryption]] refers to encryption technology that encrypts data on a [[hard disk drive]]. Disk encryption typically takes form in either software (see [[disk encryption software]]) or hardware (see [[disk encryption hardware]]). Disk encryption is often referred to as [[on-the-fly encryption]] (OTFE) or transparent encryption.

=== Software versus hardware-based mechanisms for protecting data ===

Software-based security solutions encrypt the data to protect it from theft. However, a malicious program or a hacker could corrupt the data in order to make it unrecoverable, making the system unusable. Hardware-based security solutions can prevent read and write access to data and hence offer very strong protection against tampering and unauthorized access.

Hardware based security or assisted [[computer security]] offers an alternative to software-only computer security. [[Security token]]s such as those using [[PKCS#11]] may be more secure due to the physical access required in order to be compromised. Access is enabled only when the token is connected and correct [[Personal identification number|PIN]] is entered (see [[two-factor authentication]]). However, dongles can be used by anyone who can gain physical access to it. Newer technologies in hardware-based security solves this problem offering full proof security for data.

Working of hardware-based security: A hardware device allows a user to log in, log out and set different privilege levels by doing manual actions. The device uses biometric technology to prevent malicious users from logging in, logging out, and changing privilege levels. The current state of a user of the device is read by controllers in [[peripheral devices]] such as hard disks. Illegal access by a malicious user or a malicious program is interrupted based on the current state of a user by hard disk and DVD controllers making illegal access to data impossible. Hardware-based access control is more secure than protection provided by the operating systems as operating systems are vulnerable to malicious attacks by [[Computer virus|viruses]] and hackers. The data on hard disks can be corrupted after a malicious access is obtained. With hardware-based protection, software cannot manipulate the user privilege levels. It is impossible for a [[Hacker (computer security)|hacker]] or a malicious program to gain access to secure data protected by hardware or perform unauthorized privileged operations. This assumption is broken only if the hardware itself is malicious or contains a backdoor.&lt;ref&gt;{{Citation| last1 = Waksman  | first1 = Adam | last2 = Sethumadhavan | first2 = Simha | title = Silencing Hardware Backdoors | volume = | pages =  | periodical = Proceedings of the IEEE Symposium on Security and Privacy | location = Oakland, California  | url = http://www.cs.columbia.edu/~simha/preprint_oakland11.pdf | year = 2011  | issn =  | doi =  | isbn = }}&lt;/ref&gt; The hardware protects the operating system image and file system privileges from being tampered. Therefore, a completely secure system can be created using a combination of hardware-based security and secure system administration policies.

=== Backups ===

[[Backup]]s are used to ensure data which is lost can be recovered from another source. It is considered essential to keep a backup of any data in most industries and the process is recommended for any files of importance to a user.

===Data masking===
{{main|Data masking}}
[[Data masking]] of structured data is the process of obscuring (masking) specific data within a database table or cell to ensure that data security is maintained and sensitive information is not exposed to unauthorized personnel.&lt;ref&gt;{{cite web|title=What is Data Obfuscation|url=http://www.dataobfuscation.com.au|accessdate=1 March 2016}}&lt;/ref&gt;  This may include masking the data from users (for example so banking customer representatives can only see the last 4 digits of a customers national identity number), developers (who need real production data to test new software releases but should not be able to see sensitive financial data), outsourcing vendors, etc.
&lt;ref&gt;{{Cite web
 |url = http://searchsecurity.techtarget.com/definition/data-masking
 |title = data masking
 |accessdate = 29 July 2016
}}&lt;/ref&gt;

===Data erasure===
[[Data erasure]] is a method of software-based overwriting that completely destroys all electronic data residing on a hard drive or other digital media to ensure that no sensitive data is leaked when an asset is retired or reused...

== International laws and standards ==

=== International laws ===

In the [[United Kingdom|UK]], the [[Data Protection Act 1998|Data Protection Act]] is used to ensure that personal data is accessible to those whom it concerns, and provides redress to individuals if there are inaccuracies.&lt;ref&gt;{{Cite web
 |url = https://ico.org.uk/for-organisations/guide-to-data-protection/principle-1-fair-and-lawful/
 |title = data protection act
 |accessdate = 29 July 2016
}}&lt;/ref&gt; This is particularly important to ensure individuals are treated fairly, for example for credit checking purposes. The Data Protection Act states that only individuals and companies with legitimate and lawful reasons can process personal information and cannot be shared. [[Data Privacy Day]] is an international [[holiday]] started by the [[Council of Europe]] that occurs every January 28.&lt;ref name=dataprivacyday&gt;{{cite web|url=http://googleblog.blogspot.com/2008/01/celebrating-data-privacy.html|title=Celebrating data privacy |author=[[Peter Fleischer]], [[Jane Horvath]], [[Shuman Ghosemajumder]]|publisher=[[Google Blog]] |accessdate=12 August 2011 |year=2008}}&lt;/ref&gt;

=== International standards ===

The international standards ISO/IEC 27001:2013 and ISO/IEC 27002:2013 covers data security under the topic of [[information security]], and one of its cardinal principles is that all stored information, i.e. data, should be owned so that it is clear whose responsibility it is to protect and control access to that data.

The [[Trusted Computing Group]] is an organization that helps standardize computing security technologies.

The [[PCI DSS|Payment Card Industry Data Security Standard]] is a proprietary international information security standard for organizations that handle cardholder information for the major [[Debit card|debit]], [[Credit card|credit]], prepaid, [[e-purse]], [[Cash machine|ATM]] and POS cards.&lt;ref&gt;{{cite web|title=PCI DSS Definition|url=http://www.pcmag.com/encyclopedia/term/59104/pci-dss|accessdate=1 March 2016}}&lt;/ref&gt;

== Industry and software ==
There are several data security software available to be used by consumers and one of the most used data security software with a U.S issued patent is [[Folder lock|Folder Lock]].

==See also==
* [[Copy Protection]]
* [[Data-centric security]]
* [[Data erasure]]
* [[Data masking]]
* [[Data recovery]]
* [[Digital inheritance]]
* [[Disk encryption]]
** [[Comparison of disk encryption software]]
* [[Identity Based Security]]
* [[Information security]]
* [[IT network assurance]]
* [[Pre-boot authentication]]
* [[Privacy engineering]]
* [[Secure USB drive]]
* [[Security Breach Notification Laws]]
* [[Single sign-on]]
* [[Smart card]]
* [[Trusted Computing Group]]

== Notes and references ==
{{reflist}}

==External links==
{{Commons category}}

{{Data}}
{{Privacy}}
{{Portal bar|Computer security|Information technology}}

[[Category:Data security| ]]
[[Category:Data management]]</text>
      <sha1>kx6oawomrcubukb0rskfeetss46phd7</sha1>
    </revision>
  </page>
  <page>
    <title>Computer-aided software engineering</title>
    <ns>0</ns>
    <id>627071</id>
    <revision>
      <id>761279235</id>
      <parentid>758196872</parentid>
      <timestamp>2017-01-22T01:54:42Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* Environments */Cleaning up..., [[WP:AWB/T|typo(s) fixed]]: For example → For example, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16889" xml:space="preserve">[[File:Umbrello 1.png|320px|thumb|Example of a CASE tool.]]
'''Computer-aided software engineering''' ('''CASE''') is the domain of software tools used to design and implement applications. CASE tools are similar to and were partly inspired by [[computer-aided design]] (CAD) tools used for designing hardware products.  CASE tools are used for developing high-quality, defect-free, and maintainable software.&lt;ref&gt;Kuhn, D.L (1989). "Selecting and effectively using a computer aided software engineering tool". Annual Westinghouse computer symposium; 6–7 Nov 1989; Pittsburgh, PA (U.S.); DOE Project.&lt;/ref&gt; CASE software is often associated with methods for the development of [[information system]]s together with automated tools that can be used in the [[software development process]].&lt;ref&gt;P. Loucopoulos and V. Karakostas (1995). ''System Requirements Engineerinuality software which will perform effectively.&lt;/ref&gt;

== History ==
The Information System Design and Optimization System (ISDOS) project, started in 1968 at the University of Michigan, initiated a great deal of interest in the whole concept of using computer systems to help analysts in the very difficult process of analysing requirements and developing systems. Several papers by Daniel Teichroew fired a whole generation of enthusiasts with the potential of automated systems development. His Problem Statement Language / Problem Statement Analyzer (PSL/PSA) tool was a CASE tool although it predated the term.&lt;ref&gt;{{cite journal|last1=Teichroew|first1=Daniel|last2=Hershey|first2=Ernest Allen|title=PSL/PSA a computer-aided technique for structured documentation and analysis of information processing systems|journal=Proceeding ICSE '76 Proceedings of the 2nd international conference on Software engineering|date=1976|url=http://dl.acm.org/citation.cfm?id=807641|publisher=IEEE Computer Society Press}}&lt;/ref&gt;

Another major thread emerged as a logical extension to the [[data dictionary]] of a [[database]]. By extending the range of [[metadata]] held, the attributes of an application could be held within a dictionary and used at runtime. This "active dictionary" became the precursor to the more modern [[model-driven engineering]] capability. However, the active dictionary did not provide a graphical representation of any of the metadata. It was the linking of the concept of a dictionary holding analysts' metadata, as derived from the use of an integrated set of techniques, together with the graphical representation of such data that gave rise to the earlier versions of CASE.&lt;ref&gt;{{cite book|last1=Coronel|first1=Carlos|last2=Morris|first2=Steven|title=Database Systems: Design, Implementation, &amp; Management|date=February 4, 2014|publisher=Cengage Learning|isbn=1285196147|pages=695–700|url=https://books.google.com/books?id=QWLpAgAAQBAJ&amp;pg=PA698&amp;lpg=PA698&amp;dq=case+tools+data+dictionary&amp;source=bl&amp;ots=eJt_GWYHGx&amp;sig=MZEMesWkJrGdczKSEZ_6bnqdNAY&amp;hl=en&amp;sa=X&amp;ei=HNF0VOvWDu3xigK5FQ&amp;ved=0CFIQ6AEwCQ#v=onepage&amp;q=case%20tools%20data%20dictionary&amp;f=false|accessdate=25 November 2014}}&lt;/ref&gt;

The term was originally coined by software company Nastec Corporation of Southfield, Michigan in 1982 with their original integrated graphics and text editor GraphiText, which also was the first microcomputer-based system to use hyperlinks to cross-reference text strings in documents&amp;mdash;an early forerunner of today's web page link.  GraphiText's successor product, DesignAid, was the first microprocessor-based tool to logically and semantically evaluate software and system design diagrams and build a data dictionary.

Under the direction of [[Albert F. Case, Jr.]] vice president for product management and consulting, and Vaughn Frick, director of product management, the DesignAid product suite was expanded to support analysis of a wide range of [[Structured Analysis and Design Technique|structured analysis and design methodologies]], including those of [[Ed Yourdon]] and [[Tom DeMarco]], [[Chris Gane (computer scientist)|Chris Gane]] &amp; [[Trish Sarson]], Ward-Mellor (real-time) SA/SD and [[Warnier-Orr]] (data driven).&lt;ref&gt;{{cite journal|last1=Case|first1=Albert|title=Computer-aided software engineering (CASE): technology for improving software development productivity|journal=ACM SIGMIS Database|date=Fall 1985|volume=17|issue=1|pages=35–43|url=http://dl.acm.org/citation.cfm?id=1040698}}&lt;/ref&gt;

The next entrant into the market was Excelerator from Index Technology in Cambridge, Mass.  While DesignAid ran on Convergent Technologies and later Burroughs Ngen networked microcomputers, Index launched Excelerator on the IBM PC/AT platform. While, at the time of launch, and for several years, the IBM platform did not support networking or a centralized database as did the Convergent Technologies or Burroughs machines, the allure of IBM was strong, and Excelerator came to prominence. Hot on the heels of Excelerator were a rash of offerings from companies such as Knowledgeware (James Martin, [[Fran Tarkenton]] and Don Addington), Texas Instrument's [[Information Engineering Facility|IEF]] and [[Andersen Consulting|Andersen Consulting's]] FOUNDATION toolset (DESIGN/1, INSTALL/1, FCP).

CASE tools were at their peak in the early 1990s.&lt;ref&gt;{{cite news|last1=Yourdon|first1=Ed|title=Can XP Projects Grow?|url=https://books.google.com/books?id=_faqtO2fEbkC&amp;pg=PA28&amp;lpg=PA28&amp;dq=CASE+tools+most+interest+90%27s&amp;source=bl&amp;ots=9WNDAYPU89&amp;sig=vC_s1JtRyOwcHcCvyDici5H9Z7w&amp;hl=en&amp;sa=X&amp;ei=lNd0VPr1De2rjAK6o4D4DA&amp;ved=0CDIQ6AEwAw#v=onepage&amp;q=CASE%20tools%20most%20interest%2090%27s&amp;f=false|accessdate=25 November 2014|publisher=Computerworld|date=Jul 23, 2001}}&lt;/ref&gt;  At the time [[IBM]] had proposed AD/Cycle, which was an alliance of software vendors centered on IBM's [[Software repository]] using [[IBM DB2]] in [[Mainframe computer|mainframe]] and [[OS/2]]:

:''The application development tools can be from several sources: from IBM, from vendors, and from the customers themselves. IBM has entered into relationships with [[Bachman Information Systems]], Index Technology Corporation, and [[KnowledgeWare|Knowledgeware]] wherein selected products from these vendors will be marketed through an IBM complementary marketing program to provide offerings that will help to achieve complete life-cycle coverage''.&lt;ref name="ADC_SAaA"&gt;"AD/Cycle strategy and architecture", IBM Systems Journal, Vol 29, NO 2, 1990; p. 172.&lt;/ref&gt;

With the decline of the mainframe, AD/Cycle and the Big CASE tools died off, opening the market for the mainstream CASE tools of today. Many of the leaders of the CASE market of the early 1990s ended up being purchased by [[Computer Associates]], including IEW, IEF, ADW, Cayenne, and Learmonth &amp; Burchett Management Systems (LBMS).  The other trend that led to the evolution of CASE tools was the rise of object-oriented methods and tools. Most of the various tool vendors added some support for object-oriented methods and tools.  In addition new products arose that were designed from the bottom up to support the object-oriented approach. Andersen developed its project Eagle as an alternative to Foundation. Several of the thought leaders in object-oriented development each developed their own methodology and CASE tool set: Jacobsen, Rumbaugh, [[Grady Booch|Booch]], etc. Eventually, these diverse tool sets and methods were consolidated via standards led by the [[Object Management Group]] (OMG). The OMG's [[Unified Modelling Language]] (UML) is currently widely accepted as the industry standard for object-oriented modeling.

== CASE software ==
A. Fuggetta classified CASE software into 3 categories:&lt;ref name="AF_93"&gt;{{cite journal
| author      = Alfonso Fuggetta
|date=December 1993
| title       = A classification of CASE technology
| journal     = Computer
| volume      = 26
| issue       = 12
| pages       = 25–38
| doi         = 10.1109/2.247645
| url         = http://www2.computer.org/portal/web/csdl/abs/mags/co/1993/12/rz025abs.htm
| accessdate  = 2009-03-14
}}
&lt;/ref&gt;
# ''Tools'' support specific tasks in the [[software life-cycle]].
# ''Workbenches'' combine two or more tools focused on a specific part of the software life-cycle.
# ''Environments'' combine two or more tools or workbenches and support the complete software life-cycle.

=== Tools ===
CASE tools supports specific tasks in the software development life-cycle. They can be divided into the following categories:
# Business and Analysis modeling. Graphical modeling tools. E.g., E/R modeling, object modeling, etc.
# Development. Design and construction phases of the life-cycle. Debugging environments. E.g., [[GNU Debugger]].
# Verification and validation. Analyze code and specifications for correctness, performance, etc.  
# Configuration management. Control the check-in and check-out of repository objects and files. E.g., [[Source Code Control System|SCCS]], CMS.
# Metrics and measurement. Analyze code for complexity, modularity (e.g., no "go to's"), performance, etc. 
# Project management. Manage project plans, task assignments, scheduling.
Another common way to distinguish CASE tools is the distinction between Upper CASE and Lower CASE. Upper CASE Tools support business and analysis modeling. They support traditional diagrammatic languages such as [[ER diagram]]s, [[Data flow diagram]], [[Structure chart]]s, [[Decision Tree]]s, [[Decision table]]s, etc. Lower CASE Tools support development activities, such as physical design, debugging, construction, testing, component integration, maintenance, and reverse engineering. All other activities span the entire life-cycle and apply equally to upper and lower CASE.&lt;ref&gt;Software Engineering: Tools, Principles and Techniques by Sangeeta Sabharwal, Umesh Publications&lt;/ref&gt;

=== Workbenches ===
Workbenches integrate two or more CASE tools and support specific software-process activities. Hence they achieve:
*a homogeneous and consistent interface (presentation integration).
*seamless integration of tools and tool chains (control and data integration).

An example workbench is Microsoft's [[Visual Basic]] programming environment. It incorporates several development tools: a GUI builder, smart code editor, debugger, etc. Most commercial CASE products tended to be such workbenches that seamlessly integrated two or more tools. Workbenches also can be classified in the same manner as tools; as focusing on Analysis, Development, Verification, etc. as well as being focused on upper case, lower case, or processes such as configuration management that span the complete life-cycle.

=== Environments ===
An environment is a collection of CASE tools or workbenches that attempts to support the complete software process. This contrasts with tools that focus on one specific task or a specific part of the life-cycle. CASE environments are classified by Fuggetta as follows:&lt;ref name="AF_93" /&gt;
#Toolkits. Loosely coupled collections of tools. These typically build on operating system workbenches such as the Unix Programmer's Workbench or the VMS VAX set. They typically perform integration via piping or some other basic mechanism to share data and pass control. The strength of easy integration is also one of the drawbacks. Simple passing of parameters via technologies such as shell scripting can't provide the kind of sophisticated integration that a common repository database can. 
#Fourth generation. These environments are also known as 4GL standing for fourth generation language environments due to the fact that the early environments were designed around specific languages such as Visual Basic. They were the first environments to provide deep integration of multiple tools. Typically these environments were focused on specific types of applications. For example, user-interface driven applications that did standard atomic transactions to a relational database. Examples are Informix 4GL, and Focus.
#Language-centered. Environments based on a single often object-oriented language such as the Symbolics Lisp Genera environment or VisualWorks Smalltalk from Parcplace. In these environments all the operating system resources were objects in the object-oriented language. This provides powerful debugging and graphical opportunities but the code developed is mostly limited to the specific language. For this reason, these environments were mostly a niche within CASE. Their use was mostly for prototyping and R&amp;D projects. A common core idea for these environments was the [[model-view-controller]] user interface that facilitated keeping multiple presentations of the same design consistent with the underlying model. The MVC architecture was adopted by the other types of CASE environments as well as many of the applications that were built with them. 
#Integrated. These environments are an example of what most IT people tend to think of first when they think of CASE. Environments such as IBM's AD/Cycle, Andersen Consulting's FOUNDATION, the ICL [[CADES]] system, and DEC Cohesion. These environments attempt to cover the complete life-cycle from analysis to maintenance and provide an integrated database repository for storing all artifacts of the software process. The integrated software repository was the defining feature for these kinds of tools. They provided multiple different design models as well as support for code in heterogenous languages. One of the main goals for these types of environments was "round trip engineering": being able to make changes at the design level and have those automatically be reflected in the code and vice versa. These environments were also typically associated with a particular methodology for software development. For example, the FOUNDATION CASE suite from Andersen was closely tied to the Andersen Method/1 methodology.
#Process-centered. This is the most ambitious type of integration. These environments attempt to not just formally specify the analysis and design objects of the software process but the actual process itself and to use that formal process to control and guide software projects. Examples are East, Enterprise II, Process Wise, Process Weaver, and Arcadia. These environments were by definition tied to some methodology since the software process itself is part of the environment and can control many aspects of tool invocation.

In practice, the distinction between workbenches and environments was flexible. Visual Basic for example was a programming workbench but was also considered a 4GL environment by many. The features that distinguished workbenches from environments were deep integration via a shared repository or common language and some kind of methodology (integrated and process-centered environments) or domain (4GL) specificity.&lt;ref name="AF_93" /&gt;

== Major CASE Risk Factors ==
Some of the most significant risk factors for organizations adopting CASE technology include: 
* Inadequate standardization. Organizations usually have to tailor and adopt methodologies and tools to their specific requirements. Doing so may require significant effort to integrate both divergent technologies as well as divergent methods. For example, before the adoption of the UML standard the diagram conventions and methods for designing object-oriented models were vastly different among followers of Jacobsen, Booch, Rumbaugh, 
* Unrealistic expectations. The proponents of CASE technology—especially vendors marketing expensive tool sets—often hype expectations that the new approach will be a silver bullet that solves all problems. In reality no such technology can do that and if organizations approach CASE with unrealistic expectations they will inevitably be disappointed. 
* Inadequate training. As with any new technology, CASE requires time to train people in how to use the tools and to get up to speed with them. CASE projects can fail if practitioners are not given adequate time for training or if the first project attempted with the new technology is itself highly mission critical and fraught with risk. 
* Inadequate process control. CASE provides significant new capabilities to utilize new types of tools in innovative ways. Without the proper process guidance and controls these new capabilities can cause significant new problems as well.&lt;ref&gt;[http://ithandbook.ffiec.gov/it-booklets/development-and-acquisition/development-procedures/software-development-techniques/computer-aided-software-engineering.aspx Computer Aided Software Engineering]. In: ''FFIEC IT Examination Handbook InfoBase''. Retrieved 3 Mar 2012.&lt;/ref&gt;

== See also ==
* [[Data modeling]]
* [[Domain-specific modeling]]
* [[Method engineering]]
* [[Model-driven architecture]]
* [[Modeling language]]
* [[Rapid application development]]
* [[Model-based architecture]]

== References ==
{{reflist}}

{{Authority control}}

[[Category:Computer-aided software engineering tools|*]]
[[Category:Data management]]</text>
      <sha1>dfcbvnmyv9w4gihg3l0k73r0ip5fxof</sha1>
    </revision>
  </page>
  <page>
    <title>AnalytiX DS</title>
    <ns>0</ns>
    <id>43846669</id>
    <revision>
      <id>739647310</id>
      <parentid>735226992</parentid>
      <timestamp>2016-09-16T01:25:07Z</timestamp>
      <contributor>
        <username>Dl2000</username>
        <id>917223</id>
      </contributor>
      <minor />
      <comment>en-IN</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7025" xml:space="preserve">{{Use dmy dates|date=September 2016}}
{{Use Indian English|date=September 2016}}
{{Orphan|date=September 2014}}

{{Infobox company
| name             =AnalytiX DS
| logo             =[[File:AnalytiX DS Logo.png]]
| type             =[[Private limited company|Private]]
| location_city    =[[Chantilly, Virginia|Chantilly]], [[Virginia]]
| location_country =[[United States]]
| founder          ={{Unbulleted list|Mike Boggs|}} 
| foundation       =2006 
| area_served      =Worldwide
| key_people       =Madan K. ([[CEO]])&lt;br /&gt;Mike Boggs ([[Chief Technology Officer|CTO]] &amp; Founder)&lt;br/&gt;
Sam Benedict (VP-Sales &amp; Marketing)&lt;br/&gt; John Carter (Director of Professional Services)
| industry         =[[Software Company]]
| services         =IT, [[business consulting]] and automation services
| homepage         ={{URL|www.analytixds.com}}
| intl             =yes
}}
''' AnalytiX Data Services '''  is a software vendor that provides specialized data mapping and ETL conversion tools for [[data integration]], [[data management]], [[enterprise application integration]] and [[big data]] software and services.&lt;ref&gt;{{cite web|title=Mapping Manager, the missing link in moving data around|url=http://www.bloorresearch.com/analysis/analytix-mapping-manager-missing-link-moving-data|publisher=Bloor|accessdate=17 September 2014}}&lt;/ref&gt;  Headquarter's based in Chantilly, Virginia, AnalytiX DS has offices in Dallas, TX and Hyderabad, India and an international network of technical and services partners.&lt;ref&gt;{{cite web|title=AnalytiX DS Partners|url=http://analytixds.com/technology-partners/}}&lt;/ref&gt;

Michael Boggs, the CTO and founder of AnalytiX DS coined the term "Pre ETL Mapping" now it is widely used and accepted synonym for design phase of data integration. 

Mapping Manager is the flagship product of AnalytiX DS, an agile Unified Platform designed to govern and accelerate data integration processes by eliminating manual processes and replacing them with software designed to automate, govern and accelerate manual processes.&lt;ref&gt;{{cite web|title=Mapping Manager|url=http://analytixds.com/amm/}}&lt;/ref&gt;  Several versions of Mapping Manager were released since the release of its first version in 2006. 

AnalytiX DS in April 2016 launched Mapping Manager Version 7.0, a major release version which extends the unified platform for enterprise data mapping, governance and automation.&lt;ref&gt;{{cite web|title=AnalytiX Data Services announces the launch of major release of Mapping Manager Version 7.0|url=http://www.pr.com/press-release/680901|publisher= PR |accessdate= 27 July 2016}}&lt;/ref&gt;The latest release has added plenty of features and revolutionary modules never before seen.

With every new release, out of box features and functionalities were introduced. Later, AnalytiX DS began to add new modules including Release Management, Reference Data Manager, Code Set Manager, CATfX, LiteSpeed Conversion, Code automation Templates for Data Vault, Mapping Manager Big Data Edition, Data Quality Assessment Manager(DQAM), Metadata Management, Data Vault-Code Gen Bundle, and Test Manager, which extends the tools capabilities above and beyond management of the data mapping process.

[[Mapping Manager Big Data Edition]]:&lt;ref&gt;{{cite web|title=Mapping Manager Big Data Edition|url=http://analytixds.com/mapping-manager-bigdata-edition/}}&lt;/ref&gt;  Helps automate the big data mapping process and provides a bridge between structured and unstructured data to meet big data challenges.

[[Release Management]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Release Manager|url=http://analytixds.com/arm/}}&lt;/ref&gt; Helps track the release process approvals, audits and verifications through the approval process.

[[Reference Data Manager]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Reference Data Manager|url=http://analytixds.com/rdm/}}&lt;/ref&gt;  Helps create database like structure to maintain all reference data. 

[[Code Set Manager]]:&lt;ref&gt;{{cite web|title=AnalytiX DS CodeSet Manager|url=http://analytixds.com/code-set-manager/}}&lt;/ref&gt;  Helps drive the organization of user defined reference data and Code Sets across an enterprise.

[[Customizable Code-Automation Framework (CATfX)]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Customizable Code-Automation Framework (CATfX)|url=http://analytixds.com/catfx-code-automation-templates/}}&lt;/ref&gt;  Helps automate manual coding and tasks for ETL integration and data profiling, Testing Automation and more.

[[LiteSpeed Conversion]]:&lt;ref&gt;{{cite web|title=AnalytiX DS LiteSpeed Conversion|url=http://analytixds.com/litespeed-conversion/}}&lt;/ref&gt;  Helps automate the conversion of ETL tool platforms through an automated framework.

[[Code automation Templates for Data Vault]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Code automation Templates for Data Vault|url=http://analytixds.com/data-vault-automation/}}&lt;/ref&gt; Helps automatically generate the Hub ETL code, Link ETL code and the Satellite ETL code through your existing ETL Platform. 

[[Data Quality Assessment Manager(DQAM)]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Data Quality Assessment Manager(DQAM)|url=http://analytixds.com/data-quality-assessment-manager/}}&lt;/ref&gt;  Helps standardize and execute a formal data quality assessment methodology. 

[[Metadata Management]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Metadata Management|url=http://analytixds.com/metadata-management/}}&lt;/ref&gt; Helps metadata in Mapping Manager Unified Platform to be ported into third party metadata environments. 

[[Data Vault-Code Gen Bundle]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Data Vault-Code Gen Bundle|url=http://analytixds.com/data-vault-code-gen-bundle/}}&lt;/ref&gt;  Helps automate the code-generation process for building the Data Vault through Code automation Templates (CAT's). 

[[Test Manager]]:&lt;ref&gt;{{cite web|title=AnalytiX DS Test Manager|url=http://analytixds.com/test-manager/}}&lt;/ref&gt; Helps Test Cases and Test SQL generation to be managed in a purpose built module for testing data mappings and ETL processes. 

Recently, AnalytiX DS expanded its presence in the US with the opening of a new office in Dallas. 

AnalytiX DS has been named to CIO Review's 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015.&lt;ref&gt;{{cite web|title=CIO Review's 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015|url=http://www.pr.com/press-release/662862|publisher= PR |accessdate= 18 March 2016}}&lt;/ref&gt;AnalytiX DS is a platinum sponsor for WWDVC 2016.&lt;ref&gt;{{cite web|title=AnalytiX Data Services To Platinum-Sponsor for WWDVC 2016|url=http://wwdvc.com/sponsors/}}&lt;/ref&gt;


== References ==
{{reflist}}

==External links==
* {{official website|http://www.analytixds.com/}}
* {{YouTube|u=AnalytiXDS|AnalytiX DS}}
* {{Facebook|AnalytiX.Data.Services|AnalytiX DS}}

[[Category:Software companies of India]]
[[Category:Data management]]
[[Category:Extract, transform, load tools]]
[[Category:Data mapping]]
[[Category:Data warehousing products]]
[[Category:International information technology consulting firms]]
[[Category:Multinational companies headquartered in the United States]]</text>
      <sha1>kdivrxcn5ho7e2mar2z5ore2z5khs8z</sha1>
    </revision>
  </page>
  <page>
    <title>Systems of Engagement</title>
    <ns>0</ns>
    <id>43443443</id>
    <revision>
      <id>693760401</id>
      <parentid>656736565</parentid>
      <timestamp>2015-12-04T18:24:27Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor />
      <comment>Fixing [[Wikipedia:Disambiguation pages with links|links to disambiguation pages]], replaced: [[HP]] → [[Hewlett-Packard|HP]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2676" xml:space="preserve">The concept of '''Systems of Engagement''' is attributed to [[Geoffrey Moore]], a business author of such books as Crossing the Chasm.&lt;ref&gt;Crossing the Chasm: Marketing and Selling High-tech Products to Mainstream Customers (1991, revised 1999 and 2014) – ISBN 0-06-051712-3&lt;/ref&gt; In his paper for AIIM.org entitled: "Systems of Engagement and the Future of Enterprise IT" Moore states:
“Amidst the texting and Twittering and Facebooking of a generation of digital natives, the fundamentals of next-generation communication and collaboration are being worked out. For them, it is clear, there is no going back. So at minimum, if you expect these folks to be your customers, your employees, and your citizens (and, frankly, where else could you look?), then you need to apply THEIR expectations to the next generation of enterprise IT systems....Systems of Engagement … will overlay and complement our deep investments in systems of record.”&lt;ref&gt;Moore, Geoffrey (2011). "Systems of Engagement and the Future of Enterprise IT". http://www.aiim.org/futurehistory&lt;/ref&gt;{{cite news |last=Moore |first=Geoffrey |date=2011 |title=Systems of Engagement and the Future of Enterprise IT: A Sea Change in Enterprise IT |url=http://www.aiim.org/futurehistory |accessdate=October 7, 2014}}
Since then Systems of Engagement has been adopted by organizations such as [http://blogs.forrester.com/category/systems_of_engagement Forrester Research], [[Hewlett-Packard|HP]], [[IBM]], [[AIIM]], and [http://www.avoka.com/blog/2013/07/deliver-a-system-of-engagement/ Avoka]. Forrester defines Systems of Engagement as follows: "Systems of engagement are different from the traditional systems of record that log transactions and keep the financial accounting in order: They focus on people, not processes....These new systems harness a perfect storm of mobile, social, cloud, and big data innovation to deliver apps and smart products directly in the context of the daily lives and real-time workflows of customers, partners, and employees.”&lt;ref&gt;http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement&lt;/ref&gt; {{cite news |last=Schadler |first=Ted |date=February 14, 2012 |title=A Billion Smartphones Require New Systems Of Engagement |url=http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement |accessdate=October 7, 2014 }}

==See also==
* [[System of record]] — conventional enterprise systems designed to contain the authoritative data source for a given piece of information.
==References==
{{Reflist}}


[[Category:Information systems]]
[[Category:Data management]]


{{compu-stub}}</text>
      <sha1>0r73pl47bc7gdhxndqsdlsju0e4gp08</sha1>
    </revision>
  </page>
  <page>
    <title>SciDB</title>
    <ns>0</ns>
    <id>38334530</id>
    <revision>
      <id>706251516</id>
      <parentid>701800207</parentid>
      <timestamp>2016-02-22T08:18:36Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <comment>[[Help:Cat-a-lot|Cat-a-lot]]: Moving from [[Category:Software using the AGPL license]] to [[Category:Software using the GNU AGPL license]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2813" xml:space="preserve">{{Infobox software
| title       = SciDB
| developer   = Paradigm4
| publisher   =
| genre       = Database management system
| license     = [[Affero General Public License|AGPL]] v3&lt;ref&gt;[http://www.scidb.org/forum/viewtopic.php?f=16&amp;t=364 Download &amp; licensing]&lt;/ref&gt;
| first       =
| last        =
| coauthors   =
| released    = 2008
| website     = {{URL|http://www.paradigm4.com/}}
}}
{{Distinguish2|[http://scidb.sourceforge.net/ Scidb: Chess database]}}

'''SciDB'''&lt;ref&gt;{{cite web | url = http://www.scidb.org/ | title=SciDB website}}&lt;/ref&gt; is an array database designed for multidimensional data management and analytics common to scientific, geospatial, financial, and industrial applications.  It is developed by Paradigm4, co-founded by [[Michael Stonebraker]].

==History and Characteristics==
[[Michael Stonebraker]] co-created SciDB where, he claims, arrays are 100 or so times faster than a RDBMS on this class of problem.&lt;ref&gt;{{cite web | url = http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview | title=Stonebraker interview}}&lt;/ref&gt; It is swapping rows and columns for mathematical arrays that put fewer restrictions on the data and can work in any number of dimensions unlike the conventionally widely used [[relational database management system]] model, in which each [[Relation (database)|relation]] supports only one dimension of records.

According to a Strata Conference presentation on SciDB,&lt;ref&gt;{{cite web |url=http://strataconf.com/stratany2011/public/schedule/detail/21376 |title=Big Data and Big Analytics: SciDB is not Hadoop}}&lt;/ref&gt; it natively supports:
* An array data model for efficient storage and manipulation of larger-than-memory multi-dimensional arrays.
* Data versioning and provenance to allow tracking results back to original supporting data.
* What-if modeling, back-testing, and re-analysis.
* Massive scale math on the arrays for linear algebra and analytics.
* Uncertainty can be modeled by associating error-bars with data.
* Efficient storage.

==See also==
&lt;!-- Please do not list specific implementations here! --&gt;
* [[Comparison of object database management systems]]
* [[Comparison of structured storage software]]

== References ==
{{Reflist|2}}

==External links==
* [http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview/ Michael Stonebraker interview]
* [http://itknowledgeexchange.techtarget.com/soa-talk/stonebraker-sees-high-programming-overhead-for-nosql/ Stonebraker sees high programming overhead for NoSQL]

[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Document-oriented databases]]
[[Category:Distributed computing architecture]]
[[Category:Free database management systems]]
[[Category:Structured storage]]
[[Category:NoSQL]]
[[Category:Software using the GNU AGPL license]]</text>
      <sha1>3zalu0yyeryiam7p078jwbxnu13b44m</sha1>
    </revision>
  </page>
  <page>
    <title>Tuple</title>
    <ns>0</ns>
    <id>132729</id>
    <revision>
      <id>760773110</id>
      <parentid>754254433</parentid>
      <timestamp>2017-01-19T00:14:56Z</timestamp>
      <contributor>
        <ip>49.149.201.73</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15122" xml:space="preserve">{{About|the mathematical concept|the musical term|Tuplet}}
{{Redir|Octuple|the type of rowing boat|Octuple scull}}
{{Redir|Duodecuple|the term in music|Twelve-tone technique}}

A '''tuple''' is a finite ordered list of [[Element (mathematics)|elements]].  In [[mathematics]], an '''{{math|''n''}}-tuple''' is a [[sequence]] (or ordered list) of {{math|''n''}} elements, where {{math|''n''}} is a non-negative integer. There is only one 0-tuple, an empty sequence. An {{math|''n''}}-tuple is [[Recursive definition|defined inductively]] using the construction of an [[ordered pair]]. Tuples are usually written by listing the elements within parentheses "&lt;math&gt;(\text{ })&lt;/math&gt;" and separated by commas; for example, &lt;math&gt;(2, 7, 4, 1, 7)&lt;/math&gt; denotes a 5-tuple. Sometimes other symbols are used to surround the elements, such as square brackets "[ ]" or angle brackets "&lt; &gt;". Braces "{ }" are only used in defining arrays in some programming languages such as [[Java (programming language)|Java]], but not in mathematical expressions, as they are the standard notation for [[set (mathematics)|sets]]. Tuples are often used to describe other mathematical objects, such as [[Vector (mathematics and physics)|vectors]]. In computer science, tuples are directly implemented as [[product type]]s in most [[functional programming|functional programming languages]].{{citation needed|date=January 2016}} More commonly, they are implemented as [[Record (computer science)|record types]], where the components are labeled instead of being identified by position alone.{{citation needed|date=January 2016}} This approach is also used in [[relational algebra]]. Tuples are also used in relation to programming the [[semantic web]]  with the [[Resource Description Framework]] (RDF). Tuples are also used in [[linguistics]]&lt;ref&gt;{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199202720.001.0001/acref-9780199202720-e-2276|title=N‐tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}&lt;/ref&gt; and [[philosophy]].&lt;ref&gt;{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199541430.001.0001/acref-9780199541430-e-2262|title=Ordered n-tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}&lt;/ref&gt;

==Etymology==
The term originated as an abstraction of the sequence: single, double, triple, quadruple, quintuple, sextuple, septuple, octuple, ..., {{math|''n''}}‑tuple, ..., where the prefixes are taken from the [[Latin]] names of the numerals. The unique 0‑tuple is called the null tuple. A 1‑tuple is called a singleton, a 2‑tuple is called an ordered pair and a 3‑tuple is a triple or triplet. {{math|''n''}} can be any nonnegative [[integer]]. For example, a [[complex number]] can be represented as a 2‑tuple, a [[quaternion]] can be represented as a 4‑tuple, an [[octonion]] can be represented as an 8‑tuple and a [[sedenion]] can be represented as a 16‑tuple.

Although these uses treat ''‑tuple'' as the suffix, the original suffix was ''‑ple'' as in "triple" (three-fold) or "decuple" (ten‑fold). This originates from  [[medieval Latin]]  ''plus'' (meaning "more") related to [[Greek language|Greek]] ‑πλοῦς, which replaced the classical and late antique  ''‑plex'' (meaning "folded"), as in "duplex".&lt;ref&gt;''OED'', ''s.v.'' "triple", "quadruple", "quintuple", "decuple"&lt;/ref&gt;

===Names for tuples of specific lengths===

{| class="wikitable"
|-
! Tuple Length &lt;math&gt;n&lt;/math&gt; !! Name !! Alternative names
|-
| align="right" | 0 || empty tuple || unit / empty sequence
|-
| align="right" | 1 || single || [[Singleton_(mathematics)|singleton]] / monuple 
|-
| align="right" | 2 || double || couple / (ordered) pair / dual / twin / product
|-
| align="right" | 3 || triple || treble / triplet / triad
|-
| align="right" | 4 || quadruple || quad
|-
| align="right" | 5 || quintuple || pentuple
|-
| align="right" | 6 || sextuple ||hextuple
|-
| align="right" | 7 || septuple ||heptuple
|-
| align="right" | 8 || octuple || 
|-
| align="right" | 9 || nonuple || 
|-
| align="right" | 10 || decuple || 
|-
| align="right" | 11 || undecuple || hendecuple
|-
| align="right" | 12 || duodecuple || 
|-
| align="right" | 13 || tredecuple || 
|-
| align="right" | 14 || quattuordecuple || 
|-
| align="right" | 15 || quindecuple ||
|-
| align="right" | 16 || sexdecuple ||
|-
| align="right" | 17 || septendecuple ||
|-
| align="right" | 18 || octodecuple ||
|-
| align="right" | 19 || novemdecuple ||
|-
| align="right" | 20 || vigintuple ||
|-
| align="right" | 21 || unvigintuple ||
|-
| align="right" | 22 || duovigintuple ||
|-
| align="right" | 23 || trevigintuple ||
|-
| align="right" | 24 || quattuorvigintuple ||
|-
| align="right" | 25 || quinvigintuple ||
|-
| align="right" | 26 || sexvigintuple ||
|-
| align="right" | 27 || septenvigintuple ||
|-
| align="right" | 28 || octovigintuple ||
|-
| align="right" | 29 || novemvigintuple ||
|-
| align="right" | 30 || trigintuple ||
|-
| align="right" | 31 || untrigintuple ||
|-
| align="right" | 40 || quadragintuple ||
|-
| align="right" | 50 || quinquagintuple ||
|-
| align="right" | 60 || sexagintuple ||
|-
| align="right" | 70 || septuagintuple ||
|-
| align="right" | 80 || octogintuple ||
|-
| align="right" | 90 || nongentuple ||
|-
| align="right" | 100 || centuple ||
|-
| align="right" | 1,000 || milluple ||
|-
|}

==Properties==
The general rule for the identity of two {{math|''n''}}-tuples is
: &lt;math&gt;(a_1, a_2, \ldots, a_n) = (b_1, b_2, \ldots, b_n)&lt;/math&gt; [[if and only if]] &lt;math&gt;a_1=b_1,\text{ }a_2=b_2,\text{ }\ldots,\text{ }a_n=b_n.&lt;/math&gt;

Thus a tuple has properties that distinguish it from a [[Set (mathematics)|set]].
# A tuple may contain multiple instances of the same element, so {{break|
}}tuple &lt;math&gt;(1,2,2,3) \neq (1,2,3)&lt;/math&gt;; but set &lt;math&gt;\{1,2,2,3\} = \{1,2,3\}&lt;/math&gt;.
# Tuple elements are ordered: tuple &lt;math&gt;(1,2,3) \neq (3,2,1)&lt;/math&gt;, but set &lt;math&gt;\{1,2,3\} = \{3,2,1\}&lt;/math&gt;.
# A tuple has a finite number of elements, while a set or a [[multiset]] may have an infinite number of elements.

==Definitions==

There are several definitions of tuples that give them the properties described in the previous section.

===Tuples as functions===
If we are dealing with sets, an {{math|''n''}}-tuple can be regarded as a [[Function (mathematics)#Definition|function]], {{math|''F''}},  whose domain is the tuple's implicit set of element indices, {{math|''X''}}, and whose codomain, {{math|''Y''}}, is the tuple's set of elements. Formally:
: &lt;math&gt;(a_1, a_2, \dots, a_n) \equiv (X,Y,F)&lt;/math&gt;
where:
: &lt;math&gt;
    \begin{align}
      X &amp; = \{1, 2, \dots, n\}                       \\
      Y &amp; = \{a_1, a_2, \ldots, a_n\}                \\
      F &amp; = \{(1, a_1), (2, a_2), \ldots, (n, a_n)\}. \\
    \end{align}
  &lt;/math&gt;
In slightly less formal notation this says:
:&lt;math&gt; (a_1, a_2, \dots, a_n) := (F(1), F(2), \dots, F(n)).&lt;/math&gt;

===Tuples as nested ordered pairs===
Another way of modeling tuples in Set Theory is as nested [[ordered pair]]s. This approach assumes that the notion of ordered pair has already been defined; thus a 2-tuple 
# The 0-tuple (i.e. the empty tuple) is represented by the empty set &lt;math&gt;\emptyset&lt;/math&gt;.
# An {{math|''n''}}-tuple, with {{math|''n'' &gt; 0}}, can be defined as an ordered pair of its first entry and an {{math|(''n'' − 1)}}-tuple (which contains the remaining entries when {{math|''n'' &gt; 1)}}:
#: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = (a_1, (a_2, a_3, \ldots, a_n))&lt;/math&gt;
This definition can be applied recursively to the {{math|(''n'' − 1)}}-tuple:
: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = (a_1, (a_2, (a_3, (\ldots, (a_n, \emptyset)\ldots))))&lt;/math&gt;

Thus, for example:
: &lt;math&gt;
    \begin{align}
         (1, 2, 3) &amp; = (1, (2, (3, \emptyset)))      \\
      (1, 2, 3, 4) &amp; = (1, (2, (3, (4, \emptyset)))) \\
    \end{align}
  &lt;/math&gt;

A variant of this definition starts "peeling off" elements from the other end:
# The 0-tuple is the empty set &lt;math&gt;\emptyset&lt;/math&gt;.
# For {{math|''n'' &gt; 0}}:
#: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = ((a_1, a_2, a_3, \ldots, a_{n-1}), a_n)&lt;/math&gt;
This definition can be applied recursively:
: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = ((\ldots(((\emptyset, a_1), a_2), a_3), \ldots), a_n)&lt;/math&gt;

Thus, for example:
: &lt;math&gt;
    \begin{align}
         (1, 2, 3) &amp; = (((\emptyset, 1), 2), 3)      \\
      (1, 2, 3, 4) &amp; = ((((\emptyset, 1), 2), 3), 4) \\
    \end{align}
  &lt;/math&gt;

===Tuples as nested sets===
Using [[ordered pair#Kuratowski definition|Kuratowski's representation for an ordered pair]], the second definition above can be reformulated in terms of pure [[set theory]]:
# The 0-tuple (i.e. the empty tuple) is represented by the empty set &lt;math&gt;\emptyset&lt;/math&gt;;
# Let &lt;math&gt;x&lt;/math&gt; be an {{math|''n''}}-tuple &lt;math&gt;(a_1, a_2, \ldots, a_n)&lt;/math&gt;, and let &lt;math&gt;x \rightarrow b \equiv (a_1, a_2, \ldots, a_n, b)&lt;/math&gt;. Then, &lt;math&gt;x \rightarrow b \equiv \{\{x\}, \{x, b\}\}&lt;/math&gt;.  (The right arrow, &lt;math&gt;\rightarrow&lt;/math&gt;, could be read as "adjoined with".)

In this formulation:
: &lt;math&gt;
   \begin{array}{lclcl}
     ()      &amp; &amp;                     &amp;=&amp; \emptyset                                    \\
             &amp; &amp;                     &amp; &amp;                                              \\
     (1)     &amp;=&amp; ()    \rightarrow 1 &amp;=&amp; \{\{()\},\{(),1\}\}                          \\
             &amp; &amp;                     &amp;=&amp; \{\{\emptyset\},\{\emptyset,1\}\}            \\
             &amp; &amp;                     &amp; &amp;                                              \\
     (1,2)   &amp;=&amp; (1)   \rightarrow 2 &amp;=&amp; \{\{(1)\},\{(1),2\}\}                        \\
             &amp; &amp;                     &amp;=&amp; \{\{\{\{\emptyset\},\{\emptyset,1\}\}\},     \\
             &amp; &amp;                     &amp; &amp; \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\}    \\
             &amp; &amp;                     &amp; &amp;                                              \\
     (1,2,3) &amp;=&amp; (1,2) \rightarrow 3 &amp;=&amp; \{\{(1,2)\},\{(1,2),3\}\}                    \\
             &amp; &amp;                     &amp;=&amp; \{\{\{\{\{\{\emptyset\},\{\emptyset,1\}\}\}, \\
             &amp; &amp;                     &amp; &amp; \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\}\}, \\
             &amp; &amp;                     &amp; &amp; \{\{\{\{\{\emptyset\},\{\emptyset,1\}\}\},   \\
             &amp; &amp;                     &amp; &amp; \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\},3\}\}                                       \\
    \end{array}
  &lt;/math&gt;

=={{anchor|n-tuple}}{{math|''n''}}-tuples of {{math|''m''}}-sets ==

In [[discrete mathematics]], especially [[combinatorics]] and finite [[probability theory]], {{math|''n''}}-tuples arise in the context of various counting problems and are treated more informally as ordered lists of length {{math|''n''}}.&lt;ref&gt;{{harvnb|D'Angelo|West|2000|p=9}}&lt;/ref&gt; {{math|''n''}}-tuples whose entries come from a set of {{math|''m''}} elements are also called ''arrangements with repetition'', ''permutations of a multiset'' and, in some non-English literature, ''[[Variation (disambiguation)#Mathematics|variations]] with repetition''. The number of {{math|''n''}}-tuples of an {{math|''m''}}-set is {{math|''m''&lt;sup&gt;''n''&lt;/sup&gt;}}. This follows from the combinatorial [[rule of product]].&lt;ref&gt;{{harvnb|D'Angelo|West|2000|p=101}}&lt;/ref&gt; If {{math|''S''}} is a finite set of [[cardinality]] {{math|''m''}}, this number is the cardinality of the {{math|''n''}}-fold [[Cartesian_product#Cartesian_power | Cartesian power]] {{math|''S'' × ''S'' × ... ''S''}}. Tuples are elements of this product set.

== Type theory ==
{{main|Product type}}
In [[type theory]], commonly used in [[programming language]]s, a tuple has a [[product type]]; this fixes not only the length, but also the underlying types of each component. Formally:
: &lt;math&gt;(x_1, x_2, \ldots, x_n) : \mathsf{T}_1 \times \mathsf{T}_2 \times \ldots \times \mathsf{T}_n&lt;/math&gt;
and the [[Projection (mathematics)|projection]]s are term constructors:
: &lt;math&gt;\pi_1(x) : \mathsf{T}_1,~\pi_2(x) : \mathsf{T}_2,~\ldots,~\pi_n(x) : \mathsf{T}_n&lt;/math&gt;

The tuple with labeled elements used in the [[#Relational_model|relational model]] has a [[Record (computer science)|record type]]. Both of these types can be defined as simple extensions of the [[simply typed lambda calculus]].&lt;ref name="pierce2002"&gt;{{cite book|last=Pierce|first=Benjamin|title=Types and Programming Languages|publisher=MIT Press|year=2002|isbn=0-262-16209-1|pages=126–132}}&lt;/ref&gt;

The notion of a tuple in type theory and that in set theory are related in the following way: If we consider the natural [[model theory|model]] of a type theory, and use the Scott brackets to indicate the semantic interpretation&lt;!-- do not link; that article needs to be a dab first--&gt;, then the model consists of some sets &lt;math&gt;S_1, S_2, \ldots, S_n&lt;/math&gt; (note: the use of italics here that distinguishes sets from types) such that:
: &lt;math&gt;[\![\mathsf{T}_1]\!] = S_1,~[\![\mathsf{T}_2]\!] = S_2,~\ldots,~[\![\mathsf{T}_n]\!] = S_n&lt;/math&gt;
and the interpretation of the basic terms is:
: &lt;math&gt;[\![x_1]\!] \in [\![\mathsf{T}_1]\!],~[\![x_2]\!] \in [\![\mathsf{T}_2]\!],~\ldots,~[\![x_n]\!] \in [\![\mathsf{T}_n]\!]&lt;/math&gt;.

The {{math|''n''}}-tuple of type theory has the natural interpretation as an {{math|''n''}}-tuple of set theory:&lt;ref&gt;Steve Awodey, [http://www.andrew.cmu.edu/user/awodey/preprints/stcsFinal.pdf ''From sets, to types, to categories, to sets''], 2009, [[preprint]]&lt;/ref&gt;
: &lt;math&gt;[\![(x_1, x_2, \ldots, x_n)]\!] = (\,[\![x_1]\!], [\![x_2]\!], \ldots, [\![x_n]\!]\,)&lt;/math&gt;
The [[unit type]] has as semantic interpretation the 0-tuple.

==See also==
{{Wiktionary|tuple}}
* [[Arity]]
* [[Exponential object]]
* [[Formal language]]
* [[Multidimensional Expressions#MDX data types|OLAP: Multidimensional Expressions]]
* [[Prime k-tuple]]
* [[Relation (mathematics)]]
* [[Tuplespace]]

==Notes==
{{Reflist}}

==References==

{{refbegin}}
* {{citation|first1=John P.|last1=D'Angelo|first2=Douglas B.|last2=West|title=Mathematical Thinking/Problem-Solving and Proofs|year=2000|edition=2nd|publisher=Prentice-Hall|isbn=978-0-13-014412-6}}
* [[Keith Devlin]], ''The Joy of Sets''. Springer Verlag, 2nd ed., 1993, ISBN 0-387-94094-4, pp.&amp;nbsp;7–8
* [[Abraham Adolf Fraenkel]], [[Yehoshua Bar-Hillel]], [[Azriel Lévy]], ''[https://books.google.com/books?q=Foundations+of+set+theory&amp;btnG=Search+Books Foundations of set theory]'', Elsevier Studies in Logic Vol. 67, Edition 2, revised, 1973, ISBN 0-7204-2270-1, p.&amp;nbsp;33
* [[Gaisi Takeuti]], W. M. Zaring, ''Introduction to Axiomatic Set Theory'', Springer [[Graduate texts in mathematics|GTM]] 1, 1971, ISBN 978-0-387-90024-7, p.&amp;nbsp;14
* George J. Tourlakis, ''[https://books.google.com/books?as_isbn=9780521753746 Lecture Notes in Logic and Set Theory. Volume 2: Set theory]'', Cambridge University Press, 2003, ISBN 978-0-521-75374-6, pp.&amp;nbsp;182–193

{{refend}}
{{Set theory}}

&lt;!--Interwikies--&gt;

&lt;!--Categories--&gt;
{{Authority control}}
[[Category:Data management]]
[[Category:Mathematical notation]]
[[Category:Sequences and series]]
[[Category:Basic concepts in set theory]]
[[Category:Type theory]]
[[ar:زوج مرتب]]</text>
      <sha1>l07rhz3l24h92mfbfw2ryxc0yhcy71n</sha1>
    </revision>
  </page>
  <page>
    <title>Data recovery hardware</title>
    <ns>0</ns>
    <id>46219812</id>
    <revision>
      <id>757508648</id>
      <parentid>754251533</parentid>
      <timestamp>2016-12-31T02:15:00Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>new key for [[Category:Data recovery]]: "*" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3808" xml:space="preserve">{{more footnotes|date=March 2015}}

'''Data recovery hardware''' was developed because [[data recovery software]] lacks the ability to deal with all lost or corrupted data files. Often the failures, such as media files with [[bad sectors]], [[firmware]] failures, PCB ([[Printed circuit board]]) failures, hard drive head failures, etc., cannot be fixed.

==Bad sectors==

The two types of bad sectors are "physical" and "logical" bad sectors, or "hard" and "soft" bad sectors.&lt;ref&gt;{{cite web
 | url = http://www.howtogeek.com/173463/bad-sectors-explained-why-hard-drives-get-bad-sectors-and-what-you-can-do-about-it/
 | title = Bad Sectors Explained: Why Hard Drives Get Bad Sectors and What You Can Do About It
 | date = October 9, 2013| accessdate = March 26, 2015
 | author = Chris Hoffman  | website = How-To Geek, LLC
}}&lt;/ref&gt;

When a disk has physical bad sectors, software cannot effectively offer soft reset, hard reset, power reset, error handling, read algorithm auto exchange nor skip sectors. If a disk with bad physical sectors is connected to a [[Personal computer|PC]], the condition would potentially not be detected.

Soft bad sectors can potentially be fixed by either data recovery software or hardware, depending on the damaged condition. Some amount of bad sectors can be skipped using software, while a severely corrupted disk with a large area of bad sectors may potentially only be repaired.&lt;ref&gt;{{cite web
 | url = https://www.winxdvd.com/resource/repair-mp4-file-free.htm
 | title = How to Repair Corrupted MP4 Video File
 | date = July 3, 2015| accessdate = August 24, 2016
 | author = Estrella Garcia  | website = WinXDVD
}}&lt;/ref&gt;  Bad sectors are areas on the hard drive that cannot be read. Even new hardrives sometimes contain bad sectors. Since manufacturers intensely compete to cram more space into disks, systems operate close to the limit of that generation of technology.&lt;ref&gt;{{Cite web|url=https://www.grc.com/sr/faq.htm|title=GRC {{!}} SpinRite 6.0 FAQ - Frequently Asked Questions|website=www.grc.com|access-date=2016-10-20}}&lt;/ref&gt;

==Dead PCB==

When the drive has dead PCB ([[Printed circuit board]]), users need to:
* swap in a new PCB
* put one donor [[Integrated circuit|chip]] and write by chip reader with matching ROM ([[Read-only memory]]) content
* get the dead drive spinning

When the drive has physical head damage, users need to open the drive in [[cleanroom]] environment and find donor heads or other donor components to swap.

==Data recovery hardware types==
*Disk image;
*File extraction hardware;
*Firmware repair hardware;
*ROM chip reader;
*Head and Platter Swap Tools (See [[Hard disk drive platter]]);
*Spindle release hardware;
*Other hardware

== See also ==
{{Portal|Computer security|Computing}}

{{Div col||20em}}
* [[Data recovery]]
* [[Firmware]]
* [[Bad sector]]
* [[Disk image]]
* [[Printed circuit board]]
* [[Cleanroom]]
* [[List of data recovery software]]
* [[Comparison of file systems]]
* [[Computer forensics]]
* [[Continuous data protection]]
* [[Data archaeology]]
* [[Data loss]]
* [[Error detection and correction]]
* [[File carving]]
* [[Hidden file and hidden directory]]
* [[Knowledge extraction]]
* [[Undeletion]]
{{Div col end}}

==Further reading==
* Tanenbaum, A. &amp; Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}
* {{cite web|url=https://www.grc.com/sr/faq.htm|title=GRC&amp;nbsp;-&amp;nbsp;SpinRite 6.0 FAQ - Frequently Asked Questions|publisher=}}

==References==
{{Reflist|30em}}

{{DEFAULTSORT:Data Recovery}}
[[Category:Data recovery|*]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Transaction processing]]
[[Category:Hard disk software|*]]
[[Category:Backup|Recovery]]</text>
      <sha1>cg6leipv6545xqw7o0x3njop1o20kb8</sha1>
    </revision>
  </page>
  <page>
    <title>Software intelligence</title>
    <ns>0</ns>
    <id>45124802</id>
    <revision>
      <id>718367937</id>
      <parentid>653767789</parentid>
      <timestamp>2016-05-03T03:19:43Z</timestamp>
      <contributor>
        <username>Ljgua124</username>
        <id>2738115</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="780" xml:space="preserve">{{one source|date=January 2015}}
'''Software Intelligence''' ('''SI''') is [[software]] designed to analyze [[source code]] to better understand [[Information technology|Information Technology]] environments. Similarly to [[Business intelligence|Business Intelligence]] (BI), Software Intelligence is a set of software tools and techniques for the [[Data mining|mining of data]] into meaningful and useful information.&lt;ref&gt;{{cite web|last1=Hassan|first1=Ahmed|last2=Xie|first2=Tao|title=Software Intelligence: The Future of Mining Software Engineering Data|url=http://web.engr.illinois.edu/~taoxie/publications/foser10-si.pdf|website=http://web.engr.illinois.edu|accessdate=19 January 2015}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Source code]]</text>
      <sha1>ao18cg1oukc7m7frti9mh4afgx7jos8</sha1>
    </revision>
  </page>
  <page>
    <title>COMMIT (SQL)</title>
    <ns>0</ns>
    <id>46362717</id>
    <revision>
      <id>655680694</id>
      <timestamp>2015-04-09T14:08:00Z</timestamp>
      <contributor>
        <username>Skssxf</username>
        <id>16077845</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '{{Unreferenced|date=April 2015}} A &lt;code&gt;COMMIT&lt;/code&gt; statement in [[SQL]] ends a [[database transaction|transaction]] within a relational database management...'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1092" xml:space="preserve">{{Unreferenced|date=April 2015}}
A &lt;code&gt;COMMIT&lt;/code&gt; statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a &lt;code&gt;[[Begin work (SQL)|BEGIN WORK]]&lt;/code&gt; statement, one or more SQL statements, and then the &lt;code&gt;COMMIT&lt;/code&gt; statement. Alternatively, a &lt;code&gt;[[Rollback (data management)|ROLLBACK]]&lt;/code&gt; statement can be issued, which undoes all the work performed since &lt;code&gt;BEGIN WORK&lt;/code&gt; was issued. A &lt;code&gt;COMMIT&lt;/code&gt; statement will also release any existing [[savepoint]]s that may be in use.

In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].

==See also==
* [[Commit (data management)]]
* [[Atomic commit]]
* [[Two-phase commit protocol]]
* [[Three-phase commit protocol]]

{{databases}}

{{DEFAULTSORT:Commit (Data Management)}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Transaction processing]]

{{comp-sci-stub}}</text>
      <sha1>s6ikhk8l9cz53v6vcsls8on6aoinozu</sha1>
    </revision>
  </page>
  <page>
    <title>Author Name Disambiguation</title>
    <ns>0</ns>
    <id>46474403</id>
    <revision>
      <id>718915217</id>
      <parentid>674329228</parentid>
      <timestamp>2016-05-06T12:04:02Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor />
      <comment>/* top */clean up, added [[CAT:O|orphan]], [[CAT:UL|underlinked]] tags using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1672" xml:space="preserve">{{Multiple issues|
{{Underlinked|date=May 2016}}
{{Orphan|date=May 2016}}
{{refimprove|date=April 2015}}
}}

'''Author name disambiguation''' is a type of [[Record linkage]] that is applied to scholarly documents where the goal is to find all mentions of the same author and cluster them together. Authors of scholarly documents often share names which makes it hard to distinguish each author's work. Hence, author name disambiguation aims to find all publications that belong to a given author and distinguish them from publications of other authors who share the same name.

There are multiple reasons that cause author names to be ambiguous, among which: individuals may publish under multiple names for variety of reasons including different spelling, misspelling, name change due to marriage, or the use of middle names and initials.&lt;ref&gt;{{cite journal
 | authorlink = Smalheiser, Neil R and Torvik, Vetle I
 | title = Author name disambiguation
 | journal = [[Annual Review of Information Science and Technology]]
 | url = http://onlinelibrary.wiley.com/doi/10.1002/aris.2009.1440430113/full
 | accessdate = 2015-04-20
 | doi = 10.1002/aris.2009.1440430113
}}&lt;/ref&gt;

Typical approach for author name disambiguation rely on information about the authors such as their affiliations, email addresses, year of publication, co-authors, topic information to distinguish between authors. These information can be used to learn a machine learning classifier that decides whether two mentions refer to the same author or not. Other approaches utilized heuristics to distinguish between authors.

==References==
{{Reflist}}

[[Category:Metadata]]
[[Category:Data management]]</text>
      <sha1>omckuj5zcx4m8l7hzdmcqy3kt2rbmdj</sha1>
    </revision>
  </page>
  <page>
    <title>Data processing system</title>
    <ns>0</ns>
    <id>466099</id>
    <revision>
      <id>759707342</id>
      <parentid>759706902</parentid>
      <timestamp>2017-01-12T19:03:41Z</timestamp>
      <contributor>
        <username>Marianna251</username>
        <id>27604025</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/103.59.36.58|103.59.36.58]] ([[User talk:103.59.36.58|talk]]) ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5308" xml:space="preserve">{{other uses2|Data processing}}
{{refimprove|date=July 2013}}

A '''data processing system''' is a combination of machines, &lt;!-- "data processing" is specific to machines, there is no data processing in nature, see the OED --&gt; people, and processes that for a set of inputs produces a defined set of outputs.&lt;ref&gt;The first machines used for data processing were [[Unit record equipment|punched card machines]], now [[Computer]]s are used.&lt;/ref&gt;  The inputs and outputs are interpreted as data, facts, information, ... depending on the interpreter's relation to the system. A common synonymous term is "[[Information systems#Types of information systems|information system]]".&lt;ref name=Ralston&gt;{{cite book|title=Encyclopedia of Computer Science 4th ed.|author=Anthony Ralston et al (ed.)|year=2000|publisher=Nature Publishing Group|page=865}}&lt;/ref&gt;

A data processing system may involve some combination of:
* [[Data conversion|Conversion]] converting data to another format.
* [[Data validation|Validation]] &amp;ndash; Ensuring that supplied data is "clean, correct and useful."
* [[Sorting]] &amp;ndash; "arranging items in some sequence and/or in different sets."
* [[Summary statistic|Summarization]] &amp;ndash; reducing detail data to its main points.
* [[Aggregate data|Aggregation]] &amp;ndash; combining multiple pieces of data.
* [[Statistical analysis|Analysis]] &amp;ndash; the "collection, organization, analysis, interpretation and presentation of data.".
* Reporting &amp;ndash; list detail or summary data or computed information.

==Types of data processing systems==

===By application area===

====Scientific data processing====
Scientific data processing "usually involves a great deal of computation (arithmetic and comparison operations) upon a relatively small amount of input data, resulting in a small volume of output." &lt;ref name=Reddy&gt;{{cite book|last=Reddy|first=R.J.|title=Business Data Processing &amp; Computer Applications|year=2004|publisher=A P H Publishing Corporation|location=New Dehli|isbn=8176486493|page=17|url=https://books.google.com/books?id=FLKoXCts9ssC&amp;lpg=PA17&amp;dq=%22scientific%20data%20processing%22&amp;pg=PA17#v=onepage&amp;q=%22scientific%20data%20processing%22&amp;f=false}}&lt;/ref&gt;

====Commercial data processing====
Commercial data processing "involves a large volume of input data, relatively few computational operations, and a large volume of output."&lt;ref name=Reddy /&gt;  Accounting programs are the prototypical examples of data processing applications. [[Information systems|Information systems (IS)]] is the field that studies such organizational computer systems.

====Data analysis====
"[[Data analysis]] is a body of methods that help to describe facts, detect patterns,
develop explanations, and test hypotheses."&lt;ref&gt;{{cite web|last=Dartmouth College|title=Introduction: What Is Data Analysis?|url=http://www.dartmouth.edu/~mss/data%20analysis/Volume%20I%20pdf%20/006%20Intro%20%28What%20is%20the%20weal.pdf|accessdate=July 5, 2013}}&lt;/ref&gt;  For example, data analysis might be used to look at sales and customer data to "identify connections between products to allow for cross selling campaigns."&lt;ref&gt;{{cite book|last1=Berthold|first1=M.R.|last2=Borgelt|first2=C|last3=Hőppner|first3=F.|last4=Klawonn|first4=F|title=Guide to Intelligent Data Analysis|year=2010|publisher=Springer|isbn=978-1-84882-260-3|page=15}}&lt;/ref&gt;

===By service type&lt;ref name=Ralston /&gt;=== 

* [[Transaction processing system|Transaction processing systems]]
* [[Information retrieval|Information storage and retrieval systems]]
* Command and control systems
* Computing service systems
* [[Control system|Process control systems]]
* Message switching systems

==Examples==
===Simple example===
A very simple example of a data processing system is the process of maintaining a check register.  Transactions&amp;mdash; checks and deposits&amp;mdash; are recorded as they occur and the transactions are summarized to determine a current balance.  Monthly the data recorded in the register is reconciled with a hopefully identical list of transactions processed by the bank.

A more sophisticated record keeping system might further identify the transactions&amp;mdash; for example deposits by source or checks by type, such as charitable contributions.  This information might be used to obtain information like the total of all contributions for the year.

The important thing about this example is that it is a ''system'', in which, all transactions are recorded consistently, and the same method of bank reconciliation is used each time.

===Real-world example===
This is a [[flowchart]] of a data processing system combining manual and computerized processing to handle [[accounts receivable]], billing, and [[general ledger]]

[[File:Stockbridge system flowchart example.jpg]]
&lt;ref&gt;the highest acceleration of data processing the point of software&lt;/ref&gt;

==References==
{{Reflist}}

== See also ==
* [[Data processing]]
* [[Electronic data processing]]
* [[Computational science|Scientific computing]]
* [[Information processing system]] (broader term)

== Further reading ==
* Bourque, Linda B.; Clark, Virginia A. (1992) Processing Data: The Survey Example. (Quantitative Applications in the Social Sciences, no. 07-085). Sage Publications. ISBN 0-8039-4741-0


[[Category:Data management]]
[[Category:Data processing]]</text>
      <sha1>947vdp8np5v69av8s4tqs3n7ry8kiki</sha1>
    </revision>
  </page>
  <page>
    <title>Online transaction processing</title>
    <ns>0</ns>
    <id>2329992</id>
    <revision>
      <id>759139904</id>
      <parentid>759139867</parentid>
      <timestamp>2017-01-09T13:07:19Z</timestamp>
      <contributor>
        <username>CyanoTex</username>
        <id>24347243</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/203.99.197.212|203.99.197.212]] ([[User talk:203.99.197.212|talk]]) to last revision by Alymamlani. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7608" xml:space="preserve">'''Online transaction processing''', or '''OLTP''', is a class of [[information system]]s that facilitate and manage transaction-oriented applications, typically for data entry and retrieval [[transaction processing]].

The term is somewhat ambiguous; some understand a "transaction" in the context of computer or [[database transactions]], while others (such as the [[Transaction Processing Performance Council]]) define it in terms of business or [[financial transaction|commercial transactions]].&lt;ref&gt;[http://www.tpc.org/ Transaction Processing Performance Council website]&lt;/ref&gt; OLTP has also been used to refer to processing in which the system responds immediately to user requests. An [[automated teller machine]] (ATM) for a bank is an example of a commercial transaction processing application. Online transaction processing applications are high throughput and insert or update-intensive in database management. These applications are used concurrently by hundreds of users. The key goals of OLTP applications are availability, speed, concurrency and recoverability.&lt;ref&gt;[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]&lt;/ref&gt; Reduced paper trails and the faster, more accurate forecast for revenues and expenses are both examples of how OLTP makes things simpler for businesses. However, like many modern online information technology solutions, some systems require offline maintenance, which further affects the cost–benefit analysis of on line transaction processing system.

OLTP is typically contrasted to [[Online analytical processing|OLAP]] (online analytical processing), which is generally characterized by much more complex queries, in a smaller volume, for the purpose of business intelligence or reporting rather than to process transactions. Whereas OLTP systems process all kinds of queries (read, insert, update and delete), OLAP is generally optimized for read only and might not even support other kinds of queries.

==Overview==
OLTP system is a popular data processing system in today's enterprises.  Some examples of OLTP systems include order entry, retail sales, and financial transaction systems.&lt;ref&gt;What is an OLTP System[http://docs.oracle.com/cd/E11882_01/server.112/e25523/part_oltp.htm]&lt;/ref&gt;  On line transaction processing system increasingly requires support for transactions that span a network and may include more than one company. For this reason, modern on line transaction processing software use client or server processing and brokering software that allows transactions to run on different computer platforms in a network.

In large applications, efficient OLTP may depend on sophisticated transaction management software (such as [[CICS]]) and/or [[database]] optimization tactics to facilitate the processing of large numbers of concurrent updates to an OLTP-oriented database.

For even more demanding decentralized database systems, OLTP brokering programs can distribute transaction processing among multiple computers on a [[computer network|network]]. OLTP is often integrated into [[service-oriented architecture]] (SOA) and [[Web service]]s.

On line transaction processing (OLTP) involves gathering input information, processing the information and updating existing information to reflect the gathered and processed information. As of today, most organizations use a database management system to support OLTP. OLTP is carried in a client server system.

On line transaction process concerns about concurrency and atomicity.  Concurrency controls guarantee that two users accessing the same data in the database system will not be able to change that data or the user has to wait until the other user has finished processing, before changing that piece of data.  Atomicity controls guarantee that all the steps in transaction are completed successfully as a group. That is, if any steps between the transaction fail, all other steps must fail also.&lt;ref&gt;
[http://technet.microsoft.com/en-us/library/ms187669(v=sql.105).aspx On line Transaction Processing vs. Decision Support]&lt;/ref&gt;

==Systems design==
To build an OLTP system, a designer must know that the large number of concurrent users does not interfere with the system's performance.  To increase the performance of OLTP system, designer must avoid the excessive use of indexes and clusters.

The following elements are crucial for the performance of OLTP systems:&lt;ref&gt;[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]&lt;/ref&gt;
*Rollback segments
:Rollback segments are the portions of database that record the actions of transactions in the event that a transaction is rolled back.  Rollback segments provide read consistency, roll back transactions, and recover the database.&lt;ref&gt;[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76956/rollbak.htm Rollback]&lt;/ref&gt;
*Clusters
:A cluster is a [[Database schema|schema]] that contains one or more tables that have one or more columns in common.  Clustering tables in database improves the performance of [[Join (SQL)|join]] operation.&lt;ref&gt;[http://www.iselfschooling.com/mc4articles/mc4cluster.htm cluster table]&lt;/ref&gt;
*Discrete transactions
:All changes to the data are deferred until the transaction commits during a discrete transaction.  It can improve the performance of short, non-distributed transaction.&lt;ref&gt;[http://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/transac.htm Discrete Transactions]&lt;/ref&gt;
*[[Block (data storage)]] size
:The data block size should be a multiple of the operating system's block size within the maximum limit to avoid unnecessary I/O.&lt;ref&gt;[http://docs.oracle.com/cd/B10500_01/server.920/a96524/c03block.htm Data Block]&lt;/ref&gt;
*[[Buffer cache]] size
:To avoid unnecessary resource consumption, tune [[SQL]] statements to use the database buffer cache.&lt;ref&gt;[http://docs.oracle.com/cd/E16655_01/server.121/e15857/tune_buffer_cache.htm#TGDBA294 Database buffer Cache]&lt;/ref&gt;
*[[Dynamic allocation]] of space to tables and rollback segments
*[[Transaction processing]] monitors and the multi-threaded server
:A transaction processing monitor is used for coordination of services.  It is like an operating system and does the coordination at a high level of granularity and can span multiple computing devices.&lt;ref&gt;[http://c2.com/cgi/wiki?TransactionProcessingMonitor Transaction processing monitor]&lt;/ref&gt;
*[[Partition (database)]]
:Partition increases performance for sites that have regular transactions while still maintain availability and security.&lt;ref&gt;[[Partition (database)|Partition]]&lt;/ref&gt;
*[[Database tuning]]
:With database tuning, OLTP system can maximize its performance as efficiently and rapidly as possible.

==Contrasted to==
*[[Batch processing]]
*[[Grid computing]]

==See also==
*[[On line analytical processing]] (OLAP)
*[[Transaction processing]]
*[[Database transaction]]

==References==
&lt;references /&gt;

==External links==
{{Wiktionary|OLTP}}
*[http://hstore.cs.brown.edu H-Store Project] (architectural and application shifts affecting OLTP performance)
*[http://www.ibm.com/cics IBM CICS official website]
*[http://www.tpc.org/ Transaction Processing Performance Council]
*[http://dbms.knowledgehills.com/What-is-Online-Transaction-Processing-(OLTP)-Schema/a32p2 OLTP Schema]
*[http://www.amazon.com/dp/1558601902 Transaction Processing: Concepts &amp; Techniques Management]

{{Databases}}

{{DEFAULTSORT:On line Transaction Processing}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]</text>
      <sha1>kjl3z3a5dn4a51ynvfwzc5k1artttrj</sha1>
    </revision>
  </page>
  <page>
    <title>Astroinformatics</title>
    <ns>0</ns>
    <id>28326718</id>
    <revision>
      <id>748836457</id>
      <parentid>740645884</parentid>
      <timestamp>2016-11-10T17:50:35Z</timestamp>
      <contributor>
        <username>Pleasantville</username>
        <id>3058640</id>
      </contributor>
      <comment>added [[Category:Computational fields of study]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13725" xml:space="preserve">'''Astroinformatics''' is an interdisciplinary field of study involving the combination of [[astronomy]], [[data science]], [[informatics]], and [[Information technology|information]]/[[Communications technologies|communications]] technologies.&lt;ref name="astroinfo" /&gt;&lt;ref name=pdf&gt;[http://www.math.bas.bg/~nkirov/zip/SEEDI_astro_presentation.pdf Astroinformatics and digitization of astronomical heritage], Nikolay Kirov. The fifth SEEDI International Conference Digitization of cultural and scientific heritage, May 19–20, 2010, Sarajevo. Retrieved 1 November 2012.&lt;/ref&gt;

==Background==

Astroinformatics is primarily focused on developing the tools, methods, and applications of [[computational science]], [[data science]], and [[statistics]] for research and education in data-oriented astronomy.&lt;ref name="astroinfo"&gt;{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy Research and Education|url=http://link.springer.com/article/10.1007%2Fs12145-010-0055-2|website=Journal of Earth Science Informatics, June 2010, Volume 3, Issue 1, pp 5-17|publisher=Springer Link, Netherlands|accessdate=11 January 2016}}&lt;/ref&gt; Early efforts in this direction included [[data discovery]], [[metadata standards]] development, [[data modeling]], astronomical [[data dictionary]] development, [[data access]], [[information retrieval]],&lt;ref&gt;{{cite arXiv|last1=Borne|first1=Kirk|title=Science User Scenarios for a Virtual Observatory Design Reference Mission: Science Requirements for Data Mining|arxiv=astro-ph/0008307}}&lt;/ref&gt; [[data integration]], and [[data mining]]&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Scientific Data Mining in Astronomy|url=https://www.crcpress.com/Next-Generation-of-Data-Mining/Kargupta-Han-Yu-Motwani-Kumar/9781420085860|website=CRC Press, pp. 91-114|publisher=Taylor &amp; Francis Group|accessdate=11 January 2016}}&lt;/ref&gt; in the astronomical [[Virtual Observatory]] initiatives.&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Distributed Data Mining in the National Virtual Observatory|url=http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=764620|website=SPIE Digital Library|publisher=SPIE|accessdate=11 January 2016}}&lt;/ref&gt;&lt;ref name="VOdm" /&gt;&lt;ref&gt;{{cite web|last1=Laurino|first1=O.|title=Astroinformatics of galaxies and quasars: a new general method for photometric redshifts estimation|url=http://mnras.oxfordjournals.org/content/418/4/2165|website=Monthly Notices of the Royal Astronomical Society, vol.418, pp. 2165-2195|publisher=Oxford Journals|accessdate=12 January 2016|display-authors=etal}}&lt;/ref&gt; Further development of the field, along with astronomy community endorsement, was presented to the [[National Research Council (United States)]] in 2009 in the Astroinformatics "State of the Profession" Position Paper for the 2010 [[Astronomy and Astrophysics Decadal Survey]].&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: A 21st Century Approach to Astronomy|url=http://adsabs.harvard.edu/abs/2009astro2010P...6B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}&lt;/ref&gt; That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper '''Astroinformatics: Data-Oriented Astronomy Research and Education'''.&lt;ref name="astroinfo" /&gt;

Astroinformatics as a distinct field of research was inspired by work in the fields of [[Bioinformatics]] and [[Geoinformatics]], and through the [[eScience]] work&lt;ref&gt;{{cite web|title='Online Science'|url=http://research.microsoft.com/en-us/um/people/gray/JimGrayTalks.htm|website=Talks by Jim Gray|publisher=Microsoft Research|accessdate=11 January 2015}}&lt;/ref&gt; of [[Jim Gray (computer scientist)]] at [[Microsoft Research]], whose legacy was remembered and continued through the Jim Gray eScience Awards.&lt;ref&gt;{{cite web|title=Jim Gray eScience Award|url=http://research.microsoft.com/en-us/collaboration/focus/escience/jim-gray-award.aspx|website=Microsoft Research}}&lt;/ref&gt;

Though the primary focus of Astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to [[Digital data|digitize]] historical and recent astronomical observations and images in a large [[database]] for efficient retrieval through [[World wide web|web]]-based interfaces.&lt;ref name=pdf/&gt;&lt;ref&gt;[http://www.casca.ca/lrp2010/Docs/LRPReports/astroinformatics_lrp.pdf Astroinformatics in Canada], Nicholas M. Ball, David Schade. Retrieved 1 November 2012.&lt;/ref&gt; Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.&lt;ref&gt;{{cite web|title='Astroinformatics' helps Astronomers explore the sky|url=http://phys.org/news/2013-10-astroinformatics-astronomers-exploring-sky.html|website=Phys.org|publisher=Heidelberg University|accessdate=11 January 2015}}&lt;/ref&gt;

Astroinformatics is described as the '''Fourth Paradigm''' of astronomical research.&lt;ref&gt;{{cite web|title=The Fourth Paradigm: Data-Intensive Scientific Discovery|url=https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/|website=Microsoft Research}}&lt;/ref&gt; There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science.&lt;ref name="VOdm"&gt;{{cite web|last1=Borne|first1=Kirk|title=Virtual Observations, Data Mining, and Astroinformatics|url=http://link.springer.com/referenceworkentry/10.1007/978-94-007-5618-2_9|website=Planets, Stars and Stellar Systems, Volume 2: Astronomical Techniques, Software, and Data, pp.403-443|publisher=Springer Link, Netherlands|accessdate=11 January 2015}}&lt;/ref&gt; [[Data mining]] and [[machine learning]] play significant roles in Astroinformatics as a [[Scientific method|scientific research]] discipline due to their focus on "knowledge discovery from data" ([[data mining|KDD]]) and "learning from data".&lt;ref&gt;{{cite web|last1=Ball|first1=N.M.|last2=Brunner|first2=R.J.|title=Data Mining and Machine Learrning in Astronomy|url=http://www.worldscientific.com/doi/abs/10.1142/S0218271810017160|website=International Journal of Modern Physics D|publisher=World Scientific Publishing|accessdate=12 January 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=The LSST Data Mining Research Agenda|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059074|website=Classification and Discovery in Large Astronomical Surveys, pp.347-351|publisher=American Institute of Physics|accessdate=12 January 2016}}&lt;/ref&gt;

The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the [[Large Synoptic Survey Telescope]] and into the exabytes with the [[Square Kilometre Array]].&lt;ref&gt;{{cite web|last1=Ivezić|first1=Ž.|title=Parametrization and Classification of 20 Billion LSST Objects|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059076|website=Classification and Discovery in Large Astronomical Surveys, pp.359-365|publisher=American Institute of Physics|accessdate=12 January 2016|display-authors=etal}}&lt;/ref&gt; This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part, due to this data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing sub-disciplines information and data intensive to an extent that these sub-disciplines are now becoming (or have already become) stand alone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, the most likely will in the near future.

[[Informatics]] has been recently defined as "the use of digital data, information, and related services for research and knowledge generation". However the usual, or commonly used definition is "informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support." Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., [[taxonomy (general)|taxonomies]], [[ontology (information science)|ontologies]], [[folksonomy|folksonomies]], and/or collaborative [[Tag (metadata)|tagging]]&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of the ASIS&amp;T|publisher=American Society for Information Science and Technology|accessdate=11 January 2016}}&lt;/ref&gt;) plus '''[[Astrostatistics]]''' will also be heavily involved. '''[[Citizen science]]''' projects (such as [[Galaxy Zoo]]) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.

In 2012, two position papers&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics in a Nutshell|url=https://asaip.psu.edu/Articles/astroinformatics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Feigelson|first1=Eric|title=Astrostatistics in a Nutshell|url=https://asaip.psu.edu/Articles/astrostatistics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}&lt;/ref&gt; were presented to the Council of the [[American Astronomical Society]] that led to the establishment of formal working groups in Astroinformatics and [[Astrostatistics]] for the profession of [[astronomy]] within the USA and elsewhere.&lt;ref&gt;{{cite arXiv|last1=Feigelson|first1=E.|last2=Ivezić|first2=Ž.|last3=Hilbe|first3=J.|last4=Borne|first4=K.|title=New Organizations to Support Astroinformatics and Astrostatistics|arxiv=1301.3069}}&lt;/ref&gt;

Astroinformatics provides a natural context for the integration of education and research.&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=The Revolution in Astronomy Education: Data Science for the Masses|url=http://adsabs.harvard.edu/abs/2009astro2010P...7B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}&lt;/ref&gt; The experience of research can now be implemented within the classroom to establish and grow '''[[Data Literacy]]''' through the easy re-use of data.&lt;ref&gt;{{cite web|title=Using Data in the Classroom|url=http://serc.carleton.edu/usingdata/index.html|website=Science Education Resource Center at Carleton College|publisher=National Science Digital Library|accessdate=11 January 2016}}&lt;/ref&gt; It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.&lt;ref&gt;{{cite book|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy|location=George Mason University, USA|url=http://www.iccs-meeting.org/iccs2009/PosterPapers/Poster-paper18.pdf|accessdate=January 21, 2015}}&lt;/ref&gt;

==Conferences==

{| class="wikitable"
|-
! Year
! Place
! Link
|-
| 2016
| [[Sorrento]], [[Italy]]
| [http://www.iau.org/science/meetings/future/symposia/1158/]
|-
| 2015
| [[Dubrovnik]], [[Dalmatia]]
| [http://iszd.hr/AstroInfo2015/]
|-
| 2014
| [[University of Chile]]
| [http://eventos.cmm.uchile.cl/astro2014/]
|-
| 2013
| [[Australia Telescope National Facility]], [[CSIRO]]
| [http://www.atnf.csiro.au/research/workshops/2013/astroinformatics/]
|-
| 2012
| [[Microsoft Research]]
| [http://www.astro.caltech.edu/ai12/]
|-
| 2011
| [[Sorrento]], [[Italy]]
| [http://dame.dsf.unina.it/astroinformatics2011.html]
|-
| 2010
| [[Caltech]]
| [http://www.astro.caltech.edu/ai10/]
|}

==See also==

*''[[Astronomy and Computing]]''
*[[Astrophysics Data System]]
*[[Astrophysics Source Code Library]]
*[[Astrostatistics]]
*[[Galaxy Zoo]]
*[[International Astrostatistics Association]]
*[[International Virtual Observatory Alliance]] (IVOA)
*[[MilkyWay@home]]
*[[Virtual Observatory]]
*[[WorldWide Telescope]]
*[[Zooniverse (citizen science project)|Zooniverse]]

== External links ==

* [http://www.adass.org/ Astronomical Data Analysis Software and Systems] (ADASS)
* [https://asaip.psu.edu/ Astrostatistics and Astroinformatics Portal]
* [https://asaip.psu.edu/organizations/iaa/iaa-working-group-of-cosmostatistics/ Cosmostatistics Initiative] (COIN)
* [http://www.iau.org/science/scientific_bodies/commissions/B3/ Astroinformatics and Astrostatistics Commission of the International Astronomical Union]

==References==
{{reflist}}

[[Category:Astronomy]]
[[Category:Astrophysics]]
[[Category:Big data]]
[[Category:Computational astronomy]]
[[Category:Data management]]
[[Category:Information science by discipline]]
[[Category:Applied statistics]]
[[Category:Computational fields of study]]</text>
      <sha1>ado2tm0fispvol59sdb5pztsgsavfql</sha1>
    </revision>
  </page>
  <page>
    <title>Document-oriented database</title>
    <ns>0</ns>
    <id>15002414</id>
    <revision>
      <id>762848584</id>
      <parentid>758861200</parentid>
      <timestamp>2017-01-31T01:50:37Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23726" xml:space="preserve">{{about|the software type|deployed applications of the software type|Full text database}}
{{primary sources|date=May 2012}}

A '''document-oriented database''', or '''document store''', is a [[computer program]] designed for storing, retrieving and managing document-oriented information, also known as [[Semi-structured model|semi-structured data]]. Document-oriented databases are one of the main categories of [[NoSQL]] databases, and the popularity of the term "document-oriented database" has grown&lt;ref&gt;[http://db-engines.com/en/ranking_categories DB-Engines Ranking per database model category]&lt;/ref&gt; with the use of the term NoSQL itself. [[XML database]]s are a subclass of document-oriented databases that are optimized to work with [[XML]] documents. [[Graph databases]] are similar, but add another layer, the ''relationship'', which allows them to link documents for rapid traversal.

Document-oriented databases are inherently a subclass of the [[Key-value database|key-value store]], another NoSQL database concept. The difference lies in the way the data is processed; in a key-value store the data is considered to be inherently opaque to the database, whereas a document-oriented system relies on internal structure in the ''document'' in order to extract [[metadata]] that the database engine uses for further optimization. Although the difference is often moot due to tools in the systems,{{efn|To the point that document-oriented and key-value systems can often be interchanged in operation.}} conceptually the document-store is designed to offer a richer experience with modern programming techniques.

Document databases{{efn|And key-value stores in general.}} contrast strongly with the traditional [[relational database]] (RDB). Relational databases generally store data in separate ''tables'' that are defined by the programmer, and a single object may be spread across several tables. Document databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This makes mapping objects into the database a simple task, normally eliminating anything similar to an [[object-relational mapping]]. This makes document stores attractive for programming [[web application]]s, which are subject to continual change in place, and where speed of deployment is an important issue.

== Documents ==
The central concept of a document-oriented database is the notion of a ''document''. While each document-oriented database implementation differs on the details of this definition, in general, they all assume documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include [[XML]], [[YAML]], [[JSON]], and [[BSON]], as well as binary forms like PDF and Microsoft Office documents (MS Word, Excel, and so on).

Documents in a document store are roughly equivalent to the programming concept of an object. They are not required to adhere to a standard schema, nor will they have all the same sections, slots, parts, or keys. Generally, programs using objects have many different types of objects, and those objects often have many optional fields. Every object, even those of the same class, can look very different. Document stores are similar in that they allow different types of documents in a single store, allow the fields within them to be optional, and often allow them to be encoded using different encoding systems. For example, the following is a document, encoded in JSON:

&lt;syntaxhighlight lang="javascript"&gt;
{
    FirstName: "Bob", 
    Address: "5 Oak St.", 
    Hobby: "sailing"
}
&lt;/syntaxhighlight&gt;

A second document might be encoded in XML as:
&lt;syntaxhighlight lang="xml"&gt;
  &lt;contact&gt;
    &lt;firstname&gt;Bob&lt;/firstname&gt;
    &lt;lastname&gt;Smith&lt;/lastname&gt;
    &lt;phone type="Cell"&gt;(123) 555-0178&lt;/phone&gt;
    &lt;phone type="Work"&gt;(890) 555-0133&lt;/phone&gt;
    &lt;address&gt;
      &lt;type&gt;Home&lt;/type&gt;
      &lt;street1&gt;123 Back St.&lt;/street1&gt;
      &lt;city&gt;Boys&lt;/city&gt;
      &lt;state&gt;AR&lt;/state&gt;
      &lt;zip&gt;32225&lt;/zip&gt;
      &lt;country&gt;US&lt;/country&gt;
    &lt;/address&gt;
  &lt;/contact&gt;
&lt;/syntaxhighlight&gt;

These two documents share some structural elements with one another, but each also has unique elements. The structure and text and other data inside the document are usually referred to as the document's ''content'' and may be referenced via retrieval or editing methods, (see below). Unlike a relational database where every record contains the same fields, leaving unused fields empty; there are no empty 'fields' in either document (record) in the above example. This approach allows new information to be added to some records without requiring that every other record in the database share the same structure.

Document databases typically provide for additional [[metadata]] to be associated with and stored along with the document content. That metadata may be related to facilities the datastore provides for organizing documents, providing security, or other implementation specific features.

=== CRUD operations ===
The core operations a document-oriented database supports on documents are similar to other databases and while the terminology isn't perfectly standardized, most practitioners will recognize them as [[CRUD]]

* Creation (or insertion)
* Retrieval (or query, search, finds)
* Update (or edit)
* Deletion (or removal)

=== Keys ===
Documents are addressed in the database via a unique ''key'' that represents that document. This key is a simple [[identifier]] (or ID), typically a [[String (computer science)|string]], a [[URI]], or a [[Path (computing)|path]]. The key can be used to retrieve the document from the database. Typically the database retains an [[Database index|index]] on the key to speed up document retrieval, and in some cases the key is required to create or insert the document into the database.

=== Retrieval ===
Another defining characteristic of a document-oriented database is that, beyond the simple key-to-document lookup that can be used to retrieve a document, the database offers an API or query language that allows the user to retrieve documents based on content (or metadata). For example, you may want a query that retrieves all the documents with a certain field set to a certain value.  The set of query APIs or query language features available, as well as the expected performance of the queries, varies significantly from one implementation to another. Likewise, the specific set of indexing options and configuration that are available vary greatly by implementation.

It is here that the document store varies most from the key-value store. In theory, the values in a key-value store are opaque to the store, they are essentially black boxes. They may offer search systems similar to those of a document store, but may have less understanding about the organization of the content. Document stores use the metadata in the document to classify the content, allowing them, for instance, to understand that one series of digits is a phone number, and another is a postal code. This allows them to search on those types of data, for instance, all phone numbers containing 555, which would ignore the zip code 55555.

=== Editing ===
Document databases typically provide some mechanism for updating or editing the content (or other metadata) of a document, either by allowing for replacement of the entire document, or individual structural pieces of the document.

=== Organization ===
Document database implementations offer a variety of ways of organizing documents, including notions of

* Collections: groups of documents, where depending on implementation, a document may be enforced to live inside one collection, or may be allowed to live in multiple collections
* Tags and non-visible metadata: additional data outside the document content 
* Directory hierarchies: groups of documents organized in a tree-like structure, typically based on path or URI

Sometimes these organizational notions vary in how much they are logical vs physical, (e.g. on disk or in memory), representations.

==Relationship to other databases ==

=== Relationship to key-value stores ===

A document-oriented database is a specialized  [[Key-value database|key-value store]], which itself is another NoSQL database category.  In a simple key-value store, the document content is opaque. A document-oriented database provides APIs or a query/update language that exposes the ability to query or update based on the internal structure in the ''document''. This difference may be moot for users that do not need richer query, retrieval, or editing APIs that are typically provided by document databases. Modern key-value stores often include features for working with metadata, blurring the lines between document stores.

=== Relationship to search engines ===
Some search engines (aka [[information retrieval]]) systems like [[Elasticsearch]] provide enough of the core operations on documents to fit the definition of a document-oriented database.

=== Relationship to relational databases ===

{{cleanup|section|reason="Requires cleanup"|date=July 2016}}

In a relational database, data is first categorized into a number of predefined types, and ''tables'' are created to hold individual entries, or ''records'', of each type. The tables define the data within each record's ''fields'', meaning that every record in the table has the same overall form. The administrator also defines the ''relationships'' between the tables, and selects certain fields that they believe will be most commonly used for searching and defines ''indexes'' on them. A key concept in the relational design is that any data that may be repeated is normally placed in its own table, and if these instances are related to each other, a column is selected to group them together, the ''foreign key''. This design is known as ''[[database normalization]]''.&lt;ref&gt;{{cite web |url=https://support.microsoft.com/en-ca/kb/283878 |title=Description of the database normalization basics |website=Microsoft}}&lt;/ref&gt;

For example, an address book application will generally need to store the contact name, an optional image, one or more phone numbers, one or more mailing addresses, and one or more email addresses. In a canonical relational database solution, tables would be created for each of these rows with predefined fields for each bit of data: the CONTACT table might include FIRST_NAME, LAST_NAME and IMAGE columns, while the PHONE_NUMBER table might include COUNTRY_CODE, AREA_CODE, PHONE_NUMBER and TYPE (home, work, etc.). The PHONE_NUMBER table also contains a foreign key column, "CONTACT_ID", which holds the unique ID number assigned to the contact when it was created. In order to recreate the original contact, the database engine uses the foreign keys to look for the related items across the group of tables and reconstruct the original data.

In contrast, in a document-oriented database there may be no internal structure that maps directly onto the concept of a table, and the fields and relationships generally don't exist as predefined concepts. Instead, all of the data for an object is placed in a single document, and stored in the database as a single entry. In the address book example, the document would contain the contact's name, image, and any contact info, all in a single record. That entry is accessed through its key, which allows the database to retrieve and return the document to the application. No additional work is needed to retrieve the related data; all of this is returned in a single object.

A key difference between the document-oriented and relational models is that the data formats are not predefined in the document case. In most cases, any sort of document can be stored in any database, and those documents can change in type and form at any time. If one wishes to add a COUNTRY_FLAG to a CONTACT, this field can be added to new documents as they are inserted, this will have no effect on the database or the existing documents already stored. To aid retrieval of information from the database, document-oriented systems generally allow the administrator to provide ''hints'' to the database to look for certain types of information. These work in a similar fashion to indexes in the relational case. Most also offer the ability to add additional metadata outside of the content of the document itself, for instance, tagging entries as being part of an address book, which allows the programmer to retrieve related types of information, like "all the address book entries". This provides functionality similar to a table, but separates the concept (categories of data) from its physical implementation (tables).

In the classic normalized relational model, objects in the database are represented as separate rows of data with no inherent structure beyond that given to them as they are retrieved. This leads to problems when trying to translate programming objects to and from their associated database rows, a problem known as [[object-relational impedance mismatch]].&lt;ref&gt;{{cite web |url=http://www.agiledata.org/essays/impedanceMismatch.html |title=The Object-Relational Impedance Mismatch |first=Scott |last=Wambler |website=Agile Data}}&lt;/ref&gt; Document stores more closely, or in some cases directly, map programming objects into the store. This eliminates the impedance mismatch problem, and is offered as one of the main advantages of the NoSQL approach.

== Implementations ==
{{main cat|Document-oriented databases}}

{| class="wikitable sortable"
|-
! Name
! Publisher
! License
! Languages supported
! Notes
! [[Representational State Transfer|RESTful]] API
|-
| [[BaseX]]
| BaseX Team
| {{free|[[BSD License]]}}
| [[Java (programming language)|Java]], [[XQuery]]
| Support for XML, JSON and binary formats; client-/server based architecture; concurrent structural and full-text searches and updates.
| {{yes}}
|-
| [[InterSystems Caché|Caché]]
| [[InterSystems]] Corporation
| {{proprietary}}
| [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[Node.js]]
| Commonly used in Health, Business and Government applications.
| {{yes}}
|-
| [[Cloudant]]
| Cloudant, Inc.
| {{proprietary}}
| [[Erlang (programming language)|Erlang]], [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], and [[C (programming language)|C]]
| Distributed database service based on [[BigCouch]], the company's [[open source]] fork of the [[Apache Software Foundation|Apache]]-backed [[CouchDB]] project.  Uses JSON model.
| {{yes}}
|-
| [[Clusterpoint|Clusterpoint Database]]
| Clusterpoint Ltd.
| {{proprietary}} with free download
| [[JavaScript]], [[SQL]], [[PHP]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[C (programming language)|C]], [[C++]], 
| Distributed document-oriented XML / JSON database platform with [[ACID]]-compliant [[transaction processing|transactions]]; [[high-availability]] [[data replication]] and [[sharding]]; built-in [[full text search]] engine with [[relevance]] [[ranking]]; JS/SQL [[query language]]; [[Geographic information system|GIS]]; Available as pay-per-use [[cloud database|database as a service]] or as an on-premise free software download.&lt;ref&gt;[http://www.clusterpoint.com Document-oriented Database]. Clusterpoint. Retrieved on 2015-10-08.&lt;/ref&gt;
| {{yes}}
|-
| [[Couchbase Server]]
| [[Couchbase, Inc.]]
| {{free|[[Apache License]]}}
| [[C]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[PHP]], [[SQL]], [[GoLang]], [[Spring Framework]], [[LINQ]]
|Distributed NoSQL Document Database, JSON model and SQL based Query Language.
| {{yes}}&lt;ref&gt;[http://www.couchbase.com/docs/ Documentation]. Couchbase. Retrieved on 2013-09-18.&lt;/ref&gt;
|-
| [[CouchDB]]
| [[Apache Software Foundation]]
| {{free|[[Apache License]]}}
| Any language that can make HTTP requests
| JSON over REST/HTTP with [[Multi-Version Concurrency Control]] and limited [[ACID]] properties. Uses [[map (higher-order function)|map]] and [[fold (higher-order function)|reduce]] for views and queries.&lt;ref&gt;[http://couchdb.apache.org/docs/overview.html CouchDB Overview] {{webarchive |url=https://web.archive.org/web/20111020074113/http://couchdb.apache.org/docs/overview.html |date=October 20, 2011 }}&lt;/ref&gt;
| {{yes}}&lt;ref&gt;[http://wiki.apache.org/couchdb/HTTP_Document_API CouchDB Document API]&lt;/ref&gt;
|-
| [[CrateIO]]
| CRATE Technology GmbH
| {{free|[[Apache License]]}}
| [[Java (programming language)|Java]]
| Use familiar SQL syntax for real time distributed queries across a cluster. Based on Lucene / Elasticsearch ecosystem with built-in support for binary objects (BLOBs).
| {{yes}}&lt;ref&gt;{{cite web|url=https://crate.io/docs/stable/sql/rest.html |title=Archived copy |accessdate=2015-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20150622174526/https://crate.io/docs/stable/sql/rest.html |archivedate=2015-06-22 |df= }}&lt;/ref&gt;
|-
|djondb
|djondb.com
|GNU GPL and Commercial
|C, .Net, Java, Python, NodeJS, PHP.
|Document Store with support to transactions.
|{{No}}
|-
| [[DocumentDB]]
| Microsoft
| {{proprietary}}
| [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[JavaScript]], [[SQL]]
| Platform-as-a-Service offering, part of the [[Microsoft Azure]] platform.
| {{yes}}
|-
|[[Elasticsearch]]
|[[Shay Banon]]
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|[[JSON]], Search engine.
|{{yes}}
|-
| [[eXist]]
| eXist
| {{free|[[LGPL]]}}
| [[XQuery]], [[Java (programming language)|Java]]
| XML over REST/HTTP, WebDAV, Lucene Fulltext search, binary data support, validation, versioning, clustering, triggers, URL rewriting, collections, ACLS, XQuery Update
| {{yes}}&lt;ref&gt;[http://exist-db.org eXist-db Open Source Native XML Database]. Exist-db.org. Retrieved on 2013-09-18.&lt;/ref&gt;
|-
| [[HyperDex]]
| hyperdex.org
| {{free|[[BSD License]]}}
| [[C (programming language)|C]], [[C++]], [[Go (programming language)|Go]], [[Node.js]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]] 
| Support for [[JSON]] and binary documents.
| {{no}}
|-
| [[Informix]]
| IBM
| Proprietary, with no-cost editions&lt;ref&gt;http://www.ibm.com/developerworks/data/library/techarticle/dm-0801doe/&lt;/ref&gt;
| Various (Compatible with MongoDB API)
| RDBMS with JSON, replication, sharding and ACID compliance.
| {{yes}}
|-
|[[Apache Jackrabbit|Jackrabbit]]
|Apache Foundation
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|[[Java Content Repository]] implementation
|{{dunno}}
|-
| [[Lotus Notes]] ([[IBM Lotus Domino]])
| IBM
| {{proprietary}}
| [[LotusScript]], [[Java (programming language)|Java]], Lotus @Formula
| [[MultiValue]]
| {{yes}}
|-
| [[MarkLogic]]
| MarkLogic Corporation
| Free Developer license or Commercial&lt;ref&gt;http://developer.marklogic.com/licensing&lt;/ref&gt;
| [[REST]], [[Java (programming language)|Java]], [[JavaScript]], [[Node.js]], [[XQuery]], [[SPARQL]], [[XSLT]], [[C++]]
| Distributed document-oriented database for JSON, XML, and [[Resource Description Framework|RDF triples]]. Built-in [[Full text search]], [[ACID]] transactions, [[High availability]] and [[Disaster recovery]], certified security.
| {{yes}}
|-
| [[MongoDB]]
| MongoDB, Inc
| {{free|[[Affero General Public License|GNU AGPL v3.0]] for the DBMS, [[Apache 2 License]] for the client drivers}}&lt;ref&gt;[http://www.mongodb.org/about/licensing/ MongoDB Licensing]&lt;/ref&gt;
| [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[Perl]], [[PHP]], [[Python (programming language)|Python]], [[Node.js]], [[Ruby (programming language)|Ruby]], [[Scala (programming language)|Scala]] &lt;ref&gt;[http://docs.mongodb.org/ecosystem/drivers/community-supported-drivers/ Additional 30+ community MongoDB supported drivers]&lt;/ref&gt;
| Document database with replication and sharding, [[BSON]] store (binary format [[JSON]]).
| {{yes}}&lt;ref&gt;[http://www.mongodb.org/display/DOCS/Http+Interface#HttpInterface-RESTInterfaces MongoDB REST Interfaces]&lt;/ref&gt;
|-
| [[MUMPS]] Database
| {{dunno}}
| [[proprietary software|Proprietary]] and [[Affero General Public License|Affero GPL]]&lt;ref&gt;[http://sourceforge.net/projects/fis-gtm/ GTM MUMPS FOSS on SourceForge]&lt;/ref&gt;
| [[MUMPS]]
| Commonly used in health applications.
| {{dunno}}
|-
| [[ObjectDatabase++]]
| Ekky Software
| {{proprietary}}
| [[C++]], [[C Sharp (programming language)|C#]], [[TScript]]
| Binary Native C++ class structures
| {{dunno}}
|-
| [[OrientDB]]
| Orient Technologies
| {{free|[[Apache License]]}}
| [[Java (programming language)|Java]]
| JSON over HTTP, SQL support, [[ACID]] transactions
| {{yes}}
|-
| [[PostgreSQL]]
| PostgreSQL
| {{free | [http://www.postgresql.org/about/licence/ PostgreSQL Free License]}}
| [[C (programming language)|C]]
| HStore, JSON store (9.2+), JSON function (9.3+), HStore2 (9.4+), JSONB (9.4+)
|{{no}}
|-
| [[Qizx]]
| [[Qualcomm]]
| [[Commercial software|Commercial]]
| [[REST]], [[Java (programming language)|Java]], [[XQuery]], [[XSLT]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]]
| Distributed document-oriented [[XML database]] with integrated [[full text search]]; support for [[JSON]], text, and binaries.
|{{yes}}
|-
| [[RethinkDB]]
| {{dunno}}
| {{free|[[Affero General Public License|GNU AGPL]] for the DBMS, [[Apache 2 License]] for the client drivers}}
| [[C++]], [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Java (programming language)|Java]]
| Distributed document-oriented [[JSON]] database with replication and sharding.
|{{no}}
|-
| [[Rocket U2]]
| Rocket Software
| {{proprietary}}
| {{dunno}}
| UniData, UniVerse
|{{yes}} (Beta)
|-
| [[Sedna (database)|Sedna]]
|sedna.org
|{{free|[[Apache License]]}}
|[[C++]], [[XQuery]]
|[[XML database]]
|{{no}}
|-
| [[SimpleDB]]
| Amazon
| Proprietary online service
|[[Erlang (programming language)|Erlang]]
|
|{{dunno}}
|-
| [[Solr]]
| Apache
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|Search engine
|{{yes}}
|-
| [[TokuMX]]
|Tokutek
|{{free|[[GNU Affero General Public License]]}}
|[[C++]], [[C Sharp (programming language)|C#]], [[Go (Programming language)|Go]]
|[[MongoDB]] with [[Fractal tree index|Fractal Tree indexing]]
|{{dunno}}
|-
| [[Virtuoso Universal Server|OpenLink Virtuoso]]
| [[OpenLink Software]]
|GPLv2[1] and proprietary
|[[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]]
|[[Middleware]] and [[database engine]] hybrid
|{{yes}}
|}

===XML database implementations===
{{Further information|XML database}}
Most XML databases are document-oriented databases.

== See also ==
* [[Database theory]]
* [[Data hierarchy]]
* [[Full text search]]
* [[In-memory database]]
* [[Internet Message Access Protocol]] (IMAP)
* [[Machine-Readable Documents]]
* [[NoSQL]]
* [[Object database]]
* [[Online database]]
* [[Real time database]]
* [[Relational database]]

==Notes==
{{notelist}}

==References==
{{Reflist}}

==Further reading==
* Assaf Arkin. (2007, September 20). [https://web.archive.org/web/20080327222152/http://blog.labnotes.org:80/2007/09/20/read-consistency-dumb-databases-smart-services/ Read Consistency: Dumb Databases, Smart Services.]
{{refend}}

==External links==
* [http://db-engines.com/en/ranking/document+store DB-Engines Ranking of Document Stores] by popularity, updated monthly

{{Database models}}
{{Databases}}

[[Category:Document-oriented databases| ]]
[[Category:Data management]]
[[Category:Database management systems]]
[[Category:Types of databases]]</text>
      <sha1>5cvv6wmu0da2d7pial41r5x9m3nu43a</sha1>
    </revision>
  </page>
  <page>
    <title>Sedona Canada Principles</title>
    <ns>0</ns>
    <id>48536375</id>
    <revision>
      <id>734979237</id>
      <parentid>702608210</parentid>
      <timestamp>2016-08-17T22:37:10Z</timestamp>
      <contributor>
        <username>Gvcormac</username>
        <id>3113450</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8926" xml:space="preserve">{{orphan|date=January 2016}}
&lt;!-- Don't mess with this line! --&gt;&lt;!-- Write your article below this line --&gt;
The '''Sedona Canada Principles''' are a set of authoritative guidelines published by The Sedona Conference ® to aid members of the Canadian legal community involved in the identification, collection, preservation, review and production of [[electronically stored information]] (ESI).  The principles were drafted by a small group of lawyers, judges and technologists called the Sedona Working Group 7 or ''Sedona Canada''.  Sedona Canada is an offshoot of The Sedona Conference ® which is an American “non-profit…research and educational institute dedicated to the advanced study of law and policy in the areas of antitrust law, complex litigation, and intellectual property rights.”&lt;ref&gt;{{cite web|url=https://thesedonaconference.org/|title=The Sedona Conference® - "Moving the law forward in a reasoned and just way."|work=thesedonaconference.org}}&lt;/ref&gt;

==Background==
[[Civil procedure in Canada]] is jurisdictional with each province following its own rules of civil procedure.&lt;ref&gt;{{cite web|url=https://en.wikibooks.org/wiki/Canadian_Civil_Procedure/Rules_by_Province|title=Canadian Civil Procedure/Rules by Province|work=wikibooks.org}}&lt;/ref&gt; However, each province must address the fact that due to the advancement of technology the discovery process enshrined in the rules of civil procedure can be potentially derailed due to the sheer volume of [[electronically stored information]] (ESI). &lt;ref name="mccarthy.ca"&gt;{{cite web|url=http://www.mccarthy.ca/article_detail.aspx?id=4068|title=McCarthy Tétrault - Taming the Beast of Electronic Discovery with Sedona Canada Principles - Article Detail|work=mccarthy.ca}}&lt;/ref&gt; When dealing with litigation matters that involve [[electronically stored information]] (ESI), the discovery process is commonly called [[electronic discovery|e-discovery]].  The problems associated with [[electronic discovery|e-discovery]] in Canada led to the creation of the Sedona Canada Principles. &lt;ref name="mccarthy.ca"/&gt; Rule 29.1.03(4) of the [[wikibooks:Ontario Rules of Civil Procedure]] specifically refers to the Sedona Canada Principles in referencing Principles re Electronic Discovery although it has been reported that this rule has been largely ignored in practice.&lt;ref name="canlii.org"&gt;{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2010/2010onsc3670/2010onsc3670.html|title=CanLII - 2010 ONSC 3670 (CanLII)|work=canlii.org}}&lt;/ref&gt;

==Summary==
The Sedona Canada Principles largely refer to the processes found in the Electronic Discovery Reference Model.&lt;ref&gt;{{cite web|url=http://www.edrm.net/resources/edrm-stages-explained|title=EDRM Stages|work=edrm.net}}&lt;/ref&gt;

The principles urge proportionality due to the potentially enormous volumes of documents that may be discoverable when dealing with ESI.  They also encourage [[good faith]] in the document preservation stage and regular meetings between parties to discuss the scope of the litigation.  Parties are urged to be aware of the potential costs involved in producing relevant ESI but are advised that only reasonably accessible ESI need be produced.  The principles stipulate that parties should not be required to search for or collect deleted material unless there is an agreement or court order related to those terms.  The use of electronic tools and processes such as data sampling and web harvesting are acceptable practices.  Parties are encouraged to agree early in the litigation process on production format required for the exchange of relevant documents as part of the discovery process (native files, [[pdf]], [[Tagged Image File Format|tiff]], [[metadata]] requirements etc).  Agreements or direction should be sought, if necessary, with respect to [[wikt:privilege|privilege]] or other confidential information related to production of electronic documents and data.  Parties should be aware that legal precedents can be formed as a result of [[e-discovery]] practices and sanctions can be considered for a party’s failure to meet their discovery obligations unless it can be demonstrated that the failure was not intentional.  All parties must bear the “reasonable” costs associated with [[e-discovery]] but other arrangements can be agreed upon by the parties or by court order.&lt;ref&gt;{{cite web|url=https://www.canlii.org/en/commentary/sedonacanada/principles_en.html|title=CanLII - The Sedona Canada Principles Addressing Electronic Discovery (Jan. 2008)|work=canlii.org}}&lt;/ref&gt;

==Caselaw==

In ''Warman v. National Post Company'' proportionality was at issue in a case where the plaintiff was suing the defendant for libel.  A motion was brought by the defendant to have the plaintiff provide a mirror image of his hard drive in an effort to prove an internet article was indeed authored by the plaintiff.  Issues of proportionality and the work of the Sedona Conference and Sedona Canada Principles were factored in to the decision to grant the defendant only limited access to the hard drive.&lt;ref name="canlii.org"/&gt;

In ''Innovative Health Group Inc. v. Calgary Health Region'' the plaintiff’s legal obligation to produce imaged hard drives is in question.  Justice Conrad refers to the advice of Sedona Canada on proportionality and problems associated with time and expense related to the difficulties associated with electronically stored information.&lt;ref&gt;{{cite web|url=http://www.canlii.org/en/ab/abca/doc/2008/2008abca219/2008abca219.html|title=CanLII - 2008 ABCA 219 (CanLII)|work=canlii.org}}&lt;/ref&gt;

In ''York University v. Michael Markicevic'' Justice Brown specifically refers to the need for the parties to agree upon a formal e-discovery plan to be drafted in consultation with Sedona Canada Principles.&lt;ref name="canlii.org1"&gt;{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2013/2013onsc378/2013onsc378.html|title=CanLII - 2013 ONSC 378 (CanLII)|work=canlii.org}}&lt;/ref&gt;

In ''Friends of Lansdowne v. Ottawa'' Master MacLeod refers to the need for Sedona Canada principles and states “This is particularly true in the current information age when e-mail is ubiquitous and multiple copies or variants of messages may be held on various kinds of data storage devices including individual hard drives, e-mail and Blackberry servers.  Even documents that ultimately exist in paper form normally begin their life on computers and negotiations frequently involve exchanges of electronic drafts.  To find every scrap of paper and every electronic trace of relevant information has become a nightmarish task that threatens to render any kind of litigation extravagantly expensive.”&lt;ref name="canlii.org1"/&gt;

==Criticism==

Critics of the Sedona Canada Principles believe they should address [[system integrity]] and that the true history of any file preserved cannot be identified without proof of the integrity of the electronic record systems management it comes from.&lt;ref&gt;{{cite web|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2530515|title=The Sedona Canada Principles are Very Inadequate on Records Management and for Electronic Discovery|work=ssrn.com}}&lt;/ref&gt;

Other criticism is more directed to the Sedona Canada working group and complaints that it is insular and irrelevant&lt;ref&gt;{{cite web|url=http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html|title=Sedona Canada is alive and well|author=Colin Campbell and James Swanson|work=canadianlawyermag.com}}&lt;/ref&gt;

==External links==

[https://www.canlii.org/en/commentary/sedonacanada/principles_en.html/The Sedona Canada Principles]

[http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html/ Sedona Canada is alive and well]

[https://www.highbeam.com/doc/1G1-181488000.html/ Taming the beast of eDiscovery with Sedona Canada Principles]

[https://www.highbeam.com/doc/1G1-400332555.html/ 2014 eDiscovery Year in Review includes Sedona Canada Principles]

[http://www.canadianlawyermag.com/legalfeeds/469/ontario-judge-slams-dark-ages-court-system.html/Ontario Courts discuss Sedona Canada Principles]

[http://www.canadianlawyermag.com/3988/What-is-predictive-coding-and-can-it-help-me.html/ Sedona Canada Principles and predictive coding]

[http://www.canadianlawyermag.com/5019/Alternative-routes.html/ Document review using Sedona Canada Principles]


==References==

{{reflist}}
&lt;!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --&gt;
*
*
*
*

&lt;!-- STOP! Be warned that by using this process instead of Articles for Creation, this article is subject to scrutiny. As an article in "mainspace", it will be DELETED if there are problems, not just declined. If you wish to use AfC, please return to the Wizard and continue from there. --&gt;



[[Category:Data management]]</text>
      <sha1>dkfg9hov9ztvim4tmf8zuybe6yp4cin</sha1>
    </revision>
  </page>
  <page>
    <title>Data philanthropy</title>
    <ns>0</ns>
    <id>49882988</id>
    <revision>
      <id>744370663</id>
      <parentid>740717604</parentid>
      <timestamp>2016-10-14T19:39:43Z</timestamp>
      <contributor>
        <ip>38.140.131.42</ip>
      </contributor>
      <comment>removed errant closing bracket</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15600" xml:space="preserve">{{Orphan|date=March 2016}}

'''Data philanthropy''' describes a form of collaboration in which private sector companies share data for public benefit.&lt;ref name="Pawelke"&gt;Pawelke, A. and Tatevossian, A. (2013, May 8) [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data philanthropy: where are we now?] United Nations Global Pulse.&lt;/ref&gt; There are multiple uses of data philanthropy being explored from humanitarian, corporate, human rights, and academic use. Since the introduction of this term the [[United Nations Global Pulse]] has began pushing for a global “data philanthropy movement.”&lt;ref name= "Coren"&gt;Coren, M. (2011, December 9) [http://www.fastcoexist.com/1678963/data-philanthropy-open-data-for-world-changing-solutions Data Philanthropy: Open data for world-changing solutions] Fast Company.&lt;/ref&gt;

== Definition==
A large amount of data collected from the Internet comes from [[user-generated content]]. This includes blogs, posts on social networks, and information submitted in forms. Besides user-generated data, corporations are also currently [[data mining]] data from consumers in order to understand customers, identify new markets, and make investment decisions. Kirkpatrick the Director at United Nations Global Pulse labels this data “massive passive data” or “data exhaust.”&lt;ref name="Kirkpatrick"&gt;Kirkpatrick, R. (2011, September 20). [http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business/ Data philanthropy is good for business] Forbes.&lt;/ref&gt; Data philanthropy is the idea that something positive can come from this overload of data. Data philanthropy is defined as the private sector sharing this data in ways that the public can benefit.&lt;ref name="Pawelke" /&gt; The term philanthropy helps to emphasis that [[data sharing]] is a positive act and that the shared data is a public good.&lt;ref name="Kirkpatrick" /&gt;

== Challenges ==
A challenge that comes with sharing data is the [[Internet privacy]] of the user whose data is being used. Mathematical techniques ([[differential privacy]] and space time boxes) have been introduced in order to make personal data accessible, while providing the users providing such data with anonymity. But even if these algorithms work there is always the possibility and fear of re-identification.&lt;ref name="Pawelke" /&gt;
 
The other challenge is convincing corporations to share their data. The big data corporations collect provides them with market competitiveness. They are able to infer meaning regarding [[consumer behavior]]. The fear is that by sharing all their information, they may lose their competitive edge.&lt;ref name="Pawelke" /&gt;

== Sharing strategies ==
The goal of data philanthropy is to create a global data commons where companies, governments, and individuals can contribute anonymous, aggregated datasets.&lt;ref name="Coren" /&gt; The United Nations Global Pulse offers four different tactics that companies can use to share their data that preserve consumer anonymity.  These include:&lt;ref name="Pawelke" /&gt;
# Share aggregated and derived data sets for analysis under nondisclosure agreements (NDA)
# Allow researchers to analyze data within the private company’s own network, under NDA
# Real-Time Data Commons: data pooled and aggregated between multiple companies of the same industry to protect competitiveness
# Public/Private Alerting Network: companies mine data behind their own firewalls and share indicators

By providing these four tactics United Nations Global Pulse hopes to provide initiative and options for companies to share their data with the public.

== Digital disease detection ==
Data philanthropy has led to advancements in the field of health and wellness. By using data gathered from social media, cell phones, and other communication modes health researchers have been able to track the spread of diseases.&lt;ref name="Schmidt"&gt;Schmidt, C. (2012). [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261963/ Trending Now: Using Social Media to Predict and Track Disease Outbreaks.] Environ Health Perspect, 120(1), A30–a33-A30–a33.&lt;/ref&gt;

In the United States [[HealthMap]], a freely available website and mobile app software is using data philanthropy related tactics to track the outbreak of diseases. HealthMap analyzes data from publicly available media sources such as news websites, government alerts, and social media sites like Twitter for outbreaks of various illnesses around the world.&lt;ref name="Schmidt" /&gt;&lt;ref name="Reddy"&gt;Reddy, E. (2015, July 14). [https://blog.twitter.com/2015/twitter-data-public-health Using Twitter data to study the world's health] Twitter.&lt;/ref&gt;  The creators of HealthMap have another website, Flu Near You, which allows users to report their own health status on a weekly basis. Traditional flu surveillance can take up to 2 weeks to confirm outbreaks.&lt;ref name= "Schmidt" /&gt; Doctors must wait for virological test to confirm the outbreak before reporting it to the Centers for Disease Control. This form of data philanthropy allows for up to date information regarding various health concerns by using publicly available information gathered from news outlets, government alerts, and social media sites. It is the data gathered on social media sites, where users are not aware their data is being mined that leads to HealthMap and Flue Near You being considered data philanthropy.&lt;ref name="Schmidt" /&gt; 
 
The [[Centers for Disease Control and Prevention]] collaborated with Google and launched [[Google Flu Trends]] in 2008, a website that tracks flu-related searches and user location to track the spread of the flu. Users can visit the website to compare the amount of flu-related search activity against the reported numbers of flu outbreaks on a graphic map. The difficulty with this method of tracking is that Google searched are sometimes performed due to curiosity rather than because an individual is suffering from the flu. According to Ashley Fowlkes, an epidemiologist in the CDC Influenza division, “the Google Flu Trends system tries to account for that type of media bias by modeling search terms over time to see which ones remain stable.”&lt;ref name="Schmidt" /&gt; Google Flu Trends is not longer publishing current flu estimates on the public website. Visitors to the site can still view and download previous estimates. Current data can be shared with verified researchers.&lt;ref name="O'Connor'"&gt;O'Connor, F. (2015, August 20). [http://www.pcworld.com/article/2974153/websites/google-flu-trends-calls-out-sick-indefinitely.html Google Flu Trends calls out sick, indefinitely] PC World.&lt;/ref&gt;
 
A study by Harvard School of Public Health (HSPH) released in the October 12, 2012 issues of the journal Science discussed how phone data helped curb the spread of malaria in Kenya. The researchers mapped phone calls and texts made by 14,816,521 Kenyan mobile phone subscribers.&lt;ref name= "Datz"&gt;Datz, T. (2012, October 11). [http://www.hsph.harvard.edu/news/press-releases/cell-phone-data-malaria/ Using cell phone data to curb the spread of malaria.] Harvard Chan.&lt;/ref&gt; When individuals left their primary living location the destination and length of journey was calculated. This data was then compared to a 2009 malaria prevalence map to estimate the disease’s commonness in each location.  Combining all this information the researchers can estimate the probability of an individual carrying malaria and map the movement of the disease. This research, a result of data philanthropy, can be used to track the spread of similar diseases.&lt;ref name="Datz" /&gt;

==Application in various fields==
Through data philanthropy ‘[[big data]]’ corporations such as [[social networking sites]], telecommunication companies, [[search engines]] amongst others, collect and make user generated information available to a data sharing system. This also permits institutions to give back to a beneficial cause. With the onset of [[technological]] advancements, sharing data on a global scale and an in-depth analysis of these data structures could alter the reaction towards certain occurrences, be it [[natural disaster]]s, [[epidemics]], worldwide [[economic]] problems and many other events. Some analyst have argued&lt;ref name="Forbes"&gt;[http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business Data Philanthropy is Good for Business], by Robert Kirkpatrick, Forbes, 2011-09-20&lt;/ref&gt; that this aggregated Information is beneficial for the common good and can lead to developments in [[research]] and [[data]] production in a range of varied fields.&lt;ref name="Forbes"/&gt;

===Humanitarian aid===
Calling patterns of [[mobile phone]] users can determine the [[socioeconomic]] standings of the populace which can be used to deduce “its access to housing, education, healthcare, and basic services such as water and electricity”.&lt;ref name="Forbes"/&gt; Researchers from Columbia University and Karolinska Institute utilize information from [[mobile phone]] providers, in order to assist in the dispersal of resources by deducing the movement of those displaced by natural disasters. Big data can also provide information on looming disasters and can assist relief organizations in rapid response and locating displaced individuals. By analyzing certain patterns within this ‘big data’, could successively transform the response to destructive occurrences like natural disasters, [[outbreak]]s of diseases and global economic distress, by employing real-time information to achieve a comprehension of the welfare of individuals. Corporations utilize digital services, such as human sensor systems to detect and solve impending problems within [[communities]]. This is a strategy implemented by the private sector in order to protect its citizens by anonymously dispersing customer information to the public sector, whilst also ensuring the protection of their privacy.&lt;ref name="Forbes"/&gt;

===Impoverished areas===
[[Poverty]] still remains a worldwide issue with over 2.5 billion people&lt;ref name="Smart Data Collective"&gt;[http://www.smartdatacollective.com/rick-delgado/200566/lifting-how-big-data-can-help-eliminate-poverty Lifting Up: How Big Data Can Help Eliminate Poverty], by Rick Delgado, Smart Data Collection , 2014-05-23&lt;/ref&gt; currently impoverished. Accumulating accurate data has been a complex issue but developments in [[technology]] and utilising 'big data',&lt;ref name="Smart Data Collective" /&gt; is one solution for improving this situation. Statistics indicate the widespread use of mobile phones, even within impoverished communities. This availability could prove vital in gathering data on populations living in poverty. Additional data can be collected through [[Internet access]], social media, utility payments and [[governmental]] statistics. Data-driven activities can lead to the cumulation of ‘big data’, which in turn can assist international non-governmental organization in documenting and evaluating the needs of underprivileged populations. Through data philanthropy, [[NGO]]’s can distribute information whilst cooperating with governments and private companies.&lt;ref name="Smart Data Collective" /&gt;

===Corporate===
Data philanthropy incorporates aspects of social philanthropy by permitting  [[corporations]] to create profound impacts through the act of giving back by dispersing proprietary datasets.&lt;ref name="Irevolution"&gt;[http://irevolution.net/2012/06/04/big-data-philanthropy-for-humanitarian-response/Big Data Philanthropy for Humanitarian Response], by Irevolution, 2012-07-04&lt;/ref&gt; The [[public sector]], is faced with an unequal and limited access to the frequency of data and they also produce, collect and preserve information, which has proven to be an essential asset. Company’s track and analyze users online activities, so as to gain more insight into their needs in relation to new products and services.&lt;ref&gt;[https://hbr.org/2014/07/sharing-data-is-a-form-of-corporate-philanthropy/Sharing Data Is a Form of Corporate Philanthropy], by Matt Stempeck,Harvard Business Review 2014-07-24&lt;/ref&gt;
These companies view the welfare of the population as a vital key to the expansion and progression of businesses by using their data to place a spotlight on the plight of global citizens.&lt;ref name="Forbes" /&gt;[[Expert]]s in the private sector contend the importance of merging various data streams such as retail, mobile phone and social media data to create necessary solutions to handle global issues. Despite the inevitable risk of sharing private information, it works in a beneficial manner and serves the interest of the public.&lt;ref&gt;[https://hbr.org/2013/03/a-new-type-of-philanthropy-don&amp;cm_sp=Article-_-Links-_-Top%20of%20Page%20Recirculation A New Type of Philanthropy: Donating Data], by Robert Kirkpatrick,Harvard Business Review 2013-03-21&lt;/ref&gt; The digital revolution causes an extensive production of ‘big data’ that is user-generated and available on the web. Corporations accumulate information on customer preferences through the digital services they utilize and products they purchase, in order to gain a clear insight on their clientele and future market opportunities.&lt;ref name="Forbes" /&gt; However the rights of individuals concerning privacy and ownership of data are a controversial issue as governments and other institutions can use this collective data for other unethical purposes. Companies monitor and probe consumer online activities in order to better comprehend and develop tailored needs for their clientele and in turn increase their profits.&lt;ref name="Jim Fruchterman"&gt;[https://hbr.org/2013/03/big-data-means-more-than-big-p Big Data Means More Than Big Profits], by Jim Fruchterman, Harvard Business Review, 2013-03-19&lt;/ref&gt;

===Academia===
Data philanthropy plays an important role in [[academia]]. Researchers encounter countless obstacles whilst attempting to access data. This data is available to a limited number of researchers with sole access to restricted resources who are authorized to utilize this information; like social media streams enabling them to produce more [[knowledge]] and develop new studies. For example, Twitter markets access to its real-time APIs at exorbitant prices, which often surpasses the budgets of most researchers. 'Data Grants’&lt;ref name="Jim Fruchterman" /&gt; is a trial program created by Twitter that provides a selective number of academics and researchers with access to real-time databases in order to garner more knowledge. They apply to gain entry into vast data downloads, on specific topics.&lt;ref name="Jim Fruchterman" /&gt;

===Human rights===
Data philanthropy aids the human rights movement, by assisting in the dispersal of evidence for truth commissions and war crimes tribunals. Proponents of human rights accumulate data on abuse occurring within states, which is then used for scientific analysis and propels awareness and action. For example, non-profit organizations compile data from Human Rights monitors in war zones in order to assist the UN High Commissioner for Human Rights. It uncovers inconsistencies in the number of casualties of war, which in turn leads to international attention and exerts influence on discussions relating to global policy.&lt;ref name="Jim Fruchterman" /&gt;

==See also==
* [[Big Data]]

==References==
&lt;references /&gt;

== External links ==
* [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data Philanthropy, where are we now?] in UN Global Pulse blog by Adreas Pawelke and Anoush Rima Tatevossian (2013-05-08).

[[Category:Big data| ]]
[[Category:Data management]]</text>
      <sha1>sl69so4qa9x7qxyff179krazz1xi9mi</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Clinical data management</title>
    <ns>14</ns>
    <id>34875542</id>
    <revision>
      <id>724141820</id>
      <parentid>478676895</parentid>
      <timestamp>2016-06-07T11:33:33Z</timestamp>
      <contributor>
        <username>Edgar181</username>
        <id>491706</id>
      </contributor>
      <comment>added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="96" xml:space="preserve">[[Category:Clinical research]]
[[Category:Pharmaceutical industry]]
[[Category:Data management]]</text>
      <sha1>03mm3kr2ro7sh2qbh6bnbwp22gapom4</sha1>
    </revision>
  </page>
  <page>
    <title>Bright Computing</title>
    <ns>0</ns>
    <id>50639093</id>
    <revision>
      <id>760522769</id>
      <parentid>760234817</parentid>
      <timestamp>2017-01-17T14:59:29Z</timestamp>
      <contributor>
        <username>Ec9exc</username>
        <id>30124234</id>
      </contributor>
      <comment>Corrected misleading claim about OpenStack, accidentally introduced on 21 December</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21981" xml:space="preserve">{{Infobox company
| name = Bright Computing
| logo = 
| logo_size = 
| logo_alt = 
| logo_caption = 
| logo_padding = 
| image = 
| image_size = 
| image_alt = 
| image_caption = 
| type = [[Privately held company|Private]]
| founded = 2009 &lt;!-- if known: {{start date and age|YYYY|MM|DD}} in [[city]], [[state]], [[country]] --&gt;
| founder = {{Unbulleted list|Matthijs van Leeuwen|Alex Ninaber}}
| hq_location = {{Unbulleted list|[[Amsterdam, The Netherlands]]|[[San Jose, California]]}}
| hq_location_city = 
| hq_location_country = 
| area_served = Global &lt;!-- or: | areas_served = --&gt;
| key_people = {{Unbulleted list|Bill Wagner ([[Chief executive officer|CEO]])|Martijn de Vries ([[Chief technology officer|CTO]])|Kristin Hansen ([[Chief marketing officer|CMO]])|Bill Griffin ([[Chief financial officer|CFO]])}}
| industry = [[Enterprise software]]
| products = {{Unbulleted list|Bright Cluster Manager for HPC|Bright Cluster Manager for Big Data|Bright OpenStack}}
| brands = 
| services = 
| former_name = ClusterVision (spin-off) 
| website = {{URL|brightcomputing.com}} &lt;!-- or: | homepage = --&gt;&lt;!-- {{URL|example.com}} --&gt;
}}
'''Bright Computing''', Inc. is a developer of [[software]] for deploying and managing [[Supercomputer|high-performance]] (HPC) clusters, [[big data]] clusters, and [[OpenStack]] in [[data center]]s and using [[cloud computing]].&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2016/02/03/24601/|title=Create Mixed HPC/Big Data Clusters Today Says Bright Computing|date=2016-02-03|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt;

== History ==
Bright Computing was founded by Matthijs van Leeuwen in 2009, who spun the company out of ClusterVision, which he had co-founded with Alex Ninaber and Arijan Sauer. Alex and Matthijs had worked together at UK’s Compusys, which was one of the first companies to commercially build HPC clusters.&lt;ref&gt;{{Cite web|url=http://www.bloomberg.com/research/stocks/private/person.asp?personId=282144720&amp;privcapId=115561075&amp;previousCapId=115561075&amp;previousTitle=Bright%2520Computing,%2520Inc.|title=Matthijs van Leeuwen: Executive Profile &amp; Biography - Businessweek|website=www.bloomberg.com|access-date=2016-05-24}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt; They left Compusys in 2002 to start ClusterVision in the [[Netherlands]], after determining there was a growing market for building and managing supercomputer clusters using off-the-shelf hardware components and open source software, tied together with their own customized scripts.&lt;ref&gt;{{Cite web|url=http://www.clustervision.com/content/management-team|title=ClusterVision - Europe's Leading HPC and Cloud Specialist|website=ClusterVision|language=en-GB|access-date=2016-05-24}}&lt;/ref&gt; ClusterVision also provided delivery and installation support services for HPC clusters at universities and government entities.&lt;ref&gt;{{Cite web|url=http://clustervision.com/about-the-company/|title=About the Company - ClusterVision|website=ClusterVision|language=en-GB|access-date=2016-05-24}}&lt;/ref&gt;

In 2004, Martijn de Vries joined ClusterVision and began development of cluster management software. The software was made available to customers in 2008, under the name ClusterVisionOS v4.&lt;ref&gt;{{Cite web|url=http://www.beowulf.org/pipermail/beowulf/2008-September/023265.html|title=Roll your own cluster management system with ClusterVisionOS v4 was: [Beowulf] What services do you run on your cluster nodes?|last=holway|first=andrew|date=2008-09-23|access-date=2016-05-24}}&lt;/ref&gt;

In 2009, Bright Computing was spun out of ClusterVision. ClusterVisionOS was renamed Bright Cluster Manager, and van Leeuwen was named Bright Computing’s CEO.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt;

In 2010, [[ING Group|ING Corporate Investments]] made a $2.5 million investment in Bright Computing. In 2014, [[Draper Fisher Jurvetson]] (DFJ) (US), [[DFJ Esprit]] (UK), Prime Ventures (NL), and [[ING Group|ING Corporate Investments]] invested $14.5 million in Bright Computing. At that time, Bright Computing and ClusterVision were completely separated.&lt;ref&gt;{{Cite web|url=https://www.crunchbase.com/organization/ing-corporate-investments#/entity|title=ING Corporate Investments {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-05-24}}&lt;/ref&gt;

In February 2016, Bright appointed Bill Wagner as chief executive officer. Matthijs van Leeuwen became chief strategy officer and board member.&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2016/02/bright-computing-names-bill-wagner-as-chief-executive-officer/|title=Bright Computing Names Bill Wagner as CEO - insideHPC|date=2016-02-16|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

== Customers  ==
Early customers included [[Boeing]],&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2011/12/boeing-consolidates-on-brite-cluster-manager/|title=Boeing Consolidates on Bright Cluster Manager - insideHPC|date=2011-12-06|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt; [[Sandia National Laboratories]],&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/news/sandia-labs-adopts-bright-cluster-manager|title=Sandia National Laboratories Adopts Bright Cluster Manager to Manage Departmental Clusters|last=Staff|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt; [[Virginia Tech]],&lt;ref&gt;{{Cite web|url=https://finance.yahoo.com/news/virginia-bioinformatics-institute-selects-bright-151500615.html|title=Virginia Bioinformatics Institute Selects Bright Cluster Manager for Big Data to Test New Research Methods|website=Yahoo Finance|access-date=2016-05-24}}&lt;/ref&gt; [[Hewlett Packard Enterprise Services|Hewlett Packard]],&lt;ref&gt;{{Cite web|url=http://www8.hp.com/h20195/v2/GetPDF.aspx/4AA5-2604ENW.pdf|title=HPC Accelerates SMBs|last=|first=|date=|website=|publisher=Hewlett-Packard Development Company, L.P.|access-date=2007-05-24}}&lt;/ref&gt; [[National Security Agency|NSA]], and [[Drexel University]]. Many early customers were introduced through resellers, including SICORP,&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2010/02/bright-computing-and-sicorp-sign-reseller-agreement/|title=Bright Computing And SICORP Sign Reseller Agreement - insideHPC|date=2010-02-22|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt; [[Cray]],&lt;ref&gt;{{Cite web|url=http://www.cray.com/company/collaboration/partners|title=Partner Relationships {{!}} Cray|website=www.cray.com|access-date=2016-05-24}}&lt;/ref&gt; [[Dell]],&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2015/11/11/MN54237|title=Bright Computing Announces Integration with Dell PowerEdge Servers for HPC Environments - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt; Appro, and Advanced HPC.&lt;ref&gt;{{Cite web|url=http://www.advancedhpc.com/high_performance_servers/gpu_computing/bright_computing.html|title=Advanced HPC - GPU Computing - GPU Software - Bright Computing|website=www.advancedhpc.com|access-date=2016-05-24}}&lt;/ref&gt;

By 2014, the company estimated 400 customers, including more than 20 [[Fortune 500]] Companies.&lt;ref&gt;{{Cite web|url=http://www.primeventures.com/portfolio/bright-computing/|title=Prime Ventures - Bright Computing|website=www.primeventures.com|access-date=2016-05-25}}&lt;/ref&gt;

== Products and services ==
Bright Cluster Manager for HPC lets customers deploy and manage complete clusters. It provides management for the hardware, the operating system, the HPC software, and users.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-roll-new-line-products/|title=Bright Computing to Roll Out New Line of Products|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

In 2014, Bright Computing introduced Bright Cluster Manager for [[Apache Hadoop]] clusters.&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/04/03/MN96527|title=Bright Computing Announces Full Support for Apache Hadoop at the Hadoop Summit Europe in Amsterdam - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt; It was later extended to support [[Apache Spark]] and other big data applications, and was renamed, Bright Cluster Manager for Big Data. It can be used as a complete solution or to manage big data software distributions from leading vendors, including [[Cloudera]] and [[Hortonworks]]. It also has several features that allow the combination of Big Data and HPC workloads on the same cluster.&lt;ref&gt;{{Cite web|url=http://www.deskeng.com/de/bright-computing-releases-bright-cluster-manager-7-2/|title=Bright Computing Releases Bright Cluster Manager 7.2|date=2016-01-22|website=Desktop Engineering|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

In 2014, the company announced Bright OpenStack, software to deploy, provision, and manage [[OpenStack]]-based private cloud infrastructures.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-announces-bright-openstack-integration-talligent-openbook-billing-software-openstack-clouds/|title=Bright Computing Announces Bright OpenStack Integration With Talligent Software|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

In January 2016, version 7.2 was released. The updates supported containers using [[Docker (software)|Docker]], improved integration with [[Puppet (software)|Puppet]] and job-based metrics.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-highlights-version-7-2-bright-cluster-manager-software-solution-enterprisehpc-2016/|title=Bright Computing Highlights Version 7.2 of Bright Cluster Manager Software Solutions at EnterpriseHPC 2016|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

Bright Cluster Manager software is frequently sold through [[original equipment manufacturer]] (OEM) resellers, including Dell and Cray.&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/usa|title=USA|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
Technology partners include:
{{Div col|3}}
* Adaptive Computing&lt;ref&gt;{{Cite web|url=http://www.adaptivecomputing.com/news/adaptive-computing-bright-computing-deepen-product-integration-enhance-provisioning-workflow-optimization-technical-computing-environments/|title=Adaptive Computing and Bright Computing Deepen Product Integration to Enhance Provisioning and Workflow Optimization in Technical Computing Environments - Adaptive Computing|date=2014-06-24|website=Adaptive Computing|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* Allinea&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2015/09/bright-computing-collaborates-on-openhpec-accelerator-suite/|title=Bright Computing Collaborates on OpenHPEC Accelerator Suite|date=2015-09-16|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Altair Engineering|Altair]]&lt;ref&gt;{{Cite web|url=http://www.prweb.com/releases/2011/9/prweb8788932.htm|title=Bright Computing Now Resells Altair PBS Professional Workload Manager, Delivering HPC Operational Efficiencies and Cost Savings|website=PRWeb|access-date=2016-05-24}}&lt;/ref&gt;
* [[Amazon Web Services|Amazon]]&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2011/11/08/bright_computing_bursts_into_cloud/|title=Bright Computing Bursts Into Cloud|date=2011-11-08|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt;
* [[Ansys]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-ansys|title=Technology Partner - Ansys|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Cavium]]&lt;ref&gt;{{Cite web|url=http://www.cavium.com/newsevents_Cavium-Collaboration-with-BrightComputing.html|title=Cavium Announces Collaboration with Bright Computing to Support the ThunderX Processor Family|website=www.cavium.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Cisco Systems|Cisco]]&lt;ref&gt;{{Cite web|url=https://gigaom.com/2014/07/28/bright-computing-takes-in-14-5m-to-push-its-cluster-management-smarts/|title=Bright Computing takes in $14.5M to push its cluster management smarts|last=Darrow|first=Barb|date=2014-07-28|website=gigaom.com|access-date=2016-05-24}}&lt;/ref&gt;
* Cloudera&lt;ref&gt;{{Cite web|url=http://insidebigdata.com/2014/04/10/bright-computing-achieves-cloudera-certification-bright-cluster-manager/|title=Bright Computing Achieves Cloudera Certification for Bright Cluster Manager|last=Brueckner|first=Rich|date=2014-04-10|website=insideBIGDATA|access-date=2016-05-24}}&lt;/ref&gt;
* Cray&lt;ref&gt;{{Cite web|url=https://cug.org/proceedings/cug2015_proceedings/includes/files/pap176-file2.pdf|title=Cray User Group - Bright Cluster Manager Presentation|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;
* DDN Storage&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-ddn|title=Technology Partner - DDN|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Dell&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2015/12/dellbright/|title=Bright Cluster Manager Integrates with Dell PowerEdge Servers for HPC Environments - insideHPC|date=2015-12-09|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Fujitsu]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-fujitsu|title=Technology Partner - Fujitsu|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Hewlett Packard Enterprise&lt;ref&gt;{{Cite web|url=http://www.datacenterknowledge.com/archives/2014/07/29/bright-cluster-management-raises-dough/|title=Bright Networks Cluster Management Gets $14.5m Round|date=2014-07-29|website=Data Center Knowledge|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* Hortonworks&lt;ref&gt;{{Cite web|url=http://hortonworks.com/partner/bright-computing/|title=Bright Computing partners with Hortonworks for Hadoop|website=Hortonworks|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Huawei]]&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/09/16/MN12537|title=Huawei signs global reseller agreement with Bright Computing - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt;
* [[IBM]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-ibm|title=Technology Partner - IBM|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Inspur]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-inspur|title=Technology Partner - Inspur|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Intel]]&lt;ref&gt;{{Cite web|url=https://ctovision.com/2015/08/bright-computing-integrated-intel-enterprise-edition-lustre/|title=Bright Computing Integrated with Intel Enterprise Edition for Lustre - CTOvision.com|date=2015-08-13|website=CTOvision.com|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Lenovo]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-lenovo|title=Technology Partner - Lenovo|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Magma&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-magma|title=Technology Partner - Magma|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Mellanox Technologies]]&lt;ref&gt;{{Cite web|url=http://www.openstack.org/news/view/214/bright-computing-announces-bright-openstack%25E2%2584%25A2-integration-with-mellanox-technologies-infiniband-switches-and-virtual-extensible-lan-(vxlan)-offloading|title=» OpenStack Open Source Cloud Computing Software|website=www.openstack.org|access-date=2016-05-24}}&lt;/ref&gt;
* [[Microsoft]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-microsoft|title=Technology Partner - Microsoft|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* NICE&lt;ref&gt;{{Cite web|url=https://www.nice-software.com/partners/bright-computing|title=Bright Computing - NICE|website=www.nice-software.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Nvidia|NVidia]]&lt;ref&gt;{{Cite web|url=https://developer.nvidia.com/bright-cluster-manager|title=Bright Cluster Manager|date=2012-01-12|website=NVIDIA Developer|access-date=2016-05-24}}&lt;/ref&gt;
* [[Red Hat|Redhat]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-redhat|title=Technology Partner - RedHat|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Runtime Design Automation&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-runtime|title=Technology Partner - Runtime|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Silicon Graphics International|SGI]]&lt;ref&gt;{{Cite web|url=https://www.sgi.com/partners/technology/partners.html|title=SGI - Partners: Technology Partners: SGI Partnerships|website=www.sgi.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Supermicro]]&lt;ref&gt;{{Cite web|url=http://www.montana.edu/rci/HyaliteCluster.html|title=Hyalite Cluster - Research Computing Group {{!}} Montana State University|website=www.montana.edu|access-date=2016-05-24}}&lt;/ref&gt;
* [[SUSE]]&lt;ref&gt;{{Citation|last=SUSE|title=Bright Computing and SUSE share common values system|date=2015-10-29|url=https://www.youtube.com/watch?v=4WwAKlOBhBo|accessdate=2016-05-24}}&lt;/ref&gt;
* [[Taligent]]&lt;ref&gt;{{Cite web|url=https://finance.yahoo.com/news/bright-computing-announces-bright-openstack-073000503.html|title=Bright Computing Announces Bright OpenStack™ Integration with Talligent Openbook Billing Software for OpenStack Clouds|website=Yahoo Finance|access-date=2016-05-24}}&lt;/ref&gt;
* [[Univa]]&lt;ref&gt;{{Cite web|url=http://www.prweb.com/releases/2012/10/prweb9961985.htm|title=Bright Computing and Univa Offer Combined Cluster &amp; Workload Management Solution|website=PRWeb|access-date=2016-05-24}}&lt;/ref&gt;
{{Div col end}}

Bright Computing was covered by [[Software Magazine]]&lt;ref&gt;{{Cite web|url=http://www.softwaremag.com/bright-computing-releases-version-7-2-of-bright-cluster-manager-for-hpc-bright-cluster-manager-for-big-data-and-bright-openstack/|title=Bright Computing Releases Version 7.2 of Bright Cluster Manager for HPC, Bright Cluster Manager for Big Data, and Bright OpenStack –|website=www.softwaremag.com|access-date=2016-05-24}}&lt;/ref&gt; and [[Yahoo! Finance]],&lt;ref&gt;{{Cite web|url=https://finance.yahoo.com/news/bright-computing-showcase-bright-openstack-162500598.html|title=Bright Computing to Showcase Bright OpenStack™ and HPC Cluster-as-a-Service (CaaS) at 2016 OpenStack Summit in Austin|website=Yahoo Finance|access-date=2016-05-24}}&lt;/ref&gt; among other publications.

== Awards ==
In 2016, Bright Computing was awarded a €1.5M [[Horizon 2020]] SME Instrument grant from the [[European Commission]].&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-receives-horizon-2020-grant-european-commission/|title=Bright Computing Receives Horizon 2020 Grant From European Commission|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

Bright Computing was one of only 33 grant recipients from 960 submitted proposals.&lt;ref&gt;{{Cite web|url=https://ec.europa.eu/easme/en/horizons-2020-sme-instrument|title=Horizon 2020's SME Instrument - EASME - European Commission|website=EASME|access-date=2016-05-24}}&lt;/ref&gt; In its category only 5 out of 260 grants were awarded.&lt;ref&gt;{{Cite web|url=https://ec.europa.eu/digital-single-market/en/open-disruptive-innovation-0|title=Open Disruptive Innovation - Digital Single Market - European Commission|website=Digital Single Market|access-date=2016-05-24}}&lt;/ref&gt;

* 2015 HPCwire Editor’s Choice Award for “Best HPC Cluster Solution or Technology."&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2015-hpcwire-readers-choice-awards/|title=2015 HPCwire Awards – Readers’ &amp; Editors’ Choice - HPCwire|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* Main Software 50 “Highest Growth” award winner, 2013.&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2013/11/14/MN16669|title=Bright Computing Wins Main Software 50 "Highest Growth" Award - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt;
* [[Deloitte]] Technology Fast50 “Rising Star 2013” Award Winner.&lt;ref&gt;{{Cite web|url=http://www2.deloitte.com/be/en/pages/about-deloitte/articles/ayden-wins-fast50.html|title=Adyen wins Deloitte Technology Fast50 {{!}} Deloitte Belgium {{!}} TMT {{!}} News, press release|website=Deloitte Belgium|access-date=2016-05-24}}&lt;/ref&gt;
* Bio-IT World Conference &amp; Expo ‘13, Boston, MA, Winner of “IT Hardware &amp; Infrastructure” category of the “Best of Show Award” program.&lt;ref&gt;{{Cite web|url=http://www.bio-itworld.com/2013/4/10/2013-bio-it-world-best-show-winners-named.html|title=2013 Bio-IT World Best of Show Winners Named - Bio-IT World|access-date=2016-05-24}}&lt;/ref&gt;
* [[Red Herring (magazine)|Red Herring]] Top 100 Global Award, 2013.&lt;ref&gt;{{Cite web|url=http://www.redherring.com/events/rhna/2013-rhnawinners/|title=2013 Top 100 North America: Winners — Red Herring|website=Red Herring|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

== References ==
{{reflist|30em}}

==Further reading==
* Morgan, Timothy Prickett (June 20, 2011). [http://www.theregister.co.uk/2011/06/20/bright_cluster_manager_5_2/ "Bright Computing revs up cluster manager"]. ''[[The Register]]''.
* Morgan, Timothy Prickett (November 8, 2011). [http://www.theregister.co.uk/2011/11/08/bright_cluster_manager_amazon_cloud_bursting/ "Bright Computing bursts HPC to EC2 clouds"]. ''The Register''.

[[Category:Big data companies]]
[[Category:Cloud computing]]
[[Category:Cloud infrastructure]]
[[Category:Cluster computing]]
[[Category:Data management]]
[[Category:Supercomputers]]</text>
      <sha1>7xmi6fj4y2mx9qedyusntrkapx8e4uy</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information governance</title>
    <ns>14</ns>
    <id>50290347</id>
    <revision>
      <id>729370852</id>
      <parentid>719818507</parentid>
      <timestamp>2016-07-11T19:10:01Z</timestamp>
      <contributor>
        <username>Le Deluge</username>
        <id>8853961</id>
      </contributor>
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="115" xml:space="preserve">{{catmain}}

[[Category:Data management]]
[[Category:Data security]]
[[Category:Information technology management]]</text>
      <sha1>fn19kszaz0vgwni78qqy728txk0qyjz</sha1>
    </revision>
  </page>
  <page>
    <title>Information repository</title>
    <ns>0</ns>
    <id>13255720</id>
    <revision>
      <id>732160430</id>
      <parentid>720231226</parentid>
      <timestamp>2016-07-30T01:30:19Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>where to place data is not a property of [[:Category:Computer storage]] but [[:Category:Data management]] decision; also this article lacks direct references, I don't see clear link to [[Computer storage]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3101" xml:space="preserve">{{other uses|Knowledge base}}

An '''information repository''' is an easy way to deploy a secondary tier of [[data storage device|data storage]] that can comprise multiple, networked data storage technologies running on diverse [[operating system]]s, where data that no longer needs to be in primary storage is protected, classified according to captured [[metadata]], processed, de-duplicated, and then purged, automatically, based on data service level objectives and requirements. In information repositories, data storage resources are virtualized as composite storage sets and operate as a [[Federation (information technology)|federated]] environment.

Information repositories were developed to mitigate problems arising from [[data proliferation]] and eliminate the need for separately deployed data storage solutions because of the concurrent deployment of diverse storage technologies running diverse operating systems. They feature centralized management for all deployed data storage resources. They are self-contained, support heterogeneous storage resources, support resource management to add, maintain, recycle, and terminate media, track of off-line media, and operate autonomously.

==Automated data management==
Since one of the main reasons for the implementation of an Information repository is to reduce the maintenance workload placed on IT staff by traditional data storage systems, information repositories are automated. Automation is accomplished via polices that can process data based on time, events, data age, and data content. Policies manage the following:
*File system space management
*Irrelevant data elimination (mp3, games, etc.)
*Secondary storage resource management
Data is processed according to media type, [[Storage virtualization|storage pool]], and [[data storage device|storage technology]].

Because information repositories are intended to reduce IT staff workload, they are designed to be easy to deploy and offer configuration flexibility, virtually limitless extensibility, redundancy, and reliable failover.

==Data recovery==
Information repositories feature robust, client based data search and recovery capabilities that, based on permissions, enable end users to search the information repository, view information repository contents, including data on off-line media, and recover individual files or multiple files to either their original [[Computer network|network]] computer or another network computer.

==References==
*NGDC Conference: Understand advanced IT infrastructures, Protecting Information: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.networkworld.com/ngdc/ 
*SNIA Enterprise Information World 2007 Conference: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.enterpriseinformationworld.com/abstracts/benefits_federated_info.htm

{{DEFAULTSORT:Information Repository}}
[[Category:Information technology management|*]]
[[Category:Content management systems|*]]
[[Category:Data management]]
[[Category:Data security]]
[[Category:Records management]]</text>
      <sha1>gc66rq9l3howzlhfvhm8n9jqsnxwc8s</sha1>
    </revision>
  </page>
  <page>
    <title>Intelligence Engine</title>
    <ns>0</ns>
    <id>51136320</id>
    <revision>
      <id>741695582</id>
      <parentid>739178254</parentid>
      <timestamp>2016-09-29T03:14:31Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>/* top */[[WP:AWB/GF|General fixes]], removed orphan tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7489" xml:space="preserve">'''Intelligence Engines''' are a type of [[enterprise information management]] that combine [[Business rule management system|business rule management]], [[predictive analytics|predictive]] and [[prescriptive analytics]] to form a unified information access platform that provides real-time intelligence through [[Web search engine|search technologies]], [[Dashboard (management information systems)|dashboards]] and/or existing business infrastructure.  Intelligence Engines are process and/or business problem specific, resulting in industry and/or function-specific [[marketing]] [[trademark]]s associated with them.  They can be differentiated from [[enterprise resource planning]] (ERP) software in that intelligence engines include organization-level business rules and proactive [[decision management]] functionality.

==History==
The first intelligence engine application appears to have been introduced in 2001 by [[Sonus Networks|Sonus Networks, Inc.]] in their patent US6961334 B1.&lt;ref name="sonus"&gt;{{cite patent | country = US | number = 6961334 | status = patent | title = Intelligence engine | gdate = 2005-11-01 | invent1 = Kaczmarczyk, Casimer M}}&lt;/ref&gt;  Applied to the field of telecommunications systems, the intelligence engine was composed of a database queried by a data distributor layer, received by a telephony management layer and acted upon by a facility management command &amp; control layer.&lt;ref name="sonus" /&gt;  This combined standalone business intelligence tools like a [[data warehouse]], reporting and querying software and a [[decision support system]].

The concept was reinforced in 2002 in patent application US20030236689 A1&lt;ref name="2002patent"&gt;{{cite patent | country = US | number = 20030236689 | status = application | title = Analyzing decision points in  business processes | pubdate = 2003-12-25 | invent1 = Casati, Fabio | invent2 = Sayal, Mehmet | invent3 = Guadalupe Castellanos, Maria | invent4 = Gunopulos, Dimitrios | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2003236689&amp;KC=A1&amp;FT=E&amp;locale=en_EP}}&lt;/ref&gt; which applied predictive quantitative models to data and used rules to correlate context data at different stages of the business process with business process outcomes to be presented to end users.&lt;ref name="2002patent" /&gt;

[[LogRhythm|LogRhythm Inc.]] advanced the concept in 2010 by adding event managers to the end of the intelligence engine's process to determine reporting, remediation and other outcomes.&lt;ref name="LogRhythm"&gt;{{cite patent | country = US | number = 2012131185 | status = application | title = Advanced Intelligence Engine | pubdate = 2012-05-24 | invent1 = Petersen , Chris | invent2 = Villella, Phillip | invent3 = Aisa, Brad | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2012131185A1&amp;KC=A1&amp;FT=D}}&lt;/ref&gt;

In 2016, professional service company [[KPMG]] continued to advance the concept by commercializing intelligence engines with the introduction of Third Party Intelligence, which is differentiated from past intelligence engines in its increased use of embedded intellectual property, diversity of global data inputs and focus on predictive analytics to mitigate risk and yield cost savings.&lt;ref name="CIOReview"&gt;{{cite web|url=http://www.cioreview.com/news/kmpg-launches-third-party-intelligence-intelligence-engine-to-anticipate-thirdparty-disruptions-nid-14407-cid-78.html |title=KMPG Launches Third Party Intelligence: Intelligence Engine to Anticipate Third-party Disruptions|accessdate=2016-07-22}}&lt;/ref&gt;

==Traits==
As a system that combines human intelligence, data inputs, automated decision-making and unified information access, intelligence engines are an advancement in business intelligence tools because they: 
* integrate structured data and unstructured content in a single index&lt;ref name="itbiz"&gt;{{cite web|url=http://www.itbusinessedge.com/blogs/it-unmasked/attivio-applies-predictive-analytics-to-indexed-data.html |title=Attivio Applies Predictive Analytics to Indexed Data |accessdate=2016-07-22}}&lt;/ref&gt;
* provide advanced workflow automation that can trigger multiple business processes&lt;ref name="salesforce"&gt;{{cite press release|url=http://www.salesforce.com/company/news-press/press-releases/2015/03/150309.jsp |title=Salesforce Unveils Service Cloud Intelligence Engine—Fueling Smarter Customer Service for the Connected World |publisher=Salesforce.com|accessdate=2016-07-22}}&lt;/ref&gt;
* project future impact of data&lt;ref name="inboundlog"&gt;{{cite web|url=http://resources.inboundlogistics.com/digital/issues/il_digital_may2016.pdf |title=Inbound Logistics Magazine May 2016|accessdate=2016-07-22}}&lt;/ref&gt; such as supply chain threats &lt;ref name="3pie"&gt;{{cite web|url=https://www.kpmgspectrum.com/3pie/index.html |title=KPMG Spectrum |accessdate=2016-07-22}}&lt;/ref&gt;
* recommend best actions&lt;ref name="custmatrix"&gt;{{cite web|url=http://www.customermatrix.com/news-and-press-releases/press-releases/145-customermatrix-unveils-first-ever-cognitive-intelligence-engine-for-crm-2 |title=CustomerMatrix Unveils First-Ever Cognitive Intelligence Engine for CRM |accessdate=2016-07-22}}&lt;/ref&gt; / highlight opportunities for process improvement&lt;ref name="parasoft"&gt;{{cite web|url=https://www.parasoft.com/capability/process-intelligence-engine/ |title=Process Intelligence Engine (PIE) |accessdate=2016-07-22}}&lt;/ref&gt;
* leverage business intelligence from a variety of experts&lt;ref name="inboundlog" /&gt; 
* combine human expertise with the power of technology to deliver actionable intelligence&lt;ref name="CIOReview" /&gt;
* scale data visualization capabilities with the number of users&lt;ref name="armanta"&gt;{{cite web|url=http://www.armanta.com/product/technology/intelligence-engine/ |title=A Big Data User Experience |accessdate=2016-07-22}}&lt;/ref&gt;

==Applications==
* Attivio Active Intelligence Engine®&lt;ref name="attivo-pr"&gt;{{cite web|url=http://info.attivio.com/rs/attivio/images/Attivio-Customer-Succes-Story-General-Electric.pdf |title=Active Intelligence Engine&amp;reg; (AIE&amp;reg;) Case Study: General Electric |accessdate=2016-07-22}}&lt;/ref&gt;
* [[KPMG]] Spectrum Intelligence Engine(s)&lt;ref name="KPMGIE"&gt;{{cite web|url=https://www.kpmgspectrum.com/3pie/about.html |title=KPMG Spectrum: Action through Intelligence |accessdate=2016-07-22}}&lt;/ref&gt;
* [[Salesforce.com|Salesforce]] Service Cloud Intelligence Engine&lt;ref name="salesforce" /&gt; 
* [[FireEye]] Threat Intelligence Engine&lt;ref name="fireeye"&gt;{{cite web|url=https://www.fireeye.com/products/dynamic-threat-intelligence/threat-intelligence-engine.html |title=FIREEYE THREAT INTELLIGENCE ENGINE |accessdate=2016-07-22}}&lt;/ref&gt;
* [[Factiva]] Intelligence Engine&lt;ref name="factiva"&gt;{{cite web|url=http://solutions.dowjones.com/collateral/files/dj-factivacom-brochure-F-3465.pdf |title=Factiva&amp;reg; - The Intelligence Engine |accessdate=2016-07-22}}&lt;/ref&gt;
* [[Parasoft]] Process Intelligence Engine&lt;ref name="parasoft" /&gt;

==See also==
* [[Business Intelligence]] (BI)
* [[Business intelligence tools]] 
* [[Business Rule Management System]]
* [[Enterprise Information Management]] 
* [[Predictive Analytics]] 
* [[Prescriptive analytics]]
* [[Decision Management]] 
* [[Data Science]]
* [[Data Mining]]

==References==

{{Reflist}}

{{DEFAULTSORT:Intelligence Engine}}
[[Category:Data management]]
[[Category:Information management]]
[[Category:Big data]]
[[Category:Business terms]]
[[Category:Business intelligence]]
[[Category:Information systems]]
[[Category:Supply chain management terms]]</text>
      <sha1>0qs2h97071mmikjalai5n7l7sb18r7r</sha1>
    </revision>
  </page>
  <page>
    <title>Atomicity (database systems)</title>
    <ns>0</ns>
    <id>373991</id>
    <revision>
      <id>760351490</id>
      <parentid>757041021</parentid>
      <timestamp>2017-01-16T12:58:01Z</timestamp>
      <contributor>
        <username>JordanMussi</username>
        <id>16764318</id>
      </contributor>
      <minor />
      <comment>Fix disambiguation link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5317" xml:space="preserve">{{Other uses|Atomicity (disambiguation)}}

In [[database system]]s, '''atomicity''' (or '''atomicness'''{{Citation needed|date=February 2016}}; from [[Greek language|Greek]] ''atomos'', ''undividable'') is one of the [[ACID]] [[database transaction|transaction]] properties. An '''atomic transaction''' is an ''indivisible'' and ''irreducible'' series of database operations such that either ''all'' occur, or ''nothing'' occurs.&lt;ref&gt;{{cite web
| accessdate = 2011-03-23
| location = http://www.webopedia.com/
| publisher = Webopedia
| title = atomic operation
| quote = An operation during which a processor can simultaneously read a location and write it in the same bus operation. This prevents any other processor or I/O device from writing or reading memory until the operation is complete.
| url = http://www.webopedia.com/TERM/A/atomic_operation.html}}&lt;/ref&gt; A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).

An example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a [[Data consistency|consistent state]], that is, money is not lost nor created if either of those two operations fail.&lt;ref&gt;{{cite web
| url = http://archive.oreilly.com/pub/a/onjava/2001/11/07/atomic.html
| title = Atomic File Transactions, Part 1 - O'Reilly Media
| last = Amsterdam
| first = Jonathan
| website = archive.oreilly.com
| access-date = 2016-02-28
}}&lt;/ref&gt;

==Orthogonality==
Atomicity does not behave completely [[Orthogonal (computing)|orthogonally]] with regard to the other [[ACID]] properties of the transactions. For example, [[Isolation (database systems)|isolation]] relies on atomicity to roll back changes in the event of isolation failures such as [[deadlock]]; [[Consistency (database systems)|consistency]] also relies on rollback in the event of a consistency-violation by an illegal transaction. Finally, atomicity itself relies on [[Durability (database systems)|durability]] to ensure the atomicity of transactions even in the face of external failures.

As a result of this, failure to detect errors and roll back the enclosing transaction may cause failures of isolation and consistency.

==Implementation==
Typically, systems implement Atomicity by providing some mechanism to indicate which transactions have started and which finished; or by keeping a copy of the data before any changes occurred ([[read-copy-update]]).  Several filesystems have developed methods for avoiding the need to keep multiple copies of data, using journaling (see [[journaling file system]]). Databases usually implement this using some form of logging/journaling to track changes. The system synchronizes the logs (often the [[metadata]]) as necessary once the actual changes have successfully taken place. Afterwards, crash recovery simply ignores incomplete entries. Although implementations vary depending on factors such as concurrency issues, the principle of atomicity — i.e. complete success or complete failure — remain.

Ultimately, any application-level implementation relies on [[operating system|operating-system]] functionality.  At the file-system level, [[POSIX]]-compliant systems provide [[system call]]s such as &lt;code&gt;open(2)&lt;/code&gt; and &lt;code&gt;flock(2)&lt;/code&gt; that allow applications to atomically open or lock a file. At the process level, [[POSIX Threads]] provide adequate synchronization primitives.

The hardware level requires [[linearizability|atomic operations]] such as [[Test-and-set]], [[Fetch-and-add]], [[Compare-and-swap]], or [[Load-Link/Store-Conditional]], together with [[memory barrier]]s.  Portable operating systems cannot simply block interrupts to implement synchronization, since hardware that lacks actual concurrent execution such as [[hyper-threading]] or [[multi-processing]] is now extremely rare.{{Citation needed|date=December 2016}}

In [[NoSQL (concept)|NoSQL]] [[data store]]s with eventual consistency, the atomicity is also weaker specified than in relational database systems, and exists only in ''row''s (i.e. [[Column family|column families]]).&lt;ref&gt;{{cite web
| accessdate = 2011-03-23
| author = Olivier Mallassi
| date = 2010-06-09
| location = http://blog.octo.com/en/
| publisher = OCTO Talks!
| title = Let’s play with Cassandra… (Part 1/3)
| quote = Atomicity is also weaker than what we are used to in the relational world. Cassandra guarantees atomicity within a &lt;code&gt;ColumnFamily&lt;/code&gt; so for all the columns of a row.
| url = http://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/}}&lt;/ref&gt;

==See also==
* [[Atomic operation]]
* [[Transaction processing]]
* [[Long-running transaction]]
* [[Read-copy-update]]

==References==
{{reflist}}

{{DEFAULTSORT:Atomicity (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>n3ihdq16xc1ngi3q437r55v46tduw06</sha1>
    </revision>
  </page>
  <page>
    <title>Data definition specification</title>
    <ns>0</ns>
    <id>37621028</id>
    <revision>
      <id>751623264</id>
      <parentid>692592795</parentid>
      <timestamp>2016-11-26T21:46:05Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8869" xml:space="preserve">{{technical|date=December 2012}}
In [[computing]], a '''data definition specification''' (DDS) is a guideline to ensure comprehensive and consistent data definition. It represents the attributes required to quantify data definition. A comprehensive data definition specification encompasses enterprise data, the hierarchy of [[data management]], prescribed guidance enforcement and criteria to determine compliance.

==Overview==
A data definition specification may be developed for any organization or specialized field, improving the quality of its products through consistency and transparency. It eliminates redundancy (since all contributing areas are referencing the same specification) and provides standardization, making it easier and more efficient to create, modify, verify, analyze and share information across the enterprise.&lt;ref&gt;Gouin, Deborah. &amp; Corcoran, Charmane K. (2008).  Developing the MSU Enterprise Data Definition Standard.  Michigan State University Web site:  http://eis.msu.edu/uploads/---University%20EIS%20Working%20Committee%20Meetings/05%20August%202008/Enterprise%20Data%20Definition%20Standard%20Presentation082708.pdf&lt;/ref&gt;
   
To understand how a data definition specification works in an enterprise, we must look at the elements of a DDS. Writing data definitions, defining business terms (or rules) in the context of a particular environment, provides structure for an organization’s [[data architecture]]. In developing these definitions, the words used must be traceable to clearly defined data.

A data definition specification may be used in the following activities to provide consistency and clarity between  departments supporting the activity:&lt;ref name="datagovernance"&gt;Thomas, Gwen. (2008). Writing Enterprise-Quality Data Definitions: Tips for Creating Terms and Definitions. Data Governance Institute Web site: http://www.datagovernance.com/dgi_wp_writing_enterprise-quality_data_definitions.pdf&lt;/ref&gt;
*  [[Business intelligence]]
*  [[Business process modeling]]
*  Business rules management
*  [[Data analysis]] and [[Data modeling|modeling]]
*  [[Information architecture]]
*  [[Metadata modeling]]
*  Report generation

== Criteria ==
A data definition specification requires data definitions to be:
* ''Atomic'' – singular, describing only one concept. Commonly used and ambiguous terms should be defined.&lt;ref name="datagovernance" /&gt; While a term refers to one concept, several words may be used in a term:
:*File – A concept identifiable with one word
:*File extension – A concept identifiable with more than one word 
* ''Traceable'' – Mapped to a specific data element. In business, a term may be traced to an entity (for example, a customer) or an attribute (such as a customer's name). A term may be a value in a [[data set]] (such as gender), or designate the data set itself. Traceability indicates relationships in the [[data hierarchy]].  
* ''Consistent'' - Used in a standard [[Syntax (programming languages)|syntax]]; if used in a specific context, the context is noted
* ''Accurate'' - Precise, correct and unambiguous, stating what the term is and is not&lt;ref&gt;International Organization for Standardization JTC1/SC32 Committee. (2004) ISO 11179-4. http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html.&lt;/ref&gt;
* ''Clear'' - Readily understood by the reader
* ''Complete'' - With the term, its description and contextual references
* ''Concise'' - To avoid circular references

== Applications ==

=== Enterprise data ===
A data definition specification was produced by the [[Open Mobile Alliance]] to document charging data.&lt;ref&gt;{{cite web|url=http://www.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|date=1 February 2011|website=Open Mobile Alliance|pages=6, 35|format=PDF|title=Charging Data|archiveurl=https://web.archive.org/web/20131006172727/http://technical.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|archivedate=6 October 2013|accessdate=12 March 2014}}&lt;/ref&gt; The document, the centralized catalog of data elements defined for interfaces, specifies the mapping of these data elements to protocol fields in the interfaces. Created for the exchange of financial data, Market Data Definition Language (MDDL) is an [[XML]] specification designed 
{{quote|to enable the interchange of information necessary to account, to analyze, and to trade financial instruments of the world's markets. It defines an XML-based interchange format and common data dictionary on the fields needed to describe: (1) financial instruments, (2) corporate events affecting value and tradability, and (3) market-related, economic and industrial indicators. The principal function of MDDL is to allow entities to exchange market data by standardizing formats and definitions. MDDL provides a common format for market data so that it can be efficiently passed from one processing system to another and provides a common understanding of market data content by standardizing terminology and by normalizing the relationships of various data elements to one another&amp;nbsp;... From the user perspective, the goal of MDDL is to enable users to integrate data from multiple sources by standardizing both the input feeds used for data warehousing (i.e., define what's being provided by vendors) and the output methods by which client applications request the data (i.e., ensure compatibility on how to get data in and out of applications)."&lt;ref&gt;{{cite web|title=Market Data Definition Language (MDDL)|date=December 26, 2002|website=Cover Pages |url=http://xml.coverpages.org/mddl.html|archiveurl=https://web.archive.org/web/20131214075132/http://xml.coverpages.org/mddl.html|archivedate=December 14, 2013|accessdate=March 12, 2014}}&lt;/ref&gt;}}

=== Clinical submissions ===
The [[Clinical Data Interchange Standards Consortium]], a global, multidisciplinary, non-profit organization, has established standards to support the acquisition, exchange, submission and archiving of clinical research data and metadata. CDISC standards are vendor-neutral, platform-independent and freely available from the CDISC website. The Case Report Tabulation Data Definition Specification (define.xml) draft version 2.0, the oldest data definition specification, is part of the evolution from the 1999 FDA electronic submission (eSub) guidance and electronic Common Technical Document (eCTD) documents specifying that a document describing the content and structure of included data be included in a submission. Define.xml was developed to automate the review process by generating a machine-readable data-definition document. Define.xml has standardized submissions to the [[Food and Drug Administration]], reducing review times from over two years to several months.&lt;ref&gt;{{cite web|title=Define-XML|year=2012|website=Clinical Data Interchange Standards Consortium|url=http://www.cdisc.org/define-xml|archiveurl=https://web.archive.org/web/20131004232219/http://www.cdisc.org/define-xml|archivedate=October 4, 2013|accessdate=March 12, 2014}}&lt;/ref&gt;

=== Archival data ===
A data definition specification is the foundation of [[metadata]] for [[scientific data archiving]]. The [[Metadata Encoding and Transmission Standard]] (METS) uses one principle of a DDS: consistent use of key terms to catalog digital objects for global use. The METS schema is a flexible mechanism for encoding descriptive, administrative and structural metadata for a [[digital library]] object and expressing complex links between metadata, and can provide a useful standard for the exchange of digital-library objects between repositories.&lt;ref&gt;Metadata Encoding &amp; Transmission Standard (METS) Web site from The Library of Congress- Standards http://www.loc.gov/standards/mets/&lt;/ref&gt;

A similar effort is underway to preserve complex data associated with video-game archiving. Preserving Virtual Worlds attempted to address archival-format deficiencies, citing the lack of suitable documentation for interactive fiction and games at the [[bit]] level: specifically, the absence of "representation information" needed to map raw bits into higher-level data constructs.&lt;ref&gt;“Meta Data Schema Development” (2008) [http://pvw.illinois.edu/pvw/?page_id=25 Preserving Virtual Worlds website]&lt;/ref&gt; Preserving Virtual Worlds 2 is a research project expanding on initial efforts in this field.&lt;ref&gt;Preserving Virtual Worlds 2, Researching best practices for videogame preservation. (2012). http://pvw.illinois.edu/pvw2/&lt;/ref&gt;

== See also ==
* [[Clinical Data Interchange Standards Consortium]] (CDISC)
* [[Data governance]]
* [[ISO/IEC 11179]]
* [[Metadata Encoding and Transmission Standard]] (METS)
* [[OASIS (organization)|OASIS]]

==References==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>rltmg2obyyo7wa5muus9ocixkcptmip</sha1>
    </revision>
  </page>
  <page>
    <title>Metadata</title>
    <ns>0</ns>
    <id>18933632</id>
    <revision>
      <id>762719018</id>
      <parentid>762687184</parentid>
      <timestamp>2017-01-30T10:20:09Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/14.223.186.17|14.223.186.17]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="63813" xml:space="preserve">{{pp-move-indef|small=yes}}
{{use dmy dates|date=October 2016}}
[[File:Schlagwortkatalog.jpg|thumb|200px|In the 2010s, metadata typically refers to digital forms; however, even traditional card catalogues from the 1960s and 1970s are an example of metadata, as the cards contain information about the books in the library (author, title, subject, etc.).]]

'''Metadata''' is "[[data]] [information] that provides information about other data".&lt;ref&gt;http://www.merriam-webster.com/dictionary/metadata&lt;/ref&gt; Three distinct types of metadata exist: '''descriptive metadata''', '''structural metadata''', and '''administrative metadata'''.&lt;ref name="Metadata Basics Outline"&gt;{{cite web | url=http://marciazeng.slis.kent.edu/metadatabasics/types.htm | title=Metadata Types and Functions | publisher=NISO | date=2004 | accessdate=5 October 2016 | author=Zeng, Marcia}}&lt;/ref&gt;

* Descriptive metadata describes a resource for purposes such as discovery and identification. It can include elements such as title, abstract, author, and keywords.
* Structural metadata is metadata about containers of metadata and indicates how compound objects are put together, for example, how pages are ordered to form chapters.
* Administrative metadata provides information to help manage a resource, such as when and how it was created, file type and other technical information, and who can access it.&lt;ref name="Understanding Metadata (2)"&gt;{{cite book | url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf | title=Understanding Metadata | publisher=NISO Press | author=National Information Standards Organization (NISO) | year=2001 | isbn=1-880124-62-9|page=1}}&lt;/ref&gt;

== History ==
Metadata was traditionally used in the [[library catalog|card catalogs]] of [[library|libraries]] until the 1980s, when libraries converted their catalog data to digital databases. In the 2000s, as digital formats are becoming the prevalent way of storing data and information, metadata is also used to describe digital data using [[metadata standards]].

There are different metadata standards for each different discipline (e.g., [[museum]] collections, [[digital audio file]]s, [[website]]s, etc.). Describing the [[Content (media)|contents]] and [[Context (computing)|context]] of data or [[computer file|data files]] increases its usefulness. For example, a [[web page]] may include metadata specifying what software language the page is written in (e.g., HTML), what tools were used to create it, what subjects the page is about, and where to find more information about the subject. This metadata can automatically improve the reader's experience and make it easier for users to find the web page online.&lt;ref name="Practices in Using Metadata"&gt;{{cite web | url=http://www.library.illinois.edu/dcc/bestpractices/chapter_11_structuralmetadata.html | title=Best Practices for Structural Metadata | publisher=University of Illinois | date=15 December 2010 | accessdate=17 June 2016}}&lt;/ref&gt; A [[CD]] may include metadata providing information about the musicians, singers and songwriters whose work appears on the disc.

A principal purpose of metadata is to help users find relevant information and discover resources. Metadata also helps to organize electronic resources, provide digital identification, and support the archiving and preservation of resources. Metadata assists users in resource discovery by "allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information."&lt;ref name = Understanding_Metadata/&gt; Metadata of telecommunication activities including [[Internet]] traffic is very widely collected by various national governmental organizations. This data is used for the purposes of [[traffic analysis]] and can be used for mass [[surveillance]].&lt;ref&gt;https://www.schneier.com/essays/archives/2014/03/metadata_surveillanc.html&lt;/ref&gt;

In many countries, the metadata relating to emails, telephone calls, web pages, video traffic, IP connections and cell phone locations are routinely stored by government organizations.&lt;ref name="NSA_Watching"&gt;http://www.washingtonsblog.com/2014/03/nsa-recorded-every-single-call-one-country-country-america.html&lt;/ref&gt;

== Definition ==
Metadata means "data about data". Although the "meta" prefix (from the [[Greek language|Greek]] [[preposition]] and [[prefix]] μετά-) means "after" or "beyond", it is used to mean "about" in [[epistemology]]. Metadata is defined as the data providing information about one or more aspects of the data; it is used to summarize basic information about data which can make tracking and working with specific data easier.&lt;ref&gt;{{cite web
| title = A Guardian Guide to your Metadata
| website = [[theguardian.com]]
| publisher = [[Guardian News and Media Limited]]
| date = 12 June 2013
| url = https://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=0000000
}}&lt;/ref&gt; Some examples include:
* Means of creation of the data
* Purpose of the data
* Time and date of creation
* Creator or author of the data
* Location on a [[computer network]] where the data was created
* [[Technical standard|Standards]] used
* File size

For example, a [[digital image]] may include metadata that describes how large the picture is, the color depth, the image resolution, when the image was created, the shutter speed, and other data.&lt;ref&gt;{{cite web|url=http://www.adeoimaging.com |title=ADEO Imaging: TIFF Metadata |accessdate=2013-05-20}}&lt;/ref&gt; A text document's metadata may contain information about how long the document is, who the author is, when the document was written, and a short summary of the document. Metadata within web pages can also contain descriptions of page content, as well as key words linked to the content.&lt;ref name="Rouse, M (2014)"&gt;{{cite web
| last = Rouse
| first = Margaret
| title = Metadata
| work = WhatIs
| publisher = TechTarget
| date = July 2014
| url = http://whatis.techtarget.com/definition/metadata
}}&lt;/ref&gt; These links are often called "Metatags", which were used as the primary factor in determining order for a web search until the late 1990s.&lt;ref name="Rouse, M (2014)"/&gt; The reliance of metatags in web searches was decreased in the late 1990s because of "keyword stuffing".&lt;ref name="Rouse, M (2014)"/&gt; Metatags were being largely misused to trick search engines into thinking some websites had more relevance in the search than they really did.&lt;ref name="Rouse, M (2014)"/&gt;

Metadata can be stored and managed in a [[database]], often called a [[metadata registry]] or [[metadata repository]].&lt;ref&gt;Hüner, K.; Otto, B.; Österle, H.: Collaborative management of business metadata, in: ''International Journal of Information Management'', 2011&lt;/ref&gt; However, without context and a point of reference, it might be impossible to identify metadata just by looking at it.&lt;ref&gt;{{cite web|url=http://www.bls.gov/ore/pdf/st000010.pdf |title=Metadata Standards And Metadata Registries: An Overview |format=PDF |accessdate=2011-12-23}}&lt;/ref&gt; For example: by itself, a database containing several numbers, all 13 digits long could be the results of calculations or a list of numbers to plug into an equation - without any other context, the numbers themselves can be perceived as the data. But if given the context that this database is a log of a book collection, those 13-digit numbers may now be identified as [[ISBN]]s - information that refers to the book, but is not itself the information within the book. The term "metadata" was coined in 1968 by Philip Bagley, in his book "Extension of Programming Language Concepts" where it is clear that he uses the term in the ISO 11179 "traditional" sense, which is "structural metadata" i.e. "data about the containers of data"; rather than the alternate sense "content about individual instances of data content" or metacontent, the type of data usually found in library catalogues.&lt;ref name=Bagley&gt;{{Cite journal
|author=Philip Bagley
|title=Extension of programming language concepts
|date=November 1968
| url = http://www.dtic.mil/dtic/tr/fulltext/u2/680815.pdf
|publisher=University City Science Center
|location=Philadelphia
}}&lt;/ref&gt;&lt;ref&gt;"The notion of "metadata" introduced by Bagley". {{Cite journal
 | last = Solntseff
 | first = N+1
 | last2 = Yezerski
 | first2 = A
 | year = 1974
 | title = A survey of extensible programming languages
 | series = Annual Review in Automatic Programming
 | publisher = Elsevier Science Ltd
 | volume = 7
 | pages = 267–307
 | doi = 10.1016/0066-4138(74)90001-9
}}&lt;/ref&gt; Since then the fields of information management, information science, information technology, librarianship, and [[GIS]] have widely adopted the term. In these fields the word ''metadata'' is defined as "data about data".&lt;ref name=NISO &gt;{{Cite book
| last = NISO
| authorlink =NISO
| title = Understanding Metadata
| publisher = NISO Press
| url = http://www.niso.org/publications/press/UnderstandingMetadata.pdf
| isbn = 1-880124-62-9
| accessdate = 5 January 2010 }}
&lt;/ref&gt;{{page needed|date=November 2016}} While this is the generally accepted definition, various disciplines have adopted their own more specific explanation and uses of the term.

== Types ==
While the metadata application is manifold, covering a large variety of fields, there are specialized and well-accepted models to specify types of metadata. [[Francis Bretherton|Bretherton]] &amp; Singley (1994) distinguish between two distinct classes: structural/control metadata and guide metadata.&lt;ref&gt;{{Cite conference
| first1 = F. P. | last1 = Bretherton | author1-link = Francis Bretherton
|first2 = P.T. | last2 = Singley
| title = Metadata: A User's View, Proceedings of the International Conference on Very Large Data Bases (VLDB)
| pages = 1091–1094
| publisher =
| year = 1994}}
&lt;/ref&gt; ''Structural metadata'' describes the structure of database objects such as tables, columns, keys and indexes. ''Guide metadata'' helps humans find specific items and are usually expressed as a set of keywords in a natural language. According to [[Ralph Kimball]] metadata can be divided into 2 similar categories: technical metadata and business metadata. ''Technical metadata'' corresponds to internal metadata, and ''business metadata'' corresponds to external metadata. Kimball adds a third category, ''process metadata''. On the other hand, NISO distinguishes among three types of metadata: descriptive, structural, and administrative.&lt;ref name=NISO/&gt;

''Descriptive metadata'' is typically used for discovery and identification, as information to search and locate an object, such as title, author, subjects, keywords, publisher. ''Structural metadata'' describes how the components of an object are organized. An example of structural metadata would be how pages are ordered to form chapters of a book. Finally, ''administrative metadata'' gives information to help manage the source. Administrative metadata refers to the technical information, including file type, or when and how the file was created. Two sub-types of administrative metadata are rights management metadata and preservation metadata. ''Rights management metadata'' explains intellectual property rights, while ''preservation metadata'' contains information to preserve and save a resource.&lt;ref name = Understanding_Metadata&gt;{{cite book|last=National Information Standards Organization|title=Understanding Metadata|year=2004|publisher=NISO Press|location=Bethesda, MD|isbn=1-880124-62-9|url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf |author2=Rebecca Guenther |author3=Jaqueline Radebaugh|accessdate=2 April 2014}}&lt;/ref&gt;{{page needed|date=November 2016}}

== Structures ==
Metadata (metacontent) or, more correctly, the vocabularies used to assemble metadata (metacontent) statements, is typically structured according to a standardized concept using a well-defined metadata scheme, including: [[metadata standards]] and [[Metadata modeling|metadata models]]. Tools such as [[Controlled vocabulary|controlled vocabularies]], [[Taxonomy (general)|taxonomies]], [[Thesaurus (information retrieval)|thesauri]], [[Data Dictionary|data dictionaries]], and [[Metadata registry|metadata registries]] can be used to apply further standardization to the metadata. Structural metadata commonality is also of paramount importance in [[data model]] development and in [[database design]].

=== Syntax ===
Metadata (metacontent) syntax refers to the rules created to structure the fields or elements of metadata (metacontent).&lt;ref&gt;{{cite web
| last = Cathro
| first = Warwick
| authorlink =
| title = Metadata: an overview
| year = 1997
| url = http://www.nla.gov.au/nla/staffpaper/cathro3.html
| accessdate = 6 January 2010
}}&lt;/ref&gt; A single metadata scheme may be expressed in a number of different markup or programming languages, each of which requires a different syntax. For example, Dublin Core may be expressed in plain text, [[HTML]], [[XML]], and [[Resource Description Framework|RDF]].&lt;ref&gt;{{cite web
| last = DCMI
| authorlink =Dublin_Core_Metadata_Initiative
| title = Semantic Recommendations
| date =5 October 2009
| url = http://dublincore.org/specifications/
| accessdate = 6 January 2010
}}&lt;/ref&gt;

A common example of (guide) metacontent is the bibliographic classification, the subject, the [[List of Dewey Decimal classes|Dewey Decimal class number]]. There is always an implied statement in any "classification" of some object. To classify an object as, for example, Dewey class number 514 (Topology) (i.e. books having the number 514 on their spine) the implied statement is: "&lt;nowiki&gt;&lt;book&gt;&lt;subject heading&gt;&lt;514&gt;&lt;/nowiki&gt;. This is a subject-predicate-object triple, or more importantly, a class-attribute-value triple. The first two elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. "metacontent = metadata + master data". All of these elements can be thought of as "vocabulary". Both metadata and master data are vocabularies which can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by [[ISO 25964]]: "If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved."&lt;ref&gt;https://www.iso.org/obp/ui/#iso:std:iso:25964:-1:ed-1:v1:en&lt;/ref&gt; This is particularly relevant when considering search engines of the internet, such as Google. The process indexes pages then matches text strings using its complex algorithm; there is no intelligence or "inferencing" occurring, just the illusion thereof.

=== Hierarchical, linear and planar schemata ===
Metadata schemata can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent-child relationships exist between the elements.
An example of a hierarchical metadata schema is the [[Learning object metadata|IEEE LOM]] schema, in which metadata elements may belong to a parent metadata element.
Metadata schemata can also be one-dimensional, or linear, where each element is completely discrete from other elements and classified according to one dimension only.
An example of a linear metadata schema is the [[Dublin Core Metadata Initiative|Dublin Core]] schema, which is one dimensional.
Metadata schemata are often two dimensional, or planar, where each element is completely discrete from other elements but classified according to two orthogonal dimensions.&lt;ref&gt;{{cite web
| title = Types of Metadata
| publisher = [[University of Melbourne]]
| date = 15 August 2006
| url = http://www.infodiv.unimelb.edu.au/metadata/add_info.html
| accessdate = 6 January 2010
| archiveurl = https://web.archive.org/web/20091024112353/http://www.infodiv.unimelb.edu.au/metadata/add_info.html
| archivedate = 2009-10-24}}&lt;/ref&gt;

=== Hypermapping ===
In all cases where the metadata schemata exceed the planar depiction, some type of [[hypermap]]ping is required to enable display and view of metadata according to chosen aspect and to serve special views. Hypermapping frequently applies to layering of geographical and geological information overlays.&lt;ref&gt;{{cite web |url=http://www.isprs.org/proceedings/XXXII/part4/kuebler51.pdf |title=THE DESIGN AND DEVELOPMENT OF A GEOLOGIC HYPERMAP PROTOTYPE |first1=Stefanie |last1=Kübler |first2=Wolfdietrich |last2=Skala |first3=Agnès |last3=Voisard}}&lt;/ref&gt;

=== Granularity ===
The degree to which the data or metadata is structured is referred to as its [[Data granularity|"granularity"]]. "Granularity" refers to how much detail is provided. Metadata with a high granularity allows for deeper, more detailed, and more structured information and enables greater levels of technical manipulation. A lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information. The major impact of granularity is not only on creation and capture, but moreover on maintenance costs. As soon as the metadata structures become outdated, so too is the access to the referred data. Hence granularity must take into account the effort to create the metadata as well as the effort to maintain it.

== Standards ==
International standards apply to metadata. Much work is being accomplished in the national and international standards communities, especially [[ANSI]] (American National Standards Institute) and [[International Organization for Standardization|ISO]] (International Organization for Standardization) to reach consensus on standardizing metadata and registries. The core metadata registry standard is [[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 11179 Metadata Registries (MDR), the framework for the standard is described in ISO/IEC 11179-1:2004.&lt;ref&gt;{{cite web
  |url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=39438
  |title=ISO/IEC 11179-1:2004 Information technology - Metadata registries (MDR) - Part 1: Framework
  |publisher=Iso.org |date=2009-03-18 |accessdate=2011-12-23
}}&lt;/ref&gt; A new edition of Part 1 is in its final stage for publication in 2015 or early 2016. It has been revised to align with the current edition of Part 3, ISO/IEC 11179-3:2013&lt;ref&gt;{{cite web
  |url=http://standards.iso.org/ittf/PubliclyAvailableStandards/c050340_ISO_IEC_11179-3_2013.zip
  |title=ISO/IEC 11179-3:2013 Information technology-Metadata registries - Part 3: Registry metamodel and basic attributes
  |publisher=iso.org|date=2014}}&lt;/ref&gt; which extends the MDR to support registration of Concept Systems.
(see [[ISO/IEC 11179]]). This standard specifies a schema for recording both the meaning and technical structure of the data for unambiguous usage by humans and computers. ISO/IEC 11179 standard refers to metadata as information objects about data, or "data about data". In ISO/IEC 11179 Part-3, the information objects are data about Data Elements, Value Domains, and other reusable semantic and representational information objects that describe the meaning and technical details of a data item. This standard also prescribes the details for a metadata registry, and for registering and administering the information objects within a Metadata Registry. ISO/IEC 11179 Part 3 also has provisions for describing compound structures that are derivations of other data elements, for example through calculations, collections of one or more data elements, or other forms of derived data. While this standard describes itself originally as a "data element" registry, its purpose is to support describing and registering metadata content independently of any particular application, lending the descriptions to being discovered and reused by humans or computers in developing new applications, databases, or for analysis of data collected in accordance with the registered metadata content. This standard has become the general basis for other kinds of metadata registries, reusing and extending the registration and administration portion of the standard.

The Geospatial community has a tradition of specialized [[geospatial metadata]] standards, particularly building on traditions of map- and image-libraries and catalogues. Formal metadata is usually essential for geospatial data, as common text-processing approaches are not applicable.

The [[Dublin Core]] metadata terms are a set of vocabulary terms which can be used to describe resources for the purposes of discovery. The original set of 15 classic&lt;ref&gt;{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=2009-12-14 |accessdate=2013-08-17}}&lt;/ref&gt; metadata terms, known as the Dublin Core Metadata Element Set&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=2013-08-17}}&lt;/ref&gt; are endorsed in the following standards documents:
* IETF RFC 5013&lt;ref&gt;{{cite web |url= http://www.ietf.org/rfc/rfc5013.txt |title=The Dublin Core Metadata Element Set |author=J. Kunze, T. Baker |work=ietf.org |year=2007 |accessdate=17 August 2013}}&lt;/ref&gt;
* ISO Standard 15836-2009&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=2009-02-18 |accessdate=2013-08-17}}&lt;/ref&gt;
* NISO Standard Z39.85.&lt;ref&gt;{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&amp;gid=None&amp;project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=2007-05-22 |accessdate=2013-08-17}}&lt;/ref&gt;

Although not a standard, [[Microformat]] (also mentioned in the section [[Metadata#Metadata on the Internet|metadata on the internet]] below) is a web-based approach to semantic markup which seeks to re-use existing HTML/XHTML tags to convey metadata. Microformat follows XHTML and HTML standards but is not a standard in itself. One advocate of microformats, [[Tantek Çelik]], characterized a problem with alternative approaches: {{cquote|Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry.&lt;ref name="Wharton000"&gt;{{cite web |title=What's the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&amp;ID=1247}}&lt;/ref&gt;}}

== Use ==

=== Photographs ===
Metadata may be written into a [[digital photo]] file that will identify who owns it, copyright and contact information, what brand or model of camera created the file, along with exposure information (shutter speed, f-stop, etc.) and descriptive information, such as keywords about the photo, making the file or image searchable on a computer and/or the Internet. Some metadata is created by the camera and some is input by the photographer and/or software after downloading to a computer. Most digital cameras write metadata about model number, shutter speed, etc., and some enable you to edit it;&lt;ref&gt;{{cite web|publisher=gurucamera.com|title=How To Copyright Your Photos With Metadata|url=https://gurucamera.com/copyright-photos-metadata/|work=Guru Camera}}&lt;/ref&gt; this functionality has been available on most Nikon DSLRs since the [[Nikon D3]], on most new Canon cameras since the [[Canon EOS 7D]], and on most Pentax DSLRs since the Pentax K-3. Metadata can be used to make organizing in post-production easier with the use of key-wording. Filters can be used to analyze a specific set of photographs and create selections on criteria like rating or capture time.

Photographic Metadata Standards are governed by organizations that develop the following standards. They include, but are not limited to:
* [[IPTC Information Interchange Model]] IIM (International Press Telecommunications Council),
* [[International Press Telecommunications Council|IPTC]] Core Schema for XMP
* [[Extensible Metadata Platform|XMP]] – Extensible Metadata Platform (an ISO standard)
* [[Exchangeable image file format|Exif]] – Exchangeable image file format, Maintained by CIPA (Camera &amp; Imaging Products Association) and published by JEITA (Japan Electronics and Information Technology Industries Association)
* [[Dublin Core]] (Dublin Core Metadata Initiative – DCMI)
* PLUS (Picture Licensing Universal System).
* [http://www.loc.gov/standards/vracore/schemas.html VRA Core] (Visual Resource Association)&lt;ref&gt;{{cite web|title=VRA Core Support Pages|url=http://core.vraweb.org|website=Visual Resource Association Foundation|publisher=Visual Resource Association Foundation|accessdate=27 February 2016}}&lt;/ref&gt;

=== Telecommunications ===
Information on the times, origins and destinations of phone calls, electronic messages, instant messages and other modes of telecommunication, as opposed to message content, is another form of metadata. Bulk collection of this [[call detail record]] metadata by intelligence agencies has proven controversial after disclosures by [[Edward Snowden]] Intelligence agencies such as the NSA are keeping online metadata of millions of internet user for up to a year, regardless of whether or not they are persons of interest to the agency.

=== Video ===
Metadata is particularly useful in video, where information about its contents (such as transcripts of conversations and text descriptions of its scenes) is not directly understandable by a computer, but where efficient search of the content is desirable. There are two sources in which video metadata is derived: (1) operational gathered metadata, that is information about the content produced, such as the type of equipment, software, date, and location; (2) human-authored metadata, to improve search engine visibility, discoverability, audience engagement, and providing advertising opportunities to video publishers.&lt;ref&gt;{{cite web
| last = Webcase
| first = Weblog
| authorlink =
| title = Examining video file metadata
| year = 2011
| url = http://veresoftware.com/blog/?p=364
| accessdate = 25 November 2015
}}&lt;/ref&gt; In today's society most professional video editing software has access to metadata. Avid's MetaSync and Adobe's Bridge are two prime examples of this.&lt;ref&gt;{{cite web
| last = Oak Tree Press
| authorlink =
| title = Metadata for Video
| year = 2011
| url = http://veresoftware.com/blog/?p=364
| accessdate = 25 November 2015
}}&lt;/ref&gt;

=== Web pages ===
[[Web page]]s often include metadata in the form of [[Meta element|meta tags]]. Description and keywords in meta tags are commonly used to describe the Web page's content. Meta elements also specify page description, key words, authors of the document, and when the document was last modified.&lt;ref name="Rouse, M (2014)"/&gt; Web page metadata helps search engines and users to find the types of web pages they are looking for.

== Creation ==
Metadata can be created either by automated information processing or by manual work. Elementary metadata captured by computers can include information about when an object was created, who created it, when it was last updated, file size, and file extension. In this context an ''object'' refers to any of the following:

* A physical item such as a book, CD, DVD, a paper map, chair, table, flower pot, etc.
* An electronic file such as a digital image, digital photo, electronic document, program file, database table, etc.

=== Data virtualization ===
{{main article|Data virtualization}}
Data virtualization has emerged in the 2000s as the new software technology to complete the virtualization "stack" in the enterprise. Metadata is used in data virtualization servers which are enterprise infrastructure components, alongside database and application servers. Metadata in these servers is saved as persistent repository and describe [[business object]]s in various enterprise systems and applications. Structural metadata commonality is also important to support data virtualization.

=== Statistics and census services ===
Standardization work has had a large impact on efforts to build metadata systems in the statistical community{{Citation needed|date=May 2013}}. Several metadata standards{{Which|date=May 2013}} are described, and their importance to statistical agencies is discussed. Applications of the standards{{Which|date=May 2013}} at the Census Bureau, Environmental Protection Agency, Bureau of Labor Statistics, Statistics Canada, and many others are described{{Citation needed|date=May 2013}}. Emphasis is on the impact a metadata registry can have in a statistical agency.

=== Library and information science ===

Metadata has been used in various ways as a means of cataloging items in libraries in both digital and analog format. Such data helps classify, aggregate, identify, and locate a particular book, DVD, magazine or any object a library might hold in its collection. Until the 1980s, many library catalogues used 3x5 inch cards in file drawers to display a book's title, author, subject matter, and an abbreviated [[Alphanumeric|alpha-numeric]] string ([[call number]]) which indicated the physical location of the book within the library's shelves. The [[Dewey Decimal Classification|Dewey Decimal System]] employed by libraries for the classification of library materials by subject is an early example of metadata usage. Beginning in the 1980s and 1990s, many libraries replaced these paper file cards with computer databases. These computer databases make it much easier and faster for users to do keyword searches. Another form of older metadata collection is the use by US Census Bureau of what is known as the "Long Form." The Long Form asks questions that are used to create demographic data to find patterns of distribution.&lt;ref&gt;{{cite web
| title = AGLS Metadata Element Set - Part 2: Usage Guide - A non-technical guide to using AGLS metadata for describing resources
| author = National Archives of Australia
| year = 2002
| url = http://www.naa.gov.au/records-management/publications/agls-element.aspx
| accessdate = 17 March 2010}}
&lt;/ref&gt; [[library|Libraries]] employ metadata in [[library catalog]]ues, most commonly as part of an [[Library management system|Integrated Library Management System]]. Metadata is obtained by [[Library cataloguing#Cataloging rules|cataloguing]] resources such as books, periodicals, DVDs, web pages or digital images. This data is stored in the integrated library management system, [[Library management system|ILMS]], using the [[MARC standards|MARC]] metadata standard. The purpose is to direct patrons to the physical or electronic location of items or areas they seek as well as to provide a description of the item/s in question.

More recent and specialized instances of library metadata include the establishment of [[Digital library|digital libraries]] including [[eprint|e-print]] repositories and digital image libraries. While often based on library principles, the focus on non-librarian use, especially in providing metadata, means they do not follow traditional or common cataloging approaches. Given the custom nature of included materials, metadata fields are often specially created e.g. taxonomic classification fields, location fields, keywords or copyright statement. Standard file information such as file size and format are usually automatically included.&lt;ref name=solodovnik&gt;{{cite journal | last1 = Solodovnik | first1 = Iryna | year = 2011 | title = Metadata issues in Digital Libraries: key concepts and perspectives | journal = [[JLIS.it|JLIS.it: Italian Journal of Library, Archives and Information Science]] | volume = 2 | issue = 2 | publisher = University of Florence | doi = 10.4403/jlis.it-4663 | url = http://leo.cilea.it/index.php/jlis/article/view/4663 | accessdate = 29 June 2013}}&lt;/ref&gt; Library operation has for decades been a key topic in efforts toward [[international standardization]]. Standards for metadata in digital libraries include [[Dublin Core]], [[Metadata Encoding and Transmission Standard|METS]], [[Metadata Object Description Schema|MODS]], [[Data Documentation Initiative|DDI]], [[Digital object identifier|DOI]], [[Uniform Resource Name|URN]], [[Preservation Metadata: Implementation Strategies|PREMIS]] schema, [[Ecological Metadata Language|EML]], and [[Protocol for Metadata Harvesting|OAI-PMH]]. Leading libraries in the world give hints on their metadata standards strategies.&lt;ref&gt;{{cite web |author=Library of Congress Network Development and MARC Standards Office |url=http://www.loc.gov/standards/metadata.html |title=Library of Congress Washington DC on metadata |publisher=Loc.gov |date=2005-09-08 |accessdate=2011-12-23}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.dnb.de/DE/Netzpublikationen/Ablieferung/MetadatenKernset/metadatenkernset_node.html |title=Deutsche Nationalbibliothek Frankfurt on metadata}}&lt;/ref&gt;

=== In museums ===
Metadata in a museum context is the information that trained cultural documentation specialists, such as [[archivist]]s, [[librarian]]s, museum [[registrar (museum)|registrar]]s and [[curator]]s, create to index, structure, describe, identify, or otherwise specify works of art, architecture, cultural objects and their images.&lt;ref name=":0"&gt;{{Cite journal |last=Zange|first=Charles S.|date=31 January 2015 |title=Community makers, major museums, and the Keet S'aaxw: Learning about the role of museums in interpreting cultural objects |url=http://mw2015.museumsandtheweb.com/paper/community-makers-major-museums-and-the-keet-saaxw-learning-about-the-role-of-museums-in-interpreting-cultural-objects/ |publisher=Museums and the Web }}&lt;/ref&gt;&lt;ref name=":1"&gt;{{Cite book |title=Cataloging cultural objects: a guide to describing cultural works and their images. Visual Resources Association |last=Baca |first=Murtha |publisher=Visual Resources Association |year=2006 |isbn=|location=|pages=}}&lt;/ref&gt;{{page needed|date=November 2016}}&lt;ref name=":2"&gt;{{Cite book|title=Introduction to Metadata: Second Edition. Los Angeles: Getty Information Institute|last=Baca|first=Murtha|publisher=Getty Information Institute|year=2008|isbn=|location=Los Angeles|pages=}}&lt;/ref&gt;{{page needed|date=November 2016}} Descriptive metadata is most commonly used in museum contexts for object identification and resource recovery purposes.&lt;ref name=":1" /&gt;

==== Usage ====
Metadata is developed and applied within collecting institutions and museums in order to:
* Facilitate resource discovery and execute search queries.&lt;ref name=":2" /&gt;
* Create digital archives that store information relating to various aspects of museum collections and cultural objects, and serves for archival and managerial purposes.&lt;ref name=":2" /&gt;
* Provide public audiences access to cultural objects through publishing digital content online.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt;

==== Standards ====
Many museums and cultural heritage centers recognize that given the diversity of art works and cultural objects, no single model or standard suffices to describe and catalogue cultural works.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; For example, a sculpted Indigenous artifact could be classified as an artwork, an archaeological artifact, or an Indigenous heritage item. The early stages of standardization in archiving, description and cataloging within the museum community began in the late 1990s with the development of standards such as [[Categories for the Description of Works of Art (CDWA)]], Spectrum, the [[Conceptual Reference Model (CIDOC)]], [[Cataloging Cultural Objects (CCO)]] and the [[CDWA Lite XML schema]].&lt;ref name=":1" /&gt; These standards use [[HTML]] and [[XML]] markup languages for machine processing, publication and implementation.&lt;ref name=":1" /&gt; The [[Anglo-American Cataloguing Rules (AACR)]], originally developed for characterizing books, have also been applied to cultural objects, works of art and architecture.&lt;ref name=":2" /&gt; Standards, such as the CCO, are integrated within a Museum's [[Collection Management System (CMS)]], a database through which museums are able to manage their collections, acquisitions, loans and conservation.&lt;ref name=":2" /&gt; Scholars and professionals in the field note that the "quickly evolving landscape of standards and technologies" create challenges for cultural documentarians, specifically non-technically trained professionals.&lt;ref name=":3"&gt;{{Cite book|title=Linked Data for Libraries, Archives and Museums: How to Clean, Link and Publish Your Metadata|last=Hooland|first=Seth Van|last2=Verborgh|first2=Ruben|publisher=Facet|year=2014|isbn=|location=London|pages=}}&lt;/ref&gt;{{page needed|date=November 2016}} Most collecting institutions and museums use a [[relational database]] to categorize cultural works and their images.&lt;ref name=":2" /&gt; Relational databases and metadata work to document and describe the complex relationships amongst cultural objects and multi-faceted works of art, as well as between objects and places, people and artistic movements.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; Relational database structures are also beneficial within collecting institutions and museums because they allow for archivists to make a clear distinction between cultural objects and their images; an unclear distinction could lead to confusing and inaccurate searches.&lt;ref name=":2" /&gt;

==== Cultural objects and art works ====
An object's materiality, function and purpose, as well as the size (e.g., measurements, such as height, width, weight), storage requirements (e.g., climate-controlled environment) and focus of the museum and collection, influence the descriptive depth of the data attributed to the object by cultural documentarians.&lt;ref name=":2" /&gt; The established institutional cataloging practices, goals and expertise of cultural documentarians and database structure also influence the information ascribed to cultural objects, and the ways in which cultural objects are categorized.&lt;ref name=":0" /&gt;&lt;ref name=":2" /&gt; Additionally, museums often employ standardized commercial collection management software that prescribes and limits the ways in which archivists can describe artworks and cultural objects.&lt;ref name=":3" /&gt; As well, collecting institutions and museums use [[Controlled vocabulary|Controlled Vocabularies]] to describe cultural objects and artworks in their collections.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; [[Getty Vocabularies]] and the [[Library of Congress controlled vocabularies|Library of Congress Controlled Vocabularies]] are reputable within the museum community and are recommended by CCO standards.&lt;ref name=":2" /&gt; Museums are encouraged to use controlled vocabularies that are contextual and relevant to their collections and enhance the functionality of their digital information systems.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; Controlled Vocabularies are beneficial within databases because they provide a high level of consistency, improving resource retrieval.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; Metadata structures, including controlled vocabularies, reflect the [[Ontology (information science)|ontologies]] of the systems from which they were created. Often the processes through which cultural objects are described and categorized through metadata in museums do not reflect the perspectives of the maker communities.&lt;ref name=":0" /&gt;&lt;ref&gt;{{Cite journal |last=Srinivasan |first=Ramesh |date=December 2006 |title=Indigenous, ethnic and cultural articulations of new media |url=http://ics.sagepub.com/content/9/4/497.abstract |journal=International Journal of Cultural Studies |volume=9 |issue=4 |doi=10.1177/1367877906069899}}&lt;/ref&gt;

==== Museums and the Internet ====
Metadata has been instrumental in the creation of digital information systems and archives within museums, and has made it easier for museums to publish digital content online. This has enabled audiences who might not have had access to cultural objects due to geographic or economic barriers to have access to them.&lt;ref name=":1" /&gt; In the 2000s, as more museums have adopted archival standards and created intricate databases, discussions about [[Linked data|Linked Data]] between museum databases have come up in the museum, archival and library science communities.&lt;ref name=":3" /&gt; Collection Management Systems (CMS) and [[Digital asset management|Digital Asset Management]] tools can be local or shared systems.&lt;ref name=":2" /&gt; [[Digital humanities|Digital Humanities]] scholars note many benefits of interoperability between museum databases and collections, while also acknowledging the difficulties achieving such interoperability.&lt;ref name=":3" /&gt;

=== Law ===

==== United States of America ====
{{Globalize
|date=March 2015
}}
Problems involving metadata in [[litigation]] in the [[United States]] are becoming widespread.{{when|date=February 2011}} Courts have looked at various questions involving metadata, including the [[discovery (law)|discoverability]] of metadata by parties. Although the Federal Rules of Civil Procedure have only specified rules about electronic documents, subsequent case law has elaborated on the requirement of parties to reveal metadata.&lt;ref&gt;{{Cite journal
  | last = Gelzer | first = Reed D.
  | title = Metadata, Law, and the Real World: Slowly, the Three Are Merging
  | journal = Journal of AHIMA
  | volume = 79
  | issue = 2
  | pages = 56–57, 64
  | publisher = American Health Information Management Association
  | date = February 2008
  | url = http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_036537.hcsp?dDocName=bok1_036537
  | accessdate = 8 January 2010}}&lt;/ref&gt; In October 2009, the [[Arizona Supreme Court]] has ruled that metadata records are [[public record]].&lt;ref&gt;{{Cite news
  | last = Walsh | first = Jim
  | title = Ariz. Supreme Court rules electronic data is public record
  | newspaper = The Arizona Republic
  | location = Phoenix, Arizona
  | date = 30 October 2009
  | url = http://www.azcentral.com/arizonarepublic/local/articles/2009/10/30/20091030metadata1030.html
  | accessdate = 8 January 2010
}}&lt;/ref&gt; Document metadata have proven particularly important in legal environments in which litigation has requested metadata, which can include sensitive information detrimental to a certain party in court. Using [[metadata removal tool]]s to "clean" or redact documents can mitigate the risks of unwittingly sending sensitive data. This process partially (see [[data remanence]]) protects law firms from potentially damaging leaking of sensitive data through [[electronic discovery]].

====Australia====

In Australia the need to strengthen national security has resulted in the introduction of a new metadata storage law.&lt;ref&gt;Senate passes controversial metadata laws&lt;/ref&gt; This new law means that both security and policing agencies will be allowed to access up to two years of an individual's metadata, supposedly to make it easier to stop any terrorist attacks and serious crimes from happening. In the 2000s, the law does not allow access to content of people's messages, phone calls or email and web-browsing history, but these provisions could be changed by the government.

=== In healthcare ===
Australian medical research pioneered the definition of metadata for applications in health care. That approach offers the first recognized attempt to adhere to international standards in medical sciences instead of defining a proprietary standard under the [[World Health Organization]] (WHO) umbrella. The medical community yet did not approve the need to follow metadata standards despite research that supported these standards.&lt;ref&gt;M. Löbe, M. Knuth, R. Mücke [http://ceur-ws.org/Vol-559/Paper1.pdf TIM: A Semantic Web Application for the Specification of Metadata Items in Clinical Research], CEUR-WS.org, urn:nbn:de:0074-559-9&lt;/ref&gt;

=== Data warehousing ===
[[Data warehouse]] (DW) is a repository of an organization's electronically stored data. Data warehouses are designed to manage and store the data. Data warehouses differ from [[business intelligence]] (BI) systems, because BI systems are designed to use data to create reports and analyze the information, to provide strategic guidance to management.&lt;ref&gt;Inmon, W.H. Tech Topic: What is a Data Warehouse? Prism Solutions. Volume 1. 1995.&lt;/ref&gt; Metadata is an important tool in how data is stored in data warehouses. The purpose of a data warehouse is to house standardized, structured, consistent, integrated, correct, "cleaned" and timely data, extracted from various operational systems in an organization. The extracted data are integrated in the data warehouse environment to provide an enterprise-wide perspective. Data are structured in a way to serve the reporting and analytic requirements. The design of structural metadata commonality using a [[data modeling]] method such as [[entity relationship model]] diagramming is important in any data warehouse development effort. They detail metadata on each piece of data in the data warehouse. An essential component of a [[data warehouse]]/[[business intelligence]] system is the metadata and tools to manage and retrieve the metadata. [[Ralph Kimball]]&lt;ref&gt;{{Cite book
  |last=Kimball |first=Ralph
  |authorlink=Ralph Kimball
  |title=The Data Warehouse Lifecycle Toolkit
  |edition=Second
  |location=New York |publisher=Wiley
  |year=2008
  |isbn=978-0-470-14977-5
  |ref=harv
  |pages=10, 115–117, 131–132, 140, 154–155
}}&lt;/ref&gt;{{page needed|date=November 2016}} describes metadata as the DNA of the data warehouse as metadata defines the elements of the [[data warehouse]] and how they work together.

[[Ralph Kimball|Kimball]] et al.&lt;ref&gt;{{harvnb|Kimball|2008|pages=116–117}}&lt;/ref&gt; refers to three main categories of metadata: Technical metadata, business metadata and process metadata. Technical metadata is primarily [[definitional]], while business metadata and process metadata is primarily descriptive. The categories sometimes overlap.
* '''Technical metadata''' defines the objects and processes in a DW/BI system, as seen from a technical point of view. The technical metadata includes the system metadata, which defines the data structures such as tables, fields, data types, indexes and partitions in the relational engine, as well as databases, dimensions, measures, and data mining models. Technical metadata defines the data model and the way it is displayed for the users, with the reports, schedules, distribution lists, and user security rights.
* '''Business metadata''' is content from the data warehouse described in more user-friendly terms. The business metadata tells you what data you have, where they come from, what they mean and what their relationship is to other data in the data warehouse. Business metadata may also serve as a documentation for the DW/BI system. Users who browse the data warehouse are primarily viewing the business metadata.
* '''Process metadata''' is used to describe the results of various operations in the data warehouse. Within the [[Extract, transform, load|ETL]] process, all key data from tasks is logged on execution. This includes start time, end time, CPU seconds used, disk reads, disk writes, and rows processed. When troubleshooting the ETL or [[Information retrieval|query]] process, this sort of data becomes valuable. Process metadata is the fact measurement when building and using a DW/BI system. Some organizations make a living out of collecting and selling this sort of data to companies - in that case the process metadata becomes the business metadata for the fact and dimension tables. Collecting process metadata is in the interest of business people who can use the data to identify the users of their products, which products they are using, and what level of service they are receiving.

=== On the Internet ===
The [[HTML]] format used to define web pages allows for the inclusion of a variety of types of metadata, from basic descriptive text, dates and keywords to further advanced metadata schemes such as the [[Dublin Core]], [[e-GMS]], and AGLS&lt;ref&gt;National Archives of Australia, AGLS Metadata Standard, accessed 7 January 2010, [http://www.naa.gov.au/records-management/create-capture-describe/describe/AGLS/index.aspx]&lt;/ref&gt; standards. Pages can also be [[geotagging|geotagged]] with [[Geographic coordinate system|coordinates]]. Metadata may be included in the page's header or in a separate file. [[Microformat]]s allow metadata to be added to on-page data in a way that regular web users do not see, but computers, [[web crawler]]s and [[search engine]]s can readily access. Many search engines are cautious about using metadata in their ranking algorithms due to exploitation of metadata and the practice of search engine optimization, [[Search engine optimization|SEO]], to improve rankings. See [[Meta element]] article for further discussion. This cautious attitude may be justified as people, according to Doctorow,&lt;ref&gt;Metacrap: Putting the torch to seven straw-men of the meta-utopia http://www.well.com/~doctorow/metacrap.htm&lt;/ref&gt; are not executing care and diligence when creating their own metadata and that metadata is part of a competitive environment where the metadata is used to promote the metadata creators own purposes. Studies show that search engines respond to web pages with metadata implementations,&lt;ref&gt;The impact of webpage content characteristics on webpage visibility in search engine results http://web.simmons.edu/~braun/467/part_1.pdf&lt;/ref&gt; and Google has an announcement on its site showing the meta tags that its search engine understands.&lt;ref&gt;{{cite web|url=https://support.google.com/webmasters/answer/79812?hl=en/ |title=Meta tags that Google understands |publisher=Google.com |accessdate=2014-05-22}}&lt;/ref&gt; Enterprise search startup [[Swiftype]] recognizes metadata as a relevance signal that webmasters can implement for their website-specific search engine, even releasing their own extension, known as Meta Tags 2.&lt;ref&gt;{{Cite web|url = https://swiftype.com/documentation/meta_tags2|title = Swiftype-specific Meta Tags |work=Swiftype Documentation |publisher=Swiftype |date = 3 October 2014 }}&lt;/ref&gt;

=== In broadcast industry ===
In [[broadcast]] industry, metadata is linked to audio and video [[broadcast media]] to:
* ''identify'' the media: [[Media clip|clip]] or [[playlist]] names, duration, [[timecode]], etc.
* ''describe'' the content: notes regarding the quality of video content, rating, description (for example, during a sport event, [[Index term|keywords]] like ''goal'', ''red card'' will be associated to some clips)
* ''classify'' media: metadata allows to sort the media or to easily and quickly find a video content (a [[TV news]] could urgently need some [[archiving|archive content]] for a subject). For example, the BBC have a large subject classification system, [[Lonclass]], a customized version of the more general-purpose [[Universal Decimal Classification]].

This metadata can be linked to the video media thanks to the [[Video server#Broadcast automation|video servers]]. Most major broadcast sport events like [[FIFA World Cup]] or the [[Olympic Games]] use this metadata to distribute their video content to [[TV station]]s through [[Index term|keywords]]. It is often the host broadcaster&lt;ref&gt;{{cite web|url=http://www.hbs.tv/hostbroadcasting/ |title=HBS is the FIFA host broadcaster |publisher=Hbs.tv |date=2011-08-06 |accessdate=2011-12-23}}&lt;/ref&gt; who is in charge of organizing metadata through its ''International Broadcast Centre'' and its video servers. This metadata is recorded with the images and are entered by metadata operators (''loggers'') who associate in live metadata available in ''metadata grids'' through [[software]] (such as [[Multicam(LSM)]] or [[IPDirector]] used during the FIFA World Cup or Olympic Games).&lt;ref&gt;{{cite web|url=http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |title=Host Broadcast Media Server and Related Applications |format=PDF |accessdate=2013-08-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20111102235256/http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |archivedate=2 November 2011 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://broadcastengineering.com/worldcup/fifa-world-cup-techonlogy-0610/ |title=logs during sport events |publisher=Broadcastengineering.com |accessdate=2011-12-23}}&lt;/ref&gt;

=== Geospatial ===
Metadata that describes geographic objects in electronic storage or format (such as datasets, maps, features, or documents with a geospatial component) has a history dating back to at least 1994 (refer [http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Library page on FGDC Metadata]). This class of metadata is described more fully on the [[geospatial metadata]] article.

=== Ecological and environmental ===
Ecological and environmental metadata is intended to document the "who, what, when, where, why, and how" of data collection for a particular study. This typically means which organization or institution collected the data, what type of data, which date(s) the data was collected, the rationale for the data collection, and the methodology used for the data collection. Metadata should be generated in a format commonly used by the most relevant science community, such as [[Darwin Core]], [[Ecological Metadata Language]],&lt;ref&gt;[http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html ] {{webarchive |url=https://web.archive.org/web/20110423161141/http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html |date=23 April 2011 }}&lt;/ref&gt; or [[Dublin Core]]. Metadata editing tools exist to facilitate metadata generation (e.g. Metavist,&lt;ref&gt;{{cite web|url=http://metavist.djames.net/ |title=Metavist 2 |publisher=Metavist.djames.net |accessdate=2011-12-23}}&lt;/ref&gt; [[Mercury: Metadata Search System]], Morpho&lt;ref&gt;{{cite web|url=http://knb.ecoinformatics.org/morphoportal.jsp |title=KNB Data :: Morpho |publisher=Knb.ecoinformatics.org |date=2009-05-20 |accessdate=2011-12-23}}&lt;/ref&gt;). Metadata should describe [[data provenance|provenance]] of the data (where they originated, as well as any transformations the data underwent) and how to give credit for (cite) the data products.

=== Digital music ===
When first released in 1982, Compact Discs only contained a Table Of Contents (TOC) with the number of tracks on the disc and their length in samples.[http://s3.amazonaws.com/academia.edu.documents/32801641/Morris_2012_-_Making_Music_Behave.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1477195681&amp;Signature=2TLmhapcR0M5eYsfMQ8FgG2TZa0%3D&amp;response-content-disposition=inline%3B%20filename%3DMaking_music_behave_Metadata_and_the_dig.pdf][https://books.google.com/books?id=GkIaGZ0HWcMC&amp;pg=PA48&amp;source=gbs_toc_r&amp;cad=4#v=onepage&amp;q&amp;f=false] Fourteen years later in 1996, a revision of the [[Compact Disc Digital Audio|CD Red Book]] standard added [[CD-Text]] to carry additional metadata.[http://web.ncf.ca/aa571/cdtext.htm] But CD-Text was not widely adopted. Shortly thereafter, it became common for personal computers to retrieve metadata from external sources (e.g. [[CDDB]], [[Gracenote]]) based on the TOC.

Digital [[Sound recording and reproduction|audio]] formats such as [[digital audio file]]s superseded music formats such as [[cassette tape]]s and [[CDs]] in the 2000s. Digital audio files could be labelled with more information than could be contained in just the file name. That descriptive information is called the '''audio tag''' or audio metadata in general. Computer programs specializing in adding or modifying this information are called [[tag editor]]s. Metadata can be used to name, describe, catalogue and indicate ownership or copyright for a digital audio file, and its presence makes it much easier to locate a specific audio file within a group, typically through use of a search engine that accesses the metadata. As different digital audio formats were developed, attempts were made to standardize a specific location within the digital files where this information could be stored.

As a result, almost all digital audio formats, including [[mp3]], broadcast wav and [[AIFF]] files, have similar standardized locations that can be populated with metadata. The metadata for compressed and uncompressed digital music is often encoded in the [[ID3]] tag. Common editors such as [[TagLib]] support MP3, Ogg Vorbis, FLAC, MPC, Speex, WavPack TrueAudio, WAV, AIFF, MP4, and ASF file formats.

=== Cloud applications ===
With the availability of [[Cloud computing|Cloud]] applications, which include those to add metadata to content, metadata is increasingly available over the Internet.

== Administration and management ==

=== Storage ===
Metadata can be stored either ''internally'',&lt;ref name=id3&gt;{{cite web
| first=Dan |last=O'Neill
| url=http://id3.org
| title=ID3.org
}}&lt;/ref&gt; in the same file or structure as the data (this is also called ''embedded metadata''), or ''externally'', in a separate file or field from the described data. A data repository typically stores the metadata ''detached'' from the data, but can be designed to support embedded metadata approaches. Each option has advantages and disadvantages:
* Internal storage means metadata always travels as part of the data they describe; thus, metadata is always available with the data, and can be manipulated locally. This method creates redundancy (precluding normalization), and does not allow managing all of a system's metadata in one place. It arguably increases consistency, since the metadata is readily changed whenever the data is changed.
* External storage allows collocating metadata for all the contents, for example in a database, for more efficient searching and management. Redundancy can be avoided by normalizing the metadata's organization. In this approach, metadata can be united with the content when information is transferred, for example in [[Streaming media]]; or can be referenced (for example, as a web link) from the transferred content. On the down side, the division of the metadata from the data content, especially in standalone files that refer to their source metadata elsewhere, increases the opportunities for misalignments between the two, as changes to either may not be reflected in the other.

Metadata can be stored in either human-readable or binary form. Storing metadata in a human-readable format such as [[XML]] can be useful because users can understand and edit it without specialized tools.&lt;ref name=Sutter&gt;{{Cite book
|first1=Robbie
|last1=De Sutter
|first2=Stijn
|last2=Notebaert
|first3=Rik
|last3=Van de Walle
|chapter=Evaluation of Metadata Standards in the Context of Digital Audio-Visual Libraries
|title=Research and Advanced Technology for Digital Libraries: 10th European Conference, EDCL 2006
|editor1-last=Gonzalo
|editor1-first=Julio
|editor2-last=Thanos
|editor2-first=Constantino
|editor3-last=Verdejo
|editor3-first=M. Felisa
|editor4-last=Carrasco
|editor4-first=Rafael
|date=September 2006
|url =https://books.google.com/books?id=kU7Lqqowp54C&amp;pg=PA226&amp;cad=4#v=onepage
|isbn= 978-3540446361
|publisher=Springer
|page=226
}}&lt;/ref&gt; However, text-based formats are rarely optimized for storage capacity, communication time, or processing speed. A binary metadata format enables efficiency in all these respects, but requires special software to convert the binary information into human-readable content.

=== Database management ===
Each [[relational database]] system has its own mechanisms for storing metadata. Examples of relational-database metadata include:
* Tables of all tables in a database, their names, sizes, and number of rows in each table.
* Tables of columns in each database, what tables they are used in, and the type of data stored in each column.
In database terminology, this set of metadata is referred to as the [[database catalog|catalog]]. The [[SQL]] standard specifies a uniform means to access the catalog, called the [[information schema]], but not all databases implement it, even if they implement other aspects of the SQL standard. For an example of database-specific metadata access methods, see [[Oracle metadata]]. Programmatic access to metadata is possible using APIs such as [[JDBC]], or SchemaCrawler.&lt;ref name=schemacrawler&gt;{{cite web
| author=Sualeh Fatehi
| url=http://schemacrawler.sourceforge.net/
| title=SchemaCrawler
| work=SourceForge
}}&lt;/ref&gt;

== See also ==
{{Div col||25em}}
* [[Agris: International Information System for the Agricultural Sciences and Technology]]
* [[Classification scheme]]
* [[Crosswalk (metadata)]]
* [[DataONE]]
* [[Data Dictionary]] (aka metadata repository)
* [[Dublin Core]]
* [[Folksonomy]]
* [[GEOMS – Generic Earth Observation Metadata Standard]]
* [[Geospatial metadata]]
* [[IPDirector]]
* [[ISO/IEC 11179]]
* [[Knowledge tag]]
* [[Mercury: Metadata Search System]]
* [[Meta element]]
* [[IF-MAP|Metadata Access Point Interface]]
* [[Metadata discovery]]
* [[Metadata facility for Java]]
* [[v:4-b: Metadata|Metadata from Wikiversity]]
* [[Metadata publishing]]
* [[Metadata registry]]
* [[Metamathematics]]
* [[METAFOR]] Common Metadata for Climate Modelling Digital Repositories
* [[Microcontent]]
* [[Microformat]]
* [[Multicam (LSM)]]
* [[Observations and Measurements]]
* [[Ontology (computer science)]]
* [[Official statistics]]
* [[Paratext]]
* [[Preservation Metadata]]
* [[SDMX]]
* [[Semantic Web]]
* [[SGML]]
* [[The Metadata Company]]
* [[Universal Data Element Framework]]
* [[Vocabulary OneSource]]
* [[XSD]]
{{Div col end}}

== References ==
{{Reflist|colwidth=30em}}

== External links ==
{{Wiktionary|metadata}}
* [http://www.niso.org/apps/group_public/download.php/17446/Understanding%20Metadata ''Understanding Metadata: What is metadata, and what is it for?''] — [[NISO]], 2017
* [http://web.archive.org/web/20140522165110/http://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=1111111 "A Guardian guide to your metadata"] — ''[[The Guardian]]'', Wednesday 12 June 2013.
* [http://www.well.com/~doctorow/metacrap.htm Metacrap: Putting the torch to seven straw-men of the meta-utopia] — [[Cory Doctorow]]'s opinion on the limitations of metadata on the [[Internet]], 2001
* [http://www.dataone.org DataONE] Investigator Toolkit
* [http://www.informaworld.com/openurl?genre=journal&amp;issn=1938-6389 ''Journal of Library Metadata''], Routledge, Taylor &amp; Francis Group, ISSN 1937-5034
* [http://www.inderscience.com/ijmso ''International Journal of Metadata, Semantics and Ontologies'' (''IJMSO'')], Inderscience Publishers, ISSN 1744-263X
* {{webarchive |url=https://web.archive.org/web/20130126101115/http://www.metalounge.org/_literature_52579/Stephen_Machin_%E2%80%93_ON_METADATA_AND_METACONTENT |date=26 January 2013 |title=Metadata and metacontent }} (PDF, archived version)

{{Software engineering}}
{{Data warehouse}}

{{Authority control}}

[[Category:Data management]]
[[Category:Records management]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata| ]]
[[Category:Technical communication]]
[[Category:Business intelligence]]</text>
      <sha1>sv43xmucbh0lsc816sfa0x3zk1v6oey</sha1>
    </revision>
  </page>
  <page>
    <title>Critical data studies</title>
    <ns>0</ns>
    <id>51578025</id>
    <revision>
      <id>755904168</id>
      <parentid>755904057</parentid>
      <timestamp>2016-12-20T21:05:31Z</timestamp>
      <contributor>
        <username>Slashme</username>
        <id>451287</id>
      </contributor>
      <comment>Added {{[[Template:notability|notability]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5594" xml:space="preserve">{{notability|date=December 2016}}
{{Orphan|date=December 2016}}

'''Critical data studies''' is the systematic study of data and its criticisms.&lt;ref&gt;Dalton, Craig, and Jim Thatcher, 2014.&lt;/ref&gt; The field was named by scholars [[Craig Dalton]] and [[Jim Thatcher]] in their 2015 article titled "What does a critical data studies look like, and why do we care?" Interest has developed in this domain as a response to the emergence and reliance on '[[big data]]' in contemporary society.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Some of the other key scholars in this discipline include [[Rob Kitchen]] and [[Tracey P. Lauriault]].&lt;ref&gt;Kitchin, Rob, and Tracey P. Lauriault, 2014&lt;/ref&gt;&lt;ref&gt;Kitchin, Rob, 2014&lt;/ref&gt; Scholars have attempted to make sense of data through different theoretical frameworks, some of which include analyzing data technically, ethically, politically/economically, temporally/spatially, and philosophically.&lt;ref&gt;Kitchin, Rob, 2014&lt;/ref&gt; Some of the key academic journals related to critical data studies include the ''[[Journal of Big Data]]'' and ''[[Big Data and Society]]''.

==Why is a critical approach to data needed?==

In their article in which they coin the term 'critical data studies,' Dalton and Thatcher also provide several justifications as to why data studies is a discipline worthy of a critical approach.&lt;ref&gt;Dalton, Craig, and Jim Thatcher, 2014.&lt;/ref&gt; Firstly, 'big data' is an important aspect of twenty-first century society, and the analysis of 'big data' allows for a deeper understanding of what is happening and for what reasons.&lt;ref&gt;Dalton, Craig, and Jim Thatcher, 2014.&lt;/ref&gt; Furthermore, big data as a technological tool and the information that it yields are not neutral, according to Dalton and Thatcher,&lt;ref&gt;''Ibid.''&lt;/ref&gt; making it worthy of critical analysis in order to identify and address its biases. Building off this idea, another justification for a critical approach is that the relationship between big data and society is an important one, and therefore worthy of study.&lt;ref&gt;''Ibid.''&lt;/ref&gt;  Dalton and Thatcher stress how the relationship is not an example of [[technological determinism]], but rather how big data can shape the lives of individuals. Big data technology can cause significant changes in society's structure and in the everyday lives of people,&lt;ref&gt;''Ibid.''&lt;/ref&gt; and being a product of society, big data technology is worthy of sociological investigation.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Moreover, data sets are almost never completely raw, that is to say without any influences. Dalton and Thatcher describe how data are shaped by the vision or goals of a research team, and during the data collection process, certain things are quantified, stored, sorted and even discarded by the research team.&lt;ref&gt;''Ibid.''&lt;/ref&gt; A critical approach is thus necessary in order to understand and reveal the intent behind the information being presented. Dalton and Thatcher also argue how data alone cannot speak for itself; in order to possess any concrete meaning, data must be accompanied by theoretical insight or be accompanied by alternative quantitative or qualitative research measures.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Dalton and Thatcher argue that if one were to only think of data in terms of its exploitative power, there is no possibility of using data for revolutionary, liberatory purposes.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Finally, Dalton and Thatcher propose that a critical approach in studying data allows for 'big data' to be combined with older, 'small data,' and thus create more thorough research, opening up more opportunities, questions and topics to be explored.&lt;ref&gt;''Ibid.''&lt;/ref&gt;

==Issues and Concerns for Critical Data Scholars==

The use of data in modern society brings about new ways of understanding and measuring the world, but also brings with it certain concerns or issues.&lt;ref&gt;Kitchin, Rob, 2014&lt;/ref&gt; Data scholars attempt to bring some of these issues to light in their quest to be critical of data. Rob Kitchin identifies both technical and organizational issues of data, as well as some normative and ethical questions.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Technical and organization issues concerning data range from the scope of datasets, access to the data, the quality of the data, the integration of the data, the application of analytics and ecological fallacies, as well as the skills and organizational capabilities of the research team.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Some of the normative and ethical concerns addressed by Kitchin include surveillance through one's data (dataveillance), the privacy of one's data, the ownership of one's data, the security of one's data, anticipatory or corporate governance, and finally profiling individuals by their data.&lt;ref&gt;''Ibid.''&lt;/ref&gt; All of these concerns must be taken into account by scholars of data in their objective to be critical.

==References==
{{reflist|24em}}

==Sources==
* Dalton, Craig, and Jim Thatcher. "What does a critical data studies look like, and why do we care? Seven points for a critical approach to ‘big data’." ''Society and Space open site'' (2014). Retrieved October 23, 2016.
* Elkins, James R. "The Critical Thinking Movement: Alternating Currents in One Teacher's Thinking". ''myweb.wvnet.edu''(1999). Retrieved 29 November 2016.
* Kitchin, Rob. ''The data revolution: Big data, open data, data infrastructures and their consequences.'' Sage, 2014. Retrieved October 23, 2016.
* Kitchin, Rob, and Tracey P. Lauriault. "Towards critical data studies: Charting and unpacking data assemblages and their work." (2014). Retrieved October 23, 2016.

[[Category:Data management]]</text>
      <sha1>mna8keu8k6j7g0k2ctyoetabq4zxq13</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Statistical data agreements</title>
    <ns>14</ns>
    <id>24105895</id>
    <revision>
      <id>753884296</id>
      <parentid>720923276</parentid>
      <timestamp>2016-12-09T18:41:46Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="93" xml:space="preserve">[[Category:Statistical data|Agreements]]
[[Category:Agreements]]
[[Category:Data management]]</text>
      <sha1>fehtp2uw6tomdlwa1epm5pqevh40c1p</sha1>
    </revision>
  </page>
  <page>
    <title>Data storage device</title>
    <ns>0</ns>
    <id>28174</id>
    <revision>
      <id>762116593</id>
      <parentid>762116376</parentid>
      <timestamp>2017-01-26T19:23:54Z</timestamp>
      <contributor>
        <ip>86.153.3.18</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6307" xml:space="preserve">[[File:PersonalStorageDevices.agr.jpg|thumb|Many different consumer electronic devices can store data.]]
[[File:EdisonPhonograph.jpg|thumb|Edison cylinder phonograph ca. 1899. The phonograph cylinder is a storage medium. The phonograph may be considered a storage device.]]
[[File:Reel-to-reel recorder tc-630.jpg|thumb|On a reel-to-reel tape recorder (Sony TC-630), the recorder is data storage equipment and the magnetic tape is a data storage medium.]]
[[File:RNA-comparedto-DNA thymineAndUracilCorrected.png|thumb|upright|[[RNA]] might be the oldest [[data]] storage medium.&lt;ref&gt;{{cite journal|title=The RNA World|journal=[[Nature (journal)|Nature]]|first=Walter|last=Gilbert|authorlink=Walter Gilbert|date=Feb 1986|pages=618|volume=319|doi=10.1038/319618a0|issue=6055|bibcode=1986Natur.319..618G}}&lt;/ref&gt;]]

A '''data storage device''' is a device for [[recording]] (storing) [[information]] (data). Recording can be done using virtually any form of [[energy]], spanning from manual muscle power in [[handwriting]], to acoustic vibrations in [[phonograph]]ic recording, to electromagnetic energy modulating [[magnetic tape]] and [[optical disc]]s.

A storage device may hold information, process information, or both. A device that only holds information is a recording [[Medium (communication)|medium]]. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.

Electronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require [[Visual perception|vision]] and a brain to read data fall into this category.  Electromagnetic data may be stored in either an  analog [[data]] or [[digital data]] format on a variety of media. This type of data is considered to be [[Machine-readable medium|electronically encoded]] data, whether it is electronically stored in a  [[semiconductor]] [[Computer data storage|device]], for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of [[computer data storage]]) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) [[microcircuit]]s are [[volatile memory]], for it vanishes if power is removed.

Except for [[barcode]]s, [[optical character recognition]] (OCR), and [[magnetic ink character recognition]] (MICR) data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium.&lt;ref&gt;{{Cite web|url=https://www.seas.gwu.edu/~shmuel/WORK/Differences/Chapter%203%20-%20Sources.pdf|title=The Difference between Electronic and Paper Documents|last=Rotenstreich|first=Shmuel|website=Seas.GWU.edu|publisher=The George Washington University|access-date=12 April 2016}}&lt;/ref&gt;

==Global capacity, digitization, and trends==
In a recent study in [[Science (journal)|''Science'']] it was estimated that the world's technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) [[exabyte]]s in 1986, to 295 (optimally compressed) [[exabyte]]s in 2007,&lt;ref name="HilbertLopez2011"&gt;{{cite journal | last1 = Hilbert | first1 = Martin | last2 = López | first2 = Priscila | year = 2011 | title = The World's Technological Capacity to Store, Communicate, and Compute Information | journal = [[Science (journal)|Science]] | volume = 332 | issue = 6025| pages = 60–65 | doi=10.1126/science.1200970 | pmid=21310967}}; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html&lt;/ref&gt; and doubles roughly every three years.&lt;ref name="Hilbertvideo2011"&gt;[http://ideas.economist.com/video/giant-sifting-sound-0 "video animation on The World’s Technological Capacity to Store, Communicate, and Compute Information from 1986 to 2010]&lt;/ref&gt;

It is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information digitally than on analog storage devices.&lt;ref name="HilbertLopez2011" /&gt;

==See also==
{{colbegin||22em}}
* [[Archival science]]
* [[Blank media tax]]
* [[Computer data storage]]
* [[Content format]]
* [[Data transmission]]
* [[Digital Data Storage|Digital Data Storage (DDS)]]
* [[Digital preservation]]
* [[Disk drive performance characteristics]]
* [[Format war]]
* [[Flip-flop (electronics)]]
* [[IOPS]]
* [[Library]]
* [[Media controls]]
* [[Medium format (film)]]
* [[Memristor]]
* [[Nanodot]]
* [[Nonlinear medium]] ([[random access]])
* [[Recording format]]
* [[Semiconductor memory]]
* [[Telecommunication]]
{{colend}}

==References==
{{Reflist|30em}}

==Further reading==
* {{cite journal |last = Bennett |first=John C. | title = 'JISC/NPO Studies on the Preservation of Electronic Materials: A Framework of Data Types and Formats, and Issues Affecting the Long Term Preservation of Digital Material | publisher = British Library Research and Innovation Report 50 | year = 1997 | url = http://www.ukoln.ac.uk/services/papers/bl/jisc-npo50/bennet.html }}
* [http://www.zetta.net/history-of-computer-storage/ History of Computer Storage from 1928 to 2013]
* [http://www.remosoftware.com/info/history-of-storage-from-cave-paintings-to-electrons/ History of Storage from Cave Paintings to Electrons]
* [[Plant-based digital data storage]]

==External links==
* [http://ns1758.ca/winch/winchest.html Historical Notes about the Cost of Hard Drive Storage Sp]
* @[http://ns1758.ca/winch/winchest.html ace]
* [https://www.securedatarecovery.com/infographics/the-evolution-of-data-storage The Evolution of Data Storage]

{{Magnetic storage media}}
{{Optical storage media}}
{{Paper data storage media}}
{{Primary storage technologies}}

{{Authority control}}

[[Category:Computer storage devices]]
[[Category:Data management]]
[[Category:Film and video technology]]
[[Category:Media technology]]
[[Category:Recording]]
[[Category:Sound production technology]]
[[Category:Storage media]]</text>
      <sha1>dcw3ob9igccpbqcytaa4jbtadgix2ll</sha1>
    </revision>
  </page>
  <page>
    <title>Data profiling</title>
    <ns>0</ns>
    <id>794330</id>
    <revision>
      <id>761897546</id>
      <parentid>761897543</parentid>
      <timestamp>2017-01-25T13:12:33Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/Musicbydma|Musicbydma]] to version by Widr. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2909915) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7321" xml:space="preserve">{{multiple issues|
{{Expert needed|Mathematics|ex2=Science|talk=Section title goes here|reason=it needs additional citations|date=August 2016}}
{{refimprove article|date=August 2010}}
{{copy edit|for=Incorrect citation formatting|date=August 2016}}
}}

'''Data profiling''' is the process of examining data available from an existing information source (e.g. a [[database]] or a [[computer file|file]]) and collecting [[descriptive statistics|statistics]] or informative summaries about that data.&lt;ref name="Johnson2009"&gt;[Theodore Johnson (2009), "Data Profiling", in Encyclopedia of Database Systems, Springer, Heidelberg]&lt;/ref&gt; The purpose of these statistics may be to:
# Find out whether existing data can easily be used for other purposes
# Improve the ability to search data by [[tag (metadata)|tagging]] it with [[Index term|keywords]], descriptions, or assigning it to a category
# Give [[Software metric|metrics]] on [[data quality]], including whether the data conforms to particular standards or patterns
# Assess the risk involved in [[data integration|integrating data]] in new applications, including the challenges of [[Join (SQL)|join]]s
# Discover [[metadata]] of the source database, including value patterns and [[frequency distribution|distributions]], [[candidate key|key candidates]], [[inclusion dependency|foreign-key candidates]], and [[functional dependency|functional dependencies]]
# Assess whether known metadata accurately describes the actual values in the source database
# Understanding data challenges early in any data intensive project, so that late project surprises are avoided. Finding data problems late in the project can lead to delays and cost overruns.
# Have an enterprise view of all data, for uses such as [[master data management]], where key data is needed, or [[data governance]] for improving data quality.

== Introduction ==

Data profiling refers to the analysis of information for use in a [[data warehouse]] in order to clarify the structure, content, relationships, and derivation rules of the data.&lt;ref name="Kimball2008"&gt;[Ralph Kimball et al. (2008), “The Data Warehouse Lifecycle Toolkit”, Second Edition, Wiley Publishing, Inc., ISBN 9780470149775], (p. 297) (p. 376)&lt;/ref&gt; Profiling helps to not only understand anomalies and assess data quality, but also to discover, register, and assess enterprise metadata.&lt;ref name="Loshin2009"&gt;[David Loshin (2009), “Master Data Management”, Morgan Kaufmann Publishers, ISBN 9780123742254], (pp. 94–96)&lt;/ref&gt;&lt;ref name="Loshin2003"&gt;[David Loshin (2003), “Business Intelligence: The Savvy Manager’s Guide, Getting Onboard with Emerging IT”, Morgan Kaufmann Publishers, ISBN 9781558609167], (pp. 110–111)]&lt;/ref&gt; The result of the analysis is used to determine the suitability of the candidate source systems, usually giving the basis for an early go/no-go decision, and also to identify problems for later solution design.&lt;ref name="Kimball2008"/&gt;

== How Data Profiling is Conducted ==

Data profiling utilizes methods of descriptive statistics such as minimum, maximum, mean, mode, percentile, standard deviation, frequency, variation, aggregates such as count and sum, and additional metadata information obtained during data profiling such as data type, length, discrete values, uniqueness, occurrence of null values, typical string patterns, and abstract type recognition.&lt;ref name="Loshin2009"/&gt;&lt;ref name="Rahm2000"&gt;[Erhard Rahm and Hong Hai Do (2000), “Data Cleaning: Problems and Current Approaches” in “Bulletin of the Technical Committee on Data Engineering”, IEEE Computer Society, Vol. 23, No. 4, December 2000]&lt;/ref&gt;&lt;ref name="Singh2010"&gt;[Ranjit Singh, Dr Kawaljeet Singh et al. (2010), “A Descriptive Classification of Causes of Data Quality Problems in Data Warehousing”, IJCSI International Journal of Computer Science Issue, Vol. 7, Issue 3, No. 2, May 2010]&lt;/ref&gt;
The metadata can then be used to discover problems such as illegal values, misspellings, missing values, varying value representation, and duplicates.

Different analyses are performed for different structural levels. E.g. single columns could be profiled individually to get an understanding of frequency distribution of different values, type, and use of each column. Embedded value dependencies can be exposed in a cross-columns analysis. Finally, overlapping value sets possibly representing foreign key relationships between entities can be explored in an inter-table analysis.&lt;ref name="Loshin2009"/&gt;

Normally, purpose-built tools are used for data profiling to ease the process.&lt;ref name="Kimball2008"/&gt;&lt;ref name="Loshin2009"/&gt;&lt;ref name="Rahm2000"/&gt;&lt;ref name="Singh2010"/&gt;&lt;ref name="Kimball2004"&gt;"[Ralph Kimball (2004), “Kimball Design Tip #59: Surprising Value of Data Profiling”, Kimball Group, Number 59, September 14, 2004, (www.rkimball.com/html/designtipsPDF/ KimballDT59 SurprisingValue.pdf)]&lt;/ref&gt;&lt;ref name="Olson2003"&gt;[Jack E. Olson (2003), “Data Quality: The Accuracy dimension”, Morgan Kaufmann Publishers], (pp. 140–142)&lt;/ref&gt; The computation complexity increases when going from single column, to single table, to cross-table structural profiling. Therefore, performance is an evaluation criterion for profiling tools.&lt;ref name="Loshin2003"/&gt;

== When Data Profiling is Conducted ==

According to Kimball,&lt;ref name="Kimball2008"/&gt; data profiling is performed several times and with varying intensity throughout the data warehouse developing process. A light profiling assessment should be undertaken immediately after candidate source systems have been identified and DW/BI business requirements have been satisfied. The purpose of this initial analysis is to clarify at an early stage if the correct data is available at the appropriate detail level and that anomalies can be handled subsequently. If this is not the case the project may be terminated.&lt;ref name="Kimball2008"/&gt;

Addition, more in-depth profiling is done prior to the dimensional modeling process in order assess what is required to convert data into a dimensional model. Detailed profiling extends into the ETL system design process in order to determine the appropriate data to extract and which filters to apply to the data set.&lt;ref name="Kimball2008"/&gt;

Additionally, data may be conducted in the data warehouse development process after data has been loaded into staging, the data marts, etc. Conducting data at these stages helps ensure that data cleaning and transformations have been done correctly and in compliance of requirements.

==Benefits==

The benefits of data profiling are to improve data quality, shorten the implementation cycle of major projects, and improve users' understanding of data.&lt;ref name="Olson2003"/&gt; Discovering business knowledge embedded in data itself is one of the significant benefits derived from data profiling.&lt;ref name="Loshin2003"/&gt; Data profiling is one of the most effective technologies for improving data accuracy in corporate databases.&lt;ref name="Olson2003"/&gt;

==See also==
* [[Data quality]]
* [[Data governance]]
* [[Master data management]]
* [[Database normalization]]
* [[Data visualization]]
* [[Analysis paralysis]]

==References==
{{Reflist}}

{{DEFAULTSORT:Data Profiling}}
[[Category:Data management]]
[[Category:Data quality]]
[[Category:Data analysis]]</text>
      <sha1>oldd1ucprzuvn79vz2a4o24vilevw3o</sha1>
    </revision>
  </page>
  <page>
    <title>DataverseNL</title>
    <ns>0</ns>
    <id>52809243</id>
    <revision>
      <id>760750767</id>
      <parentid>760750633</parentid>
      <timestamp>2017-01-18T21:36:09Z</timestamp>
      <contributor>
        <username>Vtyiisg</username>
        <id>18408392</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3893" xml:space="preserve">'''DataverseNL''' is data management service built on top of [[Dataverse]] data repository and jointly offered by participating institutions to the research community of the Netherlands. It's developed by the Dataverse Team at the Institute for Quantitative Social Science ([[IQSS]]) at [[Harvard University]] and [[Data Archiving and Networked Services]] (DANS) and housed and maintained by DANS team.

With DataverseNL, researchers and lecturers can store, share and register research data online, both during research and for up to ten years afterwards. Dataverse platform is used worldwide. In the Netherlands, DataverseNL was installed at the Universiteit Utrecht in 2010, after which it developed into a shared service of over other institutions. The data management is in the hands of the institutions; DANS has been managing the network since May 2014.

As of January 2017, DataverseNL offers access to more than 300 published studies.

== Persistent identifier ==
At DataverseNL, each deposited dataset receives its own persistent identifier that allows information to remain accessible in the long term, even if its location changes. [[DANS]] has developed this functionality for Dataverse repository to use handle system for its persistent identifiers.

== Open access ==
Besides storing data, DataverseNL allows you to share them with other scientists. Researchers themselves determine who gets access to which materials and what their access rights are (user, contributor or curator). Researchers may choose any license for their data, including [[CC0]] (CC Zero Waiver) or [[ODBL]] (Open Database License).

== How to use DataverseNL ==
'''Depositing data'''&lt;br /&gt;
'''1''' Check to see if your university or institution is participating in DataverseNL.&lt;br /&gt;
'''2''' Prepare your data: select the relevant data files and check for any privacy-sensitive aspects. &lt;br /&gt;
'''3''' Log in at https://dataverse.nl. New users must first sign up for an account.&lt;br /&gt;
'''4''' Upload your data to studies within your dataverse.&lt;br /&gt;
'''5''' Describe your data and determine who gets access. &lt;br /&gt;
'''6''' Share the data by publishing them or allowing others access to your dataverse or studies.&lt;br /&gt;

'''Downloading data'''&lt;br /&gt;
'''1''' Search or browse, and if necessary refine your search results, until you have found the dataset you are looking for at https://dataverse.nl. &lt;br /&gt;
'''2''' Look at the metadata to determine if the dataset meets your requirements. &lt;br /&gt;
'''3''' Depending on the access category, you can go to the Data &amp; Analysis tab and download the data. &lt;br /&gt;
'''4''' Check the ‘Data citation’ header to see the correct method for citing the data. &lt;br /&gt;

== Participating institutions ==
DataverseNL is a shared service provided by the participating institutions and DANS. DANS performs back office tasks, including server and software maintenance and administrative support.
The participating institutions are responsible for managing the deposited data.

At the moment there are following participating institutions:
* [[4TU]] Data lab
*[[ 4TU.Centre for Research Data]]
* [[Tilburg University]]
* [[TiU]]
* [[Universiteit Utrecht]]
* [[Vrije Universiteit Amsterdam]]
* [[Maastricht University]]
* [[Protestantse Theologische Universiteit]]
* [[Avans Hogeschool]]
* [[Nederlands Instituut voor Ecologie]]
* [[Rijksuniversiteit Groningen]]
* [[Erasmus University Rotterdam]]
* [[Hogeschool Windesheim]]

==External links==
*[https://www.dataverse.nl The DataverseNL Repository (Netherlands)]
*[https://www.dans.knaw.nl Data Archiving and Networked Services]
*[http://dataverse.org/ The Dataverse Project]

[[Category:Data management]]
[[Category:Open science]]
[[Category:Open data]]
[[Category:Open-access archives]]
[[Category:Open access (publishing)]]
[[Category:Academic publishing]]
[[Category:Data publishing]]
[[Category:Scholarly communication]]</text>
      <sha1>b24iwr7uvs9bxll40ln2xyi68n3o2v5</sha1>
    </revision>
  </page>
  <page>
    <title>Compound document</title>
    <ns>0</ns>
    <id>299663</id>
    <revision>
      <id>751180579</id>
      <parentid>741456471</parentid>
      <timestamp>2016-11-23T21:49:47Z</timestamp>
      <contributor>
        <username>Adam Katz</username>
        <id>136718</id>
      </contributor>
      <comment>linked to [[Compound File Binary Format]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1742" xml:space="preserve">{{about|compound documents in general|the W3C standard|Compound Document Format}}
{{refimprove|date=November 2015}}
In [[computing]], a '''compound document''' is a document type typically produced using [[word processor|word processing]] software, and is a regular text document intermingled with non-text elements such as [[spreadsheet]]s, [[picture]]s, [[digital video]]s, [[digital audio]], and other [[multimedia]] features. It can also be used to collect several documents into one.

Compound document [[technology|technologies]] are commonly utilized on top of a [[software componentry]] framework, but the idea of software componentry includes several other concepts apart from compound documents, and software components alone do not enable compound documents. Well-known technologies for compound documents include:

*[[ActiveX Document]]s
*[[Bonobo (computing)|Bonobo]] by [[Ximian]] (primarily used by [[GNOME]])
*[[KPart]]s in [[KDE]]
*[[Multipurpose Internet Mail Extensions]]
*[[Object linking and embedding]] (OLE) by [[Microsoft]]; see [[Compound File Binary Format]]
*[[Open Document Architecture]] from [[ITU-T]] (not used)
*[[OpenDoc]] by [[Apple Computer]] (now defunct)
*[http://sourceforge.net/projects/verdantium Verdantium]
*[[XML]] and [[Extensible Stylesheet Language|XSL]] are encapsulation formats used for compound documents of all kinds

The first public implementation was on the [[Xerox Star]] [[workstation]], released in 1981.&lt;ref&gt;http://www.digibarn.com/collections/systems/xerox-8010/index.html&lt;/ref&gt;

==See also==
* [[COM Structured Storage]]
* [[Transclusion]]
* [[Electronic Notebook]]

==References==
{{Reflist}}

[[Category:Electronic documents]]
[[Category:Multimedia]]

{{Multimedia-software-stub}}</text>
      <sha1>boiohdw3h7bo221drpgrid5pm5j64rr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer storage media</title>
    <ns>14</ns>
    <id>891409</id>
    <revision>
      <id>734498872</id>
      <parentid>724823490</parentid>
      <timestamp>2016-08-14T19:27:41Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category Computer storage to [[:Category:Computer data storage]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="464" xml:space="preserve">{{Cat main|Computer storage media}}
This category refers to [[digital media]] used in [[computer storage]] devices.  Examples of such media include (a) magnetic disks, cards, tapes, and drums, (b) punched cards and paper tapes, (c) optical disks, (d) barcodes and (e) magnetic ink characters.


{{Commons cat|Computer storage media}}

[[Category:Electronic documents]]
[[Category:Storage media]]
[[Category:Digital media]]
[[Category:Computer data storage| Media]]</text>
      <sha1>g6fx64ou7zxk6algnkh8jpvt7cqvjyb</sha1>
    </revision>
  </page>
  <page>
    <title>Information capture</title>
    <ns>0</ns>
    <id>3547364</id>
    <revision>
      <id>727453358</id>
      <parentid>674118710</parentid>
      <timestamp>2016-06-29T02:45:30Z</timestamp>
      <contributor>
        <username>I dream of horses</username>
        <id>9676078</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="513" xml:space="preserve">{{refimprove|date=June 2016}}
'''Information capture''' is the process of collecting paper [[documents]], [[form (document)|form]]s and e-documents, transforming them into accurate, retrievable, [[Digital data|digital]] information, and delivering the information into business applications and [[databases]] for immediate action.&lt;ref&gt;http://www.emc.com/collateral/advertorial/aiim-advertorial.pdf&lt;/ref&gt;

==See also==
* [[Ibml]]

==References==
&lt;references/&gt;

[[Category:Electronic documents]]


{{database-stub}}</text>
      <sha1>scw4p1kmvdm36r8epblg3g8ss5i6yru</sha1>
    </revision>
  </page>
  <page>
    <title>Transaction document</title>
    <ns>0</ns>
    <id>17788286</id>
    <revision>
      <id>639807736</id>
      <parentid>586807892</parentid>
      <timestamp>2014-12-27T13:30:13Z</timestamp>
      <contributor>
        <username>Eumolpo</username>
        <id>7953109</id>
      </contributor>
      <minor />
      <comment>orthographic</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1257" xml:space="preserve">'''Transaction documents'''  refers to legally relevant [[documents]] that are either printed, inserted and mailed, or electronically presented.&lt;ref&gt;[http://www.outputlinks.com/html/news/acadami_xplor_best_practices_progam_canada_043008.shtml Transaction documents]&lt;blockquote&gt;"...The course focuses on the concepts, technologies, and best practices associated with automated transaction document production."&lt;/blockquote&gt;&lt;/ref&gt; They consist of a mixture of fixed and variable data. 

These [[documents]] are usually created by organizations through their financial computing system and then delivered to other parties (such as clients) through the [[post office]] or through an [[electronic billing]] system. The printed transaction documents, once delivered to the [[post office]], conform to the [[mail box rule]]. 

Common examples of transaction documents are:
* bills
* [[bank statements]] (and credit card, financial services, etc.)
* insurance policies
* notices
* other legally relevant correspondence, etc.

[[Xplor international]] is a technical association that focuses on the best practices and technologies associated with these documents.

==References==
{{Reflist}}

[[Category:Electronic documents]]
[[Category:Contract law]]


{{law-stub}}</text>
      <sha1>sklsp5tlg6vr1kll4elchztrhex4wm6</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Office software</title>
    <ns>14</ns>
    <id>21137368</id>
    <revision>
      <id>548884230</id>
      <parentid>545545650</parentid>
      <timestamp>2013-04-05T19:40:17Z</timestamp>
      <contributor>
        <username>Chobot</username>
        <id>259798</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q6158391|]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="124" xml:space="preserve">{{Commons category|Office suites}}
[[Category:Office work]]
[[Category:Business software]]
[[Category:Electronic documents]]</text>
      <sha1>ddsjci9h6yme1tj9rni7od0bf3g0tls</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic document</title>
    <ns>0</ns>
    <id>430436</id>
    <revision>
      <id>729641538</id>
      <parentid>721262317</parentid>
      <timestamp>2016-07-13T16:13:54Z</timestamp>
      <contributor>
        <username>Drchriswilliams</username>
        <id>19349497</id>
      </contributor>
      <comment>improvement template added</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2331" xml:space="preserve">{{Unreferenced|date=July 2016}}
[[File:Text-txt.svg|thumb|An example of a [[text file]] icon, one of the common representations of an electronic document.]]
An '''electronic document''' is any electronic media content (other than computer programs or system files) that are intended to be used in either an electronic form or as printed output.
Originally, any computer data were considered as something internal &amp;mdash; the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. And the improvements in electronic display technologies mean that in most cases it is possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).

However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem &amp;mdash; e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.

Even more problems are connected with complex file formats of various [[word processor]]s, [[spreadsheet]]s and [[graphics software]]. To alleviate the problem, many software companies distribute free [[file viewer]]s for their proprietary file formats (one example is [[Adobe Systems|Adobe]]'s [[Portable Document Format|Acrobat Reader]]). The other solution is the development of standardized non-[[Proprietary software|proprietary]] file formats (such as [[HTML]] and [[OpenDocument]]), and electronic documents for specialized uses have specialized formats &amp;ndash; the specialized [[electronic article]]s in physics  use [[TeX]] or [[PostScript]].
{{reflist}} 
==See also==
*[[Digital era governance]]
*[[Electronic paper]]
*[[Paperless office]]
*[[Bureaucrat]]
*[[E-Government Act of 2002]]
*[[E-government]]
*[[Public administration]]

== External links ==
* [http://people.ischool.berkeley.edu/~buckland/digdoc.html What is a digital document]
* [http://www.msimaging.com/faq Digital Imaging Frequent Questions]

[[Category:Electronic documents]]
[[Category:Word processors]]</text>
      <sha1>j4y4r4z2pqczwji8tl2lplodfxx1mjq</sha1>
    </revision>
  </page>
  <page>
    <title>Email management</title>
    <ns>0</ns>
    <id>21888954</id>
    <revision>
      <id>748189400</id>
      <parentid>748044152</parentid>
      <timestamp>2016-11-06T22:00:00Z</timestamp>
      <contributor>
        <username>DMacks</username>
        <id>712163</id>
      </contributor>
      <comment>requires cite...predates all other date-identified content here Undid revision 748044152 by [[Special:Contributions/YatesByron|YatesByron]] ([[User talk:YatesByron|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7991" xml:space="preserve">'''Email management''' is a specific field of [[communications management]] for managing high volumes of inbound electronic mail received by organizations. Today, email management is an essential component of customer service management.  Customer service call centers currently employ email response management agents along with telephone support agents, and typically use software solutions to manage emails.&lt;ref&gt;"Communications Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp &lt;http://mdg.mit.edu/email-lab-interests.asp&gt;] on 15 Nov 2011&lt;/ref&gt;&lt;ref&gt;How to use e-mail to improve customer service. Inc.com, Guide E-mail Customer Service, Retrieved from web [http://www.inc.com/guides/cust_email/20909.html &lt;http://www.inc.com/guides/cust_email/20909.html &gt;] on 20 January 2012&lt;/ref&gt;

==Background==
Email management evolved from [[database management]] and [[customer relationship management]] (CRM).  Database management began in the 1960s. IBM provided one of the earliest solutions and established standards for database management.  Prominent database management platforms include Oracle, SQL Server etc.&lt;ref&gt;"Database Management - History Of Database Management." Free Encyclopedia of Ecommerce. Net Industries, n.d. Retrieved from Web. [http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html  &lt;http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html&gt;]. on 19 Dec 2011.&lt;/ref&gt;  Vern Watts, inventor of [[IBM Information Management System|IBM's Information Management System]] (IMS), and [[Larry Ellison]], founder of [[Oracle Corporation]], are pioneers in database management systems.&lt;ref&gt;Luongo, C. et al. (2008). The tale of Vern Watts. [Web Video]. Retrieved from [www.youtube.com/watch?v=x98hgieE08o  &lt;www.youtube.com/watch?v=x98hgieE08o&gt;]. on 19 December 2011&lt;/ref&gt;&lt;ref&gt;"Larry Ellison Biography." Academy of Achievement. American Academy of Achievement, 16 Feb 2010. Web. 19 Dec 2011. &lt;http://www.achievement.org/autodoc/page/ell0bio-1&gt;.&lt;/ref&gt;

As database management solutions became more sophisticated in functionality, marketing and customer service departments of large organizations started using information about customers for [[database marketing]].  Customer service managers soon realized that they could extend database marketing to store and retrieve all customer communications to improve visibility with key clients.  This led to the development of CRM systems which managed communication with customers and prospective customers using various media, including phone, direct mail, web site, and email.&lt;ref&gt;"The history of CRM -- evolving beyond the customer database." CRM Software Guide. crm-software-guide.com, n.d. Retrieved from Web. [http://www.crm-software-guide.com/history-of-crm.htm &lt;http://www.crm-software-guide.com/history-of-crm.htm&gt;]. on 19 Dec 2011.&lt;/ref&gt;  Pioneers in CRM include [[David Duffield]], creator of [[PeopleSoft]], and [[Thomas Siebel|Tom Siebel]], founder of [[Siebel Systems]].&lt;ref&gt;"PeopleSoft Inc." International Directory of Company Histories. 2000. In Retrieved Encyclopedia.com Retrieved from web [http://www.encyclopedia.com/doc/1G2-2843700094.html  &lt;http://www.encyclopedia.com/doc/1G2-2843700094.html&gt;] on 19 December 2011&lt;/ref&gt;&lt;ref&gt;Thomas Siebel 1952- Biography - Early life and education, Oracle, Siebel systems. ND. Reference for Business Encyclopedia of Business, 2nd ed. Retrieved from web [http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b &lt; http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b &gt;] on 20 January 2012&lt;/ref&gt;

As email became one of the most prevalent business-to-customer communication media in the 1990s, customer service departments needed specialized systems of tools and trained staff to manage email communication with their customers and prospective customers.

==History==
In 1994, Information Cybernetics, a company in Cambridge, Massachusetts, developed tools for pattern analysis and categorization of emails and other electronic communication channels. The platform of tools was called [[EchoMail]].  The first company to adopt  EchoMail was [[AT&amp;T]]. [[J. C. Penney|JC Penney]] adopted EchoMail in 1997.&lt;ref&gt;Callaway, Erin. "Return to Sender." PC Week Executive. 14 July 1997: 111, 114. Print.&lt;/ref&gt;&lt;ref&gt;O'Brien, J. A. (2002). Introduction to information systems. (10 ed., p. 370). McGraw-Hill Irwin. Retrieved from Web. [http://www.mcm.edu/~palafoxt/sixth.htm &lt;http://www.mcm.edu/~palafoxt/sixth.htm&gt;]. On 8 Dec 2011&lt;/ref&gt;&lt;ref&gt;"The EchoMail Digital Refinery." www.echomail.com. EchoMail, Inc., n.d. Retrieved from Web [http://echomail.com/technology-for-email-management-detailed/ &lt;http://echomail.com/technology-for-email-management-detailed/&gt;]. on 8 Dec 2011&lt;/ref&gt;

Another early company that developed email management software systems was FortÈ Internet Software, which produced Adante.&lt;ref&gt;Pavita, H. (1997, June 24). Forte introduces adante 1.0 server software for managing internet-based customer service and communications. Business Wire, Retrieved from Web [http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024 &lt;http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024&gt;] on 8 Dec 2011&lt;/ref&gt;  By late 1999, companies such as KANA Software, Inc., also emerged to support this effort.&lt;ref&gt;"Email Response System - Intelligent Message Handling :: KANA." www.kana.com. KANA Software, n.d. Retrieved from Web. [http://www.kana.com/customer-service/email-response-system.php  &lt;http://www.kana.com/customer-service/email-response-system.php&gt;]. on 8 Dec 2011&lt;/ref&gt;  Eventually, companies such as Siebel CRM Systems, Inc., incorporated components of email management into their CRM systems.&lt;ref&gt;"Bookshelf v7.5: Overview of Siebel eMail Response." docs.oracle.com. ORACLE Corporation, 21 April 2003. Retrieved from Web. [http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html  &lt;http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html&gt;]. on 8 Dec 2011&lt;/ref&gt;

==Typical system components==
An email management system consists of various components to handle different phases of the email management process.&lt;ref&gt;"EMAIL Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp &lt;http://mdg.mit.edu/email-lab-interests.asp&gt;] on 15 Nov 2011&lt;/ref&gt;  These components include: 
*Email ticketing system - One of the key tasks performed by email [[management system]]s is to allocate reference numbers to all incoming [[email]]s. This process is known as ticketing. All subsequent emails relating to one matter can then be grouped under the same reference. This allows users to track their correspondence in a more time effective and productive way.
*Email receipt module - Receives emails, filters out spam and unwanted content to a separate queue (sometimes called [[email filtering]]), and assigns unique ticket numbers based on certain conditions.
*Bayesian spam filters - Statistical technique of filtering spam that most current email management systems utilize.
*Data enhancement module - Adds tags to each email for further processing and may include the ability to connect to remote databases and retrieve specific information about the email author and his/her transactions with the organization.
*Intelligent Analysis module - Reads the subject, message, and attachments, and any tags added by the data enhancement module, analyzing its content in an attempt to understand the subject matter of the email.  This module may store this 'intelligence' as additional tags.

==References==
{{reflist|30em}}

{{DEFAULTSORT:E-Mail Ticketing System}}
[[Category:Email]]
[[Category:Communication software]]
[[Category:Electronic documents]]
[[Category:Records management]]</text>
      <sha1>0kgk02zzpg92fnpvqcjst41xhziqmdf</sha1>
    </revision>
  </page>
  <page>
    <title>Archival Resource Key</title>
    <ns>0</ns>
    <id>24485224</id>
    <revision>
      <id>725244681</id>
      <parentid>725244538</parentid>
      <timestamp>2016-06-14T13:11:56Z</timestamp>
      <contributor>
        <username>Except</username>
        <id>1116146</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6336" xml:space="preserve">An '''Archival Resource Key''' ('''ARK''') is a [[Uniform Resource Locator]] (URL) that is a multi-purpose [[persistent identifier]] for information objects of any type.  An ARK contains the label '''ark:''' after the URL's hostname, which sets the expectation that, when submitted to a web browser, the URL terminated by '?' returns a brief metadata record, and the URL terminated by '??' returns metadata that includes a commitment statement from the current service provider.  The ARK and its inflections ('?' and '??') gain access to three facets of a provider's ability to provide persistence.

Implicit in the design of the ARK scheme is that persistence is purely '''a matter of service''' and not a property of a naming syntax.  Moreover, that a "persistent identifier" cannot be born persistent, but an identifier from any scheme may only be proved persistent over time.  The inflections provide information with which to judge an identifier's likelihood of persistence.

ARKs can be maintained and resolved locally using open source software such as [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers)] or via services such as [http://ezid.cdlib.org EZID] and the central [http://n2t.net N2T (Name-to-Thing)] resolver.

== Structure ==
 &lt;nowiki&gt;[http://NMAH/]ark:/NAAN/Name[Qualifier]&lt;/nowiki&gt;

* NAAN: Name Assigning Authority Number - mandatory unique identifier of the organization that originally named the object
* NMAH: Name Mapping Authority Host - optional and replaceable hostname of an organization that currently provides service for the object
* Qualifier: optional string that extends the base ARK to support access to individual '''hierarchical''' subcomponents of an object,&lt;ref&gt;Hierarchy qualifiers begin with a slash character.&lt;/ref&gt; and to '''variants''' (versions, languages, formats) of components.&lt;ref&gt;Variant qualifiers begin with a dot character.&lt;/ref&gt;

== Name Assigning Authority Numbers (NAANs) ==
A complete NAAN registry&lt;ref&gt;[http://www.cdlib.org/services/uc3/naan_table.html Name Assigning Authority Number registry]&lt;/ref&gt; is maintained by the [[California Digital Library]] and replicated at the [[Bibliothèque nationale de France|Bibliothèque Nationale de France]] and the [[National Library of Medicine|US National Library of Medicine]]. In 2015 it contained over 395 entries, some of which appear below.

* 12025: [[National Library of Medicine]]
* 12148: [[Bibliothèque Nationale de France]]
* 13030: [[California Digital Library]]
* 13038: [[World Intellectual Property Organization]]
* 13960: [[Internet Archive]]
* 14023: Revista de Arte, Ciência e Comunicação
* 15230: [[Rutgers University]]
* 17101: [[Centre for Ecology &amp; Hydrology]]
* 20775: [[University of California, San Diego]]
* 21198: [[University of California Los Angeles]]
* 25031: [[University of Kansas]]
* 25593: [[Emory University]]
* 25652: [[École nationale supérieure des mines de Paris]]
* 26677: [[Library and Archives Canada]]
* 27927: Portico/Ithaka Electronic-Archiving Initiative
* 28722: [[University of California Berkeley]]
* 29114: [[University of California San Francisco]]
* 35911: [[IEEE]]
* 39331: [[National Library of Hungary]]
* 45487: Russian Linguistic Bulletin (Российский Лингвистический Бюллетень)
* 48223: [[UNESCO]]
* 52327: [[Bibliothèque et Archives Nationales du Québec]]
* 61001: [[University of Chicago]]
* 62624: [[New York University]]
* 64269: [[Digital Curation Centre]]
* 65323: [[University of Calgary]]
* 67531: [[University of North Texas]]
* 78319: [[Google]]
* 78428: [[University of Washington]]
* 80444: [[Northwest Digital Archives]]
* 81055: [[British Library]]
* 88435: [[Princeton University]]
* 87925: [[University College Dublin]]

== Generic Services ==
Three generic ARK services have been defined. They are described below in protocol-independent terms. Delivering these services may be implemented through many possible methods given available technology (today’s or future).

=== Access Service (access, location) ===
*Returns (a copy of) the object or a redirect to the same, although a sensible object proxy may be substituted (for instance a table of contents instead of a large document).
*May also return a discriminated list of alternate object locators.
*If access is denied, returns an explanation of the object’s current (perhaps permanent) inaccessibility.

=== Policy Service (permanence, naming, etc.) ===
*Returns declarations of policy and support commitments for given ARKs.
*Declarations are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.
*Policy subareas may be addressed in separate requests, but the following areas should be covered:
**object permanence,
**object naming,
**object fragment addressing, and
**operational service support.

=== Description Service ===
*Returns a description of the object. Descriptions are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.
*A description must at a minimum answer the '''who''', '''what''', '''when''', and '''where''' questions concerning an expression of the object.
*Standalone descriptions should be accompanied by the modification date and source of the description itself.
*May also return discriminated lists of ARKs that are related to the given ARK.

== See also ==
* [[Persistent identifier]]
* [[Digital object identifier]] (DOI)
* [[Handle System]] (Handle)
* [[Persistent uniform resource locator]] (PURL)
* [[Uniform resource name]] (URN)
* [[Info URI scheme]]

== Notes and references ==
&lt;references/&gt;

== External links ==
* [http://www.cdlib.org/inside/diglib/ark/ ARK (Archival Resource Key)], [[California Digital Library]]
* [https://confluence.ucop.edu/download/attachments/16744455/arkcdl.pdf Towards Electronic Persistence Using ARK Identifiers], California Digital Library
* [http://tools.ietf.org/html/draft-kunze-ark  The ARK Identifier Scheme], [[Internet Engineering Task Force]]
* [http://n2t.net Name-to-Thing Resolver]
* [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers) open source software]
* [http://ezid.cdlib.org EZID identifier manager]

[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]</text>
      <sha1>ig485glzazh4xqtg6u7m94oh1749ybq</sha1>
    </revision>
  </page>
  <page>
    <title>Xena (software)</title>
    <ns>0</ns>
    <id>27437313</id>
    <revision>
      <id>739186429</id>
      <parentid>730724064</parentid>
      <timestamp>2016-09-13T08:05:45Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6396" xml:space="preserve">{{Other uses|Xena (disambiguation)}}

'''Xena''' is [[open-source software]] for use in [[digital preservation]]. Xena is short for XML Electronic Normalising for Archives.

Xena is a [[Java (programming language)|Java]] application developed by the [[National Archives of Australia]]. It is available free of charge under the [[GNU General Public License]].

Version 6.1.0 was released 31 July 2013. Source code and binaries for Linux, OS X and Windows are available from [[SourceForge]].

==Mode of operation==
Xena attempts to avoid [[digital obsolescence]] by converting files into an openly specified format, such as [[OpenDocument|ODF]] or [[Portable Network Graphics|PNG]]. If the file format is not supported or the Binary Normalisation option is selected, Xena will perform [[ASCII]] [[Base64]] encoding on binary files and wrap the output in XML metadata. The resulting .xena file is plain text, although the content of the data itself is not directly human-readable. The exact original file can be retrieved by stripping the metadata and reversing the Base64 encoding, using an internal viewer.

==Features==
Platforms supported by Xena are [[Microsoft Windows]], [[Linux]] and [[Mac OS X]].

Xena uses a series of plugins to identify file formats and convert them to an appropriate openly specified format.

Xena has an [[application programming interface]] which allows any reasonably skilled Java developer to develop a plugin to cover a new file type.

Xena can process individual files or whole directories. When processing a whole directory, it can preserve the original directory structure of the converted records.

Xena can create plain text versions of file formats such as [[Tagged Image File Format|TIFF]], [[Microsoft Word|Word]] and [[Portable Document Format|PDF]], with the use of [[Tesseract (software)]].

The Xena interface or Xena Viewer can be used to view or export a Xena file (extension .xena) in its target file format. These files contain the normalised file as well as any extra information relevant to the normalisation process.
The Xena Viewer supports bulk export of Xena files to target file formats.

Xena can be used via its [[graphical user interface]] or the [[command line]].

For Xena to be fully functional, it requires a local installation of the following external software:
*[[LibreOffice]] suite - to convert office documents to OpenDocument format
*[[Tesseract (software)|Tesseract]] - to create plain text versions of file formats
*[[ImageMagick]] - to convert a subset of image files to [[Portable Network Graphics|PNG]]
*Readpst - to convert [[Microsoft Outlook]] PST files to XML. Readpst is part of the free and open source [http://www.five-ten-sg.com/libpst/ libpst software suite].
*[[Free Lossless Audio Codec|FLAC]] - to convert audio files to FLAC format. This is also required to play back audio files using Xena.

==Supported file types==
Xena will recognize and process the file types listed below, plus a few others of minor importance. Unsupported file types will automatically undergo binary normalization.

Office file formats:
*[[Microsoft Office]] files (including [[Microsoft Office XML formats|MS Office XML]], [[SYLK]] spreadsheets and [[Rich Text Format]]) are converted to the corresponding OpenDocument files
*[[Microsoft Outlook]] [[Personal Storage Table|PST]] files are parsed for their individual messages, which are converted to XML files and a Xena index file is created
*[[Microsoft Project]] MPP files are converted to XML
*[[OpenOffice.org XML]] files (SXC, SXI, SXW) are converted to the corresponding OpenDocument formats
*[[WordPerfect]] WPD files are converted to OpenDocument ODT
*[[OpenDocument]] documents (ODT, ODS, ODB, ODP) are preserved unchanged
*Acrobat PDF files are stored as binaries
*Mailbox files (MBX) are converted to individual XML files

Graphics:
*[[BMP file format|BMP]], [[Graphics Interchange Format|GIF]], [[Adobe Photoshop|PSD]], [[PCX]], [[.ras|RAS]], and the [[X Window System]] [[X BitMap|XBM]] and [[X PixMap|XPM]] bitmap files are converted to [[Portable Network Graphics|PNG]]; [[Tagged Image File Format|TIFF]] files additionally get embedded metadata stored in Xena XML. If the [[Tesseract (software)|Tesseract]] [[Optical character recognition|OCR software]] is installed, text will be extracted from TIFF files.
*OpenDocument Drawings (ODG) and [[Scalable Vector Graphics|SVG]] files are wrapped in Xena XML
*JPG and PNG files are stored unchanged

Archive Files:
*Files are extracted from [[File archiver|archives]] ([[ZIP (file format)|ZIP]], [[gzip|GZIP]], [[tar (file format)|TAR/TAR.gz]], [[JAR (file format)|JAR]], [[WAR (Sun file format)|WAR]], Mac binary) and normalised into a separate Xena file. A Xena index file is created, which when opened in the internal Xena viewer will display the files in a table.

Audio files:
*[[MP3]], [[WAV]], [[AIFF]], and [[Vorbis|OGG]] formats are converted to [[Free Lossless Audio Codec|FLAC]] files.

Databases:
*[[SQL]] files are processed as plain text wrapped in XML

Other file types:
*HTML is converted to XHTML
*TXT text files are stored as plain text wrapped in XML; CSS files are stored as plain text wrapped in XML

==Reviews==
An April 22, 2010 review in Practical e-Records rated Xena at 82/100 points. At present Xena has no target preservation format for video files.&lt;ref&gt;{{cite web |url=http://e-records.chrisprom.com/review-of-xena-normalization-software/ |title=Review of XENA Normalization Software |date=2010-04-22 |accessdate= |archiveurl=http://archive.is/yKw1 |archivedate=2012-07-08}}&lt;/ref&gt;

==References==
&lt;references/&gt;

==External links==
*[http://xena.sourceforge.net/ Xena on SourceForge]
*[http://sourceforge.net/apps/mediawiki/xena/index.php?title=Main_Page Xena wiki on SourceForge]
*[https://web.archive.org/web/20100610095405/http://www.ask-oss.mq.edu.au/index.php?option=com_content&amp;task=view&amp;id=66&amp;Itemid=69 Xena project description at The Australian Service for Knowledge of Open Source Software]
*[http://www.naa.gov.au/records-management/secure-and-store/e-preservation/at-naa/software.aspx#section1 National Archives of Australia - software]

{{DEFAULTSORT:Xena (Software)}}
[[Category:Digital preservation]]
[[Category:Electronic documents]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Binary-to-text encoding formats]]
[[Category:Mass digitization]]</text>
      <sha1>r6i5prgqjrfcyof6glexnqao4k2h373</sha1>
    </revision>
  </page>
  <page>
    <title>Computable Document Format</title>
    <ns>0</ns>
    <id>32785726</id>
    <revision>
      <id>753685907</id>
      <parentid>752057522</parentid>
      <timestamp>2016-12-08T17:16:06Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4462" xml:space="preserve">{{Infobox file format
| name                   = Computable Document Format
| logo                   = [[Image:WolframCDFLogoSmall.png]]
| icon                   = [[Image:WolframCDFLogoSmall.png]]
| iconcaption            = 
| screenshot             =  
| caption                =  
| extension              = .cdf
| mime                   = application/cdf
| typecode               =  
| uniform type           = com.wolfram.cdf
| magic                  =  
| owner                  = [[Wolfram Research]]
| released               = {{Start date|2011|07|21}}&lt;!-- {{Start date|YYYY|mm|dd|df=yes}} --&gt;
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|mm|dd|df=yes}} --&gt;
| genre                  =
| container for          =
| contained by           =
| extended from          =
| extended to            =
| standard               =  
| free                   =
| url                    = [http://www.wolfram.com/cdf Computable Document Format]
}}
'''Computable Document Format''' ('''CDF''') is an electronic document format&lt;ref&gt;[http://www.telegraph.co.uk/technology/news/8561619/Wolfram-Alpha-creator-plans-to-delete-the-PDF.html Wolfram Alpha Creator plans to delete the PDF] The Telegraph (UK)&lt;/ref&gt; designed to allow easy authoring&lt;ref&gt;[http://www.pcworld.com/businesscenter/article/236202/wolfram_makes_data_interactive.html Wolfram makes data interactive] PC World&lt;/ref&gt; of dynamically generated interactive content. CDF is a published public format&lt;ref&gt;{{cite web|title=About CDFs|url=http://www.wolfram.com/cdf/faq/#aboutcdf|publisher=[[Wolfram Research]]}}&lt;/ref&gt; created by [[Wolfram Research]].&lt;ref name=thinq11/&gt;

==Features==
Computable document format supports [[GUI]] elements such as sliders, menus and buttons. Content is updated using embedded computation in response to GUI interaction. Contents can include formatted text, tables, images, sounds and animations. CDF supports [[Mathematica]] typesetting and technical notation.&lt;ref&gt;[http://www.zdnet.com/blog/btl/wolfram-launches-new-document-format-meet-cdf/52917 Wolfram Launches new document format. Meet CDF] ZDNet&lt;/ref&gt; Paginated layout, structured drill down layout and slide-show mode are supported. Styles can be controlled using a [[Cascading Style Sheets|cascading style sheet]].

==Reading==
CDF files can be read using a proprietary [[CDF Player]] with a restrictive license, which can be downloaded free of charge from Wolfram Research.&lt;ref name=thinq11&gt;[http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ Wolfram punts expanded medium for technical docs] {{webarchive |url=https://web.archive.org/web/20110725121540/http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ |date=July 25, 2011 }} ThinQ&lt;/ref&gt;

==Authoring==
CDF Files can be created using [[Mathematica]].  Online authoring tools are planned.&lt;ref name=thinq11/&gt;&lt;ref&gt;[http://www.cio.com.au/article/394473/wolfram_makes_data_interactive/ Wolfram makes data interactive] CIO, 21 July 2011&lt;/ref&gt;

==Uses==
Computable Document Format has been used in electronic books by [[Pearson Education]],&lt;ref&gt;[http://www.schoollibraryjournal.com/slj/home/891371-312/wolfram_launches_pdf_killer.html.csp Wolfram launches PDF Killer] School Library Journal&lt;/ref&gt;&lt;ref&gt;[http://www.pearsonhighered.com/briggscochran1einfo/ Briggs Cochrane Calculus]&lt;/ref&gt; to provide the content for the [[Wolfram Demonstrations Project]], and to add client-side interactivity to [[Wolfram Alpha]].&lt;ref&gt;[http://thenextweb.com/apps/2011/08/12/wolfram-alpha-adds-powerful-interactive-search-results/ WolframAlpha adds powerful interactive search results] The Next Web, 12 August 2011&lt;/ref&gt;&lt;ref&gt;[http://www.pcpro.co.uk/news/enterprise/368815/wolfram-launches-its-own-interactive-document-format Wolfram Launches its own interactive document format] PC Pro, July 2011&lt;/ref&gt;

== See also ==
* [[List of numerical analysis software]]
* [[Comparison of numerical analysis software]]

== References ==
{{Reflist}}

== External links ==
* [http://www.wolfram.com/cdf/ Wolfram Research CDF]
* [http://www.wolfram.com/cdf-player/ CDF Player download]

{{Graphics file formats}}
{{Ebooks}} 
{{Wolfram Research}}&lt;!--navbox--&gt;

[[Category:2011 introductions]]
[[Category:Wolfram Research]]
[[Category:Electronic documents]]
[[Category:Open formats]]
[[Category:Page description languages]]
[[Category:Vector graphics]]
[[Category:Computer file formats]]
[[Category:Digital press]]</text>
      <sha1>oe1x29fe66r0j7mxb6c9c50g5bz8ynf</sha1>
    </revision>
  </page>
  <page>
    <title>ViXra</title>
    <ns>0</ns>
    <id>32834692</id>
    <revision>
      <id>746631893</id>
      <parentid>746631435</parentid>
      <timestamp>2016-10-28T15:35:57Z</timestamp>
      <contributor>
        <username>Trilliant</username>
        <id>27603315</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2509" xml:space="preserve">{{lowercase title}}{{Infobox Website
|name           = viXra
|logo           = 
|screenshot     = 
|caption        = 
|url            = [http://viXra.org/ viXra.org]
|alexa          = 
|commercial     = No
|type           = Science
|language       = English
|registration   = 
|owner          = 
|author         = 
|launch date    = 
|current status = Online
|revenue        = 
|slogan         =
}}

'''viXra''' is an electronic [[e-print]] archive set up by independent physicist Philip Gibbs as an alternative to the dominant [[arXiv]] service operated by [[Cornell University]].

==Description==
Although dominated by physics and mathematics submissions, viXra aims to cover topics across the whole scientific community. It accepts submissions without requiring authors to have an academic affiliation and without any threshold for quality.&lt;ref&gt;"[http://blogs.nature.com/news/2009/07/whats_arxiv_spelled_backwards.html What’s arXiv spelled backwards? A new place to publish]". ''Nature News Blog''. 16 July 2009.&lt;/ref&gt; The e-prints on viXra are grouped into seven broad categories: physics, mathematics, computational science, biology, chemistry, humanities, and other areas.&lt;ref name="vixra"&gt;{{cite web |url=http://vixra.org/ |title=ViXra.org open e-print archive |work=viXra.org | accessdate=22 August 2011}}&lt;/ref&gt; Anyone may post anything on viXra, though house rules do prohibit “vulgar, libellous [sic], plagiaristic or dangerously misleading” content.&lt;ref&gt;http://nautil.us/issue/41/selection/what-counts-as-science&lt;/ref&gt;

Gibbs' original motivation for starting the archive was to cater for researchers who believed that their preprints had been unfairly rejected or reclassified by the arXiv moderators.&lt;ref name="pw"&gt;{{cite journal |title=Fledgling site challenges arXiv server |work=[[Physics World]] |date=15 July 2009 |url= http://physicsworld.com/cws/article/news/39845}}&lt;/ref&gt; As of 2013 it had already over 4000 preprints&lt;ref&gt;{{citation|title=A Good Year for viXra|first=Philip E.|last=Gibbs|journal=Prespacetime Journal|volume=4|issue=1|year=2013|pages=87–90|url=http://prespacetime.com/index.php/pst/article/view/482}}.&lt;/ref&gt; and in October, 2016 the number had grown to 16,214.&lt;ref&gt;Official site (front page)&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* [http://vixra.org/ Official website]

[[Category:Eprint archives]]
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Online archives]]


{{website-stub}}</text>
      <sha1>bv6frstx9jrf502z5hukyij46z3ejln</sha1>
    </revision>
  </page>
  <page>
    <title>Kune (software)</title>
    <ns>0</ns>
    <id>32895691</id>
    <revision>
      <id>747110047</id>
      <parentid>734140546</parentid>
      <timestamp>2016-10-31T14:09:55Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor />
      <comment>/* History */clean up, replace deprecated parameters in cite interview templates; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27090" xml:space="preserve">{{Infobox software
|name                       = Kune
|logo                       = [[File:Kune-logo.svg|frameless|upright]]
|screenshot                 = [[File:Concurrent-edit-and-chat.png|frameless|center]]
|caption                    =
|collapsible                = yes
|author                     = [[Comunes Collective]]
|developer                  = [[Comunes Collective]], IEPALA Foundation
|released                   = {{start date and age|2007}} 
|discontinued               = 
|latest release version     = 1.0.0 (Codename "free-riders")&lt;ref name=release1.0.0&gt;{{cite web|title=Released Kune Version 1.0.0 Codename "free-riders"|url=http://kune.ourproject.org/2015/03/released-kune-version-1-0-0-codename-free-riders/|accessdate=2015-06-23|date=2015-03-18|website=Kune Blog}}&lt;/ref&gt;
|latest release date        = {{release date and age|2015|3|18}} 
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       = Java-based [[Google Web Toolkit]]
|operating system           = 
|platform                   = [[Cross-platform]]
|size                       =
|language                   = Multi-language (more than 10)
|status                     = Active (as of 2015-05)
|genre                      = [[Web application]] [[Collaborative software]] [[Distributed social network]]
|license                    = [[Affero General Public License|AGPLv3]] 
|website                    = {{URL|http://kune.ourproject.org/}} {{URL|https://kune.cc/}}
}}

'''Kune''' is a [[free software|free/open source]] distributed social network focused on collaboration rather than just on communication.&lt;ref name="kune.op.org"&gt;{{cite web|title=Kune development site|url=http://kune.ourproject.org|accessdate=3 February 2011}}&lt;/ref&gt; That is, it focuses on online [[Collaborative real-time editor|real-time collaborative editing]], [[Distributed social network|decentralized social networking]] and web publishing, while focusing on workgroups rather than just on individuals.&lt;ref&gt;{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboración libre para grupos|url=http://barrapunto.com/article.pl?sid=11/08/21/2240235|language= Spanish|accessdate=28 August 2011|newspaper=Barrapunto (Spanish Slashdot)|date=22 August 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboración libre para grupos|url=http://www.meneame.net/story/presentando-proyecto-kune-redes-sociales-colaboracion-libre|language= Spanish|accessdate=28 August 2011|newspaper=Menéame (Spanish Digg)|date=23 August 2011}}&lt;/ref&gt; It aims to allow for the creation of online spaces for collaborative work where organizations and individuals can build projects online, coordinate common agendas, set up virtual meetings, publish on the web, and join organizations with similar interests. It has a special focus on [[Free culture movement|Free Culture]] and [[social movements]] needs.&lt;ref&gt;{{cite web|title=Kune FAQ|url=http://kune.ourproject.org/faq|accessdate=7 July 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite news
| title       = Das neue Internet
| first       = Niels
| last        = Boeing
| authorlink  = 
| url         = http://www.zeit.de/zeit-wissen/2012/05/Das-alternative-Netz/komplettansicht
| format      = 
| agency      = 
| newspaper   = [[Die Zeit]]
| publisher   = 
| location    = Germany
| isbn        = 
| issn        = 
| oclc        = 
| pmid        = 
| pmd         = 
| bibcode     = 
| doi         = 
| id          = 
| date        = 31 August 2012
| page        = 
| pages       = 
| at          = 
| accessdate  = 5 September 2012
| language    = German
| trans_title = The new internet
| quote       = 
| archiveurl  = 
| archivedate =
| deadurl     =
| ref         = 
}}&lt;/ref&gt; Kune is a project of the [[Comunes Collective]].

== Technical details ==
Kune is programmed using the [[Java (programming language)|Java]]-based [[Google Web Toolkit|GWT]] in the client-side, integrating [[Apache Wave]] (formerly [[Google Wave]]) and using mainly the open protocols [[XMPP]] and [[Wave Federation Protocol]]. GWT Java sources on the client side generates [[Code obfuscation|obfuscated]] and deeply optimized [[JavaScript]] conforming a [[single page application]]. Wave extensions (gadgets, bots) run on top of Kune (as in [[Facebook apps]]) and can be programmed in Java+GWT, JavaScript or Python.

The current version has been under development since 2007,&lt;ref name="video2008"&gt;{{cite video |people= |date= 26 January 2008|title= Video: Status of Kune development (Jan 2008)|url=http://kune.ourproject.org/2008/01/status-jan08/|format= AVI |medium= |trans_title= |publisher= |location= |archiveurl= |archivedate= |accessdate=28 August 2011|time= |id= |isbn= |oclc= |quote= |ref= }}&lt;/ref&gt; with a constant, stable growth and an established codebase.&lt;ref&gt;{{cite web|title=Kune project in Ohloh|url=http://www.ohloh.net/p/kune|author=[[Ohloh]]|accessdate=28 August 2011}}&lt;/ref&gt; Nowadays the code is hosted in the GIT of [[Gitorious]],&lt;ref&gt;{{Cite web
|title=Kune repository in Gitorious
|url=https://gitorious.org/kune
| accessdate = 2 September 2012
| author = 
| last = 
| first = 
| authorlink = 
| work = 
| publisher = [[Gitorious]]
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; it has a development site&lt;ref name="kune.op.org" /&gt; and its main node&lt;ref&gt;{{Cite web
|title=Kune node "Kune.cc"
|url=http://kune.cc
| accessdate = 5 September 2012
| author = 
| last = 
| first = 
| authorlink = 
| work = 
| publisher = Maintained by [[Comunes Collective]]
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; maintained by the [[Comunes Collective]].

Kune is 100% free software and was built only using free software. Its software is licensed under the [[Affero GPL]] license while the art is under a [[Creative Commons]] BY-SA.

== Philosophy ==

Kune was born in order to face a growing concern from the community behind it. Nowadays, groups (a group of friends, activists, a NGO, a small start-up) that need to work together typically will use multiple [[Free like beer|free (like beer)]] commercial centralized for-profit services (e.g. [[Google Docs]], [[Google Groups]], [[Facebook]], [[Wordpress.com]], [[Dropbox (service)|Dropbox]], [[Flickr]], [[eBay]] ...) in order to communicate and collaborate online. However, "If you're not paying for it, you're the product".&lt;ref&gt;{{cite news|title=If You’re Not Paying for It; You’re the Product|url=http://lifehacker.com/5697167/if-youre-not-paying-for-it-youre-the-product|accessdate=7 July 2012|newspaper=Lifehacker|date=23 November 2010}}&lt;/ref&gt; In order to avoid that, such groups of users may ask a technical expert to build them mailing lists, a webpage and maybe to set up an [[etherpad]]. However, technicians are needed for any new list (as they cannot configure e.g. [[GNU Mailman]]), configuration change, etc., creating a strong dependency and ultimately a bottle-neck.&lt;ref&gt;{{cite web|title=Kune 0.0.9 published (codename "15M")|url=http://kune.ourproject.org/2011/08/kune-0-0-9-published-codename-15m/|accessdate=12 April 2012|publisher = Kune Blog|date=4 August 2011}}&lt;/ref&gt;

Kune aims to cover all those needs of groups to communicate and collaborate, in an usable way and thus without depending on technical experts
.&lt;ref&gt;{{cite news|title=Software libre, hardware libre, ¿servicios libres?|url=http://libertonia.escomposlinux.org/story/2009/5/27/12014/3120|accessdate=3 February 2011|newspaper=Libertonia News|date=27 May 2009}}&lt;/ref&gt; It aims to be a free/libre web service (and thus in [[Internet|the cloud]]), but decentralized as email so a user can choose the server they want and still interoperate transparently with the rest.

Opposite to most distributed social networks, this software focuses on collaboration and building, not only on communication and sharing. Thus, Kune does not aim to ultimately replace Facebook, but also all the above-mentioned commercial services. Kune has a strong focus on the construction of [[Free culture movement|Free Culture]] and eventually facilitate [[Commons-based peer production]].&lt;ref&gt;
{{Cite book
| publisher = IOS Press
| isbn = 9781614990642
|last1= Mass Araya|first1= Elizabeth Roxana |last2= Borsetti Gregorio Vidotti|first2= Silvana Aparecida
|editor1-first=  Ana Alice|editor1-last=Baptista
|editor2-first= Peter|editor2-last= Linde
|editor3-first= Niklas|editor3-last= Lavesson
|editor4-first=Miguel |display-editors = 3 |editor4-last=  Abrunhosa de Brito
| title = Social Shaping of Digital Publishing: Exploring the Interplay Between Culture and Technology
|url= http://www.booksonline.iospress.nl/Content/View.aspx?piid=30613
|chapter= Creative Commons: a Convergence Model Between the Ideal of Commons and the Possibilities of Creation in Contemporary TimesOpposed to Copyright Impediments
| date = 15 July 2012
|accessdate= 19 August 2012
|pages= 3–11
}}&lt;/ref&gt;

== History ==
{| class="wikitable" style="float:right; text-align:center; margin-left:1em; margin-right:0"
|-
! rowspan=1 | Version
! rowspan=1 | Code name
! rowspan=1 | Release date
|-
| 0.0.1
| --
| colspan="2" {{Version |o | 2007}}
|-
| 0.0.9
| [[15-M Movement|15M]]
| colspan="2" {{Version |o | 2011-08-04}}
|-
| 0.1.0
| [[We are the 99%|99%]]&lt;ref&gt;{{cite news|title=Kune new release "99%" &amp; production site|url=https://tech.occupy.net/2012/04/24/kune-new-release-99-production-site/|accessdate=9 June 2012|date=24 April 2012|newspaper= #Occupy Tech News}}&lt;/ref&gt;
| colspan="2" {{Version |o| 2012-04-13}}
|-
| 0.2.0
| [[Elinor Ostrom|Ostrom]]&lt;ref name=releaseOstrom&gt;{{cite news|title=New release of collaborative distributed social network Kune: "Ostrom"|url=https://tech.occupy.net/2012/11/26/new-release-of-collaborative-distributed-social-network-kune-ostrom/|accessdate=26 November 2012|date=26 November 2012|newspaper= #Occupy Tech News}}&lt;/ref&gt;
| colspan="2" {{Version |o | 2012-10-22}}
|-
| 1.0.0
| "Free-riders"&lt;ref name=release1.0.0 /&gt;
| colspan="2" {{Version |c | 2015-03-18}}

|-
| colspan="99" | &lt;small&gt;{{Version |l |show=011101}}&lt;/small&gt;
|}

The origin of Kune relies on the community behind [[Ourproject.org]]. Ourproject&lt;ref&gt;{{cite news|title=There's Life after Microsoft - Free Software Advocates|url=http://www.ipsnews.net/interna.asp?idnews=22073|accessdate=3 February 2011|newspaper=Inter Press Service News Agency|date=24 January 2004}}&lt;/ref&gt; aimed to provide for [[Free culture movement|Free Culture]] (social/cultural projects) what [[Sourceforge]] and other [[software forge]]s meant for [[free software]]: a collection of communication and collaboration tools that would boost the emergence of community-driven free projects.&lt;ref&gt;{{Cite book
| last = Camino
| first = S.
|author2=F. Javier |author3=M. Jiménez Gañán |author4=S. Frutos Cid
 | chapter = Collaborative Development within Open Source Communities
| title =Encyclopedia of Networked and Virtual Organizations
|publisher= IGI Global, Information Science Reference
|isbn = 978-1-59904-885-7
| year = 2008
}}&lt;/ref&gt; However, although Ourproject was relatively successful, it was far from the original aims. The analysis of the situation in 2005&lt;ref&gt;{{cite press release
 | title = Towards a new manager of free projects (Hacia un nuevo gestor de proyectos libres)
 | publisher = [[Ourproject.org]]
 | date = 6 December 2005
 | url = http://ourproject.org/moin/Hacia_un_nuevo_gestor_de_Proyectos_Libres
 | accessdate = 22 April 2012
}}&lt;/ref&gt; concluded that only the groups that had a [[geek|techie]] among them (who would manage [[GNU Mailman|Mailman]] or install a [[Content Management System|CMS]]) were able to move forward, while the rest would abandon the service. Thus, new free collaborative tools were needed, more usable and suitable for anyone, as the available free tools required a high degree of technical expertise. This is why Kune, whose name means "together" in [[Esperanto]], was developed.

The first prototypes of Kune were developed using [[Ruby on Rails]] and [[Pyjamas (software)|Pyjamas]]. However, with the [[Java (software platform)#Licensing|release of Java]] and the [[Google Web Toolkit]] as free software, the community embraced these technologies since 2007.&lt;ref name="video2008" /&gt; In 2009, with a stable codebase and about to release a major version of Kune,&lt;ref&gt;{{cite news|title=¡Colabora con Kune! Llamado a desarrolladores/as|url=http://www.apesol.org/news/341|publisher=Peru Free Software Association|date=5 May 2009|accessdate=3 February 2011}}&lt;/ref&gt; Google announced the [[Google Wave]] project and promised it would be released as free software. Wave was using the same technologies of Kune (Java + GWT, Guice, XMPP protocol) so it would be easy to integrate after its release. Besides, Wave was offering an open federated protocol, easy extensibility (through gadgets), easy control versioning, and very good real-time edition of documents. Thus, the community decided to halt the development of Kune, and wait for its release... in the meanwhile developing gadgets that would be integrated in Kune later on.&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = MassMob: Meetings and Smart Mobs 
 | work =
 | publisher = [[Comunes Collective]]
 | year = 2009
 | url = http://massmob.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Troco project: an experimental peer-to-peer currency
 | work = 
 | publisher = [[Comunes Collective]]
 | origyear =2009| year =2010
 | url = http://troco.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Karma: A Reputation Rating System
 | work =
 | publisher = [[Comunes Collective]]
 | origyear = 2009| year = 2010
 | url = http://karma.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt; In this same period, the community established the [[Comunes Association]] (with an acknowledged inspiration in [[Software in the Public Interest]]) as a non-profit legal umbrella for free software tools for encouraging the [[Commons]] and facilitating the work of [[social movements]].&lt;ref&gt;{{cite interview |last =  |first =  |subjectlink = Interview to [[Comunes Collective]] |interviewer = Serotonina EH |title = |url = http://ondaexpansiva.net/?p=1001  |work = Free Culture Forum 2011 |publisher = Radio Onda Expansiva |location = [[Burgos]], [[Spain]] |date = 9 November 2011 |accessdate =11 April 2012 }}&lt;/ref&gt; The umbrella covered Ourproject, Kune and Move Commons,&lt;ref&gt;{{cite news
|title=Move Commons, crowdfunding y etiquetado de proyectos sociales
|url=http://www.misapisportuscookies.com/2011/12/move-commons/
|accessdate=11 April 2012
|newspaper=Mis APIs por tus Cookies
|date=1 December 2012
}}&lt;/ref&gt; together with some other minor projects.

In November 2010, the free [[Apache Wave]] (previously Wave-in-a-Box) was released, under the umbrella of the [[Apache Foundation]]. Since then, the community began integrating its source code within the Kune previous codebase,&lt;ref&gt;{{Cite web
| url = http://ecosistemaurbano.org/castellano/move-commons-y-kune-herramientas-libres-para-el-activismo-y-la-colaboracion/
| title = Move Commons &amp; Kune: free tools for activism and collaboration (Move Commons y Kune: herramientas libres para el activismo y la colaboración)
| accessdate = 11 April 2012
| author = 
| last = Toledo
| first = Jorge
| authorlink = 
| date = 14 February 2012
| work = 
| publisher = Ecosistema Urbano
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; and with the support of the IEPALA Foundation.&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Presenting status of Kune development Jan-2011
 | work =
 | publisher =
 | date = 24 January 2011
 | url =http://kune.ourproject.org/2011/01/status-jan2011/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt; Kune released its Beta and moved to production in April 2012.

Since then, Kune has been catalogued as "activism 2.0"&lt;ref&gt;{{Cite web
| url = https://pilargonzalo.wordpress.com/2011/11/04/activismo-2-0-y-empoderamiento-ciudadano-en-red-i/
| title = Activism 2.0 and citizen empowerment in the net (I) (Activismo 2.0 y empoderamiento ciudadano en red (I))
| accessdate = 11 April 2012
| author = 
| last = Gonzalo
| first = Pilar
| authorlink = 
| date = 4 November 2011
| work = 
| publisher = 
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; and citizen tool,&lt;ref&gt;{{cite journal|title=Free Knowledge: Collective intelligence for developing free tools and community resources (Conocimiento libre: Inteligencia colectiva para desarrollar herramientas libres y recursos comunitarios)|journal=¡Rebelaos!|year=2012|volume=1|pages=10|accessdate=11 April 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite news
|title= Cooperation, Collaboration and citizen power (Cooperación, colaboración y poder ciudadano)
|url=http://www.sindikos.com/2012/01/cooperacion-colaboracion-y-poder-ciudadano/
|accessdate=11 April 2012
|newspaper=Sindikos
|date=20 January 2012
}}&lt;/ref&gt; a tool for NGOs,&lt;ref&gt;{{Cite web
| url = http://www.democraciaycooperacion.net/contenidos-sitio-web/espanol/fidc/entre-foros/iii-taller-internacional-del/informacion-398/article/las-redes-de-organizaciones
| title = Las redes de organizaciones sociales del CIS generan propuestas para la internacionalización de la acción
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| date = 5 March 2011
| work = 
| publisher = Foro Internacional Democracia y Cooperación
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite report
 | author     = &lt;!-- or |last= and |first= --&gt;
 | authorlink =
 | coauthors  = 
 | date       = February 2012
 | title      = Document Summary of the Rapporteur of Second Regional Workshop Latin America and the Caribbean
 | url        = http://www.democraciaycooperacion.net/IMG/pdf/Summary_Rapporteur_and_context.pdf
 | publisher  = [[Ministry of Foreign Affairs and Cooperation (Spain)]]
 | format     =
 | others     =
 | edition    =
 | location   = [[Mexico City]]
 | chapter    =
 | section    =
 | page       =
 | pages      = 15
 | docket     =
 | accessdate = 12 April 2012
 | quote      =
}}&lt;/ref&gt; multi-tool for general purpose&lt;ref&gt;{{Cite web
| url = http://www.contenidosenred.com/blog/kune/
| title = Kune
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| authors = Lucrecia Baquero, Clara Alba
| date = 17 February 2012
| work = 
| publisher = Contenidos en Red
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; (and following that, criticized for the risk of falling on the [[second-system effect]]&lt;ref&gt;{{Cite web
| url = http://jotarp.org/2011/10/internet/contra-las-redes-sociales.html
| title = Against social networks (Contra las redes sociales)
| accessdate = 11 April 2012
| author = 
| last = Palacios
| first = J. Ramón
| authorlink = 
| date = 24 October 2011
| work = 
| publisher = Jotarp
| pages = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt;) and example of the new paradigm.&lt;ref&gt;{{Cite web
| url = https://semillasdeinnovacion.wordpress.com/2012/03/13/sobre-la-necesidad-de-acercar-la-ciudad-al-campo-y-viceversa/
| title = On the need to bring closer city and country (Sobre la necesidad de acercar la ciudad al campo y viceversa)
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| authors = Lucrecia Baquero, Clara Alba
| date = 13 March 2012
| work = 
| publisher = Semillas de Innovación
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; It was selected as "open website of the week" by the [[Open University of Catalonia]]&lt;ref&gt;{{cite news
|title= Open website of the week: Kune
|url=http://mentesabiertas.uoc.edu/webabiertas/webabiertadelasemanakune?lang=en
|accessdate=11 April 2012
|newspaper=Open Minds, [[Open University of Catalonia]] 
|date=5 March 2012
}}&lt;/ref&gt; and as one of the [[Occupy movement|#Occupy]] Tech projects.&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = #Occupy Tech projects
 | work =
 | publisher =
 | url =https://tech.occupy.net/projects/
 | format =
 | doi =
 | accessdate = 22 April 2012}}&lt;/ref&gt; Nowadays there are plans of another federated social network, Lorea (based on [[Elgg (software)|Elgg]]), to connect with Kune.&lt;ref&gt;{{cite news
|title= Radical Community Manager
|url=https://ncomuneszgz.wordpress.com/2012/01/08/radical-community-manager/
|accessdate=11 April 2012
|newspaper=Nociones Comunes
|date=17 March 2012
}}&lt;/ref&gt;

&lt;!--
&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title =
 | work =
 | publisher =
 | url =
 | format =
 | doi =
 | accessdate = }}&lt;/ref&gt;

&lt;ref&gt;{{cite press release
 | title =
 | publisher =
 | url =
 | accessdate = }}&lt;/ref&gt;

&lt;ref&gt;{{Cite book
| last = Camino
| first = S.
|author2=F. Javier |author3=M. Jiménez Gañán |author4=S. Frutos Cid
 | chapter = Collaborative Development within Open Source Communities
| title =Encyclopedia of Networked and Virtual Organizations
|publisher= IGI Global, Information Science Reference
|isbn = 978-1-59904-885-7
| year = 2008
}}&lt;/ref&gt;

&lt;ref&gt;{{Cite journal
| volume = 32
| issue = 3
| pages = 1–1
| last = Machado
| first = H.
|author2=A. Suset |author3=GJ Martín |author4=FR Funes-Monzote
 | title = From the reductionist approach to the system approach in Cuban agriculture: a necessary change of vision
| journal = Pastos y Forrajes
| year = 2009
}}&lt;/ref&gt;

&lt;ref&gt;{{cite news
|title=
|url=
|accessdate=11 April 2012
|newspaper=
|date=29 January 2004
}}&lt;/ref&gt;

--&gt;

== Feature list ==

* All the functionalities of [[Apache Wave]], that is collaborative federated real-time editing, plus
* Communication
** Chat and chatrooms compatible with Gmail and Jabber through XMPP (with several XEP extensions), as it integrates Emite&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Emite: XMPP &amp; GWT
 | work =
 | publisher =
 | url =http://emite.googlecode.com/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt;
** Social networking (federated)
* Real-time collaboration for groups in:
** Documents: as in [[Google Docs]]
** Wikis
** Lists: as in [[Google Groups]] but minimizing emails, through waves
** Group Tasks
** Group Calendar: as in [[Google Calendar]], with ical export
** Group Blogs
** Web-creation: aiming to publish contents directly on the web (as in [[WordPress]], with a dashboard and public view) (in development)
** Bartering: aiming to decentralize bartering as in [[eBay]]
* Advanced email
** Waves: aims to replace most uses of email
** Inbox: as in email, all your conversations and documents in all kunes are controlled from your inbox
** Email notifications (Projected: replies from email)
* Multimedia &amp; Gadgets
** Image or Video galleries integrated in any doc
** Maps, mindmaps, Twitter streams, etc.
** Polls, voting, events, etc.
** and more via Apache Wave extensions, easy to program (as in [[Facebook apps]], they run on top of Kune)
* Federation
** Distributed Social Networking the same way as e-mail: from one inbox you control all your activity in all kunes, and you can collaborate with anyone or any group regardless of the kune where they were registered.
** Interoperable with any Kune server or Wave-based system
** Chat interoperable with any XMPP server
* Usability
** Strong focus on usability for any user
** Animated tutorials for each tool
** [[Drag and drop|Drag&amp;Drop]] for sharing contents, add users to a doc, change roles, delete contents, etc.
** Shortcuts
* Free culture
** Developed using free software and released under [[Affero General Public License|AGPL]]
** Easy assistant for choosing content licenses for groups. Default license is [[Creative Commons]] BY-SA.
* Developer-friendly
** Debian/Ubuntu package for easy installation
** Wave Gadgets can be programmed in Java+GWT, [[JavaScript]] or [[Python (programming language)|Python]]

== Supporters and adopters ==
Kune has the active support of several organizations and institutions:
* [[Comunes Association]], whose community is behind Kune development. It hosts a Kune server for free projects: [https://kune.cc/#! https://kune.cc]
* IEPALA Foundation,&lt;ref&gt;{{cite web|title=IEPALA Foundation homepage|url=http://www.iepala.es|accessdate=22 April 2012}}&lt;/ref&gt; which is supporting the development with economical and technical resources. It hosts a Kune server for [[non-governmental organizations]]: [http://social.gloobal.net "Social Gloobal"] (previously EuroSur).
* Grasia Software Agent Research Group&lt;ref&gt;{{cite web|title=Grasia Research Group homepage|url=http://grasia.fdi.ucm.es/main/|accessdate=22 April 2012}}&lt;/ref&gt; of the [[Complutense University of Madrid]] has provided technical resources. It seeks to host a Kune server for academic article collaboration.
* Interns from the Master of Free Software from the [[King Juan Carlos University]] are participating in the development.
* Trainees from the [[American University of Science and Technology]] (Lebanon) participate in the system administration.
* [[Paulo Freire Institute]] in Brazil participated in the early design and prototypes.
* The Kune workgroup of the Medialab Prado&lt;ref&gt;{{cite web|title=Medialab-Prado (Madrid) homepage|url=http://medialab-prado.es|accessdate=22 April 2012}}&lt;/ref&gt; are participating in the beta-testing.&lt;ref&gt;{{cite web|title=Comunes profile in Medialab-Prado|url=http://medialab-prado.es/person/comunes|accessdate=22 April 2012}}&lt;/ref&gt;

== See also ==
* [[Apache Wave]]
* [[Comunes Collective]]
* [[Distributed social network]]
* [[Comparison of software and protocols for distributed social networking]]
* [[List of AGPL web applications]]
* [[Ourproject.org]]
* [[Wave Federation Protocol]]

== References ==
{{Reflist|2}}

== External links ==
* [https://kune.cc/ Kune.cc main site]
* [http://kune.cc/?locale=de#!kune.wiki.17.1678 Sites using kune]
* [http://kune.ourproject.org Kune information webpage]

{{Cleanup|reason=[[WP:OVERCAT]]|date=May 2016}}

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Project hosting websites]]
[[Category:Creative Commons-licensed websites]]
[[Category:Collaborative projects]]
[[Category:Virtual communities]]
[[Category:Online communities for social change]]
[[Category:Free groupware]]
[[Category:Free project management software]]
[[Category:Multilingual websites]]
[[Category:Community websites]]
[[Category:Social networking services]]
[[Category:Web applications]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Blog software]]
[[Category:Collaborative real-time editors]]
[[Category:2012 software]]
[[Category:Electronic documents]]
[[Category:Free software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Cross-platform free software]]
[[Category:Internet properties established in 2007]]
[[Category:Software using the GNU AGPL license]]</text>
      <sha1>dmwdk11wc50bxeljm4eydfp4famhcrf</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of e-book formats</title>
    <ns>0</ns>
    <id>12115370</id>
    <revision>
      <id>762840547</id>
      <parentid>759567167</parentid>
      <timestamp>2017-01-31T01:00:06Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="51312" xml:space="preserve">The following is a '''comparison of e-book formats''' used to create and publish [[e-book]]s.

The [[EPUB]] format is the most widely supported vendor-independent [[XML]]-based (as opposed to [[Portable Document Format|PDF]]) e-book format; that is, it is supported by the largest number of e-Readers, including [[Kindle Fire|Amazon Kindle Fire]] (but not standard Kindle).&lt;ref name="kdp.amazon.com"&gt;{{cite web|url=https://kdp.amazon.com/help?topicId=A2GF0UFHIYG9VQ |title=Amazon Kindle Direct Publishing: Get help with self-publishing your book to Amazon's Kindle Store |publisher=Kdp.amazon.com |date= |accessdate=2015-08-31}}&lt;/ref&gt; See table below for details. 
{{TOC right}}

==Format descriptions==
Formats available include, but are not limited to:

===Broadband eBooks (BBeB) ===
{{main article |BBeB}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Sony media
|-
| style="background:#ddd;"| ''Published as'':
| .lrf; .lrx
|}
The digital book format originally used by [[Sony Corporation]].  It is a proprietary format, but some reader software for general-purpose computers, particularly under [[GNU Project|GNU]]/Linux (for example, [[Calibre (software)|Calibre]]'s internal viewer&lt;ref&gt;{{Citation | title = About | url = http://calibre-ebook.com/about | publisher = Calibre}}&lt;/ref&gt;), have the capability to read it.  The LRX file extension represents a [[Digital rights management|DRM]] encrypted eBook. More recently, Sony has converted its books from BBeB to EPUB and is now issuing new titles in EPUB.

===Comic Book Archive file ===
{{main article|Comic book archive}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| compressed images
|-
| style="background:#ddd;"| ''Published as'':
|.cbr (RAR); .cbz (ZIP); .cb7 (7z); .cbt (TAR); .cba (ACE)
|}

===Compiled HTML ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[Microsoft Compiled HTML Help]]
|-
| style="background:#ddd;"| ''Published as'':
| .chm
|}
CHM format is a proprietary format based on HTML. Multiple pages and embedded graphics are distributed along with [[metadata]] as a single compressed file. The indexing is both for keywords for full text search.

===DAISY – ANSI/NISO Z39.86  ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[DAISY Digital Talking Book|DAISY]]
|-
| style="background:#ddd;"| ''Published as'':
|
|}

The Digital Accessible Information SYstem (DAISY) is an [[XML]]-based open standard maintained by the DAISY Consortium for people with [[print disabilities]].  DAISY has wide international support with features for multimedia, navigation and synchronization. A subset of the DAISY format has been adopted by law in the United States as the National Instructional Material Accessibility Standard (NIMAS), and K-12 textbooks and instructional materials are now required to be provided to students with disabilities.

DAISY is already aligned with the EPUB technical standard, and is expected to fully converge with its forthcoming EPUB3 revision.&lt;ref&gt;{{cite web|url=http://www.daisy.org/z3986 |title=DAISY Standard &amp;#124; DAISY Consortium |publisher=Daisy.org |date= |accessdate=2015-08-31}}&lt;/ref&gt;

===DjVu ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| DjVu
|-
| style="background:#ddd;"| ''Published as'':
| .[[DjVu|djvu]]
|}
DjVu is a format specialized for storing scanned documents. It includes advanced compressors optimized for low-color images, such as text documents. Individual files may contain one or more pages. DjVu files cannot be re-flowed.

The contained page images are divided in separate layers (such as multi-color, low-resolution, background layer using [[lossy compression]], and few-colors, high-resolution, tightly compressed foreground layer), each compressed in the best available method. The format is designed to decompress very quickly, even faster than vector-based formats.

The advantage of DjVu is that it is possible to take a high-resolution scan (300–400 DPI), good enough for both on-[[screen reading]] and printing, and store it very efficiently. Several dozens of 300 DPI black-and-white scans can be stored in less than a megabyte.

===DOC===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Word
|-
| style="background:#ddd;"| ''Published as'':
| .[[Doc (computing)|DOC]]
|}

[[Doc (computing)|DOC]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format is that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.

===DOCX===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Word (XML)
|-
| style="background:#ddd;"| ''Published as'':
| .[[DOCX]]
|}

[[DOCX]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format are that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.

=== EPUB ===
{{Main article|EPUB}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IDPF/EPUB
|-
| style="background:#ddd;"| ''Published as'':
| .epub
|}
[[File:EPUB logo.svg|thumb|right|150px|The EPUB logo]]
The .epub or [[OEBPS]] format is a technical standard for e-books created by the [[International Digital Publishing Forum]] (IDPF).

The EPUB format has gained some popularity as a vendor-independent XML-based e-book format. The format can be read by the [[Kobo eReader]], [[BlackBerry]] devices, Apple's [[iBooks]] app running on [[Macintosh]] computers and [[IOS (Apple)|iOS]] devices, [[Google Play|Google Books]] app running on [[Android (operating system)|Android]] and iOS devices, Barnes &amp; Noble [[Nook]], Amazon [[Kindle Fire]],&lt;ref name="kdp.amazon.com"/&gt; [[Sony Reader]], [[BeBook]], [[Cybook Gen3|Bookeen Cybook Gen3 (with firmware v2 and up)]], COOL-ER, [[Adobe Digital Editions]], [[Lexcycle Stanza]], BookGlutton, AZARDI, [[FBReader]], [[Aldiko]], [[CoolReader]], [[Mantano Reader]], [[Moon+ Reader]], the [[Mozilla Firefox]] [[Add-on (Mozilla)|add-on]] [[EPUBReader]], [[Okular]] and other reading apps.

[[Adobe Digital Editions]] uses .epub format for its e-books, with [[digital rights management]] (DRM) protection provided through their proprietary ADEPT mechanism. The ADEPT framework and scripts have been reverse-engineered to circumvent this DRM system.&lt;ref&gt;{{cite web|author= |url=http://i-u2665-cabbages.blogspot.com/2009/02/circumventing-adobe-adept-drm-for-epub.html |title=i♥cabbages: Circumventing Adobe ADEPT DRM for EPUB |publisher=I-u2665-cabbages.blogspot.com |date=2009-02-18 |accessdate=2015-08-31}}&lt;/ref&gt;

===eReader ===
;Formerly Palm Digital Media/Peanut Press
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Palm Media
|-
| style="background:#ddd;"| ''Published as'':
| .[[PDB (Palm OS)|pdb]]
|}

eReader is a [[freeware]] program for viewing Palm Digital Media electronic books which use the pdb format used by many Palm applications. Versions are available for [[Android (operating system)|Android]], [[BlackBerry]], [[IOS (Apple)|iOS]], [[Palm OS]] (not webOS), [[Symbian]], [[Windows Mobile]] Pocket PC/Smartphone, and [[OS X]]. The reader shows text one page at a time, as paper books do. eReader supports embedded hyperlinks and images. Additionally, the [[Lexcycle Stanza|Stanza]] application for the [[iPhone]] and [[iPod touch]] can read both [[encryption|encrypted]] and unencrypted eReader files.

The program supports features like bookmarks and footnotes, enabling the user to mark any page with a bookmark and any part of the text with a footnote-like commentary. Footnotes can later be exported as a Memo document.

On July 20, 2009, [[Barnes &amp; Noble]] made an announcement&lt;ref&gt;{{cite web|url=http://www.barnesandnobleinc.com/press_releases/2009_july_20_ebookstore.html |title=Barnes &amp; Noble Booksellers |publisher=Barnesandnobleinc.com |date=2009-07-20 |accessdate=2015-08-31}}&lt;/ref&gt; implying that eReader would be the company's preferred format to deliver e-books. Exactly three months later, in a press release by [[Adobe Systems|Adobe]], it was revealed Barnes &amp; Noble would be joining forces with the software company to standardize the EPUB and PDF eBook formats.&lt;ref&gt;{{cite press release
 | title = Barnes &amp; Noble adopts open EPUB eBook Format, PDF and Adobe Content Server | publisher = [[Adobe Systems]] | date = 2009-10-20 | url = https://www.adobe.com/aboutadobe/pressroom/pressreleases/200910/AdobeandBarnesNobleJoinForcestoStandardizeeBookTechnology.html | accessdate = 2013-05-06}}&lt;/ref&gt;&lt;ref&gt;{{Citation|last=Rothman |first=David |title=‘Barnes &amp; Noble adopts open EPUB eBook Format, PDF and Adobe Content Server’ |publisher=TeleRead |date=2009-10-20 |url=http://www.teleread.com/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130506010320/http://www.teleread.com:80/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |archivedate=2013-05-06 |deadurl=yes |df= }}&lt;/ref&gt; Barnes &amp; Noble e-books are now sold mostly in EPUB format.&lt;ref&gt;{{Citation | last = Bell | first = Ian | title = Barnes &amp; Noble Adopts ePub Standard; Aligns With Adobe | publisher = [[Digital Trends]] | date = 2009-11-18 | url = http://www.digitaltrends.com/gadgets/barnes-aligns-with-adobe/ | accessdate = 2013-05-06}}&lt;/ref&gt;&lt;ref&gt;{{Citation|last=Meadows |first=Chris |title=Barnes &amp; Noble quietly changes e-book format, neglects to tell consumers |publisher=TeleRead |date=2009-12-13 |url=http://www.teleread.com/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130130085503/http://www.teleread.com:80/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |archivedate=2013-01-30 |deadurl=yes |df= }}&lt;/ref&gt;&lt;ref&gt;{{Citation | last = James | first = Kendrick | title = Has Barnes &amp; Noble Changed Its e-Book Format to ePUB? | publisher = [[GigaOM]] | date = 2009-12-14 | url = http://gigaom.com/2009/12/14/has-barnes-noble-changed-its-e-book-format-to-epub/ | accessdate = 2013-05-06}}&lt;/ref&gt;

===FictionBook (Fb2) ===

{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| FictionBook
|-
| style="background:#ddd;"| ''Published as'':
| .[[FictionBook|fb2]]
|}

[[FictionBook]]&lt;ref&gt;[http://haali.cs.msu.ru/pocketpc/FictionBook_description.html]  {{webarchive |url=https://web.archive.org/web/20070703204958/http://haali.cs.msu.ru/pocketpc/FictionBook_description.html |date=July 3, 2007 }}&lt;/ref&gt; is a popular [[XML]]-based e-book format, supported by free readers such as [[FBReader]], [[Okular]], [[CoolReader]], [[Bebook]] and [[STDU Viewer]].

The FictionBook format does not specify the appearance of a document; instead, it describes its structure and semantics. All the ebook metadata, such as the author name, title, and publisher, is also present in the ebook file. Hence the format is convenient for automatic processing, indexing, and ebook collection management. This also is convenient to store books in it for later automatic conversion into other formats.

===Founder Electronics ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Apabi Reader
|-
| style="background:#ddd;"| ''Published as'':
| .[[XEB|xeb]]; .ceb
|}
[[APABI]] is a format devised by [[Founder Electronics]]. It is a popular format for Chinese e-books. It can be read using the [[Apabi Reader]] software, and produced using [[Apabi Publisher]]. Both .xeb and .ceb files are encoded binary files. The [[iLiad (E-book Reader)|Iliad]] e-book device includes an Apabi 'viewer'.

===Hypertext Markup Language ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Hypertext
|-
| style="background:#ddd;"| ''Published as'':
| .htm; .html and typically auxiliary images, js and css
|}
[[HTML]] is the [[markup language]] used for most [[World Wide Web|web]] pages. E-books using HTML can be read using a [[Web browser]]. The specifications for the format are  available without charge from the [[W3C]].

HTML adds specially marked meta-elements to otherwise plain text encoded using [[character set]]s like [[ASCII]] or [[UTF-8]]. As such, suitably formatted files can be, and sometimes are, generated ''by hand'' using a ''[[text editor|plain text editor]]'' or ''[[Source code editor|programmer's editor]]''. Many ''HTML generator'' applications exist to ease this process and often require less intricate knowledge of the format details involved.

HTML on its own is not a particularly efficient format to store information in, requiring more storage space for a given work than many other formats. However, several e-Book formats including the Amazon Kindle, Open eBook, Compiled HTML,  Mobipocket and EPUB store each book chapter in HTML format, then use [[ZIP (file format)|ZIP]] compression to compress the HTML data, images, metadata and style sheets into a single, significantly smaller, file.

HTML files encompass a wide range of standards&lt;ref&gt;{{cite web|url=http://www.webstandards.org/learn/faq/ |title=Frequently Asked Questions (FAQ) - The Web Standards Project |publisher=Webstandards.org |date=2002-02-27 |accessdate=2015-08-31}}&lt;/ref&gt; and displaying HTML files correctly can be complicated. Additionally many of the features supported, such as forms, are not relevant to e-books.

===iBook (Apple) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| iBook
|-
| style="background:#ddd;"| ''Published as'':
| .ibooks
|}
The .ibooks format is created with the free [[iBooks Author]] ebook layout software from [[Apple Inc.]]. This proprietary format is based on the [[EPUB]] standard, with some differences in the CSS tags used in an ibooks format file, thus making it incompatible with the EPUB specification. The End-User Licensing Agreement (EULA) that comes with iBooks Author states that "If you want to charge a fee for a work that includes files in the .ibooks format generated using iBooks Author, you may only sell or distribute such work through Apple". The "through Apple" will typically be in the Apple [[iBooks]] store. The EULA further states that "This restriction does not apply to the content of such works when distributed in a form that does not include files in the .ibooks format." Therefore, Apple has not included distribution restrictions in the iBooks Author EULA for ibooks format ebooks created in iBooks Author that are made available for free, and it does not prevent authors from re-purposing the content in other ebook formats to be sold outside the iBookstore. This software currently supports import and export functionally for three formats. ibook, Plain text and PDF. The iBooks Author 2.3 and later supports importing EPUB and export EPUB 3.0.&lt;ref&gt;{{Cite web|url=https://support.apple.com/en-us/HT204884|title=About ePubs created with iBooks Author|language=en-US|access-date=2016-09-25}}&lt;/ref&gt;

===IEC 62448===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IEC 62448
|-
| style="background:#ddd;"| ''Published as'':
|
|}

IEC 62448 is an international standard created by [[International Electrotechnical Commission]] (IEC), Technical Committee 100, Technical Area 10 (Multimedia e-publishing and e-book).

The current version of IEC 62448 is an umbrella standard that contains as appendices two concrete formats, XMDF of Sharp and BBeB of Sony. However, BBeB has been discontinued by Sony and the version of XMDF that is in the specification is out of date. The IEC TA10 group is discussing the next steps, and has invited the IDPF organization which has standardized [[EPUB]] to be a liaison. It is possible that the current version of EPUB and/or the forthcoming EPUB3 revision may be added to IEC 62448.  Meanwhile, a number of Japanese companies have proposed that IEC standardize a proposed new Japanese-centric file format that is expected to unify DotBook of Voyager Japan and XMDF of Sharp.  This new format has not been publicly disclosed as of November 2010 but it is supposed to cover basic representations for the Japanese language.  Technically speaking, this revision is supposed to provide a Japanese minimum set, a Japanese extension set, and a stylesheet language. These issues were discussed in the TC100 meeting held  in October 2010 but no decisions were taken besides offering the liaison status to IDPF.

===INF (IBM) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IBM &amp; Open Source
|-
| style="background:#ddd;"| ''Published as'':
| .inf
|}
[[IBM]] created this e-book format and used it extensively for [[OS/2]] and other of its operating systems. The INF files were often digital versions of printed books that came with some bundles of OS/2 and other products. There were many other newsletters and monthly publications (e.g.: EDM/2) available in the INF format too.

The advantage of INF is that it is very compact and very fast. It also supports images, reflowed text, tables and various list formats. INF files get generated by compiling the markup text files — in the [[Information Presentation Facility]] (IPF) format — into binary files.

Originally only IBM created an INF viewer and compiler, but later open source viewers like NewView, DocView and others appeared. There is also an open source IPF compiler named WIPFC, created by the [[Open Watcom]] project.

===KF8 (Amazon Kindle) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Kindle
|-
| style="background:#ddd;"| ''Published as'':
| .azw3; .azw; .kf8
|}
With the release of the [[Kindle Fire]] reader in late 2011, [[Amazon.com]] also released [[Kindle Format 8]], their newest file format, also known as .AZW3. The .azw3 file format supports a subset of [[HTML5]] and [[CSS3]] features, with some additional nonstandard features; the new data is stored within a container which can also be used to store a Mobi content document, allowing limited backwards compatibility.&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/feature.html?docId=1000729511|title=Kindle Format 8 Overview|publisher=Amazon.com|year=2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://musingsandmarvels.com/2012/03/06/the-new-kindle-format-8-kf8/|title=The New Kindle Format KF8|publisher=Musings and Marvels:Learning the ins and outs of the publishing industry|date=2012-03-06|accessdate=2012-03-16}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/feature.html/ref=amb_link_357613502_6?ie=UTF8&amp;docId=1000729901&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_s=right-4&amp;pf_rd_r=0GN9VRRB0NJ08VXGFKWK&amp;pf_rd_t=1401&amp;pf_rd_p=1343256942&amp;pf_rd_i=1000729511|title=HTML5 tags supported by KF8|publisher=Amazon.com|accessdate=2012-03-16}}&lt;/ref&gt;

Older [[Amazon Kindle|Kindle]] e-readers use the proprietary format, AZW. It is based on the [[Mobipocket]] standard, with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]) and its own [[Digital rights management|DRM]] formatting. Because the ebooks bought on the Kindle are delivered over its wireless system called Whispernet, the user does not see the AZW files during the download process. The Kindle format is available on a variety of platforms, such as through the Kindle app for the various mobile device platforms.

===Microsoft LIT ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Reader
|-
| style="background:#ddd;"| ''Published as'':
| .[[LIT (file format)|lit]]
|}
DRM-protected LIT files are only readable in the proprietary [[Microsoft Reader]] program, as the .LIT format, otherwise similar to Microsoft's [[Microsoft Compiled HTML Help|CHM]] format, includes [[Digital Rights Management]] features. Other third party readers, such as [[Lexcycle Stanza]], can read unprotected LIT files.

The Microsoft Reader uses patented [[ClearType]] display technology. In Reader navigation works with a keyboard, mouse, stylus, or through electronic bookmarks. The Catalog Library records reader books in a personalized "home page", and books are displayed with ClearType to improve readability.  A user can add annotations and notes to any page, create large-print e-books with a single command, or create free-form drawings on the reader pages.  A built-in dictionary allows the user to look up words.

In August 2011, Microsoft announced they were discontinuing both Microsoft Reader and the use of the .lit format for ebooks&lt;ref&gt;{{cite web|url=http://aazae.com/|title=Ebooks|work=Aazae}}&lt;/ref&gt; at the end of August 2012, and ending sales of the format on November 8, 2011.&lt;ref&gt;"Microsoft is discontinuing Microsoft Reader effective August 30, 2012, which includes download access of the Microsoft Reader application from the Microsoft Reader website."[http://www.microsoft.com/reader/ Microsoft Reader] {{webarchive |url=https://web.archive.org/web/20050822035209/http://www.microsoft.com/reader/ |date=August 22, 2005 }}&lt;/ref&gt;

===Mobipocket ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Mobipocket
|-
| style="background:#ddd;"| ''Published as'':
| [[PRC (Palm OS)|.prc]]; .mobi
|}
The [[Mobipocket]] e-book format is based on the [[Open eBook]] standard using [[XHTML]] and can include [[JavaScript]] and frames. It also supports native [[SQL]] queries to be used with embedded databases. There is a corresponding e-book reader.

The [[Mobipocket]] Reader has a home page library. Readers can add blank pages in any part of a book and add free-hand drawings. Annotations – highlights, bookmarks, corrections, notes, and drawings – can be applied, organized, and recalled from a single location. Images are converted to GIF format and have a maximum size of 64K,&lt;ref&gt;{{cite web|url=http://www.mobipocket.com/dev/article.asp?BaseFolder=creatorhome&amp;File=image.htm |title=Mobipocket Developer Center - Importing Image files |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}&lt;/ref&gt; sufficient for mobile phones with small screens, but rather restrictive for newer gadgets. [[Mobipocket]] Reader has electronic bookmarks,  and a built-in dictionary.

The reader has a full screen mode for reading and support for many [[Personal digital assistant|PDAs]], [[Personal digital assistant|Communicators]], and [[Smartphone]]s. [[Mobipocket]] products support most Windows, Symbian, BlackBerry and Palm operating systems, but not the Android platform. Using WINE, the reader works under Linux or Mac OS X. Third-party applications like [[Okular]] and [[FBReader]] can also be used under Linux or Mac OS X, but they work only with unencrypted files.

The Amazon Kindle's AZW format is basically just the Mobipocket format with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]), and .prc publications can be read directly on the Kindle.  The Kindle AZW format also lacks some Mobipocket features such as JavaScript.&lt;ref&gt;{{cite web|url=http://www.mobileread.com/forums/showpost.php?p=1299906&amp;postcount=2 |title=MobileRead Forums - View Single Post - Javascript in mobi ebooks? |publisher=Mobileread.com |date=2010-12-29 |accessdate=2015-08-31}}&lt;/ref&gt;

[[Amazon.com|Amazon]] has developed an .epub to .mobi converter called KindleGen,&lt;ref&gt;{{cite web|url=http://www.mobipocket.com/dev/ |title=Mobipocket Developer Center |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}&lt;/ref&gt; and it supports IDPF 1.0 and IDPF 2.0 EPUB format.

===Multimedia eBooks ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Eveda
|-
| style="background:#ddd;"| ''Published as'':
| .exe or .html
|}
A [[multimedia ebook]] is [[media (communication)|media]] and [[book]] [[content (media and publishing)|content]] that utilizes a combination of different book [[content format]]s. The term can be used as a noun (a medium with multiple content formats) or as an adjective describing a medium as having multiple content formats.

The "multimedia ebook" term is used in contrast to media which only utilize traditional forms of printed or text books. Multimedia ebooks include a combination of [[Written language|text]], [[Audio file format|audio]], [[image]]s, [[video]], or [[interactive]] content formats.  Much like how a traditional book can contain images to help the text tell a story, a multimedia ebook can contain other elements not formerly possible to help tell the story.

With the advent of more widespread tablet-like computers, such as the [[smartphone]], some publishing houses are planning to make multimedia ebooks, such as Penguin.&lt;ref&gt;[http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ ] {{webarchive |url=https://web.archive.org/web/20100617170741/http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ |date=June 17, 2010 }}&lt;/ref&gt;

===Newton eBook ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Newton eBook
|-
| style="background:#ddd;"| ''Published as'':
| .pkg
|}
Commonly known as an [[Apple Newton]] book; a single Newton package file can contain multiple books (for example, the three books of a trilogy might be packaged together). All systems running the Newton operating system (the most common include the Newton MessagePads, eMates, Siemens Secretary Stations, Motorola Marcos, Digital Ocean Seahorses and Tarpons) have built-in support for viewing Newton books. The Newton package format was released to the public by Newton, Inc. prior to that company's absorption into Apple Computer. The format is thus arguably open and various people have written readers for it (writing a Newton book converter has even been assigned as a university-level class project&lt;ref&gt;{{cite web|url=http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |accessdate=July 6, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20060904191234/http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |archivedate=September 4, 2006 }}&lt;/ref&gt;).

Newton books have no support for DRM or encryption. They do support internal links, potentially multiple tables of contents and indexes, embedded gray scale images, and even some scripting capability (for example, it's possible to make a book in which the reader can influence the outcome).&lt;ref&gt;{{cite web|url=http://tools.unna.org/wikiwikinewt/index.php/MakeNewtonEbooksIndex |title=WikiWikiNewt Undergoing Maintenance |publisher=Tools.unna.org |date= |accessdate=2015-08-31}}&lt;/ref&gt; Newton books utilize [[Unicode]] and are thus available in numerous languages. An individual [[Newton book]] may actually contain multiple views representing the same content in different ways (such as for different screen resolutions).

===Open Electronic Package===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Open eBook
|-
| style="background:#ddd;"| ''Published as'':
| .opf
|}

[[Open eBook|OPF]] is an [[XML]]-based e-book format created by E-Book Systems; it has been superseded by the EPUB electronic publication standard.

===Portable Document Format ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Portable Document Format
|-
| style="background:#ddd;"| ''Published as'':
| .[[Portable Document Format|pdf]]
|}

Invented by [[Adobe Systems]], and first released in 1993, [[PDF]] became ISO 32000 in 2008. The format was developed to provide a platform-independent means of exchanging fixed-layout documents. Derived from [[PostScript]], but without language features like loops, PDF adds support for features such as compression, passwords, semantic structures and DRM. Because PDF documents can easily be viewed and printed by users on a variety of computer [[Platform (computing)|platforms]], they are very common on the [[World Wide Web]] and in document management systems worldwide. The current PDF specification, ISO 32000-1:2008, is available from ISO's website, and under special arrangement, without charge from Adobe.&lt;ref&gt;{{cite web|url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=PDF Reference and Adobe Extensions to the PDF Specification &amp;#124; Adobe Developer Connection |publisher=Adobe.com |date=2007-01-29 |accessdate=2015-08-31}}&lt;/ref&gt;

Because the format is designed to reproduce fixed-layout pages, re-flowing text to fit mobile device and e-book reader screens has traditionally been problematic. This limitation was addressed in 2001 with the release of PDF Reference 1.5 and "Tagged PDF",&lt;ref&gt;{{cite web|author= |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 |title=What is Tagged PDF? |publisher=Planet PDF |date= |accessdate=2015-08-31}}&lt;/ref&gt; but 3rd party support for this feature was limited until the release of [[PDF/UA]] in 2012.

Many products support creating and reading PDF files, such as Adobe Acrobat, [[PDFCreator]] and [[OpenOffice.org]], and several programming libraries such as [[iText]] and [[Formatting Objects Processor|FOP]]. Third party viewers such as [[xpdf]] and [[Nitro PDF]] are also available. Mac OS X has built-in PDF support, both for creation as part of the printing system and for display using the built-in Preview application.

PDF files are supported by almost all modern e-book readers, tablets and smartphones. However, PDF reflow based on Tagged PDF, as opposed to re-flow based on the actual sequence of objects in the content-stream, is not yet commonly supported on mobile devices. Such Re-flow options as may exist are usually found under "view" options, and may be called "word-wrap".

===Plain text files ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| text
|-
| style="background:#ddd;"| ''Published as'':
| .txt
|}
The first e-books in history were in [[text file|plain text]] (.txt) format, supplied for free by the [[Project Gutenberg]] community, but the format itself existed before the e-book era. The plain text format doesn't support digital rights management (DRM) or formatting options (such as different fonts, graphics or colors), but it has excellent portability as it is the simplest e-book encoding possible as a plain text file contains only [[ASCII]] or [[Unicode]] text (text files with [[UTF-8]] or [[UTF-16]] encoding are also popular for languages other than English). Almost all operating systems can read ASCII text files (e.g. Unix, Macintosh, Microsoft Windows, DOS and other systems) and newer operating systems support Unicode text files as well. The only potential for portability problems of ASCII text files is that operating systems differ in their preferred line ending convention and their interpretation of values outside the ASCII range (their character encoding). Conversion of files from one to another line-ending convention is easy with free software. DOS and Windows uses CRLF, Unix and Apple's OS X use LF, Mac OS up to and including OS 9 uses CR. By convention, lines are often broken to fit into 80 characters, a legacy of older terminals and consoles. Alternately, each paragraph may be a single line.

The size in bytes of a text file is simply the number of characters, including spaces, and with a new line counting for 1 or 2. For example, the [[Bible]], which is approximately 800,000 words, is about 4 MB.&lt;ref name="bible"&gt;{{cite web|url=http://www.gutenberg.org/ebooks/10 |accessdate=January 10, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20081205071232/http://www.gutenberg.org/ebooks/10 |archivedate=December 5, 2008 }}&lt;/ref&gt;

===Plucker ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Plucker
|-
| style="background:#ddd;"| ''Published as'':
|.pdb
|}
[[Plucker]] is an Open Source [[free software|free]] mobile and desktop e-book reader application with its own associated file format and software to automatically generate Plucker files from text, PDF, HTML, or other document format files, web sites or RSS feeds.  The format is public and well-documented. Free readers are available for all kinds of desktop computers and many PDAs.

===PostScript ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| PostScript
|-
| style="background:#ddd;"| ''Published as'':
| .[[PostScript|ps]]
|}
[[PostScript]] is a [[page description language]] used in the electronic and [[desktop publishing]] areas for defining the contents and layout of a printed page, which can be used by a rendering program to assemble and create the actual output [[Raster graphics|bitmap]]. Many office printers directly support interpreting PostScript and printing the result. As a result, the format also sees wide use in the [[Unix]] world.&lt;!-- IE if you don't want to fool around with output filters, ghostscript, and whatnot, get a postscript printer. Most Unix programs with specialized ``print'' functions output ps anyway (pity the firefox print renderer sucks so much). Don't see a way to comment on that here so left in a comment. Would be nice if (the higher end) ebook readers would add a ps interpreter, though --&gt;

===RTF===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Rich Text Format
|-
| style="background:#ddd;"| ''Published as'':
| .[[Rich Text Format|rtf]]
|}

[[Rich Text Format]] is a [[document]] file format that is supported by many ebook readers. Its advantages as an ebook format is that it is widely supported, and it can be reflowed. It can be easily edited. It can be easily converted to other ebook formats, increasing its support.

===SSReader ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| SSReader
|-
| style="background:#ddd;"| ''Published as'':
| .pdg
|}
The digital book format used by a popular digital library company 超星数字图书馆&lt;ref&gt;[http://www.ssreader.com/downland_index.asp ]{{dead link|date=August 2015}}&lt;/ref&gt; in China.  It is a proprietary raster image compression and binding format, with reading time OCR plug-in modules.  The company scanned a huge number of Chinese books in the China National Library and this becomes the major stock of their service.  The detailed format is not published.  There are also some other commercial e-book formats used in Chinese digital libraries.

===Text Encoding Initiative ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[TEI Lite]]
|-
| style="background:#ddd;"| ''Published as'':
| .xml{{Citation needed|date=August 2009}}
|}
[[TEI Lite]] is the most{{Citation needed|date=September 2010}} popular of the [[Text Encoding Initiative|TEI]]-based (and thus [[XML]]-based or [[Standard Generalized Markup Language|SGML]]-based) electronic text formats.

===TomeRaider ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| TomeRaider
|-
| style="background:#ddd;"| ''Published as'':
| .tr2; .tr3
|}

The [[TomeRaider]] e-book format is a proprietary format. There are versions of [[TomeRaider]] for Windows, Windows Mobile (aka Pocket PC), Palm, Symbian and iPhone. Several Wikipedias are available as [[Wikipedia:TomeRaider database|TomeRaider files]] with all articles unabridged, some even with nearly all images. Capabilities of the TomeRaider3 e-book reader vary considerably per platform: the Windows and Windows Mobile editions support full [[HTML]] and [[CSS]]. The Palm edition supports limited HTML (e.g., no tables, no fonts), and CSS support is missing. For Symbian there is only the older TomeRaider2  format, which does not render images or offer category search facilities. Despite these differences any TomeRaider e-book can be browsed on all supported platforms.  The Tomeraider website&lt;ref name="tomeraider.com"&gt;{{cite web|url=http://www.tomeraider.com/ |title=tomeraider.com |publisher=tomeraider.com |date=2015-06-24 |accessdate=2015-08-31}}&lt;/ref&gt; claims to have over 4000 e-books available, including free versions of the [[Internet Movie Database]] and Wikipedia.

===Open XML Paper Specification ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| OpenXPS
|-
| style="background:#ddd;"| ''Published as'':
| [[Open XML Paper Specification|.oxps, .xps]]
|}

'''Open XML Paper Specification''' (also referred to as '''OpenXPS''') is an open [[specification]] for a [[page description language]] and a fixed-document format. [[Microsoft]] developed it as the XML Paper Specification (XPS). In June 2009, [[Ecma International]] adopted it as international standard '''ECMA-388'''.&lt;ref&gt;{{cite web|url=http://www.ecma-international.org/publications/standards/Ecma-388.htm |title=Standard ECMA-388 |publisher=Ecma-international.org |date= |accessdate=2015-08-31}}&lt;/ref&gt;

The format is intentionally restricted to sequences of:
Glyphs (a fixed run of text),
Paths (a geometry that can be filled, or stroked, by a brush), and
Brushes (a description of a shaped brush used to in rendering paths).

This reduces the possibility of inadvertent introduction of malicious content and simplifies the implementation of compatible renderers.

== Comparison tables ==

=== Features ===
{| class="wikitable sortable" style="text-align: center; width:75%;"
|-
! '''Format'''
! [[Filename extension]]
! DRM support
! Image support
! Table support
! Sound support
! Interactivity support
! [[Word wrap]] support
! [[Open standard|Open]] [[Open standard|standard]]
! Embedded annotation support
! Book- marking
! Video support
|-
| [[Comic Book Archive]]
| .cbr, .cbz, .cb7, .cbt, .cba
| ?
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
|-
| [[DjVu]]
| .djvu
| ?
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Doc (computing)|DOC]]
| .doc
| ?
| {{yes}}
| {{yes}}
| ?
| ?
| {{yes}}
| {{no}}
| ?
| ?
| ?
|-
| [[DOCX]]
| .docx
| ?
| {{yes}}
| {{yes}}
| {{yes}}
| ?
| {{yes}}
| {{no}}
| ?
| ?
| {{yes}}
|-
| [[EPUB]] (IDPF)
| .epub
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{yes}}&lt;ref group="f"&gt;With ePub 3&lt;/ref&gt;
|-
| [[FictionBook]]
| .fb2
| {{no}}
| {{yes}}
| {{yes-no}}&lt;ref group="f"&gt;Table support added in FictionBook V2.1. Not supported in V2.0&lt;/ref&gt;
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
| ?
|-
| [[HTML]]
| .html
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f" name="html5"&gt;With HTML 5&lt;/ref&gt;
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}&lt;ref group="f" name="html5" /&gt;
|-
| [[iBooks]]
| .ibook
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
|-
| [[Information Presentation Facility|INF]]
| .inf
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| ?
| {{yes}}
| {{yes}}
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{no}}
|-
| [[Amazon Kindle|Kindle]]
| .azw
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f"&gt;Supported in all except 1st Generation Kindle. (Support level is as it is in mobipocket)&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Joshua Tallent |url=http://kindleformatting.com/blog/2009/02/kindle-2-review-formatting-perspective.php |title=Kindle 2 Review, the Formatting Perspective |publisher=Kindle Formatting |date=2009-02-25 |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}&lt;ref group="f" name="iOS"&gt;Supported only in kindle for iPhone, iPod, iPad.&lt;/ref&gt;&lt;ref name="amazon.com"&gt;{{cite web|url=http://www.amazon.com/b?ie=UTF8&amp;node=2248263011 |title=Kindle Editions with Audio-Video: Kindle Store |publisher=Amazon.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f" name="iOS"/&gt;&lt;ref name="amazon.com"/&gt;
|-
| [[Microsoft Reader]]
| .lit
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| ?
| {{yes}}
| ?
|-
| [[Mobipocket]]
| .prc, .mobi
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Multimedia EBook]]
| .exe
| {{yes}}
| {{yes}}
| ?
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Newton Book]]
| .pkg
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
|-
| [[#eReader|eReader]]
| .pdb
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Plain text]]
| .txt
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
|-
| [[Plucker]]
| .pdb
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| ?
|-
| [[Portable Document Format]]
| .pdf
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes-no}}&lt;ref group="f"&gt;"Reflow" is implemented by some readers.&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.adobe.com/uk/epaper/tips/acr5reflow/ |title=Reflow the contents of Adobe PDF documents: Tutorial |publisher=Adobe.com |date=2001-04-02 |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f"&gt;With Flash Embeded&lt;/ref&gt;
|-
| [[PostScript]]
| .ps
| {{no}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| ?
| ?
| ?
|-
| [[Tome Raider]]
| .tr2, .tr3
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| ?
| ?
| ?
|-
| [[OpenXPS]]
| .oxps, .xps
| ?
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| ?
| ?
| ?
|}
&lt;references group="f"/&gt;

=== Supporting platforms ===
{| class="wikitable sortable" style="text-align: center; width:75%;"
|-
! '''Reader&amp;nbsp;'''
! Plain text
! PDF
! ePub
! HTML
! Mobi- Pocket
! Fiction- Book (Fb2)
! DjVu
! Broadband eBook (BBeB)&lt;ref group=h name=propr&gt;Proprietary format&lt;/ref&gt;
! eReader&lt;ref group=h name=propr/&gt;
! Kindle&lt;ref group=h name=propr/&gt;
! WOLF&lt;ref group=h name=propr/&gt;
! Tome Raider&lt;ref group=h name=propr/&gt;
! Open eBook&lt;ref group=h&gt;Predecessor of ePUB&lt;/ref&gt;
! Comic Book
! OpenXPS
|-
| Amazon Kindle&amp;nbsp;1
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Amazon Kindle&amp;nbsp;2,&amp;nbsp;DX
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Amazon Kindle]]&amp;nbsp;3
| {{yes}}
| {{yes}}
| {{no}}&lt;ref group=h name=3part&gt;Yes, if the Duokan alternate Kindle OS (third-party software add-on) is used.&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Kindle Fire|Amazon Kindle Fire]]
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;By adding epub capable apps, such as [[Aldiko]]&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Android Devices
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h name=firm&gt;Requires latest firmware&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://ireader.over-blog.com/ |title=iReader |publisher=Ireader.over-blog.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}&lt;ref group=h name=firm/&gt;&lt;ref&gt;{{cite web|url=https://code.google.com/p/vudroid/ |title=vudroid - Android djvu and pdf viewer - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;&lt;ref&gt;{{cite web|author= |url=http://www.barnesandnoble.com/u/nook-for-android/379002287 |title=Rise of the Android by Apps for Nook &amp;#124; 2940147132807 &amp;#124; NOOK App &amp;#124; Barnes &amp; Noble |publisher=Barnesandnoble.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;&lt;ref name="tomeraider.com"/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{dunno}}
| {{yes}}
|-
| Apple iOS Devices
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h&gt;With third party apps, such as CloudReader&lt;/ref&gt;
| {{dunno}}
|-
| Azbooka WISEreader
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Barnes &amp; Noble Nook]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Barnes &amp; Noble Nook Color]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Bookeen Cybook Gen3, Opus
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h name=epmb&gt;Versions support either ePUB or MobiPocket&lt;/ref&gt;
| {{yes}}
| {{yes}}&lt;ref group=h name=epmb/&gt;
| {{yes}}&lt;ref group=h&gt;Only ePUB version and with FW 2.0+&lt;/ref&gt;
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{dunno}}
| {{dunno}}
|-
| COOL-ER Classic
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Linux]] Operating System
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;KDE's [[Okular]] supports fb2&lt;/ref&gt;
| {{yes}}
| {{yes}}&lt;ref group=h&gt;[[Calibre (software)|Calibre]] supports lrf/lrx&lt;/ref&gt;
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{yes}}
| {{yes}}
|-
| [[eSlick|Foxit eSlick]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Hanlin e-Reader&amp;nbsp;V3
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Hanvon WISEreader
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| iRex iLiad
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Iriver Story
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Kobo eReader]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{dunno}}
|-
| Nokia N900
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{dunno}}
|-
| NUUTbook&amp;nbsp;2
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| OLPC XO, Sugar
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Onyx Boox 60
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Mac OS X
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{dunno}}
| {{yes}}
| {{yes}}
| {{dunno}}
| {{dunno}}
| {{yes}}
| {{dunno}}
| {{dunno}}
|-
| TrekStor eBook Reader Pyrus&lt;ref&gt;{{cite web|url=http://www.trekstor.co.uk/detail-ebook-reader-en/product/ebook-reader-pyrus-mini.html |title=Home - SurfTabs, smart phones, MiniPCs, data storage, MP3-Player - TrekStor GmbH |publisher=Trekstor.co.uk |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
| Windows
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;[[ICE Book Reader]] for Windows supports fb2&lt;/ref&gt;
| {{yes}}
| {{dunno}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;DRM-protected publications are supported as of Kindle for PC v1.3.0&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
| {{yes}}
| {{dunno}}
| {{yes}}&lt;ref group="h"&gt;XP or later, not on Windows 2000&lt;/ref&gt;
|-
| Pocketbook 301&amp;nbsp;Plus, 302, 360°
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Pocketbook Aqua
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Sony Reader
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Viewsonic VEB612
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Windows Phone 7
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|}
&lt;references group=h/&gt;

== See also ==
* [[Comparison of e-book readers]]
* [[Comparison of Android e-book reader software]] – includes software e-book readers for Android devices
* [[Comparison of iOS e-book reader software]] – includes software e-book readers for iOS devices
* [[ICUE]], a British company using mobile phone (cellphone) technology to deliver books and other publications

== References ==
;General information
{{Refbegin}}
* {{cite book|last=Cavanaugh|first=T W|title=The Digital Reader: Using E-Books in K-12 Education|year=2006|publisher=International Society for Technology in Education|location=Eugene, Oregon|isbn=1564842215}}
* {{cite book|last=Chandler|first=S|title=From Entrepreneur to Infopreneur: Make Money with Books, EBooks, and Information Products|year=2010|publisher=John Wiley &amp; Sons|location=Hoboken, New Jersey|isbn=1118044770}}
* Cope, B., &amp; Mason, D. (2002). Markets for electronic book products. C-2-C series, bk. 3.2. Altona, Vic: Common Ground Pub.
* {{cite book|last=Henke|first=H|title=Electronic Books and Epublishing: A Practical guide for Authors.|year=2001|publisher=Springer|location=London|isbn=1852334355}}
* Hanttula, D. (2001). Pocket PC handbook.
* {{cite book|last=Rich|first=J|title=Self-Publishing For Dummies|year=2006|publisher=John Wiley &amp; Sons|location=Hoboken, New Jersey|isbn=0470100370}}
{{Refend}}
;Footnotes
{{Reflist|30em}}

==External links==
*[http://wiki.mobileread.com/wiki/Main_Page ebook reader articles at Mobile Read Wiki]
*[http://digbib.ubka.uni-karlsruhe.de/volltexte/1000010574 Daisy 3: A Standard for Accessible Multimedia Books]
*[https://www.eff.org/deeplinks/2009/12/e-book-privacy An E-Book Buyer's Guide to Privacy]

{{Ebooks}}

{{DEFAULTSORT:Comparison Of E-Book Formats}}
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Computing comparisons]]</text>
      <sha1>s4r2grrfdviqmt2wkhdsbmckjtrgbvl</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Customer communications management</title>
    <ns>14</ns>
    <id>47769116</id>
    <revision>
      <id>747002157</id>
      <parentid>747002110</parentid>
      <timestamp>2016-10-30T21:50:03Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Customer relationship management software]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="254" xml:space="preserve">[[Category:Software]]
[[Category:Document management systems]]
[[Category:Electronic documents]]
[[Category:Customer relationship management software]]
[[Category:Information technology management]]
{{Commons category|Customer communications management}}</text>
      <sha1>mvkrnvnmrv65hb8cr8ypljlbvdd94c2</sha1>
    </revision>
  </page>
  <page>
    <title>Data paper</title>
    <ns>0</ns>
    <id>42801446</id>
    <redirect title="Data publishing" />
    <revision>
      <id>727129301</id>
      <parentid>716138266</parentid>
      <timestamp>2016-06-26T21:57:08Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="100" xml:space="preserve">#REDIRECT [[Data publishing#Paper]]


[[Category:Data publishing]]
[[Category:Electronic documents]]</text>
      <sha1>silxgqknuobdhob719e5rgz9jdqcdqb</sha1>
    </revision>
  </page>
  <page>
    <title>Digital signature</title>
    <ns>0</ns>
    <id>59644</id>
    <revision>
      <id>761554978</id>
      <parentid>760605590</parentid>
      <timestamp>2017-01-23T16:27:58Z</timestamp>
      <contributor>
        <username>Dako98</username>
        <id>29823984</id>
      </contributor>
      <comment>/* Non-repudiation */ fixed typo in "Online"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33347" xml:space="preserve">A '''digital signature'''  is a mathematical scheme for demonstrating the authenticity of digital messages or documents. A valid digital signature gives a recipient reason to believe that the message was created by a known sender ([[authentication]]), that the sender cannot deny having sent the message ([[non-repudiation]]), and that the message was not altered in transit ([[Data integrity|integrity]]).

Digital signatures are a standard element of most [[cryptographic protocol]] suites, and are commonly used for software distribution, financial transactions, [[contract management software]], and in other cases where it is important to detect forgery or tampering.

== Explanation ==
Digital signatures are often used to implement [[electronic signature]]s, a broader term that refers to any electronic data that carries the intent of a signature,&lt;ref&gt;[http://frwebgate.access.gpo.gov/cgi-bin/getdoc.cgi?dbname=106_cong_public_laws&amp;docid=f:publ229.106.pdf US ESIGN Act of 2000]&lt;/ref&gt; but not all electronic signatures use digital signatures.&lt;ref&gt;[http://enterprise.state.wi.us/home/strategic/esig.htm State of WI]&lt;/ref&gt;&lt;ref&gt;[http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html National Archives of Australia] {{webarchive |url=https://web.archive.org/web/20141109/http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html |date=November 9, 2014 }}&lt;/ref&gt; In some countries, including the United States, [[Turkey]], [[India]], Brazil, Indonesia, Saudi Arabia,&lt;ref&gt;{{cite book|first=Government of India|title=The Information Technology Act, 2000|url=http://www.dot.gov.in/sites/default/files/itbill2000_0.pdf}}&lt;/ref&gt; [[Switzerland]] and the countries of the [[European Union]],&lt;ref name=Cryptomathic_MajorStandardsDigSig&gt;{{cite web|last1=Turner|first1=Dawn|title=Major Standards and Compliance of Digital Signatures - A World-Wide Consideration|url=http://www.cryptomathic.com/news-events/blog/major-standards-and-compliance-of-digital-signatures-a-world-wide-consideration|publisher=Cryptomathic|accessdate=7 January 2016}}&lt;/ref&gt;&lt;ref name=CryptomathicDigSigServicesAshiqJA&gt;{{cite web|last1=JA|first1=Ashiq|title=Recommendations for Providing Digital Signature Services|url=http://www.cryptomathic.com/news-events/blog/recommendations-for-providing-digital-signature-services|publisher=Cryptomathic|accessdate=7 January 2016}}&lt;/ref&gt; electronic signatures have legal significance.

Digital signatures employ [[asymmetric key algorithm|asymmetric cryptography]]. In many instances they provide a layer of validation and security to messages sent through a nonsecure channel: Properly implemented, a digital signature gives the receiver reason to believe the message was sent by the claimed sender.  Digital seals and signatures are equivalent to handwritten signatures and stamped seals.&lt;ref&gt;[http://www.arx.com/industries/engineering/regulatory-compliance/ Regulatory Compliance: Digital signatures and seals are legally enforceable ESIGN (Electronic Signatures in Global and National Commerce) Act]&lt;/ref&gt; Digital signatures are equivalent to traditional handwritten signatures in many respects, but properly implemented digital signatures are more difficult to forge than the handwritten type. Digital signature schemes, in the sense used here, are cryptographically based, and must be implemented properly to be effective. Digital signatures can also provide [[non-repudiation]], meaning that the signer cannot successfully claim they did not sign a message, while also claiming their [[private key]] remains secret; further, some non-repudiation schemes offer a time stamp for the digital signature, so that even if the private key is exposed, the signature is valid. Digitally signed messages may be anything representable as a [[bitstring]]: examples include [[electronic mail]], [[contract]]s, or a message sent via some other [[cryptographic protocol]].

==Definition of Digital Signature==
{{Main article|Public-key cryptography}}
A digital signature scheme typically consists of three algorithms;
* A ''[[key generation]]'' algorithm that selects a ''private key'' [[Uniform distribution (discrete)|uniformly at random]] from a set of possible private keys. The algorithm outputs the private key and a corresponding ''public key''.
* A ''signing'' algorithm that, given a message and a private key, produces a signature.
* A ''signature verifying'' algorithm that, given the message, public key and signature, either accepts or rejects the message's claim to authenticity.

Two main properties are required.  First, the authenticity of a signature generated from a fixed message and fixed private key can be verified by using the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party without knowing that party's private key.
A digital signature is an authentication mechanism that enables the creator of the message to attach a code that acts as a signature. 
The [[Digital Signature Algorithm]] (DSA), developed by the [[National Institute of Standards and Technology]], is one of [[Digital signature#Some digital signature algorithms|many examples]] of a signing algorithm.

In the following discussion, 1&lt;sup&gt;''n''&lt;/sup&gt; refers to a [[Unary numeral system|unary number]].

Formally, a '''digital signature scheme''' is a triple of probabilistic polynomial time algorithms, (''G'', ''S'', ''V''), satisfying:
* ''G'' (key-generator) generates a public key, ''pk'', and a corresponding private key, ''sk'', on input 1&lt;sup&gt;''n''&lt;/sup&gt;, where ''n'' is the security parameter.
* ''S'' (signing) returns a tag, ''t'', on the inputs: the private key, ''sk'', and a string, ''x''.
* ''V'' (verifying) outputs ''accepted'' or ''rejected'' on the inputs: the public key, ''pk'', a string, ''x'', and a tag, ''t''. 
For correctness, ''S'' and ''V'' must satisfy

: Pr [ (''pk'', ''sk'') ← ''G''(1&lt;sup&gt;''n''&lt;/sup&gt;), ''V''( ''pk'', ''x'', ''S''(''sk'', ''x'') ) = ''accepted'' ] = 1.&lt;ref&gt;Pass, def 135.1&lt;/ref&gt;

A digital signature scheme is '''secure''' if for every non-uniform probabilistic polynomial time [[Adversary (cryptography)|adversary]], ''A''

: Pr [ (''pk'', ''sk'') ← ''G''(1&lt;sup&gt;''n''&lt;/sup&gt;), (''x'', ''t'') ← ''A''&lt;sup&gt;''S''(''sk'', · )&lt;/sup&gt;(''pk'', 1&lt;sup&gt;''n''&lt;/sup&gt;), ''x'' ∉ ''Q'', ''V''(''pk'', ''x'', ''t'') = ''accepted''] &lt; [[Negligible function|negl]](''n''),

where ''A''&lt;sup&gt;''S''(''sk'', · )&lt;/sup&gt; denotes that ''A'' has access to the [[Oracle machine|oracle]], ''S''(''sk'', · ), and ''Q'' denotes the set of the queries on ''S'' made by ''A'', which knows the public key, ''pk'', and the security parameter, ''n''. Note that we require any adversary cannot directly query the string, ''x'', on ''S''.&lt;ref&gt;Goldreich's FoC, vol. 2, def 6.1.2. Pass, def 135.2&lt;/ref&gt;

==History of Digital Signature==
In 1976, [[Whitfield Diffie]] and [[Martin Hellman]] first described the notion of a digital signature scheme, although they only conjectured that such schemes existed.&lt;ref&gt;"New Directions in Cryptography", IEEE Transactions on Information Theory, IT-22(6):644–654, Nov. 1976.&lt;/ref&gt;&lt;ref name=lysythesis&gt;"[http://theory.lcs.mit.edu/~cis/theses/anna-phd.pdf Signature Schemes and Applications to Cryptographic Protocol Design]", Anna Lysyanskaya, PhD thesis, [[Massachusetts Institute of Technology|MIT]], 2002.&lt;/ref&gt;  Soon afterwards, [[Ronald Rivest]], [[Adi Shamir]], and [[Len Adleman]] invented the [[RSA (algorithm)|RSA]] algorithm, which could be used to produce primitive digital signatures&lt;ref name="rsa"&gt;
{{cite journal
 | first = R. | last = Rivest
 | author2 = A. Shamir; L. Adleman
 | url = http://people.csail.mit.edu/rivest/Rsapaper.pdf
 | title = A Method for Obtaining Digital Signatures and Public-Key Cryptosystems
 | journal = Communications of the ACM
 | volume = 21  | issue = 2 | pages = 120–126 | year = 1978
 | doi = 10.1145/359340.359342
}}&lt;/ref&gt; (although only as a proof-of-concept &amp;ndash; "plain" RSA signatures are not secure&lt;ref&gt;For example any integer, ''r'', "signs" ''m''=''r''&lt;sup&gt;''e''&lt;/sup&gt; and the product, ''s''&lt;sub&gt;1&lt;/sub&gt;''s''&lt;sub&gt;2&lt;/sub&gt;, of any two valid signatures, ''s''&lt;sub&gt;1&lt;/sub&gt;, ''s''&lt;sub&gt;2&lt;/sub&gt; of ''m''&lt;sub&gt;1&lt;/sub&gt;, ''m''&lt;sub&gt;2&lt;/sub&gt; is a valid signature of the product, ''m''&lt;sub&gt;1&lt;/sub&gt;''m''&lt;sub&gt;2&lt;/sub&gt;.&lt;/ref&gt;). The first widely marketed software package to offer digital signature was [[Lotus Notes]] 1.0, released in 1989, which used the RSA algorithm.&lt;ref&gt;{{cite web|title=The History of Notes and Domino|url=http://www.ibm.com/developerworks/lotus/library/ls-NDHistory/|website=developerWorks|accessdate=17 September 2014}}&lt;/ref&gt;

Other digital signature schemes were soon developed after RSA, the earliest being [[Lamport signature]]s,&lt;ref&gt;"Constructing digital signatures from a one-way function.", [[Leslie Lamport]], Technical Report CSL-98, SRI International, Oct. 1979.&lt;/ref&gt; [[Merkle tree|Merkle signatures]] (also known as "Merkle trees" or simply "Hash trees"),&lt;ref&gt;"A certified digital signature", Ralph Merkle, In Gilles Brassard, ed., Advances in Cryptology – [[CRYPTO]] '89, vol. 435 of Lecture Notes in Computer Science, pp. 218&amp;ndash;238, Spring Verlag, 1990.&lt;/ref&gt; and [[Rabin signature]]s.&lt;ref&gt;"Digitalized signatures as intractable as factorization."  [[Michael O. Rabin]], Technical Report MIT/LCS/TR-212, MIT Laboratory for Computer Science, Jan. 1979&lt;/ref&gt;

In 1988, [[Shafi Goldwasser]], [[Silvio Micali]], and [[Ronald Rivest]] became the first to rigorously define the security requirements of digital signature schemes.&lt;ref name="SJC 17(2)"&gt;"A digital signature scheme secure against adaptive chosen-message attacks.", Shafi Goldwasser, Silvio Micali, and Ronald Rivest. SIAM Journal on Computing, 17(2):281&amp;ndash;308, Apr. 1988.&lt;/ref&gt; They described a hierarchy of attack models for signature schemes, and also presented the [[GMR (cryptography)|GMR signature scheme]], the first that could be proved to prevent even an existential forgery against a chosen message attack.&lt;ref name="SJC 17(2)"/&gt;

==How they work==
To create RSA signature keys, generate a RSA key pair containing a modulus, ''N'', that is the product of two large primes, along with integers, ''e'' and ''d'', such that ''e&amp;nbsp;d''&amp;nbsp;[[Modular arithmetic|≡]]&amp;nbsp;1&amp;nbsp;(mod&amp;nbsp;φ(''N'')), where φ is the [[Euler's totient function|Euler phi-function]]. The signer's public key consists of ''N'' and ''e'', and the signer's secret key contains ''d''.

To sign a message, ''m'', the signer computes a signature, σ, such that σ ≡ ''m''&lt;sup&gt;''d''&lt;/sup&gt; (mod ''N''). To verify, the receiver checks that σ&lt;sup&gt;''e''&lt;/sup&gt; ≡ ''m'' (mod ''N'').

As noted earlier, this basic scheme is not very secure. To prevent attacks, one can first apply a [[cryptographic hash function]] to the message, ''m'', and then apply the RSA algorithm described above to the result. This approach is secure assuming the hash function is a [[random oracle model|random oracle]].

Most early signature schemes were of a similar type: they involve the use of a [[trapdoor permutation]], such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite, ''n.'' A trapdoor permutation family is a family of [[permutation]]s, specified by a parameter, that is easy to compute in the forward direction, but is difficult to compute in the reverse direction without already knowing the private key ("trapdoor").  Trapdoor permutations can be used for digital signature schemes, where computing the reverse direction with the secret key is required for signing, and computing the forward direction is used to verify signatures.

Used directly, this type of signature scheme is vulnerable to a key-only existential forgery attack. To create a forgery, the attacker picks a random signature σ and uses the verification procedure to determine the message, ''m'', corresponding to that signature.&lt;ref&gt;"Modern Cryptography: Theory &amp; Practice", Wenbo Mao, Prentice Hall Professional Technical Reference, New Jersey, 2004, pg. 308.  ISBN 0-13-066943-1&lt;/ref&gt; In practice, however, this type of signature is not used directly, but rather, the message to be signed is first [[cryptographic hash function|hashed]] to produce a short digest that is then signed. This forgery attack, then, only produces the hash function output that corresponds to σ, but not a message that leads to that value, which does not lead to an attack. In the random oracle model, this [[Full domain hash|hash-then-sign]] form of signature is existentially unforgeable, even against a [[chosen-plaintext attack]].&lt;ref name=lysythesis /&gt;{{Clarify|reason=Please give a page number or theorem number.|date=September 2010}}

There are several reasons to sign such a hash (or message digest) instead of the whole document.

;For efficiency: The signature will be much shorter and thus save time since hashing is generally much faster than signing in practice.
;For compatibility: Messages are typically bit strings, but some signature schemes operate on other domains (such as, in the case of RSA, numbers modulo a composite number ''N''). A hash function can be used to convert an arbitrary input into the proper format.
;For integrity: Without the hash function, the text "to be signed" may have to be split (separated) in blocks small enough for the signature scheme to act on them directly. However, the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order.

==Notions of security==
In their foundational paper, Goldwasser, Micali, and Rivest lay out a hierarchy of attack models against digital signatures:&lt;ref name="SJC 17(2)"/&gt;

# In a ''key-only'' attack, the attacker is only given the public verification key.
# In a ''known message'' attack, the attacker is given valid signatures for a variety of messages known by the attacker but not chosen by the attacker.
# In an ''adaptive chosen message'' attack, the attacker first learns signatures on arbitrary messages of the attacker's choice.

They also describe a hierarchy of attack results:&lt;ref name="SJC 17(2)"/&gt;

# A ''total break'' results in the recovery of the signing key.
# A [[universal forgery]] attack results in the ability to forge signatures for any message.
# A [[selective forgery]] attack results in a signature on a message of the adversary's choice.
# An [[existential forgery]] merely results in some valid message/signature pair not already known to the adversary.

The strongest notion of security, therefore, is security against existential forgery under an adaptive chosen message attack.

==Applications of digital signatures==

As organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory.  The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.  Universities including Penn State, [[University of Chicago]], and Stanford are publishing electronic student transcripts with digital signatures.

Below are some common reasons for applying a digital signature to communications:

===Authentication===
Although messages may often include information about the entity sending a message, that information may not be accurate.  Digital signatures can be used to authenticate the source of messages. When ownership of a digital signature secret key is bound to a specific user, a valid signature shows that the message was sent by that user. The importance of high confidence in sender authenticity is especially obvious in a financial context. For example, suppose a bank's branch office sends instructions to the central office requesting a change in the balance of an account. If the central office is not convinced that such a message is truly sent from an authorized source, acting on such a request could be a grave mistake.

===Integrity===
In many scenarios, the sender and receiver of a message may have a need for confidence that the message has not been altered during transmission. Although encryption hides the contents of a message, it may be possible to ''change'' an encrypted message without understanding it. (Some encryption algorithms, known as [[Malleability (cryptography)|nonmalleable]] ones, prevent this, but others do not.) However, if a message is digitally signed, any change in the message after signature invalidates the signature. Furthermore, there is no efficient way to modify a message and its signature to produce a new message with a valid signature, because this is still considered to be computationally infeasible by most cryptographic hash functions (see [[collision resistance]]).

===Non-repudiation===
Non-repudiation,&lt;ref name="Cryptomathic_MajorStandardsDigSig" /&gt; or more specifically ''non-repudiation of origin'', is an important aspect of digital signatures. By this property, an entity that has signed some information cannot at a later time deny having signed it. Similarly, access to the public key only does not enable a fraudulent party to fake a valid signature.

Note that these authentication, non-repudiation etc. properties rely on the secret key ''not having been revoked ''prior to its usage.  Public revocation of a key-pair is a required ability, else leaked secret keys would continue to implicate the claimed owner of the key-pair. Checking revocation status requires an "online" check; e.g., checking a [[certificate revocation list]] or via the &lt;ref name="CryptomathicDigSigServicesAshiqJA" /&gt;[[Online Certificate Status Protocol]].   Very roughly this is analogous to a vendor who receives credit-cards first checking online with the credit-card issuer to find if a given card has been reported lost or stolen.   Of course, with stolen key pairs, the theft is often discovered only after the secret key's use, e.g., to sign a bogus certificate for espionage purpose.

==Additional security precautions==

===Putting the private key on a smart card===
All public key / private key cryptosystems depend entirely on keeping the private key secret. A private key can be stored on a user's computer, and protected by a local password, but this has two disadvantages:

* the user can only sign documents on that particular computer
* the security of the private key depends entirely on the [[computer insecurity|security]] of the computer

A more secure alternative is to store the private key on a [[smart card]]. Many smart cards are designed to be tamper-resistant (although some designs have been broken, notably by [[Ross J. Anderson (professor)|Ross Anderson]] and his students). In a typical digital signature implementation, the hash calculated from the document is sent to the smart card, whose CPU signs the hash using the stored private key of the user, and then returns the signed hash. Typically, a user must activate his smart card by entering a [[personal identification number]] or PIN code (thus providing [[two-factor authentication]]). It can be arranged that the private key never leaves the smart card, although this is not always implemented. If the smart card is stolen, the thief will still need the PIN code to generate a digital signature. This reduces the security of the scheme to that of the PIN system, although it still requires an attacker to possess the card. A mitigating factor is that private keys, if generated and stored on smart cards, are usually regarded as difficult to copy, and are assumed to exist in exactly one copy. Thus, the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked. Private keys that are protected by software only may be easier to copy, and such compromises are far more difficult to detect.

===Using smart card readers with a separate keyboard===
Entering a PIN code to activate the smart card commonly requires a [[numeric keypad]]. Some card readers have their own numeric keypad. This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer's keyboard. Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a [[keystroke logging|keystroke logger]], potentially compromising the PIN code. Specialized card readers are also less vulnerable to tampering with their software or hardware and are often [[Evaluation Assurance Level|EAL3]] certified.

===Other smart card designs===
Smart card design is an active field, and there are smart card schemes which are intended to avoid these particular problems, though so far with little security proofs.

===Using digital signatures only with trusted applications===
One of the main differences between a digital signature and a written signature is that the user does not "see" what he signs. The user application presents a hash code to be signed by the digital signing algorithm using the private key. An attacker who gains control of the user's PC can possibly replace the user application with a foreign substitute, in effect replacing the user's own communications with those of the attacker. This could allow a malicious application to trick a user into signing any document by displaying the user's original on-screen, but presenting the attacker's own documents to the signing application.

To protect against this scenario, an authentication system can be set up between the user's application (word processor, email client, etc.) and the signing application. The general idea is to provide some means for both the user application and signing application to verify each other's integrity. For example, the signing application may require all requests to come from digitally signed binaries.

===Using a network attached [[hardware security module]]===
One of the main differences between a [[cloud]] based digital signature service and a locally provided one is risk.  Many risk averse companies, including governments, financial and medical institutions,  and payment processors require more secure standards, like [[FIPS 140-2]] level 3 and [[FIPS 201]] certification, to ensure the signature is validated and secure.&lt;ref&gt;[http://www.arx.com/products/privateserver-hsm/overview/ PrivateServer HSM Overview]&lt;/ref&gt; &lt;!--To finish: current and future applications, actual algorithms, standards, why not as adopted as widely as expected, etc.--&gt;

===WYSIWYS===
{{Main article|WYSIWYS}}
Technically speaking, a digital signature applies to a string of bits, whereas humans and applications "believe" that they sign the semantic interpretation of those bits. In order to be semantically interpreted, the bit string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system. The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content. It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed. From a semantic perspective this creates uncertainty about what exactly has been signed. [[WYSIWYS]] (What You See Is What You Sign) &lt;ref name=WYSIWYS_SeminalPaper&gt;{{cite journal|last1=Landrock|first1=Peter|last2=Pedersen|first2=Torben|title=WYSIWYS? -- What you see is what you sign?|journal=Information Security Technical Report|date=1998|volume=3|issue=2|pages=55–61}}&lt;/ref&gt; means that the semantic interpretation of a signed message cannot be changed. In particular this also means that a message cannot contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied. WYSIWYS is a necessary requirement for the validity of digital signatures, but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems. The term WYSIWYS was coined by [[Peter Landrock]] and [[Cryptomathic|Torben Pedersen]] to describe some of the principles in delivering secure and legally binding digital signatures for Pan-European projects.&lt;ref name=WYSIWYS_SeminalPaper /&gt;

===Digital signatures versus ink on paper signatures===

An ink signature could be replicated from one document to another by copying the image manually or digitally, but to have credible signature copies that can resist some scrutiny is a significant manual or technical skill, and to produce ink signature copies that resist professional scrutiny is very difficult.

Digital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document. Paper contracts sometimes have the ink signature block on the last page, and the previous pages may be replaced after a signature is applied.  Digital signatures can be applied to an entire document, such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered, but this can also be achieved by signing with ink and numbering all pages of the contract.

==Some digital signature algorithms==
*[[RSA (algorithm)|RSA]]-based signature schemes, such as [[RSA-PSS]]
*[[Digital Signature Algorithm|DSA]] and its [[elliptic curve cryptography|elliptic curve]] variant [[Elliptic Curve Digital Signature Algorithm|ECDSA]]
*[[EdDSA|Edwards-curve Digital Signature Algorithm]] and its [[EdDSA#Ed25519|Ed25519]] variant.
*[[ElGamal signature scheme]] as the predecessor to DSA, and variants [[Schnorr signature]] and [[Pointcheval–Stern signature algorithm]]
*[[Rabin signature algorithm]]
*[[Pairing]]-based schemes such as [[Boneh–Lynn–Shacham|BLS]]
*[[Undeniable signature]]s
*[[Aggregate signature]] - a signature scheme that supports aggregation: Given n signatures on n  messages from n users, it is possible to aggregate all these signatures into a single signature whose size is constant in the number of users. This single signature will convince the verifier that the n users did indeed sign the n original messages.
*[[Signatures with efficient protocols]] - are signature schemes that facilitate efficient cryptographic protocols such as [[zero-knowledge proofs]] or [[secure computation]].

==The current state of use &amp;ndash; legal and practical ==
{{Globalize|section|date=November 2009}}
All digital signature schemes share the following basic prerequisites regardless of cryptographic theory or legal provision:

#;Quality algorithms: Some public-key algorithms are known to be insecure, as practical attacks against them having been discovered.
#
#; Quality implementations: An implementation of a good algorithm (or [[cryptographic protocol|protocol]]) with mistake(s) will not work.
#
#; Users (and their software) must carry out the signature protocol properly.
#
#; The private key must remain private: If the private key becomes known to any other party, that party can produce ''perfect'' digital signatures of anything whatsoever.
#
#; The public key owner must be verifiable: A public key associated with Bob actually came from Bob. This is commonly done using a [[public key infrastructure]] (PKI) and the public key↔user association is attested by the operator of the PKI (called a [[certificate authority]]). For 'open' PKIs in which anyone can request such an attestation (universally embodied in a cryptographically protected  [[identity certificate]]), the possibility of mistaken attestation is non-trivial. Commercial PKI operators have suffered several publicly known problems. Such mistakes could lead to falsely signed, and thus wrongly attributed, documents. 'Closed' PKI systems are more expensive, but less easily subverted in this way.

Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.

Legislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in [[Utah]] in the United States, followed closely by the states [[Massachusetts]] and [[California]]. Other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time. These enactments (or proposed enactments) vary from place to place, have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying [[cryptographic engineering]], and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable. Adoption of technical standards for digital signatures have lagged behind much of the legislation, delaying a more or less unified engineering position on [[interoperability]], [[algorithm]] choice, [[key length]]s, and so on what the engineering is attempting to provide.

{{see also|ABA digital signature guidelines}}

==Industry standards==
{{unreferenced section|date=January 2015}}
Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the [[Automotive Network Exchange]] for the automobile industry and the [[SAFE-BioPharma Association]] for the healthcare industry.

===Using separate key pairs for signing and encryption===
In several countries, a digital signature has a status somewhat like that of a traditional pen and paper signature, like in the [http://europa.eu/legislation_summaries/information_society/l24118_en.htm EU digital signature legislation].&lt;ref name=Cryptomathic_MajorStandardsDigSig /&gt; Generally, these provisions mean that anything digitally signed legally binds the signer of the document to the terms therein. For that reason, it is often thought best to use separate key pairs for encrypting and signing. Using the encryption key pair, a person can engage in an encrypted conversation (e.g., regarding a real estate transaction), but the encryption does not legally sign every message he sends. Only when both parties come to an agreement do they sign a contract with their signing keys, and only then are they legally bound by the terms of a specific document. After signing, the document can be sent over the encrypted link.  If a signing key is lost or compromised, it can be revoked to mitigate any future transactions.  If an encryption key is lost, a backup or [[key escrow]] should be utilized to continue viewing encrypted content.  Signing keys should never be backed up or escrowed unless the backup destination is securely encrypted.

==See also==
* [[21 CFR 11]]
* [[Blind signature]]
* [[Detached signature]]
* [[Digital certificate]]
* [[Digital signature in Estonia]]
* [[Electronic lab notebook]]
* [[Electronic signature]]
* [[Electronic signatures and law]]
* [[eSign (India)]]
* [[GNU Privacy Guard]]
* [[Global Trust Center]]
* [[PAdES]]
* [[Public key infrastructure]]
* [[Server-based signatures]]

==Notes==
{{Reflist}}

==References==
*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography I: Basic Tools|date=2001|publisher=Cambridge University Press|location=Cambridge|isbn=978-0-511-54689-1}}
*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography II: Basic Applications|date=2004|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=978-0-521-83084-3|edition=1. publ.}}
*{{citation|last1=Pass|first1=Rafael|title=A Course in Cryptography|url=https://www.cs.cornell.edu/courses/cs4830/2010fa/lecnotes.pdf|accessdate=31 December 2015}}

==Further reading==
* J. Katz and Y. Lindell, "Introduction to Modern Cryptography" (Chapman &amp; Hall/CRC Press, 2007)
* Stephen Mason, Electronic Signatures in Law (4th edition, Institute of Advanced Legal Studies for the SAS Digital Humanities Library, School of Advanced Study, University of London, 2016). ISBN 978-1-911507-00-0.
* Lorna Brazell, Electronic Signatures and Identities Law and Regulation (2nd edn, London: Sweet &amp; Maxwell, 2008);
* Dennis Campbell, editor, E-Commerce and the Law of Digital Signatures (Oceana Publications, 2005).
* M. H. M Schellenkens, Electronic Signatures Authentication Technology from a Legal Perspective, (TMC Asser Press, 2004).
* Jeremiah S. Buckley, John P. Kromer, Margo H. K. Tank, and R. David Whitaker, The Law of Electronic Signatures (3rd Edition, West Publishing, 2010).
* [http://journals.sas.ac.uk/deeslr/ ''Digital Evidence and Electronic Signature Law Review''] Free open source

{{Cryptography navbox | public-key}}

{{DEFAULTSORT:Digital Signature}}
[[Category:Public-key cryptography]]
[[Category:Electronic documents]]
[[Category:Key management]]
[[Category:Notary]]
[[Category:Signature]]
[[Category:Records management technology]]</text>
      <sha1>kmvmxqlcysvqr83qc5e1ettxhkhde5b</sha1>
    </revision>
  </page>
  <page>
    <title>Arts and Humanities Citation Index</title>
    <ns>0</ns>
    <id>2209985</id>
    <revision>
      <id>723409022</id>
      <parentid>723231144</parentid>
      <timestamp>2016-06-02T21:50:43Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/223.176.143.79|223.176.143.79]] ([[User talk:223.176.143.79|talk]]) to last revision by DavidLeighEllis. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4426" xml:space="preserve">{{ infobox bibliographic database
| image       = 
| caption     = 
| producer    =Thomson Reuters 
| country     =United States 
| history     = 
| languages   = 
| providers   =Web of Science, Dialog Bluesheets 
| cost        =Subscription 
| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio 
| depth       =Index, abstract, citation indexing, author 
| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances 
| temporal    =1975 to present 
| geospatial  =global 
| number      = 
| updates     = 
| p_title     = 
| p_dates     = 
| ISSN        = 
| web         = 
| titles      =  
}}

The '''''Arts &amp; Humanities Citation Index''''' ('''A&amp;HCI'''), also known as '''''Arts &amp; Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore, the print counterpart is Current Contents.

Subjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio.

Available citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances.

This database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.

According to Thomson Reuters, the ''Arts &amp; Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.&lt;ref name=dialog-blue&gt;
{{Cite web
  | title =Arts &amp; Humanities Search (File 255) 
  | publisher =Dialog bluesheets  
  | date = 
  | url =http://library.dialog.com/bluesheets/html/bl0439.html 
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=Iowa&gt;
Description of Arts &amp; Humanities Search. 
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/artshm.html
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=Iowa-wos&gt;
Description of Web of Science coverage.  
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/websci.html
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=TR&gt;
See the page entitled "Tech Specs" 
{{Cite web
  | title =Database description
  | publisher =Thomson Reuters  
  | year = 
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;

==History==
The index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP &amp; Science division.

==See also==
* [[Science Citation Index]]
* [[Social Sciences Citation Index]]

==References==
{{Reflist}}

== External links ==
* {{Official website|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.
* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.

{{Thomson Reuters}}

{{DEFAULTSORT:Arts And Humanities Citation Index}}
[[Category:Citation indices]]
[[Category:Thomson Reuters]]
[[Category:Arts journals| ]]
[[Category:Humanities journals| ]]</text>
      <sha1>hjsex5qbo1zc6yyfo7ukm1dftouu8tp</sha1>
    </revision>
  </page>
  <page>
    <title>Scopus</title>
    <ns>0</ns>
    <id>582311</id>
    <revision>
      <id>760515008</id>
      <parentid>760514950</parentid>
      <timestamp>2017-01-17T13:45:56Z</timestamp>
      <contributor>
        <username>Sandbergja</username>
        <id>15752169</id>
      </contributor>
      <minor />
      <comment>/* Overview */ removing dead link as a result of my last edit -- Sorry!</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4165" xml:space="preserve">{{Use dmy dates|date=August 2013}}
{{other uses}}
{{infobox bibliographic database
| title = Scopus
| image = [[File:Scopus_type_logo.jpg]]
| caption = 
| producer = [[Elsevier]]
| country = 
| history = 
| languages = English
| providers = 
| cost = Subscription
| disciplines= 
| depth = 
| formats = 
| temporal = 1995-present
| geospatial = Worldwide
| number = 55 million
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://www.scopus.com
| titles = 
}}
'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).&lt;ref&gt;{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}&lt;/ref&gt; It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.&lt;ref&gt;{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092–6 |pmid=19738094}}&lt;/ref&gt;

==Overview==

Since Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.&lt;ref&gt;{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}&lt;/ref&gt; The board consists of scientists and subject librarians.

Evaluating ease of use and coverage of Scopus and the [[Web of Science]] (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. ... The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive."&lt;ref&gt;{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}&lt;/ref&gt;

Scopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].

Scopus IDs for individual authors can be integrated with the nonproprietary digital identifier [[ORCID]].&lt;ref name="Scopus"&gt;{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}&lt;/ref&gt;

==See also==
*[[Source Normalized Impact per Paper]]
*[[Web of Science]]

== References ==
{{reflist|30em}}

== External links ==
{{Wikidata property|P1153}}
{{Wikidata property|P1154}}
{{Wikidata property|P1155}}
{{Wikidata property|P1156}}
* {{Official website|http://www.scopus.com/}}

{{Reed Elsevier}}

[[Category:Bibliographic databases and indexes]]
[[Category:Elsevier]]
[[Category:Citation indices]]
[[Category:Library cataloging and classification]]</text>
      <sha1>ar5hsr1oqzyai5np4kltdlfcnz9x5x4</sha1>
    </revision>
  </page>
  <page>
    <title>Islamic World Science Citation Database</title>
    <ns>0</ns>
    <id>24783829</id>
    <revision>
      <id>735707083</id>
      <parentid>678205790</parentid>
      <timestamp>2016-08-22T15:38:05Z</timestamp>
      <contributor>
        <username>TheStrayDog</username>
        <id>19920863</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1769" xml:space="preserve">'''Islamic World Science Citation Database''' ('''ISC''') is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].

It was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.&lt;ref&gt;{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}&lt;/ref&gt;  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].

In 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.&lt;ref&gt;{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}&lt;/ref&gt;

== References ==
{{Reflist}}

==See also==
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Impact factor]]

== External links ==
* {{Official website|http://isc.gov.ir/Default.aspx?lan=en}}

[[Category:Bibliographic databases and indexes]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Research management]]
[[Category:Databases in Iran]]
[[Category:Science and technology in Iran]]


{{science-journal-stub}}
{{islam-stub}}
{{iran-stub}}</text>
      <sha1>ih612n610hm4psztpcn7slqnhurofh1</sha1>
    </revision>
  </page>
  <page>
    <title>Citation index</title>
    <ns>0</ns>
    <id>423362</id>
    <revision>
      <id>760499794</id>
      <parentid>760499762</parentid>
      <timestamp>2017-01-17T10:49:48Z</timestamp>
      <contributor>
        <username>Cyrus noto3at bulaga</username>
        <id>29357210</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/37.105.71.72|37.105.71.72]] ([[User talk:37.105.71.72|talk]]): Uncapitilized. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4721" xml:space="preserve">{{distinguish|Citation metric}}

A '''citation index''' is a kind of [[bibliographic index]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]] and Elsevier's [[Scopus]].

==History==
The earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study. The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.&lt;ref&gt;Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998, p. 51''ff''&lt;/ref&gt;&lt;ref&gt;Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''&lt;/ref&gt; Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.

In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.&lt;ref name='shapiro'/&gt;

The first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].&lt;ref name='shapiro'&gt;Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)&lt;/ref&gt;

==Major citation indexing services==
{{main article|Indexing and abstracting service}}
{{main cat|Citation indices}}
General-purpose academic citation indexes include:
*[[Web of Science]] by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]])
*[[Scopus]] by [[Elsevier]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].
*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.
Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: Web of Science and Scopus are available by subscription (generally to libraries).

In addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.

==See also==
* [[Microsoft Academic Search]]
* [[Google Scholar]]
* [[Scopus]]
* [[Semantic Scholar]]
* [[Citation analysis]]
* [[Acknowledgment index]]
* [[CiteSeer]]
* [[CiteSeerX]]
* [[Scientific journal]]
* [[Science Citation Index]]
* [[Indian Citation Index]]
* [[Journal Citation Reports]]
* [[Emerging Sources Citation Index (ESCI)]]
* [[SciELO]]
* [[Redalyc]]
* [[Index Copernicus]]

==References==
{{Reflist}}

{{DEFAULTSORT:Citation Index}}
[[Category:Academic publishing]]
[[Category:Bibliometrics]]
[[Category:Bibliographic databases and indexes]]
[[Category:Reputation management]]
[[Category:Citation indices| ]]</text>
      <sha1>ja0p5y3d5cl9rl2p54m400qbaulshym</sha1>
    </revision>
  </page>
  <page>
    <title>Kelly's Directory</title>
    <ns>0</ns>
    <id>3119155</id>
    <revision>
      <id>745141262</id>
      <parentid>745140482</parentid>
      <timestamp>2016-10-19T13:50:50Z</timestamp>
      <contributor>
        <username>Nick Cooper</username>
        <id>1982981</id>
      </contributor>
      <comment>Rolling back to pre-vandalism version.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6758" xml:space="preserve">'''Kelly's Directory''' (or more formally, the '''Kelly's, Post Office and Harrod &amp; Co Directory''') was a [[trade directory]] in the United Kingdom that listed all businesses and tradespeople in a particular city or town, as well as a general directory of postal addresses of local [[gentry]], landowners, charities, and other facilities.  In effect, it was a Victorian version of today's [[Yellow Pages]].&lt;ref&gt;{{cite web|url=http://www.cottinghamhistory.co.uk/Directories.htm|title=Cottingham History|accessdate=11 July 2010}}&lt;/ref&gt;  Many reference libraries still keep their copies of these directories, which are now an important source for historical research.

==Origins==
The eponymous originator of the directory was [[Frederic Festus Kelly]].  In 1835 or 1836 he became chief inspector of letter-carriers for the inland or general post office, and took over publication of the Post Office London Directory, whose copyright was in private hands despite its semi-official association with the post office, and which Kelly had to purchase from the widow of his predecessor.

He founded Kelly &amp; Co. and he and various family members gradually expanded the company over the next several decades, producing directories for an increasing number of UK [[county|counties]] and buying out or putting out of business various competing publishers of directories.&lt;ref name="pollard"/&gt;&lt;ref&gt;http://www.huthwaite-online.net/hucknall/gazetteers/&lt;/ref&gt;

Other publications followed, including the ''Handbook to the Titled, Landed and Official Classes'' (1875) and ''Merchants, Manufacturers and Shippers'' (1877). In 1897, Kelly &amp; Co Ltd became '''Kelly’s Directories Ltd.'''&lt;ref name="lg"/&gt;  This name stuck for another 106 years before being renamed Kellysearch in 2003 to reflect its focus away from hard copy directories and towards an Internet-based product search engine.

The front cover of a Kelly's Directory sometimes stated "Kelly's Directories Ltd., established 1799",&lt;ref&gt;{{cite web|title=Trade Directories|work=Stella &amp; Rose's Books|url=http://www.stellabooks.com/articles/trade_directories.php|accessdate=2011-03-28}}&lt;/ref&gt; however this was based on the date of issue of the first Post Office London Directory by an earlier inspector of letter carriers several decades before Kelly's involvement with that publication.&lt;ref name="jenorton" /&gt;

== Kellysearch ==
For a short time, Kelly's existed online as [http://www.kellysearch.co.uk/ Kellysearch (broken link)], a directory similar to the online [[Yellow Pages]]. Kellysearch.com was established in Boston in 2004. It was in many different languages and introduced a fully searchable online-catalogue library and product [[News release|press release]] section.

The old editions of the Kelly’s Directories are seen as highly collectable by many and have also become a useful reference tool for people tracing the history of local areas (with the ancient data now available to buy on CD Rom from many entrepreneurial sources for this purpose.)  Every edition of the Kelly’s Directory ever published is held in the [[Guildhall Library]]&lt;ref&gt;[http://www.cityoflondon.gov.uk/things-to-do/archives-and-city-history/guildhall-library/Documents/8-trade-directories-at-guildhall-library.pdf Trade directories and telephone books at Guildhall Library]&lt;/ref&gt; in [[London]].

==References==
{{reflist | refs=

&lt;ref name="jenorton"&gt;{{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | author=Jane Elizabeth Norton | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293–299 | quote=The Post Office London Directory was started by two inspectors of the Inland letter-carriers called Ferguson and Sparkes… A third inspector, called B. Critchett, joined the enterprise in 1803 and later it was carried on by Critchett alone, then by Critchett and Woods, and then again by Critchett alone until his death in 1835. ''[sic; he died 18 September 1836]''}}&lt;!-- Library (1966) s5-XXI(4): 293-299 --&gt;&lt;/ref&gt;

&lt;ref name="pollard"&gt;{{cite book | title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83–84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785 | quote=The first directories of counties outside London were published by Kelly in 1845; and during the next sixteen years the series was extended throughout England. In 1892 Kelly's Directories Ltd. acquired the majority of shares in [[Isaac Slater]] Ltd. [...]; and the firm of [[White's Directories|William White]] of Sheffield [...] was absorbed in 1898.}}&lt;/ref&gt;

&lt;ref name="lg"&gt;{{LondonGazette|issue=26876|date=23 July 1897|startpage=4149}}&lt;/ref&gt;

}}

==Bibliography==
*{{cite book| title=Guide to the national and provincial directories of England and Wales, excluding London, published before 1856 | author=Jane Elizabeth Norton | year=1950 | edition=1984 reprint | publisher=Offices of the Royal Historical Society | isbn=0-86193-102-5}} (original edition: ISBN 0-901050-15-6)
*{{cite book| title=The development and growth of city directories| author=A. V. Williams| year=1913 | publisher=Williams directory co.}}
*{{cite book| title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83–84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785}}
*{{cite book| title=The directories of London, 1677-1977| author=Peter J. Atkins | publisher=Cassell and Mansell | year=1990
| isbn=0-7201-2063-2}}

==External links==
{{Commons category|Kelly's Directory}}
* [http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4/hd/ Historical Directories] has extensive online versions of old editions for England and Wales
* [http://forebears.co.uk/news/kellys-directories-project-complete#kellys Forebears] has transcriptions of one edition for each county
* {{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | last=Norton | first=Jane Elizabeth | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293–299}}
* {{cite journal | journal=The London Journal | title=The Compilation and Reliability of London Directories | last=Atkins | first=Peter J. | volume=14 | issue=1 |date=May 1989 | pages=17–28 | publisher=Maney Publishing | issn=0305-8034 | url=http://www.ingentaconnect.com/content/maney/ldn/1989/00000014/00000001/art00002 | doi=10.1179/ldn.1989.14.1.17}}

{{Reed Elsevier}}

[[Category:Directories]]
[[Category:Waltham, Massachusetts]]</text>
      <sha1>gjgessssei1djzcb3x3cgkg1755x2lf</sha1>
    </revision>
  </page>
  <page>
    <title>CBD Media</title>
    <ns>0</ns>
    <id>4859787</id>
    <revision>
      <id>762771687</id>
      <parentid>743397455</parentid>
      <timestamp>2017-01-30T17:27:10Z</timestamp>
      <contributor>
        <username>ColumnInch</username>
        <id>29554328</id>
      </contributor>
      <comment>removed dead links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1940" xml:space="preserve">{{Infobox company |
 name = CBD Media LLC|
 logo = [[Image:CBD Media logo.jpg]]|
 type = Subsidiary of [[The Berry Company]]|
 slogan = |
 foundation = |
 location = |
 industry = [[Telephone directory]]|
 parent = [[Spectrum Equity]], etc. (2002-2007)&lt;br&gt;[[The Berry Company|Local Insight Media/Berry]] (2007-present)|
 products = Print Yellow Pages, Online Yellow Pages ads|
}}
'''CBD Media LLC''' (formerly '''Cincinnati Bell Directory''') is a division of Local Insight Media that publishes telephone directories under the [[Cincinnati Bell]] name. The company was created in 2002 following the sale of Cincinnati Bell Directory to a consortium led by [[Spectrum Equity]].

CBD Media publishes the '''Cincinnati Bell Yellow Pages''', which consists of 15 directories, published under the "Real Pages" name. CBD Media also operates [http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com], the [[electronic yellow pages]] directory for [[Cincinnati Bell]].

The company was acquired by Local Insight Media Holdings in 2007.&lt;ref&gt;[http://www.spectrumequity.com/investments/investment?Id=1289 Spectrum Equity | Investments]&lt;/ref&gt; Local Insight Media owned [[Local Insight Yellow Pages]], the former directory division of [[Windstream]]. In 2009, Local Insight acquired The Berry Company from [[AT&amp;T]], and changed its own name to '''The Berry Company LLC'''.

==See also==
*[[Engels Maps]]

==External links==
*[http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com]

==References==
{{reflist}}

{{Telephone directory publishers in the United States}}
{{Cincinnati Bell}}

[[Category:Advertising agencies of the United States]]
[[Category:Directories]]
[[Category:Media in Cincinnati]]
[[Category:Publishing companies established in 2002]]
[[Category:Cincinnati Bell]]
[[Category:Publishing companies of the United States]]
[[Category:Companies based in Cincinnati]]
[[Category:2002 establishments in Ohio]]</text>
      <sha1>kp5qut5mh3izmjm5txw0pr36y9newhb</sha1>
    </revision>
  </page>
  <page>
    <title>World Radio TV Handbook</title>
    <ns>0</ns>
    <id>1536556</id>
    <revision>
      <id>745298325</id>
      <parentid>620207475</parentid>
      <timestamp>2016-10-20T10:03:23Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Publications */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3122" xml:space="preserve">{{redirect|WRTH|the radio station|WRTH (FM)}}
The '''''World Radio TV Handbook''''', also known as '''''WRTH''''', is a [[directory (databases)|directory]] of virtually every [[Radio station|radio]] and [[TV station]] on Earth, published yearly. It was started in 1947 by [[Oluf Lund Johansen]] (1891–1975) as the ''World Radio Handbook'' (WRH).&lt;ref&gt;[http://oz6gh.byethost33.com/lund_johansen.htm O. Lund-Johansen], presented by OZ6GH.&lt;/ref&gt; The word "TV" was added to the title in 1965, when [[Jens M. Frost]] (1919–1999) took over as editor.&lt;ref&gt;[http://www.dswci.org/specials/membersofhonour/jens_frost.html DSWCI Member of Honour:  Jens M. Frost]&lt;/ref&gt; It had then already included data for [[television broadcasting]] for some years. After the 40th edition in 1986, Frost handed over editorship to [[Andrew G. Sennitt|Andrew G. (Andy) Sennitt]].&lt;ref&gt;[http://www.agsmedia.nl/body_who.html Andy Sennitt], own presentation.&lt;/ref&gt;

The first edition that bears an edition number is the 4th edition, published in 1949. The three previous editions appear to have been:
* the 1st edition, marked "Winter Ed. 1947" on the cover and completed in November 1947
* the 2nd edition, marked "1948 (May-November)" on the cover and completed in May 1948
* the 3rd edition, marked "1948-49" on the cover and completed in November 1948.

Summer Supplements appear to have been issued from 1959 through 1971. From 1959 through 1966 they were called the Summer Supplement. From 1967 through 1971 they were called the Summer Edition.

Through the 1969 edition, the WRTH indicated the date on which the manuscript was completed.

Issues with covers in Danish are known to have been available for the years 1948 May-November (2d ed.), 1950-51 (5th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), 1952 (6th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), and probably others. The 1952 English ed., which is completely in English, has an extra page with world times and agents, and ads in English which are sometimes different from the ads in the Danish edition. Also, the 1953 ed. mentions the availability of a German edition.

[[Oluf Lund Johansen]] published, in conjunction with [[Libreria Hispanoamericana]] of [[Barcelona]], Spain, a [[softbound]] Spanish-language version of the 1960 WRTH. The book was printed in Spain and called ''Guia Mundial de Radio y Television'', and carried the WRTH logo at the time as well as all the editorial references contained in the English-language version. 

Hardbound editions are known to have been available for the years 1963 through 1966, 1968, 1969, and 1975-1978, and probably others.

== Publications ==
* Gilbert, Sean; Nelson, John; Jacobs, George, [https://books.google.com/books?id=IBu8NHvC4fMC&amp;printsec=frontcover ''World Radio TV Handbook 2007''], Watson-Guptill, 2006. ISBN 0-9535864-9-9.

==References==
{{reflist}}

== External links ==
* http://www.wrth.com/

[[Category:Radio organizations]]
[[Category:Television organizations]]
[[Category:International broadcasting]]
[[Category:Directories]]
[[Category:1945 introductions]]</text>
      <sha1>3gtsdqcx7h3ctetf090q0i6u3k6n213</sha1>
    </revision>
  </page>
  <page>
    <title>Oregon Blue Book</title>
    <ns>0</ns>
    <id>3214053</id>
    <revision>
      <id>728647156</id>
      <parentid>708833450</parentid>
      <timestamp>2016-07-06T18:11:18Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5802" xml:space="preserve">{{Infobox book 
| name          = Oregon Blue Book
| image         = OrBlueBookCover.png
| caption       = Cover of the 2005 edition
| editor        = [[Oregon Secretary of State]]
| country       = United States
| language      = English
| subject       = Oregon history, government
| genre         = Reference
| published     = Biennially, 1911–present
| media_type    = Print, online
| isbn          = 
| external_url  = http://bluebook.state.or.us/
}}
The '''''Oregon Blue Book''''' is the official directory and fact book for the U.S. state of [[Oregon]] prepared by the [[Oregon Secretary of State]]&lt;ref name="ORS"&gt;{{cite web|url = https://www.oregonlegislature.gov/bills_laws/ors/ors177.html|title = ORS 177.120|publisher = [[Oregon Legislative Counsel]]|accessdate = February 16, 2015}}&lt;/ref&gt; and published by the Office of the Secretary's [[Oregon State Archives|Archives Division]].

The ''Blue Book'' comes in both print and online editions. The [[Oregon Revised Statutes]] require the Secretary of State to publish the print edition "biennially on or about February 15 of the same year as the regular sessions of the [[Oregon Legislative Assembly|Legislative Assembly]],"&lt;ref name=ORS/&gt; which are during odd-numbered years; it has been so published since 1911.  The online edition is updated regularly.&lt;ref name=About&gt;{{cite web |url= http://bluebook.state.or.us/misc/about/about.htm |title= About the Oregon Blue Book |publisher= Oregon Secretary of State |accessdate= February 16, 2015}}&lt;/ref&gt;

==Contents==
The book contains information on the state, city, county, and federal governments in Oregon, educational institutions, finances, the economy, resources, population figures and demographics.&lt;ref name=ERG83&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19830410&amp;id=jP5VAAAAIBAJ&amp;sjid=UeIDAAAAIBAJ&amp;pg=3537,2331038 |title= New Oregon Blue Book Published |author= [[United Press International]] |date= April 10, 1983 |newspaper= [[The Register-Guard]] |accessdate= February 16, 2015}}&lt;/ref&gt;

The 1919 edition contained a "statement of registered motor vehicles, chauffeurs, and dealers from 1905 to 1919", and "a general summary of in the taxable property in Oregon from 1858 to 1918".&lt;ref name=Received&gt;{{cite news |url= https://news.google.com/newspapers?nid=1243&amp;dat=19190911&amp;id=0NgsAAAAIBAJ&amp;sjid=HCAEAAAAIBAJ&amp;pg=3731,5589153 |date= September 11, 1919 |title= Blue Book is Received Here |newspaper= [[The Bulletin (Bend)|The Bulletin]] |accessdate= February 17, 2015}}&lt;/ref&gt;

==History==
Secretary of State [[Ben Olcott]] published the first edition in 1911 in response to an "increased demand for information of a general character concerning Oregon".&lt;ref name=Indispensable&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19950525&amp;id=4ERWAAAAIBAJ&amp;sjid=7OoDAAAAIBAJ&amp;pg=3777,5960904 |title= Blue Book Indispensable |newspaper= The Register-Guard |date= May 25, 1995 |accessdate= February 16, 2015}}&lt;/ref&gt;

Early editions of the book were available free from the State.&lt;ref name=Received/&gt; By 1937, copies cost 25; in 1981 the book cost $4.&lt;ref&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19370921&amp;id=dMpYAAAAIBAJ&amp;sjid=Q-gDAAAAIBAJ&amp;pg=2452,1169200 |title= Oregon Blue Book Being Distributed |author= United Press International |date= September 21, 1937 |newspaper= The Register-Guard |accessdate= February 17, 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite news |url= https://news.google.com/newspapers?nid=1243&amp;dat=19810427&amp;id=tl0zAAAAIBAJ&amp;sjid=EPcDAAAAIBAJ&amp;pg=3755,5317981 |title= Oregon Blue Book Makes Biennial Appearance |author= United Press International |date= April 27, 1981 |newspaper= The Bulletin |accessdate= February 17, 2015}}&lt;/ref&gt;

In 1953, a legislative ways and means subcommittee, headed by Representative [[Francis Ziegler]], was going to confer with Secretary of State [[Earl T. Newbry]] about how to improve the ''Blue Book''.&lt;ref name=Revision&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19530323&amp;id=URZWAAAAIBAJ&amp;sjid=veIDAAAAIBAJ&amp;pg=5600,1181962 |title= Legislative Group to Study Revision of 'Blue Book' |date= March 23, 1953 |newspaper= The Register-Guard |author= United Press International |accessdate= February 16, 2015}}&lt;/ref&gt; This was following complaints by Representative [[Monroe Sweetland]] that the book was "obsolete, carelessly edited, and only of limited use."&lt;ref name=Revision/&gt; Calling the book "an inferior job", Sweetland criticized the timing of book's publication long after elections, as well as the map in the back.&lt;ref name=Revision/&gt; As a result, the [[47th Oregon Legislative Assembly|1953 Legislative Assembly]] passed a law requiring the book be published soon after the legislature convenes.&lt;ref&gt;{{cite news |url= https://news.google.com/newspapers?nid=1310&amp;dat=19550125&amp;id=nPlVAAAAIBAJ&amp;sjid=p-IDAAAAIBAJ&amp;pg=6704,3150227 |title= Oregon Blue Book Printed But It Isn't Blue Any More |author= [[Associated Press]] |date= January 25, 1955 |newspaper= The Register-Guard |accessdate= February 17, 2015}}&lt;/ref&gt;

The 1993–94 edition of the book contained a four-page [[errata]].&lt;ref name=Indispensable/&gt; When [[Norma Paulus]] was Secretary of State, she would send a free copy of the book to the first person to find a mistake in each new edition.&lt;ref name=Indispensable/&gt; The 1995–96 edition was reduced in size from its predecessors.&lt;ref name=Indispensable/&gt;

==Reviews==
A 1995 ''[[Register-Guard]]'' editorial called the book "indispensable".&lt;ref name=Indispensable/&gt;

==See also==
*''[[The Oregon Encyclopedia]]''

==References==
{{reflist}}

==External links==
*[http://bluebook.state.or.us/ Oregon Blue Book] (official website)

[[Category:1911 books]]
[[Category:Government of Oregon|Blue Book]]
[[Category:Directories]]
[[Category:1911 establishments in Oregon]]</text>
      <sha1>9xlpy1z0pa4i0upfulw1ii5g1j1ee9p</sha1>
    </revision>
  </page>
  <page>
    <title>The Milepost</title>
    <ns>0</ns>
    <id>240436</id>
    <revision>
      <id>741737604</id>
      <parentid>701856976</parentid>
      <timestamp>2016-09-29T10:19:33Z</timestamp>
      <contributor>
        <username>Hugo999</username>
        <id>3006008</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2777" xml:space="preserve">{{italic title}}
[[Image:49MilePost.gif|right|thumb|The original 1949 Milepost]]'''''The Milepost''''' is an extensive [[guide book]] covering [[Alaska]], the [[Yukon]], the [[Northwest Territories]], and [[British Columbia]].  It was first published in 1949 as a guide about traveling along the [[Alaska Highway]], often locally referred to as "The ALCAN".&lt;ref name="morris"&gt;[http://morris.com/divisions/mcc_magazines/the_milepost.shtml ''The MILEPOST''] from the website of  [[Morris Communications]]&lt;/ref&gt;  It has since expanded to cover all major highways in the northwest corner of [[North America]], including the [[Alaska Marine Highway]].  It is updated annually.

==History==
&lt;!-- Deleted image removed: [[Image:Milepost2008cover.jpg|right|thumb|The 2008 edition&lt;br /&gt;{{deletable image-caption|Sunday, 10 February 2013}}]] --&gt;''The Milepost'' is packaged and distributed like a [[book]]  (2008 edition: ISBN 978-189215431-6), but like the [[Yellow Pages]] it includes paid [[advertising]].&lt;ref&gt;[http://www.themilepost.com/media_kit/testimonials.shtml Testimonials from Advertisers from ''The MILEPOST'' website]&lt;/ref&gt; The original 1949 edition was a mere 72 pages, by 2014 it had expanded to 752 pages, detailing every place a traveler might eat, sleep, or just pull off the road for a moment on all of the highways of northwestern North America. In addition to the paid ads, descriptions are provided of interesting hikes or side trip drives near the highways, campgrounds and other public facilities, as well as short histories of most of the settlements on the highways. Newer additions include special sections on selected areas popular with tourists, such as the [[Kenai Peninsula]]. It is also exhaustively cross-indexed and maps and charts are provided so that travelers can determine the total driving distance between any two points covered by the guide.&lt;ref&gt;http://milepost.com/index.php?option=com_content&amp;task=view&amp;id=71&amp;Itemid=62&lt;/ref&gt;

==Publishing==
Since 1997 ''The Milepost'' has been published by [[Morris Communications]] and currently shares publishing offices with [[Alaska magazine|''Alaska'' magazine]].&lt;ref name="morris" /&gt; Beginning in 2009, The Milepost is also available in an interactive digital format or download.&lt;ref&gt;[http://milepost.com/images/media_kit/mp_mediakit_09_email_lr.pdf The Milepost media kit]&lt;/ref&gt;

==References==
&lt;references /&gt;

==External links==
* {{Official website|http://www.themilepost.com}}

{{Morris Communications}}

{{DEFAULTSORT:Milepost, The}}
[[Category:1949 establishments in Alaska]]
[[Category:1949 books]]
[[Category:Books about Alaska]]
[[Category:Directories]]
[[Category:Morris Communications]]
[[Category:Publications established in 1949]]
[[Category:Roads in Alaska]]
[[Category:Travel guide books]]</text>
      <sha1>amuxr3orh5wa353ljbymrnp1f9elawn</sha1>
    </revision>
  </page>
  <page>
    <title>MiM</title>
    <ns>0</ns>
    <id>10926747</id>
    <revision>
      <id>601740140</id>
      <parentid>524168445</parentid>
      <timestamp>2014-03-29T00:58:24Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10065)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="910" xml:space="preserve">{{Other uses|MIM (disambiguation)}}
{{Orphan|date=February 2009}}
'''MIM''' stands for Music Industry Manual. It was founded in 1996 by James Robertson and in its first year was called The Promoter's Handbook. The Promoters Handbook was a reference manual for the [[dance music]] industry including [[DJ]]s agents, [[nightclub]]s and unusual venues, promoters, flyer designers. The following year its title was changed to give it a broader appeal.

It still caters for the dance music industry, but is now fully international with over 100,000 contacts from countries as remote as [[Azerbaijan]] to developed nations. Whilst the focus is still on DJ and club culture it has over 100 categories including bar designer, music lawyers, event management.

==External links==
* [http://www.mim.dj Official web site]

{{primary sources|date=August 2007}}

[[Category:Directories]]
[[Category:Electronic dance music]]</text>
      <sha1>fz1k66wq8yr2yzed7o4obbyy09t8fdl</sha1>
    </revision>
  </page>
  <page>
    <title>Crockford's Clerical Directory</title>
    <ns>0</ns>
    <id>9731047</id>
    <revision>
      <id>756495858</id>
      <parentid>753364568</parentid>
      <timestamp>2016-12-24T17:41:32Z</timestamp>
      <contributor>
        <username>DBD</username>
        <id>288189</id>
      </contributor>
      <minor />
      <comment>/* Locating previous issues */ UL's collection is missing only one</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15541" xml:space="preserve">{{italictitle}}'''''Crockford's Clerical Directory''''' ('''''Crockford''''') is the authoritative directory of the [[Anglican Communion]] in the United Kingdom, containing details of English, Welsh and Irish benefices and churches, and biographies of around 26,000 clergy. It was first issued in 1858 by [[John Crockford]],{{sfn|Hough &amp; Matthew|2004}} a London printer and publisher whose father – also named John – had been a Somerset schoolmaster.

''Crockford'' is currently compiled and published for the [[Archbishops' Council]] by Church House Publishing.&lt;ref&gt;{{Cite web
| title = Crockford's Clerical Directory
| author =
| work = The Church of England
| date =
| accessdate = 2014-08-12
| url = https://www.churchofengland.org/clergy-office-holders/crockford.aspx
| ref={{sfnref|CofE}}
}}&lt;/ref&gt;  It covers in detail the whole of the [[Church of England]] (including the [[Diocese in Europe]]), the [[Church in Wales]], the [[Scottish Episcopal Church]], and the [[Church of Ireland]], and it also gives some information – now more limited – about the world-wide [[Anglican Communion]].

== Previous publishers ==

[[File:Crockford1868-titlepage.jpg|thumb|''Crockford's Clerical Directory 1868'', published by Horace Cox, London]]The actual title of the first edition was simply ''The Clerical Directory'', but a footnote showed that it was published by John Crockford, 29 Essex Street, [[Strand, London|the Strand]].  The original publisher died suddenly in 1865, shortly before the appearance of the third edition of what had by then become ''Crockford’s Clerical Directory''.  For many subsequent issues the volumes were anonymously edited, but they were published under the imprint of Horace Cox – the nephew of John Crockford’s closest business associate, solicitor and publisher [[Edward William Cox]] (1809Ω–1879).  (His family was probably quite unrelated to the Charles Cox who coincidentally was the publisher of ''Crockford''{{'}}s chief rival, the ''[[Clergy List]]''.{{efn|A two-part article "Shop-talk and mordant wit" by Christopher Currie &amp; Glyn Paflin describes the background to the directory's first hundred editions, {{sfn|Currie &amp; Paflin|7 December 2007}} }}) Horace Cox died in 1918{{efn|Horace Cox’s very brief obituary in ''The Times'', 11 October 1918; p. 5, states that he had retired in 1912 and had ceased to take an active part in his business, which also produced ''The Field'', ''The Queen'' and ''The Law Times''}} and the title was subsequently sold in 1921 to the [[Oxford University Press]],{{sfn|Currie &amp; Paflin|7 December 2007}} who continued as publishers right up until the early 1980s.  For the 1985/86 issue publication was transferred to the [[Church Commissioners]] and their Central Board of Finance (who worked from their own administrative lists and databases).  It is now collated by Church House Publishing.

== Frequency of publication ==

The first four issues came  out in 1858, 1860 (with a supplement in 1861),{{efn|The 1861 supplement, experimentally issued when a switch to biennial publication was being contemplated, may be downloaded free of charge from Google Play}} 1865 {{efn|The 1865 edition was reprinted in a 1995 facsimile limited edition of 100 copies by Peter Bell (bookseller), Edinburgh.{{sfn|Bell|1995|p=}} It can also now be downloaded free of charge from Google play}} and 1868.  ''Crockford'' then reappeared biennially until 1876, when it began a long run of annual appearances which lasted until 1917. The next issue was a delayed 1918/19 edition, which had for the first time incorporated its main rival publication, the ''[[Clergy List]]''. Further issues appeared for 1920 and 1921/22; then between 1923 and 1927{{efn|There was no issue in 1928, for what the editor called "technical reasons". Production difficulties in 1941/42, 1943 and 1944 meant that it was only possible to issue short supplements to the 1941 edition. ''Crockford Prefaces: The Editor Looks Back'' (Oxford, 1947), pp. i, 257, 272, 283.}} and 1929–1940 the directory reappeared annually, followed by more late issues in 1941 and 1947/48. Since that time ''Crockford'' has generally appeared every two years, although gradually worsening delays meant that the 87th and 88th editions were dated 1977/79 and 1980/82, and the book failed to appear at all during 1983/84. Biennial publication was once again resumed in 1985/86, although the volume issued late in 1997 was designated the 1998/99 edition. The 100th edition – eventually published for 2008/09 – included within its hardback version a few facsimile pages from the first edition, together with an extended historical note describing some of the earlier volumes.

The 1858 edition was later described as seemingly “assembled in a very haphazard fashion, with names added ‘as fast as they could be obtained’, out of alphabetical order and with an unreliable index”. But nevertheless the 1860 directory “had become a very much more useful work of reference”.{{efn|Quoted by Brenda Hough in her biographical note on John Crockford, published in the 1998/99 ''Crockford'' and reprinted (with minor modifications) in all subsequent editions; also on the official Crockford's website.&lt;ref&gt;{{Cite web
| title = About John Crockford
| author = Brenda Hough
| work = Crockford's clerical Directory - online
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/standard.asp?id=126
| quote =
}}&lt;/ref&gt;}}  However the original volume was actually a consolidation of what in 1857 had been conceived as a mere series of supplements to an entirely different publication, the ''Clerical Journal''.{{efn|The ''Oxford Dictionary of National Biography'' article on Edward William Cox states that he, together with John Crockford, had founded the ''Clerical Journal'' in 1853.}}  The editors explained in the preface that they  wished it to be understood that it was “but the foundation of a great work which, with the Cordial aid of the clergy, we shall hope to make more and more perfect every year”.

== Scope of the directory ==

[[File:Crockford1910BpLichfield.jpg|thumb|''Crockford'', 1910: a biographical page in an older edition would typically include many abbreviations, including clergy academic backgrounds, and their dates ordained deacon [d] and priest [p] (the presiding bishop being indicated). Diocesan coats of arms were shown alongside episcopal entries; any publications were listed, and parish incomes and patrons were mentioned. Many overseas clergy would be covered.]] The 1858 issue was based on postal returns from the clergy in England and Wales, involving an outlay – as the preface pointed out – of "more than Five Hundred Pounds for Postage Stamps alone".  Simpler lists for the [[Scottish Episcopal Church]] and for a number of colonial clergy – obtained from alternative sources – had been added by the 1865 edition, whilst details of Irish clergy had also been extracted from [[Alexander Thom (almanac editor)|Alexander Thom]]'s ''Irish Almanack and Official Directory''.  From the 1870s onwards the scope was progressively extended to all parts of the Anglican communion with the notable exception of the [[Episcopal Church (United States)]].  The 1870 edition contained 940 pages, but this had increased to over 2,100 pages by 1892.

The earliest editions had also gradually added some details of diocesan office holders and administrators, together with the theological colleges, and the royal chapels.  They also acquired much fuller indexes – along with outline maps of dioceses, and increasingly complete lists of bishops, dating right back to the earliest years of their sees.  They further offered to all clergy an opportunity to list their publications, although these lists eventually had to be cut back as their overall length started to increase dramatically.

By the early 1980s severe economies had become necessary and 1985/86 edition had to be restricted to the "home" churches of England, Scotland and Wales.&lt;ref&gt;''Crockford’s Clerical Directory 1987/88'', pp. 47-48&lt;/ref&gt;  Retired clergy were temporarily restricted to just a few details of their final appointment, although it became possible to restore the Irish clergy in time for the 1987/88 edition.  Later editions saw a further return of the retired clergy, together with details of those overseas clergy who had originally been licensed or trained in the UK, or who occupied senior positions within their respective church hierarchies.  Details which had also become obtainable from the ''[[Church of England Yearbook]]'' or from similar sources were generally excluded. For a time too clergy who made their livings though secular jobs  were excluded from the biographies section, with the abbreviation NQ (Non-Qualifying Position) being used to cover such periods when clerics returned to parish work and were again eligible for inclusion. In that many such clergy retained diocesan licences or episcopal "Permissions to Officiate" during their periods of secular employment, this approach may have caused a degree of difficulty for clerics who needed to prove their clerical status.

By 1985/86 the first women deacons were being included (although [[Hong_Kong_Sheng_Kung_Hui#Social_issues|women priests ordained in Hong Kong]] were included even in the 1970s) while other more recent innovations – from the 1990s onwards – have included optional email addresses, together with lists of those clergy who have died since the previous edition.  Notes on "How to Address the Clergy"&lt;ref&gt;{{Cite web
| title = How to address the clergy
| author =
| work = Crockford's Clerical Directory
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/standard.asp?id=116
| quote =
}}&lt;/ref&gt; have been retained. A small number of clergy have been excluded at their own request, or have allowed their biographies to appear minus a contact address.  The Church Commissioners soon replaced the traditional black hardback bindings in favour of red and also introduced a separate softback alternative version.

Since 2004 there has also been a frequently updated Internet edition of ''Crockford'', which is available by subscription.&lt;ref&gt;{{Cite web
| title = Welcome to the Crockford web site...
| author =
| work = Crockford's Clerical Directory
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/
| subscription=yes
}}&lt;/ref&gt; More recently the directory has also joined in with [[social networking]], operating a [[Twitter]] account since 2012. 

An alternative to the main work, ''Crockford's Shorter Directory'', focused almost entirely on the Church of England and omitting all past biographical details, was issued as a single edition in 1953–54.

== Prefaces ==

The well-known tradition of having an extensive but anonymous preface offering a general review of events within the Anglican communion – together with some occasionally sharp and controversial commentary – evolved gradually during the early part of the 20th century.{{sfn|Currie &amp; Paflin|7 December 2007}} Previous prefaces had tended to be much briefer and they had often been limited merely to explaining the directory’s in-house policies.  After the events following the publication of the 1987/88 edition, which had ended with the death of Dr [[Gareth Bennett]], this tradition of the anonymous preface was discontinued.

An anthology ''Crockford Prefaces: The Editor Looks Back'', anonymously edited by [[Richard Henry Malden]]{{sfn|Currie &amp; Paflin|7 December 2007}} and covering the previous 25 years, was published by the Oxford University Press in 1947.{{sfn|Anon|1947|p=}}

== Locating previous issues ==

County libraries each have their own policies, but there are good collections in a number of major academic and ecclesiastical libraries, including [[Cambridge University Library]], [[Lambeth Palace Library]], [[Canterbury Cathedral]] Library, [[York Minster]] Library, the [[Guildhall Library]] and the [[Society of Genealogists]].

Besides the 1865 reprint,{{sfn|Bell|1995|p=}} a small number of early editions have been reissued in CD format by various publishers, including [[Archive CD Books]].  Scanned copies of other early editions have also begun to appear on the World Wide Web.

== Crockford references in fiction ==

Crockford is referenced in [[Dorothy Sayers]]'s 1927 detective novel ''[[Unnatural Death]]'' (chapter XI) where [[Lord Peter Wimsey]] uses "this valuable work of reference" in trying to trace a clergyman who is important for solving the book's mystery.

Another fictional character holding Crockford on his bookshelves was [[Sherlock Holmes]], who during one of his final short stories ("The Adventure of the Retired Colourman"), consulted his copy before dispatching his colleague Dr Watson, together with another companion, to a distant part of Essex. There they interviewed “a big solemn rather pompous clergyman” who received them angrily in his study.

The character Dulcie Mainwaring prefers Crockford's format to ''[[Who's Who]]'' while reflecting on researching in the [[Public Record Office]] in London in [[Barbara Pym]]'s ''No Fond Return of Love.  ''

== Footnotes ==
{{notelist}}

== Notes ==
{{reflist}}

== References ==
{{refbegin}}

* {{Citation

| title = Crockford, John (1824/5–1865)
| first1 = Brenda
| last1= Hough
| first2=H. C. G.
| last2= Matthew
| work = Oxford Dictionary of National Biography
| date =  2004
| accessdate = 2014-08-12
| url = http://dx.doi.org/10.1093/ref:odnb/37324
| language =
| ref={{sfnref|Hough &amp; Matthew|2004}}
}}

* {{Cite web
 |title=Shop-talk and mordant wit 
 |first1=Christopher 
 |last1=Currie 
 |first2=Glyn 
 |last2=Paflin 
 |work=The [[Church Times]] 
 |date=7 December 2007 
 |issue=7552 
 |url=http://www.churchtimes.co.uk/content.asp?id=48255 
 |accessdate=2014-08-12 
 |archive-url=https://web.archive.org/web/20120407024756/http://www.churchtimes.co.uk/content.asp?id=48255 
 |archive-date=2012-04-07 
 |ref={{sfnref|Currie &amp; Paflin|7 December 2007}} 
 |subscription=yes 
 |deadurl=yes 
 |df= 
}}

* {{cite book|ref={{sfnref|Bell|1995|p=}}|last=Bell|first=Peter|title=Crockford's Clerical Directory for 1865: Being a Biographical and Statistical Book of Reference for Facts Relating to the Clergy and the Church|url=https://books.google.com/books?id=oSqjAQAACAAJ|year=1995|publisher=Horace Cox  in 1865, republished by Peter Bell in 1995|location=Oxford and Edinburgh|isbn=978-1-871538-21-2}}
* {{cite book|ref={{sfnref|Anon|1947|p=}}|author=Anon|authorlink=Richard Henry Malden|title=Crockford prefaces: the editor looks back|url=https://books.google.com/books?id=rItbAAAAMAAJ|year=1947|publisher=Oxford Univ. Press}}
*{{cite book|ref=harv|author1=Church of England|author2=Central Board of Finance|author3=Church Commissioners|title=Crockford's Clerical Directory|url=https://books.google.com/books?id=BzkFAAAAYAAJ|year=1865|publisher=Oxford University Press}}

{{refend}}

== External links ==

* {{Official website|http://www.crockford.org.uk/}}
* [https://play.google.com/store/books/details/Church_of_England_Crockford_s_Clerical_Directory?id=BzkFAAAAYAAJ Crockford's Clerical Directory 1865 free download from Google play]
* [http://www.chpublishing.co.uk/ Church House Publishing]
* [https://archive.org/details/crockfordscleri00commgoog 1868 version available for free download at the archive.org]

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]</text>
      <sha1>5rd83ikhxy872wq7ul51ce0023dtha4</sha1>
    </revision>
  </page>
  <page>
    <title>Writer's Market</title>
    <ns>0</ns>
    <id>18980436</id>
    <revision>
      <id>754382524</id>
      <parentid>747550884</parentid>
      <timestamp>2016-12-12T10:28:30Z</timestamp>
      <contributor>
        <username>Hugo999</username>
        <id>3006008</id>
      </contributor>
      <comment>added [[Category:American literary agencies]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2478" xml:space="preserve">{{italic title}}
{{Advert|date=March 2011}}
'''''Writer's Market'' (''WM'')''' is an annual resource book for writers who wish to sell their work. The publication is released by ''[[Writer's Digest]]'' Books (an imprint of [[F+W Media]]) and usually hits bookstores around June of each year. ''Writer's Market'' was first published in 1921, and is often called "The Bible for writers" or "the freelancer's Bible."&lt;ref&gt;http://search.barnesandnoble.com/Writers-Market-2008/Robert-Lee-Brewer/e/9781582974965&lt;/ref&gt;&lt;ref&gt;http://www.epinions.com/review/Book_Writers_Market_2007/content_298510028420&lt;/ref&gt;&lt;ref&gt;http://www.thegoodwebguide.co.uk/index.php?rid=000467&lt;/ref&gt;

The most current edition is the 2016 edition; the current editor is Robert Lee Brewer.

== Listings ==
For 89 years, the book has listed thousands of markets for writers who wish to sell their work. Said markets include magazines, newspapers, theaters (for stage plays), production companies, contests of all types, greeting card companies, literary agents, and more. Each listing has detailed instructions on how to submit work, relevant contact information, as well as what work each listing seeks.

== Articles ==
The upfront section of ''WM'' has more than a dozen articles on writing topics, such as starting a freelancing business, syndication, freelancing for magazines, and a chart filled with typical payment rates concerning various writing assignments.

== "Market Books" ==
''Writer's Market'' is one of nine "[[Market (economics)|market]] books" published each year by [[Writer's Digest Books]]. Others include: ''Guide to Literary Agents'', ''Photographer's Market'', ''Children's Writer's &amp; Illustrator's Market'', ''Novel &amp; Short Story Writer's Market'', ''Artist and Graphic Designer's Market'', ''Poet's Market'', ''Screenwriter's &amp; Playwright's Market'' and ''Songwriter's Market''. Each book is designed to give writers instructions on how to submit freelance work to markets.

== See also ==
* [[Publishing]]
* ''[[Writer's Digest]]''
* [[literary agent]]
* [[Literary agent#Querying|query]]
* [[screenplay]]
* [[royalties]]
* [[Authors Guild]]
* [[poetry]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://www.writersmarket.com|The book's official website}}
*[http://www.writersdigest.com ''Writer's Digest'' magazine official site]

[[Category:Directories]]
[[Category:Literary agencies|.]]
[[Category:Literary agents|.]]
[[Category:American literary agencies]]</text>
      <sha1>gvj3jffjwwyhthbqxa3cf1yvlqcrqvq</sha1>
    </revision>
  </page>
  <page>
    <title>Thacker's Indian Directory</title>
    <ns>0</ns>
    <id>7719940</id>
    <revision>
      <id>741232778</id>
      <parentid>676469378</parentid>
      <timestamp>2016-09-26T06:08:29Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1687" xml:space="preserve">{{italic title}}
{{Refimprove|date=September 2014}}
'''''Thacker, Spink &amp; Co.''''' was a well-known [[Kolkata]] publishing company. ''Thacker's Bengal Directory'' was published from 1864 to 1884 and covered the [[Bengal Presidency]] – which included the present day [[Myanmar]] and [[Bangladesh]]. From 1885 the ''Directory'' covered the whole of [[British India]] and was renamed '''''Thacker's Indian Directory'''''.  It was later owned by [[Kameshwar Singh|Maharaja of Darbhanga]].&lt;ref&gt;{{cite book|title=Appendices|date=1982|publisher=India. Second Press Commissior Controller of Publications|pages=266, 343|url=https://books.google.com/books?id=tBwuAAAAMAAJ&amp;q=darbhanga+Thacker+Spink&amp;dq=darbhanga+Thacker+Spink&amp;hl=en&amp;sa=X&amp;ei=Ul0sU5_BH46zrgfjsoCYBw&amp;ved=0CEkQ6AEwBQ}}&lt;/ref&gt;   It continued to be published until 1960.

The directory was essentially an [[almanac]] which listed British and Foreign Merchants and Manufacturers, Commercial Industries, Army, railway and government departments and office holders, European residents, and separately, prominent non-European residents.   Earlier editions of ''Thacker'' had street directories of major cities, such as Kolkata and [[Yangon]], together with the name of the residents of each house.

Similar directories published included:
*''Thacker's Bombay Directory'', city and island (together with a directory of the chief industries of Bombay, etc.);
*Thacker's medical directory of India, Burma and Ceylon;
*''Thacker's Directory of the Chief Industries of India, Burma and Ceylon''.

== References ==
&lt;references /&gt;

[[Category:Almanacs]]
[[Category:Directories]]
[[Category:Books about British India]]
[[Category:Bengal Presidency]]</text>
      <sha1>7cu95oxwtc2zktfnjthfu6hxru1rr82</sha1>
    </revision>
  </page>
  <page>
    <title>Ves Peterburg</title>
    <ns>0</ns>
    <id>11002640</id>
    <revision>
      <id>736631806</id>
      <parentid>710445647</parentid>
      <timestamp>2016-08-28T20:33:16Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* top */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 500 to → between 500 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5217" xml:space="preserve">{{italic title}}
'''''Ves Peterburg''''' (/vʲesʲ pʲɪtʲɪrˈburg/, Literally translated "''All Petersburg''" or "''The Entire Saint Petersburg''"
") (Full name in [[cyrillic]] "Ves Petersburg; Adresnaja i spravočnaja kniga g. Petersburga") (often referred to as the ''Suvorin directories'' from the publisher's name) was the title of a series of [[city directory|city directories]] of [[Saint Petersburg]], [[Russia]] published on a yearly basis from 1894 to 1940 by [[Aleksei Sergeevich Suvorin]]. Each volume was anywhere between 500 and 1500 pages long. After changes in the name of the city the directories were called '''''Ves Petrograd''''' from 1914 to 1923 and '''''Ves Leningrad''''' from 1924 to 1940.

The directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices, public services and medium and large businesses present in the city.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.

== List of residents of St. Petersburg ==

Each directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an alphabetical list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed.

The following information can be found:
*Person's surname and first name
*[[Patronymic]]
*Street address with apartment number
*Profession
*Telephone numbers (only appear sparingly as few private residents could afford a telephone before 1918)

== List of occupants of each building on every street and square ==

A section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.

== Other sections ==

The following information can also be found in each directory

*information on the royal family
*Maps of the city
*cultural establishments (with interior theatre hall layouts and seating plans)
*Lists of personnel in state, public and private institutions
*information on academic institutions of all ranks
*information on churches and monasteries of St. Petersburg
*Original commercial advertisements of Russian and foreign companies which had offices in St. Petersburg

== Historical and genealogical value ==

Because numerous residents emigrated from Saint Petersburg after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city.

== Interruption in the series ==

No volumes were published in the following years:
*1918
*1919
*1920
*1921

This was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].

The edition of 1922 was very concise and only contained details of businesses in the city but not residents.

== Termination of series ==

Publication came to a halt after the edition of 1935, coinciding with the time of [[Joseph Stalin]]'s [[great purge]]s and [[Moscow Trials]]. The only further volumes were issued in 1939 and 1940, but these (like the edition in 1922) only contained details of state run businesses and public and governmental offices, but not residents.

== Availability ==

Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including [[The Baltic]], Finland the United Kingdom and Germany) however most only have an incomplete collection. The [[Russian National Library]] in Saint Petersburg has a complete run of all volumes published available.

== Other city directories ==

Suvorin also published city directories for [[Moscow]] under the title ''[[Vsia Moskva]]'' (All Moscow) for the years 1875 to 1936 and for the whole country under the titles ''[[Vsia Rossiia]]'' (All Russia) continued under than name ''[[Ves SSSR]]'' (All USSR) from 1924 to 1931.

Since 1993 a telephone directory under the title "Ves Petersburg" has been published annually by the publishing House Presskom but this is vastly different in content then the original directories and does not list residents.

== Sources ==

http://www.encspb.ru/en/article.php?kod=2804017249
Ves Peterburg - http://www.allinform.ru

==See also==

*''[[Vsia Moskva]]''
*''[[Vsia Rossiia]]''

== External links ==
*[http://www.nlr.ru Official website of the Russian National Library in Saint Petersburg]
*[http://surname.litera-ru.ru/ A russian website offering a search engine in cyrillic for some city directories.]

[[Category:Directories]]
[[Category:History of Saint Petersburg]]
[[Category:Russian non-fiction books]]
[[Category:Media in Saint Petersburg]]
[[Category:1894 books]]</text>
      <sha1>o9acvk45ghyemaapiac19k69eg201ad</sha1>
    </revision>
  </page>
  <page>
    <title>Blue pages</title>
    <ns>0</ns>
    <id>5507437</id>
    <revision>
      <id>762242480</id>
      <parentid>762242454</parentid>
      <timestamp>2017-01-27T15:44:47Z</timestamp>
      <contributor>
        <username>Serols</username>
        <id>9929111</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/2405:205:3080:447D:0:0:1D04:60AD|2405:205:3080:447D:0:0:1D04:60AD]] ([[User talk:2405:205:3080:447D:0:0:1D04:60AD|talk]]) ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1710" xml:space="preserve">{{Unreferenced|date=December 2009}}
'''Blue pages''' are a [[telephone directory]] listing of American and Canadian state agencies,  [[government]] agencies, federal government and other official entities, along with specific offices, departments, or bureaus located wherein.

==Canada==
Canadian yellow-page listings currently indicate "Government Of Canada-See Government Listings In The Blue Pages"; in markets where the local telephone directory is a single volume, the blue pages and community information normally appear after the alphabetical white-page listings but before the yellow pages advertising. The blue page listings include both provincial and federal entities.{{cn|date=April 2013}}

==United States==
In the [[United States]], the blue pages included state, federal, and local offices, including [[service district]]s such as school districts, port authorities, public utility providers, parks districts, fire districts, and the like. The blue pages also provided information about government services, in addition to officials' names, addresses, telephone numbers, and other contact information. The color blue is likely derived from so-called government blue books, official publications printed by a government (such as that of a state) describing its organization, and providing a list of contact information. (The blue pages published in a printed telephone directory is usually quite abridged, compared to official blue books).

==Other==
The name "blue pages" has been used for various specialised directories by private-sector entities such as the internal IBM Staff directory. 

{{DEFAULTSORT:Blue Pages}}
[[Category:Telephone numbers]]
[[Category:Directories]]

{{telephony-stub}}</text>
      <sha1>ciadjip2h3orid6lpxi2e95c1juk2s2</sha1>
    </revision>
  </page>
  <page>
    <title>Sources (website)</title>
    <ns>0</ns>
    <id>20263150</id>
    <revision>
      <id>708318686</id>
      <parentid>708318646</parentid>
      <timestamp>2016-03-04T22:18:44Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <comment>++</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13982" xml:space="preserve">{{Use dmy dates|date=May 2014}}
{{More footnotes|date=November 2011}}
'''Sources''' is a [[web portal]] for journalists, freelance writers, editors, [[authors]] and [[researchers]], focusing especially on human sources: [[expert]]s and spokespersons who are prepared to answer [[Reporter]]s' questions or make themselves available for on-air [[interview]]s.

==Structure==
The Sources website is built around a [[Controlled vocabulary|controlled-vocabulary]] subject index comprising more than 20,000 topics. This [[Subject indexing|subject index]] is underpinned by an 'Intelligent Search' system which helps reporters focus their searches by suggesting additional subjects related to their search terms. For example, a search for "cancer" will suggest terms such as "chemotherapy", "melanoma", "oncology", "radiation therapy", "tobacco diseases" and "tumours", as well as topics that actually contain the word "cancer".

Each topic reference links in turn to experts and spokespersons on that topic, with profiles describing their expertise and, where relevant, their approach to the issue, along with their phone numbers and other contact information. Sources includes listings for universities and research institutes, non-profit associations and NGOs, government and public sector bodies, businesses, and individuals including academics, public speakers, and consultants.

The subject index and the search menus are being translated into French, Spanish and German to make Sources more of an international resource.

==History==

===Print supplement===
Based in Canada, Sources was founded in 1977 as a print directory for reporters, editors, and story producers. It was first published as a supplement to ''Content'' magazine, an influential and controversial magazine of journalism criticism. ''Content'', founded by Dick MacDonald in 1970 and published by [[Barrie Wallace Zwicker|Barrie Zwicker]] after MacDonald's death in 1974, frequently took journalists to task for always relying on the same narrow range of sources representing the same conventional points of view for their stories. Zwicker and MacDonald argued in ''Content'' and in their book ''The News: Inside the Canadian Media''&lt;ref&gt;MacDonald, Dick; Zwicker, Barrie. ''The News: Inside the Canadian Media''. Deneau. 1982. ISBN 0-88879-053-8&lt;/ref&gt; that there was a “terrible sameness” in the media’s coverage of many important issues, and a shutting out of other, potentially valuable, perspectives and sources of information.

Zwicker decided to do something about the problem, and in summer 1977, ''Content'' published its first directory issue, called Sources. Billed as “A Directory of Contacts for Editors and Reporters in Canada”, Sources listed “information officers, public relations officers, media relations and public affairs people, and other contacts for groups, associations, federations, unions, societies, institutions, foundations, industries and companies and federal, provincial and municipal ministries, departments, agencies and boards.”&lt;ref&gt;Sources: A Directory of Contacts for Editors and Reporters in Canada. ''Content''. 1977. {{ISSN|0045-835X}}&lt;/ref&gt;

Explaining the rationale behind Sources, Zwicker said that “It’s a cliché that every story has two sides. An untrue cliché. Most have several. The reporter’s challenge is digging out all sides. Sources can help.”&lt;ref&gt;Sources 50. 2002. ISBN 0-920299-55-5&lt;/ref&gt; From the beginning, Zwicker saw Sources as a public service as well as a tool for journalists. He said that Sources aimed “to help promote a system of information fairness. Communications resources are equivalent to other basic needs – shelter, food, health care, for example. Everyone should have reasonable access to all.”&lt;ref name="ReferenceA"&gt;Sources 36. 1995. ISBN 0-920299-24-5&lt;/ref&gt; Therefore, he said “we attempt to provide true diversity: access to people in organizations large and small, for-profit and not-for-profit, from low-tech to high-tech, long-established to just-launched.”&lt;ref name="ReferenceA"/&gt;

Zwicker told users that “within Sources you will find both mainstream and alternative information. Some may consider alternative as off to one side, not quite up to par, more or less second hand. Here at Sources ‘alternative’ is considered differently, considered as authentic and substantial, even if normally less accessible. The surprises, the jarring notes, the flashes of insight, the ‘odd takes’, the pearls of wisdom, the cries de coeur, the avant garde, tomorrow’s news, the prophesies, the unfiltered, the exciting, the elsewhere-squelched, the memorable, the eccentric, the thought-out-at-length, the unmentionable in polite company, the outrageous, the uncensored ... these are what ‘alternative’ media offer. So far as we can, we will include the alternative with Sources. Sources’ driving philosophy is flat-out informational democracy enabled by user-friendly technology. The assumption is that there is a significant fraction of Canadians who want to use and benefit from such an information resource. The assumption is that a significant fraction of Canadians want to expand their search for solutions, and deepen their understandings, rather than chant conventional wisdoms (however freshly minted) to each other.”&lt;ref name="ReferenceA"/&gt;

===Separate publications===
After a few years, Sources become so big that it could no longer fit into ''Content'' (the print directory eventually grew to more than 500 pages), and in 1981 it became an independent publication. ''Content'' itself eventually folded, but Zwicker continued to devote a substantial editorial section in Sources to coverage of topics of interest to journalists, ranging from practical topics such as grammar, style, [[fact-checking]], [[photojournalism]], [[copyright]], fees for freelancers and [[self-publishing]], to feature articles on the state of journalism and the media, to book reviews. From the early 1990s, Sources began to feature articles about online research, notably the regular feature 'Dean's Digital World'&lt;ref&gt;[Dean's Digital World – http://www.sources.com/SSR/DeansDigital.htm&lt;/ref&gt; by informatics expert Dean Tudor.

===World Wide Web===

====Content====
Sources went on the Internet in 1995 and has been expanding its online portal ever since. It continues to publish a print edition of the directory, primarily for the benefit of freelancers who use it as a source of story ideas, but is now primarily a Web-based resource.

The Sources website includes not only the Sources directory itself, but a separate government directory, Parliamentary Names &amp; Numbers; a directory of the media, Media Names &amp; Numbers; and The Sources HotLink  [http://www.hotlink.ca (www.hotlink.ca)], which features articles about media relations and public relations. Also on the site is [http://www.sources.com/Fandf/Index.htm Fame and Fortune], a directory of awards, prizes, and scholarships available to writers and journalists, and a portal linked into the online archive of [[Connexions (Information Sharing Services)|Connexions]], a library of documents related to alternatives and social justice.

The site also houses Sources Select Resources,&lt;ref&gt;Sources Select Resources – http://www.sources.com/SSR.htm&lt;/ref&gt; a large library of articles and reviews about journalism and the media, spanning a period of more than 30 years.

====Controversy====
While much of the editorial content has focused on the nitty-gritty of writing, editing and research, Sources has also regularly published articles that have sparked controversy on topics such as censorship and [[media bias]]. One campaign waged by Zwicker and others challenged the [[journalism ethics|ethics]] of journalists accepting free gifts from the people they are supposed to cover. This campaign eventually led Canadian managing editors to agree among themselves that their newspapers would not accept free tickets from travel agencies, resorts, and hotels.

A series of articles by Zwicker on "War, Peace, and the Media"&lt;ref&gt;Zwicker, Barrie. ''War, Peace and the Media''. Sources. 1983, 1985&lt;/ref&gt; (later collected and published as a booklet) provoked a furor from readers upset by its criticisms of how the media cover [[United States foreign policy|U.S. foreign policy]]. As Zwicker put it in a publisher's letter in the next issue, the "reaction ranged from high praise to angry denunciation." The ''[[Toronto Sun]]'' newspaper devoted three stories to the series. The columnist Claire Hoy was left "trembling with rage" and the editor [[Peter Worthington]] felt "outraged" and a lead editorial denounced Zwicker.

Other controversial articles included one by Wendy Cukier on the public relations battle surrounding proposed [[Gun politics in Canada|gun control]] legislation, which drew the ire of the gun lobby.&lt;ref&gt;Cukier, Wendy. "Anatomy of the Gun Control Debate". Sources. 1996 – http://www.sources.com/SSR/Docs/PNN5-1-GunControl.htm&lt;/ref&gt; Ulli Diemer, who succeeded Zwicker as publisher in 1999, came under attack from the [[Fraser Institute]] for his article "Ten Health Care Myths: Understanding Canada’s Medicare Debate”, in which he argued that opponents of [[public health care]] were spreading [[Misinformation|mis-information]] designed to mislead and frighten the public.&lt;ref&gt;Diemer, Ulli. 'Ten Health Care Myths: Understanding Canada’s Medicare Debate’. Sources. 1995. – http://www.diemer.ca/Docs/Diemer-TenHealthCareMyths.htm&lt;/ref&gt;

====New resources====
In keeping with its mandate of encouraging a wide diversity of points of view in the media, Sources has added extra resources over time to help organizations and individuals to be heard. These include a calendar of events open to the media&lt;ref&gt;Sources Calendar – http://calendar.sources.com&lt;/ref&gt; and a [[news release]] service which Sources members can use to distribute their statements and communiques via online posting and [[RSS]]. The releases are also subject indexed and integrated into the overall search structure for information on the Sources site.

==See also==
* [[Barrie Wallace Zwicker|Barrie Zwicker]]

== Notes ==
{{reflist|33em}}

==References==
{{refbegin|33em}}
* Basch, Reva. ''Secrets of the Super Net Searchers: The Reflections, Revelations, and Hard-won Wisdom of 35 of the World’s Top Internet Researchers''. Pemberton Press. 1996. ISBN 0-910965-22-6
* Berkman, Robert. ''The Skeptical Business Searcher: The Information Advisor’s Guide to Evaluating Web Data, Sites and Sources''. Information Today, 2004. ISBN 0-910965-66-8
* Bonner, Allan. ''Media Relations''. Briston House. 2003. ISBN 1-894921-00-3
* Carney, William Wray. ''In the News The Practice of Media Relations in Canada''. University of Alberta Press', 2002. ISBN 0-88864-382-9
* Comber, Mary Anne; Mayne, Robert S. ''The Newsmongers: How The Media Distort the Political News''. 1987. McClelland &amp; Stewart
* Cormack, Paul G.; Shewchuk, Murphy (eds.) ''The Canadian Writers’ Guide''. 13th Edition. Canadian Authors Association. Fitzhenry &amp; Whiteside, 2003. ISBN 1-55041-740-1
* Hackett, Robert A.; Gruneau, Richard. ''The Missing News: Filters and Blind Spots in Canada’s Press''. Newswatch Canada. Canadian Centre for Policy Alternatives &amp; Garamond Press, 2000
* Hackett, Robert A. ''News and Dissent: The Press and The Politics of Peace in Canada''. 1993. Ablex.
* Hackett, Robert A.; Zhao, Yuezhi. ''Sustaining Democracy? Journalism and the Politics of Objectivity''. Garamond Press. 1998. ISBN 1-55193-013-7
* Kashmeri, Zuhair. ''The Gulf Within: Canadian Arabs, Racism, &amp; The Gulf War''. James Lorimer. 1991
* MacDonald, Dick; Zwicker, Barrie. ''The News: Inside the Canadian Media''. Deneau. 1982. ISBN 0-88879-053-8
* Mann, Thomas. ''The Oxford Guide to Library Research''. Oxford University Press. 1998. ISBN 0-19-512313-1
* Manson, Katherine; Hackett, Robert; Winter, James; Gutstein, Donald; Gruneau, Richard (eds.) ''Blindspots in the News? Project Censored Canada Yearbook''. Project Censored Canada. 1995.
* McGuire, Mary; Stilborne, Linda; McAdams, Melinda; Hyatt, Laurel. ''The Internet Handbook for Writers, Researchers, and Journalists''. Trifolium Books. 1997, 2002. ISBN 1-895579-17-1
* Miljan, Lydia; Cooper, Barry Cooper. ''Hidden Agendas: How Journalists Influence the News''. University of British Columbia Press. 2003. ISBN 0774810203
* Miller, John. ''Yesterday’s News: Why Canada’s Daily Newspapers are Failing Us''. Fernwood Publishing, 1999
* Ouston, Rick. ''Getting the Goods: Information in B.C.: How to Find It, How to Use It''. New Star Books, 1990
* Patriquin, Larry. ''Inventing Tax Rage: Misinformation in the National Post''. Fernwood Publishing, 2004. ISBN 1-55266-146-6
* Soderlund, Walter C.; Hildebrandt, Kai (eds.) ''Canadian Newspaper Ownership in the Era of Convergence: Rediscovering Social Responsibility''. University of Alberta Press. 2005, ISBN 0-88864-439-6
* Tudor, Dean. ''Finding Answers: Approaches to Gathering Information''. McClelland &amp; Stewart Inc., Toronto. 1993.
* Ward, Stephen J.A. ''The Invention of Journalism Ethics: The Path to Objectivity and Beyond''. McGill-Queen’s University Press. 2004. ISBN 0-7735-2810-5
* Winter, James. ''Media Think''. Black Rose Books. 2002. ISBN 1-55164-054-6
* Zwicker, Barrie. ''War, Peace and the Media''. Sources. 1983, 1985
{{refend}}

==External links==
* {{official website|http://www.sources.com/}}
** [http://www.sources.com/SSR.htm Sources Select Resources]
** [http://www.sources.com/News.htm Sources Select News]
** [http://calendar.sources.com Sources Calendar]
** [http://www.sources.com/Fandf/Index.htm Fame &amp; Fortune]
* [http://www.hotlink.ca The Sources HotLink]
* [http://www.connexions.org Connexions Information Sharing Services]

{{DEFAULTSORT:Sources (Website)}}
[[Category:Directories]]
[[Category:Journalism organizations]]
[[Category:Knowledge markets]]
[[Category:Online databases]]
[[Category:Web directories]]
[[Category:Websites]]</text>
      <sha1>lm7up8nb1jub2qhrz5pdjd5ifva7yqd</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directory assistance services</title>
    <ns>14</ns>
    <id>28706812</id>
    <revision>
      <id>410106072</id>
      <parentid>382986994</parentid>
      <timestamp>2011-01-26T04:02:55Z</timestamp>
      <contributor>
        <username>Pnm</username>
        <id>5795</id>
      </contributor>
      <comment>added [[Category:Information by telephone]] using [[Help:Gadget-HotCat|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="143" xml:space="preserve">[[Category:Directories]]
[[Category:Telephone services]]
[[Category:Telephone service enhanced features]]
[[Category:Information by telephone]]</text>
      <sha1>c06q36jxdhug7iqrdpjbp00lfufwnzb</sha1>
    </revision>
  </page>
  <page>
    <title>City directory</title>
    <ns>0</ns>
    <id>30018447</id>
    <revision>
      <id>709117334</id>
      <parentid>695750919</parentid>
      <timestamp>2016-03-09T06:40:23Z</timestamp>
      <contributor>
        <username>Except</username>
        <id>1116146</id>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1661" xml:space="preserve">{{Multiple issues|
{{Refimprove|date=July 2015}}
{{Citation style|date=July 2015}}
}}

A '''city directory''' is a listing of residents, streets,  businesses, organizations or institutions, giving their location in a [[city]].  Antedating [[telephone directories]], they have been in use for centuries.

Examples include [[Kelly's Directory]] and the [[Boston Directory]].

==See also==
* [[:de:Adressbuch]]

==References==
{{refbegin}}
*{{cite journal |title=How Reliable is the Modern City Directory? | volume= 30| issue =2| pages =154–158|date=June 1986 |journal=Canadian Geographer |author=Richard Harris, Ben Moffat |doi=10.1111/j.1541-0064.1986.tb01040.x}}
*{{cite web |url=http://www.ancestry.com/learn/library/article.aspx?article=4062 |title= City vs. Telephone Directories |work=[[Ancestry.com]] |author=George G. Morgan}}
*{{cite book 
|author=A. V. Williams 
|title=The Development and Growth of City Directories 
|publisher=
|location=Cincinnati
|year=1913 
|url=http://catalog.hathitrust.org/Record/008698693
}}
*{{cite book |author=Florence May Hopkins |title=Reference Guides that Should be Known and how to Use Them: Atlases; City Directories; Gazetteers  |publisher=The Willard Company |location= |year=1919  |pages= |isbn= |url=https://books.google.com/books?id=SPEVAAAAIAAJ |doi= |accessdate=}}
{{refend}}

==Further reading==
* {{citation |title=Direct Me NYC 1786: A History of City Directories in the United States and New York City |author= Philip Sutton |year=2012 |publisher=New York Public Library |url= http://www.nypl.org/blog/2012/06/08/direct-me-1786-history-city-directories-US-NYC |work=NYPL Blogs }}

[[Category:Directories]]</text>
      <sha1>8jkeuy27kze5xz5kcwus8cosqewrzwc</sha1>
    </revision>
  </page>
  <page>
    <title>Blogged.com</title>
    <ns>0</ns>
    <id>15955818</id>
    <revision>
      <id>747766818</id>
      <parentid>712558949</parentid>
      <timestamp>2016-11-04T07:27:53Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 4 sources and tagging 0 as dead. #IABot (v1.2.6)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5159" xml:space="preserve">{{Orphan|date=February 2009}}
{{Infobox dot-com company
| name     = Blogged.com 
| logo     = 
| company_type     = [[Private company|Private]]
| foundation       = 2008
| founder          =
| location         = [[Alhambra, California]], [[United States]]
| key_people       = 
| revenue          = unknown
| operating_income = 
| net_income       = 
| owner            = 
| num_employees    = number unknown
| company_slogan   = 
| url              = [http://www.blogged.com/ www.blogged.com]
| screenshot       = [http://www.techcrunch.com/wp-content/blogged-small.png]
| caption          = Screenshot of Blogged.com home page
| alexa            = {{IncreaseNegative}} 16,125,052 ({{as of|2014|4|1|alt=April 2014}})&lt;ref name="alexa"&gt;{{cite web|url= http://www.alexa.com/siteinfo/blogged.com |title= Blogged.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}&lt;/ref&gt;&lt;!--Updated monthly by OKBot.--&gt;
| website_type     = [[Wiki]] [[Blog directory]]
| language         = multilingual
| advertising      = 
| registration     = Optional
| launch_date      = {{launch date and age|2008|2|24|p=y}}
| current_status   = inactive{{Citation needed|date=May 2012}}
}}

'''Blogged.com''' is a blog directory that attempts to combine social networking with people's interests in blogging. It employs a method of niche social networking whereby people can connect to each other by their interests rather than their social connections. It attempts to use a method of [[crowdsourcing]] to help evaluate the quality of various blogs{{Citation needed|date=February 2008}}. Blogged symbolizes a trend of new sites that attempt to connect people with their interests rather than social connections{{Citation needed|date=February 2008}}. This type of niche social networking has been employed successfully by sites such as [[Flixster]], [[Yelp, Inc.|Yelp]], [[Last.FM]], and [[Stumbleupon]].  [[TechCrunch]] has recently compared Blogged.com to Yelp for blogs.&lt;ref&gt;
{{cite news
 | first = Erick 
 | last = Schonfeld
 | title = Blogged Hopes to Become the Yelp of Blog Directories
 | url = http://www.techcrunch.com/2008/02/24/blogged-hopes-to-become-the-yelp-of-blog-directories/
 |publisher=Tech Crunch
 |date=2008-02-24
 }}
&lt;/ref&gt;

== Method ==

Blogged.com focuses on blog discovery and displays expert reviews and ratings on popular blogs thereby providing a basis from which to introduce new blogs to a potential reader. Traditional blog search sites such as Technorati and Google Blog Search offer users a method of searching through individual blog entries or postings, but not the blog website itself. Therefore, it is sometimes difficult to gauge the quality or importance of the search results since the credibility of the website which contains the blog entry may be in question. This method of propagating high-quality blogs via user feedback has been used by websites such as Digg. Digg allows users to vote on the importance of articles and causes those articles which are most popular to rise to the top. This method, commonly called [[crowdsourcing]], is being used by Blogged to utilize user feedback to gauge the importance of various blogs.{{Citation needed|date=February 2008}}

== Status ==

As of 2/3/2016 BLOGGED.COM does not resolve to any site and returns a "Not Found" message in the upper right corner.

The last known active date for Blogged.com is July 25, 2011.&lt;ref&gt;{{cite web|url=http://www.blogged.com/ |title=Blogged.com Last Crawled Date: July 25, 2011 |publisher=Internet Archive Wayback Machine |date= |accessdate=2012-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20110725121828/http://www.blogged.com/ |archivedate=July 25, 2011 }}&lt;/ref&gt; The site currently redirects to [[Chime.in]].{{Citation needed|date=May 2012}}

== See also ==

* [[Facebook]]
* [[Yelp, Inc.]]
* [[Flixster]]
* [[Friendster]]
* [[Stumbleupon]]

==Notes==
{{reflist}}

==References==
* "[http://marketwire.com/mw/release.do?id=825026&amp;k=blogged.com Blogged.com Connects Bloggers With Readers; Increases Traffic and Promotes Quality Content Filtered by People]," Marketwire, 2/25/2008.
* "[http://www.webware.com/8301-1_109-9877585-2.html Blogged.com launches blog directory, reviews]," ''[[CNET]] Blogs'', 2/25/2008.
* "[http://mashable.com/2008/02/24/bloggedcom/ Blogged.com. More than Just Another Blog Search Tool]," ''[[Mashable]]'', 2/25/2008.
* "[http://www.blogherald.com/2008/02/25/bloggedcom-new-blog-directory-officially-launches/ Blogged.com new blog directory officially launches]," Blogherald, 2/25/2008.
* "[http://www.bloggingtips.com/2008/02/25/bloggedcom-public-beta-goes-live/ Blogged.com Public Beta Goes Live]," BloggingTips, 2/25/2008.
* "[https://web.archive.org/web/20070214023246/http://publications.mediapost.com:80/index.cfm?fuseaction=Articles.showArticleHomePage Blogged.com Ranks Blogs For Consumers, Could Help Bloggers Monetize]," Online Media Daily, 2/25/2008

==External links==
* [https://web.archive.org/web/20110725121828/http://www.blogged.com/ Blogged.com]

{{DEFAULTSORT:Blogged.Com}}
[[Category:Blogging]]
[[Category:Directories]]
[[Category:American websites]]
[[Category:Alhambra, California]]</text>
      <sha1>9ptpw474x2r9pfiz1098yulbxokryt2</sha1>
    </revision>
  </page>
  <page>
    <title>Adelskalender (directory)</title>
    <ns>0</ns>
    <id>24177992</id>
    <revision>
      <id>649375493</id>
      <parentid>606836543</parentid>
      <timestamp>2015-03-01T13:30:39Z</timestamp>
      <contributor>
        <username>Nikolaj Christensen</username>
        <id>2089102</id>
      </contributor>
      <minor />
      <comment>interwiki: removed da (which refers to the adelskalender in skating)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="570" xml:space="preserve">'''Adelskalender''' ({{lang-de|Directory of Nobility}}) is a comprehensive directory of the nobility of a country or area. The best known such directory is the German [[Almanach de Gotha]] ("The Gotha") and its successor, the [[Genealogisches Handbuch des Adels]].

[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]
[[Category:Directories]]


{{royal-bio-book-stub}}
{{bio-dict-stub}}

[[cs:Adelskalender]]
[[de:Adelskalender]]
[[nl:Adelskalender]]
[[no:Adelskalender]]
[[fi:Aateliskalenteri]]
[[sv:Adelskalender]]</text>
      <sha1>393vpub7bamsvnlhtw9sp0u1s38nt1l</sha1>
    </revision>
  </page>
  <page>
    <title>Corporate Technology Directory</title>
    <ns>0</ns>
    <id>33232179</id>
    <revision>
      <id>746857071</id>
      <parentid>692300833</parentid>
      <timestamp>2016-10-30T00:24:01Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1024" xml:space="preserve">The '''Corporate Technology Directory''' also known as the '''CorpTech directory of technology companies''' was a [[Trade directory|directory]] of technology companies published from 1986&lt;!-- maybe 1987 --&gt;-2004 by [[CorpTech]]. It listed thousands of technology companies including software, services, and hardware as well as developers.

The directory was later made available in digital form as a cd&lt;ref name="SterlingBracken1998"&gt;{{cite book|last1=Sterling|first1=Christopher H.|last2=Bracken|first2=James K.|last3=Hill|first3=Susan M.|title=Mass communications research resources: an annotated guide|url=https://books.google.com/books?id=kwOo6BiWiFkC&amp;pg=PA10|accessdate=27 September 2011|year=1998|publisher=Psychology Press|isbn=978-0-8058-2024-9|pages=10–}}&lt;/ref&gt; and subsequently database subscription.

==See also==
* [[Major Information Technology Companies of the World]]

==References==
{{reflist}}
&lt;!-- https://books.google.com/books?id=96XpVBf6pvAC&amp;pg=PA246 --&gt;

{{technology-stub}}

[[Category:Directories]]</text>
      <sha1>nfty50kh02sfc01jm82d1fdotppi5sl</sha1>
    </revision>
  </page>
  <page>
    <title>Clerical Guide or Ecclesiastical Directory</title>
    <ns>0</ns>
    <id>34261059</id>
    <revision>
      <id>748236595</id>
      <parentid>740070229</parentid>
      <timestamp>2016-11-07T04:07:40Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* The Clerical Guide after 1836 */clean up; http&amp;rarr;https for [[Google Books]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5381" xml:space="preserve">{{italic title}}
The '''''Clerical Guide or Ecclesiastical Directory''''' was the earliest ever specialist directory to cover the clergy of the [[Church of England]]. In its initial format it appeared just four times – in 1817, 1822, 1829 and 1836, under the editorial direction of [[Richard Gilbert (printer)|Richard Gilbert]].

Another edition was actually advertised for 1838,&lt;ref name="paflin"&gt;[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article ''Shop-talk and mordant wit'', by Christopher Currie &amp; Glyn Paflin, describing the background to [[Crockford's Clerical Directory]]'s first hundred editions, 6–13 December 2007&lt;/ref&gt; but no copies have in fact been found within the main academic libraries.

The title was briefly revived by Thomas Bosworth &amp; Company during the 1880s.

==Contents of the Clerical Guide==

The main alphabetical section of the directory included:

*A list of benefices together with their populations, counties, dioceses and  archdeaconries
*Their incumbents with the year of his institution
*Their values (up to the 1829 edition) in the [[Valor Ecclesiasticus]] or King's Books
*The names of their patrons.
*The 1836 edition additionally gave the income of the benefice during the year 1831, the available capacity or "church room" for the congregation, and the name of any [[impropriator]].

The preliminary pages included:

*Current lists of [[bishops]], members of [[cathedral chapter]], and other dignitaries, showing the values of their [[first fruits]]
*A section on the Doctors of Laws, the [[canon law|canonical]] specialists
*A section on the [[Chapel Royal]] together with the king's preachers and chaplains
*Sections on [[Sion College]] and [[Gresham College]]
*Sections on the two English universities ([[University of Oxford|Oxford]] and [[University of Cambridge|Cambridge]])
*Sections on the fellows and schoolmasters of [[Eton College|Eton]], [[Winchester College|Winchester]], [[Westminster School|Westminster]], [[Harrow School|Harrow]], [[Manchester Grammar School|Manchester]] and [[St Paul's School, London|St Paul's]].

The alphabetical list of benefices was also followed by an alphabetical list of the prelates, dignitaries and beneficed clergy of the Church of England (generally omitting the unbeneficed clergy).

The directories concluded with lists of ecclesiastical patronage, giving the names of those benefices within the gift of the king and also those of the lord chancellor, the chancellor of the duchy of Lancaster, the various archbishops and bishops, and the two universities.

==The publishers==

The 1817 edition stated that it was "printed for [[Rivington (publishers)|J. C. &amp; F. Rivington]], 62 St Paul's Churchyard, by R. &amp; R. Gilbert, St John's Square, [[Clerkenwell]]".  '''Richard Gilbert''' was a printer and an accountant with the [[SPCK]].  Although he appeared in the 1817 edition merely as the "printer" (alongside his brother Robert, who died the following year), he thereafter seems to have taken a more prominent role in its production.  The 1822 edition was "corrected by Richard Gilbert", as though he had been engaged in putting right someone else's mistakes.  He similarly wrote the prefaces for subsequent editions, and the 1836 edition still bore the names "Gilbert and Rivington, printers, St John's Square".

Gilbert, an industrious compiler who was additionally very active in the religious life of Clerkenwell, also produced a pocket-sized '''Clergyman's Almanack''' in 1819 &lt;ref&gt;Oxford Dictionary of National Biography: article on Richard Gilbert&lt;/ref&gt;

==The Clerical Guide after 1836==

The failure of the directory to appear after 1836 left open an opportunity for a rival publication.  This was filled after 1841 by the [[Clergy List]].

After lying dormant for fifty years, the title '''Clerical Guide and Ecclesiastical Directory''' was briefly revived in 1886 by Thomas Bosworth &amp; Company, 65 [[Great Russell Street]]. Once again the volume offered alternative listings of the clergy and the benefices, together with other "valuable information … from the office of the [[Ecclesiastical Commission (Church of England)|Ecclesiastical Commission]].&lt;ref&gt;The Times newspaper, Thursday, Mar 18, 1886; pg. 12&lt;/ref&gt;  However the relaunched title was very quickly acquired by Hamilton Adams of [[Paternoster Row]], who in 1889 merged it with their other recent acquisition, the aforementioned Clergy List.&lt;ref name="paflin" /&gt;

In the issue for 1918/19 the Clergy List was merged in its turn with [[Crockford's Clerical Directory]]. Thereafter until the 1930s the latter title still continued to advertise on its preliminary pages that it "incorporated the Clergy List", together with the "Clerical Guide and Ecclesiastical Directory".

A microfiche version of the 1829 directory was produced during the 1980s by the [[Society of Genealogists]]. In more recent years scanned copies of the early editions have also appeared on the World Wide Web.&lt;ref&gt;All four editions of the Clerical Guide from 1817-1836 may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]&lt;/&lt;/ref&gt;

==See also==
*[[Clergy of the Church of England database]]

==References==
{{reflist}}

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]</text>
      <sha1>krcngalf53enuh5glmg8joo5320gjaw</sha1>
    </revision>
  </page>
  <page>
    <title>Mobile social address book</title>
    <ns>0</ns>
    <id>20893498</id>
    <revision>
      <id>564116342</id>
      <parentid>532174633</parentid>
      <timestamp>2013-07-13T15:54:34Z</timestamp>
      <contributor>
        <ip>122.162.162.222</ip>
      </contributor>
      <comment>template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3205" xml:space="preserve">
A '''mobile social address book''' is a [[phonebook]] on a [[mobile device]] that enables [[subscribers]] to build and grow their [[social networks]]. The mobile social address book transforms the phone book on any standard mobile phone into a social networking platform that makes it easier for subscribers to exchange contact information.&lt;ref&gt;[http://www.wirelessweek.com/article.aspx?id=163626 Wireless Week, retrieved 2008-12-29]&lt;/ref&gt; The mobile social address book is the convergence of [[personal information management]] (PIM) and social networking on a mobile device. While standard mobile phonebooks force users to manually enter contacts, mobile social address books automate this process by enabling subscribers to exchange contact information following a call or SMS.&lt;ref&gt;[http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9115165 Computerworld, retrieved 2008-11-5]&lt;/ref&gt; The contact information exchange occurs instantaneously and the user’s phonebook updates automatically. Mobile social address books also provide dynamic updates of contacts if their numbers change over time.

== History ==
Mobile social address books began appearing in 2007 as a parallel social trend to the emergence of Internet-based social networking sites like [[Facebook]], [[MySpace]] and [[LinkedIn]], establishing a new paradigm for interpersonal contact and communication. Mobile social address books sought to bring the connectivity of social networking to the in-the-moment experience of the mobile phone. Users can easily exchange contact information regardless of their handset, mobile carrier, or social networking application they use.&lt;ref&gt;[http://latestgeeknews.blogspot.com/2008/02/social-address-booknext-killer-app-part.html Latest Geek News, retrieved 2008-11-5]&lt;/ref&gt;

Examples of emerging companies providing technology to support mobile social address books include: [[PicDial]] (which dynamically augments the existing address book with pictures and status from Facebook, MySpace and Twitter, integrates with the call screen so during every call you see the latest picture and status of whoever is calling.  It is a network address book so everything can be managed from Windows or Mac as well and lastly you can also set your one callerID picture and status for your friends to see when you call them) FusionOne (whose backup and synchronization solutions lets users easily transfer and update mobile content, including contact information, among different devices); [[Loopt]] (whose Loopt service provides a social compass alerting users when friends are near); OnePIN (whose CallerXchange person-to-person contact exchange service lets users share contact info with one click on the mobile phone); and VoxMobili (whose Phone Backup and Synchronized Address Book solutions let users safeguard and synchronize their contact information among different devices).

== References ==
&lt;references /&gt;

==External links==
* [http://www.loopt.com Loopt website]
* [http://www.onepin.com OnePIN website]
* [http://www.voxmobili.com VoxMobili website]
* [http://www.picdial.com PicDial website]

{{Mobile phones}}

[[Category:Social networks]]
[[Category:Directories]]</text>
      <sha1>dglk0fjixa6sf5l6cbxkf0llej2gjai</sha1>
    </revision>
  </page>
  <page>
    <title>Almanach de Bruxelles</title>
    <ns>0</ns>
    <id>36297227</id>
    <revision>
      <id>704189969</id>
      <parentid>606836690</parentid>
      <timestamp>2016-02-10T02:30:40Z</timestamp>
      <contributor>
        <username>Gioto</username>
        <id>1586414</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="844" xml:space="preserve">{{one source|date=July 2012}}
The '''''Almanach de Bruxelles''''' is a [[Belgian]] website that lists [[royal family|royal]] and [[nobility|noble]] [[dynasties]] out of [[Europe]] in the form of a database. 

It was established in 1996 and lists around 2,690 world dynasties.&lt;ref&gt;créé en 1996, est le site de référence des dynasties en dehors de l'Europe...2.690 dynasties, beaucoup d'entre elles introuvables ailleurs [http://www.almanach.be/about/index.htm About the ''Almanach'']&lt;/ref&gt;

==See also==
* ''[[Almanach de Gotha]]''
* ''[[Almanach de Bruxelles (defunct)]]''

==Sources==
{{reflist}}

==External links==
*{{Official|www.almanach.be}}

[[Category:Directories]]
[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]


{{website-stub}}
{{royal-bio-book-stub}}
{{bio-dict-stub}}</text>
      <sha1>1ryx4ds4e2qnskyggi36tl4hjhkpuh1</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Yellow pages</title>
    <ns>14</ns>
    <id>36911912</id>
    <revision>
      <id>660702741</id>
      <parentid>546401675</parentid>
      <timestamp>2015-05-04T03:47:46Z</timestamp>
      <contributor>
        <username>Jdaloner</username>
        <id>4460044</id>
      </contributor>
      <comment>Removed tag for "Wikipedia categories named after texts" category since that category is for *specific* texts, and this category is for a general type of text.. Removed "NOGALLERY" tag.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="120" xml:space="preserve">{{Commons category|Yellow pages}}
{{Cat main|Yellow pages}}

[[Category:Directories]]
[[Category:Advertising by medium]]</text>
      <sha1>lcw303d802aa27ieuqe2ct86jmwweic</sha1>
    </revision>
  </page>
  <page>
    <title>Novel &amp; Short Story Writer's Market</title>
    <ns>0</ns>
    <id>7520269</id>
    <revision>
      <id>742117597</id>
      <parentid>732502581</parentid>
      <timestamp>2016-10-01T20:30:35Z</timestamp>
      <contributor>
        <ip>2600:100F:B025:E0B9:6089:9B2D:F1CB:CB77</ip>
      </contributor>
      <comment>Outdated.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2475" xml:space="preserve">{{italic title}}
[[File:Mosko.jpg|thumb|200px|right|''Novel &amp; Short Story Writer's Market'']]

'''''Novel &amp; Short Story Writer's Market''''' (''NSSWM'') is an annual resource guide for fiction writers that compiles hundreds of listings for book publishers, magazines literary agents, writing contests, and conferences. ''NSSWM'' is published by [[Writer's Digest Books]] and usually hits bookstores around August of each year.

==The Market Listings==
For 26 years, ''NSSWM'' has listed hundreds of U.S. and international magazines and book publishers who are open to submissions from fiction writers. Listings provide current contact information, editorial needs, schedules, submission guidelines, and payment and contract terms. All listings are updated annually.

==The Articles==
In addition to the market listings, the book contains interviews with and essays by best-selling and award-winning writers, as well as editors and agents.

==Writer's Digest Books==
[[File:TheFaulknerPortable.jpg|350px|right|thumb|A copy of the 1939 edition of ''Writer's Market'' rests next to William Faulkner's [[Underwood Typewriter Company|Underwood]] Universal Portable typewriter in his office at his home, [[Rowan Oak]], which is now maintained by the [[University of Mississippi]] in [[Oxford, Mississippi|Oxford]] as a museum.]]''Novel &amp; Short Story Writer's Market'' is one of eight "[[Market (economics)|market]] books" published each year by [[Writer's Digest Books]] - the most famous of which is ''[[Writer's Market]]'', a book that lists thousands of magazine and book publishers listings for writers. Others include: ''Photographer's Market'', ''Children's Writer's and Illustrator's Market'', ''Guide to Literary Agents'', ''Artist and Graphic Designer's Market'', ''Poet's Market'' and ''Songwriter's Market''. Each book is designed to give creatives instructions on how to submit work for publication.

==See also==
* [[Publishing]] 
* ''[[Writer's Digest]]''
* ''[[Writer's Market]]''
* ''[[Writers' &amp; Artists' Yearbook]]''
* [[Literary agent#Querying|query]]
* [[royalties]]
* [[Authors Guild]]

==External links==
* [http://www.writersdigest.com/competitions Official site for the competitions of Writer's Digest Books]
* [http://www.writersdigest.com ''Writer's Digest'' magazine official site]
* [http://www.fwpublications.com F+W Publications - parent company of Writer's Digest Books]

{{DEFAULTSORT:Novel and Short Story Writer's Market}}
[[Category:Directories]]</text>
      <sha1>oih5zgjucsj5wt7jbl5qq94x787xgr7</sha1>
    </revision>
  </page>
  <page>
    <title>R.L. Polk &amp; Company</title>
    <ns>0</ns>
    <id>20018380</id>
    <revision>
      <id>760215654</id>
      <parentid>759123719</parentid>
      <timestamp>2017-01-15T17:32:28Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* Company history */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: south Dakota → South Dakota using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10507" xml:space="preserve">{{Advert|date=July 2009}}
{{Infobox company
|name = R.L. Polk &amp; Company
|logo =
|type = Acquired by [[IHS Inc.]]
|foundation = [[Detroit, MI]] {1870}
|location_city =  [[Southfield, Michigan|Southfield]], MI
|area_served = Worldwide
|founder = [[Ralph Lane Polk]]
|key_people = Stephen R. Polk
* Chairman, President and CEO
Tim Rogers
* President, Polk
Richard Raines
* President, CARFAX
Michelle Goff
* Senior Vice President/Chief Financial Officer
|homepage = [http://www.polk.com www.polk.com]
|industry = Automotive
}}

'''R. L. Polk &amp; Company''' is a provider of [[automotive]] information and marketing [[solution]]s to the automotive industry, insurance companies, and related businesses.&lt;ref name="usa.polk.com"&gt;[http://usa.polk.com/Company/WhoWeAre/ R. L. Polk &amp; Company :: Our Company :: Who We Are :: Index] Retrieved on 10/31/08  {{webarchive |url=https://web.archive.org/web/20080702225115/http://usa.polk.com/Company/WhoWeAre/ |date=July 2, 2008 }}&lt;/ref&gt;

Polk was acquired by [[IHS Inc.]] on July 15, 2013 &lt;ref&gt;http://press.ihs.com/press-release/corporate-financial/ihs-completes-acquisition-rl-polk-co&lt;/ref&gt; and is based in Southfield, Michigan with operations in several countries, including the United States, Canada, Germany, United Kingdom, France, Japan, China and Australia.&lt;ref name="usa.polk.com"/&gt;

==Company history==
[[Image:1880 spine Illinois State Gazetteer by Polk &amp; Co.png|thumb|100px|left|Polk's ''Illinois State Gazetteer'', 1880]]
R. L. Polk &amp; Company was founded by [[Ralph Lane Polk]] in 1870 in Detroit, MI as a publisher of business directories. In 1872, the company first published a City Directory, for Evansville, Indiana, plus a listing of post offices in nine states. Additional directories followed in the ensuing years as the business grew.&lt;ref name=heritage&gt;[http://usa.polk.com/Company/Heritage/ R.L. Polk : Heritage]  {{webarchive |url=https://web.archive.org/web/20091229120305/http://usa.polk.com/Company/Heritage/ |date=December 29, 2009 }}&lt;/ref&gt; claiming 1000 directories by 1960.&lt;ref&gt;{{cite book|title=Polk's Abilene (Taylor County, Texas) City Directory, 1960|date=1960|publisher=R. L. Polk &amp; Co|page=7|url=http://texashistory.unt.edu/ark:/67531/metapth160223/m1/7/|accessdate=27 September 2014}}&lt;/ref&gt;  Affiliates included the Polk-Husted Directory Co. of Oakland, California.&lt;ref&gt;{{cite book |url=https://books.google.com/books?id=TNlKAQAAIAAJ&amp;pg=PA547 |title=Polk's San Jose City and Santa Clara County Directory |year=1907 }}&lt;/ref&gt; In addition to city directories, the company published bank directories.

In 1907, R.L. Polk &amp; Co. was publishing a "[[Gazetteer]]" Business directory for the State of Michigan and Windsor and Walkerville Ontario, as well as gazetteers for Alaska, Arkansas, California, Idaho, Illinois, Oklahoma, Indiana, Iowa, Kansas, Kentucky, Maryland, Minnesota, North Dakota, South Dakota, Montana, Missouri, Nevada, Oregon, Washington State, Pennsylvania, Tennessee, Texas, Utah, West Virginia, and Wisconsin.&lt;ref&gt;{{cite book|title=Michigan State Gazetteer and Business Directory|date=1907|publisher=R.L. Polk &amp; Co.|location=Detroit|page=2|edition=1907-1908|url=https://books.google.com/books?id=absfAQAAMAAJ&amp;lpg=PA250&amp;dq=%22manitou%22%20steamship%20charlevoix&amp;pg=PA81#v=onepage&amp;q=%22manitou%22%20steamship%20charlevoix&amp;f=false|accessdate=7 June 2016}}&lt;/ref&gt;

In 1921, a conversation between Ralph Lane Polk II and [[Alfred P. Sloan]] (who later became president of General Motors) helped fuel R. L. Polk &amp; Company's entry into the automotive industry. During the conversation, Sloan asked Polk to impartially tabulate and publish statistical information on cars and trucks in operation. R.L. Polk &amp; Company launched its motor vehicle statistical operations in 1922, when the first car registration reports were published.&lt;ref&gt;http://web.archive.org/web/20071116145915/http://www.salesforce.com/customers/business-services/case-studies/rlpolk.jsp Retrieved on 11/4/08&lt;/ref&gt; In 1922, R.L. Polk &amp; Co. published its first Passenger Car Registration Report, covering 58 makes and accounting for 9.2 million passenger automobiles on America's highways.

From 1951 to 1958, the company pioneered the use of electronic punch card tabulating equipment. In 1956, Polk's reporting services included monthly statistics on boats, business aircraft, motorcycles, commercial trailers, and recreational vehicles. In 1976, the National Vehicle Population Profile (NVPP) was introduced.

===1990s===
In 1993, Polk combined their Canadian activities with Blackburn Marketing Services to form Blackburn / Polk Marketing Services Inc. (BPMSI).  Polk also acquired a 35% interest in CARFAX from Blackburn Marketing Services.  In 1995, Polk entered an alliance with Marketing Systems GmBH and acquired a substantial minority interest in The Ultimate Perspective (T.U.P).

In 1996, Polk completed acquisition of the Blackburn / Polk operations and renamed it Polk Canada Marketing Services Inc. (PCMSI).  This acquisition unified and strengthened their North American operations in Polk's strategy to be a global information services provider.  They also announced their first Automotive Loyalty Award winners.

In 1997, Polk acquired the MSS division of Automatic Data Processing's European Operations.

In 1999, Polk completed acquisition of CARFAX and sold Advertising Unlimited, Inc. to Norwood Promotional Products.

===2000 and Beyond===
In 2000, Polk sells its Consumer Information Solutions (CIS) business units Direct Marketing, Data Information Services / Polk Verity, City Directory, and the Compusearch and Prospects Unlimited units of Polk Canada to Equifax.

Polk launches Garage Predictors and Polk Canada, Inc. announces Polk Canada Net. Polk also completes its acquisition of Marketing Systems Group.

Ralph Lane Polk II is inducted into the prestigious Automotive Hall of Fame (AHF) located in Dearborn, Michigan in 2001. Stephen R. Polk is also a part of the AHF as a director&lt;ref&gt;http://ias.net/ahof/v1n3/ Retrieved on 11/5/08&lt;/ref&gt; and R. L. Polk &amp; Co. is also considered a Sapphire Level Supporter.&lt;ref&gt;http://ias.net/ahof/v1n3/ Retrieved 11/5/08&lt;/ref&gt;

In 2002, Polk launches the Polk Vehicle Lifecycle System and the Polk Cross Sell is introduced.

Also in 2002, Ralph Lane Polk II is inducted into The Direct Marketing Association (DMA) Hall of Fame, the highest professional honor in direct and interactive marketing. DMA inducts into "The Hall of Fame" as many as four individuals each year for the significant impact these leaders have had on the growth of the direct and interactive process.&lt;ref&gt;http://www.the-dma.org/awards/halloffame.shtml Retrieved on 11/4/08&lt;/ref&gt;

In 2003, PolkInsight is launched. Polk Total Market Predictor (Polk TMP) is also introduced.

In 2004, R. L. Polk &amp; Company launches Polk Cross Sell Report and RLPTechnologies, a new wholly owned subsidiary, is established. Also, The [[Software Engineering Institute|Software Engineering Institute (SEI)]] awards R. L. Polk &amp; Company with a Level II Capability Maturity Model Integrated (CMMI) rating.

In 2005, R. L. Polk &amp; Company introduces the Polk Inventory Efficiency Award. The Polk Inventory Efficiency Award recognizes and rewards outstanding aftermarket companies for process improvements relative to inventory efficiency.&lt;ref&gt;http://www.reuters.com/article/pressRelease/idUS147801+21-May-2008+PRN20080521 Retrieved 11/14/08&lt;/ref&gt;

In 2007, R. L. Polk &amp; Co. acquire a majority interest in ROADTODATA, a rapidly growing supplier of automotive price and specifications data.&lt;ref&gt;http://japan.polk.com/News/LatestNews/R.+L.+Polk+and+ROADTODATA+Merge.htm Retrieved 12/26/08&lt;/ref&gt;

In 2010, R. L. Polk &amp; Company partners with Citytwist.&lt;ref&gt;https://www.ihs.com/Customer/citytwist-auto-excellence-award.html&lt;/ref&gt;

In 2013, IHS, Inc announced a $1.4B purchase of R.L. Polk.&lt;ref&gt;http://www.mlive.com/auto/index.ssf/2013/06/information_company_ihs_to_pur.html&lt;/ref&gt;

The company's business-to-business marketing services include PolkInsight, the National Vehicle Population Profile (NVPP), Blackburn / Polk Marketing Services Inc. (BPMSI), Polk Dealer Marketing Manager,&lt;ref&gt;http://google.com/search?q=cache:2uEMeAtCgckJ:findarticles.com/p/articles/mi_hb6674/is_/ai_n26650183+polk+and+Marketing+Systems+GmBH&amp;hl=en&amp;ct=clnk&amp;cd=7&amp;gl=us Retrieved on 11/4/08&lt;/ref&gt; The Ultimate Perspective (T.U.P), Polk Canada Net, Polk Vehicle Lifecycle System, Polk CrossSell Reports,&lt;ref&gt;http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&amp;STORY=/www/story/01-27-2004/0002097303&amp;EDATE= Retrieved on 11/4/08&lt;/ref&gt; and Polk Total Market Predictor (Polk TMP).{{Citation needed|date=July 2009}}

==CARFAX==

The Polk Company announced on August 2, 1999 that it had completed acquisition of [[Carfax (company)|Carfax]]. Polk had previously owned 35 percent of Carfax, in partnership with the Blackburn Group, Inc., of London, Ontario, [[Canada]], and has now acquired the remaining 65 percent.&lt;ref name="theautochannel.com"&gt;http://www.theautochannel.com/articles/press/date/19990802/press027618.html Retrieved 11/7/08&lt;/ref&gt; Carfax compiles vehicle histories from various sources, with about 75 percent of the information coming from Polk data.  Using the [[vehicle identification number]] (VIN), each history provides potential buyers with all available facts about a used car being considered for purchase.  This may include original use of the vehicle odometer records, number of owners, and other items that might affect a purchase decision.&lt;ref name="theautochannel.com"/&gt;

==See also==
* [[St. Louis City Directories]]

==References==
{{reflist}}

==Further reading==
* {{cite book |url=https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 |title=Directory of Directories |publisher=R.L. Polk &amp; Co. |location=NY |year=1916 }}

==External links==
{{commons category|R.L. Polk &amp; Co.}}
* Internet Archive. [https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 Works published by R.L. Polk &amp; Co.], various dates
* Hathi Trust. [http://catalog.hathitrust.org/Search/Home?checkspelling=true&amp;lookfor=%22polk+%26+co%22&amp;type=publisher&amp;sethtftonly=true&amp;submit=Find Works related to R.L. Polk &amp; Co.], various dates
* OCLC WorldCat. [http://www.worldcat.org/search?q=au%3A%22polk+%26+co Works related to R.L. Polk &amp; Co.], various dates

{{DEFAULTSORT:Polk and Co.}}
[[Category:Companies based in Detroit]]
[[Category:Directories|polk]]
[[Category:Publishing companies established in 1870]]
[[Category:American companies established in 1870]]</text>
      <sha1>h2kgxkh3oh7kj9d8byt0qp6k7s50c7s</sha1>
    </revision>
  </page>
  <page>
    <title>Slater's Directory</title>
    <ns>0</ns>
    <id>40452652</id>
    <redirect title="Isaac Slater" />
    <revision>
      <id>571631200</id>
      <timestamp>2013-09-05T11:36:49Z</timestamp>
      <contributor>
        <username>M2545</username>
        <id>9455233</id>
      </contributor>
      <comment>[[WP:AES|←]]Redirected page to [[Isaac Slater]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="51" xml:space="preserve">#REDIRECT[[Isaac Slater]]

[[Category:Directories]]</text>
      <sha1>kp59oxkmh8koqjxlfhdh0fdvxpqz879</sha1>
    </revision>
  </page>
  <page>
    <title>Women Environmental Artists Directory</title>
    <ns>0</ns>
    <id>41307188</id>
    <revision>
      <id>758603968</id>
      <parentid>740025150</parentid>
      <timestamp>2017-01-06T13:00:36Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[greywater]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5766" xml:space="preserve">The '''Women Environmental Artists Directory''' (WEAD) focuses on promoting environmental and [[Social justice]] art. &lt;ref&gt;{{cite web | url=http://weadartists.org/about-us | title=About Us | publisher=Women Environmental Artists Directory | accessdate=2013-08-12}}&lt;/ref&gt; WEAD was founded in 1996 by Jo Hanson, Susan Leibovitz Steinman and Estelle Akamine.&lt;ref&gt;{{cite web | url=http://greenmuseum.org/generic_content.php?ct_id=285 | title=JO HANSON: Pioneering Environmental Artist Dies in San Francisco | publisher=Green Museum | accessdate=2013-08-12 | last=Leibovitz Steinman | first=Susan}}&lt;/ref&gt; 

WEAD has been listed among the best projects relating to [[Environmental art]],&lt;ref&gt;{{cite web | url=http://www.andrew.cmu.edu/user/md2z/greenarts/artprojects.html | title=Green Arts Web: Artists &amp; Projects | publisher=Carnegie Mellon University | accessdate=2013-08-12}}&lt;/ref&gt; and has sponsored a number of exhibits about activist eco art.&lt;ref&gt;{{cite web|title=Earthly Concerns, Activist EcoArt curated by WEAD|url=http://www.usfca.edu/uploadedFiles/Destinations/Library/thacher/archive/Earthly%20Concerns.pdf|publisher=University of San Francisco|accessdate=2013-08-12}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=CONVERGENCE/DIVERGENCE SYMPOSIUM|url=http://www.losmedanos.edu/art/archive.aspx|publisher=Los Medanos College|accessdate=2013-08-12}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=WEAD East I Women and the Environment|url=http://www.kbcc.cuny.edu/artgallery/Pages/ewead.aspx|publisher=Kingsborough Community College|accessdate=2013-08-12}}&lt;/ref&gt; 

One of the co-founders, Ms. Steinman, is considered a leader in the eco art field and has participated in roundtables and artists in residences programs,&lt;ref&gt;{{cite web|title=Artist Talk with Susan Steinman|url=http://goddard.edu/news-events/events/artist-talk-susan-steinman|publisher=Goddard College|accessdate=2013-08-12}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=Eco Art Video Salon|url=http://www.berkeleyartcenter.org/programs_Q4-2010.html|publisher=Berkeley Arts Center|accessdate=2013-08-12}}&lt;/ref&gt; and is listed in the sculptor directory of the International Sculpture Center.&lt;ref&gt;{{cite web|title=Sculptor Susan Leibovitz Steinman|url=http://www.sculpture.org/portfolio/sculptorPage.php?sculptor_id=1000451|publisher=International Sculpture Center|accessdate=2013-08-12}}&lt;/ref&gt;  Another co-founder, Jo Hanson, was instrumental in founding an EPA Artist in Residence Program, which was aimed at educating the public about recycling. Another of the WEAD co-founders, Estelle Akamine, was also one of the artists in residence.&lt;ref&gt;{{cite web|title=Recology’s Artist in Residence|url=http://www.epa.gov/wastes/conserve/smm/web-academy/2011/feb11.htm|publisher=US Environmental Protection Agency|accessdate=2013-08-12}}&lt;/ref&gt; Ms. Akamine's work has also been featured at the Museum of Craft and Folk Art museum store&lt;ref&gt;{{cite web|title=Museum Store|url=http://www.mocfa.org/store/artists.htm|publisher=Museum of Craft and Folk Art|accessdate=2013-08-12}}&lt;/ref&gt; and has lectured at a textile lecture series.&lt;ref&gt;{{cite web|last=Valoma|first=Deborah|title=Textiles Lecture Series Archive|url=https://www.cca.edu/news/2012/08/27/textiles-lecture-archive|publisher=California College for the Arts|accessdate=2013-08-12}}&lt;/ref&gt; All three co-founders were featured in a discussion about women artists of the American West whose art was about current social concerns.&lt;ref&gt;{{cite web|last=Cohn|first=Terri|title=Nature, Culture and Public Space|url=http://www.cla.purdue.edu/WAAW/Cohn/index.html|publisher=Purdue University|accessdate=2013-08-12}}&lt;/ref&gt; 

The directory lists a wide variety of [[Woman artists]], such as [[Marina DeBris]], a [[trashion]] artist, [[Betty Beaumont]], often called a pioneer of environmental art, and Shai Zakai.

WEAD also published a magazine, which focuses on such topics as dirty water, and the legacy of atomic energy. A recent guest editor was Dr. Elizabeth Dougherty, founder of Wholly H2O, and speaker at events such as Pacific Gas and Electric Company conference on [[Water conservation]]&lt;ref&gt;{{cite web|title=2010 Water Conservation Showcase Speakers Save Water by Going Paperless!|url=http://www.pge.com/pec/water/presentations.shtml|publisher=Pacific Gas and Electric Company|accessdate=2013-08-12}}&lt;/ref&gt; and Toulumne County's conference on [[greywater]].&lt;ref&gt;{{cite web|title=Greywater in California:  Designing, Managing, Monitoring|url=http://portal.co.tuolumne.ca.us/psp/ps/TUP_HS_ENVIR_HEALTH/ENTP/c/TU_DEPT_MENU.TUOCM_HTML_COMP.GBL?action=U&amp;CONTENT_PNM=EMPLOYEE&amp;CATGID=2651|publisher=TUOLUMNE COUNTY ENVIRONMENTAL HEALTH|accessdate=2013-08-12}}&lt;/ref&gt; Linda Weintraub was a contributor to a recent issue of the WEAD magazine. Ms. Weintraub is the author of well known books on art and activism&lt;ref&gt;{{cite web|title=Drop Dead Gorgeous: Beauty and the Aesthetics of Activism|url=http://artsci.ucla.edu/?q=events/art-activism-linda-weintraub|publisher=UCLA Art Sci Center|accessdate=2013-08-12}}&lt;/ref&gt; such as "To Life!"&lt;ref&gt;{{cite web|title=To Life! Eco Art in Pursuit of a Sustainable Planet|url=http://www.ucpress.edu/book.php?isbn=9780520273627|publisher=University of California Press|accessdate=2013-08-12}}&lt;/ref&gt;  and is an eco art activist.&lt;ref&gt;{{cite web|last=Lambe|first=Claire|title=An Interview with Linda Weintraub – Curator of “Dear Mother Nature: Hudson Valley Artists 2012” at The Dorsky|url=http://www.rollmagazine.com/an-interview-with-linda-weintraub-%E2%80%93-curator-of-%E2%80%9Cdear-mother-nature-hudson-valley-artists-2012%E2%80%9D-at-the-dorsky/|publisher=Roll Magazine, Mark Gruber Gallery|accessdate=2013-08-12}}&lt;/ref&gt; 

==References==
{{reflist}}

[[Category:1996 introductions]]
[[Category:Directories]]
[[Category:Environmental art]]
[[Category:Women artists]]</text>
      <sha1>ho11bcza76t0r5851fkr8tz8oa2wv74</sha1>
    </revision>
  </page>
  <page>
    <title>Deutsches Geschlechterbuch</title>
    <ns>0</ns>
    <id>41858428</id>
    <revision>
      <id>683767666</id>
      <parentid>609164339</parentid>
      <timestamp>2015-10-02T10:53:03Z</timestamp>
      <contributor>
        <username>Vanasan</username>
        <id>17280035</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2037" xml:space="preserve">{{italic title}}
The '''''Deutsches Geschlechterbuch''''', until 1943 known as the '''''Genealogisches Handbuch bürgerlicher Familien''''', is a major German genealogical handbook of [[Bourgeoisie|bourgeois]] or [[patrician (post-Roman Europe)|patrician]] families. It is the bourgeois and patrician equivalent of the ''[[Genealogisches Handbuch des Adels]]'' and the former ''[[Almanach de Gotha]]''. It includes genealogies and coats of arms of the included families. The ''Genealogisches Handbuch bürgerlicher Familien'' was started in 1889 and prior to 1943, 119 volumes covering around 1,200 families were published under the original title. From 1956, the series were continued under the title ''Deutsches Geschlechterbuch''. In 2007, the 219th and latest volume was published. In total, around 4,000 families have been covered.

The ''Hamburgisches Geschlechterbuch'', comprising 17 volumes on the [[Hanseaten (class)|Hanseatic]] families of Hamburg, is an integral part of the work, and is regarded as the most comprehensive reference work of its kind on a single city.&lt;ref&gt;Hildegard von Marchthaler: ''Die Bedeutung des Hamburger Geschlechterbuchs für Hamburgs Bevölkerungskunde und Geschichte'', in: ''Hamburgisches Geschlechterbuch'', Bd. 9, Limburg an der Lahn 1961, S. XXIII&lt;/ref&gt;

The publication has been highly influential and inspired several similar publications, such as ''[[Nederland's Patriciaat]]''. To some extent it corresponds to ''[[Burke's Landed Gentry]]'' in the United Kingdom, although it could also be said to be the equivalent of ''[[Burke's Peerage]]'' in its coverage of [[Hanseaten (class)|Hanseatic]] and patrician families who comprised the highest class in the former city-republics.

==References==
{{reflist}}

==Bibliography==
*Genealogisches Handbuch bürgerlicher Familien (1889–1943)
*Deutsches Geschlechterbuch (1956-)

[[Category:Biographical dictionaries]]
[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Publications established in 1889]]


{{bio-dict-stub}}</text>
      <sha1>2gfj2xmvvg96h4db12hi5go2aax3ffy</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Indexes</title>
    <ns>14</ns>
    <id>1789509</id>
    <revision>
      <id>743809874</id>
      <parentid>724690762</parentid>
      <timestamp>2016-10-11T11:24:02Z</timestamp>
      <contributor>
        <username>Fayenatic london</username>
        <id>1639942</id>
      </contributor>
      <comment>update link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="329" xml:space="preserve">An '''index''' is something that points the reader to other [[information]]. This category is for articles about indexes.
{{distinguish|Category:Index numbers}}
{{Cat more|Index (publishing)}}
{{For|Wikipedia content|Category:Wikipedia indexes}}

[[Category:Index (publishing)]]
[[Category:Publications]]
[[Category:Directories]]</text>
      <sha1>ms524i5me7c29vucrr4zfk3445w38vr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Domain name system</title>
    <ns>14</ns>
    <id>5737409</id>
    <revision>
      <id>604572816</id>
      <parentid>582104800</parentid>
      <timestamp>2014-04-17T09:41:24Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="300" xml:space="preserve">{{Cat main|Domain Name System}}
{{Commonscat|Domain name system}}

[[Category:Internet governance]]
[[Category:Internet Standards]]
[[Category:Internet architecture]]
[[Category:Network addressing]]
[[Category:Application layer protocols]]
[[Category:Directories]]

[[ms:Kategori:Sistem nama domain]]</text>
      <sha1>dhg3yblwc3cq2nm9pyms1samvcvxnim</sha1>
    </revision>
  </page>
  <page>
    <title>Category:File system directories</title>
    <ns>14</ns>
    <id>30139425</id>
    <revision>
      <id>604573322</id>
      <parentid>547741813</parentid>
      <timestamp>2014-04-17T09:47:41Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="59" xml:space="preserve">[[Category:Computer file systems]]
[[Category:Directories]]</text>
      <sha1>bn5mnxftiscf52bq7g5z92oni8jg5tr</sha1>
    </revision>
  </page>
  <page>
    <title>Business directory</title>
    <ns>0</ns>
    <id>1725756</id>
    <revision>
      <id>739439273</id>
      <parentid>739437627</parentid>
      <timestamp>2016-09-14T18:04:03Z</timestamp>
      <contributor>
        <username>Jmuldrow</username>
        <id>29175695</id>
      </contributor>
      <minor />
      <comment>/* Formats */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2104" xml:space="preserve">{{unreferenced|date=June 2009}}

[[File:PigotDirectory1839Kent.jpg|thumb|An example page from [[Pigot's Directory|Pigot's 1839 directory]] of businesses in the counties of Kent, Surrey and Sussex in England.]]
A '''business directory''' is a website or [[print media|printed]] listing of [[information]] which lists all businesses within some category. Businesses can be categorized by business, location, activity, or size. Business may be compiled either manually or through an automated online search software.  Online [[yellow pages]] are a type of business directory, as is the traditional [[phone book]].

The details provided in a business directory varies from business to business. They may include the business name, addresses, telephone numbers, location, type of service or products the business provides, number of employees, the service region and any [[professional association]]s. Some directories include a section for user reviews, comments, and feedback. Business directories in the past would take a printed format but have recently been upgraded to websites due to the advent of the internet.

Many business directories offer complimentary listings in addition to the premium options. There are many business directories and some of these have moved over to the [[internet]] and away from printed format. Whilst not being [[search engine]]s, business directories often have a search facility.

== Formats ==
Business directories can be in either [[hard copy]] or in [[Digital formats|digital format]]. Ease of use and distribution means that many trade directories have digital version.

Online Business Directories vary in quality and content. There is a balance between professional advertising, value for money and quality of service. Business owners are looking for ROI, web traffic, exposure for their business, plus [[Search engine optimization|SEO]] benefits of [[Backlink|backlinks]].

==See also==
*[[Web directory]]
*[[Kelly's Directory]]
*[[Surplus Record Machinery &amp; Equipment Directory]]

{{DEFAULTSORT:Business Directory}}
[[Category:Business]]
[[Category:Directories]]</text>
      <sha1>mwn912yxn3y72oscdvuidyfntws2ear</sha1>
    </revision>
  </page>
  <page>
    <title>Telephone directory</title>
    <ns>0</ns>
    <id>162263</id>
    <revision>
      <id>762819645</id>
      <parentid>762819528</parentid>
      <timestamp>2017-01-30T22:34:51Z</timestamp>
      <contributor>
        <username>ESkog</username>
        <id>88149</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/2601:181:4102:29B0:888C:399F:9C05:862|2601:181:4102:29B0:888C:399F:9C05:862]] ([[User talk:2601:181:4102:29B0:888C:399F:9C05:862|talk]]): [[WP:PROMO|Advertising or promotion]] ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9175" xml:space="preserve">{{Redirect2|Phone book|White pages|a contact list|Contact list|other uses|White pages (disambiguation)}}
{{Use dmy dates|date=June 2012}}
{{refimprove|date=November 2008}}
[[File:Telefonkatalog 1928.jpg|thumb|[[Gothenburg]] telephone directory, 1928.]]

A '''telephone directory''', also known as a '''telephone book''', '''telephone address book''', '''phone book''', or the '''white/yellow pages''', is a listing of telephone [[subscriber]]s in a geographical area or subscribers to services provided by the organization that publishes the directory. Its purpose is to allow the telephone number of a subscriber identified by name and address to be found.

The advent of the Internet and [[smart phones]] in the 21st century greatly reduced the need for a paper phone book.  Some communities, such as [[Seattle]] and [[San Francisco]], sought to ban their unsolicited distribution as wasteful, unwanted and harmful to the environment.&lt;ref name=SF&gt;[http://www.sfgate.com/bayarea/article/Yellow-Pages-ruling-endangers-SF-ban-3951477.php Yellow Pages ruling endangers SF ban], Heather Knight, ''[[San Francisco Chronicle]]'', 15 October 2012; accessed 19 March 2013&lt;/ref&gt;&lt;ref&gt;[http://seattletimes.com/html/localnews/2019441687_yellowpages16m.html Appeals court rules against Seattle's curbs on yellow pages], Emily Heffter, ''[[Seattle Times]]'', 15 October 2012; accessed 19 March 2013&lt;/ref&gt;

== Content ==
Subscriber names are generally listed in alphabetical order, together with their postal or street address and [[telephone number]].  In principle every subscriber in the geographical coverage area is listed, but subscribers may request the exclusion of their number from the directory, often for a fee; their number is then said to be "unlisted" ([[American English]]), "ex-directory" ([[British English]]), "private" or '''private number''' (Australia and New Zealand), or "non-published" (Canada).&lt;ref&gt;{{cite web|url=http://support.bell.ca/Home_phone/Phone_line/How_to_unlist_my_Bell_Home_phone_number |title=How to get a non-published Bell Home phone number |publisher=Support.bell.ca |date=2013-06-17 |accessdate=2014-04-16}}&lt;/ref&gt;

A telephone directory may also provide instructions: how to use the [[Local telephone service|telephone service]], how to dial a particular number, be it local or international, what numbers to access important and [[emergency services]], utilities, hospitals, doctors, and organizations who can provide support in times of crisis. It may also have [[civil defense]] or [[emergency management]] information. There may be transit maps, postal code/zip code guides, international dialing codes or stadium seating charts, as well as advertising.

In the US, under current rules and practices, mobile phone and [[voice over IP]] listings are not included in telephone directories.  Efforts to create cellular directories have met stiff opposition from several fronts, including those who seek to avoid [[telemarketer]]s.{{Citation needed|date=January 2011}}

== Types ==
[[File:Telefonbog ubt-1.JPG|thumb|White pages.]]
A telephone directory and its content may be known by the color of the paper it is printed on.
* White pages&lt;!--redirects here--&gt; generally indicates personal or alphabetic listings.
* [[Yellow pages]], golden pages, A2Z, or classified directory is usually a "business directory", where businesses are listed alphabetically within each of many classifications (e.g., "lawyers"), almost always with paid advertising.
* [[Reverse telephone directory|Grey pages]], sometimes called a "reverse telephone directory", allowing subscriber details to be found for a given number. Not available in all jurisdictions.{{citation needed|date=March 2014}}  (These listings are often published separately, in a city directory, [[Polk directory]], or under another name, for a price, and made available to commercial and government agencies.)

Other colors may have other meanings; for example, information on [[government agencies]] is often printed on [[blue pages]] or green pages.{{Citation needed|date=September 2011}}

== Publication ==
[[File:New haven directory 1878.jpg|thumb|upright|New Haven directory, November, 1878.]]Telephone directories can be published in [[hard copy]] or in electronic form. In the latter case, the directory can be provided as an online service through proprietary terminals or over the Internet, or on physical media such as CD-ROM. In many countries directories are both published in book form and also available over the Internet. Printed directories were usually supplied free of charge.

== History ==
[[File:Unused Phonebooks.JPG|thumb|Recently delivered 2013–2014 phone books in the trash unopened; in the 21st century some communities have tried to stop the unsolicited distribution of the books&lt;ref name=SF/&gt;]]
{{Expand section|date=September 2011}}
Telephone directories are a type of [[city directory]]. Books listing the inhabitants of an entire city were widely published starting in the 18th century, before the invention of the telephone. 

The first telephone directory, consisting of a single piece of cardboard, was issued on 21 February 1878; it listed 50 individuals, businesses, and other offices in [[New Haven, Connecticut]] that had telephones.&lt;ref&gt;{{cite web| title= The Phone Book | url= http://failuremag.com/feature/article/the_phone_book/ | author=Jason Zasky | work=Failure Magazine |accessdate=2013-12-31}}&lt;/ref&gt;

The first British telephone directory was published on 15 January 1880  by The Telephone Company. It contained 248 names and addresses of individuals and businesses in London; telephone numbers were not used at the time as subscribers were asked for by name at the exchange.&lt;ref&gt;Records of the Telephone Company Limited (Bell's Patents), BT Archives reference TPA&lt;/ref&gt; The directory is preserved as part of the British phone book collection by [[BT Archives]].

In 1938, AT&amp;T commissioned the creation of a new type font, known as [[Bell Gothic|BELL GOTHIC]], the purpose of which was to be readable at very small font sizes when printed on newsprint where small imperfections were common.

In 1981 France was the first country to have an electronic directory&lt;ref&gt;{{cite web|url=http://whitepages.fr/minitel/ |title=Telephone History in France by |publisher=Whitepages.fr |date= |accessdate=2014-04-16}}&lt;/ref&gt; on a system called [[Minitel]]. The directory is called "11" after its telephone access number.

In 1991 the [[U.S. Supreme Court]] ruled (in ''[[Feist v. Rural]]'') that telephone companies do not have a [[copyright]] on telephone listings, because copyright protects creativity and not the mere labor of collecting existing information.

1996 is the year the first telephone directories go online in the USA. [[Yellowpages.com]] and [[Whitepages.com]] both see their start in April.&lt;ref&gt;[http://www.whitepages.fr/telecom-history-ft-late-with-internet.html Telephone Directory History by Whitepages.fr]&lt;/ref&gt;

In 1999, the first online telephone directories and people finding sites such as [[LookupUK.com]] go online in the UK. In 2003, more advanced UK searching including Electoral Roll become available on [[LocateFirst.com]].

In the 21st century, printed telephone directories are increasingly criticized as waste. In 2012, after some North American cities passed laws banning the distribution of telephone books, an industry group sued and obtained a court ruling permitting the distribution to continue.&lt;ref name=SF/&gt; Manufacture and distribution of telephone directories produces over 1,400,000 metric tons of [[greenhouse gases]] and consumes over 600,000 tons of paper annually.&lt;ref&gt;{{cite web|last= Paster |first= Pablo |url=http://www.treehugger.com/culture/ask-pablo-what-is-the-impact-of-all-those-unwanted-phone-books.html |title=Ask Pablo: What Is The Impact Of All Those Unwanted Phone Books? |publisher=TreeHugger |date=2010-01-11 |accessdate=2014-04-16}}&lt;/ref&gt;

== Reverse directories ==
{{main|Reverse telephone directory}}
A reverse telephone directory is sorted by number, which can be looked up to give the name and address of the subscriber.

== In popular culture ==
Ripping phone books in half has often been considered a [[Feats of strength|feat of strength]]. The Guinness World Record for ripping the most telephone directories is 27; the record for French telephone directories is 29, held by [[Georges Christen]].{{citation needed|date=October 2012}}

== See also ==
* [[Domain Name System|DNS]]
* [[Lightweight Directory Access Protocol|LDAP]]
* [[Silent number]]
* [[Whois]]
* [[City directory]]

== References ==
{{reflist|colwidth=30em}}

== Further reading ==
* {{cite book|title=The Phone Book: The Curious History of the Book That Everyone Uses But No One Reads| last= Shea|first=Ammon|publisher=Perigee Trade|year=2010|ISBN=978-0-399-53593-2}}

== External links ==
*{{Commonscat-inline|Phone books}}
*{{wikt-inline}}
* {{dmoz|Reference/Directories/Address_and_Phone_Numbers}}

{{Authority control}}

{{DEFAULTSORT:Telephone Directory}}
[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:History of the telephone]]
[[Category:American inventions]]
[[Category:1878 introductions]]</text>
      <sha1>eysyv004a9dsqlxx0b2a0gv2y34ijd6</sha1>
    </revision>
  </page>
  <page>
    <title>Encyclopedia of Associations</title>
    <ns>0</ns>
    <id>44017130</id>
    <revision>
      <id>722553795</id>
      <parentid>674600626</parentid>
      <timestamp>2016-05-28T22:14:53Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2194" xml:space="preserve">{{Italic title}}
The '''''Encyclopedia of Associations''''' (''EA'') is a comprehensive directory of more than 20,000 [[Voluntary associations|associations]], [[Society|societies]], and other non-profit membership organizations in the United States of America.&lt;ref&gt;[http://www.gale.cengage.com/DirectoryLibrary/GML33507EA%20GDL.pdf Encyclopedia of Associations]&lt;/ref&gt;

Originally titled the ''Encyclopedia of American Associations'', ''EA'' was created by [[Frederick Gale Ruffner, Jr.]] in 1954 while working as a market researcher in [[Detroit, Michigan]].&lt;ref&gt;[http://lj.libraryjournal.com/2014/08/publishing/gale-founder-frederick-ruffner-dies-at-88/#_ "Gale Founder Frederick Ruffner Dies at 88" ''Library Journal''. – Retrieved October 3, 2014]&lt;/ref&gt;

More than 140 scholarly articles have made use of ''EA''.&lt;ref&gt;[http://www.unc.edu/~fbaum/papers/JSTOR-EA-annotated-bibliography.pdf "An Annotated Bibliography of Articles Using the ''Encyclopedia of Associations''" - Retrieved October 3, 2014.]&lt;/ref&gt;

Past extracts from ''EA'' have included "Organized Obsessions" &lt;ref&gt;[http://lccn.loc.gov/92219621 - Library of Congress LCCN Permalink for 92219621]&lt;/ref&gt; and the "Gale Encyclopedia of Business and Professional Associations".&lt;ref&gt;[http://lccn.loc.gov/95649648 - Library of Congress LCCN Permalink for 95649648]&lt;/ref&gt;

A detailed history of ''EA'' is available in an article in ''Distinguished Classics of Reference Publishing''&lt;ref&gt;[https://archive.org/stream/DistinguishedClassicsOfReferencePublishing#page/n101/mode/2up - Tobin, Carol M. "The Book that Built Gale Research: The ''Encyclopedia of Associations''."  ''Distinguished Classics of Reference Publishing'']&lt;/ref&gt;&lt;ref&gt;[http://lccn.loc.gov/91033629 - Library of Congress LCCN Permalink for 91033629]&lt;/ref&gt;

== See also ==
* [[Gale Research]]
* [[Frederick Gale Ruffner, Jr.]]

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using&lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://gale.cengage.com/ Gale website]

&lt;!--- Categories ---&gt;
[[Category:Directories]]
[[Category:Specialized encyclopedias]]</text>
      <sha1>dxv24kp19m0nqpx5vtn1do4vicbm628</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Australian directories</title>
    <ns>14</ns>
    <id>45583078</id>
    <revision>
      <id>654629053</id>
      <parentid>654628936</parentid>
      <timestamp>2015-04-02T12:52:24Z</timestamp>
      <contributor>
        <username>JarrahTree</username>
        <id>278097</id>
      </contributor>
      <comment>ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="147" xml:space="preserve">{{portal|Australia}}
''Directories - of businesses, addresses and people in Australia''
[[Category:Directories]]
[[Category:Books about Australia]]</text>
      <sha1>nc6blyrxg9bkldx9utqoksnpv66o6ds</sha1>
    </revision>
  </page>
  <page>
    <title>Lighthouse Directory</title>
    <ns>0</ns>
    <id>47962147</id>
    <redirect title="Lists of lighthouses and lightvessels" />
    <revision>
      <id>741664733</id>
      <parentid>741402946</parentid>
      <timestamp>2016-09-28T23:05:30Z</timestamp>
      <contributor>
        <username>De728631</username>
        <id>4919722</id>
      </contributor>
      <comment>this is where we have an article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="93" xml:space="preserve">#REDIRECT [[Lists of lighthouses and lightvessels]]
{{R from merge}}
[[Category:Directories]]</text>
      <sha1>mwnnukqe44lzsfwq1ih7yjm3sirpk42</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Internet search engines</title>
    <ns>14</ns>
    <id>699876</id>
    <revision>
      <id>666714753</id>
      <parentid>666713139</parentid>
      <timestamp>2015-06-13T03:52:22Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>-Category:Data search engines (redundant)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="335" xml:space="preserve">{{Commons category|Internet search engines}}
General [[search engine (computing)|search engine]]s that search for information on the [[Internet]]. 

[[Category:Websites|Search engines]]
[[Category:Internet search]]
[[Category:Online databases]]
[[Category:Indexes]]
[[Category:Aggregation websites]]
[[Category:Search engine software]]</text>
      <sha1>hnpjlqhdvf7bl1lb4vllx0k7m91ej7n</sha1>
    </revision>
  </page>
  <page>
    <title>Prospective search</title>
    <ns>0</ns>
    <id>3345817</id>
    <revision>
      <id>687750579</id>
      <parentid>687750527</parentid>
      <timestamp>2015-10-27T14:30:41Z</timestamp>
      <contributor>
        <ip>90.29.27.135</ip>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2268" xml:space="preserve">{{one source|date=April 2014}}
'''Prospective search''', or '''persistent search''', is a method of [[Search engine technology|searching]] on the [[Internet]] where the query is given first and the information for the results are then acquired. This differs from traditional, or "retrospective", search such as [[search engines]], where the information for the results is acquired and then queried.&lt;ref name=globalpr2005&gt;{{cite web|url=http://www.globalprblogweek.com/2005/09/21/wyman-reputation-management/|title=Blogs &amp; Prospective Search Technology for Corporate Reputation Management|year=2005|author=Bob Wyman|publisher=Global PR Blog Week website}}&lt;/ref&gt;

== Comparison to retrospective search ==
Retrospective search starts by gathering the information, indexing it, then letting users query the information. The results don't change until the index is rebuilt, often months apart. Prospective search starts with the user's queries, gathers the information in a targeted way, indexing it and then providing the results as they arrive. Sometimes [[Ping blog|Ping Servers]] are used to gather notification of changes to websites so that the information received is as fresh as possible. Users can be notified in a number of ways of new results.

Prospective search is well suited to queries where the results change over time, such as the current news, [[blog]]s and trends.

== See also ==
* [[PubSub]]
* [[Google Alerts]]
* Google AppEngine Prospective Search Service&lt;ref&gt;https://code.google.com/appengine/docs/python/prospectivesearch/&lt;/ref&gt; (deprecated as of December 1st 2015&lt;ref&gt;https://cloud.google.com/appengine/docs/deprecations/prospective_search?hl=en&lt;/ref&gt;)
* [[Selective dissemination of information]]
* [[Superfeedr]] ('tracker' API&lt;ref&gt;http://blog.superfeedr.com/full-text-trackers/&lt;/ref&gt;)

== Quotes ==
{{quote|Prospective search is emerging as a way of keeping up-to-date on any subject of interest. This technology constantly monitors relevant blogs and Web feeds for matches to users’ subscriptions and delivers results in real time. Thus, users are notified whenever something new appears on their subject of choice|Global PR Blog Week&lt;ref name=globalpr2005/&gt;}}

==References==
{{reflist}}

[[Category:Internet search]]


{{compu-prog-stub}}</text>
      <sha1>lwz4e9pua2z3eo15528hftnpyolxa7n</sha1>
    </revision>
  </page>
  <page>
    <title>VisualRank</title>
    <ns>0</ns>
    <id>17303714</id>
    <revision>
      <id>684863454</id>
      <parentid>684863392</parentid>
      <timestamp>2015-10-09T07:06:32Z</timestamp>
      <contributor>
        <ip>125.22.103.70</ip>
      </contributor>
      <comment>/* Methods */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3277" xml:space="preserve">'''VisualRank''' is a system for [[image retrieval|finding]] and ranking images by analysing and comparing their content, rather than searching image names, Web links or other text.  [[Google]] scientists made their VisualRank work public in a paper describing applying [[PageRank]] to Google image search at the International World Wide Web Conference in [[Beijing]] in 2008.
&lt;ref name=Jing08&gt;
{{cite journal
 | author = Yushi Jing and Baluja, S.
 | title = VisualRank: Applying PageRank to Large-Scale Image Search
 | journal = Pattern Analysis and Machine Intelligence, IEEE Transactions on
 | year = 2008
 | volume = 30
 | number = 11
 | pages = 1877–1890
 | ISSN = 0162-8828
 | doi = 10.1109/TPAMI.2008.121}}.
&lt;/ref&gt;

&lt;blockquote&gt;
We cast the image-ranking problem into the task of identifying "authority" nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images. The images found to be "authorities" are chosen as those that answer the image-queries well. 
&lt;/blockquote&gt;

==Methods==
Both [[computer vision]] techniques and [[locality-sensitive hashing]] (LSH) are used in the VisualRank [[algorithm]].  Consider an image search initiated by a text query.  An existing search technique based on image metadata and surrounding text is used to retrieve the initial result candidates ([[PageRank]]), which along with other images in the index are clustered in a [[Graph (data structure)|graph]] according to their similarity (which is precomputed).  [[Centrality]] is then measured on the clustering, which will return the most canonical image(s) with respect to the query.  The idea here is that agreement between users of the web about the image and its related concepts will result in those images being deemed more similar.  VisualRank is defined iteratively by &lt;math&gt;VR = S^* \times VR&lt;/math&gt;, where &lt;math&gt;S^*&lt;/math&gt; is the image similarity matrix.  As matrices are used, [[eigenvector centrality]] will be the measure applied, with repeated multiplication of &lt;math&gt;VR&lt;/math&gt; and &lt;math&gt;S^*&lt;/math&gt; producing the [[eigenvector]] we're looking for.  Clearly, the image similarity measure is crucial to the performance of VisualRank since it determines the underlying graph structure.

The main VisualRank system begins with local feature vectors being extracted from images using [[scale-invariant feature transform]] (SIFT).  Local feature descriptors are used instead of color histograms as they allow similarity to be considered between images with potential rotation, scale, and perspective transformations. Locality-sensitive hashing is then applied to these feature vectors using the [[locality-sensitive hashing#methods|p-stable distribution scheme]].  In addition to this, LSH amplification using AND/OR constructions are applied.  As part of the applied scheme, a [[Gaussian distribution]] is used under the [[L2 norm#Euclidean norm|&lt;math&gt;l_2&lt;/math&gt; norm]].

==References==
{{Reflist}}

==External links==
*[http://www.nytimes.com/2008/04/28/technology/28google.html?adxnnl=1&amp;ref=business&amp;adxnnlx=1210140241-DOwaJr/5AjMPCYJDerw++Q New York Times article]
*[http://tech.slashdot.org/article.pl?sid=08/04/28/1852254&amp;from=rss Slashdot article]
[[Category:Internet search]]
[[Category:Image processing]]</text>
      <sha1>8rsfn6od6jutfs0knmdoln12c6h5371</sha1>
    </revision>
  </page>
  <page>
    <title>Search link optimization</title>
    <ns>0</ns>
    <id>23265516</id>
    <revision>
      <id>644601921</id>
      <parentid>590641130</parentid>
      <timestamp>2015-01-28T20:36:07Z</timestamp>
      <contributor>
        <username>Gmodi94</username>
        <id>19457248</id>
      </contributor>
      <minor />
      <comment>/* External links and references */ 
The Link was broken
Deleting a broken link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3292" xml:space="preserve">{{orphan|date =August 2009}}

'''Search Link Optimization''' ('''SLO''') is a process by which internal and external incoming links are optimized for [[search engine]] algorithms to determine the relevance of [[web page]] content.  Relevant [[anchor text]] integration, text that contains keywords for optimizing a web page, is key to this process.

==Inbound links, outbound links, internal links==

Inbound and outbound links are those that hyperlink two independent web pages together whereas inbound links would hyperlink domain “A” to domain “B” and outbound links would hyperlink domain “B” to domain “A.”

Inbound and outbound links are essential to web page visibility often enhancing web page relevance, ranking, &amp; placement.  There are few instances where inbound links would be discouraged.  Outbound links however should be given sparingly and should only link material to other material of same or similar relevance.  Often, developers will utilize a [[nofollow]] tag used mostly to further optimize hyperlinks by “instructing” search engines not to distribute any [[PageRank]] from the hyperlink.  An example of a nofollow tag might be:

&lt;syntaxhighlight lang="html5"&gt;
&lt;a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia" rel="nofollow"&gt;Wikipedia, Online Encyclopedia&lt;/a&gt;
&lt;/syntaxhighlight&gt;

[[Internal link]]s are those that hyperlink within a single domain. Hyperlinks listed higher within the source code typically gain greater relevance. Some developers use a  practice known as PageRank Sculpting by using the nofollow tag to adjust the flow of PageRank.

==Proper coding of hyperlinks==

The mere presence of hyperlinks within a web page may not yield desired optimization results. For example, when coding a web page about "blue widgets," anchor text containing links referencing "red widgets" may alter relevance which can result in gain/loss ranking scenario where “blue widgets” gains while “red widgets” actually loses position.  Moreover, another result can be a complete loss of overall web page ranking altogether for either keyword.

A properly coded keyword contains these elements: Relevant anchor text, relevant keyword titling, and compliance-based [[Character encodings in HTML|HTML code]] structures.  Additionally, use of titles that are linked within the [[HTML element|title tag]] is also a recommended practice provided the title tag for the web page has also been properly optimized for the desired keyword(s).  Below is the proper coding of a hyperlink:

&lt;syntaxhighlight lang="html5"&gt;
&lt;a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia"&gt;Wikipedia, Online Encyclopedia&lt;/a&gt;
&lt;/syntaxhighlight&gt;

==External links and references==
The footnotes below are given in support of the statements above. Because some facts are proprietary secrets held by private companies and therefore not documented in journals, such facts are reasoned from facts that are public.
* [http://www.mattcutts.com/blog/pagerank-sculpting/ PageRank Sculpting]
* [http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html Prevent Comment Spam]
* [http://www.textlinks2u.com/search_engine_optimization.html Search Engine Optimization]

{{DEFAULTSORT:Search Link Optimization}}
[[Category:Internet search]]</text>
      <sha1>i3u6jxki6f5x616rihu4elwnlfzoj9l</sha1>
    </revision>
  </page>
  <page>
    <title>SpyFu</title>
    <ns>0</ns>
    <id>25580778</id>
    <revision>
      <id>744129094</id>
      <parentid>683305395</parentid>
      <timestamp>2016-10-13T08:48:41Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3933" xml:space="preserve">'''SpyFu''', originally GoogSpy, is a [[search analytics]] company based out of Scottsdale, AZ. Started in April 2005, SpyFu shows the keywords that websites buy on [[Google Adwords]]&lt;ref&gt;{{Cite web|url=http://searchenginewatch.com/3632613|accessdate=December 28, 2009|title=Advanced Keyword Research Checklist: Using Multiple Datasets}}&lt;/ref&gt; as well as the keywords that websites are showing up for within search results. The service also gives cost per click and search volume statistics on keywords and uses that data to approximate what websites are spending on advertising.&lt;ref&gt;{{Cite web|url=http://www.entrepreneur.com/ebusiness/searchoptimization/searchengineoptimizationcolumnistjonrognerud/article175856.html|accessdate=December 28, 2009|title=Using the Competition to Boost Your SEO Performance }}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.pcworld.com/businesscenter/article/141728/top_11_moneywasting_adwords_mistakes.html|accessdate=December 28, 2009|title=Top 11 Money-Wasting AdWords Mistakes}}&lt;/ref&gt; Historical advertising budgets offered by SpyFu also help advertisers project what an advertising campaign will cost in the future.&lt;ref&gt;{{Cite web|url=http://searchengineland.com/spying-on-your-paid-search-competitors-13235|accessdate=December 28, 2009|title=Spying On Your Paid Search Competitors}}&lt;/ref&gt; The main value proposition is to see or to "spy on" the keywords that competitors use and improve [[Search Engine Marketing|SEM]] and [[Search Engine Optimization|SEO]] strategies based on those.&lt;ref&gt;{{Cite news|url=http://www.wired.com/epicenter/2009/06/coolsearchengines/|accessdate=December 28, 2009|title=Cool Search Engines That Are Not Google | work=Wired|first=Ryan|last=Singel|date=June 30, 2009}}&lt;/ref&gt; SpyFu's data was also used in the [[Washington Post]] during the [[United States presidential election, 2008|2008 Presidential election]] to disclose various keywords that candidates were advertising on.&lt;ref&gt;{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/10/15/AR2008101503574_2.html?sid=ST2008101503923|accessdate=December 28, 2009|title=In Targeting Online Ads, Campaigns Ask: Who's Searching for What? | work=The Washington Post | first=Peter | last=Whoriskey | date=October 16, 2008}}&lt;/ref&gt; SpyFu can also uncover emerging or niche markets.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=DrQtbOroN0UC&amp;pg=PT188&amp;dq=spyfu&amp;ei=oz44S4ehGqmKkATX1KjNAQ&amp;cd=1#v=onepage&amp;q=spyfu&amp;f=false|accessdate=December 28, 2009|title=The Findability Formula | first=Heather F. | last=Lutze | isbn=978-0-470-42090-4 | year=2009 | publisher=John Wiley and Sons}}&lt;/ref&gt; SpyFu has been mentioned in ''[[4_hour_work_week|The 4-Hour Work Week]]'', Oreilly's ''[[Complete Web Monitoring]]'', and ''[[SEO Warrior]]''.

SpyFu's data is obtained via [[web scraping]], based on technology developed by [[Velocityscape]], a company that makes web scraping software. The accuracy of its data, especially advertising budgets, was found to be somewhat dependent on the size of the website in question.&lt;ref&gt;{{Cite web|url=http://www.seoptimise.com/blog/2008/09/the-small-but-great-spyfu-experiment.html|accessdate=December 28, 2009|title=The Small (but Great) SpyFu Experiment}}&lt;/ref&gt; SpyFu refreshes its data on a monthly basis, and as such is used as a guide to what's happening with larger trends in SEM/SEO rather than as a real time tracking engine.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=5G4kfJ3DG2kC&amp;pg=PA113&amp;dq=spyfu&amp;ei=gvg4S-WvLZPIlAT3nuzSAQ&amp;cd=2#v=onepage&amp;q=spyfu&amp;f=false|accessdate=December 28, 2009|title=The complete guide to Google advertising | first=Bruce C. | last=Brown | isbn=978-1-60138-045-6 | year=2007 | publisher=Atlantic Publishing Company}}&lt;/ref&gt;
==References==
{{Reflist}}

==External links==
* [http://spyfu.com/ SpyFu Corporate Website]

[[Category:Internet search]]
[[Category:Companies based in Scottsdale, Arizona]]
[[Category:Companies established in 2005]]</text>
      <sha1>k871ub9igcw2zhk2ev96iagtsysqlvo</sha1>
    </revision>
  </page>
  <page>
    <title>Google (verb)</title>
    <ns>0</ns>
    <id>375665</id>
    <revision>
      <id>759870388</id>
      <parentid>759688628</parentid>
      <timestamp>2017-01-13T16:30:56Z</timestamp>
      <contributor>
        <username>AnotherOnymous</username>
        <id>8638379</id>
      </contributor>
      <comment>/* Causes */ Copyedit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9857" xml:space="preserve">{{cleanup|reason= Substantive entry outdated, summary does not refer to content in body of entry, body reads like dictionary entry for 'ungoogleable')|date=October 2016}}

{{about|the verb|the use of the verb in cricket|Googly|other uses|Google (disambiguation)}}
{{redirect|Googled|the book of the same name|Googled: The End of the World as We Know It}}
{{wiktionary|google}}
As a result of the increasing popularity and dominance of the [[Google Search|Google search engine]],&lt;ref&gt;{{cite web |last=Burns |first=Enid |date=June 19, 2007 |url=http://searchenginewatch.com/showPage.html?page=3626208 |title=Top 10 Search Providers, April 2007 |publisher=SearchEngineWatch.com |accessdate=2007-08-11 }}&lt;/ref&gt; usage of the [[transitive verb]]&lt;ref&gt;{{cite web|url=http://www.merriam-webster.com/dictionary/google |title=Google - Definition and More from the Free Merriam-Webster Dictionary |publisher=Merriam-webster.com |date= |accessdate=2011-09-19}}&lt;/ref&gt; '''to google''' (also spelled '''Google''') grew ubiquitously. The [[neologism]] commonly refers to searching for information on the [[World Wide Web]], regardless of which [[search engine]] is used.&lt;ref&gt;{{cite web|url=http://www.thelinguafile.com/2013/02/how-google-became-verb.html |title=How Google Became a Verb |publisher=The Lingua File - The Language Blog |date= |accessdate=2013-11-22}}&lt;/ref&gt; The [[American Dialect Society]] chose it as the "most useful word of 2002."&lt;ref&gt;{{cite web |date=January 13, 2003 |url=http://www.americandialect.org/index.php/amerdial/2002_words_of_the_y/ |title=2002 Words of the Year |publisher=American Dialect Society |accessdate=2007-08-11 }}&lt;/ref&gt; It was added to the ''[[Oxford English Dictionary]]'' on June 15, 2006,&lt;ref&gt;Bylund, Anders. "[http://www.fool.com/investing/dividends-income/2006/07/05/to-google-or-not-to-google.aspx To Google or Not to Google]." ''[[The Motley Fool]].'' July 5, 2006. Retrieved on March 28, 2007.&lt;/ref&gt; and to the eleventh edition of the ''[[Merriam-Webster|Merriam-Webster Collegiate Dictionary]]'' in July 2006.&lt;ref&gt;Harris, Scott D. "[http://www.mercurynews.com/mld/mercurynews/business/14985574.htm Dictionary adds verb: to google]." ''[[San Jose Mercury News]].'' July 7, 2006. Retrieved on July 7, 2006.&lt;/ref&gt;

==Etymology==
The first recorded usage of ''google'' used as a [[participle]], thus supposing an [[intransitive verb]], was on July 8, 1998, by [[Google]] co-founder [[Larry Page]] himself, who wrote on a mailing list: "Have fun and keep googling!"&lt;ref&gt;{{cite web |last=Page |first=Larry |authorlink=Larry Page |date=July 8, 1998 |url=http://www.egroups.com/group/google-friends/3.html |title=Google Search Engine: New Features |publisher=Google Friends Mailing List |accessdate=2007-08-06 |archiveurl=https://web.archive.org/web/19991009052012/http://www.egroups.com/group/google-friends/3.html |archivedate=1999-10-09 }}&lt;/ref&gt; Its earliest known use (as a transitive verb) on American television was in the "[[Help (Buffy episode)|Help]]" episode of ''[[Buffy the Vampire Slayer (TV series)|Buffy the Vampire Slayer]]'' (October 15, 2002), when [[Willow Rosenberg|Willow]] asked [[Buffy Summers|Buffy]], "Have you googled her yet?"&lt;ref&gt;{{Cite book |title=Digital Wars: Apple, Google, Microsoft and the Battle for the Internet |last=Arthur |first=Charles |year=2012 |publisher=Kogan Page Publishers |location= |isbn= |page=48 |url=https://books.google.com/books?id=IXiYi-dQenEC&amp;pg=PA48#v=onepage&amp;q&amp;f=false |accessdate=January 2, 2013 }}&lt;/ref&gt;
&lt;!-- Fearing the [[generic trademark|genericizing]] and potential loss of its [[trademark]], Google has discouraged use of the word as a verb, particularly when used as a synonym for general web searching. --&gt; 
On February 23, 2003,&lt;ref&gt;{{cite web |last=McFedries |first=Paul |date=February 23, 2003 |url=http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0302D&amp;L=ads-l&amp;P=R2450 |title=Google trademark concerns |publisher=American Dialect Society Mailing List |accessdate=2007-08-11 }}&lt;/ref&gt; the company sent a [[cease and desist]] letter to [[Paul McFedries]], creator of [[Word Spy]], a website that tracks [[neologism]]s.&lt;ref&gt;Duffy, Jonathan. "[http://news.bbc.co.uk/2/hi/uk_news/3006486.stm Google calls in the 'language police']." ''[[BBC News]].'' June 20, 2003. Retrieved on July 7, 2006.&lt;/ref&gt; In an article in the ''[[Washington Post]]'', Frank Ahrens discussed the letter he received from a Google lawyer that demonstrated "appropriate" and "inappropriate" ways to use the verb "google".&lt;ref&gt;{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2006/08/04/AR2006080401536.html|title=So Google Is No Brand X, but What Is 'Genericide'?|author=Frank Ahrens|date=2006-08-05|accessdate=2006-08-05|publisher=Washington Post}}&lt;/ref&gt; It was reported that, in response to this concern, [[lexicographer]]s for the ''Merriam-Webster Collegiate Dictionary'' lowercased the actual entry for the word, ''google'', while maintaining the capitalization of the search engine in their definition, "to use the [[Google search|Google search engine]] to seek online information" (a concern which did not deter the Oxford editors from preserving the history of both "cases").&lt;ref&gt;Noon, Chris. "[http://www.forbes.com/2006/07/06/page-brin-google-cx_cn_0706autofacescan01.html Brin, Page See 'Google' Take Its Place In Dictionary]." ''[[Forbes]].'' July 6, 2006. Retrieved on July 7, 2006.&lt;/ref&gt; On October 25, 2006, Google sent a request to the public requesting that "You should please only use 'Google' when you’re actually referring to Google Inc. and our services."&lt;ref&gt;{{cite web |last=Krantz |first=Michael |date=October 25, 2006 |url=http://googleblog.blogspot.com/2006/10/do-you-google.html |title=Do you "Google?" |publisher=The Official Google Blog |accessdate=2007-08-11 }}&lt;/ref&gt;

==Ungoogleable==
{{main|Censorship by Google|Deep Web (search indexing)}}
{{wiktionary|unGoogleable}}
Ungoogleable, (or unGoogleable) is a term for something that cannot be "googled" – i.e. it is a term for something that cannot be found easily using the [[Google Search]] [[web search engine]]. It is increasingly used to mean something that cannot be found using any web search engine.&lt;ref&gt;{{cite news| url=http://www.bbc.co.uk/news/magazine-21956743 | title=Who, What, Why: What is 'ungoogleable'? | publisher=[[BBC]] |work=[[BBC News Magazine]] | date=27 March 2013 | accessdate=5 April 2013 }}&lt;/ref&gt;

In 2013 the [[Swedish Language Council]] attempted to include the [[Swedish language|Swedish]] version of the word ("''[[:sv:Ogooglebar|ogooglebar]]''") in its list of new words, but Google objected to the definition not being specifically related to Google, and the Council was forced to briefly remove it to avoid a legal confrontation with Google.&lt;ref&gt;{{cite news| url=http://www.bbc.co.uk/news/world-europe-21944834 | title=Google gets ungoogleable off Sweden's new word list | first=Sean | last=Fanning | publisher=[[BBC]] | work=[[BBC News]] | date=26 March 2013 | accessdate=5 April 2013 }}&lt;/ref&gt;&lt;ref&gt;{{cite news| url=http://www.independent.co.uk/news/world/europe/ungoogleable-removed-from-list-of-swedish-words-after-row-over-definition-with-google-8550096.html | title='Ungoogleable' removed from list of Swedish words after row over definition with Google: California based search engine giant asked Swedish to amend definition | first=Rob | last=Williams | newspaper=[[The Independent]] | date=26 March 2013 | accessdate=5 April 2013 }}&lt;/ref&gt;

===Causes===
Google Search generally ignores punctuation and [[letter case]] even when using the "quotation" operator to denote exact searches.&lt;ref&gt;[https://support.google.com/websearch/answer/2466433?hl=en Search operators - Search Help]&lt;/ref&gt; Thus, Google may not be able to differentiate terms for which punctuation impacts meaning{{--}}for example, "man eating chicken" and "man-eating chicken" (the former meaning a human who is consuming chicken meat and the latter a chicken that eats humans). Because Google treats upper and lower case letters as one and the same, it also is unable to differentiate between the pronoun ''[[he]]'' and the surname ''[[He (surname)|He]]'', which, when combined with its disregard for punctuation, could bury results for an obscure person named &lt;code&gt;"Thomas He"&lt;/code&gt; among results such as:
:&lt;q&gt;... Assisted by '''Thomas, he''' was able to provide incontrovertible proof of this theory, and in so doing, he gained wide recognition in the medical ...&lt;/q&gt;&lt;ref&gt;[https://www.google.com/webhp?hl=en&amp;sa=X#hl=en&amp;q=%22Thomas+He%22 "Thomas He" - Google Search]&lt;/ref&gt;

The above also exemplifies how Google's [[PageRank]] algorithm, which sorts results by "importance", could also cause something to become ungoogleable: results for those with the 17th most common Chinese surname&lt;ref&gt;{{cite web|url=http://cdn.theatlantic.com/newsroom/img/posts/2013/10/chinassurnames/0886ab335.jpg |title=China's Surnames |publisher=Cdn.theatlantic.com |accessdate=2016-07-12}}&lt;/ref&gt; are difficult to separate from results containing the 16th [[most common words in English|most common word in English]]. In other words, a specific subject may be ungoogleable because its results are a [[wikt:needle in a haystack|needle in a haystack]] of results for a more "important" term.

==See also==
{{Portal|Internet}}
* [[grep#Usage as a verb|grep]]&lt;!--lowercase--&gt;
* [[Swedish_Language_Council#Controversy|Ogooglebar, Swedish for Ungoogleable]]
* [[Photo manipulation#Photoshopping|Photoshop (verb)]], a similar neologism referring to digital photo editing

==References==
{{reflist|30em}}

{{Google Inc.}}

{{DEFAULTSORT:Google (Verb)}}
[[Category:Google]]
[[Category:Verbs]]
[[Category:Internet terminology]]
[[Category:Internet search]]
[[Category:Words coined in the 1990s]]
[[Category:Computer-related introductions in 1998]]

[[ja:Google#派生語]]
[[ru:Google (компания)#to google]]</text>
      <sha1>5r8iaa12icr25yaq7bpur64wneeyuuv</sha1>
    </revision>
  </page>
  <page>
    <title>URL redirection</title>
    <ns>0</ns>
    <id>636686</id>
    <revision>
      <id>756414315</id>
      <parentid>753147294</parentid>
      <timestamp>2016-12-24T02:30:12Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor />
      <comment>Bot: [[User:FrescoBot/Links|link syntax]] and minor changes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32585" xml:space="preserve">{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}
{{Merge from|Rewrite engine|discuss=Talk:URL redirection#Merge Rewrite engine|date=November 2015}}
{{refimprove|date=December 2015}}
'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, domain redirection or domain forwarding is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org]. URL redirection is done for various reasons: for [[URL shortening]]; to prevent [[link rot|broken links]] when web pages are moved; to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]]; to guide navigation into and out of a website; for privacy protection; and for less innocuous purposes such as [[phishing]] attacks.

== Purposes ==
There are several reasons to use URL redirection:

=== Similar domain names ===
A user might mistype a URL, for example, "example.com" and "exmaple.com". Organizations often register these "misspelled" domains and redirect them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.

=== Moving pages to a new domain ===
Web pages may be redirected to a new domain for three reasons:
* a site might desire, or need, to change its domain name;
* an author might move his or her individual pages to a new domain;
* two web sites might merge.

With URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers. The same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.

=== Logging outgoing links ===
The access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link. This information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.&lt;ref&gt;
{{cite journal
  | title = Google revives redirect snoopery
  | journal = blog.anta.net
  | date = 2009-01-29
  | url = http://blog.anta.net/2009/01/29/509/
  | issn = 1797-1993
  | archiveurl=https://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/
  | archivedate=2011-08-17
}}&lt;/ref&gt; The same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.

=== Short aliases for long URLs ===
{{Main article|URL shortening}}

Web applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.

=== Meaningful, persistent aliases for long or changing URLs ===
{{See also|Permalink|PURL|Link rot}}

Sometimes the URL of a page changes even though the content stays the same. Therefore, URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.

=== Post/Redirect/Get ===
{{Main article|Post/Redirect/Get}}

Post/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).

=== Device targeting and geotargeting ===

Redirects can be effectively used for targeting purposes like [[device targeting]] or [[geotargeting]]. Device targeting has become increasingly important with the rise of mobile clients. There are two approaches to serve mobile users: Make the website [[responsive web design|responsive]] or redirect to a mobile website version. If a mobile website version is offered, users with mobile clients will be automatically forwarded to the corresponding mobile content. For device targeting, client side redirects or non-cacheable server side redirects are used. Geotargeting is the approach to offer localized content and automatically forward the user to a localized version of the requested URL. This is helpful for websites that target audience in more than one location and/or language. Usually server side redirects are used for Geotargeting but client side redirects might be an option as well, depending on requirements.&lt;ref&gt;{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects &amp; SEO - The Total Guide |accessdate=2015-11-29 |publisher=Audisto}}&lt;/ref&gt;

=== Manipulating search engines ===
Redirects have been used to manipulate search engines with unethical intentions, e.g. [[sneaky redirects]] or [[URL hijacking]]. The goal of misleading redirects is to drive search traffic to landing pages, which do not have enough ranking power on their own or which are only remotely or not at all related to the search target. The approach requires a rank for a range of search terms with a number of URLs that would utilize sneaky redirects to forward the searcher to the target page. This method had a revival with the uprise of mobile devices and device targeting. URL hijacking is an off-domain redirect technique&lt;ref&gt;{{cite web|url=https://www.mattcutts.com/blog/seo-advice-discussing-302-redirects/ |title=SEO advice: discussing 302 redirects |date=4 January 2006 |publisher=Matt Cutts, former Head of Google Webspam Team}}&lt;/ref&gt; that exploited the nature of the search engine's handling for temporary redirects. If a temporary redirect is encountered, search engines have to decide whether they assign the ranking value to the URL that initializes the redirect or to the redirect target URL. The URL that initiates the redirect may be kept to show up in search results, as the redirect indicates a temporary nature. Under certain circumstances it was possible to exploit this behaviour by applying temporary redirects to well ranking URLs, leading to a replacement of the original URL in search results by the URL that initialized the redirect, therefore "stealing" the ranking. This method was usually combined with sneaky redirects to re-target the user stream from the search results to a target page. Search engines have developed efficient technologies to detect these kind of manipulative approaches. Major search engines usually apply harsh ranking penalties on sites that get caught applying techniques like these.&lt;ref&gt;{{cite web|url=https://support.google.com/webmasters/answer/2721217?hl=en |title=Sneaky Redirects |date=3 December 2015 |publisher=Google Webmaster Guidelines}}&lt;/ref&gt;

=== Manipulating visitors ===
URL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.&lt;ref&gt;{{cite web|url=https://www.owasp.org/index.php/Unvalidated_Redirects_and_Forwards_Cheat_Sheet |title=Unvalidated Redirects and Forwards Cheat Sheet |date=21 August 2014 |publisher=Open Web Application Security Project (OWASP)}}&lt;/ref&gt; Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.

=== Removing &lt;code&gt;referer&lt;/code&gt; information ===
When a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, &lt;code&gt;&lt;nowiki&gt;http://company.com/plans-for-the-next-release-of-our-product&lt;/nowiki&gt;&lt;/code&gt;), it is not desirable for the &lt;code&gt;referer&lt;/code&gt; URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example &lt;code&gt;&lt;nowiki&gt;http://externalsite.com/page&lt;/nowiki&gt;&lt;/code&gt; into &lt;code&gt;&lt;nowiki&gt;http://redirect.company.com/http://externalsite.com/page&lt;/nowiki&gt;&lt;/code&gt;. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.

== Implementation ==
Several different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.

=== Manual redirect ===
The simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:

&lt;source lang="html4strict"&gt;
Please follow &lt;a href="http://www.example.com/"&gt;this link&lt;/a&gt;.
&lt;/source&gt;

This method is often used as a fall-back&amp;nbsp;— if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.

=== HTTP status codes 3xx ===
In the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page. If a client encounters a redirect, it needs to make a number of decisions how to handle the redirect. Different status codes are used by clients to understand the purpose of the redirect, how to handle caching and which request method to use for the subsequent request.

HTTP/1.1 defines several status codes for redirection (RFC 7231):
* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)
* [[HTTP 301|301 moved permanently]]
* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)
* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)
* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)
* [[HTTP 308|308 permanent redirect]] (provides a new URL for the browser to resubmit a GET or POST request)

==== Redirect status codes and characteristics ====

{| class="wikitable"
|-
! HTTP Status Code !! HTTP Version !! Temporary / Permanent !! Cacheable !! Request Method Subsequent Request
|-
| 301 || HTTP/1.0 || Permanent || yes || GET / POST may change
|-
| 302 || HTTP/1.0 || Temporary || not by default || GET / POST may change
|-
| 303 || HTTP/1.1 || Temporary || never || always GET
|-
| 307 || HTTP/1.1 || Temporary || not by default || may not change
|-
| 308 || HTTP/1.1 || Permanent || by default || may not change
|-
|}&lt;ref&gt;{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects &amp; SEO - The Complete Guide |accessdate=2015-11-29 |publisher=Audisto}}&lt;/ref&gt;

All of these status codes require the URL of the redirect target to be given in the Location: header of the HTTP response. The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.

(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).

==== Example HTTP response for a 301 redirect ====

A [[HTTP]] response with the 301 "moved permanently" redirect looks like this:

&lt;syntaxhighlight lang="http"&gt;
HTTP/1.1 301 Moved Permanently
Location: http://www.example.org/
Content-Type: text/html
Content-Length: 174

&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Moved&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Moved&lt;/h1&gt;
&lt;p&gt;This page has moved to &lt;a href="http://www.example.org/"&gt;http://www.example.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/syntaxhighlight&gt;

==== Using server-side scripting for redirection ====
Web authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:

&lt;source lang="php"&gt;
header('HTTP/1.1 301 Moved Permanently');
header('Location: http://www.example.com/');
exit();
&lt;/source&gt;

More headers may be required to prevent caching.&lt;ref name="php-301-robust-solution"&gt;{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=https://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}&lt;/ref&gt; The programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using &lt;code&gt;response.buffer=true&lt;/code&gt; and &lt;code&gt;response.redirect &lt;nowiki&gt;"http://www.example.com/"&lt;/nowiki&gt;&lt;/code&gt; HTTP/1.1 allows for either a relative URI reference or an absolute URI reference.&lt;ref&gt;{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 7.1.2 | sectionname = Location | page = 68 | editor1 = Roy T. Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt; If the URI reference is relative the client computes the required absolute URI reference according to the rules defined in RFC 3986.&lt;ref&gt;{{cite IETF | title = Uniform Resource Identifier (URI): Generic Syntax | rfc = 3986 | section = 5 | sectionname = Reference Resolution | page = 28 | first1 = Tim | last1 = Berners-Lee | author1-link = Tim Berners-Lee | first2 = Roy T. | last2 = Fielding | author2-link = Roy Fielding | first3 = Larry | last3 = Masinter | year = 2005 | month = January | publisher = [[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt;

==== Apache mod_rewrite ====
The [[Apache HTTP Server]] mod_alias extension can be used to redirect certain requests. Typical configuration directives look like:
&lt;source lang="apache"&gt;
Redirect permanent /oldpage.html http://www.example.com/newpage.html
Redirect 301 /oldpage.html http://www.example.com/newpage.html
&lt;/source&gt;

For more flexible [[URL rewriting]] and redirection, Apache mod_rewrite can be used. E.g., to redirect a requests to a canonical domain name:
&lt;source lang="apache"&gt;
RewriteEngine on
RewriteCond %{HTTP_HOST} ^([^.:]+\.)*oldsite\.example\.com\.?(:[0-9]*)?$ [NC]
RewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]
&lt;/source&gt;

Such configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a &lt;code&gt;[[.htaccess]]&lt;/code&gt; file.

==== nginx rewrite ====
[[Nginx]] has an integrated http rewrite module,&lt;ref&gt;{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}&lt;/ref&gt; which can be used to perform advanced URL processing and even web-page generation (with the &lt;tt&gt;return&lt;/tt&gt; directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.&lt;ref&gt;{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://mdoc.su/ |title=mdoc.su — Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}&lt;/ref&gt;

For example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 &lt;tt&gt;/DragonFlyBSD/HAMMER.5&lt;/tt&gt;] were to come along, it would first be redirected internally to &lt;tt&gt;/d/HAMMER.5&lt;/tt&gt; with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:&lt;ref&gt;{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}&lt;/ref&gt;
&lt;source lang="nginx"&gt;
	location /DragonFly {
		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;
	}
	location /d {
		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";
		set	$ds	"&amp;section=";
		rewrite	^/./([^/]+)\.([1-9])$		$db$1$ds$2	redirect;
	}
&lt;/source&gt;

=== Refresh Meta tag and HTTP refresh header ===
[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.&lt;ref&gt;[http://www.w3schools.com/tags/tag_meta.asp HTML &lt;meta&gt; tag]&lt;/ref&gt;&lt;ref&gt;[http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]&lt;/ref&gt; A timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.&lt;ref&gt;[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.&lt;/ref&gt;

This is an example of a simple HTML document that uses this technique:
&lt;source lang="html4strict"&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv="Refresh" content="0; url=http://www.example.com/" /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Please follow &lt;a href="http://www.example.com/"&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

This technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.

The same effect can be achieved with an HTTP &lt;code&gt;refresh&lt;/code&gt; header:
&lt;source lang="http"&gt;
HTTP/1.1 200 ok
Refresh: 0; url=http://www.example.com/
Content-type: text/html
Content-length: 78

Please follow &lt;a href="http://www.example.com/"&gt;this link&lt;/a&gt;.
&lt;/source&gt;

This response is easier to generate by CGI programs because one does not need to change the default status code.

Here is a simple CGI program that effects this redirect:
&lt;source lang="perl"&gt;
#!/usr/bin/perl
print "Refresh: 0; url=http://www.example.com/\r\n";
print "Content-type: text/html\r\n";
print "\r\n";
print "Please follow &lt;a href=\"http://www.example.com/\"&gt;this link&lt;/a&gt;!"
&lt;/source&gt;

Note: Usually, the HTTP server adds the status line and the Content-length header automatically.

The [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].

=== JavaScript redirects ===
[[JavaScript]] can cause a redirect by setting the &lt;code&gt;window.location&lt;/code&gt; attribute, e.g.:
&lt;syntaxhighlight lang="ecmascript"&gt;
window.location='http://www.example.com/'
&lt;/syntaxhighlight&gt;
Normally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.&lt;ref&gt;{{cite web|url=http://insider.zone/tools/client-side-url-redirect-generator/|title=Cross-browser client side URL redirect generator|publisher=Insider Zone}}&lt;/ref&gt;
&lt;syntaxhighlight lang="ecmascript"&gt;
window.location.replace('http://www.example.com/')
&lt;/syntaxhighlight&gt;
However, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.

=== Frame redirects ===

A slightly different effect can be achieved by creating an inline frame:

&lt;source lang="html4strict"&gt;
&lt;iframe height="100%" width="100%" src="http://www.example.com/"&gt;
Please follow &lt;a href="http://www.example.com/"&gt;link&lt;/a&gt;.
&lt;/iframe&gt;
&lt;/source&gt;

One main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar. This ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].&lt;ref&gt;Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.&lt;/ref&gt;

Before HTML5,&lt;ref&gt;
https://www.w3.org/TR/html5/obsolete.html&lt;/ref&gt; the same effect could be done with an [[Framing (World Wide Web)|HTML frame]] that contains the target page:
&lt;source lang="html4strict"&gt;
&lt;frameset rows="100%"&gt;
  &lt;frame src="http://www.example.com/"&gt;
  &lt;noframes&gt;
    &lt;body&gt;Please follow &lt;a href="http://www.example.com/"&gt;link&lt;/a&gt;.&lt;/body&gt;
  &lt;/noframes&gt;
&lt;/frameset&gt;
&lt;/source&gt;

=== Redirect chains ===
One redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (with [[domain name]] in [[.com]]) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] (with domain name in [[.org]]), then to the [[HTTPS]] URL [[:www:URL redirection|'''https:'''//www.wikipedia.org/wiki/URL redirection]] and finally to the language-specific site https://'''en'''.wikipedia.org/wiki/URL redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''[[rewriting]]'' the URL as much as possible on the server before returning it to the browser as a redirect.

=== Redirect loops ===
Sometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.

The HTTP/1.1 Standard states:&lt;ref name="rfc7231sec6.4"&gt;{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 6.4 | sectionname = Redirection 3xx | page = 54 | editor1 = Roy T. Fielding | editor1-link = Roy Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt;
&lt;blockquote&gt;
A client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).

Note: An earlier version of this specification recommended a maximum of five redirections ([RFC 2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.
&lt;/blockquote&gt;
Note that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -&gt; http://www.example.com/2 -&gt; http://www.example.com/3 ...

== Services ==
There exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.

=== URL redirection services ===
A '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]]. Hyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.
Recently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs. A major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.

==== History ====
The first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.&lt;ref&gt;{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}&lt;/ref&gt; As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined. With the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.

=== Referrer masking ===
Redirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.) This type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .

Here is a simplistic example of such a service, written in [[PHP]].
&lt;source lang="html+php"&gt;
&lt;?php
$url = htmlspecialchars($_GET['url']);
header( 'Refresh: 0; url=http://'.$url );
?&gt;
&lt;!-- Fallback using meta refresh. --&gt;
&lt;html&gt;
 &lt;head&gt;
  &lt;title&gt;Redirecting...&lt;/title&gt;
  &lt;meta http-equiv="refresh" content="0;url=http://&lt;?php echo $url; ?&gt;"&gt;
 &lt;/head&gt;
 &lt;body&gt;
 Attempting to redirect to &lt;a href="http://&lt;?php echo $url; ?&gt;"&gt;http://&lt;?php echo $url; ?&gt;&lt;/a&gt;.
 &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

The above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.

==Security issues==
URL redirection can be abused by attackers for [[phishing]] attacks, such as [[Open Redirect|open redirect]] and [[Covert Redirect|covert redirect]]. "An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."&lt;ref name="Open_Redirect"&gt;{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}&lt;/ref&gt; "Covert redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."&lt;ref name="Covert_Redirect"&gt;{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}&lt;/ref&gt; It was disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.&lt;ref name="CNET"&gt;{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}&lt;/ref&gt;

==See also==
* [[Link rot]]
* [[Canonical link element]]
* [[Canonical meta tag]]
* [[Domain masking]]
* [[URL normalization]]
* [[Semantic URL]]

==References==
{{Reflist}}

==External links==
* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]
* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)
* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification

{{Spamming}}

{{Use dmy dates|date=November 2010}}

{{DEFAULTSORT:Url Redirection}}
[[Category:Uniform Resource Locator]]
[[Category:Black hat search engine optimization]]
[[Category:Internet search]]
[[Category:Internet terminology]]</text>
      <sha1>swx8dpnrwvs691ebuiurh0wnhg81a09</sha1>
    </revision>
  </page>
  <page>
    <title>User intent</title>
    <ns>0</ns>
    <id>52689741</id>
    <revision>
      <id>762244517</id>
      <parentid>761220138</parentid>
      <timestamp>2017-01-27T16:01:02Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo/[[WP:AWB/GF|general]] fixes, replaced: the the → the, [[WP:AWB/T|typo(s) fixed]]: ie.  → i.e. using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7836" xml:space="preserve">'''User intent''' or '''query intent''' is the identification and categorization of what a user online intended or wanted when they typed their [[web search query|search terms]] into an online [[web search engine]] for the purpose of [[search engine optimization]] or [[conversion rate optimization]].&lt;ref name="Understanding Sponsored Search: Core Elements of Keyword Advertising"&gt;{{cite book|last1=Jansen|first1=Jim|title=Understanding Sponsored Search: Core Elements of Keyword Advertising|date=July 2011|publisher=Cambridge University Press|location=New York, NY, USA|isbn=9781107011977|page=44|url=https://books.google.com/books?id=L4LIGyLOwDoC&amp;pg=PA44&amp;dq=what+is+user+intent&amp;hl=en&amp;gl=us&amp;sa=X&amp;redir_esc=y#v=onepage&amp;q=what%20is%20user%20intent&amp;f=false}}&lt;/ref&gt; When a user goes online, there is always a purpose, an intent. The goal can be fact-checking, comparison shopping, filling downtime, or any other activity online.&lt;ref name="The Different Types of User Intent"&gt;{{cite web|last1=Shih|first1=Joseph|title=The Different Types of User Intent|url=https://www.twinword.com/blog/understanding-different-types-user-intent/|website=Twinword Blog|publisher=Twinword, Inc.|accessdate=26 December 2016}}&lt;/ref&gt;

==Types==
Though there are various ways of classifying or naming the categories of the different types of user intent, overall they seem to follow the same clusters. In general and up until the rise and explosion&lt;ref name="The Rise of Mobile Search: From 2012 to 2015"&gt;{{cite web|title=The Rise of Mobile Search: From 2012 to 2015|url=http://www.texodesign.com.au/the-rise-of-mobile-search/|website=Texo Design|publisher=Texo Design|accessdate=26 December 2016}}&lt;/ref&gt; of [[mobile search]], there are and were [[Web search query#Types|three very broad categories]]: informational, transactional, and navigational.&lt;ref&gt;{{cite journal|last1=Broder|first1=Andrei|title=A Taxonomy of Web Search|journal=SIGIR Forum|date=Fall 2002|volume=36|issue=2|pages=5–6|url=http://www.cis.upenn.edu/~nenkova/Courses/cis430/p3-broder.pdf|accessdate=27 December 2016}}&lt;/ref&gt; However over time and with the rise&lt;ref name="The Rise of Mobile Search: From 2012 to 2015" /&gt; of [[mobile search]], other categories have appeared or categories have segmented into more specific categorization. The following is a table showing how different organizations have categorize the different types.

{| class="wikitable" style="text-align: center;"
|+ style="text-align: left;" | The Different Types of User Intents&lt;ref name="The Different Types of User Intent" /&gt;
|-
! !! Type 1 !! colspan="2" | Type 2 (a/b) !! Type 3 !! Type 4
|-
| || "who wrote the Matrix" || "online IQ test" || "office supplies" || "google play store" || "restaurants near me"
|-
| [[Microsoft]]&lt;ref&gt;{{cite journal|last1=KhudaBukhsh|first1=Ashiqur|last2=Bennett|first2=Paul|last3=White|first3=Ryen|title=Building Effective Query Classifiers: A Case Study in Self-harm Intent Detection|journal=CIKM '15 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management|date=2015|pages=1735–1738|url=http://research.microsoft.com/en-us/um/people/pauben/papers/cikm-2015-KhudaBukhsh-et-al.pdf|accessdate=26 December 2016|format=PDF}}&lt;/ref&gt; || Informational || colspan="2" | Transactional || Navigational || --
|-
| [[Google]]&lt;ref&gt;{{cite book|title=Search Quality Evaluator Guidelines|date=28 March 2016|publisher=Google|pages=61–74|url=http://static.googleusercontent.com/media/www.google.com/en//insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf|accessdate=26 December 2016}}&lt;/ref&gt; || Know || colspan="2" | Do || Website || Visit-in-person
|-
| [[Hubspot]]&lt;ref&gt;{{cite web|title=Keyword Development: Without a computer!|url=https://cdn2.hubspot.net/hub/137828/file-331703896-pdf/docs/hubspot_keyword_development_worksheet.pdf|website=Hubspot|publisher=Hubspot|accessdate=26 December 2016}}&lt;/ref&gt; || Problem based || colspan="2" | Solution based || Brand based || --
|-
| [[SEMRush]]&lt;ref&gt;{{cite web|title=Types of keywords: commercial, informational, navigational, transactional|url=https://www.semrush.com/blog/types-of-keywords-commercial-informational-navigational-transactional/|website=SEMRush Blog|publisher=SEMRush|accessdate=26 December 2016}}&lt;/ref&gt; || Informational || Commercial || Transactional || Navigational || --
|-
| [[Web Analytics World]]&lt;ref&gt;{{cite web|last1=Levitt|first1=Dean|title=Using Intent, Demographics and Micro-Moments to Better Understand your Web Traffic|url=http://www.webanalyticsworld.net/2016/09/intent-demographics-and-micro-moments-in-analytics.html|website=Web Analytics World|publisher=Jump Digital|accessdate=26 December 2016}}&lt;/ref&gt; || Know || Do || Buy || -- || Go
|-
| Summary || Know || Do || Buy || Web || Local
|}

* Know - An informational search query looking for facts or other information (e.g. "who wrote the Matrix")
* Do - A transactional search query wanting to fulfill a task online (e.g. "online IQ test")
* Buy - A transactional search query wanting to buy something (e.g. "office supplies")
* Web - A navigational search query wanting to visit to a specific web site or page (e.g. "google play store")
* Local - A search query wanting to visit-in-person a physical location (e.g. "restaurants near me")

Please note that many search queries may be ambiguous and thus may be classified into multiple intents. For example, a user who typed a query "matrix" into a search bar may want to purchase the [[The Matrix|1999 American-Australian philosophical sci-fi film]] or may want to learn more about the [[Matrix (mathematics)|matrices in mathematics]].

==Importance==
With the prevalence of search engines being the first starting point of many online sessions,&lt;ref&gt;{{cite web|last1=Purcell|first1=Kristen|title=Search and email still top the list of most popular online activities|url=https://searchenginewatch.com/sew/study/2101282/search-engines-92-adult-internet-users-study|website=Pew Research Center Internet, Science &amp; Tech|publisher=Pew Research Center|accessdate=26 December 2016}}&lt;/ref&gt; search engines are tasked with surfacing the [[Search engine results page|best results]] or best [[Online advertising|ads]] that will satisfy the various user intents. Because [[Web search engine#Search engine bias|search engines do not actually read and understand]] web pages and ad copy completely, [[Digital marketing|digital marketers]] have to align their [[Keyword research|target keywords]] to the correct user intent that they are trying to satisfy&lt;ref name="The Different Types of User Intent" /&gt; if they want to rank high on [[Search engine result page|SERPs]] and improve their [[Conversion rate optimization|conversion rate]].

Take for example, a company selling colored contact lenses who wants their ad to show up for relevant searches may target the keyword "blue eyes". However, this may not be the most effective strategy as users who search "blue eyes" may want to learn biological facts about blue eyes. Instead, the company can target keywords that clearly indicates that the user is looking to buy colored contact lenses (i.e. "blue contact lenses" most likely implies "buy blue contact lenses"). With the correct keyword intent targeting, studies have shown that conversion rates increase significantly.&lt;ref&gt;{{cite web|last1=daSilva|first1=Tiffany|title=Why Ignoring User Intent is Costing You Money in AdWords|url=http://unbounce.com/ppc/ignoring-user-intent-costs-you-money-in-adwords/|website=unbounce Pay Per Click|publisher=unbounce|accessdate=26 December 2016}}&lt;/ref&gt;

==See also==
* [[Web search query]]
* [[Keyword research]]
* [[Intent marketing]]
* [[Search engine optimization]]
* [[Conversion rate optimization]]
* [[Search engine result page]]
* [[Principle of least astonishment]]

==References==
{{reflist}}

[[Category:Internet search]]</text>
      <sha1>jxyxb0dcq6349vb3ng74epevjcyzupe</sha1>
    </revision>
  </page>
  <page>
    <title>National Centre for Text Mining</title>
    <ns>0</ns>
    <id>10795520</id>
    <revision>
      <id>729685598</id>
      <parentid>729685541</parentid>
      <timestamp>2016-07-13T21:31:31Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Services */ add word</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16006" xml:space="preserve">{{Multiple issues|
{{COI|date=December 2015}}
{{advert|date=December 2015}}
{{external links|date=December 2015}}
}}

{{Infobox academic division
| name           = National Centre for Text Mining (NaCTeM)
| image_name     = 
| image_alt      = National Centre for Text Mining 
| established    = 2004
| type           = 
| parent         = [[School of Computer Science, University of Manchester]] 
| affiliation    = [[University of Manchester]]
| city           = [[Manchester]]
| country        = [[United Kingdom]]
| director       = Prof. Sophia Ananiadou
| website        = {{URL|www.nactem.ac.uk}}
| logo           =&lt;!-- Deleted image removed:  [[File:Nactem hires.tif|300px]] --&gt;
}}

The '''National Centre for Text Mining''' ('''NaCTeM''')&lt;ref name="ariadne"&gt;{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}&lt;/ref&gt; is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community.

The [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest – examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].

The Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[natural language processing]] and [[information extraction]], including [[named-entity recognition]] and extractions of complex relationships (or events) that hold between named entitites, along with parallel and distributed data mining systems in biomedical and clinical applications.

==Services==
[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them.&lt;ref name="multi-word"&gt;{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117–132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}&lt;/ref&gt;

[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[disambiguation (metadata)|disambiguates]] them.&lt;ref name="pmid17050571"&gt;{{cite journal|vauthors=Okazaki N, Ananiadou S | title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089–95 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&amp;tool=sumsearch.org/cite&amp;retmode=ref&amp;cmd=prlinks&amp;id=17050571  }}&lt;/ref&gt;

[http://www.nactem.ac.uk/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts &lt;ref&gt;{{cite conference |author=Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J.|title=Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases |year=2006 |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1017–1024 |doi=10.3115/1220175.1220303}}&lt;/ref&gt;

[http://www.nactem.ac.uk/facta/ '''Facta+'''] is a [[Medline]]  search engine for finding associations between biomedical concepts.&lt;ref name="pmid18772154"&gt;{{cite journal|vauthors=Tsuruoka Y, Tsujii J, Ananiadou S | title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559–60 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }}&lt;/ref&gt;

[http://www.nactem.ac.uk/facta-visualizer/ '''Facta+ Visualizer'''] is a web application that aids in understanding FACTA+ search results through intuitive graphical visualisation.&lt;ref&gt;{{Cite journal| last1 = Tsuruoka| first1 = Y
| last2 = Miwa| first2 = M| last3 = Hamamoto| first3 = K| last4 = Tsujii| first4 = J| last5 = Ananiadou| first5 = S
 | year = 2011| title = Discovering and visualizing indirect associations between biomedical concepts
 | journal = Bioinformatics| volume = 27| issue = 13 | pages = i111-9| publisher =  | jstor = | doi = 10.1093/bioinformatics/btr214 }}&lt;/ref&gt;

[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system over [[Medline]] abstracts.

[http://labs.europepmc.org/evf '''Europe PMC EvidenceFinder'''] helps users to explore facts that involve entities of interest within the full text articles of the [[Europe PubMed Central]] database.&lt;ref&gt;{{cite journal| author=The Europe PMC Consortium| title=Europe PMC: a full-text literature database for the life sciences and platform for innovation | journal=Nucleic Acids Research| year= 2014 | volume= 43 | issue=D1 | pages=D1042-D1048 | doi=10.1093/nar/gku1061 | pmid=25378340 | pmc=4383902}}&lt;/ref&gt;

[http://www.nactem.ac.uk/EvidenceFinderAnatomyMK/ '''EUPMC Evidence Finder for Anatomical entities with meta-knowledge'''] – similar to the Europe PMC EvidenceFinder, allowing exploration of facts involving anatomical entities within the full text articles of the [[Europe PubMed Central]] database.  Facts can be filtered according to various aspects of their interpretation (e.g., negation, certainly level, novelty).

[http://www.nactem.ac.uk/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].

[http://www.nactem.ac.uk/ClinicalTrialProtocols/ '''Clinical Trial Protocols (ASCOT) '''] is an efficient, semantically-enhanced search application, customised for clinical trial documents.&lt;ref&gt;{{cite journal| author=Korkontzelos, I., Mu, T. and Ananiadou, S.| title=ASCOT: a text mining-based web-service for efficient search and assisted creation of clinical trials | journal=BMC Medical Informatics and Decision Making | year= 2012 | volume= 12 | issue= Suppl 1 | pages= S3 | doi=10.1186/1472-6947-12-S1-S3}}&lt;/ref&gt;

[http://www.nactem.ac.uk/hom/ '''History of Medicine (HOM)'''] is a semantic search system over historical medical document archives

==Resources==

[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] – a large-scale terminological resource for the biomedical domain.&lt;ref&gt;{{cite journal| author=Thompson, P., McNaught, J., Montemagni, S., Calzolari, N., del Gratta, R., Lee, V., Marchi, S., Monachini, M., Pezik, P., Quochi, V., Rupp, C. J., Sasaki, Y., Venturi, G., Rebholz-Schuhmann, D. and Ananiadou, S.| title=The BioLexicon: a large-scale terminological resource for biomedical text mining | journal=BMC Bioinformatics | year= 2011 | volume= 12 | pages=397 | doi=10.1186/1471-2105-12-397}}&lt;/ref&gt;

[http://www.nactem.ac.uk/genia/ '''GENIA'''] – a collection of reference materials for the development of biomedical text mining systems.

[http://www.nactem.ac.uk/GREC/ '''GREC''']  – a semantically annotated corpus of [[Medline]] abstracts intended for training IE systems and/or resources which are used to extract events from biomedical literature.&lt;ref&gt;{{cite journal| author=Thompson, P., Iqbal, S. A., McNaught, J. and Ananiadou, S.| title=Construction of an annotated corpus to support biomedical information extraction| journal=BMC Bioinformatics| year= 2009| volume= 10 | pages=349| doi=10.1186/1471-2105-10-349}}&lt;/ref&gt;

[http://www.nactem.ac.uk/metabolite-corpus/ '''Metabolite and Enzyme Corpus''']  – a corpus of [[Medline]] abstracts annotated by experts with metabolite and enzyme names.

[http://www.nactem.ac.uk/anatomy_corpora/ '''Anatomy Corpora''']  – A collection of corpora manually annotated with fine-grained, species-independent anatomical entities, to facilitate the development of text mining systems that can carry out detailed and comprehensive analyses of biomedical scientific text.&lt;ref&gt;{{cite journal| author=Pyysalo, S., Ohta, T., Miwa, M., Cho, H. -C., Tsujii, J. and Ananiadou, S.| title=Event extraction across multiple levels of biological organization| journal=Bioinformatics| year= 2012| volume= 28 | issue=18 |pages=i575-i581| doi=10.1093/bioinformatics/bts407}}&lt;/ref&gt;
&lt;ref&gt;{{cite journal|author1=Pyysalo, S.  |author2=Ananiadou, S. |lastauthoramp=yes | title=Anatomical Entity Mention Recognition at Literature Scale| journal=Bioinformatics| year= 2014| volume= 30 | issue=6 |pages=868–875| doi=10.1093/bioinformatics/btt580}}&lt;/ref&gt;

[http://www.nactem.ac.uk/meta-knowledge/ '''Meta-knowledge corpus''']  – an enrichment of the [http://www.nactem.ac.uk/genia/ '''GENIA Event corpus'''], in which events are enriched with various levels of information pertaining to their interpretation. The aim is to allow systems to be trained that can distinguish between events that factual information or experimental analyses, definite information from speculated information, etc.&lt;ref&gt;{{cite journal| author=Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S.| title=Enriching a biomedical event corpus with meta-knowledge annotation| journal=BMC Bioinformatics| year= 2011| volume= 12  |pages=393| doi=10.1186/1471-2105-12-393}}&lt;/ref&gt;

==Projects==

[http://nactem.ac.uk/argo/ '''Argo'''] – The objective of the Argo project is to develop a workbench for analysing (primarily annotating) textual data. The workbench, which is accessed as a web application, supports the combination of elementary text-processing components to form comprehensive processing workflows. It provides functionality to manually intervene in the otherwise automatic process of annotation by correcting or creating new annotations, and facilitates user collaboration by providing sharing capabilities for user-owned resources. Argo benefits users such as text-analysis designers by providing an integrated environment for the development of processing workflows; annotators/curators by providing manual annotation functionalities supported by automatic pre-processing and post-processing; and developers by providing a workbench for testing and evaluating text analytics.

[http://nactem.ac.uk/big_mechanism/ '''Big Mechanism'''] –  Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by [[DARPA]], the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a 'Robot Scientist' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.

[http://nactem.ac.uk/copious/ '''COPIOUS'''] – This project aims to produce a knowledge repository of Philippine biodiversity by combining the domain-relevant expertise and resources of Philippine partners with the text mining-based big data analytics of the University of Manchester's National Centre for Text Mining. The repository will be a synergy of different types of information, e.g., taxonomic, occurrence, ecological, biomolecular, biochemical, thus providing users with a comprehensive view on species of interest that will allow them to (1) carry out predictive analysis on species distributions, and (2) investigate potential medicinal applications of natural products derived from Philippine species.

[http://nactem.ac.uk/europepmc/ '''Europe PMC Project'''] – This is a collaboration with the Text-Mining group at the [[European Bioinformatics Institute]] (EBI) and [[Mimas (data centre)]], forming a work package in the [[Europe PubMed Central]] project (formerly UKPMC) hosted and coordinated by the [[British Library]]. Europe PMC, as a whole, forms a European version of the [[PubMed Central]] paper repository, in collaboration with the [[National Institutes of Health]] (NIH) in the United States. Europe PMC is funded by a consortium of key funding bodies from the biomedical research funders. The contribution to this major project is in the application of text mining solutions to enhance information retrieval and knowledge discovery. As such this is an application of technology developed in other NaCTeM projects on a large scale and in a prominent resource for the Biomedicine community.

[http://nactem.ac.uk/DID-MIBIO/ '''Mining Biodiversity'''] – This project aims to transform the [[Biodiversity Heritage Library]] (BHL) into a next-generation social digital library resource to facilitate the study and discussion (via social media integration) of legacy science documents on biodiversity by a worldwide community and to raise awareness of the changes in biodiversity over time in the general public. The project integrates novel text mining methods, visualisation, crowdsourcing and social media into the BHL. The resulting digital resource will provide fully interlinked and indexed access to the full content of BHL library documents, via semantically enhanced and interactive browsing and searching capabilities, allowing users to locate precisely the information of interest to them in an easy and efficient manner.

[http://nactem.ac.uk/text-mining-mrc/ '''Mining for Public Health''']  – This project aims to conduct novel research in text mining and machine learning to transform the way in which evidence-based public health (EBPH) reviews are conducted. The aims of the project are to develop new text mining unsupervised methods for deriving term similarities, to support screening while searching in EBPH reviews and to develop new algorithms for ranking and visualising meaningful associations of multiple types in a dynamic and iterative manner. These newly developed methods will be evaluated in EBPH reviews, based on implementation of a pilot, to ascertain the level of transformation in EBPH reviewing.

==References==
{{Reflist}}

==External links==
* http://www.nactem.ac.uk

[[Category:Computational linguistics]]
[[Category:Computer science organizations]]
[[Category:Information retrieval organizations]]
[[Category:Linguistics organizations]]
[[Category:School of Computer Science, University of Manchester]]</text>
      <sha1>7wh8c64xaeq4arqbrqbvhkinlm0z6jv</sha1>
    </revision>
  </page>
  <page>
    <title>IFACnet</title>
    <ns>0</ns>
    <id>7344222</id>
    <revision>
      <id>666879145</id>
      <parentid>666704099</parentid>
      <timestamp>2015-06-14T09:12:55Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* External links */Removed invisible unicode characters + other fixes, removed: ‎ using [[Project:AWB|AWB]] (11140)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2733" xml:space="preserve">'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.

The following 31 organizations participate in IFACnet:

*[[American Institute of Certified Public Accountants]] (AICPA)
*[[Association of Chartered Certified Accountants]] (ACCA)
*[[Canadian Institute of Chartered Accountants]]
*[[Certified General Accountants Association of Canada]]
*[[Chartered Institute of Management Accountants]] (CIMA)
*[[Chartered Institute of Public Finance and Accountancy]]
*[[CMA Canada]]
*[[Compagnie Nationale des Commissaires aux Comptes]]
*[[Conseil Supérieur de l'Ordre des Experts-Comptables]]
*[[Consiglio Nazionale Dottori Commercialisti]]
*[[CPA Australia]]
*[[Délégation Internationale Pour l'Audit et la Comptabilité]]
*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)
*[[International Federation of Accountants]]  (IFAC)
*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)
*[[Institute of Certified Public Accountants in Ireland]]
*[[Institute of Certified Public Accountants of Singapore]]
*[[Institute of Chartered Accountants of Australia]]
*[[Institute of Chartered Accountants in England &amp; Wales]] (ICAEW)
*[[Institute of Chartered Accountants in Ireland]]
*[[Institute of Chartered Accountants of India]]
*[[Institute of Chartered Accountants of Pakistan]]
*[[Institute of Chartered Accountants of Scotland]] (ICAS)
*[[Institute of Management Accountants]]
*[[Japanese Institute of Certified Public Accountants]] (JICPA)
*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)
*[[Malaysian Institute of Accountants]]
*[[Malta Institute of Accountants]]
*[[National Association of State Boards of Accountancy]] (NASBA)
*[[South African Institute of Chartered Accountants]] (SAICA)
*[[Union of Chambers of Certified Public Accountants of Turkey]] (TÜRMOB)

==External links==
*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]
*[http://www.ifac.org/ International Federation of Accountants Homepage]

[[Category:Information retrieval organizations]]
[[Category:Internet search engines]]
[[Category:Accounting organizations]]</text>
      <sha1>4vyql77us7cl15p1emnr3xawj85yjhd</sha1>
    </revision>
  </page>
  <page>
    <title>International Society for Music Information Retrieval</title>
    <ns>0</ns>
    <id>30882491</id>
    <revision>
      <id>757776187</id>
      <parentid>738542537</parentid>
      <timestamp>2017-01-01T17:49:24Z</timestamp>
      <contributor>
        <username>Yurichev</username>
        <id>19013952</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13312" xml:space="preserve">{{Infobox non-profit
| Non-profit_name   = The International Society for Music Information Retrieval
| Non-profit_logo   = [[Image:LogoInternationalSocietyMIR.png|250px]]
| Non-profit_type   = Non-profit organization
| founded_date      = 2008
| founder           = 
| location          = [[Canada]]
| origins           = International Symposium for Music Information Retrieval
| key_people        = 
| area_served       = Worldwide
| focus             = [[Music information retrieval|Music Information Retrieval (MIR)]]
| method            = Conferences, publications
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| owner             = 
| Non-profit_slogan = The world's leading research forum on processing, searching, organising and accessing music-related data
| homepage          = {{URL|http://www.ismir.net/}}
| tax_exempt        = 
| dissolved         = 
| footnotes         = 
}}

The '''[http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]''' is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000&lt;ref&gt;[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11], {{ISSN|1082-9873}}.&lt;/ref&gt; which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.&lt;ref&gt;[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]&lt;/ref&gt;

==Purpose==
Given the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.

As the term [[Music Information Retrieval|Music Information Retrieval (MIR)]]  indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science, electrical engineering and many others.

==Annual conferences==
Since its inception in 2000, ISMIR has been the world’s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.

* ISMIR 2019, Delft (The Netherlands)
* ISMIR 2018, Paris (France)
* ISMIR 2017, Suzhou (China)
* [http://ismir2016.ismir.net ISMIR 2016], 8–12 August 2016, New York City (USA) [http://dblp.uni-trier.de/db/conf/ismir/ismir2016.html proceedings]
* [http://ismir2015.ismir.net ISMIR 2015], 26–30 October 2015, Malaga (Spain) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2015.html proceedings]
* [http://ismir2014.ismir.net ISMIR 2014], 27–31 October 2014, Taipei (Taiwan) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2014.html proceedings]
* [http://ismir2013.ismir.net ISMIR 2013], 4–8 November 2013, Curitiba (Brazil) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2013.html proceedings]
* [http://ismir2012.ismir.net ISMIR 2012], 8–12 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2012'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2011.ismir.net ISMIR 2011], 24–28 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2011'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2010.ismir.net ISMIR 2010], 9–13 August 2010, Utrecht (The Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2010'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2009.ismir.net ISMIR 2009], 26–30 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2009'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2008.ismir.net ISMIR 2008], 14–18 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2008'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2007.ismir.net ISMIR 2007], 23–30 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2007'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2006.ismir.net ISMIR 2006], 8–12 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2006'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2005.ismir.net ISMIR 2005], 11–15 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2005'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2004.ismir.net ISMIR 2004], 10–15 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2004'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2003.ismir.net ISMIR 2003], 26–30 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2003'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2002.ismir.net ISMIR 2002], 13–17 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2002'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2001.ismir.net ISMIR 2001], 15–17 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2001'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2000.ismir.net ISMIR 2000], 23–25 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='200'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]

The [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences. An overview of all papers published at ISMIR can be found at [http://dblp.uni-trier.de/db/conf/ismir/index.html DBLP].

==Research areas and topics==
The following list gives an overview of the main research areas and topics that are within the scope of 
[[Music Information Retrieval]].

===MIR data and fundamentals===
*    music signal processing
*    symbolic music processing
*    metadata, linked data and semantic web
*    social tags and user generated data
*    natural language processing, text and web mining
*    multi-modal approaches to MIR

===Methodology===
*    methodological issues and philosophical foundations
*    evaluation methodology
*    corpus creation
*    legal, social and ethical issues

===Domain knowledge===
*    representation of musical knowledge and meaning
*    music perception and cognition
*    computational music theory
*    computational musicology and ethnomusicology

===Musical features and properties===
*    melody and motives
*    harmony, chords and tonality
*    rhythm, beat, tempo
*    structure, segmentation and form
*    timbre, instrumentation and voice
*    musical style and genre
*    musical affect, emotion and mood
*    expression and performative aspects of music

===Music processing===
*    sound source separation
*    music transcription and annotation
*    optical music recognition
*    alignment, synchronization and score following
*    music summarization
*    music synthesis and transformation
*    fingerprinting
*    automatic classification
*    indexing and querying
*    pattern matching and detection
*    similarity metrics

===Application===
*    user behavior and modelling
*    user interfaces and interaction
*    digital libraries and archives
*    music retrieval systems
*    music recommendation and playlist generation
*    music and health, well-being and therapy
*    music training and education
*    MIR applications in music composition, performance and production
*    music and gaming
*    MIR in business and marketing

==MIREX==
The ''Music Information Retrieval Evaluation eXchange'' (MIREX) is an annual evaluation campaign for MIR algorithms, coupled to the ISMIR conference. Since it started in 2005, MIREX has fostered advancements both in specific areas of MIR and in the general understanding of how MIR systems and algorithms are to be evaluated.&lt;ref name=DownieEBJ10&gt;
{{citation 
|author1=J. Stephen Downie |author2=Andreas F. Ehmann |author3=Mert Bay |author4=M. Cameron Jones |title=The Music Information Retrieval Evaluation eXchange: Some Observations and Insights
|journal=Advances in Music Information Retrieval, Springer
|year=2010
|pages=93–115
|doi=10.1007/978-3-642-11674-2_5}}
&lt;/ref&gt;&lt;ref name=DownieEEV05_ISMIR&gt;
{{cite journal
|last1=Downie
|first1=J. Stephen
|last2=West
|first2=Kris 
|last3=Ehmann
|first3=Andreas F.
|last4=Vincent
|first4=Emmanuel
|title=The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview
|journal=Proceedings of the International Conference on Music Information Retrieval
|year=2005
|pages=320–323}}
&lt;/ref&gt; MIREX is to the MIR community what the [[Text Retrieval Conference]] (TREC) is to the text information retrieval community: A set of community-defined formal evaluations through which a wide variety of state-of-the-art systems, algorithms and techniques are evaluated under controlled conditions. MIREX is managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana-Champaign (UIUC).&lt;ref name="DownieIMIRSEL"&gt;{{cite web|last1=Downie|first1=J. Stephen|title=The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) Project|url=http://www.music-ir.org/evaluation/|publisher=University of Illinois|accessdate=22 April 2016}}&lt;/ref&gt;

==Related conferences==
* [[ACM Multimedia]]
* [[International Computer Music Conference|International Computer Music Conference (ICMC)]]
* [[International Conference on Acoustics, Speech, and Signal Processing|International Conference on Acoustics, Speech, and Signal Processing (ICASSP)]]
* [[International Conference on Digital Audio Effects|International Conference on Digital Audio Effects (DAFx)]]
* [[New Interfaces for Musical Expression|International Conference on New Interfaces for Musical Expression (NIME)]]
* International Symposium on Computer Music Modeling and Retrieval (CMMR)
* [[Sound and Music Computing Conference|Sound and Music Computing Conference (SMC)]]

==Related journals==
* [[Computer Music Journal|Computer Music Journal (CMJ)]]
* [http://asmp.eurasipjournals.springeropen.com/ EURASIP Journal on Audio, Speech, and Music Processing]
* [http://www.signalprocessingsociety.org/publications/periodicals/taslp/ IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)]
* [http://www.signalprocessingsociety.org/tmm/ IEEE Transactions on Multimedia (TMM)]
* [http://mp.ucpress.edu/ Music Perception]
* [[Journal of New Music Research|Journal of New Music Research (JNMR)]]

==Further links==
* [[Audio Engineering Society]]
* [http://www.signalprocessingsociety.org/technical-committees/list/audio-tc/ Audio and Acoustic Signal Processing]
* [[Music Technology]]
* [http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]
* [[Sound and music computing|Sound and Music Computing]]

==References==
{{Reflist}}

[[Category:Music information retrieval]]
[[Category:Computer science conferences]]
[[Category:Music technology]]
[[Category:Multimedia]]
[[Category:Information retrieval organizations]]
[[Category:Music search engines]]</text>
      <sha1>kk7h6oum4j32q5ipc33fbisu8x0euj6</sha1>
    </revision>
  </page>
  <page>
    <title>DtSearch</title>
    <ns>0</ns>
    <id>14388058</id>
    <revision>
      <id>762587204</id>
      <parentid>758155363</parentid>
      <timestamp>2017-01-29T18:06:16Z</timestamp>
      <contributor>
        <username>Green Cardamom</username>
        <id>8931761</id>
      </contributor>
      <comment>remove comma</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5586" xml:space="preserve">{{Lowercase}}

{{Infobox company |
  name   = dtSearch Corp. |
  slogan = "The Smart Choice for Text Retrieval since 1991" |
  type   =  Private company |
  foundation     = 1991 |
  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]], [[United States|US]] |
  key_people     = David Thede, President |
  industry       = [[Software]] |

  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]
}}

'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.

==History==
dtSearch Corp was founded by David Thede;&lt;ref&gt;[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm Lets talk computers - Interview May 31, 2003]&lt;/ref&gt;&lt;ref&gt;[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]&lt;/ref&gt;&lt;ref&gt;[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]&lt;/ref&gt; the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of ''[[PC Magazine]]''&lt;ref&gt;"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)&lt;/ref&gt; as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[askSam]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.

In the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft’s initial release of its 32-bit Windows operating system, [[Windows 95]].&lt;ref&gt;[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

In 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.&lt;ref&gt;[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&amp;PageNum=22007 EContent 100 list]&lt;/ref&gt;

==Products==
The current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.

*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)
*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)
*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)
*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)
*dtSearch Engine for Linux - SDK with C++ and Java APIs
*dtSearch Publish &lt;ref&gt;[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&amp;slreturn=1&amp;hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]&lt;/ref&gt; - a search front-end for CD\DVD publishing (32 and 64 bit indexers)

==Licensing Partners==
* COMPANY:  PRODUCT
* Docupoint, LLC:  DrawingSearcher
* FileHold Systems Inc.: FileHold Document Management System
* ...

==See also==
* [[Enterprise search]]
* [[List of enterprise search vendors]]

==References==
{{Reflist}}

==External links==
*[http://www.dtsearch.com/ Company Website]
*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]
*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]
*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]
*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan–Feb]
*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233–1246] 
*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

{{DEFAULTSORT:Dtsearch Corp.}}
[[Category:Desktop search engines]]
[[Category:Information retrieval organizations]]
[[Category:Software companies based in Maryland]]</text>
      <sha1>lnkx1u0l0xs8elsk9ye074fed2lcw2y</sha1>
    </revision>
  </page>
  <page>
    <title>Dandelon.com</title>
    <ns>0</ns>
    <id>41725036</id>
    <revision>
      <id>748144486</id>
      <parentid>718751029</parentid>
      <timestamp>2016-11-06T16:08:46Z</timestamp>
      <contributor>
        <username>The American Farmer</username>
        <id>26806929</id>
      </contributor>
      <comment>copy edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6420" xml:space="preserve">
{{more footnotes|date=January 2014}}
'''Dandelon.com''' is a collaborative community of libraries in multiple countries as well as a [[Search engine (computing)|search engine]], a search or discovery service, a library information system for the academic community. It is additionally a platform allowing registered libraries to exchange library catalogue enrichment data: tables of content of monographs, deep indexing data, cover pages and bibliographic descriptions of articles published in periodicals, with abstracts and / or full texts provided for part of the items. The domain name was created in 2004. It is derived from the plant [[Taraxacum|dandelion]]. The name is an allusion to the flower's worldwide occurrence: It is thought to spread around the world as easily as human words and thoughts. Dandelon's aim is to uncover knowledge assets for students from around the world. It is free of charge for private use and without user tracking or advertising.&lt;ref&gt;[http://www.dandelon.com Dandelon.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

Traditionally, the number of searchable relevant subject words comes up to about five semantically different subjects words, located in titles and generated by human indexing. A book registered at dandelon.com typically is assigned between 20 and 500 subject words depending on the size of the book and the knowledge domain. Based on this extended set of terms representing each library item, queries can be more specific, and relevance ranking can be more efficient. Dandelon.com also expands user queries by adding closely related words (default: synonyms and translations, optionally: narrower terms) from multilingual [[Thesaurus|thesauri]] from various knowledge domains.

Search results can be restricted to a specific library. Automatic backlinks to the related library management system allows online access or requesting a book. Dandelon.com does not replace library management systems, it is an additional option for searching and first of all a platform for data exchange between libraries associated with its community. Its user interface supports a number of languages, and it provides content in about 130 languages.

The core of dandelon.com is the content production software ““intelligentCAPTURE mobile”” employed by all member libraries. It reads from and sends data to each library management system, receiving text content via digitization and [[optical character recognition]] (OCR) for close to 200 languages or via native digital content import. Additionally, it automatically extracts major subject words, which are translated into 60 languages by machine translation. Computers and scanners can be placed in a special mobile furniture to be used between shelves and narrow compactus.

The provider of production software and search and distribution services is the German-based company AGI-Information Management Consultants &lt;ref&gt;[http://www.agi-imc.de&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; as well as the hosting center of  GBV - Gemeinsamer Bibliotheksverbund - , a state-owned German library service center for more than 800 libraries.&lt;ref&gt;[http://www.gbv.de&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; The solution was invented in 2001 by Manfred Hauer of AGI and Karl Raedler from Vorarlberger Landesbibliothek, Austria.&lt;ref&gt;[http://vlb.vorarlberg.at&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Dandelon.com shares part of its data with GBV. GBV, in turn, exchanges some of its catalogue enrichment data with OCLC, [[WorldCat]] and other service centers. HEBIS,&lt;ref&gt;[http://www.hebis.de&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; another state-owned service center shares with the German National Library. German National Library charges fees for enrichment content.&lt;ref&gt;[http://www.dnb.de/kataloganreicherung&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; AGI and a number of the producing libraries have been pioneering catalogue enrichment in Europe since 2001 and form one of the largest communities of producers of digitalized tables of content of monographs in Europe. In 2013 close to 2&amp;nbsp;million tables of contents were digitalized, not all of which are available on dandelon.com for the general public. The large collection produced for the [[German National Library]] is not yet shared and was announced for public use in 2014. Dandelon.com and intelligentCAPTURE are [[IBM Lotus Domino|IBM Domino and Notes]] applications. Dandelon.com runs [[Apache Lucene]] as retrieval engine.

== References ==
{{Reflist}}
*Manfred Hauer: 2012 "[http://www.agi-imc.de/internet.nsf/dda9df579aa6429dc12567f5004ad7ed/659e168d74f0bc38c12579bb004ea5b8/$FILE/HAUER_SLA_Bahrain_2012_gb.pdf Web 2.0: Which features are wanted by academic library clients? A HEBIS Survey Report]" (PDF; 172&amp;nbsp;kB) ''Gulf special library association (SLA)'', Conference Proceeding - on CD
*Nienerza,Heike / Sunckel, Bettina / Meier, Berthold: 2011 "[http://www.degruyter.com/view/j/abitech.2011.31.issue-3/ABI.2011.020/ABI.2011.020.xml?format=INT Unser Katalog soll besser werden! Kataloge und Portale im Web-2.0-Zeitalter. Ergebnisse einer Online-Umfrage im HeBIS-Verbund]"  ''ABI-Technik'', De Gruyter, Berlin, Issue 31, pp. 130-149, DOI: 10.1515/ABI.2011.020
*Manfred Hauer: 2013 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3d26118ce2a8ebccc1257b1800356e8b?OpenDocument Zur Bedeutung normierter Terminologien in Zeiten moderner Sprach- und Information-Retrieval-Technologien]" (PDF; 205&amp;nbsp;kB) ''[http://www.degruyter.com/view/j/abitech] ABI-Technik'', De Gruyter, Berlin, issue 1, pp. 2-6
*Manfred Hauer, Rainer Diedrichs: 2010 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3f191bb231f0d57ec1257749004a9e7d/$FILE/Kataloganreicherung_in_Europa_2010_c.pdf  Kataloganreicherung in Europa]" (PDF; 525&amp;nbsp;kB) ''Buch und Bibliothek [http://www.b-u-b.de/]'', issue 5, pp.&amp;nbsp;394–397
*Manfred Hauer: 2005 ''[http://www.agi-imc.de/internet.nsf/94280a18b17ee318c12567d2003c3bb2/3267dae6428c5f02c125711600527ffd?OpenDocument/Vergleich der Retrievalleistungen von Bibliothekskatalogen gegen erweiterte und neue Konzept. Benchmarking: Google Scholar, dandelon.com, Vorarlberger Landesbibliothek, weitere OPACs.] In: [http://www.degruyter.com/view/j/abitech] ABI-Technik, De Gruyter, Berlin, December, pp.&amp;nbsp;295–301.

[[Category:Information retrieval organizations]]
[[Category:Information retrieval systems]]
[[Category:Digital library projects]]</text>
      <sha1>miq5t7jmwcfaescors083w2tnguwdk3</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Google</title>
    <ns>14</ns>
    <id>853521</id>
    <revision>
      <id>742382110</id>
      <parentid>742380738</parentid>
      <timestamp>2016-10-03T10:46:12Z</timestamp>
      <contributor>
        <username>Marianna251</username>
        <id>27604025</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/119.82.64.37|119.82.64.37]] ([[User talk:119.82.64.37|talk]]) ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="590" xml:space="preserve">{{Commons category|Google}}
{{Cat main|Google}}

[[Category:Internet search engines]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Internet companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Web portals]]
[[Category:Information retrieval organizations]]
[[Category:Alphabet Inc.]]
[[Category:Wikipedia categories named after information technology companies of the United States]]
[[Category:Wikipedia categories named after websites]]</text>
      <sha1>1bvoyv3r22op39dtt9fextb81e1ypig</sha1>
    </revision>
  </page>
  <page>
    <title>Smartlogic</title>
    <ns>0</ns>
    <id>39083726</id>
    <revision>
      <id>731887173</id>
      <parentid>714748020</parentid>
      <timestamp>2016-07-28T04:37:50Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>USA is deprecated, per [[MOS:NOTUSA]], and correct [[MOS:OVERLINK|overlinking]] of common places using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3133" xml:space="preserve">{{Multiple issues|
{{advert|date=June 2015}}
{{COI|date=June 2015}}
{{notability|Companies|date=November 2015}}
}}
{{Infobox company |
name = Smartlogic  |
logo =  |
slogan = "The Content Intelligence Company" |
type = [[Privately held company|Private]] |
foundation = 2006 |
location = United States, UK | 
area_served = Global |
industry = [[Information retrieval]] |
products = Semaphore Cloud, Semaphore Ontology Editor, Semaphore Classification Server, Semaphore Semantic Enhancement Server, Advanced Language Packs, Search Appliance Framework, Text Miner, Classification Review Tool, Classification Analysis Tool  | 
num_employees  = 55|
homepage = http://www.smartlogic.com
}}

'''Smartlogic ''' is a [[software company]] which specializes in developing [[information retrieval]], [[text analytics]] and [[knowledge management]] solutions.

==History==
Smartlogic was founded in the United Kingdom in 2006. It is a privately held company and has offices in San Jose, CA; Alexandria, VA; Cambridge, MA and London, UK. The company develops and sells a suite of products; Semaphore Ontology Editor, Classification Server, Advanced Language Packs, Semantic Enhancement Server, Text Miner, Classification Review tool, and Classification Analysis tool.

==Products==

===Semaphore Ontology Editor===
Semaphore Ontology Editor is a web-based tool used to build taxonomies, ontologies, controlled vocabularies as well as other knowledge organization systems. Models are used by organizations to enhance the capabilities of enterprise search engines,&lt;ref&gt;[http://www.cmswire.com/events/item/webinar-leverage-metadata-to-drive-critical-business-processes-022370.php] CMSWire Leverage Metadata to Drive Critical Business Processes&lt;/ref&gt; content management and workflow systems deployed by clients to augment and enhance their investment.

===Semaphore Classification Server===
Semaphore Classification Server uses the model structure from Semaphore Ontology Editor and auto classifies unstructured information assets by applying metadata tags to the unstructured information.

===Semaphore Advanced Language Packs===

===Semantic Enhancement Server===

==Integrations==
Semaphore integrates with [[Microsoft Sharepoint]],&lt;ref&gt;[http://www.cmswire.com/cms/information-management/sharepoint-2013-office-365-get-semantic-search-with-smartlogic-semaphore-018353.php] CMSWire SharePoint 2013 Office 365 Get Semantic Search with Smartlogic Semaphore&lt;/ref&gt; [[Google Search Appliance]],&lt;ref&gt;[https://www.google.com/enterprise/marketplace/viewVendorListings?vendorId=33&amp;pli=1] Google Enterprise Catalogue&lt;/ref&gt; [[Apache Solr]],&lt;ref&gt;[http://www.flatironssolutions.com/blog/alfresco-semaphore-integration/] Alfresco-Semaphore Integration&lt;/ref&gt; FAST ESP&lt;ref&gt;[http://arnoldit.com/wordpress/2009/10/23/smartlogic-and-fast-esp-integration/] Stephen E. Arnold - Beyond Search&lt;/ref&gt; and others.

==References==
{{Reflist}}

==External links==
* [http://www.smartlogic.com/ Smartlogic]

[[Category:Software companies of the United Kingdom]]
[[Category:Information retrieval organizations]]
[[Category:Analytics companies]]
[[Category:Knowledge management]]</text>
      <sha1>5vzqowbooya9gblfepzmhdozctlyr6e</sha1>
    </revision>
  </page>
  <page>
    <title>Artificial Solutions</title>
    <ns>0</ns>
    <id>40218456</id>
    <revision>
      <id>729468129</id>
      <parentid>725107594</parentid>
      <timestamp>2016-07-12T11:44:13Z</timestamp>
      <contributor>
        <username>Nick Number</username>
        <id>1526960</id>
      </contributor>
      <minor />
      <comment>repaired link(s) to disambiguation pages ([[WP:DPL|you can help]]) - Mountain View</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9555" xml:space="preserve">{{Infobox company
|name= Artificial Solutions
|logo=[[Image:Artificial Solutions Logo.png]]
|type=[[Private company]]
|foundation=(2001)
|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström 
|location=[[Barcelona]], [[Spain]]
|locations=Offices worldwide with R&amp;D centers in [[Barcelona]], [[Hamburg]], [[London]], [[Mountain View, California|Mountain View]], [[Milan]], [[Utrecht]] and [[Stockholm]] 
|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], 
|products= Teneo platform
|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]
}}

'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.&lt;ref&gt;{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}&lt;/ref&gt; The company's natural language solutions have been deployed in a wide range of industries including finance,&lt;ref&gt;{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}&lt;/ref&gt; telecoms,&lt;ref&gt;{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}&lt;/ref&gt; the public sector,&lt;ref&gt;{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}&lt;/ref&gt; retail&lt;ref&gt;{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}&lt;/ref&gt; and travel.&lt;ref&gt;{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}&lt;/ref&gt;

==History==
Artificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.&lt;ref&gt;{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}&lt;/ref&gt;

The company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.&lt;ref&gt;{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}&lt;/ref&gt; Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.&lt;ref&gt;{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}&lt;/ref&gt;

In 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.&lt;ref&gt;{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}&lt;/ref&gt;
[[Elbot]], Artificial Solutions’ test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.&lt;ref&gt;[[Loebner Prize]]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &amp;#124; England &amp;#124; Berkshire &amp;#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}&lt;/ref&gt;

With a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.&lt;ref&gt;{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}&lt;/ref&gt;
In 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.&lt;ref&gt;{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &amp;#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}&lt;/ref&gt;
A new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.&lt;ref&gt;{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &amp;#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}&lt;/ref&gt;

In February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].&lt;ref&gt;{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}&lt;/ref&gt;

==References==
{{Reflist|30em}}

==External links==
*[http://www.hello-indigo.com Indigo]
*[http://www.elbot.com Elbot]

[[Category:Natural language processing software]]
[[Category:Intelligent software assistants]]
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval organizations]]</text>
      <sha1>hzubh30h9s774s7cem7jcxlehujhtq0</sha1>
    </revision>
  </page>
  <page>
    <title>TeLQAS</title>
    <ns>0</ns>
    <id>21727808</id>
    <revision>
      <id>666703845</id>
      <parentid>554506388</parentid>
      <timestamp>2015-06-13T01:42:09Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1746" xml:space="preserve">'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.&lt;ref&gt;Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.&lt;/ref&gt;

==Architecture==
TeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.&lt;ref&gt;Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).&lt;/ref&gt; The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.

==References==
&lt;references/&gt;

[[Category:Computational linguistics]]
[[Category:Information retrieval systems]]
[[Category:Natural language processing software]]</text>
      <sha1>if93i1ftck0zqzd8dr0swf94hg141gh</sha1>
    </revision>
  </page>
  <page>
    <title>EXCLAIM</title>
    <ns>0</ns>
    <id>8239120</id>
    <revision>
      <id>762772034</id>
      <parentid>762771888</parentid>
      <timestamp>2017-01-30T17:29:42Z</timestamp>
      <contributor>
        <ip>2A01:E35:8B97:C020:25F9:6719:65F1:4026</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5192" xml:space="preserve">{{For|the Canadian magazine|Exclaim!}}
The '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' was an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006, with some support for more than a dozen languages. The lead developers were Justin Nuger and Jesse Saba Kirchner.

Early work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.&lt;ref&gt;
{{cite web
|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web
|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf
|format=PDF|publisher=ACM-SIGIR 1999
|accessdate=2006-12-02
}}
&lt;/ref&gt;

EXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).

One of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.

==Current status==

In 2009, EXCLAIM was in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, was available for the following twenty-three languages:

{| class="wikitable"
| [[Albanian language|Albanian]]
|-
| [[Amharic]]
|-
| [[Bengali language|Bengali]]
|-
| [[Gothic language|Gothic]]
|-
| [[Greek language|Greek]]
|-
| [[Icelandic language|Icelandic]]
|-
| [[Indonesian language|Indonesian]]
|-
| [[Irish language|Irish]]
|-
| [[Javanese language|Javanese]]
|-
| [[Latvian language|Latvian]]
|-
| [[Malagasy language|Malagasy]]
|-
| [[Mandarin Chinese]]
|-
| [[Nahuatl]]
|-
| [[Navajo language|Navajo]]
|-
| [[Quechua languages|Quechua]]
|-
| [[Sardinian language|Sardinian]]
|-
| [[Swahili language|Swahili]]
|-
| [[Tagalog language|Tagalog]]
|-
| [[Standard Tibetan|Tibetan]]
|-
| [[Turkish language|Turkish]]
|-
| [[Welsh language|Welsh]]
|-
| [[Wolof language|Wolof]]
|-
| [[Yiddish]]
|}

Support using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:

{| class="wikitable"
|-
| [[Dutch language|Dutch]]
|-
| [[Spanish language|Spanish]]
|}

Significant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.

Future versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.

The EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.

==Further applications==

EXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[Front and back ends|backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].&lt;ref&gt;{{cite web
|title=A crosslinguistic readability framework
|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf
|format=PDF|publisher=ACL-IJNLP 2009
|accessdate=2009-09-04
}}
&lt;/ref&gt;

==Notes and references==

{{reflist}}

==External links==
*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website] (dead link)
*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]
*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]
*[http://ju-st.in/ Justin Nuger's professional webpage]

{{DEFAULTSORT:Exclaim}}
[[Category:Information retrieval systems]]</text>
      <sha1>d3mhwtko1u07rqs0u0io16n5w6tmaan</sha1>
    </revision>
  </page>
  <page>
    <title>AUTINDEX</title>
    <ns>0</ns>
    <id>43739701</id>
    <revision>
      <id>666705511</id>
      <parentid>649208076</parentid>
      <timestamp>2015-06-13T01:55:48Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5720" xml:space="preserve">{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
}}

'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.&lt;ref&gt;Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen&lt;/ref&gt;&lt;ref&gt;Paul Schmidt, Mahmoud Gindiyeh &amp; Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6–8 October 2009, Portugal&lt;/ref&gt;&lt;ref&gt;Paul Schmidt &amp; Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19–20 November&lt;/ref&gt;

'''AUTINDEX''' resulting from research in [[information extraction]] &lt;ref&gt;Paul Schmidt, Thomas Bähr &amp; Dr.-Ing. Jens Biesterfeld &amp;Thomas Risse &amp; Kerstin Denecke &amp; Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt&lt;/ref&gt;&lt;ref&gt;Ursula Deriu, Jörn Lehmann &amp; Paul Schmidt, 2009: ‚Erstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie’. In: Proceedings Knowtech, Frankfurt&lt;/ref&gt; is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbrücken, Germany.

'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX &lt;ref&gt;[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.&lt;/ref&gt;), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch &lt;ref&gt;[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.&lt;/ref&gt; and WISSMER,&lt;ref&gt;[//www.wissmer.info/index.php/de/]. Project Wissmer.&lt;/ref&gt; see also the reference to IAI-Webite.&lt;ref&gt;[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.&lt;/ref&gt;

The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.&lt;ref&gt;Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.&lt;/ref&gt; Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.&lt;br&gt; 
AUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems &lt;ref&gt;[//www.wti-frankfurt.de]. WTI Information system.&lt;/ref&gt; and in document management and content management environments. &lt;br&gt; 
 
Together with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] &lt;ref&gt;Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.&lt;/ref&gt; software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].&lt;ref&gt;[//www.wissmer.info]. Electro mobility information system.&lt;/ref&gt;

==See also==

* [[Information retrieval]]
* [[Linguistics]]
* [[Knowledge Management]]
* [[Natural Language Processing]]
* [[Semantics]]

== References ==
{{reflist}}

== Publications ==
* Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.
* Paul Schmidt, Thomas Bähr &amp; Dr.-Ing. Jens Biesterfeld &amp;Thomas Risse &amp; Kerstin Denecke &amp; Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.
* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.
* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.–20. November 2009.
* Rösener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD für Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.
* Svenja Siedle: ''Suchst du noch oder weißt du schon? Inhaltserschließung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''
* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum für Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)
* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.

== External links ==
* http://www.iai-sb.de/ Institute for Applied Information Sciences

[[Category:Natural language processing]]
[[Category:Information retrieval systems]]</text>
      <sha1>oy6cg0i2ew9hmjr9jcompdp8sd6s36w</sha1>
    </revision>
  </page>
  <page>
    <title>Agrep</title>
    <ns>0</ns>
    <id>308939</id>
    <revision>
      <id>739026236</id>
      <parentid>739026203</parentid>
      <timestamp>2016-09-12T10:43:14Z</timestamp>
      <contributor>
        <ip>94.219.155.25</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4209" xml:space="preserve">{{lowercase|title=agrep}}
{{Infobox software
| name                   = agrep
| logo                   = &lt;!-- Image name is enough --&gt;
| logo caption           = 
| logo_size              = 
| logo_alt               = 
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot_size        = 
| screenshot_alt         = 
| collapsible            = 
| developer              = {{Plainlist|
* [[Udi Manber]]
* Sun Wu
}}
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| status                 = 
| programming language   = C
| operating system       = {{Plainlist|
* [[Unix-like]]
* [[OS/2]]
* [[DOS]]
* [[Microsoft Windows|Windows]]
}}
| platform               = 
| size                   = 
| language               = 
| language footnote      = 
| genre                  = [[Pattern matching]]
| license                = [https://raw.githubusercontent.com/Wikinaut/agrep/master/COPYRIGHT ISC open source license]
| standard               = 
| website                = {{URL|http://www.tgries.de/agrep}}
}}

'''agrep''' (approximate [[grep]]) is an [[open-source]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].

It selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.

agrep is also the [[Search engine (computing)|search engine]] in the indexer program [[GLIMPSE]]. agrep is under a free [[ISC License]].&lt;ref&gt;[http://webglimpse.net/sublicensing/licensing.html WebGlimpse, Glimpse and also AGREP license] since 18.09.2014 ([http://opensource.org/licenses/ISC ISC License]).&lt;/ref&gt;

== Alternative implementations ==
A more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.&lt;ref&gt;{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}&lt;/ref&gt; Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].

FREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.&lt;ref&gt;{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}&lt;/ref&gt; However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.

==References==
{{Reflist}}

==External links==
* Wu-Manber agrep
**[http://www.tgries.de/agrep AGREP home page]
**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add &lt;code&gt;-Wno-return-type&lt;/code&gt; to the &lt;code&gt;CFLAGs  = -O&lt;/code&gt; line in the Makefile)
*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph's Personal Wiki]
*See also
**[http://laurikari.net/tre TRE regexp matching package]
**[https://web.archive.org/web/20080513225010/http://www1.bell-labs.com/project/wwexptools/cgrep/ cgrep a defunct command line approximate string matching tool]
**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool
**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]

[[Category:Information retrieval systems]]
[[Category:Unix text processing utilities]]
[[Category:Software using the ISC license]]</text>
      <sha1>hkizr3c9na791kzxmzi3mcnkqsudbu3</sha1>
    </revision>
  </page>
  <page>
    <title>Ptx (Unix)</title>
    <ns>0</ns>
    <id>1442890</id>
    <revision>
      <id>744303325</id>
      <parentid>666706695</parentid>
      <timestamp>2016-10-14T11:30:07Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="602" xml:space="preserve">{{Unreferenced stub|auto=yes|date=December 2009}}
{{Lowercase|title=ptx}}
'''ptx''' is a [[Unix utility]], named for the ''[[permuted index]]'' which can perform the function of the [[Keyword in Context]] (KWIC) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, [[thesaurus]]es, or web sites to aid in locating entries of interest.

==See also==
* [[Concordancer]]

[[Category:Information retrieval systems]]
[[Category:Unix text processing utilities]]


{{Unix-stub}}</text>
      <sha1>99vz846szdr0goe3urfsc1ttejc1gce</sha1>
    </revision>
  </page>
  <page>
    <title>Reverse DNS lookup</title>
    <ns>0</ns>
    <id>1286913</id>
    <revision>
      <id>758928774</id>
      <parentid>755018458</parentid>
      <timestamp>2017-01-08T08:45:36Z</timestamp>
      <contributor>
        <username>Cnwilliams</username>
        <id>10190671</id>
      </contributor>
      <minor />
      <comment>Disambiguated: [[octet]] → [[octet (computing)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7443" xml:space="preserve">{{Redirect|Reverse DNS|Java-like naming convention|Reverse domain name notation}}
{{Refimprove|date=September 2016}}

In [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' ('''rDNS''') is the determination of a [[domain name]] associated with an [[IP address]] via querying [[Domain Name System|DNS]] – the reverse of the usual "forward" DNS lookup of an IP from a domain name.

The process of reverse resolving an IP address uses [[PTR record]]s. The reverse DNS database of the Internet is rooted in the [[.arpa|arpa]] [[top-level domain]].

Although the informational RFC 1912 (Section 2.1) specifies that "Every Internet-reachable host should have a name" and that "For every IP address, there should be a matching PTR record...", it is not an [[Internet Standard]] requirement, and not all IP addresses have a reverse entry.

== Implementation details ==
===IPv4 reverse resolution===
Reverse DNS lookups for [[IPv4]] addresses use the special domain &lt;code&gt;in-addr.arpa&lt;/code&gt;. In this domain, an IPv4 address is represented as a concatenated sequence of four decimal numbers, separated by dots, to which is appended the second level domain suffix &lt;code&gt;.in-addr.arpa&lt;/code&gt;. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four [[octet (computing)|octet]]s and converting each octet into a decimal number. These decimal numbers are then concatenated in the order: least significant octet first (leftmost), most significant octet last (rightmost). It is important to note that this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses in textual form.

For example, to do a reverse lookup of the IP address &lt;code&gt;8.8.4.4&lt;/code&gt; the PTR record for the domain name &lt;code&gt;4.4.8.8.in-addr.arpa&lt;/code&gt; would be looked up, and found to point to &lt;code&gt;google-public-dns-b.google.com&lt;/code&gt;. 

If the [[A record]] for &lt;code&gt;google-public-dns-b.google.com&lt;/code&gt; in turn pointed back to &lt;code&gt;8.8.4.4&lt;/code&gt; then it would be said to be [[Forward-confirmed reverse DNS|forward-confirmed]].

====Classless reverse DNS method====
Historically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A. By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using [[CNAME record]]s.

===IPv6 reverse resolution===
Reverse DNS lookups for [[IPv6]] addresses use the special domain &lt;code&gt;ip6.arpa&lt;/code&gt; (previously &lt;code&gt;ip6.int&lt;/code&gt;&lt;ref&gt;RFC 4159&lt;/ref&gt;). An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address &lt;code&gt;2001:db8::567:89ab&lt;/code&gt; is &lt;code&gt;b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa&lt;/code&gt;.

===Multiple pointer records===
While most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended{{by whom|date=August 2016}}, unless there is a specific need. For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically. Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.&lt;ref&gt;[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]&lt;/ref&gt; In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause the query to be requested over TCP when they exceed the DNS 512 byte UDP message limit.

===Records other than PTR records===
Record types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]], [[Secure Shell|SSH]] and [[Internet Key Exchange|IKE]]. [[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.&lt;ref&gt;{{Citation | publisher = IETF | title = RFC 6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}&lt;/ref&gt; Less standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.

==Uses==
The most common uses of the reverse DNS include:

* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.
* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, or dynamically assigned addresses unlikely to be used by legitimate mail servers. Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com." Some anti-spam filters assume that email that originates from such addresses is likely to be spam, and may refuse connection.&lt;ref&gt;[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]&lt;/ref&gt;&lt;ref&gt;[http://postmaster.aol.com/info/rdns.html reference page from AOL] {{webarchive |url=https://web.archive.org/web/20061210223820/http://postmaster.aol.com/info/rdns.html |date=December 10, 2006 }}&lt;/ref&gt;
* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, since [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually cannot achieve forward validation when they use [[zombie computer]]s to forge domain records.
* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address.

==References==
{{reflist}}

==External links==
* {{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}
* [http://dns.icann.org ICANN DNS Operations]
* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]
* RDNS policies: [https://web.archive.org/web/20121106162649/http://postmaster.aol.com:80/Postmaster.Errors.php#554rlyb1#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]

[[Category:Information retrieval systems]]
[[Category:Domain name system]]

[[nl:Domain Name System#Omgekeerde lookups]]</text>
      <sha1>b01bjll8l2i0uon6dxxsxnw8b45mf41</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Image search</title>
    <ns>14</ns>
    <id>25810647</id>
    <revision>
      <id>666714786</id>
      <parentid>337897553</parentid>
      <timestamp>2015-06-13T03:52:54Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="89" xml:space="preserve">

[[Category:Applications of computer vision]]
[[Category:Information retrieval systems]]</text>
      <sha1>po4imn1f4bvk9ggrkjink85i70hwk3o</sha1>
    </revision>
  </page>
  <page>
    <title>Outline of search engines</title>
    <ns>0</ns>
    <id>34320324</id>
    <revision>
      <id>754383548</id>
      <parentid>754373476</parentid>
      <timestamp>2016-12-12T10:37:43Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>add section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6494" xml:space="preserve">&lt;!--... Attention:  THIS IS AN OUTLINE

        part of the set of 700+ outlines listed at
             [[Portal:Contents/Outlines]].

                 Wikipedia outlines are
              a special type of list article.
              They make up one of Wikipedia's
                content navigation systems

                See [[Wikipedia:Outlines]]
                      for more details.
                   Further improvements
              to this outline are on the way
...--&gt;
The following [[Outline (list)|outline]] is provided as an overview of and topical guide to search engines. 

'''[[Search engine (computing)|Search engine]]''' &amp;ndash; [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented as a list, and are commonly called ''hits''.

{{TOC limit|limit=2}}

== What ''type'' of thing is a search engine? ==

A search engine can be described as all of the following:

* [[Software]] &amp;ndash;
** [[Computer program]] &amp;ndash;
*** [[Application software]] &amp;ndash; computer software designed to help the user to perform specific tasks. Also known as an application or an "app".

== Types of search engines ==

* [[Database search engine]] &amp;ndash;
* [[Desktop search engine]] &amp;ndash;
* [[Distributed search engine]] &amp;ndash; search engine where there is no central server. Unlike traditional centralized search engines, work such as crawling, data mining, indexing, and query processing is distributed among several peers in decentralized manner where there is no single point of control.
* [[Enterprise search engine]] &amp;ndash; search engine employed on and for access to the information on an organization's computer network.
* [[Human search engine]] &amp;ndash; uses human participation to filter the search results and assist users in clarifying their search request. The goal is to provide users with a limited number of relevant results, as opposed to traditional search engines that often return a large number of results that may or may not be relevant.
* [[Hybrid search engine]] &amp;ndash; uses different types of data with or without ontologies to produce the algorithmically generated results based on web crawling. Previous types of search engines only use text to generate their results.
* [[Intelligent medical search engine]]
* [[Metasearch engine]] &amp;ndash; search tool[1] that sends user requests to several other search engines and/or databases and aggregates the results into a single list or displays them according to their source. Metasearch engines enable users to enter search criteria once and access several search engines simultaneously.
** [[Search aggregator]]
* [[Organic search engine]] &amp;ndash; manually operated search service which uses a combination of computer algorithms and human researchers to look up a search query. A search query submitted to an organic search engine is analysed by a human operator who researches the query then formats the response to the user.
* [[Web search engine]] &amp;ndash; designed to search for information on the World Wide Web and FTP servers. The search results are generally presented in a list of results often referred to as SERPS, or "search engine results pages".
** [[Audio search engine]] &amp;ndash; web-based search engine which crawls the web for audio content.
** [[Collaborative search engine]] &amp;ndash; emerging trend for Web search and Enterprise search within company intranets. CSEs let users concert their efforts in information retrieval (IR) activities, share information resources collaboratively using knowledge tags, and allow experts to guide less experienced people through their searches.
** [[Social search engine]] &amp;ndash; type of web search that takes into account the Social Graph of the person initiating the search query.
** [[Video search engine]] &amp;ndash; web-based search engine which crawls the web for video content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers.
* [[Visual search engine]] &amp;ndash; designed to search for information on the World Wide Web through the input of an image or a search engine with a visual display of the search results. Information may consist of web pages, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query).

== Specific search engines ==
{{Main|List of search engines}}

== Search engine software ==

* [[List of search engine software]]

== Search-based applications ==

[[Search-based application]] &amp;ndash;
* [[Bibliographic database]]
* [[Online database]]
** [[List of online databases]]
*** [[List of academic databases and search engines]]
* [[Digital library]]
** [[List of digital library projects]]
*** [[List of online magazines]]
*** [[Wikipedia:List of online newspaper archives]]
* [[Electronic journal]]
** [[Lists of academic journals]]
*** [[List of open-access journals]]
* Digital encyclopedia
** [[Internet encyclopedia]]*
*** [[List of online encyclopedias]]
* [[Wiki]]
** [[List of wikis]]
* Digital dictionary
** [[Online dictionary]]
*** [[List of online dictionaries]]

== Search engine technology ==

[[Search engine technology]]
* [[Search algorithm]]
* [[Search engine image protection]]
* [[Search engine indexing]]
* [[Search engine optimization]]
* [[Search engine results page]]
* [[List of search engine software|Search engine software]]
* [[Search engine submission]]
** [[Search engine optimization copywriting]]
* [[Web crawler]]

== Search engine marketing ==
[[Search engine marketing]]
* [[Pay per click]]
* [[Cost per impression]]
* [[Search analytics]]
* [[Web analytics]]

== Persons influential in search engines ==
* [[Sergey Brin]]
* [[Larry Page]]
* [[Eric Schmidt]]

== See also ==
* [[Outline of the Internet]]
** [[Outline of Google]]
* [[Human flesh search engine]]
{{Clear}}

== References ==
{{Reflist}}

== External links ==
{{Sisterlinks|Search engine}}

* [http://wikimindmap.com/viewmap.php?wiki=en.wikipedia.org&amp;topic=Outline+of+search+engines&amp;Submit=Search This outline displayed as a mindmap], at ''wikimindmap.com''
* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}

{{Outline footer}}

[[Category:Information retrieval systems]]
[[Category:Wikipedia outlines|Search engines]]
[[Category:Articles created via the Article Wizard|Search engines]]</text>
      <sha1>06r1edv9lqr7ni39lukk8l21ov48ztd</sha1>
    </revision>
  </page>
  <page>
    <title>Phynd</title>
    <ns>0</ns>
    <id>2353165</id>
    <revision>
      <id>749640039</id>
      <parentid>726558084</parentid>
      <timestamp>2016-11-15T13:10:58Z</timestamp>
      <contributor>
        <username>Zyxw</username>
        <id>473593</id>
      </contributor>
      <minor />
      <comment>/* External links */update archive links using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2408" xml:space="preserve">'''Phynd''' (Find) is a LAN-indexing [[search engine]] used to facilitate [[peer-to-peer]] [[file sharing]] over a [[local-area network]].  It was developed by [[Rensselaer Polytechnic Institute]] student researcher Jesse Jordan to solve various problems experienced by Microsoft browsers and networks when trying to index files within a large network.  

One of the results of Jordan's file indexing exercise was that large numbers of downloaded music files were found on other users' local systems.  Jordan was relatively unconcerned with the nature of the content he was indexing.  His objective was enabling a network to index all its files without crashing any elements of the network. &lt;ref&gt;Lessig 2004, p. 48&lt;/ref&gt;

Although Jordan's search engine, Phynd, merely indexed public data that users elected to share through an integrated sharing feature in [[Microsoft Windows]], Jordan was sued by [[RIAA]] for copyright infringement. The original Phynd search engine, rpi.phynd.net (defunct), existing years before and months after Jesse's lawsuit was shut down by the enormous pressure that the [[RIAA]] in November 2003 brought upon Jordan and his family. The RIAA was demanding $15,000,000 to settle.&lt;ref&gt;Lessig 2004, p. 51&lt;/ref&gt;  As a student researcher, Jordan had only modest life savings of approximately $12,000, and his family had only modest assets.  His limited options were to fight the RIAA at enormous personal expense, or to settle. Jordan, chose to settle outside of court for $12,000, his entire life savings from student employment. He subsequently raised $12,005.67 via contributions on a personal web site in July 2003.

==References==
*Lessig, Lawrence (2004) . "Free Culture" . ISBN 1-59420-006-8 . The Penguin Press . New York 

==Notes==
{{reflist}} 


==External links==
* [http://poly.rpi.edu/old/article_view.php3?view=2599&amp;part=1 Phynd server shut down by threat of lawsuit]
* {{webarchive |date=2013-01-20 |url=http://archive.is/20130120043253/http://news.com.com/2100-1027-995429.html?tag=fd_lede1_hed |title=RIAA sues campus file-swappers}}
* [http://www.isp-planet.com/news/2003/riaa_030505.html Students to Pay in RIAA Song-Swapping Suit]
* [http://articles.chicagotribune.com/2003-07-07/news/0307080008_1_recording-industry-donations-settlement $12,005.67: Amount Jesse Jordan, sued by the recording...]

[[Category:Information retrieval systems]]

{{compu-network-stub}}</text>
      <sha1>4dzg7d2qtek3na3okyuiqppm3t7htj9</sha1>
    </revision>
  </page>
  <page>
    <title>RetrievalWare</title>
    <ns>0</ns>
    <id>23327147</id>
    <revision>
      <id>666716823</id>
      <parentid>572116340</parentid>
      <timestamp>2015-06-13T04:23:16Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8795" xml:space="preserve">{{COI|date=June 2009}}
{{Infobox software
| name                   = RetrievalWare
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Fast Search &amp; Transfer]], [[Convera]], Excalibur Technologies, ConQuest Software, Microsoft
| latest release version = 8.2
| latest release date    = {{release date|2006|10|13}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]]
| genre                  = [[Search algorithm|Search]] and [[Index (search engine)|Index]]
| website                =
}}
'''RetrievalWare''' is an [[enterprise search|enterprise search engine]] emphasizing [[natural language processing]] and [[semantic networks]] which was commercially available from 1992 to 2007 and is especially known for its use by government intelligence agencies.&lt;ref&gt;{{cite news
| url = http://www.washingtonpost.com/ac2/wp-dyn/A30161-2004Dec2
| title = Agencies Find What They're Looking For|publisher = The Washington Post
| date = 2004-12-03
| first=David A.
| last=Vise
| accessdate=2010-05-22
}}&lt;/ref&gt;

== History ==

RetrievalWare was initially created by [http://www.linkedin.com/pub/paul-nelson/3/316/146 Paul Nelson], [http://kenclark7.home.comcast.net/~kenclark7/ Kenneth Clark], and [http://www.linkedin.com/in/edaddison Edwin Addison] as part of ConQuest Software. Development began in 1989, but the software was not commercially available on a wide scale until 1992. Early funding was provided by [[Rome Laboratory]] via a [[Small Business Innovation Research]] grant.&lt;ref&gt;
{{citation
| title = FY 1991 SBIR SOLICITATION - PHASE I AWARD ABSTRACTS - AIR FORCE PROJECTS - VOLUME III
| pages = 70–71
| date = 1992-07-06
| url = http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA252509&amp;Location=U2&amp;doc=GetTRDoc.pdf
}} - Note that "Synchronetics" was the original name for ConQuest Software Incorporated.
&lt;/ref&gt;

On July 6, 1995, ConQuest Software was merged with Excalibur Technologies&lt;ref&gt;{{cite press release
 | title = Excalibur Technologies to merge with ConQuest Software; text and multimedia information retrieval leaders join forces to expand products, channels and markets| publisher = Business Wire
 | date = 1995-07-06
 | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_July_6/ai_17215774/?tag=content;col1
 }}&lt;/ref&gt; and the product was rebranded as RetrievalWare. On December 21, 2000, Excalibur Technologies was combined with [[Intel|Intel Corporation]]'s Interactive Media Services division to form the [[Convera|Convera Corporation]].&lt;ref&gt;{{cite news
 | title = Intel and Excalibur Form Convera Corporation| publisher = Silicon Valley / San Jose Business Journal
 | date = 2000-12-21
 | url = http://sanjose.bizjournals.com/sanjose/stories/2000/12/25/daily5.html
 }}&lt;/ref&gt; Finally, on April 9, 2007, the RetrievalWare software and business was purchased by [[Fast Search &amp; Transfer]] at which point the product was officially retired.&lt;ref name="fastpurchase"&gt;{{cite news
 | title = FAST Acquires Convera’s RetrievalWare Business| publisher = Information Today, Inc.
 | date = 2007-04-09
 | url = http://newsbreaks.infotoday.com/NewsBreaks/FAST-Acquires-Converas-RetrievalWare-Business-35840.asp
 | quote = While FAST will continue to support the RetrievalWare platform, it will not continue development on it or add new features. RetrievalWare customers will be offered an upgrade path to FAST’s own offering.
 }}&lt;/ref&gt; [[Microsoft|Microsoft Corporation]] continues to maintain the product for its existing customer base.

Annual revenues for RetrievalWare peaked in 2001 at around $40 million US dollars.&lt;ref&gt;{{citation
| title = Convera Corp · 10-K · For 1/1/01
| date = 2001-01-01
| url = http://www.secinfo.com/d12B5f.4f89a.c.htm
}} - Indicates that Convera products accounted for 85% of the total revenue of $51.5 million.&lt;/ref&gt;

== Use of natural language techniques ==

RetrievalWare is a relevancy ranking text search system with processing enhancements drawn from the fields of [[natural language processing|natural language processing (NLP)]] and [[semantic networks]]. NLP algorithms include dictionary-based [[stemming]] (also known as [[lemmatisation]]) and dictionary-based phrase identification. Semantic networks are used by RetrievalWare to expand the query words entered by the user to related terms with terms weights determined by the distance from the user's original terms. In addition to automatic expansion, a feedback-mode whereby users could choose the meaning of the word before performing the expansion was available. The first semantic networks were built using [[WordNet]].

In addition, RetrievalWare implemented a form of [[n-gram]] search (branded as APRP - Adaptive Pattern Recognition Processing&lt;ref&gt;[http://www.thefreelibrary.com/Excalibur+Announces+Excalibur+RetrievalWare+6.5+Featuring+...-a019849416 Excalibur Announces Excalibur RetrievalWare 6.5 Featuring RetrievalWare FileRoom] - Contains a description of APRP&lt;/ref&gt;), designed to search over documents with [[Optical character recognition|OCR]] errors. Query terms are divided into sets of 2-grams which are used to locate similarly matching terms from the [[inverted index]]. The resulting matches are weighted based on similarly measures and then used to search for documents.

All of these features were available no later than 1993&lt;ref name="trec2"&gt;[http://trec.nist.gov/pubs/trec2/papers/txt/25.txt Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC2)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec2/t2_proceedings.html here]&lt;/ref&gt; and ConQuest software has claimed that it was the first commercial text-search system to implement these techniques.&lt;ref&gt;{{cite press release
 | title = Homework Helper debuts on Prodigy using ConQuest search engine| publisher = Business Wire
 | date = 1995-02-09
 | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_Feb_9/ai_16432681/
 | quote = ConQuest is the only search engine which uses dictionaries, thesauri and other lexical resources to build in a semantic knowledgebase of over 440,000 word meanings, and 1.6 million word relationships.
 }}&lt;/ref&gt;

== Other notable features ==

Other notable features of RetrievalWare include distributed search servers,&lt;ref name="trec2"/&gt; synchronizers for indexing external [[content management system]]s and [[relational database]]s,&lt;ref name="kmref"&gt;{{cite news
| url = http://www.kmworld.com/Articles/Editorial/Feature/Excalibur-RetrievalWare-more-than-information-retrieval--9139.aspx
| title = Excalibur RetrievalWare: more than information retrieval
| publisher = KMWorld
| date = 1999-10-01
}}&lt;/ref&gt; a heterogeneous security model,&lt;ref name="kmref"/&gt; [[document classification|document categorization]],&lt;ref name="kmref"/&gt; real-time document-query matching (profiling),&lt;ref name="trec2"/&gt; multi-lingual searches (queries containing terms from multiple languages searching for documents containing terms from multiple languages), and cross-lingual searches (queries in one language searching for documents in a different language).&lt;ref&gt;{{cite news
| title = Multimedia search, retrieval, categorization
| url = http://www.kmworld.com/Articles/News/Breaking-News/Multimedia-search,-retrieval,-categorization-12763.aspx
| date = 2002-03-25
| publisher = KMWorld
}}&lt;/ref&gt;

== Participation in TREC ==

RetrievalWare participated in the [[Text REtrieval Conference (TREC)|Text REtrieval Conference]] in 1992 (TREC-1), 1993 (TREC-2), and 1995 (TREC-4).

In TREC-1&lt;ref name="trec1"&gt;[http://trec.nist.gov/pubs/trec1/papers/21.txt   Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC-1)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec1/t1_proceedings.html here]&lt;/ref&gt; and TREC-4,&lt;ref&gt;[http://trec.nist.gov/pubs/trec4/papers/excalibur.ps.gz The Excalibur TREC-4 System, Preparations, and Results] - A PDF version of which can be found [http://www.pnelsoncomposer.com/writings/excalibur-trec4.pdf here] and the complete proceedings can be found [http://trec.nist.gov/pubs/trec4/t4_proceedings.html here]&lt;/ref&gt; the RetrievalWare runs for manually entered queries produced the best results based on the 11-point averages over all search engines which participated in the ''ad hoc'' category where search engines are allowed a single opportunity to process previously unknown queries against an existing database.

== References ==
{{Reflist}}

== External links ==

*  [http://www.saoug.org.za/archive/1999/9907.pdf Marketing presentation on RetrievalWare semantic networks and adaptive pattern recognition algorithms]

{{DEFAULTSORT:Retrievalware}}
[[Category:Information retrieval systems]]</text>
      <sha1>e0pjugk02u3pub6ptf4n5cigu4jxfcv</sha1>
    </revision>
  </page>
  <page>
    <title>Trip (search engine)</title>
    <ns>0</ns>
    <id>14069461</id>
    <revision>
      <id>729838341</id>
      <parentid>666716945</parentid>
      <timestamp>2016-07-14T22:11:08Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>/* Users */USA is deprecated, per [[MOS:NOTUSA]], and correct [[MOS:OVERLINK|overlinking]] of common places using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3853" xml:space="preserve">{{Infobox software
|name = Trip
|logo =
|screenshot =
|caption =
|developer = Trip Database Ltd
|latest_release_version =
|latest_release_date =
|latest_preview_version =
|latest_preview_date =
|operating_system =
|genre = [[Search engine]]
|language =
|license = [[Freeware]]
|website = [http://www.tripdatabase.com Trip]
}}

'''Trip''' is a free [[Illness|clinical]] [[search engine]].  Its primary function is to help [[clinician]]s identify the best available evidence with which to answer clinical questions.  Its roots are firmly in the world of [[evidence-based medicine]].

==History==
The site was created in 1997 as a search tool to help the staff of ATTRACT&lt;ref&gt;[http://www.attract.wales.nhs.uk ATTRACT]&lt;/ref&gt; answer clinical questions for [[General practitioner|GP]]s in [[Gwent (county)|Gwent]], [[South Wales]].  Shortly afterwards ''[[Bandolier (journal)|Bandolier]]'' highlighted the Trip Database and this helped establish the site.  In 2003, after a period of steady growth, Trip became a subscription-only service.  This was abandoned In September 2006 and since then the growth in usage has been significant. Originally "Trip" stood for Turning Research Into Practice, but the system is now simply called Trip.&lt;ref&gt;{{cite web |url= http://www.tripdatabase.com/about |title=About |work=Trip |publisher=Trip Database Ltd |accessdate=3 April 2013}}&lt;/ref&gt;

==Process==
The core to Trip’s system is the identification and incorporation of new evidence.  The people behind Trip are heavily involved in clinical question answering systems (e.g., [[NLH Q&amp;A Service]]).  Therefore, if resources are identified that are useful in the Q&amp;A process they tend to be added to Trip.

==Users==
A site survey (September 2007) showed that the site was searched over 500,000 times per month, with 69% from [[health professional]]s and 31% from members of the public. Of the health professionals around 43% are doctors.  Most users come from either the United Kingdom or the United States.  In September 2008 the site was searched 1.4 million times. To date the site has been searched over 100 millions times.

==Recent updates==
At the end of 2012 Trip had a major upgrade which saw significant new enhancements:

* New content - widening the coverage
* New design
* Advanced search
* PICO search - to help users formulate focused searches
* Improved filtering
* Search history/timeline - recording all a user activity on the site
* Related articles

==Education tracker==
Trip has an education tracker which allows users to record their activity on Trip which can then be used, subject to local regulations, for revalidation/re-licensing.

==Future areas of work==
Trip is exploring numerous innovative technologies to improve the site, these include:
* Link out to full-text articles via Trip.
* RCT database.
* Rapid (within a week) systematic review quality reviews.
* Learning from users prior use of the site and that of similar users to improve search results.

==Trip Answers==
In November 2008, Trip released a new website, Trip Answers.  This is a repository of clinical Q&amp;As from a variety of Q&amp;A services. At launch it had over 5,000 Q&amp;As and currently has over 6,300.  This content has been integrated into Trip.

==References==
{{reflist}}

==External links==
* [http://www.tripdatabase.com Trip]
* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1852632/ Using the Turning Research Into Practice (TRIP) database: how do clinicians really search?] an evaluation of the website.
* [http://libguides.lhl.uab.edu/content.php?pid=108596&amp;sid=1099056 Reviews: From Systematic to Narrative] review of the site
* [http://guides.library.manoa.hawaii.edu/content.php?pid=250484&amp;sid=2157277 Evidence Based Pyramid] a pictorial representation of TRIP's approach to the evidence

[[Category:Medical websites]]
[[Category:Information retrieval systems]]</text>
      <sha1>j5dyq9hft2degfoqu79xgr9alxmyjct</sha1>
    </revision>
  </page>
  <page>
    <title>Pleade</title>
    <ns>0</ns>
    <id>35952152</id>
    <revision>
      <id>720657870</id>
      <parentid>692590571</parentid>
      <timestamp>2016-05-17T05:35:03Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Examples */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5340" xml:space="preserve">{{Infobox software
| name                   = Pleade-infoxbox
| title                  = Pleade
| logo                   = [[File:Pleade-logo.png]]
| logo caption           = Logo de Pleade
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = AJLSM
| developer              = AJLSM
| released               = &lt;!-- {{Start date|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 3.4
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = &lt;!-- 3.5 --&gt;
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]
| operating system       = [[Unix-like]], [[Microsoft Windows]]
| platform               = 
| size                   = 
| language               = French, English, German, Chinese
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| status                 = Active
| genre                  = Digital Library
| license                = GNU General Public License
| alexa                  = 
| website                = {{URL|http://www.pleade.com/}}
}}

'''Pleade''' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[Secure Document Exchange|SDX]] platform, it is a very flexible web application.

== History ==
The software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.&lt;ref&gt;[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]&lt;/ref&gt;

==Technologies==
Pleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.

It is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]] is under active development.&lt;ref&gt;[http://pleade.com/ Pleade 2012 : les imprimés numérisés et les formats XML METS / ALTO]&lt;/ref&gt;

== Features ==
* Customizable publication ;
* Customizable index creation ;
* Customizable search form ;
* Simple and advanced search among publish documents ;
* Federate search among different bases (e.g. EAD, METS) ;
* basket (for database and for images), a search history, printing, etc. ;
* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;
* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;
* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;
* Printing resulting and finding aids as PDF documents (with embedded images) ;
* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;
* Ability to import metadata from an [[Integrated library system|ILS]].

=== Pleade-Entreprise ===
* Pleade-Entreprise extended features to others XML format, such as [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]].

== Examples ==
These are examples of websites based on Pleade:
{{columns-list|2|
* Archival portals
** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]
** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]
** [http://odysseo.org/ Odysseo: Resources for the history of immigration]
** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]
** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]
** [http://jubilotheque.upmc.fr/ Jubilothèque, UPMC's scientific digital library]
** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault's Library "les Mots et les Choses" ENS]
* Portals documentary
** [http://www.michael-culture.org/fr/home Michael]
** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]
* Digital Libraries
** Digital Library of Lille
** Lille III
** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]
}}

== Related resources ==
* {{Official website|http://pleade.com}}
* [http://demo.pleade.com Official demo]
* [http://www.pleadeenpratique.org/ Pleade in practice]
* [http://www.ajlsm.com/produits/sdx SDX]
* [http://www.ajlsm.com AJLSM company]

== References ==
&lt;references/&gt;

[[Category:Digital library software]]
[[Category:Free software]]
[[Category:Information retrieval systems]]
[[Category:Archival science]]</text>
      <sha1>ebujfg66n9mmti7dd8taj14g3bwp9y8</sha1>
    </revision>
  </page>
  <page>
    <title>Dynatext</title>
    <ns>0</ns>
    <id>14460441</id>
    <revision>
      <id>740772359</id>
      <parentid>708496452</parentid>
      <timestamp>2016-09-23T05:29:59Z</timestamp>
      <contributor>
        <username>Cnilep</username>
        <id>3729738</id>
      </contributor>
      <comment>disambiguate</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4594" xml:space="preserve">{{primary sources|date=October 2011}}
'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.

DynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.

DynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996, when it had about 150 employees.

DynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as one of the originators of the notion of [[well-formed document|well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].

[[Inso]] corporation went out of business in 2002.

==Technology==

DynaText accepted [[SGML]] as input, and built a binary representation of the structure (similar to [[Document Object Model|DOM]] for [[XML]], but persistent), as well as a full-text [[inverted index]] of the text, elements, and attributes. Customers typically distributed such compiled e-books on CD-ROM or via network servers. Later versions of DynaText could also read SGML on the fly, providing exactly the same interface.

Unlike many prior systems, DynaText was not limited to any particular [[Document type definition|DTD]] (or [[XML schema|schema]]). Rather, customers could build style sheets in a simple language (also SGML-based), using properties very much like the later [[DSSSL]], [[CSS]], and [[XSL-FO]]. However, every property could have an expression as its value, which would be evaluated (if necessary) for each element the style applied to. Graphics, tables, formulae, and plug-ins could be included in documents.

Unlike nearly all prior SGML systems, DynaText was not limited to documents that could fit in [[RAM]] on the viewing or serving computer system. Users commonly created documents in the tens to hundreds of MB. DynaText customers included aerospace, workstation and other computer industry firms, government, literary and technical publishers, and others.

Full-text searches were based on an inverted index of words and other tokens (except for Japanese text, which was handled specially). Dynatext could report the number of "hits" for a given search, that occur within each section in the table of contents (by default, the table of contents appeared in a separate pane as an expandable outline, and clicking on any entry scrolled the full-text pane to the start of the corresponding section). Searches could also restrict hits to particular SGML element types, or sequences of types; refer to attributes; and use Boolean operators and parentheses. The "and" operator restricted its operands to occurring near each other, by default in the same paragraph or comparable element.

==References==
*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]] (this note refers to a pre-release or very early release of DynaText).
*{{cite journal
 | id = MS
 | last = Smith
 | first = MacKenzie
 | title = Review: DynaText: An Electronic Publishing System
 | journal = Computers and the Humanities
 | volume = 27
 | issue = 5/6
 | pages = 415–420
 | publisher = Springer
 | location =
 | date = 1993
 | jstor = http://www.jstor.org/stable/30204569
 | issn = 0010-4817
 }}

*{{cite book
 | url = http://techpubs.sgi.com/library/dynaweb_docs/0630/SGI_EndUser/books/IIDWeb_UG/sgi_html/ch05.html
 | title = IRIS InSight™ DynaWeb™ User's Guide: Chapter 5. Introduction to the DynaText Search Language
 | publisher = Silicon Graphics, Inc.
}} Document Number: 007-3229-001

*{{cite journal
 | url = http://www.w3.org/Conferences/WWW4/ora_951122/112.html
 | title = DynaWeb: Interfacing Large SGML Repositories and the WWW
 | journal = Fourth International World Wide Web Conference: ``The Web Revolution''
 | date = 1995
 | location = Boston
 | first = Gavin Thomas 
 | last = Nicol
}}

[[Category:Information retrieval systems]]</text>
      <sha1>kye32nudur6b79p136mi79hn09fkmfe</sha1>
    </revision>
  </page>
  <page>
    <title>Relevance (information retrieval)</title>
    <ns>0</ns>
    <id>442684</id>
    <revision>
      <id>758504944</id>
      <parentid>758504623</parentid>
      <timestamp>2017-01-05T21:09:20Z</timestamp>
      <contributor>
        <username>Nihiltres</username>
        <id>236191</id>
      </contributor>
      <minor />
      <comment>Added "(disambiguation)" suffix in hatnote per [[WP:INTDAB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9308" xml:space="preserve">{{other uses|Relevance (disambiguation)}}

In [[information science]] and [[information retrieval]], '''relevance''' denote how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.

== History ==

The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.{{citation needed|date=June 2015}}

The formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.&lt;ref&gt;Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810‐832.&lt;/ref&gt;

Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".{{citation needed|date=June 2015}}

== Evaluation ==
{{main article|Information retrieval#Performance and correctness measures}}

The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.{{citation needed|date=June 2015}}

In order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.

In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance.{{citation needed|date=June 2015}} These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).

== Clustering and relevance ==

The [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.&lt;ref name=diazthesis&gt;F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.&lt;/ref&gt;    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:
* cluster-based information retrieval&lt;ref name=croftcbir&gt;W. B. Croft, “A model of cluster searching based on classification,” Information Systems, vol. 5, pp. 189–195, 1980.&lt;/ref&gt;&lt;ref name=griffithscbir&gt;A. Griffiths, H. C. Luckhurst, and P. Willett, “Using interdocument similarity information in document retrieval systems,” Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3–11, 1986.&lt;/ref&gt;
* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.&lt;ref name=lmcbir&gt;X. Liu and W. B. Croft, “Cluster-based retrieval using language models,” in SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186–193, ACM Press, 2004.&lt;/ref&gt;    It is important to ensure that clusters – either in isolation or combination – successfully model the set of possible relevant documents.

A second interpretation, most notably advanced by Ellen Voorhees,&lt;ref name=voorheescbir&gt;E. M. Voorhees, “The cluster hypothesis revisited,” in SIGIR ’85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188–196, ACM Press, 1985.&lt;/ref&gt;    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,
* multiple cluster retrieval&lt;ref name=griffithscbir/&gt;&lt;ref name=voorheescbir/&gt;
* spreading activation&lt;ref name=preece&gt;S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.&lt;/ref&gt; and relevance propagation&lt;ref name=relprop&gt;T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, “A study of relevance propagation for web search,” in SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408–415, ACM Press, 2005.&lt;/ref&gt; methods
* local document expansion&lt;ref name=docexpansion&gt;A. Singhal and F. Pereira, “Document expansion for speech retrieval,” in SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34–41, ACM Press, 1999.&lt;/ref&gt;
* score regularization&lt;ref name=diazreg&gt;F. Diaz, “Regularizing query-based retrieval scores,” Information Retrieval, vol. 10, pp. 531–562, December 2007.&lt;/ref&gt;
Local methods require an accurate and appropriate document similarity measure.

==Problems and alternatives==

The documents which are most relevant are not necessarily those which are most useful to display in the first page of search results.  For example, two duplicate documents might be individually considered quite relevant, but it is only useful to display one of them.  A measure called "maximal marginal relevance" (MMR) has been proposed to overcome this shortcoming. It considers the relevance of each document only in terms of how much new information it brings given the previous results.&lt;ref&gt;{{cite journal|last1=Carbonell|first1=Jaime|last2=Goldstein|first2=Jade|title=The use of MMR, diversity-based reranking for reordering documents and producing summaries|journal=Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval|date=1998|doi=10.1145/290941.291025|url=http://dl.acm.org/citation.cfm?id=291025}}&lt;/ref&gt;

In some cases, a query may have an ambiguous interpretation, or a variety of potential responses.  Providing a diversity of results can be a consideration when evaluating the utility of a result set.&lt;ref&gt;http://www.dcs.gla.ac.uk/workshops/ddr2012/&lt;/ref&gt;

==References==
 {{reflist}}

==Additional reading==
*Hjørland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.
*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9
*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])
*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])
*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])
*Introduction to Information Retrieval: Evaluation. Stanford. ([http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf presentation in PDF])

[[Category:Information retrieval evaluation]]</text>
      <sha1>eaqe0iui82ul4m0yk21s6hphf010uuf</sha1>
    </revision>
  </page>
  <page>
    <title>Relevance feedback</title>
    <ns>0</ns>
    <id>5818361</id>
    <revision>
      <id>727842398</id>
      <parentid>703409284</parentid>
      <timestamp>2016-07-01T15:53:37Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* Blind feedback */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 10 to → between 10 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8071" xml:space="preserve">'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.

== Explicit feedback ==

Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.

Users may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.

The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

A performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].

== Implicit feedback ==

Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf]. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response&lt;ref&gt;Jansen, B. J. and McNeese, M. D. 2005. [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_assistance_jasist2005.pdf Evaluating the effectiveness of and patterns of interactions with automated assistance in IR systems]. Journal of the American Society for Information Science and Technology. 56(14), 1480-1503&lt;/ref&gt;&lt;ref&gt;Kelly, Diane, and Jaime Teevan. "Implicit feedback for inferring user preference: a bibliography." ACM SIGIR Forum. Vol. 37. No. 2. ACM, 2003.&lt;/ref&gt;

The key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:

# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and
# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback

An example of this is [[dwell time (information retrieval)|dwell time]], which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.
Another example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.

== Blind feedback ==

Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:

# Take the results returned by initial query as relevant results (only top k with k being between 10 and 50 in most experiments).
# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.
# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.

Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.

This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.&lt;ref&gt;Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.&lt;/ref&gt; Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.&lt;ref&gt;Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.&lt;/ref&gt; 
Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.

Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.

== Using relevance information ==

Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

==Further reading==
*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's
*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''
*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

== References ==
{{reflist|2}}

[[Category:Internet search algorithms]]
[[Category:Information retrieval evaluation]]
[[zh:相关反馈]]</text>
      <sha1>3px44bcr09bhfa7my6y6xoy5g4c52nk</sha1>
    </revision>
  </page>
  <page>
    <title>Precision and recall</title>
    <ns>0</ns>
    <id>14343887</id>
    <revision>
      <id>762972179</id>
      <parentid>755633573</parentid>
      <timestamp>2017-01-31T18:41:11Z</timestamp>
      <contributor>
        <ip>63.66.64.244</ip>
      </contributor>
      <comment>/* Introduction */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18105" xml:space="preserve">[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]
In [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]].

Suppose a computer program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a [[Search engine (computing)|search engine]] returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.  So, in this case, precision is "how useful the search results are", and recall is "how complete the results are".

In [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &amp;minus; 4 = 3 type I errors and 9 &amp;minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.

In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.

==Introduction==
In an information retrieval scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.

In a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of true positives'' (i.e. the number of items correctly labeled as belonging to the positive class) ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and [[Type I and type II errors|false positives]], which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and [[Type I and type II errors|false negatives]], which are items which were not labeled as belonging to the positive class but should have been).

In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).

In a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).

Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).

Usually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s [[Informedness]] (DeltaP') and [[Markedness]] (DeltaP).&lt;ref name="Powers2011" /&gt;&lt;ref&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt; [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).&lt;ref name="Powers2011"/&gt; Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.&lt;ref name="Powers2011"/&gt;  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, [[Informedness]] and [[Markedness]] are Kappa-like renormalizations of Recall and Precision,&lt;ref&gt;{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}&lt;/ref&gt; and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.

== Definition (information retrieval context) ==

In [[information retrieval]] contexts, precision and recall are defined in terms of a set of ''retrieved documents'' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of ''relevant documents'' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]]. The measures were defined in {{harvtxt|Perry|Kent|Berry|1955}}.

===Precision===

In the field of [[information retrieval]], precision is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:

&lt;math display="block"&gt; \text{precision}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|} &lt;/math&gt;

Precision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

For example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.

Precision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.

===Recall===

Recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.

&lt;math display="block"&gt; \text{recall}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{relevant documents}\}|} &lt;/math&gt;

For example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned.

In binary classification, recall is called [[Sensitivity and specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

== Definition (classification context) ==
For classification tasks, the terms ''true positives'', ''true negatives'', ''false positives'', and ''false negatives'' (see [[Type I and type II errors]] for definitions) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation'').

Let us define an experiment from ''P'' positive instances and ''N'' negative instances for some condition. The four outcomes can be formulated in a 2×2 [[contingency table]] or [[confusion matrix]], as follows:

{{DiagnosticTesting_Diagram}}
{{Confusion matrix terms}}

&lt;!--
{| border="0" align="center" style="text-align: center; background: #FFFFFF;"
|+
!
! colspan="2" style="background: #ddffdd;"|actual class &lt;br/&gt; (observation)
|-
!
|-----
|+
! rowspan="2" style="background: #ffdddd;"|predicted class &lt;br/&gt; (expectation)
| '''tp''' &lt;br&gt; (true positive) &lt;br/&gt; Correct result
| '''fp''' &lt;br&gt; (false positive) &lt;br/&gt; Unexpected result
|-bgcolor="#EFEFEF"
| '''fn''' &lt;br&gt; (false negative) &lt;br/&gt; Missing result
| '''tn''' &lt;br&gt; (true negative) &lt;br/&gt; Correct absence of result
|+
|}

--&gt;

Precision and recall are then defined as:&lt;ref name="OlsonDelen"&gt;Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1&lt;/ref&gt;

&lt;math display="block"&gt;\text{Precision}=\frac{tp}{tp+fp} \, &lt;/math&gt;

&lt;math display="block"&gt;\text{Recall}=\frac{tp}{tp+fn} \, &lt;/math&gt;

Recall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy and precision#In binary classification|accuracy]].&lt;ref name="OlsonDelen" /&gt; True negative rate is also called [[Specificity (tests)#Specificity|specificity]].

&lt;math display="block"&gt;\text{True negative rate}=\frac{tn}{tn+fp} \, &lt;/math&gt;

&lt;math display="block"&gt;\text{Accuracy}=\frac{tp+tn}{tp+tn+fp+fn} \, &lt;/math&gt;

== Probabilistic interpretation ==

It is possible to interpret precision and recall not as ratios but as probabilities:

* Precision is the probability that a (randomly selected) retrieved document is relevant.
* Recall is the probability that a (randomly selected) relevant document is retrieved in a search.

Note that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by ''randomly selected retrieved document'', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected.

Note that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation).

Another interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.

== F-measure ==
{{main article|F1 score}}
A measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:

&lt;math display="block"&gt;F = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{ \mathrm{precision} + \mathrm{recall}}&lt;/math&gt;

This measure is approximately the average of the two when they are close, and is more generally the [[harmonic mean]], which, for the case of two numbers, coincides with the square of the [[geometric mean]] divided by the [[arithmetic mean]]. There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric.&lt;ref name="Powers2011" /&gt; This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

It is a special case of the general &lt;math&gt;F_\beta&lt;/math&gt; measure (for non-negative real values of&amp;nbsp;&lt;math&gt;\beta&lt;/math&gt;):

&lt;math display="block"&gt;F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}&lt;/math&gt;

Two other commonly used &lt;math&gt;F&lt;/math&gt; measures are the &lt;math&gt;F_2&lt;/math&gt; measure, which weights recall higher than precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which puts more emphasis on precision than recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; "measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E_{\alpha} = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;, the second term being the weighted harmonic mean of precision and recall with weights &lt;math&gt;(\alpha, 1-\alpha)&lt;/math&gt;.  Their relationship is &lt;math&gt;F_\beta = 1 - E_{\alpha}&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;.

==Limitations as goals==
There are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).&lt;ref&gt;Zygmunt Zając. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/&lt;/ref&gt;

For [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,&lt;ref&gt;Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']&lt;/ref&gt;
{{quote|[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).
* Search results don't have to be very good.
* Recall?    Not important (as long as you get at least some good hits).
* Precision? Not important (as long as at least some of the hits on the first page you return are good).}}

==See also==
* [[Uncertainty coefficient]], also called ''proficiency''
* [[Sensitivity and specificity]]

== References ==
{{Reflist}}
{{refbegin}}
* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X
* Hjørland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237
* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''
* {{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |year=1955 |doi=10.1002/asi.5090060411}}
* van Rijsbergen, Cornelis Joost "Keith" (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4
{{refend}}

== External links ==
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval – C. J. van Rijsbergen 1979]
* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]

[[Category:Information retrieval evaluation]]
[[Category:Information science]]
[[Category:Bioinformatics]]

[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]</text>
      <sha1>1o065lccur4zt25gka29nsba8m7kosd</sha1>
    </revision>
  </page>
  <page>
    <title>Overlap coefficient</title>
    <ns>0</ns>
    <id>22049756</id>
    <revision>
      <id>725310648</id>
      <parentid>721860843</parentid>
      <timestamp>2016-06-14T21:54:34Z</timestamp>
      <contributor>
        <username>GermanJoe</username>
        <id>12935443</id>
      </contributor>
      <comment>+ tag, ref section, - SPA spam</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="721" xml:space="preserve">{{unreferenced|date=June 2016}}
The '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the smaller of the size of the two sets:

:&lt;math&gt;\mathrm{overlap}(X,Y) = \frac{| X \cap Y | }{\min(|X|,|Y|)}&lt;/math&gt;

If set ''X'' is a [[subset]] of ''Y'' or the converse then the overlap coefficient is equal to one.

==References==
&lt;references /&gt;

[[Category:Information retrieval techniques]]
[[Category:Information retrieval evaluation]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>5v4i27p6yzl6rl4pa7y4ghgm6rel1s8</sha1>
    </revision>
  </page>
  <page>
    <title>Discounted cumulative gain</title>
    <ns>0</ns>
    <id>19542049</id>
    <revision>
      <id>753666505</id>
      <parentid>753665876</parentid>
      <timestamp>2016-12-08T15:08:55Z</timestamp>
      <contributor>
        <username>Antoine-sac</username>
        <id>29832403</id>
      </contributor>
      <comment>/* Example */ Removed unnecessary (thus confusing) emphasis on rel_1</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10866" xml:space="preserve">'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.&lt;ref&gt;Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422–446 (2002)&lt;/ref&gt;

== Overview ==

Two assumptions are made in using DCG and its related measures.

# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)
# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.

DCG originates from an earlier, more primitive, measure called Cumulative Gain.

=== Cumulative Gain ===

Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{CG_{p}} = \sum_{i=1}^{p} rel_{i} &lt;/math&gt;

Where &lt;math&gt;rel_{i}&lt;/math&gt; is the graded relevance of the result at position &lt;math&gt;i&lt;/math&gt;.

The value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document &lt;math&gt;d_{i}&lt;/math&gt; above a higher ranked, less relevant, document &lt;math&gt;d_{j}&lt;/math&gt; does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.

=== Discounted Cumulative Gain ===

The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:&lt;ref name="stanfordireval"&gt;{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}&lt;/ref&gt;

:&lt;math&gt; \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{rel_{i}}{\log_{2}(i+1)} = rel_1 + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}(i+1)} &lt;/math&gt;

Previously there has not been  any theoretically sound justification for using a [[logarithm]]ic reduction factor&lt;ref name=CMS2009&gt;{{cite book | title=Search Engines: Information Retrieval in Practice |author1=B. Croft |author2=D. Metzler |author3=T. Strohman |year=2010 | publisher=''Addison Wesley"}}&lt;/ref&gt; other than the fact that it produces a smooth reduction. But Wang et al. (2013)&lt;ref&gt;Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of Normalized Discounted Cumulative Gain (NDCG) Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).&lt;/ref&gt; give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner.

An alternative formulation of DCG&lt;ref&gt;Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363&lt;/ref&gt; places stronger emphasis on retrieving relevant documents:

:&lt;math&gt; \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} &lt;/math&gt;

The latter formula is commonly used in industry including major web search companies&lt;ref name="stanfordireval"/&gt; and data science competition platform such as Kaggle.&lt;ref&gt;{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}&lt;/ref&gt;

These two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]];&lt;ref name=CMS2009/&gt;{{rp|320}} &lt;math&gt;rel_{i} \in \{0,1\}&lt;/math&gt;.

Note that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.&lt;!-- Not very clear, does it affect or no the value of DCG? --&gt;

=== Normalized DCG ===

Search result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of &lt;math&gt;p&lt;/math&gt; should be normalized across queries. This is done by sorting all '''relevant''' documents in the corpus by their relative relevance, producing the maximum possible DCG through position &lt;math&gt;p&lt;/math&gt;, also called Ideal DCG (IDCG) through that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG_{p}} &lt;/math&gt;,

where:

:&lt;math&gt; \mathrm{IDCG_{p}} = \sum_{i=1}^{|REL|} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} &lt;/math&gt;

and |REL| represents the list of relevant documents (ordered by their relevance) in the corpus up to position p.

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.

== Example ==

Presented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning not relevant, 3 meaning highly relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as

:&lt;math&gt; D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} &lt;/math&gt;

the user provides the following relevance scores:

:&lt;math&gt; 3, 2, 3, 0, 1, 2 &lt;/math&gt;

That is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:

:&lt;math&gt; \mathrm{CG_{6}} = \sum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11&lt;/math&gt;

Changing the order of any two documents does not affect the CG measure. If &lt;math&gt;D_3&lt;/math&gt; and &lt;math&gt;D_4&lt;/math&gt; are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:


{| class="wikitable" border="1"
|-
! &lt;math&gt;i&lt;/math&gt;
! &lt;math&gt;rel_{i}&lt;/math&gt;
! &lt;math&gt;\log_{2}(i+1)&lt;/math&gt;
! &lt;math&gt; \frac{rel_{i}}{\log_{2}(i+1)} &lt;/math&gt;
|-
| 1
| 3
| 1
| 3
|-
| 2
| 2
| 1.585
| 1.262
|-
| 3
| 3
| 2
| 1.5
|-
| 4
| 0
| 2.322
| 0
|-
| 5
| 1
| 2.585
| 0.387
|-
| 6
| 2
| 2.807
| 0.712
|}

So the &lt;math&gt;DCG_{6}&lt;/math&gt; of this ranking is:

:&lt;math&gt; \mathrm{DCG_{6}} = \sum_{i=1}^{6} \frac{rel_{i}}{\log_{2}(i+1)} = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861&lt;/math&gt;

Now a switch of &lt;math&gt;D_3&lt;/math&gt; and &lt;math&gt;D_4&lt;/math&gt; results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.

The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.

To normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:

:&lt;math&gt; 3, 3, 2, 2, 1, 0 &lt;/math&gt;

The DCG of this ideal ordering, or ''IDCG (Ideal DCG)'' , is then:

:&lt;math&gt; \mathrm{IDCG_{6}} = 7.141 &lt;/math&gt;

And so the nDCG for this query is given as:

:&lt;math&gt; \mathrm{nDCG_{6}} = \frac{DCG_{6}}{IDCG_{6}} = \frac{6.861}{7.141} = 0.961 &lt;/math&gt;

== Limitations ==
# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores &lt;math&gt; 1,1,1 &lt;/math&gt; and &lt;math&gt; 1,1,1,0 &lt;/math&gt; respectively, both would be considered equally good even if the latter contains a bad result. One way to take into account this limitation is to use &lt;math&gt;1 - 2^{rel_{i}}&lt;/math&gt; in the numerator for scores for which we want to penalize and &lt;math&gt;2^{rel_{i}} - 1&lt;/math&gt; for all others. For example, for the ranking judgments &lt;math&gt;Excellent, Fair, Bad&lt;/math&gt; one might use numerical scores &lt;math&gt;1,0,-1&lt;/math&gt; instead of &lt;math&gt;2,1,0&lt;/math&gt;.
# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores &lt;math&gt; 1,1,1 &lt;/math&gt; and &lt;math&gt; 1,1,1,1,1 &lt;/math&gt; respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores &lt;math&gt; 1,1,1,0,0 &lt;/math&gt; and &lt;math&gt; 1,1,1,1,1 &lt;/math&gt; and quote nDCG as nDCG@5.
# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.

== References ==
{{Reflist|1}}

[[Category:Information retrieval evaluation]]</text>
      <sha1>9525lsrvg9khdq6kf8yhnp4r4rr1916</sha1>
    </revision>
  </page>
  <page>
    <title>Spearman's rank correlation coefficient</title>
    <ns>0</ns>
    <id>235623</id>
    <revision>
      <id>750510792</id>
      <parentid>746638684</parentid>
      <timestamp>2016-11-20T05:23:41Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Covariance and correlation]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20456" xml:space="preserve">[[File:spearman fig1.svg|300px|thumb|A Spearman correlation of 1 results when the two variables being compared are monotonically related, even if their relationship is not linear. This means that all data-points with greater x-values than that of a given data-point will have greater y-values as well. In contrast, this does not give a perfect Pearson correlation.]][[File:spearman fig2.svg|300px|thumb|When the data are roughly elliptically distributed and there are no prominent outliers, the Spearman correlation and Pearson correlation give similar values.]]
[[File:spearman fig3.svg|300px|thumb|The Spearman correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples. That is because Spearman's rho limits the outlier to the value of its rank.]] In [[statistics]], '''Spearman's rank correlation coefficient''' or '''Spearman's rho''', named after [[Charles Spearman]] and often denoted by the Greek letter [[rho (letter)|&lt;math&gt;\rho&lt;/math&gt;]] (rho) or as &lt;math&gt;r_s&lt;/math&gt;, is a [[non-parametric statistics|nonparametric]] measure of [[rank correlation]] ([[correlation and dependence|statistical dependence]] between the [[ranking]] of two [[Variable (mathematics)#Applied statistics|variables]]). It assesses how well the relationship between two variables can be described using a [[monotonic]] function.

The '''Spearman correlation''' between two variables is equal to the [[Pearson product-moment correlation coefficient|Pearson correlation]] between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.

Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) [[Ranking|rank]] (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of -1) rank between the two variables.

Spearman's coefficient is appropriate for both [[continuous variable|continuous]] and [[discrete variable]]s, including [[Level of measurement#Ordinal scale|ordinal]] variables.&lt;ref&gt;[[Level of measurement#Typology|Scale types]]&lt;/ref&gt;&lt;ref&gt;{{cite book|title=Jmp For Basic Univariate And Multivariate Statistics: A Step-by-step Guide|last=Lehman|first=Ann|publisher=SAS Press|year=2005|isbn=1-59047-576-3|location=Cary, NC|page=123}}&lt;/ref&gt; Both Spearman's &lt;math&gt;\rho&lt;/math&gt; and [[Kendall tau rank correlation coefficient|Kendall's &lt;math&gt;\tau&lt;/math&gt;]] can be formulated as special cases of a more [[general correlation coefficient]].

==Definition and calculation==
The Spearman correlation coefficient is defined as the [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]] between the [[Ranking|ranked variables]].&lt;ref name="myers2003"&gt;{{Cite book | last1=Myers | first1=Jerome L. | first2=Arnold D.  |last2= Well | title=Research Design and Statistical Analysis | publisher=Lawrence Erlbaum | year=2003 | edition=2nd | isbn=0-8058-4037-0 | pages=508}}&lt;/ref&gt;

For a sample of size ''n'', the ''n'' [[raw score]]s &lt;math&gt;X_i, Y_i&lt;/math&gt; are converted to ranks &lt;math&gt;\operatorname{rg} X_i, \operatorname{rg} Y_i&lt;/math&gt;, and &lt;math&gt;r_s&lt;/math&gt; is computed from:

:&lt;math&gt;r_s = \rho_{\operatorname{rg}_X,\operatorname{rg}_Y} = \frac {\operatorname{cov}(\operatorname{rg}_X,\operatorname{rg}_Y)} { \sigma_{\operatorname{rg}_X} \sigma_{\operatorname{rg}_Y} }&lt;/math&gt;
:: where
::* &lt;math&gt;\rho&lt;/math&gt; denotes the usual [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]], but applied to the rank variables.
::* &lt;math&gt;\operatorname{cov}(\operatorname{rg}_X, \operatorname{rg}_Y)&lt;/math&gt; is the [[covariance]] of the rank variables.
::* &lt;math&gt;\sigma_{\operatorname{rg}_X}&lt;/math&gt; and &lt;math&gt;\sigma_{\operatorname{rg}_Y}&lt;/math&gt; are the [[standard deviation]]s of the rank variables.

Only if all ''n'' ranks are ''distinct integers'', it can be computed using the popular formula

:&lt;math&gt; r_s = {1- \frac {6 \sum d_i^2}{n(n^2 - 1)}}.&lt;/math&gt;
:: where
::* &lt;math&gt;d_i = \operatorname{rg}(X_i) - \operatorname{rg}(Y_i)&lt;/math&gt;, is the difference between the two ranks of each observation.
::* ''n'' is the number of observations

Identical values are usually{{citation needed|date=May 2016}} each assigned [[Ranking#Fractional ranking .28.221 2.5 2.5 4.22 ranking.29|fractional ranks]] equal to the average of their positions in the ascending order of the values, which is equivalent to averaging over all possible permutations.

If ties are present in the data set, this equation yields incorrect results: Only if in both variables all ranks are distinct, then &lt;math&gt;\sigma_{\operatorname{rg}_X} \sigma_{\operatorname{rg}_Y} = \operatorname{Var}{\operatorname{rg}_X} = \operatorname{Var}{\operatorname{rg}_Y} = n(n^2 - 1)/6&lt;/math&gt; (cf. [[tetrahedral number]] &lt;math&gt;T_{n-1}&lt;/math&gt;).
The first equation—normalizing by the standard deviation—may even be used even when ranks are normalized to [0;1] ("relative ranks") because it is insensitive both to translation and linear scaling.
&lt;!-- For example, if [1,2,3,4,5] vs. [1,3,3,3,5] has r_s=0.894, but the simplified formula yields 0.877. --&gt;

This method should also not be used in cases where the data set is truncated; that is, when the Spearman correlation coefficient is desired for the top X records (whether by pre-change rank or post-change rank, or both), the user should use the Pearson correlation coefficient formula given above.{{citation needed|date=September 2015}}

The standard error of the coefficient (''σ'') was determined by Pearson in 1907 and Gosset in 1920. It is

: &lt;math&gt; \sigma_{r_s} = \frac{ 0.6325 }{ \sqrt{n-1} } &lt;/math&gt;

==Related quantities==
{{Main article|Correlation and dependence}}

There are several other numerical measures that quantify the extent of [[statistical dependence]] between pairs of observations. The most common of these is the [[Pearson product-moment correlation coefficient]], which is a similar correlation method to Spearman's rank, that measures the “linear” relationships between the raw numbers rather than between their ranks.

An alternative name for the Spearman [[rank correlation]] is the “grade correlation”;&lt;ref name="Yule and Kendall"&gt;{{cite book |last=Yule |first=G.  U. |last2=Kendall |first2=M.  G. |orig-year=1950 |title=An Introduction to the Theory of Statistics |edition=14th |year=1968 |publisher=Charles Griffin &amp; Co. |page=268 }}&lt;/ref&gt; in this, the “rank” of an observation is replaced by the “grade”. In continuous distributions, the grade of an observation is, by convention, always one half less than the rank, and hence the grade and rank correlations are the same in this case. More generally, the “grade” of an observation is proportional to an estimate of the fraction of a population less than a given value, with the half-observation adjustment at observed values. Thus this corresponds to one possible treatment of tied ranks. While unusual, the term “grade correlation” is still in use.&lt;ref&gt;{{cite journal |last=Piantadosi |first=J. |last2=Howlett |first2=P. |last3=Boland |first3=J. |year=2007 |title=Matching the grade correlation coefficient using a copula with maximum disorder |journal=Journal of Industrial and Management Optimization |volume=3 |issue=2 |pages=305–312 |doi= |url=http://aimsciences.org/journals/pdfs.jsp?paperID=2265&amp;mode=abstract }}&lt;/ref&gt;

==Interpretation==
{| style="float: right;"
|+ '''Positive and negative Spearman rank correlations'''
|- 
| [[File:spearman fig5.svg|300px|left|thumb|A positive Spearman correlation coefficient corresponds to an increasing monotonic trend between ''X'' and ''Y''.]]
| [[File:spearman fig4.svg|300px|thumb|A negative Spearman correlation coefficient corresponds to a decreasing monotonic trend between ''X'' and ''Y''.]]
|}

The sign of the Spearman correlation indicates the direction of association between ''X'' (the independent variable) and ''Y'' (the dependent variable).  If  ''Y'' tends to increase when ''X'' increases, the Spearman correlation coefficient is positive.  If ''Y'' tends to decrease when ''X'' increases, the Spearman correlation coefficient is negative.  A Spearman correlation of zero indicates that there is no tendency for ''Y'' to either increase or decrease when ''X'' increases.  The Spearman correlation increases in magnitude as ''X'' and ''Y'' become closer to being perfect monotone functions of each other.  When ''X'' and ''Y'' are perfectly monotonically related, the Spearman correlation coefficient becomes 1.  A perfect monotone increasing relationship implies that for any two pairs of data values {{math|''X''&lt;sub&gt;''i''&lt;/sub&gt;, ''Y''&lt;sub&gt;''i''&lt;/sub&gt;}} and {{math|''X''&lt;sub&gt;''j''&lt;/sub&gt;, ''Y''&lt;sub&gt;''j''&lt;/sub&gt;}}, that {{math|''X''&lt;sub&gt;''i''&lt;/sub&gt; − ''X''&lt;sub&gt;''j''&lt;/sub&gt;}} and {{math|''Y''&lt;sub&gt;''i''&lt;/sub&gt; − ''Y''&lt;sub&gt;''j''&lt;/sub&gt;}} always have the same sign.  A perfect monotone decreasing relationship implies that these differences always have opposite signs.

The Spearman correlation coefficient is often described as being "nonparametric".  This can have two meanings:  First, a perfect Spearman correlation results when ''X'' and ''Y'' are related by any [[monotonic function]]. Contrast this with the Pearson correlation, which only gives a perfect value when ''X'' and ''Y'' are related by a ''linear'' function. The other sense in which the Spearman correlation is nonparametric in that its exact sampling distribution can be obtained without requiring knowledge (''i.e.'', knowing the parameters) of the joint [[probability distribution]] of ''X'' and ''Y''.

==Example==
In this example, the raw data in the table below is used to calculate the correlation between the [[IQ]] of a person with the number of hours spent in front of [[TV]] per week.
{| class="wikitable sortable" style="text-align:right;"
|-
![[IQ]], &lt;math&gt;X_i&lt;/math&gt;
!Hours of [[TV]] per week, &lt;math&gt;Y_i&lt;/math&gt;
|-
|106
|7
|-
|86
|0
|-
|100
|27
|-
|101
|50
|-
|99
|28
|-
|103
|29
|-12
|97
|20
|-
|113
|12
|-
|112
|6
|-
|110
|17
|}

Firstly, evaluate &lt;math&gt;d^2_i&lt;/math&gt;. To do so use the following steps, reflected in the table below.
# Sort the data by the first column (&lt;math&gt;X_i&lt;/math&gt;). Create a new column &lt;math&gt;x_i&lt;/math&gt; and assign it the ranked values 1,2,3,...''n''.
# Next, sort the data by the second column (&lt;math&gt;Y_i&lt;/math&gt;). Create a fourth column &lt;math&gt;y_i&lt;/math&gt; and similarly assign it the ranked values 1,2,3,...''n''.
# Create a fifth column &lt;math&gt;d_i&lt;/math&gt; to hold the differences between the two rank columns (&lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;y_i&lt;/math&gt;).
# Create one final column &lt;math&gt;d^2_i&lt;/math&gt; to hold the value of column &lt;math&gt;d_i&lt;/math&gt; squared.

{| class="wikitable sortable" style="text-align:right;"
|-
![[IQ]], &lt;math&gt;X_i&lt;/math&gt;
!Hours of [[TV]] per week, &lt;math&gt;Y_i&lt;/math&gt;
!rank &lt;math&gt;x_i&lt;/math&gt;
!rank &lt;math&gt;y_i&lt;/math&gt;
!&lt;math&gt;d_i&lt;/math&gt;
!&lt;math&gt;d^2_i&lt;/math&gt;
|-
|86
|0
|1
|1
|0
|0
|-
|97
|20
|2
|6
| −4
|16
|-
|99
|28
|3
|8
| −5
|25
|-
|100
|27
|4
|7
| −3
|9
|-
|101
|50
|5
|10
| −5
|25
|-
|103
|29
|6
|9
| −3
|9
|-
|106
|7
|7
|3
|4
|16
|-
|110
|17
|8
|5
|3
|9
|-
|112
|6
|9
|2
|7
|49
|-
|113
|12
|10
|4
|6
|36
|}

With &lt;math&gt;d^2_i&lt;/math&gt; found, add them to find &lt;math&gt;\sum d_i^2 = 194&lt;/math&gt;. The value of ''n'' is 10. These values can now be substituted back into the equation: &lt;math&gt; \rho = 1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}}.&lt;/math&gt; to give

:&lt;math&gt; \rho = 1- {\frac {6\times194}{10(10^2 - 1)}}&lt;/math&gt;

which evaluates to {{math|1=''ρ'' = -29/165 = −0.175757575...}}
with a [[P-value]] = 0.627188 (using the [[Student's t-distribution|t distribution]])

[[File:Spearman's Rank chart.png|thumb|Chart of the data presented. It can be seen that there might be a negative correlation, but that the relationship does not appear definitive.]]
This low value shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above).

==Determining significance==
One approach to test whether an observed value of ρ is significantly different from zero (''r'' will always maintain −1 ≤ ''r'' ≤ 1) is to calculate the probability that it would be greater than or equal to the observed ''r'', given the [[null hypothesis]], by using a [[Resampling (statistics)#Permutation tests|permutation test]]. An advantage of this approach is that it automatically takes into account the number of tied data values there are in the sample, and the way they are treated in computing the rank correlation.

Another approach parallels the use of the [[Fisher transformation]] in the case of the Pearson product-moment correlation coefficient. That is, [[confidence intervals]] and [[hypothesis test]]s relating to the population value ρ can be carried out using the Fisher transformation:

: &lt;math&gt;F(r) = {1 \over 2}\ln{1+r \over 1-r} = \operatorname{artanh}(r).&lt;/math&gt;

If ''F''(''r'') is the Fisher transformation of ''r'', the sample Spearman rank correlation coefficient, and ''n'' is the sample size, then

:&lt;math&gt;z = \sqrt{\frac{n-3}{1.06}}F(r)&lt;/math&gt;

is a [[standard score|z-score]] for ''r'' which approximately follows a standard [[normal distribution]] under the [[null hypothesis]] of [[statistical independence]] ({{math|1=''ρ'' = 0}}).&lt;ref&gt;{{cite journal |last=Choi |first=S. C. |year=1977 |title=Tests of Equality of Dependent Correlation Coefficients |journal=[[Biometrika]] |volume=64 |issue=3 |pages=645–647 |doi=10.1093/biomet/64.3.645 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |last=Fieller |first=E. C. |last2=Hartley |first2=H. O. |last3=Pearson |first3=E. S. |year=1957 |title=Tests for rank correlation coefficients. I |journal=Biometrika |volume=44 |issue= |pages=470–481 |doi=10.1093/biomet/44.3-4.470}}&lt;/ref&gt;

One can also test for significance using

:&lt;math&gt;t = r \sqrt{\frac{n-2}{1-r^2}}&lt;/math&gt;

which is distributed approximately as [[Student's t distribution]] with {{math|''n'' − 2}} degrees of freedom under the [[null hypothesis]].&lt;ref&gt;{{cite book |last=Press |last2=Vettering |last3=Teukolsky |last4=Flannery |year=1992 |title=Numerical Recipes in C: The Art of Scientific Computing |edition=2nd |page=640 }}&lt;/ref&gt; A justification for this result relies on a permutation argument.&lt;ref&gt;{{cite book |last=Kendall |first=M. G. |last2=Stuart |first2=A. |year=1973 |title=The Advanced Theory of Statistics, Volume 2: Inference and Relationship |publisher=Griffin |isbn=0-85264-215-6 }} (Sections 31.19, 31.21)&lt;/ref&gt;

pvrank&lt;ref&gt;{{cite web|last1=Amerise|first1=I.L.|last2=Marozzi|first2=M.|last3=Tarsitano|first3=A.|title=R package pvrank|url=https://cran.r-project.org/web/packages/pvrank/index.html}}&lt;/ref&gt; is a very recent [[R (programming language)|R]] package that computes rank correlations and their p-values with various options for tied ranks. It is possible to compute exact Spearman coefficient test p-values for ''n'' ≤ 26.

A generalization of the Spearman coefficient is useful in the situation where there are three or more conditions, a number of subjects are all observed in each of them, and it is predicted that the observations will have a particular order.  For example, a number of subjects might each be given three trials at the same task, and it is predicted that performance will improve from trial to trial.  A test of the significance of the trend between conditions in this situation was developed by E. B. Page&lt;ref&gt;{{cite journal |author=Page, E. B. |title=Ordered hypotheses for multiple treatments: A significance test for linear ranks |journal=Journal of the American Statistical Association |volume=58 |pages=216–230 |year=1963 |doi=10.2307/2282965 |issue=301}}
&lt;/ref&gt; and is usually referred to as [[Page's trend test]] for ordered alternatives.

==Correspondence analysis based on Spearman's rho==
Classic [[correspondence analysis]] is a statistical method that gives a score to every value of two nominal variables. In this way the Pearson [[Pearson product-moment correlation coefficient|correlation coefficient]] between them is maximized.

There exists an equivalent of this method, called [[grade correspondence analysis]], which maximizes Spearman's rho or [[Kendall's tau]].&lt;ref&gt;{{cite book|editor1-last=Kowalczyk|editor1-first=T.|editor2-last=Pleszczyńska|editor2-first=E.|editor3-last=Ruland|editor3-first=F.| year=2004|title=Grade Models and Methods for Data Analysis with Applications for the Analysis of Data Populations|series=Studies in Fuzziness and Soft Computing |volume=151|publisher=Springer Verlag|location=Berlin Heidelberg New York|isbn=978-3-540-21120-4}}&lt;/ref&gt;

==See also==
{{Portal|Statistics}}
* [[Kendall tau rank correlation coefficient]]
* [[Chebyshev's sum inequality]], [[rearrangement inequality]] (These two articles may shed light on the mathematical properties of Spearman's ρ.)
*[[Distance correlation]]

==References==
{{Reflist|30em}}

==Further reading==
* Corder, G.W. &amp; Foreman, D.I. (2014). Nonparametric Statistics: A Step-by-Step Approach, Wiley. ISBN 978-1118840313.
* {{cite book |last=Daniel |first=Wayne W. |chapter=Spearman rank correlation coefficient |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=0-534-91976-6 |pages=358–365 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&amp;pg=PA358 }}
* {{Cite journal |author=Spearman C |title=The proof and measurement of association between two things |journal=American Journal of Psychology |volume=15 |year=1904 |pages=72–101 |doi=10.2307/1412159}}
* {{Cite journal |author=Bonett DG, Wright, TA |title=Sample size requirements for Pearson, Kendall, and Spearman correlations |journal=Psychometrika |volume=65 |year=2000 |pages=23–28 |doi=10.1007/bf02294183}}
* {{Cite book |author=Kendall MG |title=Rank correlation methods |location=London |publisher=Griffin |year=1970 |edition=4th |isbn=978-0-852-6419-96 |oclc=136868}}
* {{Cite book |vauthors=Hollander M, Wolfe DA |title=Nonparametric statistical methods |location=New York |publisher=Wiley |year=1973 |isbn=978-0-471-40635-8 |oclc=520735}}
* {{Cite journal |vauthors=Caruso JC, Cliff N |title=Empirical size, coverage, and power of confidence intervals for Spearman's Rho |journal=Educational and Psychological Measurement |volume=57 |year=1997 |pages=637–654 |doi=10.1177/0013164497057004009}}

==External links==
{{Wikiversity}}
*[http://www.crystalballservices.com/Resources/ConsultantsCornerBlog/EntryId/73/Copulas-Vs-Correlation.aspx "Understanding Correlation vs. Copulas in Excel"] by Eric Torkia, Technology Partnerz 2011
*[http://www.sussex.ac.uk/Users/grahamh/RM1web/Rhotable.htm Table of critical values of ρ for significance with small samples]
*[http://www.maccery.com/maths Spearman's rank online calculator]
*[https://www.answerminer.com/calculators/correlation-test Spearman correlation calculator with human-readable explanation]
*[http://faculty.vassar.edu/lowry/webtext.html Chapter 3 part 1 shows the formula to be used when there are ties]
*[http://statistical-research.com/wp-content/uploads/2012/08/Spearman.pdf An example of how to calculate Spearman's Rho along with basic R code.]
* [https://www.rgs.org/NR/rdonlyres/4844E3AB-B36D-4B14-8A20-3A3C28FAC087/0/OASpearmansRankExcelGuidePDF.pdf Spearman’s Rank Correlation Coefficient – Excel Guide]: sample data and formulae for Excel, developed by the [[Royal Geographical Society]].
*[http://udel.edu/~mcdonald/statspearman.html Spearman's rank correlation]: Simple notes for students with an example of usage by biologists and a spreadsheet for [[Microsoft Excel]] for calculating it (a part of materials for a ''Research Methods in Biology'' course).
{{Statistics|descriptive}}

{{DEFAULTSORT:Spearman's Rank Correlation Coefficient}}
[[Category:Covariance and correlation]]
[[Category:Information retrieval evaluation]]
[[Category:Nonparametric statistics]]
[[Category:Statistical tests]]</text>
      <sha1>hnu81ng3vb2oe4vfiz61fqf93blv5mr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Search algorithms</title>
    <ns>14</ns>
    <id>1406201</id>
    <revision>
      <id>666714135</id>
      <parentid>666703092</parentid>
      <timestamp>2015-06-13T03:43:27Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="140" xml:space="preserve">{{Commons category|Search algorithms}}
{{Cat main|Search algorithms}}

[[Category:Algorithms]]
[[Category:Information retrieval techniques]]</text>
      <sha1>m2hbfmy4pnl1084029gbltgm055p9c2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Ranking functions</title>
    <ns>14</ns>
    <id>19988453</id>
    <revision>
      <id>666714674</id>
      <parentid>608644061</parentid>
      <timestamp>2015-06-13T03:51:25Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="67" xml:space="preserve">[[Category:Information retrieval techniques]]
[[Category:Rankings]]</text>
      <sha1>3xe4cn7gysqdyvegjdpo0ba1bqx758r</sha1>
    </revision>
  </page>
  <page>
    <title>Term Discrimination</title>
    <ns>0</ns>
    <id>15101979</id>
    <revision>
      <id>712583609</id>
      <parentid>666715963</parentid>
      <timestamp>2016-03-29T23:10:26Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2721" xml:space="preserve">'''Term Discrimination''' is a way to rank keywords in how useful they are for [[information retrieval]].

== Overview ==

This is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.

This method uses the concept of ''Vector Space Density'' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.

An optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.

The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density.

 Let:
 &lt;math&gt;A&lt;/math&gt; be the occurrence matrix
 &lt;math&gt;A_k&lt;/math&gt; be the occurrence matrix without the index term &lt;math&gt;k&lt;/math&gt;
 and &lt;math&gt;Q(A)&lt;/math&gt; be density of &lt;math&gt;A&lt;/math&gt;.
 Then:
 The discrimination value of the index term &lt;math&gt;k&lt;/math&gt; is: 
 &lt;math&gt;DV_k = Q(A) - Q(A_k)&lt;/math&gt;

== How to compute ==

Given an [[occurrency matrix]]: &lt;math&gt;A&lt;/math&gt; and one keyword: &lt;math&gt;k&lt;/math&gt;
* Find the global document [[centroid]]: &lt;math&gt;C&lt;/math&gt; (this is just the average document vector)
* Find the average [[euclidean distance]] from every document vector, &lt;math&gt;D_i&lt;/math&gt; to &lt;math&gt;C&lt;/math&gt;
* Find the average euclidean distance from every document vector, &lt;math&gt;D_i&lt;/math&gt; to &lt;math&gt;C&lt;/math&gt; ''IGNORING'' &lt;math&gt;k&lt;/math&gt;
* The difference between the two values in the above step is the ''discrimination value'' for keyword &lt;math&gt;K&lt;/math&gt;

A higher value is better because including the keyword will result in better information retrieval.

== Qualitative Observations ==
Keywords that are ''[[Sparse matrix|sparse]]'' should be poor discriminators because they have poor ''[[Precision and recall|recall]],''
whereas
keywords that are ''frequent'' should be poor discriminators because they have poor ''[[Precision and recall|precision]].''

== References ==
* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(The article in which the vector space model was first presented)''
* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." ''Journal of the American Society for Information Science'', vol. 38, nr. 3, pages 171-183.

[[Category:Information retrieval techniques]]</text>
      <sha1>65atdn29f7vbhj9xkredsfrg9bayu0l</sha1>
    </revision>
  </page>
  <page>
    <title>Category:String similarity measures</title>
    <ns>14</ns>
    <id>9833053</id>
    <revision>
      <id>666717206</id>
      <parentid>544714325</parentid>
      <timestamp>2015-06-13T04:29:13Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="220" xml:space="preserve">{{Cat main|String metrics}}

[[Category:Algorithms on strings|Similarity]]
[[Category:Information retrieval techniques]]
[[Category:Metric geometry]]
[[Category:Information theory]]
[[Category:String (computer science)]]</text>
      <sha1>b0zfcvc850esdc5ft6zvz838xqljlvr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Vector space model</title>
    <ns>14</ns>
    <id>36475839</id>
    <revision>
      <id>666717303</id>
      <parentid>503023195</parentid>
      <timestamp>2015-06-13T04:30:48Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="45" xml:space="preserve">[[Category:Information retrieval techniques]]</text>
      <sha1>d5cl9w748m6esl94ihxfydykzgembzh</sha1>
    </revision>
  </page>
  <page>
    <title>Extended Boolean model</title>
    <ns>0</ns>
    <id>25271852</id>
    <revision>
      <id>735217565</id>
      <parentid>735217500</parentid>
      <timestamp>2016-08-19T08:55:27Z</timestamp>
      <contributor>
        <ip>14.139.180.72</ip>
      </contributor>
      <comment>Undid revision 735217500 by [[Special:Contributions/14.139.180.72|14.139.180.72]] ([[User talk:14.139.180.72|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7211" xml:space="preserve">The '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.&lt;ref&gt;	
{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last1=Salton | first1=Gerard | first2=Edward A. | last2=Fox | first3= Harry | last3=Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}&lt;/ref&gt;

Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.

==Definitions==
In the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.

The weight of term {{math|''K&lt;sub&gt;x&lt;/sub&gt;''}} associated with document {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} is measured by its normalized [[Term frequency]] and can be defined as:

&lt;math&gt;
w_{x,j}=f_{x,j}*\frac{Idf_{x}}{max_{i}Idf_{i}}
&lt;/math&gt;

where {{math|''Idf&lt;sub&gt;x&lt;/sub&gt;''}} is [[inverse document frequency]].

The weight vector associated with document {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} can be represented as:

&lt;math&gt;\mathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \ldots, w_{i,j}]&lt;/math&gt;

==The 2 Dimensions Example==
{{multiple image
 | width     = 150
 | image1    = 2D_Extended_Boolean_model_OR_example.png
 | alt1      = Figure 1
 | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;or; ''K&lt;sub&gt;y&lt;/sub&gt;'')}} with documents {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} and {{math|''d''&lt;sub&gt;''j''+1&lt;/sub&gt;}}.
 | image2    = 2D_Extended_Boolean_model_AND_example.png
 | alt2      = Figure 2
 | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;and; ''K&lt;sub&gt;y&lt;/sub&gt;'')}} with documents {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} and {{math|''d''&lt;sub&gt;''j''+1&lt;/sub&gt;}}.
}}

Considering the space composed of two terms {{math|''K&lt;sub&gt;x&lt;/sub&gt;''}} and {{math|''K&lt;sub&gt;y&lt;/sub&gt;''}} only, the corresponding term weights are {{math|''w''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''w''&lt;sub&gt;2&lt;/sub&gt;}}.&lt;ref&gt;[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]&lt;/ref&gt;  Thus, for query {{math|''q&lt;sub&gt;or&lt;/sub&gt;'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;or; ''K&lt;sub&gt;y&lt;/sub&gt;'')}}, we can calculate the similarity with the following formula:
 
&lt;math&gt;sim(q_{or},d)=\sqrt{\frac{w_1^2+w_2^2}{2}}&lt;/math&gt;

For query {{math|''q&lt;sub&gt;and&lt;/sub&gt;'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;and; ''K&lt;sub&gt;y&lt;/sub&gt;'')}}, we can use:

&lt;math&gt;sim(q_{and},d)=1-\sqrt{\frac{(1-w_1)^2+(1-w_2)^2}{2}}&lt;/math&gt;

==Generalizing the idea and P-norms==
We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.

This can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &amp;le; ''p'' &amp;le; &amp;infin;}} is a new parameter.&lt;ref&gt;{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}&lt;/ref&gt;

*A generalized conjunctive query is given by:
:&lt;math&gt;q_{or}=k_1 \lor^p k_2 \lor^p .... \lor^p k_t  &lt;/math&gt;

*The similarity of &lt;math&gt;q_{or}&lt;/math&gt; and &lt;math&gt;d_j&lt;/math&gt; can be defined as:
''':&lt;math&gt;sim(q_{or},d_j)=\sqrt[p]{\frac{w_1^p+w_2^p+....+w_t^p}{t}}&lt;/math&gt;'''

*A generalized disjunctive query is given by:
:&lt;math&gt;q_{and}=k_1 \land^p k_2 \land^p .... \land^p k_t  &lt;/math&gt;

*The similarity of &lt;math&gt;q_{and}&lt;/math&gt; and &lt;math&gt;d_j&lt;/math&gt; can be defined as:
:&lt;math&gt;sim(q_{and},d_j)=1-\sqrt[p]{\frac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}&lt;/math&gt;

==Examples==
Consider the query {{math|''q'' {{=}} (''K''&lt;sub&gt;1&lt;/sub&gt; &amp;and; ''K''&lt;sub&gt;2&lt;/sub&gt;) &amp;or; ''K''&lt;sub&gt;3&lt;/sub&gt;}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:

&lt;math&gt;sim(q,d)=\sqrt[p]{\frac{(1-\sqrt[p]{(\frac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}&lt;/math&gt;

==Improvements over the Standard Boolean Model==

Lee and Fox&lt;ref&gt;{{citation | last1=Lee | first1=W. C. | first2=E. A. | last2=Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries | url = http://eprints.cs.vt.edu/archive/00000112/01/TR-88-27.pdf}}&lt;/ref&gt; compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.
Using P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.&lt;br&gt;
The P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.

==Further reading==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]
* [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6VC8-454T5MS-2&amp;_user=513551&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1117914301&amp;_rerunOrigin=google&amp;_acct=C000025338&amp;_version=1&amp;_urlVersion=0&amp;_userid=513551&amp;md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | first2=S. | last2=Betrabet | first3=M. | last3=Koushik | first4=W. | last4=Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}
* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first1= Lucie | last1= Skorkovská | first2=Pavel | last2=Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}

==See also==
*[[Information retrieval]]

==References==
{{reflist}}

{{DEFAULTSORT:Extended Boolean Model}}
[[Category:Information retrieval techniques]]</text>
      <sha1>dqzp4lhnpncigjmr8w05tm6yesy2tbw</sha1>
    </revision>
  </page>
  <page>
    <title>Learning to rank</title>
    <ns>0</ns>
    <id>25050663</id>
    <revision>
      <id>759280081</id>
      <parentid>759127247</parentid>
      <timestamp>2017-01-10T07:00:12Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Approaches */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27393" xml:space="preserve">{{machine learning bar}}
'''Learning to rank'''&lt;ref name="liu"&gt;{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|series=Foundations and Trends in Information Retrieval
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225–331
|journal=Foundations and Trends in Information Retrieval
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
&lt;/ref&gt; or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.&lt;/ref&gt; [[Training data]] consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], and [[online advertising]].

A possible architecture of a machine-learned search engine is shown in the figure to the right.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
&lt;!-- "assessor" is the more standard term, used e.g. by TREC conference --&gt;
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used — only the top few documents, retrieved by some existing ranking models are checked. &lt;!--
  TODO: write something about selection bias caused by pooling
--&gt; Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),&lt;ref name="Joachims2002"&gt;{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}&lt;/ref&gt; ''query chains'',&lt;ref&gt;{{citation
 |author1=Joachims T. |author2=Radlinski F. | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
}}&lt;/ref&gt; or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.&lt;ref&gt;{{citation
 |author1=B. Cambazoglu |author2=H. Zaragoza |author3=O. Chapelle |author4=J. Chen |author5=C. Liao |author6=Z. Zheng |author7=J. Degenhardt. | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. 
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}&lt;/ref&gt; First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,&lt;ref&gt;{{citation
 |author1=Broder A. |author2=Carmel D. |author3=Herscovici M. |author4=Soffer A. |author5=Zien J. | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the twelfth international conference on Information and knowledge management
 | year=2003
 | pages=426–434
 | isbn=1-58113-723-0
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 }}&lt;/ref&gt; and [[Okapi BM25|BM25]]. This phase is called ''top-&lt;math&gt;k&lt;/math&gt; document retrieval'' and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.&lt;ref name="manning-q-eval"&gt;{{citation
 |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]&lt;/ref&gt; In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;&lt;ref name="Duh09"&gt;{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}&lt;/ref&gt;
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.&lt;ref name="Duh09" /&gt;
* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.&lt;ref&gt;Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.&lt;/ref&gt;

== Feature vectors ==
For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such an approach is sometimes called ''bag of features'' and is analogous to the [[bag of words]] model and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features — those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.&lt;ref name="manning-q-eval" /&gt;&lt;ref&gt;
{{cite conference
 | first=M. |last=Richardson |author2=Prakash, A. |author3=Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707–715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}&lt;/ref&gt;
* ''Query-dependent'' or ''dynamic'' features — those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:&lt;ref name="letor3"&gt;[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]&lt;/ref&gt;
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
* [[Information retrieval#Mean average precision|Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]]
* [[Spearman's rank correlation coefficient|Spearman's Rho]]

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.&lt;ref&gt;http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt&lt;/ref&gt; Other metrics such as MAP, MRR and precision, are defined only for binary judgements.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* [[Expected reciprocal rank]] (ERR);&lt;ref&gt;{{citation
|author1=Olivier Chapelle |author2=Donald Metzler |author3=Ya Zhang |author4=Pierre Grinspan |title=Expected Reciprocal Rank for Graded Relevance
|url=https://web.archive.org/web/20120224053008/http://research.yahoo.com/files/err.pdf
|journal=CIKM
|year=2009
|pages=
}}&lt;/ref&gt;
* [[Yandex]]'s pfound.&lt;ref&gt;{{citation
|author1=Gulin A. |author2=Karpovich P. |author3=Raskovalov D. |author4=Segalovich I. |title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163–168
}} (in Russian)&lt;/ref&gt;
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] has analyzed existing algorithms for learning to rank problems in his paper "Learning to Rank for Information Retrieval".&lt;ref name="liu" /&gt; He categorized them into three groups by their input representation and [[loss function]]:

=== Pointwise approach ===
In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case learning-to-rank problem is approximated by a classification problem — learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class="wikitable sortable"
! Year || Name || Type || Notes
|-
| 1989 || OPRF &lt;ref name="Fuhr1989"&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183–204 
 | year=1989
 | doi=10.1145/65943.65944
}}&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR &lt;ref name="Cooperetal1992"&gt;{{citation
 |author1=Cooper, William S. |author2=Gey, Frederic C. |author3=Dabney, Daniel P. | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval 
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198–210 
 | year=1992
 | doi=10.1145/133160.133199
}}&lt;/ref&gt;   || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise || Staged logistic regression
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||  A more recent exposition is in,&lt;ref name="Joachims2002" /&gt; which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.20.378 | title = Pranking }}&lt;/ref&gt; || &lt;span style="display:none"&gt;1&lt;/span&gt; pointwise || Ordinal regression.
|-
| 2003 &lt;!-- or 1998? --&gt; || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || pairwise/listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || 
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || &lt;span style="display:none"&gt;1&lt;/span&gt; pointwise ||
|-
| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || RankGP&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.90.220 | title = RankGP }}&lt;/ref&gt; || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
Regularized least-squares based ranking. The work is extended in
&lt;ref name=pahikkala2009efficient&gt;{{Citation|last=Pahikkala|first=Tapio |author2=Tsivtsivadze, Evgeni |author3=Airola, Antti |author4=Järvinen, Jouni |author5=Boberg, Jorma |title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129–165|doi=10.1007/s10994-008-5097-z|postscript=.}}&lt;/ref&gt; to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM&lt;sup&gt;map&lt;/sup&gt;] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || pairwise/listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.&lt;ref&gt;C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].&lt;/ref&gt;
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]&lt;ref&gt;Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]&lt;ref&gt;Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.&lt;/ref&gt;  || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]&lt;ref&gt;Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || A method combines Plackett-Luce Model and neural network to minimize the expected Bayes risk, related to NDCG, from the decision-making aspect.
|-
| 2010 || [https://people.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]&lt;ref&gt;Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.&lt;/ref&gt; || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise &amp; listwise || 
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise &amp; pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;&lt;ref name="Fuhr1992"&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243–255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
}}&lt;/ref&gt; a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.&lt;ref name="Fuhr1989" /&gt; Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 &lt;ref name="Cooperetal1992" /&gt; and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.&lt;ref&gt;{{citation |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]&lt;/ref&gt;  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.&lt;ref&gt;Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]&lt;/ref&gt;&lt;ref&gt;{{US Patent|7197497}}&lt;/ref&gt;

[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,&lt;ref&gt;[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]&lt;/ref&gt;{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced&lt;ref name="snezhinsk"&gt;[http://webmaster.ya.ru/replies.xml?item_no=5707&amp;ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)&lt;/ref&gt; that it had significantly increased its [[search quality]] due to deployment of a new proprietary [[MatrixNet]] algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.&lt;ref&gt;The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].&lt;/ref&gt; Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"&lt;ref&gt;[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]&lt;/ref&gt; based on their own search engine's production data. Yahoo has announced a similar competition in 2010.&lt;ref&gt;[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]&lt;/ref&gt;

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.&lt;ref&gt;{{cite web
  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
  | archiveurl = http://www.webcitation.org/5sq8irWNM
  | archivedate = 2010-09-18
  | title = Are Machine-Learned Models Prone to Catastrophic Errors?
  | date = 2008-05-24
  | last = Rajaraman
  | first = Anand
  | authorlink = Anand Rajaraman}}&lt;/ref&gt; [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".&lt;ref&gt;{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = http://www.webcitation.org/5sq7DX3Pj
  | archivedate = 2010-09-15
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}&lt;/ref&gt;

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]
* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]
[[Category:Ranking functions]]</text>
      <sha1>7jrapcs7hclbs67527gdg2vs9bskkuu</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic technology</title>
    <ns>0</ns>
    <id>4416107</id>
    <revision>
      <id>738468053</id>
      <parentid>737660756</parentid>
      <timestamp>2016-09-09T03:50:19Z</timestamp>
      <contributor>
        <username>Lourdes</username>
        <id>26951022</id>
      </contributor>
      <comment>[[Wikipedia:Articles for deletion/Semantic technology]] closed as keep</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3746" xml:space="preserve">{{no footnotes|date=March 2013}}
[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]
In [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. 

This enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.

With traditional [[information technology]], on the other hand, meanings and relationships must be predefined and “hard wired” into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.

Off-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.

Semantic technologies are “meaning-centered.” They include tools for:

* autorecognition of topics and concepts, 
* information and meaning extraction, and
* categorization. 

Given a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.

Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.

== See also ==
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Metadata]]
* [[Ontology (computer science)]]
* [[Semantic web]]

==References==

* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004
* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 — Proc. of the 12th international conference on World Wide Web'', pp 700–709. [[ACM Press]], 2003.
* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.
* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.
* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.
* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, 
* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September
2004.
* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&amp;Hall/CRC, 2009, ISBN 978-1-4200-9050-5

== External links ==
* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]

[[Category:Information retrieval techniques]]
[[Category:Semantics]]</text>
      <sha1>4wsx74a60bs1nd7n5436x8ivbgkyxpn</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Music information retrieval</title>
    <ns>14</ns>
    <id>46973988</id>
    <revision>
      <id>712094053</id>
      <parentid>666858897</parentid>
      <timestamp>2016-03-26T22:34:42Z</timestamp>
      <contributor>
        <username>Clusternote</username>
        <id>11739815</id>
      </contributor>
      <comment>+{{Commons category|Music information retrieval}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="136" xml:space="preserve">{{Commons category|Music information retrieval}}
[[Category:Information retrieval genres]]
[[Category:Information retrieval techniques]]</text>
      <sha1>oqd3qqqncv3osv5bs9g64xg9ce5kk5g</sha1>
    </revision>
  </page>
  <page>
    <title>Binary Independence Model</title>
    <ns>0</ns>
    <id>25957127</id>
    <revision>
      <id>689664883</id>
      <parentid>679093728</parentid>
      <timestamp>2015-11-08T17:53:11Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>/* Further reading */Update deprecated cite parameter (coauthors) and genfixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5603" xml:space="preserve">{{context|date=June 2012}}
The '''Binary Independence Model''' (BIM)&lt;ref name="cyu76" /&gt;&lt;ref name="jones77"/&gt; is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.

==Definitions==
The Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.
The representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x&lt;sub&gt;1&lt;/sub&gt;, ..., x&lt;sub&gt;m&lt;/sub&gt;)'' where ''x&lt;sub&gt;t&lt;/sub&gt;=1'' if term ''t'' is present in the document ''d'' and ''x&lt;sub&gt;t&lt;/sub&gt;=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.
"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.

The probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:

&lt;math&gt;P(R|x,q) = \frac{P(x|R,q)*P(R|q)}{P(x|q)}&lt;/math&gt;

where ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.
The exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.

''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.
Since a document is either relevant or nonrelevant to a query we have that:

&lt;math&gt;P(R=1|x,q) + P(R=0|x,q) = 1&lt;/math&gt;

=== Query Terms Weighting ===
Given a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the
terms in the query such that the retrieval effectiveness will be high. Let &lt;math&gt;p_i&lt;/math&gt; and &lt;math&gt;q_i&lt;/math&gt; be the probability that a relevant document and an irrelevant document has the &lt;math&gt;i^{th}&lt;/math&gt; term respectively. Yu and [[Gerard Salton|Salton]],&lt;ref name="cyu76" /&gt; who first introduce BIM, propose that the weight of the &lt;math&gt;i^{th}&lt;/math&gt; term is an increasing function of &lt;math&gt;Y_i =  \frac{p_i *(1-q_i)}{(1-p_i)*q_i}&lt;/math&gt;. Thus, if &lt;math&gt;Y_i&lt;/math&gt; is higher than &lt;math&gt;Y_j&lt;/math&gt;, the weight
of term &lt;math&gt;i&lt;/math&gt; will be higher than that of term &lt;math&gt;j&lt;/math&gt;. Yu and Salton&lt;ref name="cyu76" /&gt; showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Spärck Jones|Spärck Jones]]&lt;ref name="jones77"/&gt; later showed that if the &lt;math&gt;i^{th}&lt;/math&gt; term is assigned the weight of &lt;math&gt;log Y_i&lt;/math&gt;, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.

The Binary Independence Model was introduced by Yu and Salton.&lt;ref name="cyu76" /&gt; The name Binary Independence Model was coined by Robertson and Spärck Jones.&lt;ref name="jones77"/&gt;

== See also ==

* [[Bag of words model]]

==Further reading==
* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | author2=Prabhakar Raghavan|author3=Hinrich Schütze | publisher=Cambridge University Press | year=2008}}
* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&amp;uuml;ttcher | author2=Charles L. A. Clarke |author3= Gordon V. Cormack | publisher=MIT Press | year=2010}}

==References==
{{Reflist|refs=
&lt;ref name="cyu76"&gt;{{Cite journal | doi = 10.1145/321921.321930| title = Precision Weighting – An Effective Automatic Indexing Method| journal = Journal of the ACM| volume = 23| pages = 76| year = 1976| last1 = Yu | first1 = C. T.| last2 = Salton | first2 = G. | authorlink2 = Gerard Salton}}&lt;/ref&gt;
&lt;ref name="jones77"&gt;{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}&lt;/ref&gt; 
}}

[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]</text>
      <sha1>tqoxpd505n5o03bamz7p9l1r0mgp0hf</sha1>
    </revision>
  </page>
  <page>
    <title>Contextual searching</title>
    <ns>0</ns>
    <id>44571310</id>
    <revision>
      <id>760143005</id>
      <parentid>749432317</parentid>
      <timestamp>2017-01-15T06:16:44Z</timestamp>
      <contributor>
        <ip>74.177.99.126</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8655" xml:space="preserve">{{Orphan|date=May 2015}}

'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.&lt;ref&gt;{{cite journal | first = Susan E. | last = Feldman | title = The Answer Machine | journal = Synthesis Lectures on Information Concepts, Retrieval, and Services | doi = 10.2200/S00442ED1V01Y201208ICR023 }}&lt;/ref&gt; Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.&lt;ref&gt;{{cite journal | last1 = Pitokow | first1 = James | first2 = Hinrich | last2 = Schütze | first3 = Todd | last3 = Cass | first4 = Rob | last4 = Cooley | first5 = Don | last5 = Turnbull | first6 = Andy | last6 = Edmonds | first7 = Eytan | last7 = Adar | first8 = Thomas | last8 = Breuel | date = 2002 | title = Personalized search | url = http://www.cond.org/p50-pitkow.pdf | journal = Communications of the ACM (CACM) | volume = 45 | issue = 9 | pages = 50–55 }}&lt;/ref&gt;

== Basic Contextual Search ==
The basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are not rated higher. Users have limited control over the context of their query based on the words they use to search with.&lt;ref&gt;Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.&lt;/ref&gt;  For example, users looking for the menu portion of a website can add “menu” to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.

== Explicitly Supplied Context ==
Certain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.&lt;ref&gt;Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.&lt;/ref&gt; For example, a user looking for research papers can specify documents with “references” or “abstracts” to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.&lt;ref&gt;Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]&lt;/ref&gt;

Explicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.&lt;ref&gt;[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results&lt;/ref&gt; Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.&lt;ref&gt;[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators&lt;/ref&gt; Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.&lt;ref&gt;[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks&lt;/ref&gt;

== Automatically Inferred Context ==
There are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.&lt;ref&gt;[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM&lt;/ref&gt; Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.

Major search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example, if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.&lt;ref&gt;{{cite journal | first1 = Eric J. | last1 = Glover | first2 = Steve | last2 = Lawrence | first3 = Michael D. | last3 = Gordon | first4 = William P. | last4 = Birmingham | first5 = C. Lee | last5 = Giles | title = Web Search - Your Way | publisher = NEC Research Institution | citeseerx = 10.1.1.41.7499 }}&lt;/ref&gt; Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&amp;go=Submit&amp;qs=n&amp;form=GEOMA1&amp;pq=pizza&amp;sc=8-1&amp;sp=-1&amp;sk=&amp;cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.

=== Contextual Mobile Search ===
The drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".&lt;ref&gt;[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop&lt;/ref&gt; Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.&lt;ref&gt;[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices&lt;/ref&gt;

== References ==
{{reflist}}

{{Internet search}}

{{DEFAULTSORT:Contextual Searching}}
[[Category:Internet search engines]]
[[Category:Semantic Web]]
[[Category:Information retrieval techniques]]
[[Category:Internet terminology]]</text>
      <sha1>2peqn69i802rwv7kafcb0uwnwhnrtb3</sha1>
    </revision>
  </page>
  <page>
    <title>Fuzzy retrieval</title>
    <ns>0</ns>
    <id>25935906</id>
    <revision>
      <id>745299586</id>
      <parentid>745299532</parentid>
      <timestamp>2016-10-20T10:15:29Z</timestamp>
      <contributor>
        <username>564dude</username>
        <id>10738273</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8396" xml:space="preserve">'''Fuzzy retrieval''' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.

==Mixed Min and Max model (MMM)==

In fuzzy-set theory, an element has a varying degree of membership, say ''d&lt;sub&gt;A&lt;/sub&gt;'', to a given set ''A'' instead of the traditional membership choice (is an element/is not an element).&lt;br /&gt;
In MMM&lt;ref&gt;{{citation | last1=Fox | first1=E. A. | author2=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}&lt;/ref&gt; each index term has a fuzzy set associated with it. A document's weight with respect to an index term ''A'' is considered to be the degree of membership of the document in the fuzzy set associated with ''A''. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:&lt;br/&gt;
:&lt;math&gt;d_{A\cap B}= min(d_A, d_B)&lt;/math&gt;
:&lt;math&gt;d_{A\cup B}= max(d_A,d_B)&lt;/math&gt;

According to this, documents that should be retrieved for a query of the form ''A or B'', should be in the fuzzy set associated with the union of the two sets ''A'' and ''B''. Similarly, the documents that should be retrieved for a query of the form ''A and B'', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the ''or'' query to be ''max(d&lt;sub&gt;A&lt;/sub&gt;, d&lt;sub&gt;B&lt;/sub&gt;)'' and the similarity of the document to the ''and'' query to be ''min(d&lt;sub&gt;A&lt;/sub&gt;, d&lt;sub&gt;B&lt;/sub&gt;)''. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the ''min'' and ''max'' document weights.

Given a document ''D'' with index-term weights ''d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;'' for terms ''A&lt;sub&gt;1&lt;/sub&gt;, A&lt;sub&gt;2&lt;/sub&gt;, ..., A&lt;sub&gt;n&lt;/sub&gt;'', and the queries:

''Q&lt;sub&gt;or&lt;/sub&gt; = (A&lt;sub&gt;1&lt;/sub&gt; or A&lt;sub&gt;2&lt;/sub&gt; or ... or A&lt;sub&gt;n&lt;/sub&gt;)''&lt;br /&gt;
''Q&lt;sub&gt;and&lt;/sub&gt; = (A&lt;sub&gt;1&lt;/sub&gt; and A&lt;sub&gt;2&lt;/sub&gt; and ... and A&lt;sub&gt;n&lt;/sub&gt;)''

the query-document similarity in the MMM model is computed as follows:

''SlM(Q&lt;sub&gt;or&lt;/sub&gt;, D) = C&lt;sub&gt;or1&lt;/sub&gt; * max(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;) + C&lt;sub&gt;or2&lt;/sub&gt; * min(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;)''&lt;br /&gt;
''SlM(Q&lt;sub&gt;and&lt;/sub&gt;, D) = C&lt;sub&gt;and1&lt;/sub&gt; * min(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;) + C&lt;sub&gt;and2&lt;/sub&gt; * max(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt; ..., d&lt;sub&gt;An&lt;/sub&gt;)''

where ''C&lt;sub&gt;or1&lt;/sub&gt;, C&lt;sub&gt;or2&lt;/sub&gt;'' are "softness" coefficients for the ''or'' operator, and ''C&lt;sub&gt;and1&lt;/sub&gt;, C&lt;sub&gt;and2&lt;/sub&gt;'' are softness coefficients for the ''and'' operator. Since we would like to give the maximum of the document weights more importance while considering an ''or'' query and the minimum more importance while considering an ''and'' query, generally we have ''C&lt;sub&gt;or1&lt;/sub&gt; &gt; C&lt;sub&gt;or2&lt;/sub&gt; and C&lt;sub&gt;and1&lt;/sub&gt; &gt; C&lt;sub&gt;and2&lt;/sub&gt;''. For simplicity it is generally assumed that ''C&lt;sub&gt;or1&lt;/sub&gt; = 1 - C&lt;sub&gt;or2&lt;/sub&gt;'' and ''C&lt;sub&gt;and1&lt;/sub&gt; = 1 - C&lt;sub&gt;and2&lt;/sub&gt;''.

Lee and Fox&lt;ref name="leefox"&gt;{{citation | last1=Lee | first1=W. C. | author2=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}&lt;/ref&gt; experiments indicate that the best performance usually occurs with ''C&lt;sub&gt;and1&lt;/sub&gt;'' in the range [0.5, 0.8] and with ''C&lt;sub&gt;or1&lt;/sub&gt;'' &gt; 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].

==Paice model==

The Paice model&lt;ref&gt;{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}&lt;/ref&gt; is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:

:&lt;math&gt;S(D,Q) = \sum_{i=1}^n\frac{r^{i-1}*w_{di}}{\sum_{j=1}^n r^{j-1}}&lt;/math&gt;

where ''r'' is a constant coefficient and ''w&lt;sub&gt;di&lt;/sub&gt;'' is arranged in ascending order for ''and'' queries and descending order for ''or'' queries. When n = 2 the Paice model shows the same behavior as the MMM model.

The experiments of Lee and Fox&lt;ref name="leefox"/&gt; have shown that setting the ''r'' to 1.0 for ''and'' queries and 0.7 for ''or'' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of ''min'' or ''max'' of a set of term weights each time an ''and'' or ''or'' clause is considered, which can be done in ''O(n)''. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an ''and'' clause or an ''or'' clause is being considered. This requires at least an ''0(n log n)'' sorting algorithm. A good deal of floating point calculation is needed too.

==Improvements over the Standard Boolean model==
Lee and Fox&lt;ref name="leefox"/&gt; compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:
{| class="wikitable"
|-
!
! CISI
! CACM
! INSPEC
|-
! MMM
| 68%
| 109%
| 195%
|-
! Paice
| 77%
| 104%
| 206%
|}

These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.

==Recent work==

Recently '''Kang ''et al.'''.&lt;ref&gt;{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last1=Kang | first1=Bo-Yeong | author2=Dae-Won Kim |author3=Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}&lt;/ref&gt; have devised a fuzzy retrieval system indexed by concept identification.

If we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.&lt;br /&gt;
They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.

Zadrozny&lt;ref&gt;{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first1=Sławomir | last1=Zadrozny | last2=Nowacka | first2=Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}&lt;/ref&gt; revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:
* assuming linguistic terms as importance weights of keywords also in documents
* taking into account the uncertainty concerning the representation of documents and queries
* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh’s fuzzy logic (calculus of linguistic statements)
* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries

The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.

==See also==
*[[Information retrieval]]

==Further reading==
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | author2=S. Betrabet | author3=M. Koushik | author4=W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}

==References==
{{reflist}}

{{DEFAULTSORT:Fuzzy Retrieval}}
[[Category:Information retrieval techniques]]</text>
      <sha1>p1d5venl6w978oshhxl2qtoplhul3ov</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted search</title>
    <ns>0</ns>
    <id>10715937</id>
    <revision>
      <id>762163166</id>
      <parentid>761809870</parentid>
      <timestamp>2017-01-27T01:39:57Z</timestamp>
      <contributor>
        <ip>47.208.26.15</ip>
      </contributor>
      <comment>removed irrelevant self-promotion</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5439" xml:space="preserve">'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.&lt;ref name="Faceted Search"&gt;[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan &amp; Claypool, 2009&lt;/ref&gt;

Facets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.

Within the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].&lt;ref name="sigir06"&gt;[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]&lt;/ref&gt;

==Development==

The [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:
&lt;blockquote&gt;
The web search world, since its very beginning, has offered two paradigms:
*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.
*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. 
Over the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].&lt;ref name="sigir06"&gt;[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]&lt;/ref&gt;
&lt;/blockquote&gt;

==Mass market use==

Faceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] provide software for implementing faceted search applications.

Online retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. &lt;ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)"&gt;[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.&lt;/ref&gt; Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.

==Libraries and information science==

In 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.&lt;ref name="Major classification systems : the Dewey Centennial"&gt;[https://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]&lt;/ref&gt;

Modern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system. The [[CiteSeerX]] project&lt;ref&gt;[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.&lt;/ref&gt; at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.

==See also==
* [[Enterprise search]]
* [[Exploratory search]]
* [[Faceted classification]]
* [[Human–computer information retrieval]]
* [[Information extraction]]
* [[NoSQL]]

==References==
&lt;References/&gt;

{{DEFAULTSORT:Faceted Search}}
[[Category:Information retrieval techniques]]</text>
      <sha1>679qro0109cm5m20aloozugpv228i91</sha1>
    </revision>
  </page>
  <page>
    <title>Negative search</title>
    <ns>0</ns>
    <id>21692300</id>
    <revision>
      <id>666859965</id>
      <parentid>641411527</parentid>
      <timestamp>2015-06-14T05:41:22Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3471" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=March 2009}}
{{orphan|date=February 2009}}
{{confusing|date=March 2009}}
}}

'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.

Negative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.

Negative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.

Negative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.

Examples of Negative Intent are:

- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.

- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.

- An investigator is looking for a car but has no other information on that car on which to base a search.

==Negative Search Classifiers==

If there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.

[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.

==Irrelevancy as a Desirable Construct==

Positive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.

It follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.

==Degrees of Passivity==

Positive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\'s Eve|New Years Eve]]."

Discovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know"

Negative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don't care where as long as its good."

Searchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.

==References==
{{Reflist}}

[[Category:Information retrieval techniques]]</text>
      <sha1>i34d66mir39pqkx3o7zj1fnd84zn80q</sha1>
    </revision>
  </page>
  <page>
    <title>Anchor text</title>
    <ns>0</ns>
    <id>1225632</id>
    <revision>
      <id>754471045</id>
      <parentid>751323820</parentid>
      <timestamp>2016-12-12T21:07:08Z</timestamp>
      <contributor>
        <username>Andy028</username>
        <id>27064433</id>
      </contributor>
      <comment>Added a wiki link to Google Penguin update and a case study by Moz.com</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7577" xml:space="preserve">{{Use dmy dates|date=February 2013}}
The '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, web search engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],&lt;ref&gt;{{cite web|author1=Bader Aljaber |author2=Nicola Stokes |author3=James Bailey |author4=Jian Pei |url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}&lt;/ref&gt; and anchor text from documents linked in [[mind maps]] may be used too.&lt;ref&gt;Needs new reference link&lt;/ref&gt; [[File:Anchor text.png|thumb|Visual implementation of anchor text]]

==Overview==
Anchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:

:&lt;code&gt;&lt;nowiki&gt;&lt;a href="http://en.wikipedia.org/wiki/Main_Page"&gt;Wikipedia&lt;/a&gt;&lt;/nowiki&gt;&lt;/code&gt;

The anchor text in this example is "Wikipedia"; the longer, but vital, URL &lt;code&gt;&lt;nowiki&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/nowiki&gt;&lt;/code&gt; needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.

==Common misunderstanding of the concept==

This proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds significant [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}

==Search engine algorithms==
Anchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.&lt;ref name="Search Engine Watch 1"&gt;{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=
How the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}&lt;/ref&gt;

[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.&lt;ref&gt;{{cite web
|last=Fox
|first=Vanessa
|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html
|title=Get a more complete picture about how other sites link to you
|date=15 March 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= https://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
In the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.&lt;ref&gt;{{cite web
|last=Cutts
|first=Matt
|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html
|title=A quick word about Googlebombs
|date=25 January 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= https://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

In April 2012, Google announced in its March "[[Google Penguin|Penguin]]" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.&lt;ref&gt;{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}&lt;/ref&gt; Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.
.&lt;ref name="Search Engine Watch 2"&gt;{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=
Google Penguin Update: Impact of Anchor Text Diversity &amp; Link Relevancy|accessdate=6 July 2012}}&lt;/ref&gt;

However a 2016 study of anchor text influence across 16,000 keywords found that presence of exact and partial match anchor links continues to have a strong correlation with Google rankings.&lt;ref&gt;{{cite web|publisher=Ahrefs|url=https://ahrefs.com/blog/anchor-text|title=
Everything You Ever Wanted To Know About Anchor Text|accessdate=27 July 2016}}&lt;/ref&gt;

August 2016 study conducted by Moz, found that Exact and partial match domains can be affected by over optimization penalty since Google considers domain Brand and naked URL links as Exact match.&lt;ref&gt;{{Cite news|url=https://moz.com/ugc/case-study-the-interconnectedness-of-local-seo-and-exact-match-domains|title=Case Study: The Interconnectedness of Local SEO and Exact Match Domains|newspaper=Moz|access-date=2016-12-12}}&lt;/ref&gt;

==Terminology==
There are different classifications of anchor text that are used within the search engine optimization community such as the following:

;Exact Match: an anchor that is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it's linking to a page about "search engine optimization.
;Branded: a brand that is used as the anchor. "[[Wikipedia]]" is a branded anchor text.
;Naked Link: a URL that is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.
;Generic: a generic word or phrase that is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.
;Images: whenever an image is linked, Google will use the "ALT" tag as the anchor text.

==References==

{{reflist|colwidth=30em}}

[[Category:Information retrieval techniques]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]
[[Category:Search engine optimization]]
[[Category:Hypertext]]</text>
      <sha1>iwzmwcuze067dqbedqj64brbydkmzf7</sha1>
    </revision>
  </page>
  <page>
    <title>Webometrics</title>
    <ns>0</ns>
    <id>703145</id>
    <revision>
      <id>724066476</id>
      <parentid>715051625</parentid>
      <timestamp>2016-06-06T23:01:18Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* Bibliography */refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4691" xml:space="preserve">{{For|Webometrics Ranking of World Universities|Webometrics Ranking of World Universities}}
{{refimprove|date=May 2014}}
The science of '''webometrics''' (also '''cybermetrics''') tries to measure the [[World Wide Web]] to get knowledge about the number and types of [[hyperlink]]s, structure of the World Wide Web and usage patterns. According to Björneborn and Ingwersen (2004), the definition of '''webometrics''' is "the study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [[Bibliometrics|bibliometric]] and [[informetrics|informetric]] approaches." The term ''webometrics'' was first coined by Almind and Ingwersen (1997). A second definition of webometrics has also been introduced, "the study of web-based content with primarily quantitative methods for social science research goals using techniques that are not specific to one field of study" (Thelwall, 2009), which emphasizes the development of applied methods for use in the wider social sciences. The purpose of this alternative definition was to help publicize appropriate methods outside of the information science discipline rather than to replace the original definition within information science.

Similar scientific fields are [[Bibliometrics]], [[Informetrics]], [[Scientometrics]], [[Virtual ethnography]], and [[Web mining]].
[[File:Site based graph relationship.jpg|thumb|Site based graph relationship. The idea was taken from paper "Web-communicator creation costs sharing problem as a cooperative game"{{sfn|Mazalov|Pechnikov|Chirkov|Chuyko|2010|p=189}}]]

One relatively straightforward measure is the "Web Impact Factor" (WIF) introduced by Ingwersen (1998). The WIF measure may be defined as the number of web pages in a web site receiving links from other web sites, divided by the number of web pages published in the site that are accessible to the crawler. However the use of WIF has been disregarded due to the mathematical artifacts derived from power law distributions of these variables. Other similar indicators using size of the institution instead of number of webpages have been proved more useful.

== See also ==
* [[Altmetrics]]
* [[Impact factor]]
* [[PageRank]]
* [[Network mapping]]
* [[Search engine]]
* [[Webometrics Ranking of World Universities]]

== References ==
&lt;references /&gt;

== Bibliography ==

* {{cite journal |author1=Tomas C. Almind  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 1997 | title = Informetric analyses on the World Wide Web: Methodological approaches to 'webometrics' | journal = Journal of Documentation | volume = 53 | issue = 4 | pages = 404–426 | doi = 10.1108/EUM0000000007205}}
* {{cite journal |author1=Lennart Björneborn  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 2004 | title = Toward a basic framework for webometrics | journal = Journal of the American Society for Information Science and Technology | volume = 55 | issue = 14 | pages = 1216–1227 | url = http://www3.interscience.wiley.com/cgi-bin/abstract/109594194/ABSTRACT | doi = 10.1002/asi.20077}}
*{{cite journal |author = Peter Ingwersen | year = 1998 |title = The calculation of web impact factors | journal = Journal of Documentation |volume = 54 |issue = 2 | pages = 236–243 |doi = 10.1108/EUM0000000007167}}
*{{cite journal |author1=Mike Thelwall |author2=Liwen Vaughan |author3=Lennart Björneborn | year = 2005 |title = Webometrics | journal = Annual Review of Information Science and Technology |volume = 39 | pages = 81–135 |doi = 10.1002/aris.1440390110}}
* {{cite book |author=Mike Thelwall |title= Introduction to Webometrics: Quantitative Web Research for the Social Sciences |publisher= Morgan &amp; Claypool |year= 2009 |isbn= 978-1-59829-993-9 |url=http://www.morganclaypool.com/doi/abs/10.2200/S00176ED1V01Y200903ICR004}}
* {{cite conference 
|url            = http://www.mtas.ru/upload/library/UBS30112.pdf
|title          = Web-communicator creation costs sharing problem as a cooperative game (in Russian)
|last1=Mazalov |first1= Vladimir
|last2=Pechnikov |first2=Andrey
|last3=Chirkov |first3=Alexandr
|last4=Chuyko |first4=Julia
|year           = 2010
|booktitle      = Управление большими системами: сборник трудов
|pages          = 
|location       = 
|ref = harv
}}

* {{cite web
 |url        = http://eprints.rclis.org/7554/
 |title      = Webometrics: ten years of expansion
 |last       = Ingwersen
 |first      = Peter
 |year       = 2006
 |accessdate = 2013
 |ref = harv
}}

[[Category:World Wide Web]]
[[Category:Information science]]
[[Category:Information retrieval techniques]]


{{web-stub}}

[[pt:Webometria]]</text>
      <sha1>gi450mzk7ntg00b01u9r56nsoy8b2xy</sha1>
    </revision>
  </page>
  <page>
    <title>Music alignment</title>
    <ns>0</ns>
    <id>49926925</id>
    <revision>
      <id>721229225</id>
      <parentid>715647044</parentid>
      <timestamp>2016-05-20T13:31:04Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes / WPC #106 list using [[Project:AWB|AWB]] (12016)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8943" xml:space="preserve">[[Image:MusicAlignment_BeethovenFifth.png|thumb|300px|right|First theme of Symphony No. 5 by Ludwig van Beethoven in a sheet music, audio,
and piano-roll representation. The red bidirectional arrows indicate the aligned time positions of corresponding note events in the different representations.]]

[[Music]] can be described and represented in many different ways including [[sheet music]], symbolic representations, and audio recordings. For each of these representations, there may exist different versions that correspond to the same musical work. The general goal of '''music alignment''' (sometimes also referred to as '''music synchronization''') is to automatically link the various data streams, thus interrelating the multiple information sets related to a given musical work. More precisely, music alignment is taken to [[mean]] a procedure which, for a given position in one representation of a piece of music, determines the corresponding position within another representation.&lt;ref name=Mueller15_Chapter3FMP_SPRINGER&gt;
{{cite book
| last = Müller
| first = Meinard
| title = Music Synchronization. In Fundamentals of Music Processing, chapter 3, pages 115-166
| url = http://www.music-processing.de
| publisher = Springer
| year = 2015
| doi = 10.1007/978-3-319-21945-5
| isbn = 978-3-319-21944-8 }}
&lt;/ref&gt; In the figure on the right, such an alignment is visualized by the red bidirectional arrows. Such [[synchronization]] results form the basis for novel interfaces that allow users to access, search, and browse musical content in a convenient way.&lt;ref name=DammFTCKM12_DML_IJDL&gt;
{{cite journal
|last1=Damm
|first1=David 
|last2=Fremerey
|first2=Christian
|last3=Thomas
|first3=Verena
|last4=Clausen
|first4=Michael
|last5=Kurth
|first5=Frank
|last6=Müller
|first6=Meinard
|title=A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction
|url = http://link.springer.com/article/10.1007%2Fs00799-012-0087-y 
|journal=International Journal on Digital Libraries: Special Issue on Music Digital Libraries
|volume=12
|issue=2-3
|year=2012
|pages=53–71
|doi=10.1007/s00799-012-0087-y}}
&lt;/ref&gt;&lt;ref name=MuellerCKEF10_Sync_ISR&gt;
{{cite journal
|last1=Müller
|first1=Meinard 
|last2=Clausen
|first2=Michael
|last3=Konz
|first3=Verena
|last4=Ewert
|first4=Sebastian
|last5=Fremerey
|first5=Christian 
|title=A Multimodal Way of Experiencing and Exploring Music
|url = https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/03-publications/2010_MuellerClausenKonzEwertFremerey_MusicSynchronization_ISR.pdf 
|journal=Interdisciplinary Science Reviews (ISR)
|volume=35
|issue=2
|year=2010
|pages=138–153
|doi=10.1179/030801810X12723585301110}}&lt;/ref&gt;

==Basic procedure==
[[File:MusicAlignment Procedure.png|thumb|300px|right|Overview of the processing pipeline of a typical music alignment procedure.]]

Given two different music representations, typical music alignment approaches proceed in two steps.&lt;ref name=Mueller15_Chapter3FMP_SPRINGER/&gt; In the first step, the two representations are transformed into sequences of suitable features. In general, such feature representations need to find a compromise between two conflicting goals. On the one hand, features should show a large degree of [[robustness]] to variations that are to be left unconsidered for the task at hand. On the other hand, features should capture enough characteristic information to accomplish the given task. For music alignment, one often uses '''[[chroma feature|chroma-based features]]''' (also called [[chromagram]]s or [[harmonic pitch class profiles|pitch class profiles]]), which capture harmonic and melodic characteristics of music, while being robust to changes in timbre and instrumentation, are being used.

In the second step, the derived feature sequences have to be brought into (temporal) correspondence. To this end, techniques related to '''[[dynamic time warping|dynamic time warping (DTW)]]''' or '''[[hidden Markov model|hidden Markov models (HMMs)]]''' are used to compute an optimal alignment between two given feature sequences.

==Related tasks==
Music alignment and related synchronization tasks have been studied extensively within the field of [[music information retrieval]]. In the following, we give some pointers to related tasks. Depending upon the respective types of music representations, one can distinguish between various synchronization scenarios. For example, audio alignment refers to the task of temporally aligning two different audio recordings of a piece of music. Similarly, the goal of score–audio alignment is to coordinate note events given in the score representation with audio data. In the  [[offline]] scenario, the two data streams to be aligned are known prior to the actual alignment. In this case, one can use global optimization procedures such as [[dynamic time warping|dynamic time warping (DTW)]] to find an optimal alignment. In general, it is harder to deal with scenarios where the data streams are to be processed online. One prominent online scenario is known as '''[[score following]]''', where a musician is performing a piece according to a given musical score. The goal is then to identify the currently played musical events depicted in the score with high accuracy and low latency.&lt;ref&gt;
{{cite journal
|last1=Cont
|first1=Arshia
|title=A Coupled Duration-Focused Architecture for Real-Time Music-to-Score Alignment
|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence
|volume=32
|issue=6
|year=2010
|pages=974–987
|issn=0162-8828
|doi=10.1109/TPAMI.2009.106}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
|last1=Orio
|first1=Nicola
|last2=Lemouton
|first2=Serge
|last3=Schwarz
|first3=Diemo
|title=Score following: State of the art and new developments
|url = http://recherche.ircam.fr/equipes/temps-reel/suivi/resources/orio.2002.nime.pdf
|journal=Proceedings of the International Conference on New Interfaces for Musical Expression (NIME)
|date=2003
|pages=36–41}}&lt;/ref&gt; In this scenario, the score is known as a whole in advance, but the performance is known only up to the current point in time. In this context, alignment techniques such as hidden Markov models or particle filters have been employed, where the current score position and tempo are modeled in a statistical sense.&lt;ref&gt;
{{cite journal
|last1=Duan
|first1=Zhiyao
|last2=Pardo
|first2=Bryan
|journal = Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
|title=A state space model for online polyphonic audio-score alignment
|url = http://www.ece.rochester.edu/~zduan/resource/DuanPardo_ScoreFollowing_ICASSP11.pdf
|year=2011
|pages=197–200
|doi=10.1109/ICASSP.2011.5946374}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
|last1=Montecchio
|first1=Nicola
|last2=Cont
|first2=Arshia
|title=A unified approach to real time audio-to-score and audio-to-audio alignment using sequential Montecarlo inference techniques
|url = http://articles.ircam.fr/textes/Montecchio11a/index.pdf
|year=2011
|pages=193–196
|doi=10.1109/ICASSP.2011.5946373}}&lt;/ref&gt; As opposed to classical DTW, such an online synchronization procedure inherently has a running time that is linear in the duration of the performed version. However, as a main disadvantage, an online strategy is very sensitive to local tempo variations and deviations from the score - once the procedure is out of sync, it is very hard to recover and return to the right track. A further online synchronization problem is known as '''[[Pop music automation#Automatic accompaniment|automatic accompaniment]]'''. Having a solo part played by a musician, the task of the computer is to accompany the musician according to a given score by adjusting the tempo and other parameters in real time. Such systems were already proposed some decades ago.&lt;ref&gt;{{cite journal
|last1=Dannenberg
|first1=Roger B.
|title=An on-line algorithm for real-time accompaniment
|journal=Proceedings of the International Computer Music Conference (ICMC)
|url = http://www.cs.cmu.edu/~rbd/papers/icmc84accomp.pdf
|date=1984
|pages=193–198}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|last1=Raphael
|first1=Christopher
|title=A probabilistic expert system for automatic musical accompaniment
|journal = Journal of Computational and Graphical Statistics
|url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6559&amp;rep=rep1&amp;type=pdf
|year=2001
|pages=487–512
}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|last2=Raphael
|first2=Christopher
|year=2006
|title=Music score alignment and computer accompaniment
|url=http://www.cs.cmu.edu/~rbd/papers/accompaniment-cacm-06.pdf
|journal=Communications of the ACM
|volume=49
|issue=8
|pages=38–43
|doi=10.1145/1145287.1145311
|issn=0001-0782
|last1=Dannenberg
|first1=Roger B.}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Music information retrieval]]
[[Category:Music technology]]
[[Category:Musicology]]
[[Category:Information retrieval techniques]]</text>
      <sha1>j7cc05i1ljp5wzh9gv46iwbi1pmomvh</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic analysis</title>
    <ns>0</ns>
    <id>689427</id>
    <revision>
      <id>762103594</id>
      <parentid>760873000</parentid>
      <timestamp>2017-01-26T17:41:54Z</timestamp>
      <contributor>
        <username>Nicegilles</username>
        <id>29988969</id>
      </contributor>
      <minor />
      <comment>/* Additional uses of LSI */ added link to e-discovery wiki page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="53375" xml:space="preserve">{{semantics}}
'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular [[distributional semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.&lt;ref&gt;{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188–230}}&lt;/ref&gt;

An information retrieval technique using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853], now expired) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].&lt;ref&gt;{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}&lt;/ref&gt;

== Overview ==

=== Occurrence matrix ===
LSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

=== Rank lowering ===
After the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]&lt;ref&gt;Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}&lt;/ref&gt; to the [[term-document matrix]]. There could be various reasons for these approximations:

* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a "least and necessary evil").
* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).
* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each document—generally a much larger set due to [[synonymy]].

The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

:: {(car), (truck), (flower)} --&gt;  {(1.3452 * car + 0.2828 * truck), (flower)}

This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.

=== Derivation ===
Let &lt;math&gt;X&lt;/math&gt; be a matrix where element &lt;math&gt;(i,j)&lt;/math&gt; describes the occurrence of term &lt;math&gt;i&lt;/math&gt; in document &lt;math&gt;j&lt;/math&gt; (this can be, for example, the frequency). &lt;math&gt;X&lt;/math&gt; will look like this:

:&lt;math&gt;
\begin{matrix} 
 &amp; \textbf{d}_j \\
 &amp; \downarrow \\
\textbf{t}_i^T \rightarrow &amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
\end{matrix}
&lt;/math&gt;

Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:

:&lt;math&gt;\textbf{t}_i^T = \begin{bmatrix} x_{i,1} &amp; \dots &amp; x_{i,n} \end{bmatrix}&lt;/math&gt;

Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:

:&lt;math&gt;\textbf{d}_j = \begin{bmatrix} x_{1,j} \\ \vdots \\ x_{m,j} \end{bmatrix}&lt;/math&gt;

Now the [[dot product]] &lt;math&gt;\textbf{t}_i^T \textbf{t}_p&lt;/math&gt; between two term vectors gives the [[correlation]] between the terms over the set of documents. The [[matrix product]] &lt;math&gt;X X^T&lt;/math&gt; contains all these dot products. Element &lt;math&gt;(i,p)&lt;/math&gt; (which is equal to element &lt;math&gt;(p,i)&lt;/math&gt;) contains the dot product &lt;math&gt;\textbf{t}_i^T \textbf{t}_p&lt;/math&gt; (&lt;math&gt; = \textbf{t}_p^T \textbf{t}_i&lt;/math&gt;). Likewise, the matrix &lt;math&gt;X^T X&lt;/math&gt; contains the dot products between all the document vectors, giving their correlation over the terms: &lt;math&gt;\textbf{d}_j^T \textbf{d}_q = \textbf{d}_q^T \textbf{d}_j&lt;/math&gt;.

Now, from the theory of linear algebra, there exists a decomposition of &lt;math&gt;X&lt;/math&gt; such that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are [[orthogonal matrix|orthogonal matrices]] and &lt;math&gt;\Sigma&lt;/math&gt; is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):

:&lt;math&gt;
\begin{matrix}
X = U \Sigma V^T
\end{matrix}
&lt;/math&gt;

The matrix products giving us the term and document correlations then become

:&lt;math&gt;
\begin{matrix}
X X^T &amp;=&amp; (U \Sigma V^T) (U \Sigma V^T)^T = (U \Sigma V^T) (V^{T^T} \Sigma^T U^T) = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T = U \Sigma^2 U^T \\
X^T X &amp;=&amp; (U \Sigma V^T)^T (U \Sigma V^T) = (V^{T^T} \Sigma^T U^T) (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T = V \Sigma^2 V^T
\end{matrix}
&lt;/math&gt;

Since &lt;math&gt;\Sigma \Sigma^T&lt;/math&gt; and &lt;math&gt;\Sigma^T \Sigma&lt;/math&gt; are diagonal we see that &lt;math&gt;U&lt;/math&gt; must contain the [[eigenvector]]s of &lt;math&gt;X X^T&lt;/math&gt;, while &lt;math&gt;V&lt;/math&gt; must be the eigenvectors of &lt;math&gt;X^T X&lt;/math&gt;. Both products have the same non-zero eigenvalues, given by the non-zero entries of &lt;math&gt;\Sigma \Sigma^T&lt;/math&gt;, or equally, by the non-zero entries of &lt;math&gt;\Sigma^T\Sigma&lt;/math&gt;. Now the decomposition looks like this:

:&lt;math&gt;
\begin{matrix} 
 &amp; X &amp; &amp; &amp; U &amp; &amp; \Sigma &amp; &amp; V^T \\
 &amp; (\textbf{d}_j) &amp; &amp; &amp; &amp; &amp; &amp; &amp; (\hat{\textbf{d}}_j) \\
 &amp; \downarrow &amp; &amp; &amp; &amp; &amp; &amp; &amp; \downarrow \\
(\textbf{t}_i^T) \rightarrow 
&amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\\
\vdots &amp; \ddots &amp; \vdots \\
\\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
&amp;
=
&amp;
(\hat{\textbf{t}}_i^T) \rightarrow
&amp;
\begin{bmatrix} 
\begin{bmatrix} \, \\ \, \\ \textbf{u}_1 \\ \, \\ \,\end{bmatrix} 
\dots
\begin{bmatrix} \, \\ \, \\ \textbf{u}_l \\ \, \\ \, \end{bmatrix}
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\sigma_1 &amp; \dots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \sigma_l \\
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\begin{bmatrix} &amp; &amp; \textbf{v}_1 &amp; &amp; \end{bmatrix} \\
\vdots \\
\begin{bmatrix} &amp; &amp; \textbf{v}_l &amp; &amp; \end{bmatrix}
\end{bmatrix}
\end{matrix}
&lt;/math&gt;

The values &lt;math&gt;\sigma_1, \dots, \sigma_l&lt;/math&gt; are called the singular values, and &lt;math&gt;u_1, \dots, u_l&lt;/math&gt; and &lt;math&gt;v_1, \dots, v_l&lt;/math&gt; the left and right singular vectors.
Notice the only part of &lt;math&gt;U&lt;/math&gt; that contributes to &lt;math&gt;\textbf{t}_i&lt;/math&gt; is the &lt;math&gt;i\textrm{'th}&lt;/math&gt; row.
Let this row vector be called &lt;math&gt;\hat{\textrm{t}}^T_i&lt;/math&gt;.
Likewise, the only part of &lt;math&gt;V^T&lt;/math&gt; that contributes to &lt;math&gt;\textbf{d}_j&lt;/math&gt; is the &lt;math&gt;j\textrm{'th}&lt;/math&gt; column, &lt;math&gt;\hat{ \textrm{d}}_j&lt;/math&gt;.
These are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.

It turns out that when you select the &lt;math&gt;k&lt;/math&gt; largest singular values, and their corresponding singular vectors from &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, you get the rank &lt;math&gt;k&lt;/math&gt; approximation to &lt;math&gt;X&lt;/math&gt; with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The row "term" vector &lt;math&gt;\hat{\textbf{t}}^T_i&lt;/math&gt; then has &lt;math&gt;k&lt;/math&gt; entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the "document" vector &lt;math&gt;\hat{\textbf{d}}_j&lt;/math&gt; is an approximation in this lower-dimensional space. We write this approximation as

:&lt;math&gt;X_k = U_k \Sigma_k V_k^T&lt;/math&gt;

You can now do the following:
* See how related documents &lt;math&gt;j&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; are in the low-dimensional space by comparing the vectors &lt;math&gt;\Sigma_k \hat{\textbf{d}}_j &lt;/math&gt; and &lt;math&gt;\Sigma_k \hat{\textbf{d}}_q &lt;/math&gt; (typically by [[vector space model|cosine similarity]]).
* Comparing terms &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt; by comparing the vectors &lt;math&gt;\Sigma_k \hat{\textbf{t}}_i&lt;/math&gt; and &lt;math&gt;\Sigma_k \hat{\textbf{t}}_p&lt;/math&gt;. Note that &lt;math&gt;\hat{\textbf{t}}&lt;/math&gt; is now a column vector.
* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.
* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.

To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:

:&lt;math&gt;\hat{\textbf{d}}_j = \Sigma_k^{-1} U_k^T \textbf{d}_j&lt;/math&gt;

Note here that the inverse of the diagonal matrix &lt;math&gt;\Sigma_k&lt;/math&gt; may be found by inverting each nonzero value within the matrix.

This means that if you have a query vector &lt;math&gt;q&lt;/math&gt;, you must do the translation &lt;math&gt;\hat{\textbf{q}} = \Sigma_k^{-1} U_k^T \textbf{q}&lt;/math&gt; before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:

:&lt;math&gt;\textbf{t}_i^T = \hat{\textbf{t}}_i^T \Sigma_k V_k^T&lt;/math&gt;

:&lt;math&gt;\hat{\textbf{t}}_i^T = \textbf{t}_i^T V_k^{-T} \Sigma_k^{-1} = \textbf{t}_i^T V_k \Sigma_k^{-1}&lt;/math&gt;

:&lt;math&gt;\hat{\textbf{t}}_i = \Sigma_k^{-1}  V_k^T \textbf{t}_i&lt;/math&gt;

== Applications ==

The new low-dimensional space typically can be used to:
* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).
* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).
* Find relations between terms ([[synonymy]] and [[polysemy]]).
* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).
* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.&lt;ref name="Alain2009"&gt;{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model |author1=Alain Lifchitz |author2=Sandra Jhean-Larose |author3=Guy Denhière | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201–1209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}&lt;/ref&gt;
* Expand the feature space of machine learning / text mining systems &lt;ref name="Galvez2017"&gt;{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. Gálvez |author2=Agustín Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}&lt;/ref&gt;

Synonymy and polysemy are fundamental problems in [[natural language processing]]: 
* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.
* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.

=== Commercial applications ===

LSA has been used to assist in performing [[prior art]] searches for [[patents]].&lt;ref name="Gerry2007"&gt;{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435–436 | postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

=== Applications in human memory ===

The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].&lt;ref&gt;{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall |author1=Marc W. Howard |author2=Michael J. Kahana |year=1999}}&lt;/ref&gt;

When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.&lt;ref&gt;{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb| booktitle=Interspeech'2005|year=2006|display-authors=etal}}&lt;/ref&gt;

Another model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.&lt;ref&gt;{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=May 8, 2011}}&lt;/ref&gt;

== Implementation ==

The [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.&lt;ref name="Genevi2005"&gt;{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis |author1=Geneviève Gorrell |author2=Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}&lt;/ref&gt;
A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.&lt;ref name="brand2006"&gt;{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20–30 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}&lt;/ref&gt; [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.
In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.&lt;ref&gt;{{cite journal | doi = 10.1109/ICCSNT.2011.6182070 | title=A parallel implementation of Singular Value Decomposition based on Map-Reduce and PARPACK | journal=Proceedings of 2011 International Conference on Computer Science and Network Technology}}&lt;/ref&gt;

== Limitations ==
Some of LSA's drawbacks include:

* The resulting dimensions might be difficult to interpret. For instance, in
:: {(car), (truck), (flower)} ↦  {(1.3452 * car + 0.2828 * truck), (flower)}
:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to
:: {(car), (bottle), (flower)} ↦  {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}
:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.

* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).
* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words. To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.&lt;ref&gt;{{cite journal|url=http://www.translational-medicine.com/content/12/1/324|title=Empirical study using network of semantically related associations in bridging the knowledge gap|first1=Vida|last1=Abedi|first2=Mohammed|last2=Yeasin|first3=Ramin|last3=Zand|date=27 November 2014|publisher=|volume=12|issue=1|doi=10.1186/s12967-014-0324-9|pmid=25428570|pmc=4252998|journal=Journal of Translational Medicine}}&lt;/ref&gt;
* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.&lt;ref name="Thomas1999"&gt;{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}&lt;/ref&gt;

==Alternative methods==

===Semantic hashing===
In semantic hashing &lt;ref&gt;Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." RBM 500.3 (2007): 500.&lt;/ref&gt; documents are mapped to memory addresses by means of a [[neural network]] in such a way that semantically similar documents are located at nearby addresses. [[Deep learning|Deep neural network]] essentially builds a [[graphical model]] of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than [[locality sensitive hashing]], which is the fastest current method.

== Latent semantic indexing ==
'''Latent semantic indexing''' ('''LSI''') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the [[terminology|term]]s and [[concept]]s contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a [[Text corpus|body of text]] by establishing associations between those terms that occur in similar [[context (language use)|context]]s.&lt;ref name=deerwester1988&gt;Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36–40.&lt;/ref&gt;

LSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benzécri]]&lt;ref&gt;{{ cite book
 | author = Benzécri, J.-P.
 | publisher=Dunod |location= Paris, France
 | year = 1973
 | title = L'Analyse des Données. Volume II. L'Analyse des Correspondences
 }}&lt;/ref&gt; in the early 1970s, to a [[contingency table]] built from word counts in documents.

Called Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at [[Bellcore]] in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria.

== Benefits of LSI ==

LSI overcomes two of the most problematic constraints of Boolean [[keyword search|keyword queries]]:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of [[information retrieval]] systems.&lt;ref&gt;{{Cite journal | last1 = Furnas | first1 = G. W. | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | title = The vocabulary problem in human-system communication | doi = 10.1145/32206.32212 | journal = Communications of the ACM | volume = 30 | issue = 11 | pages = 964–971 | year = 1987 | pmid =  | pmc = }}&lt;/ref&gt;  As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.

LSI is also used to perform automated [[document categorization]].  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.&lt;ref name=landauer2008&gt;Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns &amp; S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45–51.&lt;/ref&gt;    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.&lt;ref&gt;{{Cite book | last1 = Dumais | first1 = S. | last2 = Platt | first2 = J. | last3 = Heckerman | first3 = D. | last4 = Sahami | first4 = M. | chapter = Inductive learning algorithms and representations for text categorization | doi = 10.1145/288627.288651 | title = Proceedings of the seventh international conference on Information and knowledge management  - CIKM '98 | pages = 148 | year = 1998 | isbn = 1581130619 | url = http://research.microsoft.com/en-us/um/people/jplatt/cikm98.pdf| pmid =  | pmc = }}&lt;/ref&gt;   LSI uses ''example'' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.

Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.

Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.{{Citation needed|date=July 2015}}

LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.&lt;ref&gt;Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC&lt;/ref&gt;   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.&lt;ref&gt;{{Cite journal | last1 = Homayouni | first1 = R. | last2 = Heinrich | first2 = K. | last3 = Wei | first3 = L. | last4 = Berry | first4 = M. W. | title = Gene clustering by Latent Semantic Indexing of MEDLINE abstracts | doi = 10.1093/bioinformatics/bth464 | journal = Bioinformatics | volume = 21 | issue = 1 | pages = 104–115 | year = 2004 | pmid =  15308538| pmc = }}&lt;/ref&gt;

LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).&lt;ref&gt;{{Cite book | last1 = Price | first1 = R. J. | last2 = Zukas | first2 = A. E. | chapter = Application of Latent Semantic Indexing to Processing of Noisy Text | doi = 10.1007/11427995_68 | title = Intelligence and Security Informatics | series = Lecture Notes in Computer Science | volume = 3495 | pages = 602 | year = 2005 | isbn = 978-3-540-25999-2 | pmid =  | pmc = }}&lt;/ref&gt;   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.

Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.

LSI has proven to be a useful solution to a number of conceptual matching problems.&lt;ref&gt;Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59–65.&lt;/ref&gt;&lt;ref&gt;Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161–167.&lt;/ref&gt;  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.&lt;ref&gt;{{cite journal|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.5444&amp;rep=rep1&amp;type=pdf|author1=Graesser, A. |author2=Karnavat, A.|title=Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures|journal=Proceedings of CogSci 2000|pages=184–189}}&lt;/ref&gt;

== LSI timeline ==

*'''Mid-1960s''' – Factor analysis technique first described and tested (H. Borko and M. Bernick)
*'''1988''' – Seminal paper on LSI technique published &lt;ref name=deerwester1988/&gt;
*'''1989''' – Original patent granted &lt;ref name=deerwester1988/&gt;
*'''1992''' – First use of LSI to assign articles to reviewers&lt;ref&gt;{{cite journal|last1=Dumais |first1=S. |last2=Nielsen |first2=J. |title=Automating the Assignment of Submitted Manuscripts to Reviewers|journal=Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval|year=1992|pages=233–244|doi=10.1145/133160.133205|isbn=0897915232 }}&lt;/ref&gt;
*'''1994''' – Patent granted for the cross-lingual application of LSI (Landauer et al.)
*'''1995''' – First use of LSI for grading essays (Foltz, et al., Landauer et al.)
*'''1999''' – First implementation of LSI technology for intelligence community for analyzing unstructured text ([[Science Applications International Corporation|SAIC]]).
*'''2002''' – LSI-based product offering to intelligence-based government agencies (SAIC)
*'''2005''' – First vertical-specific application – publishing – EDB (EBSCO, [[Content Analyst Company]])

== Mathematics of LSI ==

LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a [[Singular value decomposition|'''Singular Value Decomposition''']] on the matrix, and using the matrix to identify the concepts contained in the text.

=== Term-document matrix ===

LSI begins by constructing a term-document matrix, &lt;math&gt;A&lt;/math&gt;, to identify the occurrences of the &lt;math&gt;m&lt;/math&gt; unique terms within a collection of &lt;math&gt;n&lt;/math&gt; documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, &lt;math&gt;a_{ij}&lt;/math&gt;, initially representing the number of times the associated term appears in the indicated document, &lt;math&gt;\mathrm{tf_{ij}}&lt;/math&gt;.  This matrix is usually very large and very sparse.

Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, &lt;math&gt;a_{ij}&lt;/math&gt; of &lt;math&gt;A&lt;/math&gt;, to be the product of a local term weight, &lt;math&gt;l_{ij}&lt;/math&gt;, which describes the relative frequency of a term in a document, and a global weight, &lt;math&gt;g_i&lt;/math&gt;, which describes the relative frequency of the term within the entire collection of documents.

Some common local weighting functions &lt;ref&gt;
Berry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).&lt;/ref&gt; are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
|  style="width:22%" | '''Binary''' ||
| &lt;math&gt;l_{ij} = 1&lt;/math&gt; if the term exists in the document, or else &lt;math&gt;0&lt;/math&gt;
|-
|  style="width:22%" | '''TermFrequency''' ||
| &lt;math&gt;l_{ij} = \mathrm{tf}_{ij}&lt;/math&gt;, the number of occurrences of term &lt;math&gt;i&lt;/math&gt; in document &lt;math&gt;j&lt;/math&gt;
|-
|  style="width:22%" | '''Log''' ||
| &lt;math&gt;l_{ij} = \log(\mathrm{tf}_{ij} + 1)&lt;/math&gt;
|-
|  style="width:22%" | '''Augnorm''' ||
| &lt;math&gt;l_{ij} = \frac{\Big(\frac{\mathrm{tf}_{ij}}{\max_i(\mathrm{tf}_{ij})}\Big) + 1}{2}&lt;/math&gt;
|}

Some common global weighting functions are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
| style="width:22%" | '''Binary''' ||
| &lt;math&gt;g_i = 1&lt;/math&gt;
|-
| style="width:22%" | '''Normal''' ||
| &lt;math&gt;g_i = \frac{1}{\sqrt{\sum_j \mathrm{tf}_{ij}^2}}&lt;/math&gt;
|-
| style="width:22%" | '''GfIdf''' ||
| &lt;math&gt;g_i = \mathrm{gf}_i / \mathrm{df}_i&lt;/math&gt;, where &lt;math&gt;\mathrm{gf}_i&lt;/math&gt; is the total number of times term &lt;math&gt;i&lt;/math&gt; occurs in the whole collection, and &lt;math&gt;\mathrm{df}_i&lt;/math&gt; is the number of documents in which term &lt;math&gt;i&lt;/math&gt; occurs.
|-
| style="width:22%" | '''[[Tf–idf#Inverse document frequency 2|Idf (Inverse Document Frequency)]]''' ||
| &lt;math&gt;g_i = \log_2 \frac{n}{1+ \mathrm{df}_i}&lt;/math&gt;
|-
| style="width:22%" | '''Entropy''' ||
| &lt;math&gt;g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}&lt;/math&gt;, where &lt;math&gt;p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}&lt;/math&gt;
|}

Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets.&lt;ref&gt;Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.&lt;/ref&gt;  In other words, each entry &lt;math&gt;a_{ij}&lt;/math&gt; of &lt;math&gt;A&lt;/math&gt; is computed as:

:&lt;math&gt;g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}&lt;/math&gt;

:&lt;math&gt;a_{ij} = g_i \ \log (\mathrm{tf}_{ij} + 1)&lt;/math&gt;

=== Rank-reduced singular value decomposition ===

A rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.&lt;ref&gt;Berry, Michael W., Dumais, Susan T., O'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573–595.&lt;/ref&gt;   It computes the term and document vector spaces by approximating the single term-frequency matrix, &lt;math&gt;A&lt;/math&gt;, into three other matrices— an '''''m''''' by '''''r'''''  term-concept vector matrix &lt;math&gt;T&lt;/math&gt;, an '''''r''''' by '''''r''''' singular values matrix &lt;math&gt;S&lt;/math&gt;, and a '''''n''''' by '''''r''''' concept-document vector matrix, &lt;math&gt;D&lt;/math&gt;, which satisfy the following relations:

&lt;math&gt;A \approx TSD^T&lt;/math&gt;

&lt;math&gt;T^T T = I_r \quad D^T D = I_r &lt;/math&gt;

&lt;math&gt;S_{1,1} \geq S_{2,2} \geq \ldots \geq  S_{r,r} &gt; 0 \quad S_{i,j} = 0 \; \text{where} \; i \neq j&lt;/math&gt;

In the formula, '''A''' is the supplied '''''m''''' by '''''n''''' weighted matrix of term frequencies in a collection of text where '''''m''''' is the number of unique terms, and '''''n''''' is the number of documents.  '''T''' is a computed '''''m''''' by '''''r''''' matrix of term vectors where '''''r''''' is the rank of '''A'''—a measure of its unique dimensions '''≤ min(''m,n'')'''.  '''S''' is a computed '''''r''''' by '''''r''''' diagonal matrix of decreasing singular values, and '''D''' is a computed '''''n''''' by '''''r''''' matrix of document vectors.

The SVD is then [[Singular value decomposition#Truncated SVD|truncated]] to reduce the rank by keeping only the largest '''''k''''' « '''''r''''' diagonal entries in the singular value matrix '''S''',
where '''''k''''' is typically on the order 100 to 300 dimensions.
This effectively reduces the term and document vector matrix sizes to '''''m''''' by '''''k''''' and '''''n''''' by '''''k''''' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of '''A'''.  This reduced set of matrices is often denoted with a modified formula such as:

:::::::'''A ≈ A''&lt;sub&gt;k''&lt;/sub&gt; = T''&lt;sub&gt;k''&lt;/sub&gt; S''&lt;sub&gt;k''&lt;/sub&gt; D''&lt;sub&gt;k''&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;'''

Efficient LSI algorithms only compute the first '''''k''''' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.

Note that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix '''A''', except that PCA subtracts off the means.  PCA loses the sparseness of the '''A''' matrix, which can make it infeasible for large lexicons.

== Querying and augmenting LSI vector spaces ==

The computed '''T''&lt;sub&gt;k''&lt;/sub&gt;''' and '''D''&lt;sub&gt;k''&lt;/sub&gt;''' matrices define the term and document vector spaces, which with the computed singular values, '''S''&lt;sub&gt;k''&lt;/sub&gt;''', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.

The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the '''A = T S D&lt;sup&gt;T&lt;/sup&gt;''' equation into the equivalent '''D = A&lt;sup&gt;T&lt;/sup&gt; T S&lt;sup&gt;−1&lt;/sup&gt;''' equation, a new vector, '''''d''''', for a query or for a new document can be created by computing a new column in '''A''' and then multiplying the new column by '''T S&lt;sup&gt;−1&lt;/sup&gt;'''.  The new column in '''A''' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.

A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.

The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called ''folding in''.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in &lt;ref name="brand2006"/&gt;) be used.

== Additional uses of LSI ==

It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.

LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.&lt;ref&gt;Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.&lt;/ref&gt;   Below are some other ways in which LSI is being used:

* Information discovery&lt;ref&gt;Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189–223.&lt;/ref&gt;  ([[Electronic Discovery|eDiscovery]], Government/Intelligence community, Publishing)
* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)&lt;ref&gt;Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.&lt;/ref&gt;
* Text summarization&lt;ref&gt;Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903–907.&lt;/ref&gt;  (eDiscovery, Publishing)
* Relationship discovery&lt;ref&gt;Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374–380.&lt;/ref&gt;  (Government, Intelligence community, Social Networking)
* Automatic generation of link charts of individuals and organizations&lt;ref&gt;Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23–24, 2006, Springer, LNCS vol. 3975, pp. 674–675.&lt;/ref&gt;  (Government, Intelligence community)
* Matching technical papers and grants with reviewers&lt;ref&gt;Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220–230.&lt;/ref&gt;  (Government)
* Online customer support&lt;ref&gt;Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master's Thesis, May 2000.&lt;/ref&gt;  (Customer Management)
* Determining document authorship&lt;ref&gt;Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43–48.&lt;/ref&gt;  (Education)
* Automatic keyword annotation of images&lt;ref&gt;Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275–278.&lt;/ref&gt;
* Understanding software source code&lt;ref&gt;{{cite journal|author1=Maletic, J. |author2=Marcus, A.|title=Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding|journal=Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence|location=Vancouver, British Columbia|date=November 13–15, 2000|pages= 46–53|doi=10.1109/TAI.2000.889845|isbn=0-7695-0909-6}}&lt;/ref&gt;  (Software Engineering)
* Filtering [[Spam (electronic)|spam]]&lt;ref&gt;Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460–464.&lt;/ref&gt;  (System Administration)
* Information visualization&lt;ref name=landauer2004&gt;Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Sciences, 101, 2004, pp. 5214–5219.&lt;/ref&gt;
* [[Automated essay scoring|Essay scoring]]&lt;ref&gt;Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.&lt;/ref&gt;  (Education)
* [[Literature-based discovery]]&lt;ref&gt;Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674–685.&lt;/ref&gt;
* Stock resturns prediction&lt;ref name="Galvez2017"&gt;{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. Gálvez |author2=Agustín Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}&lt;/ref&gt;

LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.&lt;ref&gt;There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.&lt;/ref&gt;

== Challenges to LSI ==

Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.&lt;ref&gt;Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.&lt;/ref&gt;  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.&lt;ref name="rehurek2011"&gt;{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim Řehůřek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289–300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 |series=Lecture Notes in Computer Science|isbn=978-3-642-20160-8}}&lt;/ref&gt;

Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).&lt;ref&gt;Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153–162.&lt;/ref&gt;   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.&lt;ref name=landauer2008&gt;Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.&lt;/ref&gt;

Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.&lt;ref&gt;Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).&lt;/ref&gt;&lt;ref&gt;Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).&lt;/ref&gt;&lt;ref&gt;Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.&lt;/ref&gt;

== See also ==
* [[Compound term processing]]
* [[Explicit semantic analysis]]
* [[Latent semantic mapping]]
* [[Latent Semantic Structure Indexing]]
* [[Principal components analysis]]
* [[Probabilistic latent semantic analysis]]
* [[Spamdexing]]
* [[Topic model]]
** [[Latent Dirichlet allocation]]
* [[Distributional semantics]]
* [[Coh-Metrix]]

== References ==
{{Reflist|30em}}

==Further reading==
* {{cite journal
 | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
 | format=PDF
 | title=Introduction to Latent Semantic Analysis
 | author-link1= Thomas Landauer |first1=Thomas |last1=Landauer |first2=Peter W. |last2=Foltz |first3=Darrell |last3=Laham
 | journal=Discourse Processes
 | volume=25
 | pages=259–284
 | year=1998
 | doi=10.1080/01638539809545028
 | issue=2–3
}}
* {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 | format=PDF| title=Indexing by Latent Semantic Analysis
 | first1=Scott |last1=Deerwester |first2=Susan T. |last2=Dumais |first3=George W. |last3=Furnas |first4=Thomas K. |last4=Landauer |first5=Richard |last5=Harshman
 | author-link1=Scott Deerwester |author-link2=Susan Dumais |author-link3=George Furnas |author-link4=Thomas Landauer |author-link5=Richard Harshman
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391–407
 | year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9
}} Original article where the model was first exposed.
* {{cite journal
 | url=http://citeseer.ist.psu.edu/berry95using.html
 | title=Using Linear Algebra for Intelligent Information Retrieval
 | first1=Michael |last1= Berry |first2=Susan T. |last2=Dumais  |first3=Gavin W. |last3=O'Brien
 | author-link1=Susan Dumais
 |year=1995
}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.
* {{cite web
 | url=http://iv.slis.indiana.edu/sw/lsa.html
 | title=Latent Semantic Analysis
 | publisher=InfoVis
}}
* {{cite web
 | url=http://cran.at.r-project.org/web/packages/lsa/index.html
 | title=An Open Source LSA Package for R
 | publisher=CRAN
 | author=Fridolin Wild
 | date=November 23, 2005
 | accessdate=November 20, 2006
}}
* {{ cite web
 | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM
 | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge
 | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]
 | accessdate=2007-07-02
}}

==External links==

===Articles on LSA===
* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.

===Talks and demonstrations===
* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].
* [http://www.semanticquery.com/archive/semanticsearchart/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.

===Implementations===

Due to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.
* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA
* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA
* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices
* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)
* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA
* [[Gensim]] contains a Python implementation of LSA for matrices larger than RAM.

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
[[Category:Latent variable models]]</text>
      <sha1>q00jsvi1va7q04kjpowabj0pshg9b3j</sha1>
    </revision>
  </page>
  <page>
    <title>XML retrieval</title>
    <ns>0</ns>
    <id>21106742</id>
    <revision>
      <id>747606906</id>
      <parentid>738982032</parentid>
      <timestamp>2016-11-03T09:26:36Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <minor />
      <comment>change |id={{citeseerx}} to |citeseerx=</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6546" xml:space="preserve">{{Multiple issues|
{{expert-subject|Computer science|date=January 2015}}
{{COI|date=February 2009}}
}}

'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.&lt;ref&gt;{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}&lt;/ref&gt;

==Queries==
Most XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.

==Exploiting XML structure==
Taking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.

==Ranking==
Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.&lt;ref name="INEX2006"&gt;{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |title=Overview of INEX 2006 |last=Malik |first=Saadia |author2=Trotman, Andrew |author3=Lalmas, Mounia |author4=Fuhr, Norbert |year=2007 |work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081016101202/http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |archivedate=October 16, 2008 }}&lt;/ref&gt;

==Existing XML search engines==
An overview of two potential approaches is available.&lt;ref&gt;{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}&lt;/ref&gt;&lt;ref&gt;{{Cite paper|citeseerx = 10.1.1.109.5986|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR }}&lt;/ref&gt; The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.&lt;ref name="INEX2006" /&gt; Three different areas influence XML-Retrieval:&lt;ref name="INEX2002"&gt;{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |title=INEX: Initiative for the Evaluation of XML Retrieval |last=Fuhr |first=Norbert |author2=Gövert, N. |author3=Kazai, Gabriella |author4=Lalmas, Mounia |year=2003 |work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002 |publisher=ERCIM Workshop Proceedings, France |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081121135758/http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |archivedate=November 21, 2008 }}&lt;/ref&gt;

===Traditional XML query languages===
[[Query language]]s such as the [[W3C]] standard [[XQuery]]&lt;ref&gt;{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernández, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Siméon, Jérôme |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}&lt;/ref&gt; supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].&lt;ref name="Schlieder2002"&gt;{{Cite journal|url=http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |title=Querying and Ranking XML Documents |last=Schlieder |first=Torsten |author2=Meuss, Holger |year=2002 |work=Journal of the American Society for Information Science and Technology, Vol. 53, No. 6 |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |archivedate=June 10, 2007 }}&lt;/ref&gt;

===Databases===
Classic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]&lt;ref name="INEX2002" /&gt; and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.

===Information retrieval===
Classic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.&lt;ref name="Schlieder2002"/&gt; They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.&lt;ref&gt;{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}&lt;/ref&gt;

==See also==
*[[Document retrieval]]
*[[Information retrieval applications]]

==References==
{{Reflist}}

{{DEFAULTSORT:Xml-Retrieval}}
[[Category:XML]]
[[Category:Information retrieval genres]]</text>
      <sha1>1nr3r6p6oad13cuska25jwgg8h2so63</sha1>
    </revision>
  </page>
  <page>
    <title>Multimedia search</title>
    <ns>0</ns>
    <id>5987236</id>
    <revision>
      <id>735904849</id>
      <parentid>735904754</parentid>
      <timestamp>2016-08-23T21:41:14Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Audio search engine */ move link, c/e</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3643" xml:space="preserve">'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.
Multimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.
We can distinguish two methodologies in multimedia search:
*'''Metadata search''': the search is made on the layers of [[metadata]].
* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.

==Metadata search==

Search is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.

There are three processes which should be done in this method:
*'''[[Multimedia information retrieval#Feature extraction methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.
*'''[[Multimedia information retrieval#Feature extraction methods|Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])
*'''[[Multimedia information retrieval#Categorization methods|Categorization of media descriptions]]''' into classes.

==Query by example==

In [[query by example]], the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often, it's used [[Search engine indexing|audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:
*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].
*Compare descriptors of the query and our database’s media.
*List the media sorted by maximum coincidence.

==Multimedia search engine==
There are two big search families, in function of the content:
* [[Visual search engine]]
*[[Audio search engine]]

===Visual search engine===
Inside this family we can distinguish two topics: [[image search]] and [[video search]]

*'''[[Image search]]''': Although usually it's used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example, [[QR codes]].
*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.

===Audio search engine===
There are different methods of [[Audio search engine|audio searching]]:
*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].
*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album…) . There are some programs of [[music recognition]], for example [[Shazam (service)|Shazam]] or [[SoundHound]].

==See also==
*''[[Journal of Multimedia]]''
*[[List of search engines#Multimedia|List of search engines]]
*[[Multimedia]]
*[[Multimedia information retrieval]]
*[[Search engine indexing]]
*[[Streaming media]]
*[[Video search engine]]

==External links==

[[Category:Information retrieval genres]]
[[Category:Multimedia]]</text>
      <sha1>qan1tcazolqdrw3k2rawn61dtvmeo6c</sha1>
    </revision>
  </page>
  <page>
    <title>Cross-language information retrieval</title>
    <ns>0</ns>
    <id>296950</id>
    <revision>
      <id>735708554</id>
      <parentid>707776937</parentid>
      <timestamp>2016-08-22T15:50:42Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Filled in 1 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2389" xml:space="preserve">{{refimprove|date=September 2014}}

'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.&lt;ref&gt;"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011.&lt;/ref&gt;  CLIR techniques can be classified into different categories based on different translation resources: 
* Dictionary-based CLIR techniques
* Parallel corpora based CLIR techniques
* Comparable corpora based CLIR techniques
* Machine translator based CLIR techniques

The first workshop on CLIR was held in Zürich during the SIGIR-96 conference.&lt;ref&gt;The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.&lt;/ref&gt; Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).

The term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.

[[Google Search]] had a cross-language search feature that was removed in 2013.&lt;ref&gt;{{cite web|url=http://searchengineland.com/google-drops-translated-foreign-pages-search-option-due-to-lack-of-use-160157|title=Google Drops "Translated Foreign Pages" Search Option Due To Lack Of Use|date=20 May 2013|publisher=}}&lt;/ref&gt;

==See also==
*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)

==References==
&lt;references /&gt;

==External links==
*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]
*[http://www.2lingual.com/ A search engine for CLIR]
{{DEFAULTSORT:Cross-Language Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Natural language processing]]


{{linguistics-stub}}</text>
      <sha1>cn90554bq43so8c0q7jjbisqc708hcv</sha1>
    </revision>
  </page>
  <page>
    <title>Question answering</title>
    <ns>0</ns>
    <id>360030</id>
    <revision>
      <id>762383617</id>
      <parentid>761838673</parentid>
      <timestamp>2017-01-28T13:26:41Z</timestamp>
      <contributor>
        <ip>118.102.129.98</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26428" xml:space="preserve">{{other uses|question|answer}}
{{multiple issues|
{{cleanup|date=January 2012|reason='''appearance of plagiarised text (now in extensive footnote), use of draft rather than published sources, extensive appearance of text violating [[WP:VERIFY]] and/or [[WP:OR]], use of jargon to define jargon, etc.'''}}
{{cleanup rewrite|date=January 2012}}
{{more footnotes|date=February 2014}}
{{citation style|date=January 2016}}
}}

'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].

A QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents.

Some examples of natural language document collections used for QA systems include:
* a local collection of reference texts
* internal organization documents and web pages
* compiled [[newswire]] reports
* a set of [[Wikipedia]] pages
* a subset of [[World Wide Web]] pages

 with a wide range of question types including: fact, list, [[definition]], ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.

* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease &lt;ref&gt;Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer's Disease. CLEF 2012 Evaluation Labs and Workshop. September 17, 2012&lt;/ref&gt;
* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.

==History==
{{unreferenced section|date=January 2016}}
Two early QA systems were BASEBALL&lt;ref&gt;{{cite journal|last1=GREEN JR|first1=Bert F|title=Baseball: an automatic question-answerer.|journal=western joint IRE-AIEE-ACM computer conference|date=1961|pages=219–224|display-authors=etal}}&lt;/ref&gt; and LUNAR.&lt;ref&gt;{{cite journal|last1=Woods|first1=William A|last2=Kaplan|first2=R.|title=Lunar rocks in natural English: Explorations in natural language question answering|journal=Linguistic structures processing 5|date=1977|volume=5|pages=521–569}}&lt;/ref&gt; BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.

[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.

In the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with treport[[expert system|&lt;nowiki/&gt;]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.

The 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.

Recently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.

==Architecture==
{{refimprove section|date=January 2016}}
Most modern QA systems use [[natural language]] text documents as their underlying knowledge source.{{citation needed|date=January 2016}}  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted.{{citation needed|date=January 2016}} An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge; however, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates, etc.) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.{{citation needed|date=January 2016}}

In an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s.{{citation needed|date=January 2016}} It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system.{{citation needed|date=January 2016}} Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors.{{citation needed|date=January 2016}} An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.{{citation needed|date=January 2016}}

As of 2001, QA systems typically included a ''question classifier'' module that determines the type of question and the type of answer.&lt;ref&gt;Hirschman, L. &amp; Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.&lt;/ref&gt; After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text; thus, a ''document retrieval module'' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer, and a ''filter'' preselects small text fragments that contain strings of the same type as the expected answer.{{citation needed|date=January 2016}} For example, if the question is "Who invented
penicillin?", the filter returns text that contain names of people. Finally, an ''answer extraction'' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.{{citation needed|date=January 2016}}

A ''multiagent'' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s).&lt;ref&gt;{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124 |doi=10.5210/fm.v10i1.1204
}}&lt;/ref&gt;

==Question answering methods==
QA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,&lt;ref&gt;Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).&lt;/ref&gt; leading to two benefits:
# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.
# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.

Question answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],&lt;ref&gt;{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=https://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}&lt;/ref&gt; a [[logic programming]] language associated with [[artificial intelligence]].

===Open domain question answering===
{{unreferenced section|date=January 2016}}
In [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.

The system takes a [[natural language]] question as an input rather than a set of keywords, for example, "When is the national day of China?" The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.

Keyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. "Who", "Where" or "How many", these words tell the system that the answers should be of type "Person", "Location", "Number" respectively. In the example above, the word "When" indicates that the answer should be of type "Date". POS (Part of Speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is "Chinese National Day", the predicate is "is" and the adverbial modifier is "when", therefore the answer type is "Date". Unfortunately, some interrogative words like "Which", "What" or "How" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.

Once the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as "Who" or "Where", a Named Entity Recogniser is used to find relevant "Person" and "Location" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.

A [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is "1st Oct."

==Issues==
In 2002, a group of researchers presented an unpublished and largely unsourced report as a funding support document, in which they describe a 5-year roadmap of research current to the state of the question answering filed at that time.&lt;ref&gt;Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R., date unknown, "Tasks and Program Structures to Roadmap Research in Question Answering (QA)," at [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues] [DRAFT DOCUMENT], accessed 1 January 2016.&lt;/ref&gt;&lt;ref&gt;Here is some content taken verbatim from that roadmap (see preceding citation): "[1] Question classes: Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}} [2] Question processing: The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification. [3] Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.) [4] Data sources for QA: Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained. [4] Answer extraction: Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}} [5] Answer formulation: The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents. [6] Real time question answering: There is need for developing Q&amp;A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question. [7] Multilingual (or cross-lingual) question answering: The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].) [8] Interactive QA: It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions. (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.) [9] Advanced reasoning for QA: More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system. [10] Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process. [11] User profiling for QA: The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}} [12] Deep Question Answering: Deep QA complement traditional Question Answering by adding some machine learning capabilities within a standard factoid question answering pipeline. The idea is to leverage curated data repositories or knowledge bases, which can be general ones such as Wikipedia, or domain-specific (e.g. molecular biology) in order to provide more accurate answers to the end-users.&lt;/ref&gt;
&lt;ref&gt;On the subject of interactive QA, see also Perera, R. and Nand, P. (2014). "Interaction History Based Answer Formulation for Question Answering," at [http://rivinduperera.com/publications/kesw2014.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}&lt;/ref&gt;{{full citation needed|date=January 2016}}
&lt;ref&gt;On the subject of information clustering for QA, see also Perera, R. (2012). "IPedagogy: Question Answering System Based on Web Information Clustering," at [http://rivinduperera.com/publications/t4e2012.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}&lt;/ref&gt;{{full citation needed|date=January 2016}}
&lt;ref&gt;On the subject of deep question answering, see the following citation.&lt;/ref&gt;&lt;ref&gt;{{cite journal | pmc = 4572360 | pmid=26384372 | doi=10.1093/database/bav081 | volume=2015 | title=Deep Question Answering for protein annotation | year=2015 | journal=Database (Oxford) |vauthors=Gobeill J, Gaudinat A, Pasche E, Vishnyakova D, Gaudet P, Bairoch A, Ruch P }}&lt;/ref&gt;
&lt;!-- 
Because much of the text in this section was copied and pasted from the "roadmap" document, which itself is a draft and unpublished document, the text was moved into a footnote. 
--&gt;

==Progress==
QA systems have been extended in recent years to encompass additional domains of knowledge&lt;ref&gt;Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.&lt;/ref&gt;  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:

* interactivity—clarification of questions or answers
* answer reuse or caching
* knowledge representation and reasoning
* social media analysis with QA systems
* [[sentiment analysis]]&lt;ref&gt;{{webarchive |url=https://web.archive.org/web/20121027153311/http://totalgood.com/bitcrawl/ |date=October 27, 2012 |title=BitCrawl by Hobson Lane }}&lt;/ref&gt;
* utilization of thematic roles&lt;ref&gt;Perera, R. and Perera, U. 2012. [http://rivinduperera.com/publications/qacd_coling2012.html Towards a thematic role based target identification model for question answering.]&lt;/ref&gt;
* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts&lt;ref&gt;{{cite conference |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430–437 | publisher=Springer Berlin Heidelberg | url=http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 | title=The impact of semantic class identification and semantic role labeling on natural language answer extraction}}&lt;/ref&gt;
* utilization of linguistic resources,&lt;ref&gt;{{cite journal |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes |title=The impact of frame semantic annotation levels, frame‐alignment techniques, and fusion methods on factoid answer processing | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247–263 |year =2009 |url=http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&amp;userIsAuthenticated=false |doi=10.1002/asi.20989}}&lt;/ref&gt; such as [[WordNet]], [[FrameNet]], and the similar

IBM's question answering system, [[Watson (computer)|Watson]], defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.
&lt;ref&gt;http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0&lt;/ref&gt;

==References==
{{reflist}}

==Further reading==
{{citation style|section|date=January 2016}}
* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.
* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.
*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}
* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.

==External links==
* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]
* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]
* [http://nlp.uned.es/clef-qa/ Question Answering Evaluation at CLEF]
* [http://www.gyanibano.com Quiz Question Answers]

{{Computable knowledge}}
{{Natural Language Processing}}


[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval genres]]</text>
      <sha1>fc174yoongokvg7xj4js3qv2jeaylh8</sha1>
    </revision>
  </page>
  <page>
    <title>Exploratory search</title>
    <ns>0</ns>
    <id>4881262</id>
    <revision>
      <id>715773250</id>
      <parentid>715575692</parentid>
      <timestamp>2016-04-17T22:42:41Z</timestamp>
      <contributor>
        <username>Dtunkelang</username>
        <id>5293022</id>
      </contributor>
      <comment>/* Major figures */ updated Nick Belkin's home page URL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12623" xml:space="preserve">'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are:&lt;ref&gt;Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.&lt;/ref&gt;
* unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)
* or unsure about the ways to achieve their goals (either the technology or the process)
* or unsure about their goals in the first place.

Consequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.

==History==
Exploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn't know which keywords to use?" or "what if the user isn't looking for a single answer?". Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.

In the last few years,{{When|date=April 2016}} a series of workshops has been held at various related and key events. In 2005, the Exploratory Search Interfaces workshop focused on beginning to define some of the key challenges in the field.&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw/xsi/index.html|title=HCIL SOH 2005 Workshop on Exploratory Search Interfaces|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt; Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw/eess/index.html|title=SIGIR 2006 Workshop - Evaluating Exploratory Search Systems|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt; at SIGIR06&lt;ref&gt;{{cite web|url=http://www.sigir2006.org|title=Sigir 2006|publisher=|accessdate=8 April 2016}}&lt;/ref&gt; and Exploratory Search and HCI&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw/esi/index.html|title=CHI 2007 Workshop - Exploratory Search and HCI|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt; at CHI07&lt;ref&gt;{{cite web|url=http://www.chi2007.org|title=CHI 2007 Reach Beyond - welcome|publisher=|accessdate=8 April 2016}}&lt;/ref&gt; (in order to meet with the experts in [[human–computer interaction]]).

In March 2008, an ''Information Processing and Management'' special issue&lt;ref&gt;{{cite web|url=http://www.sciencedirect.com/science/journal/03064573|title=Information Processing &amp; Management|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;&lt;ref&gt;Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&amp;nbsp;433–436&lt;/ref&gt; focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.

In June 2008, the [[National Science Foundation]] sponsored an invitational workshop to identify a research agenda for exploratory search and similar fields for the coming years.&lt;ref&gt;{{cite web|url=http://www.ils.unc.edu/ISSS_workshop/|title=Moved|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;

==Research challenges==

===Important scenarios===
With the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by mSpace,&lt;ref&gt;[http://mspace.fm mSpace]&lt;/ref&gt; states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.

===Designing new interfaces===
With one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the users, so that they can choose from a list instead of guess a possible keyword query.

Many of the [[human–computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.

Computational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.&lt;ref&gt;Fu, W.-T., Kannampalill, T. G., &amp; Kang, R. (2010). [http://portal.acm.org/citation.cfm?id=1719970.1719998 Facilitating exploratory search by model-based navigational cues.] In Proceedings of the ACM International conference on Intelligent User Interface. 199–208. &lt;/ref&gt;

===Evaluating interfaces===
As the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.{{cn|date=April 2016}}

===Models of exploratory search behavior===
There have been recent{{When|date=April 2016}} attempts to develop a process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].&lt;ref&gt;{{Citation
  | doi = 10.1145/1460563.1460600
  | last1 = Fu  | first1 = Wai-Tat
  | title = The Microstructures of Social Tagging: A Rational Model
  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work.
  | pages = 66–72
  | date = April 2008
  | url = http://portal.acm.org/citation.cfm?id=1460600
  | isbn = 978-1-60558-007-4 }}
&lt;/ref&gt;
&lt;ref&gt;{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | title = A Semantic Imitation Model of Social Tagging
  | journal = Proceedings of the IEEE conference on Social Computing
  | pages = 66–72
  | date = Aug 2009
  | url = http://www.humanfactors.illinois.edu/Reports&amp;PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}&lt;/ref&gt; The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.&lt;ref&gt;
{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | last2 = Pirolli  | first2 = Peter
  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web
  | journal = Human-Computer Interaction
  | pages = 335–412
  | year = 2007
  | url = http://portal.acm.org/citation.cfm?id=1466608
  | volume = 22}}&lt;/ref&gt;&lt;ref&gt;Kitajima, M., Blackmon, M. H., &amp; Polson, P. G. (2000). A comprehension-based model of Web navigation and its application to Web usability analysis. In S. Mc-Donald, Y. Waern, &amp; G. Cockton (Eds.), People and computers XIV—Usability or else!
New York: Springer-Verlag.&lt;/ref&gt;&lt;ref&gt;Miller, C. S., &amp; Remington, R.W. (2004). Modeling information navigation: Implications for information architecture. Human Computer Interaction, 19, 225–271.&lt;/ref&gt;
Recent{{When|date=April 2016}} development in exploratory search is often concentrated in predicting users' search intents in interaction with the user.&lt;ref&gt;
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Athukorala  | first2 = Kumaripaba
  | last3 = Glowacka  | first3 = Dorota
  | last4 = Konuyshkova  | first4 = Ksenia
  | last5 = Oulasvrita  | first5 = Antti
  | last6 = Kaipiainen  | first6 = Samuli
  | last7 = Kaski  | first7 = Samuel
  | last8 = Jacucci  | first8 = Giulio
  | title = Supporting exploratory search tasks with interactive user modeling
  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&amp;T
  | year = 2013}}
&lt;/ref&gt;
Such predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs.&lt;ref&gt;
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Peltonen  | first2 = Jaakko
  | last3 = Eugster | first3 = Manuel J.A.
  | last4 = Glowacka  | first4 = Dorota
  | last5 = Konuyshkova  | first5 = Ksenia
  | last6 = Athukorala  | first6 = Kumaripaba
  | last7 = Kosunen | first7 = Ilkka   
  | last8 = Reijonen  | first8 = Aki
  | last9 = Myllymäki | first9 = Petri
  | last10 = Kaski  | first10 = Samuel
  | last11 = Jacucci  | first11 = Giulio
  | title = Directing Exploratory Search with Interactive Intent Modeling
  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM
  | year = 2013}}
&lt;/ref&gt;
&lt;ref&gt;
{{Citation
  | last1 = Glowacka  | first1 = Dorota
  | last2 = Ruotsalo  | first2 = Tuukka
  | last3 = Konuyshkova  | first3 = Ksenia
  | last4 = Athukorala  | first4 = Kumaripaba
  | last5 = Kaski  | first5 = Samuel
  | last6 = Jacucci  | first6 = Giulio
  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords
  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI
  | url = http://dl.acm.org/citation.cfm?id=2449413
  | pages = 117–128
  | year = 2013}}
&lt;/ref&gt;

==Major figures==
Key figures, including experts from both [[information seeking]] and [[human–computer interaction]], are:{{Says who|date=April 2016}}
* [[Marcia Bates]]
* Nicholas Belkin&lt;ref&gt;{{cite web|url=http://comminfo.rutgers.edu/~belkin/|title=Nick's home page|publisher=|accessdate=17 April 2016}}&lt;/ref&gt;
* Gary Marchionini&lt;ref&gt;{{cite web|url=http://ils.unc.edu/~march|title=Gary's Home Page|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;
* m.c. schraefel&lt;ref&gt;{{cite web|url=http://users.ecs.soton.ac.uk/mc|title=m.c. schraefel: design for innovation, creativity, discovery|publisher=|accessdate=8 April 2016}}&lt;/ref&gt;
* Ryen White&lt;ref&gt;{{cite web|url=http://research.microsoft.com/~ryenw|title=Ryen W. White|publisher=Microsoft|accessdate=8 April 2016}}&lt;/ref&gt;

==References==
&lt;references /&gt;

==Sources==
# White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&amp;nbsp;36–39.
# Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&amp;nbsp;433–436
# Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.
# P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009

{{DEFAULTSORT:Exploratory Search}}
[[Category:Human–computer interaction]]
[[Category:Information retrieval genres]]
[[Category:Information science]]</text>
      <sha1>3ggj6me67fgkdgl700myz5ruxjj8uia</sha1>
    </revision>
  </page>
  <page>
    <title>Gerard Salton</title>
    <ns>0</ns>
    <id>509624</id>
    <revision>
      <id>758638559</id>
      <parentid>728275225</parentid>
      <timestamp>2017-01-06T17:42:35Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <comment>See also 'List of pioneers in computer science'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7684" xml:space="preserve">{{Infobox scientist
| name              = Gerard Salton
| birth_date        = {{birth date|1927|03|08}}
| birth_place       = [[Nuremberg]]
| death_date        = {{death date and age|1995|08|28 |1927|03|08}}
| death_place       = 
| fields            = [[information retrieval]]
| workplaces        = [[Cornell University]]
| alma_mater        = [[Harvard University]]
| thesis_title      = An automatic data processing system for public utility revenue accounting
| thesis_url        = http://hollis.harvard.edu/?itemid=%7Clibrary/m/aleph%7C003918090
| thesis_year       = 1958
| doctoral_advisor  = [[Howard Aiken]]
| doctoral_students = [[Amit Singhal]]
| known_for         = the father of information retrieval&lt;ref name=father-IR /&gt; &lt;br&gt; [[Gerard Salton Award]]
}}
'''Gerard A. "Gerry" Salton''' (8 March 1927 in [[Nuremberg]] – 28 August 1995), was a Professor of [[Computer Science]] at [[Cornell University]].  Salton was perhaps the leading computer scientist working in the field of [[information retrieval]] during his time, and "the father of information retrieval".&lt;ref name=father-IR&gt;{{cite web |url=http://www.cs.cornell.edu/gries/40brochure/pg24_25.pdf |title=The father of information retrieval |last1= |first1= |last2= |first2= |date= |website=cs.cornell.edu |publisher= |quote= a founding member of the department and the father of information retrieval. |access-date=10 March 2015}}&lt;/ref&gt;  His group at Cornell developed the [[SMART Information Retrieval System]], which he initiated when he was at Harvard.

Salton was born Gerhard Anton Sahlmann on March 8, 1927 in [[Nuremberg, Germany]].  He received a Bachelor's (1950) and Master's (1952) degree in mathematics from [[Brooklyn College]], and a Ph.D. from [[Harvard University|Harvard]] in [[Applied Mathematics]] in 1958, the last of [[Howard Aiken]]'s doctoral students, and taught there until 1965, when he joined [[Cornell University]] and co-founded its department of Computer Science.

Salton was perhaps most well known for developing the now widely used [[vector space model]] for Information Retrieval.&lt;ref&gt;{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Wong | first2 = A. | last3 = Yang | first3 = C. S. | doi = 10.1145/361219.361220 | title = A vector space model for automatic indexing | journal = Communications of the ACM | volume = 18 | issue = 11 | pages = 613 | year = 1975 | pmid =  | pmc = }}&lt;/ref&gt;  In this model, both documents and queries are represented as vectors of term counts, and the similarity between a document and a query is given by the cosine between the term vector and the document vector.  In this paper, he also introduced [[TF-IDF]], or term-frequency-inverse-document frequency, a model in which the score of a term in a document is the ratio of the number of terms in that document divided by the frequency of the number of documents in which that term occurs. (The concept of inverse document frequency, a measure of specificity, had been introduced in 1972 by [[Karen Spärck Jones|Karen Sparck-Jones]].&lt;ref&gt;{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11–21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}&lt;/ref&gt;) Later in life, he became interested in automatic text summarization and analysis,&lt;ref&gt;{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Allan | first2 = J. | last3 = Buckley | first3 = C. | last4 = Singhal | first4 = A. | title = Automatic Analysis, Theme Generation, and Summarization of Machine-Readable Texts | doi = 10.1126/science.264.5164.1421 | journal = Science | volume = 264 | issue = 5164 | pages = 1421–1426 | year = 1994 | pmid =  17838425| pmc = }}&lt;/ref&gt; as well as automatic hypertext generation.&lt;ref&gt;{{cite web|url=http://www.cs.cornell.edu/Info/Department/Annual95/Faculty/Salton.html |title=Gerard Salton |publisher=Cs.cornell.edu |date= |accessdate=2013-09-14}}&lt;/ref&gt;  He published over 150 research articles and 5 books during his life.

Salton was editor-in-chief of the [[Communications of the ACM]] and the [[Journal of the ACM]], and chaired [[Special Interest Group on Information Retrieval]] (SIGIR).  He was an associate editor of the [[ACM Transactions on Information Systems]]. He was an [[List of Fellows of the Association for Computing Machinery|ACM Fellow]] (elected 1995),&lt;ref name=fellow-acm&gt;{{cite web |url=http://awards.acm.org/award_winners/salton_2316166.cfm |title=Gerard Salton ACM Fellows  1995 |last1= |first1= |last2= |first2= |date= |website=acm.org |publisher= |quote=contributions over 30 years to information organization and retrieval |access-date=10 March 2015}}&lt;/ref&gt; received an Award of Merit from the [[American Society for Information Science]] (1989), and was the first recipient of the SIGIR Award for outstanding contributions to study of information retrieval (1983) -- now called the [[Gerard Salton Award]].

==Bibliography==
*Salton, ''Automatic Information Organization and Retrieval'', 1968.
*{{cite book
 | author     = Gerard Salton
 | title      = A Theory of Indexing
 | publisher  = Society for Industrial and Applied Mathematics
 | year       = 1975
 | page      = 56
}}
*--- and Michael J. McGill, ''Introduction to modern information retrieval'', 1983.  ISBN 0-07-054484-0
*{{cite book
 | author     = Gerard Salton
 | title      = Automatic Text Processing
 | publisher  = Addison-Wesley Publishing Company
 | year       = 1989
 | page      = 530
 | isbn       = 0-201-12227-8
}}
*[http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salton:Gerard.html DBLP Bibliography]
* G. Salton, A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(Article in which a vector space model was presented)''

==See also==
* [[List of pioneers in computer science]]

==References==
{{Reflist}}

==External links==
* [http://www.cs.cornell.edu/Info/Department/Annual96/Beginning/salton.html In Memoriam]
* [http://blog.tomevslin.com/2006/01/search_down_mem.html Fractals of Change: Search Down Memory Lane]
* [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] - This 2004 Library Trends paper by David Dubin serves as a historical review of the metamorphosis of the term discrimination value model (TDV) into the vector space model as an information retrieval model (VSM as an IR model). This paper calls into question what the Information Retrieval research community believed Salton's vector space model was originally intended to model. What much later became an information retrieval model was originally a data-centric mathematical–computational model used as an explanatory device. In addition, Dubin's paper points out that a 1975 Salton paper oft cited does not exist but is probably a combination of two other papers, neither of which actually refers to the VSM as an IR model.

{{Authority control}}

{{DEFAULTSORT:Salton, Gerard}}
[[Category:1927 births]]
[[Category:1995 deaths]]
[[Category:American computer scientists]]
[[Category:Harvard University alumni]]
[[Category:Harvard University faculty]]
[[Category:Cornell University faculty]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Guggenheim Fellows]]
[[Category:Information retrieval researchers]]</text>
      <sha1>eerm70yrnhtib3iex8p9gy1s3t2bkk1</sha1>
    </revision>
  </page>
  <page>
    <title>Cyril Cleverdon</title>
    <ns>0</ns>
    <id>20632884</id>
    <revision>
      <id>754009445</id>
      <parentid>735921474</parentid>
      <timestamp>2016-12-10T10:19:39Z</timestamp>
      <contributor>
        <username>Sfan00 IMG</username>
        <id>4906524</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4861" xml:space="preserve">{{Infobox scientist
| name = Cyril Cleverdon
| image =
| caption = 
| birth_date = {{birth date|1914|9|9|df=y}}
| birth_place = [[Bristol]], [[United Kingdom|UK]]
| death_date = {{death date and age|1997|12|4|1914|9|9|df=y}}
| death_place = [[Cranfield]], [[United Kingdom|UK]]
| residence = United Kingdom
| nationality = British
| field = Computer Science
| work_institution = [[Cranfield Institute of Technology]]
| known_for  = work on the evaluation of information retrieval systems
| prizes = Professional Award of the Special Libraries Association (1962), Award of Merit of the American Society for Information Science (1971), The [[Gerard Salton Award]] of the Special Interest Group on Information Retrieval of the Association for Computing Machinery (1991)
}}
'''Cyril Cleverdon''' (9 September 1914 – 4 December 1997) was a [[United Kingdom|British]] librarian and computer scientist who is best known for his work on the evaluation of [[information retrieval]] systems.

Cyril Cleverdon was born in [[Bristol]], [[England]]. He worked at the Bristol Libraries from 1932 to 1938, and from 1938 to 1946 he was the librarian of the Engine Division of the Bristol Aeroplane Co. Ltd. In 1946 he was appointed librarian of the College of Aeronautics at Cranfield (later the [[Cranfield Institute of Technology]] and [[Cranfield University]]), where he served until his retirement in 1979, the last two years as professor of Information Transfer Studies.

With the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:
# a collection of documents,
# a set of user requests or queries, and 
# a set of relevance judgments—that is, a set of documents judged to be [[Relevance (information retrieval)|relevant]] to each query. 
Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: [[Precision (information retrieval)|precision]] and [[Recall (information retrieval)|recall]]. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful [[Text Retrieval Conference]] series that began in 1992.

Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the [[automatic indexing]] as done in today's [[search engine]]s. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:

''This conclusion is so controversial and so unexpected that it is bound to throw considerable doubt on the methods which have been used (...) A complete recheck has failed to reveal any discrepancies (...) there is no other course except to attempt to explain the results which seem to offend against every canon on which we were trained as librarians.'' 

Cyril Cleverdon also ran, for many years, the Cranfield conferences, which provided a major international forum for discussion of ideas and research in information retrieval. This function was taken over by the [[Special Interest Group on Information Retrieval|SIGIR]] conferences in the 1970s.

==References==
* {{cite journal|author=Cyril Cleverdon|title=Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems|publisher=The College of Aeronautics, Cranfield|year=1960|url=http://www.sigir.org/museum/pdfs/Report_on_the_Testing_and_Analysis_of_an_Investigation_Into_the_Comparative_Efficiency_of_Indexing_Systems/pdfs/frontmatter.pdf}}
* Cyril Cleverdon and Michael Keen, Factors Determining the Performance of Indexing Systems, Volume 2, ''The College of Aeronautics, Cranfield'', 1966
* Stephen Robertson, In Memoriam Cyril W. Cleverdon, ''Journal of the American Society for Information Science 49''(10):866, 1998

{{DEFAULTSORT:Cleverdon, Cyril}}
[[Category:1914 births]]
[[Category:1997 deaths]]
[[Category:British computer scientists]]
[[Category:English librarians]]
[[Category:People associated with Cranfield University]]
[[Category:People from Bristol]]
[[Category:Information retrieval researchers]]</text>
      <sha1>ebt0x6rdxbqj1hwyon4ae7l6xhrcv04</sha1>
    </revision>
  </page>
  <page>
    <title>Susan Dumais</title>
    <ns>0</ns>
    <id>2232087</id>
    <revision>
      <id>736646137</id>
      <parentid>706527914</parentid>
      <timestamp>2016-08-28T22:37:23Z</timestamp>
      <contributor>
        <username>GorillaWarfare</username>
        <id>4968133</id>
      </contributor>
      <comment>not a stub</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5618" xml:space="preserve">{{ Infobox scientist
| name              = Susan T. Dumais
| image             = Susan Dumais.jpg
| image_size        = 200px
| caption           = Susan Dumais in 2009 in her office at Microsoft Research.
| birth_date        = 
| birth_place       = [[Maine]], [[United States|US]]  
| death_date        = 
| death_place       = 
| nationality       = American
| fields            = [[Computer Science]]
| workplaces        = [[Microsoft Research]]
| alma_mater        = [[Indiana University]] &lt;br /&gt;[[Bates College]]
| doctoral_advisor  = 
| doctoral_students = 
| known_for         = [[Human Computer Interaction]]&lt;br /&gt; [[Information Retrieval]]
| website           = {{URL|http://research.microsoft.com/~sdumais/}} 
| awards            = ACM-W Athena Lecturer Award (2014)
}}

'''Susan Dumais''' is an American computer scientist who is a leader in the field of information retrieval, and has been a significant contributor to Microsoft's search technologies.&lt;ref&gt;{{cite news|title=100 Top Women in Seattle Tech|url=http://www.bizjournals.com/seattle/blog/techflash/2009/05/Top_100_Women_in_Seattle_Tech_44225472.html|accessdate=23 February 2016|newspaper=Puget Sound Business Journal|date=8 May 2009}}&lt;/ref&gt;
According to Mary Jane Irwin, who heads the Athena Lecture awards committee, “Her sustained contributions have shaped the thinking and direction of human-computer interaction and information retrieval."&lt;ref&gt;{{cite news|last=Burns|first=Jay|title=Microsoft’s Susan Dumais ’75 Is a Big Reason Why, Computer-Wise, You Find What You Seek|url=https://www.bates.edu/news/2014/05/01/microsoft-susan-dumais-75/|accessdate=23 February 2016|newspaper=Bates News|date=28 October 2015}}&lt;/ref&gt;

==Biography==

Susan Dumais is a Distinguished Scientist at Microsoft and deputy managing director of the [[Microsoft Research]] lab in Redmond. She is also an Affiliate Professor at the [[University of Washington Information School]].

Before joining Microsoft in 1997, Dumais was a researcher at Bellcore (now [[Telcordia Technologies]]), where she and her colleagues conducted research into what is now called the [[vocabulary problem]] in [[information retrieval]].&lt;ref&gt;{{cite journal
| title=The Vocabulary Problem in Human-System Communication
| journal=Communications of the ACM
| author=[[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], L. M. Gomez, S. T. Dumais
| volume = 30
| pages = 964–971
| year = 1987
| url = http://citeseer.ist.psu.edu/furnas87vocabulary.html
| doi=10.1145/32206.32212
| issue = 11
}}&lt;/ref&gt; Their study demonstrated, through a variety of experiments, that different people use different vocabulary to describe the same thing, and that even choosing the "best" term to describe something is not enough for others to find it.  One implication of this work is that because the author of a document may use different vocabulary than someone searching for the document, traditional [[information retrieval]] methods will have limited success.

Dumais and the other Bellcore researchers then began investigating ways to build search systems that avoided the vocabulary problem.  The result was their invention of [[Latent Semantic Indexing]].&lt;ref&gt;
 {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 | title=Indexing by Latent Semantic Analysis
 | author=[[Scott Deerwester|S. Deerwester]], Susan Dumais, [[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], [[Richard Harshman|R. Harshman]]
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391–407
 |year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9
}}&lt;/ref&gt;

==Awards==

In 2006, Dumais was inducted as a [[Fellow]] of the [[Association for Computing Machinery]]. In 2009, she received the [[Gerard Salton Award]], an information retrieval lifetime achievement award. In 2011, she was inducted to the [[National Academy of Engineering]] for innovation and leadership in organizing, accessing, and interacting with information. In 2014, Dumais received the Athena Lecturer Award for "fundamental contributions to computer science.".&lt;ref&gt;{{cite news|last=Knies|first=Rob|title=Dumais Receives Athena Lecturer Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/08/dumais-receives-athena-lecturer-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}&lt;/ref&gt; and the [[Tony Strix]] Award for "sustained contributions that are both innovative and practical" with "significant impact". &lt;ref&gt;{{cite web|title=The winner of the 2014 Tony Kent Strix Award is Dr Susan Dumais|url=http://www.ukeig.org.uk/awards/tony-kent-strix|accessdate=17 September 2014}}&lt;/ref&gt;
In 2015, she was inducted into the [[American Academy of Arts and Sciences]].&lt;ref&gt;{{cite news|last=Tice|first=Lindsay|title=Lewiston native inducted into American Academy of Arts and Sciences|url=http://www.sunjournal.com/news/lewiston-auburn/0001/11/30/lewiston-native-inducted-american-academy-arts-and-sciences/1808943|accessdate=23 February 2016|newspaper=Lewinston-Auburn Sun-Journal|date=28 October 2015}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://research.microsoft.com/~sdumais/ Home page at Microsoft Research]

{{DEFAULTSORT:Dumais, Susan}}
[[Category:People in information technology]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Microsoft employees]]
[[Category:Living people]]
[[Category:Women computer scientists]]
[[Category:University of Washington faculty]]
[[Category:Information retrieval researchers]]</text>
      <sha1>2ykimtt87um12yxv97oszb6vnqa2xx5</sha1>
    </revision>
  </page>
  <page>
    <title>Jaime Teevan</title>
    <ns>0</ns>
    <id>31120153</id>
    <revision>
      <id>759866842</id>
      <parentid>759866553</parentid>
      <timestamp>2017-01-13T16:05:04Z</timestamp>
      <contributor>
        <ip>50.47.104.39</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4739" xml:space="preserve">{{Infobox scientist
| name        = Jaime Teevan
| image       = 
| caption     =
| birth_date  = {{Birth year and age|1976}}
| birth_place = 
| death_date  =
| death_place =
| nationality = 
| residence   = 
| fields      = [[Computer science]]&lt;br/ &gt;[[Human-Computer Interaction]]&lt;br/ &gt;[[Information Retrieval]]
| work_institution = [[Microsoft Research]]
| alma_mater  = [[Massachusetts Institute of Technology]]&lt;br/&gt;[[Yale University]]
| known_for   =
| doctoral_advisor = [[David Karger]]
| awards = [[TR35]] (2009)&lt;br/ &gt;Borg Early Career Award (2014)&lt;br/ &gt;[[Karen Spärck Jones]] Award (2016)
| website           = {{URL|http://teevan.org}} 
}}

'''Jaime Teevan''' is an American [[computer scientist]] known for her research in [[human-computer interaction]] and [[information retrieval]]. She is particularly known for the work she has done on [[personalized search]]. According to the [[Technology Review]], Teevan "is a leader in using data about people's knowledge, preferences, and habits to help them manage information.&lt;ref name="tr35"&gt;{{cite news|last=Kleiner|first=Kurt|title=TR35: Jaime Teevan, 32|url=http://www.technologyreview.com/tr35/Profile.aspx?Cand=T&amp;TRID=778|accessdate=10 March 2011|newspaper=Technology Review|date=August 2009}}&lt;/ref&gt;"

==Biography==
Teevan received and a [[Bachelor of Science|B.S.]] in [[Computer Science]] from [[Yale University]] and a Ph.D. and S.M. from [[MIT]].&lt;ref&gt;http://www.csail.mit.edu/~teevan/work/publications/theses/phd/thesis.pdf&lt;/ref&gt;&lt;ref&gt;http://www.csail.mit.edu/~teevan/work/publications/theses/masters/thesis.pdf&lt;/ref&gt;

She is currently a researcher at [[Microsoft Research]] and an affiliate professor at the [[University of Washington]]. There she co-authored the first book on [[collaborative information seeking]],.&lt;ref&gt;{{cite book|last=Morris|first=Meredith Ringel and Teevan, Jaime|title=Collaborative Search: Who, What, Where, When, Why, and How|year=2010|publisher=Morgan and Claypool Publishers|isbn=1-60845-121-6|url=http://www.amazon.com/dp/1608451216/}}&lt;/ref&gt; She also edited a book on [[Personal Information Management]] (PIM),&lt;ref&gt;{{cite book|editor=Jones, William |editor2=Teevan, Jaime|title=Personal Information Management|year=2007|publisher=University of Washington Press|isbn=0-295-98737-5|url=http://www.amazon.com/dp/0295987375}}&lt;/ref&gt; 
edited a special issue of Communications of the ACM on the topic, and organized workshops on [[PIM (software)|PIM]] and query log analysis. She has published numerous technical papers, including several best papers, and was chair of the Web Search and Data Mining (WSDM) 2012 conference.

==Awards==

Teevan was named a Technology Review ([[TR35]]) 2009 Young Innovator for her research on [[personalized search]]&lt;ref name="tr35" /&gt; and received the [[CRA-W]] Borg Early Career Award (BECA) in 2014.&lt;ref name="borg"&gt;{{cite news|last=Knies|first=Rob|title=Researcher Teevan Wins Borg Early Career Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/22/researcher-teevan-wins-borg-early-career-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}&lt;/ref&gt; In 2016 she received the [[Karen Spärck Jones]] award from the [[British Computer Society]] for her "technically strong and exceptionally creative contributions to the intersection of information retrieval, user experience and social media." &lt;ref&gt;http://irsg.bcs.org/ksjaward.php&lt;/ref&gt;

==Personal==
Teevan is married to Alexander Hehmeyer.&lt;ref&gt;{{cite news|title=WEDDINGS; Jaime Teevan, Alexander Hehmeyer|url=http://www.nytimes.com/2002/06/16/style/weddings-jaime-teevan-alexander-hehmeyer.html|accessdate=14 September 2015|newspaper=New York Times|date=June 16, 2002}}&lt;/ref&gt;
The couple live in [[Bellevue, Washington]]
and have four children.&lt;ref&gt;{{cite news|last=Vanderkam|first=Laura|title=Women with Big Jobs and Big Families: Balancing Really Isn't That Hard|url=http://fortune.com/2015/06/06/women-with-big-jobs-and-big-families-balancing-really-isnt-that-hard/|accessdate=14 September 2015|newspaper=Fortune|date=6 June 2015}}&lt;/ref&gt;
Teevan is an advocate for helping researchers successfully integrate parenthood and academic efforts.&lt;ref name="borg" /&gt;

==References==
&lt;references /&gt;

==External links==
* [http://teevan.org/ Professional home page]

{{DEFAULTSORT:Teevan, Jaime}}
[[Category:People in information technology]]
[[Category:Information retrieval researchers]]
[[Category:Human–computer interaction researchers]]
[[Category:Women computer scientists]]
[[Category:Microsoft employees]]
[[Category:Living people]]
[[Category:Yale University alumni]]
[[Category:Massachusetts Institute of Technology alumni]]
[[Category:University of Washington faculty]]
[[Category:1976 births]]</text>
      <sha1>j36ley9ctdjqvsmjwps47aqnhf0g9zw</sha1>
    </revision>
  </page>
  <page>
    <title>Cutter Expansive Classification</title>
    <ns>0</ns>
    <id>7515</id>
    <revision>
      <id>762354927</id>
      <parentid>762265323</parentid>
      <timestamp>2017-01-28T07:44:26Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14317" xml:space="preserve">The '''Cutter Expansive Classification''' system is a [[library classification]] system devised by [[Charles Ammi Cutter]]. The system was the basis for the top categories of the [[Library of Congress Classification]].&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 226.&lt;/ref&gt;

==History of the Expansive Classification==
[[Charles Ammi Cutter]] (1837&amp;ndash;1903), inspired by the decimal classification of his contemporary [[Melvil Dewey]], and with Dewey's initial encouragement, developed his own classification scheme for the Winchester Town Library and then the [[Boston Athenaeum]],&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 208.&lt;/ref&gt; at which he served as librarian for twenty-four years. He began work on it around the year 1880, publishing an overview of the new system in 1882. The same classification would later be used, but with a different notation, also devised by Cutter, at the [[Cary Memorial Library|Cary Library]] in [[Lexington, Massachusetts]].&lt;ref&gt;Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93, p. 1.&lt;/ref&gt;

Many libraries found this system too detailed and complex for their needs, and Cutter received many requests from librarians at small libraries who wanted the classification adapted for their collections. He devised the Expansive Classification in response, to meet the needs of growing libraries, and to address some of the complaints of his critics
.&lt;ref&gt;For the Expansive Classification as a response to Cutter's critics, see: Miksa, Francis L., ed. ''Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.
* For the Expansive Classification as a response to the growing needs of libraries, see Miksa, above, and also: LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 209.
* The above issues are also discussed by Cutter in his [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93.
&lt;/ref&gt; Cutter completed and published an introduction and schedules for the first six classifications of his new system ([https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']), but his work on the seventh was interrupted by his death in 1903.&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 210.&lt;/ref&gt;

The Cutter Expansive Classification, although adopted by comparatively few libraries, mostly in [[New England]]{{Citation needed|date=August 2011}}, has been called one of the most logical and scholarly of American classifications{{Citation needed|date=August 2011}}. Library historian Leo E. LaMontagne writes:

&lt;blockquote&gt;Cutter produced the best classification of the nineteenth century. While his system was less "scientific" than that of [[J. P. Lesley]], its other key features – notation, specificity, and versatility – make it deserving of the praise it has received.&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 215&lt;/ref&gt;&lt;/blockquote&gt;

Its top level divisions served as a basis for the Library of Congress classification, which also took over some of its features.&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, Connecticut, Shoe String Press. 1961, p. 226.&lt;/ref&gt; It did not catch on as did Dewey's system because Cutter died before it was completely finished, making no provision for the kind of development necessary as the bounds of knowledge expanded and scholarly emphases changed throughout the twentieth century.&lt;ref&gt;https://journals.ala.org/index.php/lrts/article/view/5419/6654&lt;/ref&gt;

==Structure of the Expansive Classification==
The Expansive Classification uses seven separate schedules, each designed to be used by libraries of different sizes. After the first, each schedule was an expansion of the previous one,&lt;ref&gt;Miksa, Francis L., ed. ''Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.&lt;/ref&gt; and Cutter provided instructions for how a library might change from one expansion to another as it grows.&lt;ref&gt;Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93, p. 21–23.&lt;/ref&gt;

==Summary of the Expansive Classification Schedules==

===First Classification===
The first classification is meant for only the very smallest libraries. The first classification has only seven top level classes, and only eight classes in all.

* '''A''' Works of reference and general works which include several of the following sections, and so could not go in any one.
* '''B''' [[Outline of philosophy|Philosophy]] and [[Outline of religion|Religion]]
* '''E''' Biography
* '''F''' [[Outline of history|History]] and [[Outline of geography|Geography]] and Travels
* '''H''' [[Outline of social science|Social sciences]]
* '''L''' [[Outline of natural science|Natural sciences]] and [[The arts|Arts]]
* '''Y''' [[Outline of linguistics|Language]] and [[Outline of literature|Literature]]
* '''YF''' [[Outline of fiction|Fiction]]

===Further Classifications===
Further expansions add more top level classes and subdivisions. Many subclasses arranged systematically, with common divisions, such as those by geography and language, following a consistent system throughout.&lt;ref&gt;https://archive.org/details/cu31924092476229&lt;/ref&gt;

By the fifth classification all the letters of the alphabet are in use for top level classes. These are:

* '''A''' General Works
* '''B''' [[Outline of philosophy|Philosophy]]
* '''C''' [[Outline of Christianity|Christianity]] and [[Outline of Judaism|Judaism]]
* '''D''' Ecclesiastical History
* '''E''' Biography
* '''F''' [[Outline of history|History]], Universal History
* '''G''' [[Outline of geography|Geography]] and Travels
* '''H''' [[Outline of social science|Social Sciences]]
* '''I''' Demotics, [[Outline of sociology|Sociology]]
* '''J''' Civics, Government, [[Outline of political science|Political Science]]
* '''K''' Legislation
* '''L''' [[Outline of science|Science]] and [[The arts|Arts]] together
* '''M''' Natural History
* '''N''' [[Outline of botany|Botany]]
* '''O''' [[Outline of zoology|Zoölogy]]
* '''P''' [[Outline of anthropology|Anthropology]] and Ethnology
* '''Q''' [[Outline of medicine|Medicine]]
* '''R''' Useful arts, [[Outline of technology|Technology]]
* '''S''' Constructive arts ([[Outline of engineering|Engineering]] and [[Outline of construction|Building]])
* '''T''' [[Outline of manufacturing|Manufactures]] and Handicrafts
* '''U''' [[Outline of military science and technology|Art of War]]
* '''V''' Recreative arts, [[Outline of sports|Sports]], [[Outline of games|Games]], [[Outline of festivals|Festivals]]
* '''W''' [[Outline of the visual arts|Art]]
* '''X''' English Language
* '''Y''' English and American literature
* '''Z''' Book arts

These schedules were not meant to be fixed, but were to be adapted to meet the needs of each library. For example, books on the English language may be put in X, and books on language in general in a subclass of X, or this can be reversed. The first option is less logical, but results in shorter marks for most English language libraries.&lt;ref&gt;Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93, p. 27.&lt;/ref&gt;

==How Expansive Classification call numbers are constructed==
{{Expand section|citations and corrections|date=August 2011}}
Most call numbers in the Expansive Classification follow conventions offering clues to the book's subject. The first line represents the subject, the second the author (and perhaps title), the third and fourth dates of editions, indications of translations, and critical works on particular books or authors. All numbers in the Expansive Classification are (or should be) shelved as if in decimal order.

Size of volumes is indicated by points (.), pluses (+), or slashes (/ or //).

For some subjects a numerical geographical subdivision follows the classification letters on the first line. The number 83 stands for the United States&amp;mdash;hence, F83 is U.S. history, G83 U.S. travel, JU83 U.S. politics, WP83 U.S. painting. Geographical numbers are often further expanded decimally to represent more specific areas, sometimes followed by a capital letter indicating a particular city.

The second line usually represents the author's name by a capital letter plus one or more numbers arranged decimally. This may be followed by the first letter or letters of the title in lower-case, and/or sometimes the letters a,b,c indicating other printings of the same title. When appropriate, the second line may begin with a 'form' number&amp;mdash;e.g., 1 stands for history and criticism of a subject, 2 for a bibliography, 5 for a dictionary, 6 for an atlas or maps, 7 for a periodical, 8 for a society or university publication, 9 for a collection of works by different authors.

On the third line a capital Y indicates a work about the author or book represented by the first two lines, and a capital E (for English&amp;mdash;other letters are used for other languages) indicates a translation into English. If both criticism and translation apply to a single title, the number expands into four lines.

=== Cutter Numbers (Cutter Codes) ===
{{Expand section|examples and additional citations|date=August 2011}}
One of the features adopted by other systems, including Library of Congress, is the Cutter number. It is an alphanumeric device to code text so that it can be arranged in alphabetical order using the fewest characters. It contains one or two initial letters and Arabic numbers, treated as a decimal. To construct a Cutter number, a cataloguer consults a Cutter table as required by the classification rules.  Although Cutter numbers are mostly used for coding the names of authors, the system can be used for titles, subjects, geographic areas, and more.

{| class=wikitable
|+Cutter table
|-
! || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9
|-
| S || a || ch || e || h-i || m-p || t || u || w-z
|-
| Qu || || a || e || i || o || r || t || y
|-
| other consonants || || a || e || i || o || r || u || y
|-
| vowels || b || d || l-m || n || p || r || s-t || u-y
|-
| additional letters || || a-d || e-h || i-l || m-o || p-s || t-v || w-z
|}

Initial letters Qa-Qt are assigned Q2-Q29, while entries beginning with numerals have a Cutter number A12-A19, therefore sorting before the first A entry.&lt;ref&gt;{{cite web|title=LC Cutter Tables |url=http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |website=Queen Elizabeth II Libraries |publisher=Memorial University of Newfoundland |accessdate=14 August 2014 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20140814173419/http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |archivedate=14 August 2014 |df= }}&lt;/ref&gt;

So to make the three digit Cutter number for "Cutter", you would start with "C", then looking under ''other consonants'', find that "u" gives the number 8, and under ''additional letters'', "t" is 8, giving a Cutter number of "C88".

==Notes==
{{reflist}}

==References==
* Bliss, Henry Evelyn. ''The Organization of Knowledge in Libraries: and the Subject-Approach to Books'', 2nd ed. New York: H. W. Wilson, 1939.
* Cutter, Charles A. ''Rules for a Dictionary Catalog''. W. P. Cutter, ed. 4th ed. Washington, D.C.: Government Printing Office, 1904. London: The Library Association, 1962.
* Cutter, William Parker. ''Charles Ammi Cutter''. Chicago: American Library Association, 1931. Ann Arbor, MI: University Microfilms, 1969.
* Foster, William E. "Charles Ammi Cutter: A Memorial Sketch". ''Library Journal'' 28 (1903): 697-704.
* Hufford, Jon R. "The Pragmatic Basis of Catalog Codes: Has the User Been Ignored?". ''Cataloging and Classification Quarterly'' 14 (1991): 27-38.
* Immroth, John Philip. "Cutter, Charles Ammi". ''Encyclopedia of Library and Information Science''. [[Allen Kent]] and Harold Lancour, ed. 47 vols. New York, M. Dekker [1968- ]
* LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961.
*Slavis, Dobrica. "CUTT-x: An Expert System for Automatic Assignment of Cutter Numbers". ''Cataloging and Classification Quarterly''. Vol 22, no. 2, 1996.
* Tauber, Maurice F., and Edith Wise. "Classification Systems". [[Ralph R. Shaw (Librarian)|Ralph R. Shaw]], ed.. ''The State of the Library Art''. New Brunswick, NJ: Rutgers U. Graduate School of Library Service, 1961. 1-528.

==External links==
* [http://catalog.bostonathenaeum.org/cutterguide.html The Boston Athenaeum's Guide to the classification system developed by Cutter for their collection]
* [http://www.forbeslibrary.org/research/index.php?n=Main.CutterClassification Forbes Library's Outline of Cutter's Expansive Classification system]
* [http://www.forbeslibrary.org/pathfinders/Shelvingrules.pdf A brief guide to the Expansive Classification from Forbes Library]
* [http://digital.library.unt.edu/permalink/meta-dc-1048:1 ''Rules for a dictionary catalog, by Charles A. Cutter, fourth edition''], hosted by the [http://digital.library.unt.edu/ UNT Libraries Digital Collections]
* [http://www.loc.gov/aba/pcc/053/table.html Library of Congress Guidelines for using the LC Online Shelflist and formulating a literary author number: Cutter Table]
* [http://www.oclc.org/dewey/support/program/default.htm Dewey Cutter Program]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>5a3a5s0nwdmxthbqyoho2rjko6qdx18</sha1>
    </revision>
  </page>
  <page>
    <title>Colon classification</title>
    <ns>0</ns>
    <id>6888</id>
    <revision>
      <id>749094918</id>
      <parentid>749094878</parentid>
      <timestamp>2016-11-12T10:38:48Z</timestamp>
      <contributor>
        <username>Jim1138</username>
        <id>7695475</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/14.139.243.34|14.139.243.34]] ([[User talk:14.139.243.34|talk]]): Editing tests ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5042" xml:space="preserve">'''Colon classification''' ('''CC''') is a system of [[library classification]] developed by [[S. R. Ranganathan]]. It was the first ever [[Faceted classification|faceted]] (or analytico-synthetic) [[Taxonomic classification|classification]]. The first edition was published in 1933. Since then six more editions have been published. It is especially used in [[library|libraries]] in [[India]].

Its name "colon classification" comes from the use of [[Colon (punctuation)|colons]] to separate facets in class numbers. However, many other classification schemes, some of which are completely unrelated, also use colons and other [[punctuation]] in various functions. They should not be confused with colon classification.

In CC, facets describe "personality" (the most specific subject), matter, energy, space, and time (PMEST).  These facets are generally associated with every item in a library, and so form a reasonably universal sorting system.&lt;ref&gt;GOPINATH (M A). Colon classification: Its theory and practice.
Library Herald
.
26, 1
-
2; 1987; 1
-
3.&lt;/ref&gt;

As an example, the subject "research in the cure of tuberculosis of lungs by x-ray conducted in India in 1950" would be categorized as:

:Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India'1950

This is summarized in a specific call number:

:L,45;421:6;253:f.44'N5

== Organization ==

The colon classification uses 42 main classes that are combined with other letters, numbers and marks in a manner resembling the [[Library of Congress Classification]] to sort a publication.

=== Facets ===

CC uses five primary categories, or facets, to further specify the sorting of a publication. Collectively, they are called ''PMEST'':

:&lt;nowiki&gt;,&lt;/nowiki&gt; Personality, the most specific or focal subject.
:&lt;nowiki&gt;;&lt;/nowiki&gt; Matter or property, the substance, properties or materials of the subject.
:&lt;nowiki&gt;:&lt;/nowiki&gt; Energy, including the processes, operations and activities.
:&lt;nowiki&gt;.&lt;/nowiki&gt; Space, which relates to the geographic location of the subject.
:&lt;nowiki&gt;'&lt;/nowiki&gt; Time, which refers to the dates or seasons of the subject.

=== Classes ===

The following are the main classes of CC, with some subclasses, the main method used to sort the subclass using the PMEST scheme and examples showing application of PMEST.

:z Generalia
:1 Universe of Knowledge
:2 [[Library Science]]
:3 Book science
:4 [[Journalism]]
:B [[Mathematics]]
::B2 [[Algebra]]
:C [[Physics]]
:D [[Engineering]]
:E [[Chemistry]]
:F [[Technology]]
:G [[Biology]]
:H [[Geology]]
::HX [[Mining]]
:I [[Botany]]
:J [[Agriculture]]
::J1 [[Horticulture]]
::J2 Feed
::J3 Food
::J4 Stimulant
::J5 Oil
::J6 Drug
::J7 Fabric
::J8 Dye
:K [[Zoology]] 
::KZ Animal Husbandry 
:L Medicine
::LZ3 [[Pharmacology]]
::LZ5 [[Pharmacopoeia]]
:M [[Useful arts]]
::M7 Textiles ''[material]:[work]''
:Δ Spiritual experience and [[mysticism]] ''[religion],[entity]:[problem]''
:N [[Fine arts]]
::ND Sculpture
::NN Engraving
::NQ Painting
::NR Music
:O Literature
:P Linguistics
:Q [[Religion]]
:R [[Philosophy]]
:S [[Psychology]]
:T [[Education]]
:U [[Geography]]
:V [[History]]
:W [[Political science]]
:X [[Economics]]
:Y [[Sociology]]
:YZ [[Social Work]]
:Z [[Law]]

== Example ==

A common example of the colon classification is:

* "Research in the cure of the tuberculosis of lungs by x-ray conducted in India in 1950s":
* Main classification is Medicine
** (Medicine)
* Within Medicine, the Lungs are the main concern
** (Medicine,Lungs)
* The property of the Lungs is that they are afflicted with Tuberculosis
** (Medicine,Lungs;Tuberculosis)
* The Tuberculosis is being performed (:) on, that is the intent is to cure (Treatment)
** (Medicine,Lungs;Tuberculosis:Treatment)
* The matter that we are treating the Tuberculosis with are X-Rays
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray)
* And this discussion of treatment is regarding the Research phase
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research)
* This Research is performed within a geographical space (.) namely India
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India)
* During the time (') of 1950
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India'1950)
* And translating into the codes listed for each subject and facet the classification becomes
** L,45;421:6;253:f.44'N5

==See also==
*[[Bliss bibliographic classification]]
*[[Subject (documents)]]
*[[Universal Decimal Classification]]

== References ==
{{Reflist|2}}
* [http://www.essessreference.com/servlet/esGetBiblio?bno=000374 ''Colon Classification'' (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Ess Publications, Delhi, India
* Chan, Lois Mai. ''Cataloging and Classification: An Introduction''. 2nd ed. New York: McGraw-Hill, c1994. ISBN 0-07-010506-5.

==External links==
* [http://www.iskoi.org/doc/colon.htm More Detail about the Colon Classification at ISKO Italia]

{{Library classification systems}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>r8ncyrbwmp80w1hg7f5zd35d2cyi0o1</sha1>
    </revision>
  </page>
  <page>
    <title>Nippon Decimal Classification</title>
    <ns>0</ns>
    <id>2471086</id>
    <revision>
      <id>688030250</id>
      <parentid>688003149</parentid>
      <timestamp>2015-10-29T05:11:53Z</timestamp>
      <contributor>
        <username>Alex Sims</username>
        <id>240189</id>
      </contributor>
      <comment>Unsure what was meant, but not correct. Undid revision 562404720 by [[Special:Contributions/Mukeshwar|Mukeshwar]] ([[User talk:Mukeshwar|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4276" xml:space="preserve">The '''Nippon Decimal Classification''' ('''NDC''', also called the '''Nippon Decimal System''') is a system of [[library classification]] developed for mainly Japanese language books maintained by the Japan Library Association since 1956. It is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. The system is based upon using each successive digit to divide into nine divisions with the digit zero used for those not belonging to any of the divisions.

== Main classes ==
The system is made up of ten categories:

* 000 General
* 100 [[Philosophy]]
* 200 [[History]]
* 300 [[Social sciences]]
* 400 [[Natural sciences]]
* 500 [[Technology]] and [[engineering]]
* 600 [[Industry]] and [[commerce]]
* 700 [[Arts]]
* 800 [[Language]]
* 900 [[Literature]]

== Description of the classes ==

*000 General
**010 [[Libraries]], [[Library and information science|Library &amp; information science]]
**020 [[Books]], [[Bibliography]]
**030 [[Encyclopaedia]]s
**040 General collected [[essays]]
**050 General [[Periodical literature|serial publications]]
**060 [[Organizations]]
**070 [[Journalism]], [[Newspapers]]
**080 General collections
**090 [[Rare books]], Local collections, [[Special collections]]
*100 Philosophy
**110 Special treatises on philosophy
**120 [[Oriental philosophy]]
**130 [[Western philosophy]]
**140 [[Psychology]]
**150 [[Ethics]] &amp; morals
**160 [[Religion]]
**170 [[Shintoism]]
**180 [[Buddhism]]
**190 [[Christianity]]
*200 History
**210 [[History of Japan]]
**220 [[History of Asia]] and the Orient
**230 [[History of Europe]] and the West
**240 [[History of Africa]]
**250 [[History of North America]]
**260 [[History of South America]]
**270 [[History of Oceania]] &amp; [[Polar region]]s
**280 [[Biography]]
**290 [[Geography]], [[Topography]], [[Travel]]
*300 Social Sciences
**310 [[Politics]]
**320 [[Law]]
**330 [[Economics]]
**340 [[Finance]]
**350 [[Statistics]]
**360 [[Sociology]]
**370 [[Education]]
**380 [[Customs]], [[Folklore]], [[Ethnology]]
**390 National defence, [[Military science]]
*400 Natural Sciences
**410 [[Mathematics]]
**420 [[Physics]]
**430 [[Chemistry]]
**440 [[Astronomy]], [[Space science]]
**450 [[Earth science]]
**460 [[Biology]]
**470 [[Botany]]
**480 [[Zoology]]
**490 [[Medicine]], [[Pharmacology]]
*500 Technology &amp; Engineering
**510 [[Construction]], [[Civil engineering]]
**520 [[Architecture]]
**530 [[Mechanical engineering]], [[Nuclear engineering]]
**540 [[Electrical engineering|Electrical]] &amp; [[Electronic engineering]]
**550 Maritime &amp; [[Naval engineer]]ing
**560 Metal &amp; [[Mining engineering]]
**570 [[Chemical technology]]
**580 [[Manufacturing]]
**590 [[Domestic science|Domestic arts and sciences]]
*600 Industry and Commerce
**610 [[Agriculture]]
**620 [[Horticulture]]
**630 [[Sericulture|Silk industry]]
**640 [[Animal husbandry]]
**650 [[Forestry]]
**660 [[Fishing]]
**670 [[Commerce]]
**680 [[Transportation]] &amp; [[Traffic]]
**690 [[Communications]]
*700 Arts
**710 [[Plastic arts]] (sculpture)
**720 [[Painting]] &amp; [[Calligraphy]]
**730 [[Engraving]]
**740 [[Photography]] &amp; [[Printing]]
**750 [[Craft]]
**760 [[Music]] &amp; [[Dance]]
**770 [[Theatre]], [[Motion Pictures]]
**780 [[Sports]], [[Physical Education]]
**790 [[Recreation]], Amusements
*800 Language
**810 [[Japanese language|Japanese]]
**820 [[Chinese language|Chinese]], other [[oriental languages]]
**830 [[English language|English]]
**840 [[German language|German]]
**850 [[French language|French]]
**860 [[Spanish language|Spanish]]
**870 [[Italian language|Italian]]
**880 [[Russian language|Russian]]
**890 Other languages
*900 Literature
**910 [[Japanese literature]]
**920 [[Chinese literature]], Other [[Oriental literature]]
**930 [[English literature|English]] &amp; [[American literature]]
**940 [[German literature]]
**950 [[French literature]]
**960 [[Spanish literature]]
**970 [[Italian literature]]
**980 [[Russian literature|Russian]] &amp; [[Soviet literature]]
**990 Other language literature

== External links ==
*[http://www.jla.or.jp/index-e.html Japan Library Association]
*[http://www.asahi-net.or.jp/~ax2s-kmtn/ref/ndc/e_ndc.html CyberLibrarian]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]</text>
      <sha1>bbjv0pc5tweq5xv3fhjfh6q8od1148o</sha1>
    </revision>
  </page>
  <page>
    <title>Resource Description Framework</title>
    <ns>0</ns>
    <id>53847</id>
    <revision>
      <id>760123809</id>
      <parentid>760123227</parentid>
      <timestamp>2017-01-15T03:01:30Z</timestamp>
      <contributor>
        <username>K6ka</username>
        <id>11801436</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/66.87.152.25|66.87.152.25]] ([[User talk:66.87.152.25|talk]]) to last version by FockeWulf FW 190</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="38557" xml:space="preserve">{{Infobox technology standard
| title             = RDF 1.1 Concepts and Abstract Syntax
| status            = Published, W3C Recommendation
| year_started      = 1997
| editors           = Richard Cyganiak, David Wood, Markus Lanthaler
| base_standards    = [[URI]]
| related_standards = [[RDFS]], [[Web Ontology Language|OWL]], [[Rule Interchange Format|RIF]], [[RDFa]]
| domain            = [[Semantic Web]]
| abbreviation      = RDF
| website           = {{url|http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}
}}

The '''Resource Description Framework''' ('''RDF''') is a family of [[World Wide Web Consortium]] (W3C) [[specification]]s&lt;ref&gt;{{cite web|url=http://www.dblab.ntua.gr/~bikakis/XMLSemanticWebW3CTimeline.pdf |title=XML and Semantic Web W3C Standards Timeline
|date=2012-02-04}}&lt;/ref&gt; originally designed as a [[metadata]] [[data model]]. It has come to be used as a general method for conceptual description or modeling of information that is implemented in [[web resource]]s, using a variety of syntax notations and [[data serialization]] formats. It is also used in [[knowledge management]] applications.

RDF was adopted as a W3C recommendation in 1999. The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.

== Overview ==
The RDF data model&lt;ref&gt;http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework
(RDF) Model and Syntax Specification"&lt;/ref&gt;  is similar to classical conceptual modeling approaches (such as [[entity–relationship model|entity–relationship]] or [[class diagram]]s). It is based upon the idea of making [[statement (programming)|statement]]s about [[resource (computer science)|resource]]s (in particular [[web resource]]s) expressions, known as ''[[Semantic triple|triples]]''.  Triples are so named because they follow a &lt;var&gt;subject&lt;/var&gt;–&lt;var&gt;predicate&lt;/var&gt;–&lt;var&gt;object&lt;/var&gt; structure. The &lt;var&gt;subject&lt;/var&gt; denotes the resource, and the &lt;var&gt;predicate&lt;/var&gt; denotes traits or aspects of the resource, and expresses a relationship between the &lt;var&gt;subject&lt;/var&gt; and the &lt;var&gt;object&lt;/var&gt;. 

For example, one way to represent the notion "The sky has the color blue" in RDF is as the triple: a [[Subject (grammar)|subject]] denoting "the sky", a [[Predicate (grammar)|predicate]] denoting "has the color", and an [[Object (grammar)|object]] denoting "blue". Therefore, RDF swaps &lt;var&gt;object&lt;/var&gt; for &lt;var&gt;subject&lt;/var&gt; in contrast to the typical approach of an [[entity–attribute–value model]] in [[object-oriented design]]: entity (sky), attribute (color), and value (blue).  

RDF is an abstract model with several [[Serialization|serialization formats]] (i.e. file formats), so the particular encoding for resources or triples varies from format to format.

This mechanism for describing resources is a major [[software componentry|component]] in the W3C's [[Semantic Web]] activity: an evolutionary stage of the [[World Wide Web]] in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and [[certainty]]. RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in [[knowledge management]] applications unrelated to Semantic Web activity.

A collection of RDF statements intrinsically represents a [[Glossary of graph theory|labeled, directed multi-graph]]. This theoretically makes an RDF [[data model]] better suited to certain kinds of [[knowledge representation]] than other [[relational model|relational]] or [[Ontology (computer science)|ontological]] models. However, in practice, RDF data is often persisted in [[RDBMS|relational database]] or native representations (also called [[Triplestore]]s—or Quad stores, if context (i.e. the [[named graph]]) is also persisted for each RDF triple).&lt;ref&gt;[http://sw.deri.org/2005/02/dexa/yars.pdf Optimized Index Structures for Querying RDF from the Web] Andreas Harth, Stefan Decker, 3rd Latin American Web Congress, Buenos Aires, Argentina, October 31 to November 2, 2005, pp. 71–80&lt;/ref&gt; 

ShEX, or Shape Expressions,&lt;ref&gt;[http://www.w3.org/2001/sw/wiki/ShEx]  Shape Expressions language&lt;/ref&gt; is a language for expressing constraints on RDF graphs. It includes the cardinality constraints from [[Open Services for Lifecycle Collaboration|OSLC]] Resource Shapes and [[Dublin Core]] Description Set Profiles, as well as logical connectives for disjunction and polymorphism. 

As [[RDFS]] and [[Web Ontology Language|OWL]] demonstrate, one can build additional [[ontology language]]s upon RDF.

== History ==
The initial RDF design, intended to "build a vendor-neutral and operating system-independent system of metadata,"&lt;ref name="press-release-1997"&gt;{{Cite news| last = | first = | title = World Wide Web Consortium Publishes Public Draft of Resource Description Framework| work = W3C| location = Cambridge, MA| date = 1997-10-03| url = http://www.w3.org/Press/RDF}}&lt;/ref&gt;  derived from the W3C's [[Platform for Internet Content Selection]] (PICS), an early web content labelling system,&lt;ref name="lash" /&gt; but the project was also  shaped by ideas from [[Dublin Core]], and from the [[Meta Content Framework]] (MCF),&lt;ref name="press-release-1997" /&gt; which had been developed during 1995–1997 by [[Ramanathan V. Guha]] at [[Apple Computer|Apple]] and [[Tim Bray]] at [[Netscape Communications Corporation|Netscape]].&lt;ref&gt;{{Cite book| publisher = O’Reilly| isbn = 0-596-00881-3| last = Hammersley| first = Ben| title = Developing Feeds with RSS and Atom| pages=2–3|location = Sebastopol| date = 2005}}&lt;/ref&gt;

A first public draft of RDF appeared in October 1997,&lt;ref&gt;{{Cite web| last1 = Lassila| first1 = Ora| last2 = Swick| first2 = Ralph R.| title = Resource Description Framework (RDF): Model and Syntax| work = W3C| accessdate = 2015-11-24| date = 1997-10-02| url = http://www.w3.org/TR/WD-rdf-syntax-971002/}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|last=Swick |first=Ralph |title=Resource Description Framework (RDF) |work=W3C |accessdate=2015-11-24 |date=1997-12-11 |url=http://www13.w3.org/RDF/Overview.html |deadurl=yes |archiveurl=https://web.archive.org/web/19980214043631/http://www13.w3.org/RDF/Overview.html |archivedate=February 14, 1998 }}&lt;/ref&gt; issued by a W3C working group that included representatives from [[IBM]], [[Microsoft]], [[Netscape]], [[Nokia]], [[Reuters]], [[SoftQuad Software|SoftQuad]], and the [[University of Michigan]].&lt;ref name="lash"&gt;{{Cite news|last=Lash |first=Alex |title=W3C takes first step toward RDF spec |work=CNET News |accessdate=2015-11-28 |date=1997-10-03 |url=http://news.cnet.com/2100-1001-203893.html |deadurl=yes |archiveurl=https://web.archive.org/web/20110616023126/http://news.cnet.com/2100-1001-203893.html |archivedate=June 16, 2011 }}&lt;/ref&gt;

The W3C published a specification of RDF's data model and an [[XML]] serialization as a recommendation in February 1999.&lt;ref&gt;{{cite web|url=http://www.w3.org/TR/1999/REC-rdf-syntax-19990222|title=Resource Description Framework (RDF) Model and Syntax Specification| date=22 Feb 1999|accessdate=5 May 2014}}&lt;/ref&gt;

Two persistent misunderstandings developed around RDF at this time: firstly, from the MCF influence and the RDF "Resource Description" acronym, the idea that RDF was specifically for use in representing metadata. Secondly that RDF was an XML format, rather than RDF being a data model and only the RDF/XML serialisation being XML-based. RDF saw little take-up in this period, but there was significant work carried out in [[Bristol]], around ILRT at [[Bristol University]] and [[HP Labs]], and also in Boston at [[MIT]]. [[RSS 1.0]] and [[FOAF (ontology)|FOAF]] became exemplar applications for RDF in this period.

The recommendation of 1999 was replaced in 2004 by a set of six specifications: "The RDF Primer",&lt;ref&gt;{{Citation| publisher = W3C| last1 = Manola| first1 = Frank| last2 = Miller| first2 = Eric| title = RDF Primer| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-primer-20040210/}}&lt;/ref&gt; "RDF Concepts and Abstract",&lt;ref&gt;{{Citation| publisher = W3C| last1 = Klyne| first1 = Graham| last2 = Carroll| first2 = Jeremy J.| title = Resource Description Framework (RDF): Concepts and Abstract Syntax| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/}}&lt;/ref&gt; "RDF/XML Syntax Specification (revised)",&lt;ref&gt;{{Citation| publisher = W3C| last = Beckett| first = Dave| title = RDF/XML Syntax Specification (Revised)| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/}}&lt;/ref&gt; "RDF Semantics",&lt;ref&gt;{{Citation| last = Hayes| first = Patrick| title = RDF Semantics| accessdate = 2015-11-21| date = 2014-02-10| url = http://www.w3.org/TR/2004/REC-rdf-mt-20040210/}}&lt;/ref&gt; "RDF Vocabulary Description Language 1.0",&lt;ref&gt;{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Vocabulary Description Language 1.0: RDF Schema: W3C Recommendation 10 February 2004| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-schema-20040210/}}&lt;/ref&gt; and "The RDF Test Cases".&lt;ref&gt;{{Citation| publisher = W3C| last1 = Grant| first1 = Jan| last2 = Beckett| first2 = Dave| title = RDF Test Cases| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/}}&lt;/ref&gt;

This series was superseded in 2014 by the following six "RDF 1.1" documents: "RDF 1.1 Primer,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Schreiber| first1 = Guus| last2 = Raimond| first2 = Yves| title = RDF 1.1 Primer| accessdate = 2015-11-22| date = 2014-06-24| url = http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/}}&lt;/ref&gt; "RDF 1.1 Concepts and Abstract Syntax,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Cyganiak| first1 = Richard| last2 = Wood| first2 = David| last3 = Lanthaler| first3 = Markus| title = RDF 1.1 Concepts and Abstract Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}&lt;/ref&gt; "RDF 1.1 XML Syntax,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Gandon| first1 = Fabien| last2 = Schreiber| first2 = Guus| title = RDF 1.1 XML Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-syntax-grammar/}}&lt;/ref&gt;
"RDF 1.1 Semantics,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Hayes| first1 = Patrick J.| last2 = Patel-Schneider| first2 = Peter F.| title = RDF 1.1 Semantics| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-mt-20140225/}}&lt;/ref&gt; "RDF Schema 1.1,"&lt;ref&gt;{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Schema 1.1| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-schema/}}&lt;/ref&gt; and "RDF 1.1 Test Cases".&lt;ref&gt;{{Citation| publisher = W3C| last1 = Kellogg| first1 = Gregg| last2 = Lanthaler| first2 = Markus| title = RDF 1.1 Test Cases| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/NOTE-rdf11-testcases-20140225/}}&lt;/ref&gt;

==RDF topics==

===RDF vocabulary===
The vocabulary defined by the RDF specification is as follows:&lt;ref name="rdfschema"&gt;{{cite web|url=http://www.w3.org/TR/rdf-schema/|title=RDF Vocabulary Description Language 1.0: RDF Schema|publisher=[[W3C]]|date=2004-02-10|accessdate=2011-01-05}}&lt;/ref&gt;

====Classes====

===== rdf =====
* '''&lt;code&gt;rdf:XMLLiteral&lt;/code&gt;''' – the class of XML literal values
* '''&lt;code&gt;rdf:Property&lt;/code&gt;''' – the class of properties
* '''&lt;code&gt;rdf:Statement&lt;/code&gt;''' – the class of RDF statements
* '''&lt;code&gt;rdf:Alt&lt;/code&gt;''', '''&lt;code&gt;rdf:Bag&lt;/code&gt;''', '''&lt;code&gt;rdf:Seq&lt;/code&gt;''' – containers of alternatives, unordered containers, and ordered containers (&lt;code&gt;rdfs:Container&lt;/code&gt; is a super-class of the three)
* '''&lt;code&gt;rdf:List&lt;/code&gt;''' – the class of RDF Lists
* '''&lt;code&gt;rdf:nil&lt;/code&gt;''' – an instance of &lt;code&gt;rdf:List&lt;/code&gt; representing the empty list

===== rdfs =====
* '''&lt;code&gt;rdfs:Resource&lt;/code&gt;''' – the class resource, everything
* '''&lt;code&gt;rdfs:Literal&lt;/code&gt;''' – the class of literal values, e.g. [[string literal|string]]s and [[integer]]s
* '''&lt;code&gt;rdfs:Class&lt;/code&gt;''' – the class of classes
* '''&lt;code&gt;rdfs:Datatype&lt;/code&gt;''' – the class of RDF datatypes
* '''&lt;code&gt;rdfs:Container&lt;/code&gt;''' – the class of RDF containers
* '''&lt;code&gt;rdfs:ContainerMembershipProperty&lt;/code&gt;''' – the class of container membership properties, &lt;code&gt;rdf:_1&lt;/code&gt;, &lt;code&gt;rdf:_2&lt;/code&gt;, ..., all of which are sub-properties of &lt;code&gt;rdfs:member&lt;/code&gt;

====Properties====

=====rdf=====
* '''&lt;code&gt;rdf:type&lt;/code&gt;''' – an instance of &lt;code&gt;rdf:Property&lt;/code&gt; used to state that a resource is an instance of a class
* '''&lt;code&gt;rdf:first&lt;/code&gt;''' – the first item in the subject RDF list
* '''&lt;code&gt;rdf:rest&lt;/code&gt;''' – the rest of the subject RDF list after &lt;code&gt;rdf:first&lt;/code&gt;
* '''&lt;code&gt;rdf:value&lt;/code&gt;''' – idiomatic property used for structured values
* '''&lt;code&gt;rdf:subject&lt;/code&gt;''' – the subject of the subject RDF statement
* '''&lt;code&gt;rdf:predicate&lt;/code&gt;''' – the predicate of the subject RDF statement
* '''&lt;code&gt;rdf:object&lt;/code&gt;''' – the object of the subject RDF statement

&lt;code&gt;rdf:Statement&lt;/code&gt;, &lt;code&gt;rdf:subject&lt;/code&gt;, &lt;code&gt;rdf:predicate&lt;/code&gt;, &lt;code&gt;rdf:object&lt;/code&gt; are used for [[reification (knowledge representation)|reification]] (see [[#Statement reification and context|below]]).

=====rdfs=====
* '''&lt;code&gt;rdfs:subClassOf&lt;/code&gt;''' – the subject is a subclass of a class
* '''&lt;code&gt;rdfs:subPropertyOf&lt;/code&gt;''' – the subject is a subproperty of a property
* '''&lt;code&gt;rdfs:domain&lt;/code&gt;''' – a domain of the subject property
* '''&lt;code&gt;rdfs:range&lt;/code&gt;''' – a range of the subject property
* '''&lt;code&gt;rdfs:label&lt;/code&gt;''' – a human-readable name for the subject
* '''&lt;code&gt;rdfs:comment&lt;/code&gt;''' – a description of the subject resource
* '''&lt;code&gt;rdfs:member&lt;/code&gt;''' – a member of the subject resource
* '''&lt;code&gt;rdfs:seeAlso&lt;/code&gt;''' – further information about the subject resource
* '''&lt;code&gt;rdfs:isDefinedBy&lt;/code&gt;''' – the definition of the subject resource

This vocabulary is used as a foundation for [[RDF Schema]] where it is extended.

=== Serialization formats ===
{{Infobox file format
| name = RDF 1.1 Turtle serialization
| icon = 
| extension = .ttl
| mime = text/turtle&lt;ref&gt;{{cite web |url=http://www.w3.org/TR/turtle/#h2_sec-mediaReg |title=RDF 1.1 Turtle: Terse RDF Triple Language |publisher=W3C |date=9 Jan 2014 |accessdate=2014-02-22}}&lt;/ref&gt;
| owner = [[World Wide Web Consortium]]
| standard = [http://www.w3.org/TR/turtle/ RDF 1.1 Turtle: Terse RDF Triple Language] {{release date and age|2014|01|09}}
| free = Yes
}}

{{Infobox file format
| name = RDF/XML serialization
| icon = [[Image:XML.svg|100px]]
| extension = .rdf
| mime = application/rdf+xml&lt;ref&gt;{{cite web |url=http://tools.ietf.org/html/rfc3870 |title=application/rdf+xml Media Type Registration |page=2 |publisher=IETF |date=September 2004 |accessdate=2011-01-08}}&lt;/ref&gt;
| owner = [[World Wide Web Consortium]]
| standard = [http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/ Concepts and Abstract Syntax] {{release date and age|2004|02|10}}
| free = Yes
}}

Several common [[Serialization|serialization formats]] are in use, including:
* '''[[Turtle (syntax)|Turtle]],'''&lt;ref name="turtle"&gt;{{cite web
  |title=RDF 1.1 Turtle: Terse RDF Triple Language
  |url=http://www.w3.org/TR/turtle/
  |date=9 January 2014
  |publisher=W3C
}}&lt;/ref&gt; a compact, human-friendly format.
* '''[[N-Triples]],'''&lt;ref name="n-triples" &gt;{{cite web
  |title=RDF 1.1 N-Triples: A line-based syntax for an RDF graph
  |date=9 January 2014
  |url=http://www.w3.org/TR/n-triples/
  |publisher=[[W3C]]
}}&lt;/ref&gt; a very simple, easy-to-parse, line-based format that is not as compact as Turtle.
* '''[[N-Quads]],'''&lt;ref&gt;{{cite web
  |title=N-Quads: Extending N-Triples with Context
  |date=2012-06-25 
  |url=http://sw.deri.org/2008/07/n-quads/
}}&lt;/ref&gt;&lt;ref name="n-quads" &gt;{{cite web
  |title=RDF 1.1 N-Quads
  |date=January 2014
  |url=http://www.w3.org/TR/n-quads/
  |publisher=[[W3C]]
}}&lt;/ref&gt; a superset of N-Triples, for serializing multiple RDF graphs.
* '''[[JSON-LD]],'''&lt;ref name="json-ld"&gt;{{cite web|title=
JSON-LD 1.0: A JSON-based Serialization for Linked Data|url=http://www.w3.org/TR/json-ld/|publisher=W3C}}&lt;/ref&gt; a [[JSON]]-based serialization.
* '''N3''' or [[Notation3]], a non-standard serialization that is very similar to Turtle, but has some additional features, such as the ability to define inference rules.
* '''[[RDF/XML]]''',&lt;ref name="rdf-xml" &gt;{{cite web
  |title=RDF 1.1 XML Syntax
  |date=25 February 2014
  |url=http://www.w3.org/TR/rdf-syntax-grammar/
  |publisher=[[W3C]]
}}&lt;/ref&gt; an XML-based syntax that was the first standard format for serializing RDF.

RDF/XML is sometimes misleadingly called simply RDF because it was introduced among the other W3C specifications defining RDF and it was historically the first W3C standard RDF serialization format. However, it is important to distinguish the RDF/XML format from the abstract RDF model itself. Although the RDF/XML format is still in use, other RDF serializations are now preferred by many RDF users, both because they are more human-friendly,&lt;ref name="rdf-xml-syntax-criticism"&gt;{{cite web|title=
Problems of the RDF syntax|url=http://milicicvuk.com/blog/2011/07/21/problems-of-the-rdf-syntax/|publisher=Vuk Miličić}}&lt;/ref&gt;  and because some RDF graphs are not representable in RDF/XML due to restrictions on the syntax of XML [[QName]]s.

With a little effort, virtually any arbitrary [[XML]] may also be interpreted as RDF using [[GRDDL]] (pronounced 'griddle'), Gleaning Resource Descriptions from Dialects of Languages.

RDF triples may be stored in a type of database called a [[triplestore]].

=== Resource identification ===
The subject of an RDF statement is either a [[uniform resource identifier]] (URI) or a [[blank node]], both of which denote  [[web resource|resource]]s. Resources indicated by [[blank node]]s are called anonymous resources. They are not directly identifiable from the RDF statement. The predicate is a URI which also indicates a resource, representing a relationship. The object is a URI, blank node or a [[Unicode]] [[string literal]]. 
As of RDF 1.1 resources are identified by IRI's. IRI is a generalization of URI.&lt;ref&gt;RDF 1.1 Concepts and Abstract Syntax https://www.w3.org/TR/rdf11-concepts/&lt;/ref&gt;

In Semantic Web applications, and in relatively popular applications of RDF like [[RSS (file format)|RSS]] and [[FOAF (software)|FOAF]] (Friend of a Friend), resources tend to be represented by URIs that intentionally denote, and can be used to access, actual data on the World Wide Web. But RDF, in general, is not limited to the description of Internet-based resources. In fact, the URI that names a resource does not have to be dereferenceable at all. For example, a URI that begins with "http:" and is used as the subject of an RDF statement does not necessarily have to represent a resource that is accessible via [[HTTP]], nor does it need to represent a tangible, network-accessible resource — such a URI could represent absolutely anything. However, there is broad agreement that a bare URI (without a # symbol) which returns a 300-level coded response when used in an HTTP GET request should be treated as denoting the internet resource that it succeeds in accessing.

Therefore, producers and consumers of RDF statements must agree on the semantics of resource identifiers. Such agreement is not inherent to RDF itself, although there are some controlled vocabularies in common use, such as [[Dublin Core]] Metadata, which is partially mapped to a URI space for use in RDF. The intent of publishing RDF-based ontologies on the Web is often to establish, or circumscribe, the intended meanings of the resource identifiers used to express data in RDF. For example, the URI:
&lt;blockquote&gt;
&lt;code&gt;
&lt;nowiki&gt;
http://www.w3.org/TR/2004/REC-owl-guide-20040210/wine#Merlot
&lt;/nowiki&gt;
&lt;/code&gt;
&lt;/blockquote&gt;
is intended by its owners to refer to the class of all [[Merlot]] red wines by vintner (i.e., instances of the above URI each represent the class of all wine produced by a single vintner), a definition which is expressed by the OWL ontology — itself an RDF document — in which it occurs.  Without careful analysis of the definition, one might erroneously conclude that an instance of the above URI was something physical, instead of a type of wine.

Note that this is not a 'bare' resource identifier, but is rather a [[Uniform Resource Identifier#URI reference|URI reference]], containing the '#' character and ending with a [[fragment identifier]].

=== Statement reification and context ===
The body of knowledge modeled by a collection of statements may be subjected to [[Reification (knowledge representation)|reification]], in which each ''statement'' (that is each triple ''subject-predicate-object'' altogether) is assigned a URI and treated as a resource about which additional statements can be made, as in "''Jane says that'' John is the author of document X". Reification is sometimes important in order to deduce a level of confidence or degree of usefulness for each statement.

In a reified RDF database, each original statement, being a resource, itself, most likely has at least three additional statements made about it: one to assert that its subject is some resource, one to assert that its predicate is some resource, and one to assert that its object is some resource or literal. More statements about the original statement may also exist, depending on the application's needs.

Borrowing from concepts available in [[logic]] (and as illustrated in graphical notations such as [[conceptual graphs]] and [[topic map]]s), some RDF model implementations acknowledge that it is sometimes useful to group statements according to different criteria, called ''situations'', ''contexts'', or ''scopes'', as discussed in articles by RDF specification co-editor [[Graham Klyne]].&lt;ref&gt;[http://www.ninebynine.org/RDFNotes/RDFContexts.html Contexts for RDF Information Modelling]&lt;/ref&gt;&lt;ref&gt;[http://www.ninebynine.org/RDFNotes/UsingContextsWithRDF.html Circumstance, Provenance and Partial Knowledge]&lt;/ref&gt; For example, a statement can be associated with a context, named by a URI, in order to assert an "is true in" relationship. As another example, it is sometimes convenient to group statements by their source, which can be identified by a URI, such as the URI of a particular RDF/XML document. Then, when updates are made to the source, corresponding statements can be changed in the model, as well.

Implementation of scopes does not necessarily require fully reified statements. Some implementations allow a single scope identifier to be associated with a statement that has not been assigned a URI, itself.&lt;ref&gt;[http://uche.ogbuji.net/tech/akara/nodes/2003-01-01/scopes The Concept of 4Suite RDF Scopes]&lt;/ref&gt;&lt;ref&gt;[http://librdf.org/notes/contexts.html Redland RDF Library – Contexts]&lt;/ref&gt;  Likewise ''named graphs'' in which a set of triples is named by a URI can represent context without the need to reify the triples.&lt;ref&gt;[http://www.w3.org/2004/03/trix/ Named Graphs]&lt;/ref&gt;

=== Query and inference languages ===
{{main|RDF query language}}
The predominant query language for RDF graphs is [[SPARQL]]. SPARQL is an [[SQL]]-like language, and a [[W3C recommendation|recommendation]] of the [[W3C]] as of January 15, 2008.

An example of a SPARQL query to show country capitals in Africa, using a fictional ontology.
&lt;source lang="sparql"&gt;
PREFIX ex: &lt;http://example.com/exampleOntology#&gt;
SELECT ?capital ?country
WHERE {
  ?x ex:cityname ?capital ;
     ex:isCapitalOf ?y .
  ?y ex:countryname ?country ;
     ex:isInContinent ex:Africa .
}
&lt;/source&gt;

Other non-standard ways to query RDF graphs include:
* [[RDQL]], precursor to [[SPARQL]], SQL-like
* Versa, compact syntax (non–SQL-like), solely implemented in [[4Suite]] ([[Python (programming language)|Python]])
* RQL, one of the first declarative languages for uniformly querying RDF schemas and resource descriptions, implemented in RDFSuite.&lt;ref name=RQL&gt;{{cite web|title=The RDF Query Language (RQL)|url=http://139.91.183.30:9090/RDF/RQL/index.html|work=The ICS-FORTH RDFSuite|publisher=ICS-FORTH}}&lt;/ref&gt;
* [[SeRQL]], part of [[Sesame (framework)|Sesame]]
* [[XUL]] has a template element in which to declare rules for matching data in RDF. XUL uses RDF extensively for databinding.

== Examples ==

=== Example 1: RDF Description of a person named Eric Miller&lt;ref name="rdf-primer"&gt;{{cite web|url= http://www.w3.org/TR/rdf-primer/|title= RDF Primer |publisher=[[W3C]]|accessdate=2009-03-13}}&lt;/ref&gt; ===

The following  example is taken from the W3C website&lt;ref name="rdf-primer" /&gt; describing a resource with statements "there is a Person identified by &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me&lt;/nowiki&gt;, whose name is Eric Miller, whose email address is e.miller123(at)example (changed for security purposes), and whose title is Dr. [[Image:Rdf graph for Eric Miller.png|thumb|An RDF Graph Describing Eric Miller&lt;ref name="rdf-primer" /&gt;]]

The resource "&lt;nowiki&gt;http://www.w3.org/People/EM/contact#me&lt;/nowiki&gt;" is the subject.

The objects are:
* "Eric Miller" (with a predicate "whose name is"),
* &lt;nowiki&gt;mailto:e.miller123&lt;/nowiki&gt;(at)example (with a predicate "whose email address is"), and
* "Dr." (with a predicate "whose title is").

The subject is a URI.

The predicates also have URIs. For example, the URI for each predicate:
* "whose name is" is &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#fullName&lt;/nowiki&gt;,
* "whose email address is" is &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#mailbox&lt;/nowiki&gt;,
* "whose title is" is &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#personalTitle&lt;/nowiki&gt;.

In addition, the subject has a type (with URI &lt;nowiki&gt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&lt;/nowiki&gt;), which is person (with URI &lt;nowiki&gt;http://www.w3.org/2000/10/swap/pim/contact#Person&lt;/nowiki&gt;).

Therefore, the following "subject, predicate, object" RDF triples can be expressed:
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#fullName,&lt;/nowiki&gt; "Eric Miller"
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#mailbox, mailto:e.miller123(at)example&lt;/nowiki&gt;
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#personalTitle,&lt;/nowiki&gt; "Dr."
* &lt;nowiki&gt;http://www.w3.org/People/EM/contact#me, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://www.w3.org/2000/10/swap/pim/contact#Person&lt;/nowiki&gt;

In standard N-Triples format, this RDF can be written as:
&lt;source lang="turtle"&gt;
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#fullName&gt; "Eric Miller" .
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#mailbox&gt; &lt;mailto:e.miller123(at)example&gt; .
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#personalTitle&gt; "Dr." .
&lt;http://www.w3.org/People/EM/contact#me&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://www.w3.org/2000/10/swap/pim/contact#Person&gt; .
&lt;/source&gt;

Equivalently, it can be written in standard Turtle (syntax) format as:

&lt;source lang="turtle"&gt;
@prefix eric:    &lt;http://www.w3.org/People/EM/contact#&gt; .
@prefix contact: &lt;http://www.w3.org/2000/10/swap/pim/contact#&gt; .
@prefix rdf:     &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .

eric:me contact:fullName "Eric Miller" .
eric:me contact:mailbox &lt;mailto:e.miller123(at)example&gt; .
eric:me contact:personalTitle "Dr." .
eric:me rdf:type contact:Person .
&lt;/source&gt;

Or, it can be written in RDF/XML format as:
&lt;source lang="xml" enclose="div"&gt;
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;rdf:RDF xmlns:contact="http://www.w3.org/2000/10/swap/pim/contact#" xmlns:eric="http://www.w3.org/People/EM/contact#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;contact:fullName&gt;Eric Miller&lt;/contact:fullName&gt;
  &lt;/rdf:Description&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;contact:mailbox rdf:resource="mailto:e.miller123(at)example"/&gt;
  &lt;/rdf:Description&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;contact:personalTitle&gt;Dr.&lt;/contact:personalTitle&gt;
  &lt;/rdf:Description&gt;
  &lt;rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me"&gt;
    &lt;rdf:type rdf:resource="http://www.w3.org/2000/10/swap/pim/contact#Person"/&gt;
  &lt;/rdf:Description&gt;
&lt;/rdf:RDF&gt;
&lt;/source&gt;

=== Example 2: The postal abbreviation for New York ===

Certain concepts in RDF are taken from [[logic]] and [[linguistics]], where subject-predicate and subject-predicate-object structures have meanings similar to, yet distinct from, the uses of those terms in RDF. This example demonstrates:

In the [[English language]] statement '' 'New York has the postal abbreviation NY' '','' 'New York' '' would be the subject, '' 'has the postal abbreviation' '' the predicate and '' 'NY' '' the object.

Encoded as an RDF triple, the subject and predicate would have to be resources named by URIs. The object could be a resource or literal element. For example, in the N-Triples form of RDF, the statement might look like:

&lt;source lang="turtle"&gt;
&lt;urn:x-states:New%20York&gt; &lt;http://purl.org/dc/terms/alternative&gt; "NY" .
&lt;/source&gt;

In this example, "&lt;nowiki&gt;urn:x-states:New%20York&lt;/nowiki&gt;" is the URI for a resource that denotes the US state [[New York (state)|New York]], "&lt;nowiki&gt;http://purl.org/dc/terms/alternative&lt;/nowiki&gt;" is the URI for a predicate (whose human-readable definition can be found at here &lt;ref&gt;[http://dublincore.org/documents/dcmi-terms/index.shtml#terms-alternative DCMI Metadata Terms]. Dublincore.org. Retrieved on 2014-05-30.&lt;/ref&gt;), and "NY" is a literal string.  Note that the URIs chosen here are not standard, and don't need to be, as long as their meaning is known to whatever is reading them.

=== Example 3: A Wikipedia article about Tony Benn ===
In a like manner, given that &lt;nowiki&gt;"http://en.wikipedia.org/wiki/Tony_Benn"&lt;/nowiki&gt; identifies a particular resource (regardless of whether that URI could be traversed as a hyperlink, or whether the resource is ''actually'' the [[Wikipedia]] article about [[Tony Benn]]), to say that the title of this resource is "Tony Benn" and its publisher is "Wikipedia" would be two assertions that could be expressed as valid RDF statements. In the N-Triples form of RDF, these statements might look like the following:

&lt;source lang="turtle"&gt;
&lt;http://en.wikipedia.org/wiki/Tony_Benn&gt; &lt;http://purl.org/dc/elements/1.1/title&gt; "Tony Benn" .
&lt;http://en.wikipedia.org/wiki/Tony_Benn&gt; &lt;http://purl.org/dc/elements/1.1/publisher&gt; "Wikipedia" .
&lt;/source&gt;

To an English-speaking person, the same information could be represented simply as:
&lt;blockquote&gt;The title of this resource, which is published by Wikipedia, is 'Tony Benn'&lt;/blockquote&gt;
However, RDF puts the information in a formal way that a machine can understand. The purpose of RDF is to provide an [[Semantics encoding|encoding]] and interpretation mechanism so that [[Resource (computer science)|resources]] can be described in a way that particular [[software]] can understand it; in other words, so that software can access and use information that it otherwise couldn't use.

Both versions of the statements above are wordy because one requirement for an RDF resource (as a subject or a predicate) is that it be unique. The subject resource must be unique in an attempt to pinpoint the exact resource being described. The predicate needs to be unique in order to reduce the chance that the idea of [[Title]] or [[Publisher]] will be ambiguous to software working with the description. If the software recognizes  ''&lt;nowiki&gt;http://purl.org/dc/elements/1.1/title&lt;/nowiki&gt;'' (a specific [[definition]] for the [[concept]] of a title established by the [[Dublin Core]] Metadata Initiative), it will also know that this title is different from a land title or an honorary title or just the letters t-i-t-l-e put together.

The following example, written in Turtle, shows how such simple claims can be elaborated on, by combining multiple RDF vocabularies. Here, we note that the primary topic of the Wikipedia page is a "Person" whose name is "Tony Benn":

&lt;source lang="turtle"&gt;
@prefix rdf:  &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .
@prefix dc:   &lt;http://purl.org/dc/elements/1.1/&gt; .

&lt;http://en.wikipedia.org/wiki/Tony_Benn&gt;
    dc:publisher "Wikipedia" ;
    dc:title "Tony Benn" ;
    foaf:primaryTopic [
        a foaf:Person ;
        foaf:name "Tony Benn"
    ] .
&lt;/source&gt;

== Applications ==
* [[DBpedia]] – Extracts facts from Wikipedia articles and publishes them as RDF data.
* [[Creative Commons]] – Uses RDF to embed license information in web pages and mp3 files.
* [[FOAF (software)|FOAF (Friend of a Friend)]] – designed to describe [[person|people]], their interests and interconnections.
* [[Haystack (PIM)|Haystack client]] – Semantic web browser from MIT CS &amp; AI lab.&lt;ref&gt;[http://groups.csail.mit.edu/haystack/ Haystack]&lt;/ref&gt;
* [[IDEAS Group]] – developing a formal [[Ontology components|4D ontology]] for [[Enterprise Architecture]] using RDF as the encoding.&lt;ref&gt;[http://www.ideasgroup.org The IDEAS Group Website]&lt;/ref&gt;
* Microsoft shipped a product, Connected Services Framework,&lt;ref&gt;[http://www.microsoft.com/serviceproviders/solutions/connectedservicesframework.mspx Connected Services Framework]&lt;/ref&gt; which provides RDF-based Profile Management capabilities.
* [[MusicBrainz]] – Publishes information about Music Albums.&lt;ref&gt;[http://wiki.musicbrainz.org/RDF RDF on MusicBrainz Wiki]&lt;/ref&gt;
* [[NEPOMUK (framework)|NEPOMUK]], an open-source software specification for a Social Semantic desktop uses RDF as a storage format for collected metadata. NEPOMUK is mostly known because of its integration into the [[KDE Software Compilation 4|KDE SC 4]] desktop environment.
* [[Press Association]] is a news agency in the UK. They use ontologies to dynamically identify and link their NoSQL data to do [[semantic publishing]] but in a dynamic, rules based way that creates custom content on the fly.&lt;ref&gt;[http://www.datalanguage.com/blog/2012/05/17/ontology-driven-software-engineering/]&lt;/ref&gt;
* RDF Site Summary – one of several "[[RSS (file format)|RSS]]" languages for publishing information about updates made to a web page; it is often used for disseminating news article summaries and sharing [[weblog]] content.
* [[Simple Knowledge Organization System]] (SKOS) – a KR representation intended to support vocabulary/thesaurus applications
* [[SIOC|SIOC (Semantically-Interlinked Online Communities)]] – designed to describe online communities and to create connections between Internet-based discussions from message boards, weblogs and mailing lists.&lt;ref&gt;[http://sioc-project.org/ SIOC (Semantically-Interlinked Online Communities)]&lt;/ref&gt;
* [[Smart-M3]] – provides an infrastructure for using RDF and specifically uses the ontology agnostic nature of RDF to enable heterogeneous mashing-up of information&lt;ref&gt;Oliver Ian, Honkola Jukka, Ziegler Jurgen (2008). “Dynamic, Localized Space Based Semantic Webs”. IADIS WWW/Internet 2008. Proceedings, p.426, IADIS Press, ISBN 978-972-8924-68-3&lt;/ref&gt;

Some uses of RDF include research into social networking. It will also help people in business fields understand better their relationships with members of industries that could be of use for product placement.&lt;ref&gt;An RDF Approach for Discovering the Relevant Semantic Associations in a Social Network By Thushar A.K, and P. Santhi Thilagam&lt;/ref&gt;  It will also help scientists understand how people are connected to one another.

RDF is being used to have a better understanding of road traffic patterns.  This is because the information regarding traffic patterns is  on different websites, and RDF is used to integrate information from different sources on the web. Before, the common methodology was using keyword searching, but this method is problematic because it does not  consider synonyms. This is why ontologies are useful in this situation. But one of the issues that comes up when trying to efficiently study traffic is that to fully understand traffic,  concepts related to people, streets, and roads must be well understood. Since these are human  concepts, they require the addition of [[fuzzy logic]]. This is because values that are useful  when describing roads, like slipperiness, are not precise concepts and cannot be measured. This would imply that the best solution would incorporate both fuzzy logic and ontology.&lt;ref&gt;Traffic Information Retrieval Based on Fuzzy Ontology and RDF on the Semantic Web By Jun Zhai, Yi Yu, Yiduo Liang, and Jiatao Jiang (2008)&lt;/ref&gt;

== See also ==
;Notations for RDF
* [[TriG (syntax)|TRiG]]
* [[TriX (syntax)|TRiX]]
* [[RDF/XML]]
* [[RDFa]]
* [[JSON-LD]]
;Similar concepts
* [[Entity-attribute-value model]]
* [[Graph theory]] – An RDF model is a labeled, directed multi-graph.
* [[Website Parse Template]]
* [[Tag (metadata)|Tagging]]
* [[SciCrunch]]
* [[Semantic network]]
; Other (unsorted):
*[[Associative model of data]]
*[[Business Intelligence 2.0]] (BI 2.0)
*DataPortability
* [[EU Open Data Portal]]
*[[Folksonomy]]
*[[LSID|Life Science Identifiers]]
*[[Swoogle]]
*[[Universal Networking Language]] (UNL)

== References ==
{{Reflist|2}}

== Further reading ==
* [http://www.w3.org/RDF/ W3C's RDF at W3C]: specifications, guides, and resources
* [http://www.w3.org/TR/2004/REC-rdf-mt-20040210/ RDF Semantics]: specification of semantics, and complete systems of inference rules for both RDF and RDFS

== External links ==
{{Commons category|Resource Description Framework}}
*{{DMOZ|Reference/Libraries/Library_and_Information_Science/Technical_Services/Cataloguing/Metadata/RDF/}}
{{Semantic Web}}
{{W3C Standards}}
{{Data Exchange}}

{{Authority control}}

[[Category:Resource Description Framework| ]]
[[Category:Knowledge representation]]
[[Category:World Wide Web Consortium standards]]
[[Category:XML]]
[[Category:XML-based standards]]
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Bibliography file formats]]</text>
      <sha1>6s0927s9w5cip40tkm4qcbhbfnjoqgc</sha1>
    </revision>
  </page>
  <page>
    <title>Conceptual graph</title>
    <ns>0</ns>
    <id>346755</id>
    <revision>
      <id>761060488</id>
      <parentid>751303674</parentid>
      <timestamp>2017-01-20T17:15:30Z</timestamp>
      <contributor>
        <username>Jefferythomas</username>
        <id>13631841</id>
      </contributor>
      <comment>/* External links */ added john sowa link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7368" xml:space="preserve">'''Conceptual graphs''' ('''CGs''') are a formalism for [[knowledge representation]]. In the first published paper  on CGs, [[John F. Sowa]] {{harv|Sowa|1976}} used them to represent the [[conceptual schema]]s used in [[database system]]s. The first book on CGs {{harv|Sowa|1984}} applied them to a wide range of topics in [[artificial intelligence]], [[computer science]], and [[cognitive science]].

Since 1984, the model has been developed along three main directions.

== A graphical interface for first-order logic ==
In this approach, a formula in [[first-order logic]] (predicate calculus) is represented by a labeled graph.

A linear notation, called the '''Conceptual Graph Interchange Format (CGIF)''', has been standardized in the ISO standard for [[common logic]].

[[Image:Cat-on-mat.svg|thumb|250px|Elsie the cat is sitting on a mat]]
The diagram on the right is an example of the '''display form''' for a conceptual graph.  Each box is called a '''concept node''', and each oval is called a '''relation node'''.  In CGIF, this CG would be represented by the following statement:

    [Cat Elsie] [Sitting *x] [Mat *y] (agent ?x Elsie) (location ?x ?y)
In CGIF, brackets enclose the information inside the concept nodes, and parentheses enclose the information inside the relation nodes.  The letters x and y, which are called '''coreference labels''', show how the concept and relation nodes are connected.  In the '''Common Logic Interchange Format (CLIF)''', those letters are mapped to variables, as in the following statement:

    (exists ((x Sitting) (y Mat)) (and (Cat Elsie) (agent x Elsie) (location x y)))

As this example shows, the asterisks on the coreference labels *x and *y in CGIF map to existentially quantified variables in CLIF, and the question marks on ?x and ?y map to bound variables in CLIF.  A universal quantifier, represented '''@every*z''' in CGIF, would be represented '''forall (z)''' in CLIF.

Reasoning can be done by translating graphs into logical formulas, then applying a logical inference engine.

== Diagrammatic calculus of logics ==
Another research branch continues the work on [[existential graph]]s of [[Charles Sanders Peirce]], which were one of the origins of conceptual graphs as proposed by Sowa.
In this approach, developed in particular by Dau {{harv|Dau|2003}}, conceptual graphs are conceptual [[diagram]]s rather than graphs in the sense of [[graph theory]], and reasoning operations are performed by operations on these diagrams.

== Graph-based knowledge representation and reasoning model ==
Key features of '''GBKR''', the graph-based knowledge representation and reasoning model developed by Chein and Mugnier and the Montpellier group {{harv|Chein|Mugnier|2009}},
can be summarized as follows:

* all kinds of knowledge (ontology, rules, constraints and facts) are labeled graphs, which provide an intuitive and easily understandable means to represent knowledge,
* reasoning mechanisms are based on graph notions, basically the classical notion of graph homomorphism; this allows, in particular, to link basic reasoning problems to other fundamental problems in computer science (problems concerning conjunctive queries in relational databases, constraint satisfaction problem, ...),
* the formalism is logically founded, i.e., it has a semantics in first-order logic and the inference mechanisms are sound and complete with respect to deduction in first-order logic,
* from a computational viewpoint, the graph homomorphism notion was recognized in the 1990s as a central notion, and complexity results and efficient algorithms have been obtained in several domains.

COGITANT and COGUI are tools that implement the '''GBKR''' model. COGITANT is a library of C++ classes that implement most of the GBKR notions and reasoning mechanisms. COGUI  is a graphical user interface dedicated to the construction of a GBKR knowledge base (it integrates COGITANT and, among numerous functionalities, it contains a translator from GBKR to RDF/S and conversely).

== Sentence generalization and generalization diagrams ==
Sentence [[generalization]] and generalization diagrams can be defined as a special sort of conceptual graphs which can be constructed automatically from syntactic [[parse tree]]s and support semantic classification task {{harv|Galitsky et al|2010}}. Similarity measure between syntactic parse trees can be done as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the [[syntax]] generalization level and [[semantics]] generalization level (anti-unification of [[logic forms]]). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at semantic level.

==See also==
*[[Resource Description Framework]] (RDF)
*[[SPARQL]] (Graph Query Language)
*[[Semantic network]]
*[[Knowledge representation]]
*[[Chunking (psychology)]]
*[[Concept map]]
*[[Conceptual schema]]
*[[Diagrammatic reasoning]]

==References==
* {{cite book |last=Chein |first=Michel |last2=Mugnier |first2=Marie-Laure |year=2009 |title=Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs |publisher=Springer |url=http://www.lirmm.fr/gbkrbook/ |isbn=978-1-84800-285-2 |ref=harv | doi=10.1007/978-1-84800-286-9}}
* {{cite journal |last=Dau |first=F. |year=2003 |title=The Logic System of Concept Graphs with Negation and Its Relationship to Predicate Logic |journal=[[Lecture Notes in Computer Science]] |volume=2892 |publisher=Springer |isbn= |ref=harv }}
* {{cite journal |last=Sowa |authorlink = John Sowa |first=John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336–357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}
* {{cite book |last=Sowa |first=John F. |year=1984 |title=Conceptual Structures:  Information Processing in Mind and Machine |location=Reading, MA |publisher=Addison-Wesley |isbn=978-0-201-14472-7 |ref=harv }}
* {{cite journal | last=Galitsky | first=Boris | last2=Dobrocsi |first2=Gabor | last3=de la Rosa |first3=Josep Lluis | last4=Kuznetsov |first4=Sergei O. |year=2010 |title=From Generalization of Syntactic Parse Trees to Conceptual Graphs |journal=Lecture Notes in Computer Science |volume=6208 |publisher=Springer |isbn= |url=http://dl.acm.org/citation.cfm?id=1881190|ref=harv }}
* {{cite journal|title=Conceptual graphs for the analysis and generation of sentences
|first1=Paola |last1=Velardi |first2=Maria Teresa |last2=Pazienza |first3=Mario |last3=De' Giovanetti |journal=IBM Journal of Research and Development |volume=32 |number=2 |date=March 1988 |pages=251–267 |publisher=IBM Corp. Riverton, NJ, USA |doi=10.1147/rd.322.0251}}

==External links==
* [http://conceptualstructures.org Conceptual Structures Home Page]. (Old site:  [http://conceptualgraphs.org Conceptual Graphs Home Page])
* [http://www.informatik.uni-trier.de/~ley/db/conf/iccs/index.html Yearly international conferences (ICCS)]
* [http://www.jfsowa.com/cg/index.htm Conceptual Graphs on John F. Sowa's Website]

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Application-specific graphs]]</text>
      <sha1>dcj1fcdgy0lnqgloe3a1wb9jc40to0k</sha1>
    </revision>
  </page>
  <page>
    <title>Brinkler classification</title>
    <ns>0</ns>
    <id>3388492</id>
    <revision>
      <id>680103537</id>
      <parentid>517350881</parentid>
      <timestamp>2015-09-08T18:54:43Z</timestamp>
      <contributor>
        <ip>69.135.193.98</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2666" xml:space="preserve">'''Brinkler Classification''' is the [[library classification]] system  of [[Bartol Brinkler]] described in his article "The Geographical Approach to Materials in the Library of Congress Subject Headings".&lt;ref&gt;Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings: report of a study project. s.l.: s.n., 1960. [Accession No: {{OCLC|3853830}}].&lt;/ref&gt; The geographical aspect of a subject may be conveyed through three types of headings labeled A, B, and C. Heading A uses a primary topical description with geographical subdivisions (e.g. Art—Paris).  Type B uses a place-name for the main heading with a topical subdivision (e.g. Paris—Description). C headings use a geographical description of a phrase (e.g. Paris Literature).  

Brinkler explores what type of heading is more useful to a [[patron]], and he finds that it depends on the level of familiarity a patron has with a topic and what approach they take when searching for resources on their topic. Ideally readers will either be looking for everything on a particular topic, or everything regarding a particular place. Bartol Brinkler investigates a system of classification that will best serve these two ideal types of patrons.  He finds working with Type A headings will best assist a patron who is more topic oriented, while using Type B headings is preferable for those who are primarily interested in one place. 

However this is problematic in practice. One possibility is to assign Type A and Type B headings to every resource, but the cataloguing cost would be high.  A system that aids readers regardless of their approach to a topic involves using cross-references (e.g. Canada—Botany, See Botany—Canada).  Admitting that see and see also references would require more work on the part of librarians, Bartol Brinkler notes that librarians must keep in mind "...readers do not have the same knowledge [of classification] and do need all the help they can get..."{{Citation needed|date=March 2008}}

==References==
{{reflist}}
*Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings. Library Resources &amp; Technical Services 6, no. 1 (Winter 1962): 49-64.

==External links==
*[http://hcl.harvard.edu/libraries/#widener  Harvard University. Widener Library.]
*[http://www.loc.gov/catdir/cpso/lcco/lcco.html Library of Congress Classification Outline.]
*[http://www.princeton.edu/~paw/memorials/memorials_1930s/memorials_1937.html Princeton Alumni Weekly: Memorials 1937.]

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]</text>
      <sha1>531yu33u8zo74zu2q8olqx0kpt8wyix</sha1>
    </revision>
  </page>
  <page>
    <title>Simple Knowledge Organization System</title>
    <ns>0</ns>
    <id>4916685</id>
    <revision>
      <id>753634092</id>
      <parentid>749208144</parentid>
      <timestamp>2016-12-08T09:40:15Z</timestamp>
      <contributor>
        <ip>134.2.65.26</ip>
      </contributor>
      <comment>/* Tools */ added ThesauRex</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27082" xml:space="preserve">'''Simple Knowledge Organization System''' ('''SKOS''') is a [[W3C recommendation]] designed for representation of [[Thesaurus (information retrieval)|thesauri]], [[classification scheme]]s, [[Taxonomy (general)|taxonomies]], [[Authority control|subject-heading systems]], or any other type of structured [[controlled vocabulary]]. SKOS is part of the [[Semantic Web]] family of standards built upon [[Resource Description Framework|RDF]] and [[RDF Schema|RDFS]], and its main objective is to enable easy publication and use of such vocabularies as [[linked data]].

== History ==

=== DESIRE II project (1997&amp;ndash;2000) ===

The most direct ancestor to SKOS was the RDF Thesaurus work undertaken in the second phase of the EU DESIRE project &lt;ref name="Desire Project"&gt;{{Citation |publication-date=August 7, 2000 |title=Desire: Development of a European Service for Information on Research and Education |publisher=Desire Consortium |url=http://www.desire.org/ |archiveurl=https://web.archive.org/web/20110725230823/http://www.desire.org/ |archivedate=July 25, 2011 }}&lt;/ref&gt;{{Citation needed|reason=The Desire Project reference does not appear to directly address the SKOS ancestry statement made here.|date=August 2012}}.  Motivated by the need to improve the user interface and usability of multi-service browsing and searching,&lt;ref name="Desire Deliverable D.36b"&gt;{{Citation |title=Desire: Research Deliverables: D3.1 |publisher=Desire Consortium |url=http://www.desire.org/docs/research/deliverables/D3.6/d36b.html |archiveurl=https://web.archive.org/web/20080509135041/http://www.desire.org/html/research/deliverables/D3.6/#d36b |archivedate=May 9, 2008 }}&lt;/ref&gt; a basic RDF vocabulary for Thesauri was [http://www.desire.org/results/discovery/rdfthesschema.html produced]. As noted later in the [http://www.w3.org/2001/sw/Europe/plan/workpackages/live/esw-wp-8.html SWAD-Europe workplan], the DESIRE work was adopted and further developed in the SOSIG and LIMBER projects. A version of the DESIRE/SOSIG implementation was described in W3C's QL'98 workshop, motivating early work on RDF rule and query languages: [http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF].&lt;ref&gt;[http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF]&lt;/ref&gt;

=== LIMBER (1999&amp;ndash;2001) ===

SKOS built upon the output of the Language Independent Metadata Browsing of European Resources (LIMBER) project funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. In the LIMBER project [[CCLRC]] further developed an [[Resource Description Framework|RDF]]  thesaurus interchange format&lt;ref&gt;[http://journals.tdl.org/jodi/article/viewArticle/34/35 Miller, K. &amp; Matthews, B. (2001). Having the right connections: the LIMBER Project. Journal of Digital Information, 1 (8), 5 February. ]&lt;/ref&gt; which was demonstrated on the European Language Social Science Thesaurus ([http://www.cessda.org/results.html?query=elsst ELSST]) at the [[UK Data Archive]] as a multilingual version of the English language Humanities and Social Science Electronic Thesaurus (HASSET) which was planned to be used by the Council of European Social Science Data Archives [http://www.cessda.org/ CESSDA].

=== SWAD-Europe (2002&amp;ndash;2004) ===

SKOS as a distinct initiative began in the SWAD-Europe project, bringing together partners from both DESIRE, SOSIG (ILRT) and LIMBER (CCLRC) who had worked with earlier versions of the schema. It was developed in the Thesaurus Activity Work Package, in the Semantic Web Advanced Development for Europe (SWAD-Europe) project.&lt;ref&gt;[http://www.w3.org/2001/sw/Europe/ SWAD-Europe]&lt;/ref&gt; SWAD-Europe was funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. The project was designed to support W3C's Semantic Web Activity through research, demonstrators and outreach efforts conducted by the five project partners, [[ERCIM]], the [http://www.ilrt.bris.ac.uk/ ILRT] at [[Bristol University]], [[HP Labs]], [[CCLRC]] and Stilo.&lt;ref&gt;[http://www.stilo.com Stilo Home Page]&lt;/ref&gt;
The first release of SKOS Core and SKOS Mapping were published at the end of 2003, along with other deliverables on RDF encoding of multilingual thesauri&lt;ref&gt;[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.3.html SWAD-Europe Deliverable 8.3 : RDF Encoding of Multilingual Thesauri]&lt;/ref&gt; and thesaurus mapping.&lt;ref&gt;[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.4.html SWAD-Europe Deliverable 8.4 : Inter-Thesaurus Mapping]&lt;/ref&gt;

=== Semantic web activity (2004&amp;ndash;2005) ===

Following the termination of SWAD-Europe, SKOS effort was supported by the W3C Semantic Web Activity&lt;ref&gt;[http://www.w3.org/2001/sw/ W3C Semantic Web Activity]&lt;/ref&gt; in the framework of the Best Practice and Deployment Working Group.&lt;ref&gt;[http://www.w3.org/2004/03/thes-tf/mission W3C Semantic Web Best Practice and Deployment Working Group : Porting Thesauri Task Force]&lt;/ref&gt; During this period, focus was put both on consolidation of SKOS Core, and development of practical guidelines for porting and publishing thesauri for the Semantic Web.

=== Development as W3C Recommendation (2006&amp;ndash;2009)===

The SKOS main published documents — the SKOS Core Guide,&lt;ref&gt;[http://www.w3.org/TR/swbp-skos-core-guide SKOS Core Guide] W3C Working Draft 2 November 2005&lt;/ref&gt; the SKOS Core Vocabulary Specification,&lt;ref&gt;[http://www.w3.org/TR/swbp-skos-core-spec SKOS Core Vocabulary Specification] W3C Working Draft 2 November 2005&lt;/ref&gt; and the Quick Guide to Publishing a Thesaurus on the Semantic Web&lt;ref&gt;[http://www.w3.org/TR/swbp-thesaurus-pubguide Quick Guide to Publishing a Thesaurus on the Semantic Web] W3C Working Draft 17 May 2005&lt;/ref&gt; — were developed through the W3C Working Draft process. Principal editors of SKOS were Alistair Miles,&lt;ref&gt;[http://purl.org/net/aliman Alistair Miles Home Page]&lt;/ref&gt; initially Dan Brickley,&lt;ref&gt;[http://danbri.org/ Dan Brickley Home Page]&lt;/ref&gt; and Sean Bechhofer.&lt;ref&gt;[http://www.cs.man.ac.uk/~seanb/#me Sean Bechhofer Home Page]&lt;/ref&gt;

The Semantic Web Deployment Working Group,&lt;ref&gt;[http://www.w3.org/2006/07/SWD/ W3C Semantic Web Deployment Working Group]&lt;/ref&gt; chartered for two years (May 2006 - April 2008), has put in its charter to push SKOS forward on the [[W3C Recommendation]] track. The roadmap projects SKOS as a Candidate Recommendation by the end of 2007, and as a Proposed Recommendation in the first quarter of 2008. The main issues to solve are determining its precise scope of use, and its articulation with other RDF languages and standards used in libraries (such as [[Dublin Core]]).&lt;ref&gt;[http://isegserv.itd.rl.ac.uk/public/skos/press/dc2006/camera-ready-paper.pdf SKOS: Requirements for Standardization]. The paper by Alistair Miles presented in October 2006 at the International Conference on Dublin Core and Metadata Applications.&lt;/ref&gt;&lt;ref&gt;[http://purl.org/net/retrieval Retrieval and the Semantic Web, incorporating a Theory of Retrieval Using Structured Vocabularies]. Dissertation on the theory of retrieval using structured vocabularies by Alistair Miles.&lt;/ref&gt;

=== Formal release (2009) ===
On August 18, 2009, [[W3C]] released the new standard that builds a bridge between the world of knowledge organization systems - including thesauri, classifications, subject headings, taxonomies, and [[folksonomy|folksonomies]] - and the [[linked data]] community, bringing benefits to both. Libraries, museums, newspapers, government portals, enterprises, social networking applications, and other communities that manage large collections of books, historical artifacts, news reports, business glossaries, blog entries, and other items can now use SKOS&lt;ref&gt;[http://www.w3.org/TR/2009/REC-skos-reference-20090818/ Simple Knowledge Organization System (SKOS)]&lt;/ref&gt; to leverage the power of linked data.

=== Historical view of components ===

SKOS was originally designed as a modular and extensible family of languages, organized as SKOS Core, SKOS Mapping, and SKOS Extensions, and a Metamodel. The entire specification is now complete within the namespace [http://www.w3.org/2004/02/skos/core# http://www.w3.org/2004/02/skos/core#].

== Overview ==

In addition to the reference itself, the [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/ SKOS Primer] (a W3C Working Group Note) summarizes the Simple Knowledge Organization System.

The SKOS&lt;ref&gt;[http://www.w3.org/TR/skos-reference SKOS Reference]&lt;/ref&gt; defines the classes and properties sufficient to represent the common features found in a standard thesaurus. It is based on a concept-centric view of the vocabulary, where primitive objects are not terms, but abstract notions represented by terms. Each SKOS concept is defined as an [[web resource|RDF resource]]. Each concept can have RDF properties attached, including:
* one or more preferred [[index term]]s (at most one in each natural language)
* alternative terms or [[synonym]]s
* definitions and notes, with specification of their language

Concepts can be organized in [[hierarchy|hierarchies]] using broader-narrower relationships, or linked by non-hierarchical (associative) relationships.
Concepts can be gathered in concept schemes, to provide consistent and structured sets of concepts, representing whole or part of a controlled vocabulary.

=== Element categories ===

The principal element categories of SKOS are concepts, labels, notations, semantic relations, mapping properties, and collections. The associated concepts are listed in the table below.

{| border="1" class="wikitable"
|+ SKOS Vocabulary Organized by Theme
! Concepts
! Labels &amp; Notation
! Documentation
! Semantic Relations
! Mapping Properties
! Collections
|-
| Concept || prefLabel  || note || broader || broadMatch || Collection 
|-
| ConceptScheme || altLabel  || changeNote || narrower || narrowMatch || orderedCollection 
|-
| inScheme || hiddenLabel || definition || related || relatedMatch || member
|-
| hasTopConcept || notation  || editorialNote || broaderTransitive || closeMatch || memberList
|-
| topConceptOf ||   || example || narrowerTransitive || exactMatch ||  
|-
|   ||   || historyNote || semanticRelation || mappingRelation || 
|-
|   ||   || scopeNote ||  ||  || 
|-

|}

=== Concepts ===

The SKOS vocabulary is based on concepts. Concepts are the units of thought—ideas, meanings, or objects and events (instances or categories)—which underlie many knowledge organization systems. As such, concepts exist in the mind as abstract entities which are independent of the terms used to label them. In SKOS, a &lt;code&gt;Concept&lt;/code&gt; (based on the OWL &lt;code&gt;Class&lt;/code&gt;) is used to represent items in a knowledge organization system (terms, ideas, meanings, etc.) or such a system's conceptual or organizational structure.

A &lt;code&gt;ConceptScheme&lt;/code&gt; is analogous to a vocabulary, thesaurus, or other way of organizing concepts. SKOS does not constrain a concept to be within a particular scheme, nor does it provide any way to declare a complete scheme—there is no way to say the scheme consists only of certain members. A topConcept is (one of) the upper concept(s) in a hierarchical scheme.

=== Labels and notations ===

Each SKOS &lt;code&gt;label&lt;/code&gt; is a string of [[Unicode]] characters, optionally with language tags, that are associated with a concept. The &lt;code&gt;prefLabel&lt;/code&gt; is the preferred human-readable string (maximum one per language tag), while &lt;code&gt;altLabel&lt;/code&gt; can be used for alternative strings, and &lt;code&gt;hiddenLabel&lt;/code&gt; can be used for strings that are useful to associate, but not meant for humans to read.

A SKOS &lt;code&gt;notation&lt;/code&gt; is similar to a label, but the literal string has a datatype, like integer, float, or date; the datatype can even be made up (see [http://www.w3.org/TR/skos-reference/#L2613 6.5.1 Notations, Typed Literals and Datatypes] in the SKOS Reference). The notation is useful for classification codes and other strings not recognizable as words.

=== Documentation ===

The Documentation or Note properties provide basic information about SKOS concepts. All the concepts are considered a type of &lt;code&gt;skos:note&lt;/code&gt;; they just provide more specific kinds of information. The property &lt;code&gt;definition&lt;/code&gt;, for example, should contain a full description of the subject resource.  More specific note types can be defined in a SKOS extension, if desired. A query for &lt;code&gt;&amp;lt;A&amp;gt; skos:note ?&lt;/code&gt; will obtain all the notes about &amp;lt;A&amp;gt;, including definitions, examples, and scope, history and change, and editorial documentation.

Any of these SKOS Documentation properties can refer to several object types: a literal (e.g., a string); a resource node that has its own properties; or a reference to another document, for example using a URI. This enables the documentation to have its own [[metadata]], like creator and creation date.

Specific guidance on SKOS documentation properties can be found in the SKOS Primer [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/#secdocumentation Documentary Notes].

=== Semantic relations ===

SKOS semantic relations are intended to provide ways to declare relationships between concepts within a concept scheme. While there are no restrictions precluding their use with two concepts from separate schemes, this is discouraged because it is likely to overstate what can be known about the two schemes, and perhaps link them inappropriately.

The property &lt;code&gt;related&lt;/code&gt; simply makes an association relationship between two concepts; no hierarchy or generality relation is implied. The properties &lt;code&gt;broader&lt;/code&gt; and &lt;code&gt;narrower&lt;/code&gt; are used to assert a direct hierarchical link between two concepts. The meaning may be unexpected; the relation &lt;code&gt;&amp;lt;A&amp;gt; broader &amp;lt;B&amp;gt;&lt;/code&gt; means that A has a broader concept called B—hence that B is broader than A. Narrower follows in the same pattern.

While the casual reader might expect broader and narrower to be transitive properties, SKOS does not declare them as such. Rather, the properties &lt;code&gt;broaderTransitive&lt;/code&gt; and &lt;code&gt;narrowerTransitive&lt;/code&gt; are defined as transitive super-properties of broader and narrower. These super-properties are (by convention) not used in declarative SKOS statements. Instead, when a broader or narrower relation is used in a triple, the corresponding transitive super-property also holds; and transitive relations can be inferred (and queried) using these super-properties.

=== Mapping ===

SKOS mapping properties are intended to express matching (exact or fuzzy) of concepts from one concept scheme to another, and by convention are used only to connect concepts from different schemes. The concepts &lt;code&gt;relatedMatch&lt;/code&gt;, &lt;code&gt;broadMatch&lt;/code&gt;, and &lt;code&gt;narrowMatch&lt;/code&gt; are a convenience, with the same meaning as the semantic properties &lt;code&gt;related&lt;/code&gt;, &lt;code&gt;broader&lt;/code&gt;, and &lt;code&gt;narrower&lt;/code&gt;. (See previous section regarding the meanings of broader and narrower.)

The property relatedMatch makes a simple associative relationship between two concepts. When concepts are so closely related that they can generally be used interchangeably, &lt;code&gt;exactMatch&lt;/code&gt; is the appropriate property (exactMatch relations are transitive, unlike any of the other Match relations). The &lt;code&gt;closeMatch&lt;/code&gt; property that indicates concepts that only sometimes can be used interchangeably, and so it is not a transitive property.

=== Concept collections ===

The concept collections (&lt;code&gt;Collection&lt;/code&gt;, &lt;code&gt;orderedCollection&lt;/code&gt;) are labeled and/or ordered (&lt;code&gt;orderedCollection&lt;/code&gt;) groups of SKOS concepts. Collections can be nested, and can have defined URIs or not (which is known as a blank node). Neither a SKOS &lt;code&gt;Concept&lt;/code&gt; nor a &lt;code&gt;ConceptScheme&lt;/code&gt; may be a Collection, nor vice versa; and SKOS semantic relations can only be used with a Concept (not a Collection). The items in a Collection can not be connected to other SKOS Concepts through the Collection node; individual relations must be defined to each Concept in the Collection.

== Community and participation ==

All development work is carried out via the mailing list which is a completely open and publicly archived&lt;ref&gt;[http://lists.w3.org/Archives/Public/public-esw-thes/ public-esw-thes@w3.org online archive]. Archives of mailing list used for SKOS development.&lt;/ref&gt; mailing list devoted to discussion of issues relating to knowledge organisation systems, information retrieval and the Semantic Web. Anyone may participate informally in the development of SKOS by joining the discussions on public-esw-thes@w3.org - informal participation is warmly welcomed. Anyone who works for a [http://www.w3.org/Consortium/join W3C member] organisation may formally participate in the development process by joining the [http://www.w3.org/2006/07/SWD/ Semantic Web Deployment Working Group] - this entitles individuals to edit specifications and to vote on publication decisions.

== Applications ==

*Some important vocabularies have been migrated into SKOS format and are available in the public domain, including [[EuroVoc]], [[AGROVOC]] and [[GEMET]]. [[Library of Congress Subject Headings]] (LCSH) also support the SKOS format.&lt;ref&gt;[http://id.loc.gov/authorities/about.html About the Library of Congress Authorities]&lt;/ref&gt;
*SKOS has been used as the language for the thesauri used in the [[SWED Environmental Directory]]&lt;ref&gt;[http://www.swed.org.uk/swed Semantic Web Environmental Directory]&lt;/ref&gt; developed in the SWAD-Europe project framework.
*A way to convert thesauri to SKOS,&lt;ref&gt;[http://thesauri.cs.vu.nl/eswc06/ A Method to Convert Thesauri to SKOS]&lt;/ref&gt; with examples including the [[Medical Subject Headings|MeSH]] thesaurus, has been outlined by the [[Vrije Universiteit Amsterdam]].
*Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS has been developed by [[IBM]].&lt;ref&gt;[http://www-128.ibm.com/developerworks/xml/library/x-dita10/ Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS] by IBM developerWorks.&lt;/ref&gt;
*SKOS is used to represent geographical feature types in the [[GeoNames]] ontology.

== Tools ==
* [https://github.com/eScienceCenter/ThesauRex ThesauRex] is an open-source, web-based SKOS editor. It is limited to broader/narrower relations among concepts and offers tree-based interaction and with thesauri and drag&amp;drop creation of new thesauri based on a master thesaurus.
* Mondeca's [http://www.mondeca.com/Products/ITM Intelligent Topic Manager] (ITM) is a full-featured SKOS-compliant solution for managing taxonomies, thesauri, and other controlled vocabularies.
*[http://pactols.frantiq.fr/opentheso/ Opentheso] is an open source web-based thesaurus management system compliant with ISO 25964:2011 and ISO 25964-2:2012 standards (Information and Documentation. Thesauri and Interoperability with other vocabularies). It offers SKOS and csv exports and imports, REST and SOAP web services and manages persistent identifiers (ARK). It has been developed at the French National Center for Scientific Research since 2007. It is currently used by the French archaeological libraries network [http://www.frantiq.fr Frantiq] and by research teams  and by the Hospices Civils de Lyon as a collaborative thesaurus management tool. It can be dowloaded on [https://github.com/frantiq/opentheso github]. 
* [http://openskos.org OpenSKOS] is a web service-based approach to publication, management and use of vocabulary data that can be mapped to SKOS. Its source code is available on [https://github.com/CatchPlus/OpenSKOS GitHub]. It includes [[CRUD]] like [[RESTful]] operations on SKOS concepts and a web-based editor for searching and editing concepts. It was developed by [http://picturae.com Picturae] and funded by the Dutch heritage fond [http://www.catchplus.nl/ CATCHPlus].
* TemaTres Vocabulary Server&lt;ref&gt;[http://www.vocabularyserver.com/ TemaTres] is a Web tool to manage formal and linguistic representations of knowledge.&lt;/ref&gt; is an open source web-based vocabulary server for managing controlled vocabularies, taxonomies and thesauruses. [http://sourceforge.net/projects/tematres Tematres] provides complete export of vocabularies into SKOS-core in addition to Zthes, TopicMaps, MADS, Dublin Core,VDEX, BS 8723, SiteMap, SQL and text.
* ThManager&lt;ref&gt;[http://thmanager.sourceforge.net/ ThManager] an Open Source Tool for creating and visualizing SKOS RDF vocabularies.&lt;/ref&gt; is a [[Java (programming language)|Java]] [[Open-source software|open-source]] application for creating and visualizing SKOS vocabularies.
* The W3C provides an experimental on-line validation service.&lt;ref&gt;[http://www.w3.org/2004/02/skos/core/validation SKOS Core Validation Service]&lt;/ref&gt;
* SKOS files can also be imported and edited in RDF-OWL editors such as [[Protege (software)|Protégé]] and [[SWOOP]] developed by Maryland Information and Network Dynamics Lab Semantic Web Agents Project [[Mindswap]].&lt;ref&gt;[http://www.mindswap.org/2004/SWOOP/ SWOOP] A Hypermedia-based Featherweight OWL Ontology Editor, developed by [[Mindswap]] - Maryland Information and Network Dynamics Lab Semantic Web Agents Project&lt;/ref&gt;
* SKOS synonyms can be transformed from [[WordNet]] RDF format using an [[XSLT]] style sheet; see [http://www.w3.org/TR/wordnet-rdf W3C RDF]
* PoolParty&lt;ref&gt;[http://www.poolparty.biz/ PoolParty] is a thesaurus management system and a SKOS editor for the Semantic Web.&lt;/ref&gt; is a commercial-quality thesaurus management system and a SKOS editor for the Semantic Web including text analysis functionalities and [[Linked Data]] capabilities.
* qSKOS&lt;ref&gt;[https://github.com/cmader/qSKOS/ qSKOS] is an open-source tool for SKOS vocabulary quality assessment.&lt;/ref&gt; is an open-source tool for performing quality assessment of SKOS vocabularies by checking against a quality issue catalog.
* SKOSEd&lt;ref&gt;[http://code.google.com/p/skoseditor/ SKOSEd] SKOS plugin for Protege 4&lt;/ref&gt; is an open source plug-in for the Protégé 4&lt;ref&gt;[http://www.co-ode.org/downloads/protege-x/ Protégé 4] Protégé 4 OWL editor&lt;/ref&gt; [[Web Ontology Language|OWL]] ontology editor that supports authoring SKOS vocabularies. SKOSEd has an accompanying SKOS API&lt;ref&gt;[http://skosapi.sourceforge.net/ SKOS Java API] Java API for SKOS&lt;/ref&gt; written in Java that can be used to build SKOS-based applications.
* Model Futures SKOS Exporter&lt;ref&gt;[http://www.modelfutures.com/software Model Futures Excel SKOS Exporter]&lt;/ref&gt; for [[Microsoft Excel]] allows simple vocabularies to be developed as indented Excel spreadsheets and exported as SKOS RDF. BETA version.
* Lexaurus&lt;ref&gt;[http://www.vocman.com/ Lexaurus] is an enterprise thesaurus management system and multi-format editor.&lt;/ref&gt; is an enterprise thesaurus management system and multi-format editor. Its extensive API includes full revision management. SKOS is one of its many supported formats.
* TopBraid Enterprise Vocabulary Net (EVN) &lt;ref&gt;[http://www.topquadrant.com/solutions/ent_vocab_net.html TopBraid EVN]&lt;/ref&gt; is a web-based solution for simplified development and management of interconnected controlled vocabularies. It supports collaboration on defining and linking enterprise vocabularies, taxonomies, thesauri and ontologies used for information integration, customization and search.
* [http://www.dataharmony.com/products/thesaurus_master.html Thesaurus Master], for creating, developing, and maintaining taxonomies and thesauri, is part of Access Innovations' [http://www.dataharmony.com/ Data Harmony] knowledge management software line. It offers SKOS-compliant export.
* [http://www.cognitum.eu/semantics/FluentEditor/ Fluent Editor 2014] - an ontology editor which allows to work and edit directly OWL annotations and SKOS. Annotations will processed also for referenced ontologies as well as imported/exported to OWL/RDF and can be processed on the server.
* [https://trial.smartlogic.com/S4Trials/ Smartlogic Semaphore Ontology Editor] - a SKOS and SKOS-XL based ontology editor which allows creating models based strictly on the SKOS standards.

== Data ==
There are publicly available SKOS data sources.
* SKOS Datasets wiki&lt;ref&gt;[http://www.w3.org/2001/sw/wiki/SKOS/Datasets SKOS/Datasets]&lt;/ref&gt; The W3C recommends using this list of publicly available SKOS data sources. Most data found in this wiki can be used for commercial and research applications.

== Relationships with other standards ==

=== Metamodel ===
The SKOS metamodel is broadly compatible with the data model of [[ISO 25964-1]] - Thesauri for Information Retrieval. This data model can be viewed and downloaded from the website for [[ISO 25964]].&lt;ref name="niso.org"&gt;[http://www.niso.org/schemas/iso25964 ''ISO 25964 – the international standard for thesauri and interoperability with other vocabularies'']&lt;/ref&gt;
[[File:Skos metamodel.png|thumb|alt=Alt text|Semantic model of the information elements of SKOS]]

=== SKOS and thesaurus standards ===
SKOS development has involved experts from both RDF and library community, and SKOS intends to allow easy migration of thesauri defined by standards such as [[NISO]] Z39.19 - 2005&lt;ref&gt;[http://www.niso.org/standards/ NISO Standards] Z39.19 - 2005 : Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies&lt;/ref&gt; or [[ISO 25964]].&lt;ref name="niso.org"/&gt;

=== SKOS and other semantic web standards ===
SKOS is intended to provide a way to make a legacy of concept schemes available to Semantic Web applications, simpler than the more complex ontology language, [[Web Ontology Language|OWL]]. OWL is intended to express complex conceptual structures, which can be used to generate rich metadata and support inference tools. However, constructing useful web ontologies is demanding in terms of expertise, effort, and cost. In many cases, this type of effort might be superfluous or unsuited to requirements, and SKOS might be a better choice. The extensibility of RDF makes possible further incorporation or extension of SKOS vocabularies into more complex vocabularies, including OWL ontologies.

== See also ==
* [[Glossary]]
* [[Knowledge representation]]
* [[Metadata registry]]

== References ==
{{Reflist|2}}

==External links==
* [http://www.w3.org/TR/skos-reference/ SKOS Simple Knowledge Organization System Reference]
* [http://www.w3.org/2004/02/skos/ W3C SKOS Home Page]
* [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818 W3C Simple Knowledge Organization System Primer]
* [http://www.idealliance.org/proceedings/xtech05/papers/03-04-01/ Presentation of SKOS at XTech 2005 Conference]
* [http://www.w3.org/News/2009#item35 W3C Invites Implementations of SKOS (Simple Knowledge Organization System) Reference; Primer Also Published]
* [http://demo.semantic-web.at:8080/SkosServices/index SKOS Validator and Zthes Converter]

{{Semantic Web}}
{{W3C standards}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:School of Computer Science, University of Manchester]]</text>
      <sha1>stpq50op4c92g15mz2xumixsctvk4n5</sha1>
    </revision>
  </page>
  <page>
    <title>National Library of Medicine classification</title>
    <ns>0</ns>
    <id>4165078</id>
    <revision>
      <id>714125777</id>
      <parentid>662532684</parentid>
      <timestamp>2016-04-07T20:08:35Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* top */clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4044" xml:space="preserve">The '''National Library of Medicine (NLM) classification system''' is a [[Library classification|library indexing system]] covering the fields of [[medicine]] and preclinical basic sciences. The [[National Library of Medicine|NLM]] classification is patterned after the [[Library of Congress Classification|Library of Congress (LC) Classification system]]: [[Alphabet|alphabetical letters ]]  denote broad subject categories which are subdivided by numbers.&lt;ref name=NLM_Factsheet&gt;{{cite web | title =Fact Sheet: NLM Classification | work = | url = https://www.nlm.nih.gov/pubs/factsheets/nlmclassif.html | date = 2005-07-15 | accessdate = 2007-05-12}}&lt;/ref&gt; For example, ''QW 279'' would indicate a book on an aspect of [[microbiology]] or [[immunology]].

The one- or two-letter alphabetical codes in the NLM classification use a limited range of letters: only QS–QZ and W–WZ. This allows the NLM system to co-exist with the larger LC coding scheme as neither of these ranges are used in the LC system. There are, however, three pre-existing codes in the LC system which overlap with the NLM: ''Human Anatomy'' (QM), ''Microbiology'' (QR), and ''Medicine'' (R). To avoid further confusion, these three codes are not used in the NLM.

The headings for the individual ''schedules'' (letters or letter pairs) are given in brief form (e.g., QW - ''Microbiology and Immunology''; WG - ''Cardiovascular System'') and together they provide an outline of the subjects covered by the NLM classification. Headings are interpreted broadly and include the [[Physiology|physiological]] system, the specialties connected with them, the regions of the body chiefly concerned and subordinate related fields. The NLM system is [[hierarchical]], and within each schedule, division by [[Organ (anatomy)|organ]] usually has priority. Each main schedule, as well as some sub-sections, begins with a group of form numbers ranging generally from 1–49 which  classify materials by publication type, e.g., [[Dictionary|dictionaries]], [[atlas]]es, laboratory manuals, etc.

The main schedules QS-QZ, W-WY, and WZ (excluding the range WZ 220–270)  classify works published after 1913; the 19th century schedule is used for works published 1801-1913; and WZ 220-270 is used to provide century groupings for works published before 1801.

==Overview of the NLM Classification categories==

'''Preclinical Sciences'''
* QS Human Anatomy
* QT Physiology
* QU Biochemistry
* QV Pharmacology
* QW Microbiology &amp; Immunology
* QX Parasitology
* QY Clinical Pathology
* QZ Pathology

'''Medicine and Related Subjects'''

* W  Health Professions
* WA Public Health
* WB Practice of Medicine
* WC Communicable Diseases
* WD Disorders of Systemic, Metabolic, or Environmental Origin, etc.
* WE Musculoskeletal System
* WF Respiratory System
* WG Cardiovascular System
* WH Hemic and Lymphatic Systems
* WI Digestive System
* WJ Urogenital System
* WK Endocrine System
* WL Nervous System
* WM Psychiatry
* WN Radiology. Diagnostic Imaging
* WO Surgery
* WP Gynecology
* WQ Obstetrics
* WR Dermatology
* WS Pediatrics
* WT Geriatrics. Chronic Disease
* WU Dentistry. Oral Surgery
* WV Otolaryngology
* WW Ophthalmology
* WX Hospitals &amp; Other Health Facilities
* WY Nursing
* WZ History of Medicine
* 19th Century Schedule

==See also==
*[[Dewey Decimal Classification]]
*[[Colon Classification]]
*[[Library of Congress Classification]]
*[[Universal Decimal Classification]]

==References==
&lt;!-- ---------------------------------------------------------------
See http://en.wikipedia.org/wiki/Wikipedia:Footnotes for a
discussion of different citation methods and how to generate
footnotes using the&lt;ref&gt; &amp; &lt;/ref&gt;  tags and the {{Reflist}} template
-------------------------------------------------------------------- --&gt;
{{Reflist}}

{{refbegin}}
* {{USGovernment|sourceURL=[http://wwwcf.nlm.nih.gov/class/ The NLM Classification 2005]}}
{{refend}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>a5s88zmkq2s6ytwy4ye08omh4xam1lz</sha1>
    </revision>
  </page>
  <page>
    <title>Korean decimal classification</title>
    <ns>0</ns>
    <id>6978390</id>
    <revision>
      <id>666864078</id>
      <parentid>581227952</parentid>
      <timestamp>2015-06-14T06:20:45Z</timestamp>
      <contributor>
        <username>Antunesi</username>
        <id>17428904</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="796" xml:space="preserve">{{Unreferenced stub|date=December 2009}}
The '''Korean decimal classification''' ('''KDC''') is a system of [[library classification]] used in [[South Korea]]. The main classes are the same as in the Dewey Decimal Classification but these are in a different order: Natural sciences 400; Technology and engineering 500; Arts 600; Language 700.

==Main classes==
* 000 General
* 100 Philosophy
* 200 Religion
* 300 Social sciences
* 400 Natural sciences
* 500 Technology and engineering
* 600 Arts
* 700 Language
* 800 Literature
* 900 History

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]
[[Category:Libraries in North Korea]]
[[Category:Libraries in South Korea]]

{{Library classification systems}}
{{Library-stub}}</text>
      <sha1>rb8xn0z25au6uqw0enkm8859zd2j3y4</sha1>
    </revision>
  </page>
  <page>
    <title>Agent Communications Language</title>
    <ns>0</ns>
    <id>3348350</id>
    <revision>
      <id>753455131</id>
      <parentid>709886768</parentid>
      <timestamp>2016-12-07T07:12:02Z</timestamp>
      <contributor>
        <username>Jessicapierce</username>
        <id>2003421</id>
      </contributor>
      <minor />
      <comment>minor copy edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3007" xml:space="preserve">{{Refimprove|date=September 2014}}
'''Agent Communication Language''' ('''ACL'''), proposed by the [[Foundation for Intelligent Physical Agents]] (FIPA), is a proposed standard language for [[Software agent|agent]] communications.  [[Knowledge Query and Manipulation Language]] (KQML) is another proposed standard.

The most popular ACLs are:
* FIPA-ACL &lt;ref&gt;{{cite journal|last=Poslad|first=Stefan|title=Specifying Protocols for Multi-agent System Interaction|year=2007|journal=ACM Transactions on Autonomous and Adaptive Systems|volume=4|issue=4|doi=10.1145/1293731.1293735}}&lt;/ref&gt; (by the [[Foundation for Intelligent Physical Agents]], a standardization consortium)
* [[KQML]] &lt;ref&gt;{{cite journal|last=Finin|first=Tim|author2= Richard Fritzson, Don McKay and Robin McEntire |title=KQML as an agent communication language|year=1994|conference=Proceedings of the third international conference on Information and knowledge management, CIKM '94 |pages= 456–463}}&lt;/ref&gt; (Knowledge Query and Manipulation Language)

Both rely on [[speech act]] theory developed by [[John Searle|Searle]] in the 1960s &lt;ref&gt;{{cite book|last= Searle|first=J.R.|year=1969|title=Speech Acts|publisher=Cambridge University Press, Cambridge, UK}}&lt;/ref&gt; and enhanced by [[Terry Winograd|Winograd]] and [[Fernando Flores|Flores]] in the 1970s. They define a set of [[Performative utterance|performatives]], also called Communicative Acts, and their meaning (e.g. ask-one). The content of the performative is not standardized, but varies from system to system.

To make agents understand each other they have to not only speak the same language, but also have a common [[Ontology (computer science)|ontology]]. An ontology is a part of the agent's knowledge base that describes what kind of things an agent can deal with and how they are related to each other.

Examples of frameworks that implement a standard agent communication language (FIPA-ACL) include FIPA-OS &lt;ref&gt;{{cite conference|last=Poslad|first=Stefan|author2=Philip Buckle and Robert Hadingham|title=The FIPA-OS agent platform: Open Source for Open Standards|year=2000|conference=Proceedings of 5th International Conference on the Practical Application Of Intelligent Agents And Multi-Agent Technology (PAAM)|pages=355–368}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Poslad|first= S|author2=Buckle P, Hadingham R.G|title=Open Source, Standards and Scaleable Agencies|journal=Lecture Notes in Computer Science|year= 2001|volume=1887|pages=296–303|DOI=10.1007/3-540-47772-1_30}}&lt;/ref&gt;
and [[Java Agent Development Framework|Jade]]. &lt;ref&gt;{{cite conference|last=Bellifeminee|first=Fabio|author2=Agostino Poggi and Giovanni Rimassa|title=JADE: a FIPA2000 compliant agent development environment|year=2001|conference=Proceedings of the fifth international conference on Autonomous agents|pages=216–217}}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Formal languages]]
[[Category:Knowledge representation]]
[[Category:Multi-agent systems]]


{{Measurement-stub}}
{{computing-stub}}</text>
      <sha1>depv2ta47dl7kf6nl993hup24n5w5uw</sha1>
    </revision>
  </page>
  <page>
    <title>MultiNet</title>
    <ns>0</ns>
    <id>2473220</id>
    <revision>
      <id>729278174</id>
      <parentid>710620820</parentid>
      <timestamp>2016-07-11T03:59:39Z</timestamp>
      <contributor>
        <username>Yaron K.</username>
        <id>2276977</id>
      </contributor>
      <comment>Removed extra newlines</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2803" xml:space="preserve">'''Multilayered extended semantic networks''' ('''MultiNets''') are both a [[knowledge representation]] paradigm and a language for meaning representation of [[natural language]] expressions that has been developed by Prof. Dr. Hermann Helbig on the basis of earlier [[Semantic network|Semantic Networks]].

MultiNet is claimed to be one of the most comprehensive and thoroughly described knowledge representation systems. It specifies conceptual structures by means of about 140 predefined relations and functions, which are systematically characterized and underpinned by a formal [[axiomatic]] apparatus. Apart from their relational connections, the concepts are embedded in a multidimensional space of layered attributes and their values. Another characteristic of MultiNet discerning it from simple semantic networks is the possibility to encapsulate whole partial networks and represent the resulting conceptual capsule as a node of higher order, which itself can be an argument of relations and functions. MultiNet has been used in practical [[Natural language processing|NLP]] applications such as natural language interfaces to the Internet or [[question answering]] systems over large semantically annotated [[Corpus linguistics|corpora]] with millions of sentences. MultiNet is also a cornerstone of the commercially available search engine SEMPRIA Search, where it is used for the description of the computational lexicon and the background knowledge, for the syntactic-semantic analysis, for logical answer finding, as well as for the generation of natural language answers.

MultiNet is supported by a set of [[software tools]] and has been used to build large semantically based computational lexicons. The tools include a semantic interpreter WOCADI which translates natural language expressions (phrases, sentences, texts) into formal MultiNet expressions, a workbench MWR+ for the knowledge engineer (comprising modules for automatic knowledge acquisition and reasoning), and a workbench LIA+ for the computer [[lexicographer]] supporting the creation of large semantically based computational lexica.

== References ==
* Hermann Helbig, ''Die semantische Struktur natürlicher Sprache - Wissensrepräsentation mit MultiNet''. Springer, Heidelberg, 2001.
* Hermann Helbig. ''Knowledge Representation and the Semantics of Natural Language'', (2006) Springer, Berlin
* Sven Hartrumpf, Hermann Helbig, Johannes Leveling, Rainer Osswald. ''An Architecture for Controlling Simple Language in Web Pages'', eMinds: International Journal on Human-Computer Interaction, 1(2), 2006.

== External links ==
* [http://pi7.fernuni-hagen.de/forschung/multinet/multinet_en.html MultiNet] and its software environment

[[Category:Semantic Web]]
[[Category:Knowledge representation]]


{{software-stub}}</text>
      <sha1>tvpsp1mjd6b22clyogfc0bbihittvaw</sha1>
    </revision>
  </page>
  <page>
    <title>Is-a</title>
    <ns>0</ns>
    <id>294441</id>
    <revision>
      <id>757843230</id>
      <parentid>747485106</parentid>
      <timestamp>2017-01-02T01:44:22Z</timestamp>
      <contributor>
        <username>Mx. Granger</username>
        <id>4871659</id>
      </contributor>
      <comment>removing/replacing citations to Wiktionary, which is not a [[WP:Reliable sources|reliable source]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10070" xml:space="preserve">In [[knowledge representation]], [[object-oriented programming]] and [[Object-oriented design|design]] (see [[object oriented]] [[program architecture]]), '''is-a''' ('''is_a''' or '''is a''') is a [[wikt:subsume|subsumption]]&lt;ref&gt;See [[Liskov substitution principle]].&lt;/ref&gt; relationship between [[abstractions]] (e.g. [[type (disambiguation)|types]], [[class (disambiguation)|classes]]), where one [[Class (computer programming)|class]] ''A'' is a [[subclass (disambiguation)|subclass]] &lt;!-- This deliberately links to the disambiguation page --&gt; of another class ''B'' (and so ''B'' is a [[superclass (disambiguation)|superclass]] &lt;!--This deliberately links to the disambiguation page--&gt; of ''A'').
In other words, type A is a subtype of type B when A’s [[Formal specification|speciﬁcation]] implies B’s speciﬁcation. That is, any object (or class) that satisﬁes A’s speciﬁcation also satisﬁes B’s speciﬁcation, because B’s speciﬁcation is weaker.&lt;ref&gt;{{cite web|title=Subtypes and Subclasses|url=http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-170-laboratory-in-software-engineering-fall-2005/lecture-notes/lec14.pdf|publisher=MIT OCW|accessdate=2 October 2012}}&lt;/ref&gt;

The ''is-a'' relationship is to be contrasted with the ''[[has-a]]'' (''has_a'' or ''has a'') relationship between types (classes); confusing the relations ''has-a'' and ''is-a'' is a common error when designing a model (e.g., a [[computer program]]) of the real-world relationship between an object and its subordinate. The ''is-a'' relationship may also be contrasted with the ''[[Typeof|instance-of]]'' relationship between objects (instances) and types (classes): see "[[type-token distinction]]" and "[[type-token relations]]."&lt;ref&gt;[[Type–token relations]]&lt;/ref&gt; 

To summarize the relations, we have

* [[hyperonym]]-[[hyponym]] (supertype-subtype) relations between types (classes) defining a taxonomic hierarchy, where
** for a [[Inheritance (object-oriented programming)|subsumption]] relation: a hyponym (subtype, subclass) has a ''type-of'' (''is-a'') relationship with its hypernym (supertype, superclass);
* [[holonym]]-[[meronym]] (whole/entity/container-part/constituent/member) relations between types (classes) defining a possessive hierarchy, where 
** for an [[Aggregation (object-oriented programming)|aggregation]] (i.e. without ownership) relation: 
*** a holonym (whole) has a ''has-a'' relationship with its meronym (part),
** for a [[Composition (object-oriented programming)|composition]] (i.e. with ownership) relation: 
*** a meronym (constituent) has a ''[[part-of]]'' relationship with its holonym (entity),
** for a [[Object composition#Containment|containment]]&lt;ref&gt;See also [[Containment (computer programming)]].&lt;/ref&gt; relation:
*** a meronym (member) has a ''[[member-of]]'' relationship with its holonym ([[Container (abstract data type)|container]]);
* concept-object (type-token) relations between types (classes) and objects (instances), where
** a token (object) has an ''[[Instance (computer science)|instance-of]]'' relationship with its type (class).

==Examples of subtyping==

[[Subtype polymorphism|Subtyping]] enables a given type to be substituted for another type or abstraction. Subtyping is said to establish an '''is-a''' relationship between the subtype and some existing abstraction, either implicitly or explicitly, depending on language support. The relationship can be expressed explicitly via inheritance in languages that support inheritance as a subtyping mechanism.

===C++===
The following C++ code establishes an explicit inheritance relationship between classes '''B''' and '''A''', where '''B''' is both a subclass and a subtype of '''A''', and can be used as an '''A''' wherever a '''B''' is specified (via a reference, a pointer or the object itself).

&lt;source lang=cpp&gt;class A 
{ public:
   void DoSomethingALike() const {}
};

class B : public A 
{ public:
   void DoSomethingBLike() const {}
};

void UseAnA(A const&amp; some_A)
{
   some_A.DoSomethingALike();
}

void SomeFunc()
{
   B b;
   UseAnA(b); // b can be substituted for an A.
}
&lt;/source&gt;&lt;ref name="Mitchell2002"&gt;
{{cite book
 | last=Mitchell
 | first=John
 | authorlink=John C. Mitchell
 | title=Concepts in programming language
 | year=2002
 | publisher=Cambridge University Press
 | location=Cambridge, UK
 | isbn=0-521-78098-5
 | page=287
 | chapter=10 "Concepts in object-oriented languages"}}
&lt;/ref&gt;

===Python===
The following python code establishes an explicit inheritance relationship between classes '''B''' and '''A''', where '''B''' is both a subclass and a subtype of '''A''', and can be used as an '''A''' wherever a '''B''' is required.

&lt;source lang=python&gt;class A:
    def doSomethingALike(self):
        pass

class B(A):
    def doSomethingBLike(self):
        pass

def useAnA(some_A):
    some_A.doSomethingALike()

def someFunc():
    b = B();
    useAnA(b)  # b can be substituted for an A.
&lt;/source&gt;

The following example, type(a) is a "regular" type, and type(type(a)) is a metatype. While as distributed all types have the same metatype (PyType_Type, which is also its own metatype), this is not a requirement. The type of classic classes, known as types.ClassType, can also be considered a distinct metatype.&lt;ref&gt;{{cite web|last=Guido van Rossum|title=Subtyping Built-in Types|url=https://www.python.org/dev/peps/pep-0253/|accessdate=2 October 2012}}&lt;/ref&gt;

&lt;source lang=python&gt;
&gt;&gt;&gt; a = 0
&gt;&gt;&gt; type(a)
&lt;type 'int'&gt;
&gt;&gt;&gt; type(type(a))
&lt;type 'type'&gt;
&gt;&gt;&gt; type(type(type(a)))
&lt;type 'type'&gt;
&gt;&gt;&gt; type(type(type(type(a))))
&lt;type 'type'&gt;
&lt;/source&gt;

===Java===

In Java, '''is-a''' relation between the type parameters of one class or interface and the type parameters of another are determined by the extends and [[Interface (Java)|implements]] clauses.

Using the Collections classes, ArrayList&lt;E&gt; implements List&lt;E&gt;, and List&lt;E&gt; extends Collection&lt;E&gt;. So ArrayList&lt;String&gt; is a subtype of List&lt;String&gt;, which is a subtype of Collection&lt;String&gt;. The subtyping relationship is preserved between the types automatically. When we define an interface, PayloadList, that associates an optional value of generic type P with each element. Its declaration might look like:

&lt;source lang=java&gt;
interface PayloadList&lt;E, P&gt; extends List&lt;E&gt; {
    void setPayload(int index, P val);
    ...
}
&lt;/source&gt;

The following parameterizations of PayloadList are subtypes of List&lt;String&gt;:

&lt;source lang=java&gt;
PayloadList&lt;String, String&gt;
PayloadList&lt;String, Integer&gt;
PayloadList&lt;String, Exception&gt;
&lt;/source&gt;

==Liskov substitution principle==
{{main|Liskov substitution principle}}
Liskov substitution principle explains a property, ''"If for each object o1 of type S there is an object o2 of type T such that for all programs P deﬁned in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T,"''.&lt;ref&gt;{{cite book|last=Liskov|first=Barbara|title=Data Abstraction and Hierarchy|date=May 1988|publisher=SIGPLAN Notices}}&lt;/ref&gt; Following example shows a violation of LSP.

&lt;source lang=cpp&gt;void DrawShape(const Shape&amp; s)
{
  if (typeid(s) == typeid(Square))
    DrawSquare(static_cast&lt;Square&amp;&gt;(s));
  else if (typeid(s) == typeid(Circle))
    DrawCircle(static_cast&lt;Circle&amp;&gt;(s));
}&lt;/source&gt;
Obviously, the DrawShape function is badly formatted. It has to know about every derivative classes of Shape class. Also, it should be changed whenever new subclass of Shape are created. In [[Object-oriented design|Object Oriented Design]], many view the structure of this as anathema.

Here is a more subtle example of violation of LSP

&lt;source lang=cpp&gt;
class Rectangle
{
  public:
    void   SetWidth(double w)  { itsWidth = w; }
    void   SetHeight(double h) { itsHeight = h; }
    double GetHeight() const   { return itsHeight; }
    double GetWidth() const    { return itsWidth; }
  private:
    double itsWidth;
    double itsHeight;
};
&lt;/source&gt;
This works well but when it comes to Square class, which inherits Rectangle class, it violates LSP even though the '''is-a''' relationship holds between Rectangle and Square. Because square is rectangular. The following example overrides two functions, Setwidth and SetHeight, to fix the problem. But fixing the code implies that the design is faulty.

&lt;source lang=cpp&gt;
public class Square : Rectangle
{
  public:
    virtual void SetWidth(double w);
    virtual void SetHeight(double h);
};
void Square::SetWidth(double w)
{
    Rectangle::SetWidth(w);
    Rectangle::SetHeight(w);
}
void Square::SetHeight(double h)
{
    Rectangle::SetHeight(h);
    Rectangle::SetWidth(h);
}
&lt;/source&gt;

The following example, function g just works for Rectangle class but not for Square, and so the open-closed principle has been violated.

&lt;source lang=cpp&gt;
void g(Rectangle&amp; r)
{
  r.SetWidth(5);
  r.SetHeight(4);
  assert(r.GetWidth() * r.GetHeight()) == 20);
}
&lt;/source&gt;
&lt;ref&gt;{{cite web|title=The Liskov Substitution Principle|url=http://www.objectmentor.com/resources/articles/lsp.pdf|publisher=Robert C. Martin, 1996|accessdate=2 October 2012}}&lt;/ref&gt;

== See also ==

* [[Inheritance (object-oriented programming)]]
* [[Liskov substitution principle]] (in [[object-oriented programming]])
* [[Subsumption (disambiguation)|Subsumption]]&lt;!--This deliberately links to the disambiguation page--&gt;
* Is-a
** [[Hypernymy]] (and [[supertype]])
** [[Hyponymy]] (and [[subtype]])
* [[Has-a]]
** [[Holonymy]]
** [[Meronymy]]

==Notes==
{{reflist|30em}}

==References==
* [[Ronald J. Brachman]]; [http://dblp.uni-trier.de/rec/bibtex/journals/computer/Brachman83 What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks]. IEEE Computer, 16 (10); October 1983
* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57

[[Category:Object-oriented programming]]
[[Category:Knowledge representation]]
[[Category:Abstraction]]
[[Category:Articles with example Java code]]</text>
      <sha1>idyhmbc1oi9n4yan61btcud1ojytwg7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Microformats</title>
    <ns>14</ns>
    <id>10220551</id>
    <revision>
      <id>567179058</id>
      <parentid>548256812</parentid>
      <timestamp>2013-08-04T23:52:55Z</timestamp>
      <contributor>
        <username>Pasqui23</username>
        <id>9191315</id>
      </contributor>
      <comment>category on commons was deleted</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="237" xml:space="preserve">{{Cat main|Microformat}}



[[Category:Knowledge representation]]
[[Category:Metadata publishing]]
[[Category:Metadata]]
[[Category:Semantic HTML]]
[[Category:Semantic Web]]
[[Category:Domain-specific knowledge representation languages]]</text>
      <sha1>smys7h0335u6lk849ybghqyo7gr0bsl</sha1>
    </revision>
  </page>
  <page>
    <title>Living graph</title>
    <ns>0</ns>
    <id>10308920</id>
    <revision>
      <id>613367185</id>
      <parentid>607865037</parentid>
      <timestamp>2014-06-18T02:05:25Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor />
      <comment>/* References */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1617" xml:space="preserve">In terms of knowledge representation, a '''living graph''' (also referred to as a "lifeline", "living timeline"&lt;ref name="history"/&gt; or "fortune line".&lt;ref name="NatStrat"/&gt;) is a graph similar to a [[chronology]] timeline which places events along a vertical axis to reflect changes over time. The vertical axis can be used to represent many factors, such as relative importance, degrees of success/failure, danger/safety or happiness/sadness. In this sense they have been described as being "timelines with attitude".&lt;ref name="history"/&gt;

==References==
{{Reflist|refs=
&lt;ref name="history"&gt; {{Cite web
  | last = Dawson
  | first = Ian
  | authorlink = 
  |author2=Dawson, Patricia Ann
   | title = Introducing Living Graphs
  | work = 
  | publisher = 
  | url = http://thinkinghistory.co.uk/ActivityModel/ActModTimeline.html#graph
  | format = 
  | doi = 
  | accessdate = 25 March 2010}} from Thinking History website
&lt;/ref&gt;
&lt;ref name="NatStrat"&gt; {{Cite web
  | last = 
  | first = 
  | authorlink = 
  | title = Living Graphs and Fortune Lines
  | work = 
  | publisher = The National Strategies
  | url = http://downloads.nationalstrategies.co.uk/pdf/67dbff4bdcf5e5534122e1d6ead53abc.pdf
  | format = pdf
  | doi = 
  | accessdate = 25 March 2010}}&lt;/ref&gt;}}

==External links==
*[http://classtools.net/samples/sample.php?livingGraph Interactive Living Graph Templates in Flash]
*[http://www.face-online.org.uk/index.php?option=com_content&amp;task=view&amp;id=57&amp;Itemid=172 FACE Living Graph Exercise]

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Quality control tools]]
{{comm-design-stub}}</text>
      <sha1>6ahin5esj4kb9cllk309j36r11jl88d</sha1>
    </revision>
  </page>
  <page>
    <title>Frame problem</title>
    <ns>0</ns>
    <id>11306</id>
    <revision>
      <id>744479298</id>
      <parentid>723788348</parentid>
      <timestamp>2016-10-15T13:55:40Z</timestamp>
      <contributor>
        <username>The Anome</username>
        <id>76</id>
      </contributor>
      <comment>"[[block world]]", [[axiom]]s</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="25168" xml:space="preserve">In [[artificial intelligence]], the '''frame problem''' describes an issue with using [[first-order logic]] (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a "[[block world]]" with rules about stacking blocks together. In a FOL system, additional [[axiom]]s are required to make inferences about the environment (for example, that a block cannot change position unless it's physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment.&lt;ref&gt;{{cite journal|last=Hayes|first=Patrick|title=The Frame Problem and Related Problems in Artificial Intelligence|journal=University of Edinburgh|url=http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Hayes-FrameProblem.pdf}}&lt;/ref&gt;

[[John McCarthy (computer scientist)|John McCarthy]] and [[Patrick J. Hayes]] defined this problem in their 1969 article, ''Some Philosophical Problems from the Standpoint of Artificial Intelligence''.  In this paper and many that came after the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment.&lt;ref&gt;{{cite journal|last=McCarthy|first=J|author2=P.J. Hayes|title=Some philosophical problems from the standpoint of artificial intelligence|journal=Machine Intelligence|year=1969|volume=4|pages=463–502}}&lt;/ref&gt;  Later, the term acquired a broader meaning in [[philosophy]], where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged.

==Description==
The frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two [[proposition]]s &lt;math&gt;\textit{open}&lt;/math&gt; and &lt;math&gt;\textit{on}&lt;/math&gt;. If these conditions can change, they are better represented by two [[Predicate (computer programming)|predicate]]s &lt;math&gt;\textit{open}(t)&lt;/math&gt; and &lt;math&gt;\textit{on}(t)&lt;/math&gt; that depend on time; such predicates are called [[fluent (artificial intelligence)|fluent]]s. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic{{clarify|reason=What kind of logic? If ordinary predicate logic is meant, what is the purpose of the 'true →' in the 3rd formula? If some other logic (situation calculus?) is meant, it should be stated explicitly here, together with the purpose of the 'true →' (e.g. some empty action?) in that logic.|date=August 2013}} by the following formulae:

:&lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
:&lt;math&gt;\textit{true} \rightarrow \textit{open}(1)&lt;/math&gt;

The first two formulae represent the initial situation; the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by &lt;math&gt;\neg \textit{locked}(0) \rightarrow \textit{open}(1)&lt;/math&gt;. In practice, one would have a predicate &lt;math&gt;\textit{executeopen}(t)&lt;/math&gt; for specifying when an action is executed and a rule &lt;math&gt;\forall t . \textit{executeopen}(t) \wedge \textit{true} \rightarrow \textit{open}(t+1)&lt;/math&gt; for specifying the effects of actions.  The article on the [[situation calculus]] gives more details.

While the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions (representing the expected situation) are consistent with the three formulae above, they are not the only ones.

:{|
| &lt;math&gt;\neg \textit{open}(0)&lt;/math&gt; &amp;nbsp; &amp;nbsp;  || &lt;math&gt;\textit{open}(1)&lt;/math&gt;
|-
| &lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;   || &lt;math&gt;\neg \textit{on}(1)&lt;/math&gt;
|}

Indeed, another set of conditions that is consistent with the three formulae above is:

:{|
| &lt;math&gt;\neg \textit{open}(0)&lt;/math&gt; &amp;nbsp; &amp;nbsp;  || &lt;math&gt;\textit{open}(1)&lt;/math&gt;
|-
| &lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;   || &lt;math&gt;\textit{on}(1)&lt;/math&gt;
|}

The frame problem is that specifying only which conditions are changed by the actions do not allow, in logic, to conclude that all other conditions are not changed. This problem can be solved by adding the so-called “frame axioms”, which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1:

:&lt;math&gt;\textit{on}(0) \leftrightarrow \textit{on}(1)&lt;/math&gt;

The frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition.{{clarify|reason=Shouldn't then the frame axiom be the following modification of the above rule: '∀t.executeopen(t)→open(t+1)∧(on(t+1)↔on(t))' ? In contrast, the formula 'on(0)↔on(1)' seems to be too particular taylored to the 'executeopen(0)' situation.|date=August 2013}} In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms.

The solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred; this solution is formalized using the framework of [[Circumscription (logic)|circumscription]]. The [[Yale shooting problem]], however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, [[successor state axiom]]s, etc.; they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved{{clarify|reason=Mention the (combination of) approach(es) by which the frame problem was solved.|date=August 2013}}. Even after that, however, the term “frame problem” was still used, in part to refer to the same problem but under different settings (e.g., concurrent actions), and in part to refer to the general problem of representing and reasoning with dynamical domains.

== Solutions ==
The following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full: what is presented are simplified versions that are sufficient to explain the full solution.

===Fluent occlusion solution===
This solution was proposed by [[Erik Sandewall]], who also defined a [[formal language]] for the specification of dynamical domains; therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names.

The rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be ''occluded'' in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as “permission to change”: if a condition is occluded, it is relieved from obeying the constraint of inertia.

In the simplified example of the door and the light, occlusion can be formalized by two predicates &lt;math&gt;\textit{occludeopen}(t)&lt;/math&gt; and &lt;math&gt;\textit{occludeon}(t)&lt;/math&gt;. The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed.

:&lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
:&lt;math&gt;\textit{true} \rightarrow \textit{open}(1) \wedge \textit{occludeopen}(1)&lt;/math&gt;
:&lt;math&gt;\forall t . \neg \textit{occludeopen}(t) \rightarrow (\textit{open}(t-1) \leftrightarrow \textit{open}(t))&lt;/math&gt;
:&lt;math&gt;\forall t . \neg \textit{occludeon}(t) \rightarrow (\textit{on}(t-1) \leftrightarrow \textit{on}(t))&lt;/math&gt;

In general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case, &lt;math&gt;\textit{occludeopen}(1)&lt;/math&gt; is true, making the antecedent of the fourth formula above false for &lt;math&gt;t=1&lt;/math&gt;; therefore, the constraint that &lt;math&gt;\textit{open}(t-1) \leftrightarrow \textit{open}(t)&lt;/math&gt; does not hold for &lt;math&gt;t=1&lt;/math&gt;. Therefore, &lt;math&gt;\textit{open}&lt;/math&gt; can change value, which is also what is enforced by the third formula.

In order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by [[Circumscription (logic)|circumscription]] or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change: for example, executing the action of opening the door when it was already open (in the formalization above) makes the predicate &lt;math&gt;\textit{occludeopen}&lt;/math&gt; true and makes &lt;math&gt;\textit{open}&lt;/math&gt; true; however, &lt;math&gt;\textit{open}&lt;/math&gt; has not changed value, as it was true already.

===Predicate completion solution===
This encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example, &lt;math&gt;\textit{changeopen}(t)&lt;/math&gt; represents the fact that the predicate &lt;math&gt;\textit{open}&lt;/math&gt; will change from time &lt;math&gt;t&lt;/math&gt; to &lt;math&gt;t+1&lt;/math&gt;. As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa.

:&lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{open}(0) \wedge \textit{true} \rightarrow \textit{changeopen}(0)&lt;/math&gt;
:&lt;math&gt;\forall t. \textit{changeopen}(t) \leftrightarrow (\neg \textit{open}(t) \leftrightarrow \textit{open}(t+1))&lt;/math&gt;
:&lt;math&gt;\forall t. \textit{changeon}(t) \leftrightarrow (\neg \textit{on}(t) \leftrightarrow \textit{on}(t+1))&lt;/math&gt;

The third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time &lt;math&gt;t&lt;/math&gt; if and only if the corresponding change predicate is true at time &lt;math&gt;t&lt;/math&gt;. To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions.

===Successor state axioms solution===
The value of a condition after the execution of an action can be determined by
the fact that the condition is true if and only if:

# the action makes the condition true; or
# the condition was previously true and the action does not make it false.

A [[successor state axiom]] is a formalization in logic of these two facts. For
example, if &lt;math&gt;\textit{opendoor}(t)&lt;/math&gt; and &lt;math&gt;\textit{closedoor}(t)&lt;/math&gt; are two
conditions used to denote that the action executed at time &lt;math&gt;t&lt;/math&gt; was
to open or close the door, respectively, the running example is encoded as
follows.

: &lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
: &lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
: &lt;math&gt;\textit{opendoor}(0)&lt;/math&gt;
: &lt;math&gt;\forall t . \textit{open}(t+1) \leftrightarrow \textit{opendoor}(t) \vee (\textit{open}(t) \wedge \neg \textit{closedoor}(t))&lt;/math&gt;

This solution is centered around the value of conditions, rather than the
effects of actions. In other words, there is an axiom for every condition,
rather than a formula for every action. Preconditions to actions (which are not
present in this example) are formalized by other formulae. The successor state
axioms are used in the variant to the [[situation calculus]] proposed by
[[Ray Reiter]].

===Fluent calculus solution===
The [[fluent calculus]] is a variant of the situation calculus. It solves the frame problem by using first-order logic
[[First-order logic#Formation rules|terms]], rather than predicates, to represent the states. Converting
predicates into terms in first order logic is called [[Reification (knowledge representation)|reification]]; the
fluent calculus can be seen as a logic in which predicates representing the
state of conditions are reified.

The difference between a predicate and a term in first order logic is that a term is a representation of an object (possibly a complex object composed of other objects), while a predicate represents a condition that can be true or false when evaluated over a given set of terms.

In the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term &lt;math&gt;\textit{open} \circ \textit{on}&lt;/math&gt;. It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term &lt;math&gt;\textit{open} \circ \textit{on}&lt;/math&gt; represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g., &lt;math&gt;\textit{state}(\textit{open} \circ \textit{on}, 10)&lt;/math&gt; means that this is the state at time &lt;math&gt;10&lt;/math&gt;.

The solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula:

: &lt;math&gt;\textit{state}(s \circ \textit{open}, 1) \leftrightarrow \textit{state}(s,0)&lt;/math&gt;

The action of closing the door, which makes a condition false instead of true, is represented in a slightly different way:

: &lt;math&gt;\textit{state}(s, 1) \leftrightarrow \textit{state}(s \circ \textit{open}, 0)&lt;/math&gt;

This formula works provided that suitable axioms are given about &lt;math&gt;\textit{state}&lt;/math&gt; and &lt;math&gt;\circ&lt;/math&gt;, e.g., a term containing two times the same condition is not a valid state (for example, &lt;math&gt;\textit{state}(\textit{open} \circ s \circ \textit{open}, t)&lt;/math&gt; is always false for every &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt;).

===Event calculus solution===
The [[event calculus]] uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated.

===Default logic solution===
The frame problem can be thought of as the problem of formalizing the principle that, by default, "everything is presumed to remain in the state in which it is" ([[Gottfried Wilhelm Leibniz|Leibniz]], "An Introduction to a Secret Encyclopædia", ''c''. 1679).  This default, sometimes called the ''commonsense law of inertia'', was expressed by [[Raymond Reiter]] in [[default logic]]:

: &lt;math&gt;\frac{R(x,s)\; :\ R(x,\textit{do}(a,s))}{R(x,\textit{do}(a,s))}&lt;/math&gt;

(if &lt;math&gt;R(x)&lt;/math&gt; is true in situation &lt;math&gt;s&lt;/math&gt;, and it can be assumed&lt;ref&gt;i.e., no contradicting information is known&lt;/ref&gt; that &lt;math&gt;R(x)&lt;/math&gt; remains true after executing action &lt;math&gt;a&lt;/math&gt;, then we can conclude that &lt;math&gt;R(x)&lt;/math&gt; remains true).

Steve Hanks and [[Drew McDermott]] argued, on the basis of their [[Yale shooting problem|Yale shooting]] example, that this solution to the frame problem is unsatisfactory.  Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates.

===Answer set programming solution===
The counterpart of the default logic solution in the language of [[answer set programming]] is a rule with [[stable model semantics#Strong negation|strong negation]]:

:&lt;math&gt;r(X,T+1) \leftarrow r(X,T),\ \hbox{not }\sim r(X,T+1)&lt;/math&gt;

(if &lt;math&gt;r(X)&lt;/math&gt; is true at time &lt;math&gt;T&lt;/math&gt;, and it can be assumed that &lt;math&gt;r(X)&lt;/math&gt; remains true at time &lt;math&gt;T+1&lt;/math&gt;, then we can conclude that &lt;math&gt;r(X)&lt;/math&gt; remains true).

===Action description languages===
[[Action description language]]s elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action &lt;math&gt;\textit{opendoor}&lt;/math&gt; makes the door open if not locked is expressed by:

: &lt;math&gt;\textit{opendoor}&lt;/math&gt; causes &lt;math&gt;\textit{open}&lt;/math&gt; if &lt;math&gt;\neg \textit{locked}&lt;/math&gt;

The semantics of an action description language depends on what the language can express (concurrent actions, delayed effects, etc.) and is usually based on [[transition system]]s.

Since domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to [[answer set programming]] rather than first-order logic.

==See also==
* [[Yale shooting problem]]
* [[Binding problem]]
* [[Ramification problem]]
* [[Qualification problem]]
* [[Common sense]]
* [[Commonsense reasoning]]
* [[Defeasible reasoning]]
* [[Non-monotonic logic]]
* [[Symbol grounding]]
* [[Linear logic]]

==Notes==
{{reflist}}

==References==
* {{cite journal | last1 = Doherty | first1 = P. | last2 = Gustafsson | first2 = J. | last3 = Karlsson | first3 = L. | last4 = Kvarnström | first4 = J. | year = 1998 | title = TAL: Temporal action logics language specification and tutorial | url = http://www.ep.liu.se/ej/etai/1998/009 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 273–306 }}
* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1993 | title = Representing action and change by logic programs | url = | journal = Journal of Logic Programming | volume = 17 | issue = | pages = 301–322 | doi=10.1016/0743-1066(93)90035-f}}
* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1998 | title = Action languages | url = http://www.ep.liu.se/ej/etai/1998/007 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 193–210 }}
* {{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic logic and temporal projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379–412 | doi=10.1016/0004-3702(87)90043-9}}
* {{cite journal | last1 = Levesque | first1 = H. | authorlink3 = Raymond Reiter | last2 = Pirri | first2 = F. | last3 = Reiter | first3 = R. | year = 1998 | title = Foundations for the situation calculus | url = http://www.ep.liu.se/ej/etai/1998/005 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 159–178 }}
* {{cite journal | last1 = Liberatore | first1 = P. | year = 1997 | title = The complexity of the language A | url = http://www.ep.liu.se/ej/etai/1997/002 | journal = [[Electronic Transactions on Artificial Intelligence]] | volume = 1 | issue = 1-3| pages = 13–37 }}
* {{cite journal |first=V. |last=Lifschitz |year=2012 |url=http://www.cs.utexas.edu/~vl/papers/jmc.pdf |title=The frame problem, then and now |publisher=[[University of Texas at Austin]]}} Presented at ''Celebration of John McCarthy's Accomplishments'', [[Stanford University]], March 25, 2012.
* {{cite journal | last1 = McCarthy | first1 = J. | last2 = Hayes | first2 = P. J. | year = 1969 | title = Some philosophical problems from the standpoint of artificial intelligence | url = http://www-formal.stanford.edu/jmc/mcchay69.html | journal = Machine Intelligence | volume = 4 | issue = | pages = 463–502 }}
* {{cite journal | last1 = McCarthy | first1 = J. | year = 1986 | title = Applications of circumscription to formalizing common-sense knowledge | url = http://www-formal.stanford.edu/jmc/applications.html | journal = Artificial Intelligence | volume = 28 | issue = | pages = 89–116 | doi=10.1016/0004-3702(86)90032-9}}
* {{cite journal | last1 = Miller | first1 = R. | last2 = Shanahan | first2 = M. | year = 1999 | title = The event-calculus in classical logic - alternative axiomatizations | url = http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html | journal = Electronic Transactions on Artificial Intelligence | volume = 3 | issue = 1| pages = 77–105 }}
* {{cite journal | last1 = Pirri | first1 = F. | last2 = Reiter | first2 = R. | year = 1999 | title = Some contributions to the metatheory of the Situation Calculus | url = | journal = [[Journal of the ACM]] | volume = 46 | issue = 3| pages = 325–361 | doi = 10.1145/316542.316545 }}
* {{cite journal | last1 = Reiter | first1 = R. | authorlink = Raymond Reiter | year = 1980 | title = A logic for default reasoning | url = | journal = Artificial Intelligence | volume = 13 | issue = | pages = 81–132 | doi=10.1016/0004-3702(80)90014-4}}
* {{cite book |authorlink=Raymond Reiter |first=Raymond |last=R. |year=1991 |chapter=The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression |editor=Lifschitz, Vladimir |title=Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy |pages=359–380 |publisher=Academic Press |location=New York}}
* {{cite journal | last1 = Sandewall | first1 = E. | year = 1972 | title = An approach to the Frame Problem and its Implementation | url = | journal = Machine Intelligence | volume = 7 | issue = | pages = 195–204 }}
* {{cite book |first=E. |last=Sandewall |year=1994 |title=Features and Fluents |volume=(vol. 1) |publisher=Oxford University Press |location=New York |isbn=0-19-853845-6}}
* {{cite book |first1=E. |last1=Sandewall |first2=Y. |last2=Shoham |year=1995 |chapter=Non-monotonic Temporal Reasoning |editor1=Gabbay, D. M. |editor2=Hogger, C. J. |editor3=Robinson, J. A. |title=Handbook of Logic in Artificial Intelligence and Logic Programming |volume=(vol. 4) |pages=439–498 |publisher=Oxford University Press |isbn=0-19-853791-3}}
* {{cite journal | last1 = Sandewall | first1 = E. | year = 1998 | title = Cognitive robotics logic and its metatheory: Features and fluents revisited | url = http://www.ep.liu.se/ej/etai/1998/010 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 307–329 }}
* {{cite book |first=M. |last=Shanahan |year=1997 |title=Solving the frame problem: A mathematical investigation of the common sense law of inertia |publisher=MIT Press}}
* {{cite journal | last1 = Thielscher | first1 = M. | year = 1998 | title = Introduction to the fluent calculus | url = http://www.ep.liu.se/ej/etai/1998/006 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 179–192 }}
* {{cite journal | last1 = Toth | first1 = J.A. | year = 1995 | title = Book review. Kenneth M. and Patrick J. Hayes, eds | url = | journal = Reasoning agents in a dynamic world: The frame problem. Artificial Intelligence | volume = 73 | issue = | pages = 323–369 | doi=10.1016/0004-3702(95)90043-8}}
* {{cite journal | last1 = Turner | first1 = H. | year = 1997 | title = Representing actions in logic programs and default theories: a situation calculus approach | url = http://www.d.umn.edu/~hudson/papers/ralpdt6.pdf | format = PDF | journal = Journal of Logic Programming | volume = 31 | issue = | pages = 245–298 | doi=10.1016/s0743-1066(96)00125-2}}

==External links==
* {{cite SEP |url-id=frame-problem |title=The Frame Problem}}
* [http://www-formal.stanford.edu/jmc/mcchay69/mcchay69.html Some Philosophical Problems from the Standpoint of Artificial Intelligence]; the original article of McCarthy and Hayes that proposed the problem.

{{John McCarthy navbox}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Epistemology]]
[[Category:Logic programming]]
[[Category:Philosophical problems]]
[[Category:1969 introductions]]</text>
      <sha1>3ggdzenmo6r4hr9cr5oeu1t5trdn1g3</sha1>
    </revision>
  </page>
  <page>
    <title>Historical Thesaurus of the Oxford English Dictionary</title>
    <ns>0</ns>
    <id>12612212</id>
    <revision>
      <id>752664885</id>
      <parentid>747479544</parentid>
      <timestamp>2016-12-02T15:51:31Z</timestamp>
      <contributor>
        <username>Marcalexander</username>
        <id>29801393</id>
      </contributor>
      <comment>Update logo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7272" xml:space="preserve">{{italic title}}
{{Infobox website
| name            = ''Historical Thesaurus of English''
| logo            = Historical Thesaurus of English logo.png
| logo_size       = &lt;!-- default is 250px --&gt;
| logo_alt        =
| url             = {{URL|www.glasgow.ac.uk/thesaurus}}
| commercial      = No
| type            = Academic
| registration    = None
| content_licence = Free for personal and non-commercial research&lt;ref name=hte-using&gt;{{cite web|title=Using ''Historical Thesaurus'' Data|url=http://historicalthesaurus.arts.gla.ac.uk/using-data/|website=The Historical Thesaurus of English|publisher=University of Glasgow|accessdate=25 October 2014}}&lt;/ref&gt;
| programming_language = 
| owner           = [[University of Glasgow]]
| author          = Marc Alexander and Christian Kay&lt;ref name=hte-cite&gt;{{cite web|last1=Alexander|first1=Marc|last2=Kay|first2=Christian|title=How to Cite|url=http://historicalthesaurus.arts.gla.ac.uk/how-to-cite/|website=The Historical Thesaurus of English, version 4.2|publisher=University of Glasgow|accessdate=25 October 2014}}&lt;/ref&gt;
| editor          = [[Christian Kay]], Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], Irené Wotherspoon, and Marc Alexander (editors)
| current_status  = Version 4.2, since September, 2014&lt;ref name=hte-versions /&gt;
| footnotes       = 
}}
{{Infobox book
|name           = Historical Thesaurus of the Oxford English Dictionary : with additional material from "A Thesaurus of Old English"
|image          = Historical Thesaurus.jpg
|caption        = Print edition of version 1.0 of the ''Historical Thesaurus of English''&lt;ref name="hte-versions" /&gt;
|alt            = Printed boxed set
|author         = Christian Kay, Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], and Irené Wotherspoon (editors)
|title_working  = Historical Thesaurus of English
|country        = Great Britain
|language       = English
|subject        = [[History of the English language]]
|genre          = [[Thesaurus|Thesauri]] 
|published      = 2009 ([[Oxford University Press]])
|pages          = 4,448
|awards         = Scottish Research Book of the Year Award, [[Saltire Society Literary Awards]], 2009
|isbn           = 978-0199208999
|oclc           = 318409912
|dewey          = 
|congress       =  PE1591 .H55 2009
}}

The '''''Historical Thesaurus of the Oxford English Dictionary''''' ('''''HTOED''''') is the print edition of the largest [[thesaurus]] in the world, the '''''Historical Thesaurus of English''''' ('''''HTE'''''), conceived and compiled by the English Language Department of the [[University of Glasgow]]. The ''HTE'' is a complete database of all the words in the second edition of [[Oxford English Dictionary|''The Oxford English Dictionary'']], arranged by [[semantic field]] and date. In this way, the ''HTE'' arranges the whole vocabulary of [[English language|English]], from the earliest written records in [[Old English language|Old English]] to the present, alongside types and dates of use. It is the first historical thesaurus to be compiled for any of the world's languages and contains 800,000 meanings for 600,000 words, within 230,000 categories, covering more than 920,000 words and meanings.&lt;ref name="Woolcock"&gt;{{cite news| url=http://entertainment.timesonline.co.uk/tol/arts_and_entertainment/books/article6644646.ece | location=London | work=The Times | first=Nicola | last=Woolcock | title=After a 44-year labour of love worlds biggest thesaurus is born | date=2009-07-06}}{{subscription required}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last1=Hitchings|first1=Henry|authorlink1=Henry Hitchings|title=Historical Thesaurus is a masterpiece worth waiting 40 years for|url=http://www.telegraph.co.uk/comment/personal-view/6413166/Historical-Thesaurus-is-a-masterpiece-worth-waiting-40-years-for.html|accessdate=25 October 2014|publisher=The Telegraph|date=23 October 2009|ref=Hitchings-2009|location=London}}&lt;/ref&gt;  As the ''HTE'' website states, "in addition to providing hitherto unavailable information for linguistic and textual scholars, the ''Historical Thesaurus'' online is a rich resource for students of social and cultural history, showing how concepts developed through the words that refer to them."&lt;ref name="hte"&gt;{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/ |title=Home page |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}&lt;/ref&gt;

The ambitious project was announced at a 1965 meeting of the [[Philological Society]] by its originator, [[Michael Samuels (academic)|Michael Samuels]].&lt;ref name=Crystal-2014&gt;{{cite book|last1=Crystal|first1=David|authorlink1=David Crystal|title=Words in Time and Place: Exploring Language Through the ''Historical Thesaurus of the Oxford English Dictionary''|date=2014|publisher=Oxford University Press|location=Oxford|isbn=0199680477|page=vii}}&lt;/ref&gt;  Work on the ''HTE'' started in 1965.

On 22 October 2009, after 44 years of work, version 1.0 was published as a two-volume set as ''HTOED''.&lt;ref&gt;{{cite news|url=http://news.bbc.co.uk/1/hi/england/oxfordshire/8136122.stm |title=UK &amp;#124; England &amp;#124; Oxfordshire &amp;#124; Forty-year wait for new thesaurus |publisher=BBC News |date=2009-07-06 |accessdate=2010-04-15}}&lt;/ref&gt; It consists of two slipcased hardcover volumes, totaling nearly 4,000 pages. The ''HTE'', released as version 4.2 in September 2014, is freely available online from the University of Glasgow.&lt;ref name="hte-versions"&gt;{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/versions-and-changes/ |title=Versions of the Thesaurus |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}&lt;/ref&gt;

==Main sections==
The work is divided into three main sections: the External World, the Mind, and Society. These are broken down into successively narrower domains. The text eventually discriminates more than 236,000 categories.
The second order categories are:&lt;ref&gt;{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/classification/ |title=Classification |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-22}} An oversize, one-page listing of all categories in top three tiers is available for download here.&lt;/ref&gt;
{{col-begin-small}}
{{col-break}}
;I. The External World
 
# The Earth
# Life
# Physical sensibility
# Matter
# Existence
# Relative properties
# The Supernatural
{{col-break}} 
;II. The Mind
 
# Soul, spirit, mind
# Emotion/feeling
# Judgement, opinion
# Aesthetics
# Will/faculty of will
# Expectation
# Having/possession
# Languages
{{col-break}}
;III. Society
 
# Society/life in association with others
# Inhabiting/dwelling
# Relations between social groups
# Authority
# Law
# Education
# Religion
# Communications
# Travel/travelling
# Work / Serious occupation
# Leisure/The Arts
{{col-end}}

==References==
{{reflist}}

==External links==
* {{cite web|title=Search|url=http://historicalthesaurus.arts.gla.ac.uk/search/|website=The Historical Thesaurus of English|publisher=University of Glasgow}} {{open access}}
{{Dictionaries of English}}

[[Category:Thesauri]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Language histories]]
[[Category:History of the English language]]</text>
      <sha1>qhgui4h1sxxjw9ce74o94i8izwic2pr</sha1>
    </revision>
  </page>
  <page>
    <title>Yale shooting problem</title>
    <ns>0</ns>
    <id>2778728</id>
    <revision>
      <id>697559148</id>
      <parentid>697559115</parentid>
      <timestamp>2015-12-31T07:43:14Z</timestamp>
      <contributor>
        <username>Koavf</username>
        <id>205121</id>
      </contributor>
      <minor />
      <comment>fixed [[MOS:DASH|dashes]] using a [[User:GregU/dashes.js|script]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7552" xml:space="preserve">The '''Yale shooting problem''' is a conundrum or scenario in formal situational [[logic]] on which early logical solutions to the [[frame problem]] fail. The name of this problem derives from its inventors, [[Steve Hanks]] and [[Drew McDermott]], working at [[Yale University]] when they proposed it. In this scenario, Fred (later identified as a [[turkey (bird)|turkey]]) is initially alive and a gun is initially unloaded. Loading the gun, waiting for a moment, and then shooting the gun at Fred is expected to kill Fred. However, if [[inertia]] is formalized in logic by minimizing the changes in this situation, then it cannot be uniquely proved that Fred is dead after loading, waiting, and shooting. In one solution, Fred indeed dies; in another (also logically correct) solution, the gun becomes mysteriously unloaded and Fred survives.

Technically, this scenario is described by two [[fluent (artificial intelligence)|fluents]] (a fluent is a condition that can change [[truth value]] over time): &lt;math&gt;alive&lt;/math&gt; and &lt;math&gt;loaded&lt;/math&gt;. Initially, the first condition is true and the second is false. Then, the gun is loaded, some time passes, and the gun is fired. Such problems can be formalized in logic by considering four time points &lt;math&gt;0&lt;/math&gt;, &lt;math&gt;1&lt;/math&gt;, &lt;math&gt;2&lt;/math&gt;, and &lt;math&gt;3&lt;/math&gt;, and turning every fluent such as &lt;math&gt;alive&lt;/math&gt; into a predicate &lt;math&gt;alive(t)&lt;/math&gt; depending on time. A direct formalization of the statement of the Yale shooting problem in logic is the following one:

: &lt;math&gt;alive(0)&lt;/math&gt;
: &lt;math&gt;\neg loaded(0)&lt;/math&gt;
: &lt;math&gt;true \rightarrow loaded(1)&lt;/math&gt;
: &lt;math&gt;loaded(2) \rightarrow \neg alive(3)&lt;/math&gt;

The first two formulae represent the initial state. The third formula formalizes the effect of loading the gun at time &lt;math&gt;0&lt;/math&gt;. The fourth formula formalizes the effect of shooting at Fred at time &lt;math&gt;2&lt;/math&gt;. This is a simplified formalization in which action names are neglected and the effects of actions are directly specified for the time points in which the actions are executed. See [[situation calculus]] for details.

The formulae above, while being direct formalizations of the known facts, do not suffice to correctly characterize the domain. Indeed, &lt;math&gt;\neg alive(1)&lt;/math&gt; is consistent with all these formulae, although there is no reason to believe that Fred dies before the gun has been shot. The problem is that the formulae above only include the effects of actions, but do not specify that all fluents not changed by the actions remain the same. In other words, a formula &lt;math&gt;alive(0) \equiv alive(1)&lt;/math&gt; must be added to formalize the implicit assumption that loading the gun ''only'' changes the value of &lt;math&gt;loaded&lt;/math&gt; and not the value of &lt;math&gt;alive&lt;/math&gt;. The necessity of a large number of formulae stating the obvious fact that conditions do not change unless an action changes them is known as the [[frame problem]].

An early solution to the frame problem was based on minimizing the changes. In other words, the scenario is formalized by the formulae above (that specify only the effects of actions) and by the assumption that the changes in the fluents over time are as minimal as possible. The rationale is that the formulae above enforce all effect of actions to take place, while minimization should restrict the changes to exactly those due to the actions.

In the Yale shooting scenario, one possible evaluation of the fluents in which the changes are minimized is the following one.

{| cellpadding="5"
| &lt;math&gt;alive(0)&lt;/math&gt;
| &lt;math&gt;alive(1)&lt;/math&gt; 
| &lt;math&gt;alive(2)&lt;/math&gt; 
| &lt;math&gt;\neg alive(3)&lt;/math&gt;
|-
| &lt;math&gt;\neg loaded(0)&lt;/math&gt;
| &lt;math&gt;loaded(1)&lt;/math&gt;
| &lt;math&gt;loaded(2)&lt;/math&gt;
| &lt;math&gt;loaded(3)&lt;/math&gt;
|}

This is the expected solution. It contains two fluent changes: &lt;math&gt;loaded&lt;/math&gt; becomes true at time 1 and &lt;math&gt;alive&lt;/math&gt; becomes false at time 3. The following evaluation also satisfies all formulae above.

{| cellpadding="5"
| &lt;math&gt;alive(0)&lt;/math&gt;
| &lt;math&gt;alive(1)&lt;/math&gt; 
| &lt;math&gt;alive(2)&lt;/math&gt; 
| &lt;math&gt;alive(3)&lt;/math&gt;
|-
| &lt;math&gt;\neg loaded(0)&lt;/math&gt;
| &lt;math&gt;loaded(1)&lt;/math&gt;
| &lt;math&gt;\neg loaded(2)&lt;/math&gt;
| &lt;math&gt;\neg loaded(3)&lt;/math&gt;
|}

In this evaluation, there are still two changes only: &lt;math&gt;loaded&lt;/math&gt; becomes true at time 1 and false at time 2. As a result, this evaluation is considered a valid description of the evolution of the state, although there is no valid reason to explain &lt;math&gt;loaded&lt;/math&gt; being false at time 2. The fact that minimization of changes leads to wrong solution is the motivation for the introduction of the Yale shooting problem.

While the Yale shooting problem has been considered a severe obstacle to the use of logic for formalizing dynamical scenarios, solutions to it are known since the late 1980s. One solution involves the use of [[predicate completion]] in the specification of actions: according to this solution, the fact that shooting causes Fred to die is formalized by the preconditions: ''alive'' and ''loaded'', and the effect is that ''alive'' changes value (since ''alive'' was true before, this corresponds to ''alive'' becoming false). By turning this implication into an ''if and only if'' statement, the effects of shooting are correctly formalized. (Predicate completion is more complicated when there is more than one implication involved.)

A solution proposed by [[Erik Sandewall]] was to include a new condition of occlusion, which formalizes the “permission to change” for a fluent. The effect of an action that might change a fluent is therefore that the fluent has the new value, and that the occlusion is made (temporarily) true. What is minimized is not the set of changes, but the set of occlusions being true. Another constraint specifying that no fluent changes unless occlusion is true completes this solution.

The Yale shooting scenario is also correctly formalized by the [[Ray Reiter|Reiter]] version of the [[situation calculus]], the [[fluent calculus]], and the [[action description language]]s.

In 2005, the 1985 paper in which the Yale shooting scenario was first described received the [[AAAI Classic Paper award]]. In spite of being a solved problem, that example is still sometimes mentioned in recent research papers, where it is used as an illustrative example (e.g., for explaining the syntax of a new logic for reasoning about actions), rather than being presented as a problem.

==See also==

* [[Circumscription (logic)]]
* [[Frame problem]]
* [[Situation calculus]]

==References==

* M. Gelfond and V. Lifschitz (1993). Representing action and change by logic programs. ''Journal of Logic Programming'', 17:301–322.
* S. Hanks and D. McDermott (1987). Nonmonotonic logic and temporal projection. ''Artificial Intelligence'', 33(3):379–412.
* J. McCarthy (1986). Applications of circumscription to formalizing common-sense knowledge. ''Artificial Intelligence'', 28:89–116.
* T. Mitchell and H. Levesque (2006). The 2005 AAAI Classic Paper awards. "AI Magazine", 26(4):98–99.
* R. Reiter (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Vladimir Lifschitz, editor, ''Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy'', pages 359–380. Academic Press, New York.
* E. Sandewall (1994). ''Features and Fluents''. Oxford University Press.

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:1987 introductions]]</text>
      <sha1>8gx9o9sexs4v9cv997s926nq1pdgfx4</sha1>
    </revision>
  </page>
  <page>
    <title>Belief revision</title>
    <ns>0</ns>
    <id>1187311</id>
    <revision>
      <id>746894877</id>
      <parentid>745138546</parentid>
      <timestamp>2016-10-30T06:09:07Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.6)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="43581" xml:space="preserve">'''Belief revision''' is the process of changing beliefs to take into account a new piece of information. The [[formal logic|logical]] formalization of belief revision is researched in [[philosophy]], in [[databases]], and in artificial intelligence for the design of [[intelligent agent|rational agent]]s.

What makes belief revision non-trivial is that several different ways for performing this operation may be possible. For example, if the current knowledge includes the three facts "&lt;math&gt;A&lt;/math&gt; is true", "&lt;math&gt;B&lt;/math&gt; is true" and "if &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are true then &lt;math&gt;C&lt;/math&gt; is true", the introduction of the new information "&lt;math&gt;C&lt;/math&gt; is false" can be done preserving consistency only by removing at least one of the three facts. In this case, there are at least three different ways for performing revision. In general, there may be several different ways for changing knowledge.

==Revision and update==

Two kinds of changes are usually distinguished:

; update : the new information is about the situation at present, while the old beliefs refer to the past; update is the operation of changing the old beliefs to take into account the change;

; revision : both the old beliefs and the new information refer to the same situation; an inconsistency between the new and old information is explained by the possibility of old information being less reliable than the new one; revision is the process of inserting the new information into the set of old beliefs without generating an inconsistency.

The main assumption of belief revision is that of minimal change: the knowledge before and after the change should be as similar as possible. In the case of update, this principle formalizes the assumption of inertia. In the case of revision, this principle enforces as much information as possible to be preserved by the change.

===Example===

The following classical example shows that the operations to perform in the two settings of update and revision are not the same. The example is based on two different interpretations of the set of beliefs &lt;math&gt;\{a \vee b\}&lt;/math&gt; and the new piece of information &lt;math&gt;\neg a&lt;/math&gt;:

; update : in this scenario, two satellites, Unit A and Unit B, orbit around Mars; the satellites are programmed to land while transmitting their status to Earth; Earth has received a transmission from one of the satellites, communicating that it is still in orbit; however, due to interference, it is not known which satellite sent the signal; subsequently, Earth receives the communication that Unit A has landed; this scenario can be modeled in the following way; two [[propositional variable]]s &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; indicate that Unit A and Unit B, respectively, are still in orbit; the initial set of beliefs is &lt;math&gt;\{a \vee b\}&lt;/math&gt; (either one of the two satellites is still in orbit) and the new piece of information is &lt;math&gt;\neg a&lt;/math&gt; (Unit A has landed, and is therefore not in orbit); the only rational result of the update is &lt;math&gt;\neg a&lt;/math&gt;; since the initial information that one of the two satellites had not landed yet was possibly coming from the Unit A, the position of the Unit B is not known;

; revision : the play "Six Characters in Search of an Author" will be performed in one of the two local theatres; this information can be denoted by &lt;math&gt;\{a \vee b\}&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; indicates that the play will be performed at the first or at the second theatre, respectively; a further information that "Jesus Christ Superstar" will be performed at the first theatre indicates that &lt;math&gt;\neg a&lt;/math&gt; holds; in this case, the obvious conclusion is that "Six Characters in Search of an Author" will be performed at the second but not the first theatre, which is represented in logic by &lt;math&gt;\neg a \wedge b&lt;/math&gt;.

This example shows that revising the belief &lt;math&gt;a \vee b&lt;/math&gt; with the new information &lt;math&gt;\neg a&lt;/math&gt; produces two different results &lt;math&gt;\neg a &lt;/math&gt; and &lt;math&gt;\neg a \wedge b&lt;/math&gt; depending on whether the setting is that of update or revision.

==Contraction, expansion, revision, consolidation, and merging==

In the setting in which all beliefs refer to the same situation, a distinction between various operations that can be performed is made:

; contraction : removal of a belief;

; expansion : addition of a belief without checking consistency;

; revision : addition of a belief while maintaining consistency;

; consolidation : restoring consistency of a set of beliefs;

; merging : fusion of two or more sets of beliefs while maintaining consistency.

Revision and merging differ in that the first operation is done when the new belief to incorporate is considered more reliable than the old ones; therefore, consistency is maintained by removing some of the old beliefs. Merging is a more general operation, in that the priority among the belief sets may or may not be the same.

Revision can be performed by first incorporating the new fact and then restoring consistency via consolidation. This is actually a form of merging rather than revision, as the new information is not always treated as more reliable than the old knowledge.

==The AGM postulates==

The AGM postulates (named after the names of their proponents, [[Carlos Alchourrón|Alchourrón]], [[Peter Gärdenfors|Gärdenfors]], and [[David Makinson|Makinson]]) are properties that an operator that performs revision should satisfy in order for that operator to be considered rational. The considered setting is that of revision, that is, different pieces of information referring to the same situation. Three operations are considered: expansion (addition of a belief without a consistency check), revision (addition of a belief while maintaining consistency), and contraction (removal of a belief).

The first six postulates are called "the basic AGM postulates". In the settings considered by Alchourrón, Gärdenfors, and Makinson, the current set of beliefs is represented by a [[Deductive closure|deductively closed]] set of logical formulae &lt;math&gt;K&lt;/math&gt; called belief base, the new piece of information is a logical formula &lt;math&gt;P&lt;/math&gt;, and revision is performed by a binary operator &lt;math&gt;*&lt;/math&gt; that takes as its operands the current beliefs and the new information and produces as a result a belief base representing the result of the revision. The &lt;math&gt;+&lt;/math&gt; operator denoted expansion: &lt;math&gt;K+P&lt;/math&gt; is the deductive closure of &lt;math&gt;K \cup \{P\}&lt;/math&gt;. The AGM postulates for revision are:

# Closure: &lt;math&gt;K*P&lt;/math&gt; is a belief base (i.e., a deductively closed set of formulae);
# Success: &lt;math&gt;P \in K*P&lt;/math&gt;
# Inclusion: &lt;math&gt;K*P \subseteq K+P&lt;/math&gt;
# Vacuity: &lt;math&gt;\text{If }(\neg P) \not \in K,\text{ then }K*P=K+P&lt;/math&gt;
# &lt;math&gt;K*P&lt;/math&gt; is [[inconsistent]] only if &lt;math&gt;P&lt;/math&gt; is inconsistent or &lt;math&gt;K&lt;/math&gt; is inconsistent
# Extensionality: &lt;math&gt;\text{If }P\text{ and }Q\text{ are logically equivalent, then }K*P=K*Q&lt;/math&gt; (see [[logical equivalence]])
# &lt;math&gt;K*(P \wedge Q) \subseteq (K*P)+Q&lt;/math&gt;
# &lt;math&gt;\text{If }(\neg Q) \not\in K*P\text{ then }(K*P)+Q \subseteq K*(P \wedge Q)&lt;/math&gt;

A revision operator that satisfies all eight postulates is the full meet revision, in which &lt;math&gt;K*P&lt;/math&gt; is equal to &lt;math&gt;K+P&lt;/math&gt; if consistent, and to the deductive closure of &lt;math&gt;P&lt;/math&gt; otherwise. While satisfying all AGM postulates, this revision operator has been considered to be too conservative, in that no information from the old knowledge base is maintained if the revising formula is inconsistent with it.{{Citation needed|date=November 2011}}

==Conditions equivalent to the AGM postulates==

The AGM postulates are equivalent to several different conditions on the revision operator; in particular, they are equivalent to the revision operator being definable in terms of structures known as selection functions, epistemic entrenchments, systems of spheres, and preference relations. The latter are [[reflexive relation|reflexive]], [[transitive relation|transitive]], and [[total relation]]s over the set of models.

Each revision operator &lt;math&gt;*&lt;/math&gt; satisfying the AGM postulates is associated to a set of preference relations &lt;math&gt;\leq_K&lt;/math&gt;, one for each possible belief base &lt;math&gt;K&lt;/math&gt;, such that the models of &lt;math&gt;K&lt;/math&gt; are exactly the minimal of all models according to &lt;math&gt;\leq_K&lt;/math&gt;. The revision operator and its associated family of orderings are related by the fact that &lt;math&gt;K*P&lt;/math&gt; is the set of formulae whose set of models contains all the minimal models of &lt;math&gt;P&lt;/math&gt; according to &lt;math&gt;\leq_K&lt;/math&gt;. This condition is equivalent to the set of models of &lt;math&gt;K*P&lt;/math&gt; being exactly the set of the minimal models of &lt;math&gt;P&lt;/math&gt; according to the ordering &lt;math&gt;\leq_K&lt;/math&gt;.

A preference ordering &lt;math&gt;\leq_K&lt;/math&gt; represents an order of implausibility among all situations, including those that are conceivable but yet currently considered false. The minimal models according to such an ordering are exactly the models of the knowledge base, which are the models that are currently considered the most likely. All other models are greater than these ones, and are indeed considered less plausible. In general, &lt;math&gt;I &lt;_K J&lt;/math&gt; indicates that the situation represented by the model &lt;math&gt;I&lt;/math&gt; is believed to be more plausible than the situation represented by &lt;math&gt;J&lt;/math&gt;. As a result, revising by a formula having &lt;math&gt;I&lt;/math&gt; and &lt;math&gt;J&lt;/math&gt; as models should select only &lt;math&gt;I&lt;/math&gt; to be a model of the revised knowledge base, as this model represent the most likely scenario among those supported by &lt;math&gt;P&lt;/math&gt;.

==Contraction==

Contraction is the operation of removing a belief &lt;math&gt;P&lt;/math&gt; from a knowledge base &lt;math&gt;K&lt;/math&gt;; the result of this operation is denoted by &lt;math&gt;K-P&lt;/math&gt;. The operators of revision and contractions are related by the Levi and Harper identities:

: &lt;math&gt;K*P=(K-\neg P)+P&lt;/math&gt;
: &lt;math&gt;K-P=K \cap (K*\neg P)&lt;/math&gt;

Eight postulates have been defined for contraction. Whenever a revision operator satisfies the eight postulates for revision, its corresponding contraction operator satisfies the eight postulates for contraction, and vice versa. If a contraction operator satisfies at least the first six postulates for contraction, translating it into a revision operator and then back into a contraction operator using the two identities above leads to the original contraction operator. The same holds starting from a revision operator.

One of the postulates for contraction has been longly discussed: the recovery postulate:

: &lt;math&gt;K=(K-P)+P&lt;/math&gt;

According to this postulate, the removal of a belief &lt;math&gt;P&lt;/math&gt; followed by the reintroduction of the same belief in the belief base should lead to the original belief base. There are some examples showing that such behavior is not always reasonable: in particular, the contraction by a general condition such as &lt;math&gt;a \vee b&lt;/math&gt; leads to the removal of more specific conditions such as &lt;math&gt;a&lt;/math&gt; from the belief base; it is then unclear why the reintroduction of &lt;math&gt;a \vee b&lt;/math&gt; should also lead to the reintroduction of the more specific condition &lt;math&gt;a&lt;/math&gt;. For example, if George was previously believed to have German citizenship, it was also believed to be European. Contracting this latter belief amounts to stop believing that George is European; therefore, that George has German citizenship is also retracted from the belief base. If George is later discovered to have Austrian citizenship, then the fact that he is European is also reintroduced. According to the recovery postulate, however, the belief that he also has German citizenship should also be reintroduced.

The correspondence between revision and contraction induced by the Levi and Harper identities is such that a contraction not satisfying the recovery postulate is translated into a revision satisfying all eight postulates, and that a revision satisfying all eight postulates is translated into a contraction satisfying all eight postulates, including recovery. As a result, if recovery is excluded from consideration, a number of contraction operators are translated into a single revision operator, which can be then translated back into exactly one contraction operator. This operator is the only one of the initial group of contraction operators that satisfies recovery; among this group, it is the operator that preserves as much information as possible.

==The Ramsey test==
&lt;!--[[Ramsey test]], [[Ramsey Test]], [[Ramsey's test]], [[Ramsey's Test]] redirect here.--&gt;

The evaluation of a [[counterfactual conditional]] &lt;math&gt;a &gt; b&lt;/math&gt; can be done, according to the '''Ramsey test''' (named for [[Frank P. Ramsey]]), to the hypothetical addition of &lt;math&gt;a&lt;/math&gt; to the set of current beliefs followed by a check for the truth of &lt;math&gt;b&lt;/math&gt;. If &lt;math&gt;K&lt;/math&gt; is the set of beliefs currently held, the Ramsey test is formalized by the following correspondence:

: &lt;math&gt;a &gt; b&lt;/math&gt; if and only if &lt;math&gt;b \in K * a&lt;/math&gt;

If the considered language of the formulae representing beliefs is propositional, the Ramsey test gives a consistent definition for counterfactual conditionals in terms of a belief revision operator. However, if the language of formulae representing beliefs itself includes the counterfactual conditional connective &lt;math&gt;&gt;&lt;/math&gt;, the Ramsey test leads to the Gardenfors triviality result: there is no non-trivial revision operator that satisfies both the AGM postulates for revision and the condition of the Ramsey test. This result holds in the assumption that counterfactual formulae like &lt;math&gt;a&gt;b&lt;/math&gt; can be present in belief bases and revising formulae. Several solutions to this problem have been proposed.

==Non-monotonic inference relation==

Given a fixed knowledge base &lt;math&gt;K&lt;/math&gt; and a revision operator &lt;math&gt;*&lt;/math&gt;, one can define a non-monotonic inference relation using the following definition: &lt;math&gt;P \vdash Q&lt;/math&gt; if and only if &lt;math&gt;K*P \models Q&lt;/math&gt;. In other words, a formula &lt;math&gt;P&lt;/math&gt; [[logical consequence|entails]] another formula &lt;math&gt;Q&lt;/math&gt; if the addition of the first formula to the current knowledge base leads to the derivation of &lt;math&gt;Q&lt;/math&gt;. This inference relation is non-monotonic.

The AGM postulates can be translated into a set of postulates for this inference relation. Each of these postulates is entailed by some previously considered set of postulates for non-monotonic inference relations. Vice versa, conditions that have been considered for non-monotonic inference relations can be translated into postulates for a revision operator. All these postulates are entailed by the AGM postulates.

==Foundational revision==

In the AGM framework, a belief set is represented by a deductively closed set of [[propositional formula]]e. While such sets are infinite, they can always be finitely representable. However, working with deductively closed sets of formulae leads to the implicit assumption that equivalent belief bases should be considered equal when revising. This is called the ''principle of irrelevance of syntax''.

This principle has been and is currently debated: while &lt;math&gt;\{a, b\}&lt;/math&gt; and &lt;math&gt;\{a \wedge b\}&lt;/math&gt; are two equivalent sets, revising by &lt;math&gt;\neg a&lt;/math&gt; should produce different results. In the first case, &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are two separate beliefs; therefore, revising by &lt;math&gt;\neg a&lt;/math&gt; should not produce any effect on &lt;math&gt;b&lt;/math&gt;, and the result of revision is &lt;math&gt;\{\neg a, b\}&lt;/math&gt;. In the second case, &lt;math&gt;a \wedge b&lt;/math&gt; is taken a single belief. The fact that &lt;math&gt;a&lt;/math&gt; is false contradicts this belief, which should therefore be removed from the belief base. The result of revision is therefore &lt;math&gt;\{\neg a\}&lt;/math&gt; in this case.

The problem of using deductively closed knowledge bases is that no distinction is made between pieces of knowledge that are known by themselves and pieces of knowledge that are merely consequences of them. This distinction is instead done by the ''foundational'' approach to belief revision, which is related to [[foundationalism]] in philosophy. According to this approach, retracting a non-derived piece of knowledge should lead to retracting all its consequences that are not otherwise supported (by other non-derived pieces of knowledge).  This approach can be realized by using knowledge bases that are not deductively closed and assuming that all formulae in the knowledge base represent self-standing beliefs, that is, they are not derived beliefs. In order to distinguish the foundational approach to belief revision to that based on deductively closed knowledge bases, the latter is called the ''coherentist'' approach. This name has been chosen because the coherentist approach aims at restoring the coherence
(consistency) among ''all'' beliefs, both self-standing and derived ones. This approach is related to [[coherentism]] in philosophy.

Foundationalist revision operators working on non-deductively closed belief bases typically select some subsets of &lt;math&gt;K&lt;/math&gt; that are consistent with &lt;math&gt;P&lt;/math&gt;, combined them in some way, and then conjoined them with &lt;math&gt;P&lt;/math&gt;. The following are two non-deductively closed base revision operators.

; WIDTIO : (When in Doubt, Throw it Out) the maximal subsets of &lt;math&gt;K&lt;/math&gt; that are consistent with &lt;math&gt;P&lt;/math&gt; are intersected, and &lt;math&gt;P&lt;/math&gt; is added to the resulting set; in other words, the result of revision is composed by &lt;math&gt;P&lt;/math&gt; and of all formulae of &lt;math&gt;K&lt;/math&gt; that are in all maximal subsets of &lt;math&gt;K&lt;/math&gt; that are consistent with &lt;math&gt;P&lt;/math&gt;;

; Ginsberg-Fagin-Ullman-Vardi : the maximal subsets of &lt;math&gt;K \cup \{P\}&lt;/math&gt; that are consistent and contain &lt;math&gt;P&lt;/math&gt; are combined by disjunction;

; Nebel : similar to the above, but a priority among formulae can be given, so that formulae with higher priority are less likely to being retracted than formulae with lower priority.

A different realization of the foundational approach to belief revision is based on explicitly declaring the dependences among beliefs. In the [[truth maintenance system]]s, dependence links among beliefs can be specified. In other worlds, one can explicitly declare that a given fact is believed because of one or more other facts; such a dependency is called a ''justification''. Beliefs not having any justifications play the role of non-derived beliefs in the non-deductively closed knowledge base approach.

==Model-based revision and update==

A number of proposals for revision and update based on the set of models of the involved formulae were developed independently of the AGM framework. The principle behind this approach is that a knowledge base is equivalent to a set of ''possible worlds'', that is, to a set of scenarios that are considered possible according to that knowledge base. Revision can therefore be performed on the sets of possible worlds rather than on the corresponding knowledge bases.

The revision and update operators based on models are usually identified by the name of their authors: [[Marianne Winslett|Winslett]], Forbus, Satoh, Dalal, Hegner, and Weber. According to the  first four of these proposal, the result of revising/updating a formula &lt;math&gt;K&lt;/math&gt; by another formula &lt;math&gt;P&lt;/math&gt; is characterized by the set of models of &lt;math&gt;P&lt;/math&gt; that are the closest to the models of &lt;math&gt;K&lt;/math&gt;. Different notions of closeness can be defined, leading to the difference among these proposals.

; Dalal : the models of &lt;math&gt;P&lt;/math&gt; having a minimal [[Hamming distance]] to models of &lt;math&gt;K&lt;/math&gt; are selected to be the models that result from the change;

; Satoh : similar to Dalal, but distance between two models is defined as the set of literals that are given different values by them; similarity between models is defined as set containment of these differences;

; Winslett : for each model of &lt;math&gt;K&lt;/math&gt;, the closest models of &lt;math&gt;P&lt;/math&gt; are selected; comparison is done using set containment of the difference;

; Borgida : equal to Winslett's if &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; are inconsistent; otherwise, the result of revision is &lt;math&gt;K \wedge P&lt;/math&gt;;

; [[Ken Forbus|Forbus]] : similar to Winslett, but the Hamming distance is used.

The revision operator defined by Hegner makes &lt;math&gt;K&lt;/math&gt; not to affect  the value of the variables that are mentioned in &lt;math&gt;P&lt;/math&gt;. What results from this operation is a formula &lt;math&gt;K'&lt;/math&gt; that is consistent with &lt;math&gt;P&lt;/math&gt;, and can therefore be conjoined with it. The revision operator by Weber is similar, but the literals that are removed from &lt;math&gt;K&lt;/math&gt; are not all literals of &lt;math&gt;P&lt;/math&gt;, but only the literals that are evaluated differently by a pair of closest models of &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; according to the Satoh measure of closeness.

==Iterated revision==

The AGM postulates are equivalent to a preference ordering (an ordering over models) to be associated to every knowledge base &lt;math&gt;K&lt;/math&gt;. However, they do not relate the orderings corresponding to two non-equivalent knowledge bases. In particular, the orderings associated to a knowledge base &lt;math&gt;K&lt;/math&gt; and its revised version &lt;math&gt;K*P&lt;/math&gt; can be completely different. This is a problem for performing a second revision, as the ordering associated with &lt;math&gt;K*P&lt;/math&gt; is necessary to calculate &lt;math&gt;K*P*Q&lt;/math&gt;.

Establishing a relation between the ordering associated with &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;K*P&lt;/math&gt; has been however recognized not to be the right solution to this problem. Indeed, the preference relation should depend on the previous history of revisions, rather than on the resulting knowledge base only. More generally, a preference relation gives more information about the state of mind of an agent than a simple knowledge base. Indeed, two states of mind might represent the same piece of knowledge &lt;math&gt;K&lt;/math&gt; while at the same time being different in the way a new piece of knowledge would be incorporated. For example, two people might have the same idea as to where to go on holiday, but yet they differ on how they would change this idea if they win a million-dollar lottery. Since the basic condition of the preference ordering is that their minimal models are exactly the models of their associated knowledge base, a knowledge base can be considered implicitly represented by a preference ordering (but not vice versa).

Given that a preference ordering allows deriving its associated knowledge base but also allows performing a single step of revision, studies on iterated revision have been concentrated on how a preference ordering should be changed in response of a revision. While single-step revision is about how a knowledge base &lt;math&gt;K&lt;/math&gt; has to be changed into a new knowledge base &lt;math&gt;K*P&lt;/math&gt;, iterated revision is about how a preference ordering (representing both the current knowledge and how much situations believed to be false are considered possible) should be turned into a new preference relation when &lt;math&gt;P&lt;/math&gt; is learned. A single step of iterated revision produces a new ordering that allows for further revisions.

Two kinds of preference ordering are usually considered: numerical and non-numerical. In the first case, the level of plausibility of a model is  representing by a non-negative integer number; the lower the rank, the more plausible the situation corresponding to the model. Non-numerical preference orderings correspond to the preference relations used in the AGM framework: a possibly total ordering over models. The non-numerical preference relation were initially considered unsuitable for iterated revision because of the impossibility of reverting a revision by a number of other revisions, which is instead possible in the numerical case.

Darwiche and [[Judea Pearl|Pearl]]&lt;ref name="darwiche-pearl"&gt;Darwiche, A. and Pearl, J. (1997) On the logic of iterated belief revision.  ''Artificial Intelligence'' '''89'''(1-2): 1-29.&lt;/ref&gt; formulated the following postulates for iterated revision.

# if &lt;math&gt;\alpha \models \mu&lt;/math&gt; then &lt;math&gt;(\psi * \mu) * \alpha \equiv \psi * \alpha&lt;/math&gt;;
# if &lt;math&gt;\alpha \models \neg \mu&lt;/math&gt;, then &lt;math&gt;(\psi * \mu) * \alpha \equiv \psi * \alpha&lt;/math&gt;;
# if &lt;math&gt;\psi * \alpha \models \mu&lt;/math&gt;, then &lt;math&gt;(\psi * \mu) * \alpha \models \mu&lt;/math&gt;;
# if &lt;math&gt;\psi * \alpha \not\models \neg \mu&lt;/math&gt;, then &lt;math&gt;(\psi * \mu) * \alpha \not\models \neg \mu&lt;/math&gt;.

Specific iterated revision operators have been proposed by Spohn, Boutilier, Williams, Lehmann, and others.

; Spohn rejected revision : this non-numerical proposal has been first considered by Spohn, who rejected it based on the fact that revisions can change some orderings in such a way the original ordering cannot be restored with a sequence of other revisions; this operator change a preference ordering in view of new information &lt;math&gt;P&lt;/math&gt; by making all models of &lt;math&gt;P&lt;/math&gt; being preferred over all other models; the original preference ordering is maintained when comparing two models that are both models of &lt;math&gt;P&lt;/math&gt; or both non-models of &lt;math&gt;P&lt;/math&gt;;

; Natural revision : while revising a preference ordering by a formula &lt;math&gt;P&lt;/math&gt;, all minimal models (according to the preference ordering) of &lt;math&gt;P&lt;/math&gt; are made more preferred by all other ones; the original ordering of models is preserved when comparing two models that are not minimal models of &lt;math&gt;P&lt;/math&gt;; this operator changes the ordering among models minimally while preserving the property that the models of the knowledge base after revising by &lt;math&gt;P&lt;/math&gt; are the minimal models of &lt;math&gt;P&lt;/math&gt; according to the preference ordering;

; Transmutations : these are two forms of revision, conditionalization and adjustment, which work on numerical preference orderings; revision requires not only a formula but also a number indicating its degree of plausibility; while the preference ordering is still inverted (the lower a model, the most plausible it is) the degree of plausibility of a revising formula is direct (the higher the degree, the most believed the formula is);

; Ranked revision : a ranked model, which is an assignment of non-negative integers to models, has to be specified at the beginning; this rank is similar to a preference ordering, but is not changed by revision; what is changed by a sequence of revisions are a current set of models (representing the current knowledge base) and a number called the rank of the sequence; since this number can only monotonically non-decrease, some sequences of revision lead to situations in which every further revision is performed as a full meet revision.

==Merging==

The assumption implicit in the revision operator is that the new piece of information &lt;math&gt;P&lt;/math&gt; is always to be considered more reliable than the old knowledge base &lt;math&gt;K&lt;/math&gt;. This is formalized by the second of the AGM postulates: &lt;math&gt;P&lt;/math&gt; is always believed after revising &lt;math&gt;K&lt;/math&gt; with &lt;math&gt;P&lt;/math&gt;. More generally, one can consider the process of merging several pieces of information (rather than just two) that might or might not have the same reliability. Revision becomes the particular instance of this process when a less reliable piece of information &lt;math&gt;K&lt;/math&gt; is merged with a more reliable &lt;math&gt;P&lt;/math&gt;.

While the input to the revision process is a pair of formulae &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt;, the input to merging is a [[multiset]] of formulae &lt;math&gt;K&lt;/math&gt;, &lt;math&gt;T&lt;/math&gt;, etc. The use of multisets is necessary as two sources to the merging process might be identical.

When merging a number of knowledge bases with the same degree of plausibility, a distinction is made between arbitration and majority. This distinction depends on the assumption that is made about the information and how it has to be put together.

; arbitration : the result of arbitrating two knowledge bases &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;T&lt;/math&gt; entails &lt;math&gt;K \vee T&lt;/math&gt;; this condition formalizes the assumption of maintaining as much as the old information as possible, as it is equivalent to imposing that every formula entailed by both knowledge bases is also entailed by the result of their arbitration; in a possible world view, the "real" world is assumed one of the worlds considered possible according to at least one of the two knowledge bases;

; majority : the result of merging a knowledge base &lt;math&gt;K&lt;/math&gt; with other knowledge bases can be forced to entail &lt;math&gt;K&lt;/math&gt; by adding a sufficient number of other knowledge bases equivalent to &lt;math&gt;K&lt;/math&gt;; this condition corresponds to a kind of vote-by-majority: a sufficiently large number of knowledge bases can always overcome the "opinion" of any other fixed set of knowledge bases.

The above is the original definition of arbitration. According to a newer definition, an arbitration operator is a merging operator that is insensitive to the number of equivalent knowledge bases to merge. This definition makes arbitration the exact opposite of majority.

Postulates for both arbitration and merging have been proposed. An example of an arbitration operator satisfying all postulates is the classical disjunction. An example of a majority operator satisfying all postulates is that selecting all models that have a minimal total Hamming distance to models of the knowledge bases to merge.

A merging operator can be expressed as a family of orderings over models, one for each possible multiset of knowledge bases to merge: the models of the result of merging a multiset of knowledge bases are the minimal models of the ordering associated to the multiset. A merging operator defined in this way satisfies the postulates for merging if and only if the family of orderings meets a given set of conditions. For the old definition of arbitration, the orderings are not on models but on pairs (or, in general, tuples) of models.

==Social choice theory==

Many revision proposals involve orderings over models representing the relative plausibility of the possible alternatives. The problem of merging amounts to combine a set of orderings into a single one expressing the combined  plausibility of the alternatives. This is similar with what is done in [[social choice theory]], which is the study of how the preferences of a group of agents can be combined in a rational way. Belief revision and social choice theory are similar in that they combine a set of orderings into one. They differ on how these orderings are interpreted: preferences in social choice theory; plausibility in belief revision. Another difference is that the alternatives are explicitly enumerated in social choice theory, while they are the propositional models over a given alphabet in belief revision.

==Complexity==

The problem about belief revision that is the most studied from the point of view of [[Computational complexity theory|computational complexity]] is that of query answering in the propositional case. This is the problem of establishing whether a formula follows from the result of a revision, that is, &lt;math&gt;K*P \models Q&lt;/math&gt;, where &lt;math&gt;K&lt;/math&gt;, &lt;math&gt;P&lt;/math&gt;, and &lt;math&gt;Q&lt;/math&gt; are propositional formulae. More generally, query answering is the problem of telling whether a formula is entailed by the result of a belief revision, which could be update, merging, revision, iterated revision, etc. Another problem that has received some attention is that of model checking, that is, checking whether a model satisfies the result of a belief revision. A related question is whether such result can be represented in space polynomial in that of its arguments.

Since a deductively closed knowledge base is infinite, complexity studies on belief revision operators working on deductively closed knowledge bases are done in the assumption that such deductively closed knowledge base are given in the form of an equivalent finite knowledge base.

A distinction is made among belief revision operators and belief revision schemes. While the former are simple mathematical operators mapping a pair of formulae into another formula, the latter depend on further information such as a preference relation. For example, the Dalal revision is an operator because, once two formulae &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; are given, no other information is needed to compute &lt;math&gt;K*P&lt;/math&gt;. On the other hand, revision based on a preference relation is a revision scheme, because &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; do not allow determining the result of revision if the family of preference orderings between models is not given. The complexity for revision schemes is determined in the assumption that the extra information needed to compute revision is given in some compact form. For example, a preference relation can be represented by a sequence of formulae whose models are increasingly preferred. Explicitly storing the relation as a set of pairs of models is instead not a compact representation of preference because the space required is exponential in the number of propositional letters.

The complexity of query answering and model checking in the propositional case is in the second level of the [[polynomial hierarchy]] for most belief revision operators and schemas. Most revision operators suffer from the problem of representational blow up: the result of revising two formulae is not necessarily representable in space polynomial in that of the two original formulae. In other words, revision may exponentially increase the size of the knowledge base.

==Implementations==

Systems specifically implementing belief revision are: [http://portal.acm.org/citation.cfm?id=122296.122301 Immortal], [https://web.archive.org/web/20051018054730/http://magic.it.uts.edu.au:80/systems/saten.html SATEN], and [http://www.dis.uniroma1.it/~liberato/brels/brels.html BReLS]. Two systems including a belief revision feature are [http://www.cse.buffalo.edu/sneps/ SNePS] and [[Cyc]]. [[Truth maintenance systems]] are used in [[Artificial Intelligence]] to implement belief revision.

==See also==

* [[Artificial intelligence]]
* [[Inquiry]]
* [[Knowledge representation]]
* [[Belief propagation]]
* [[Reason maintenance]]
* [[Epistemic closure]]
* [[Non-monotonic logic]]
* [[Defeasible reasoning]]
* [[Reasoning]]
* [[Philosophy of science]]
* [[Discursive dilemma]]

==Notes==
{{reflist}}

==References==
* C. E. Alchourròn, P. Gärdenfors, and D. Makinson (1985). On the logic of theory change: Partial meet contraction and revision functions. ''Journal of Symbolic Logic'', 50:510–530.
* C. Boutilier (1993). Revision sequences and nested conditionals. In ''Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI'93)'', pages 519–525.
* C. Boutilier (1995). Generalized update: belief change in dynamic settings. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1550–1556.
* C. Boutilier (1996). Abduction to plausible causes: an event-based model of belief update. ''Artificial Intelligence'', 83:143–166.
* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (1999). The size of a revised knowledge base. ''Artificial Intelligence'', 115(1):25–64.
* T. Chou and [[Marianne Winslett|M. Winslett]] (1991). Immortal: A model-based belief revision system. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 99–110. Morgan Kaufmann Publishers.
* M. Dalal (1988). Investigations into a theory of knowledge base revision: Preliminary report. In ''Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI'88)'', pages 475–479.
* T. Eiter and G. Gottlob (1992). On the complexity of propositional knowledge base revision, updates and counterfactuals. ''Artificial Intelligence'', 57:227–270.
* T. Eiter and G. Gottlob (1996). The complexity of nested counterfactuals and iterated knowledge base revisions. ''Journal of Computer and System Sciences'', 53(3):497–512.
* R. Fagin, J. D. Ullman, and M. Y. Vardi (1983). On the semantics of updates in databases. In ''Proceedings of the Second ACM SIGACT SIGMOD Symposium on Principles of Database Systems (PODS'83)'', pages 352–365.
* M. A. Falappa, G. Kern-Isberner, G. R. Simari (2002): Explanations, belief revision and defeasible reasoning. ''Artificial Intelligence'', 141(1–2): 1–28.
* M. Freund and D. Lehmann (2002). Belief Revision and Rational Inference. [http://arxiv.org/abs/cs.AI/0204032 Arxiv preprint cs.AI/0204032].
* N. Friedman and J. Y. Halpern (1994). A knowledge-based framework for belief change, part II: Revision and update. In ''Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94)'', pages 190–200.
* A. Fuhrmann (1991). Theory contraction through base contraction. ''Journal of Philosophical Logic'', 20:175–203.
* D. Gabbay, G. Pigozzi, and J. Woods (2003). Controlled Revision&amp;nbsp;– An algorithmic approach for belief revision, ''Journal of Logic and Computation'', 13(1): 15–35.
* P. Gärdenfors and D. Makinson (1988). Revision of knowledge systems using epistemic entrenchment. In ''Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge (TARK'88)'', pages 83–95.
* P. Gärdenfors and H. Rott (1995). Belief revision. In ''Handbook of Logic in Artificial Intelligence and Logic Programming, Volume 4'', pages 35–132. Oxford University Press.
* G. Grahne and [[Alberto O. Mendelzon]] (1995). Updates and subjunctive queries. ''Information and Computation'', 2(116):241–252.
* G. Grahne, [[Alberto O. Mendelzon]], and P. Revesz (1992). Knowledge transformations. In ''Proceedings of the Eleventh ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'92)'', pages 246–260.
* S. O. Hansson (1999). ''A Textbook of Belief Dynamics''. Dordrecht: Kluwer Academic Publishers.
* A. Herzig (1996). The PMA revised. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 40–50.
* A. Herzig (1998). Logics for belief base updating. In D. Dubois, D. Gabbay, H. Prade, and P. Smets, editors, ''Handbook of defeasible reasoning and uncertainty management'', volume 3 – Belief Change, pages 189–231. Kluwer Academic Publishers.
* H. Katsuno and A. O. Mendelzon (1991). On the difference between updating a knowledge base and revising it. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 387–394.
* H. Katsuno and A. O. Mendelzon (1991). Propositional knowledge base revision and minimal change. ''Artificial Intelligence'', 52:263–294.
* S. Konieczny and R. Pino Perez (1998). On the logic of merging. In ''Proceedings of the Sixth International Conference on Principles of Knowledge Representation and Reasoning (KR'98)'', pages 488–498.
* D. Lehmann (1995). Belief revision, revised. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1534–1540.
* P. Liberatore (1997). The complexity of iterated belief revision. In ''Proceedings of the Sixth International Conference on Database Theory (ICDT'97)'', pages 276–290.
* P. Liberatore and M. Schaerf (1998). Arbitration (or how to merge knowledge bases). ''IEEE Transactions on Knowledge and Data Engineering'', 10(1):76–90.
* P. Liberatore and M. Schaerf (2000). BReLS: A system for the integration of knowledge bases. In ''Proceedings of the Seventh International Conference on Principles of Knowledge Representation and Reasoning (KR 2000)'', pages 145–152.
* D. Makinson (1985). How to give up: A survey of some formal aspects of the logic of theory change. ''Synthese'', 62:347–363.
* A. Perea (2003). ''Proper Rationalizability and Belief Revision in Dynamic Games''. Research Memoranda 048: METEOR, Maastricht Research School of Economics of Technology and Organization.
* B. Nebel (1991). Belief revision and default reasoning: Syntax-based approaches. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 417–428.
* B. Nebel (1994). Base revision operations and schemes: Semantics, representation and complexity. In ''Proceedings of the Eleventh European Conference on Artificial Intelligence (ECAI'94)'', pages 341–345.
* B. Nebel (1996). How hard is it to revise a knowledge base? Technical Report 83, Albert-Ludwigs-Universität Freiburg, Institut für Informatik.
* G. Pigozzi (2005). Two aggregation paradoxes in social decision making: the Ostrogorski paradox and the [[discursive dilemma]], ''Episteme: A Journal of Social Epistemology'', 2(2): 33–42.
* G. Pigozzi (2006). [http://pigozzi.org/Pigozzi_Judgment_Aggregation.pdf Belief merging and the discursive dilemma: an argument-based account to paradoxes of judgment aggregation]. ''Synthese'' 152(2): 285–298.
* P. Z. Revesz (1993). On the semantics of theory change: Arbitration between old and new information. In ''Proceedings of the Twelfth ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'93)'', pages 71–82.
* K. Satoh (1988). Nonmonotonic reasoning by minimal belief revision. In ''Proceedings of the International Conference on Fifth Generation Computer Systems (FGCS'88)'', pages 455–462.
* {{cite book | last1=Shoham | first1=Yoav | last2=Leyton-Brown | first2=Kevin | title=Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations | publisher=[[Cambridge University Press]] | isbn=978-0-521-89943-7 | url=http://www.masfoundations.org | year=2009 | location=New York}} See Section 14.2; [http://www.masfoundations.org/download.html downloadable free online].
* V. S. Subrahmanian (1994). Amalgamating knowledge bases. ''ACM Transactions on Database Systems'', 19(2):291–331.
* A. Weber (1986). Updating propositional formulas. In ''Proc. of First Conf. on Expert Database Systems'', pages 487–500.
* M. Williams (1994). Transmutations of knowledge systems. In ''Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94)'', pages 619–629.
* M. Winslett (1989). Sometimes updates are circumscription. In ''Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI'89)'', pages 859–863.
* M. Winslett (1990). ''Updating Logical Databases''. Cambridge University Press.
* Y. Zhang and N. Foo (1996). Updating knowledge bases with disjunctive information. In ''Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI'96)'', pages 562–568.

==External links==
* {{PhilPapers|category|belief-revision}}
* {{InPho|idea|1448|Logic of Belief Revision}}
* {{cite SEP |url-id=logic-belief-revision |title=Logic of Belief Revision}}
* [http://www.beliefrevision.org/ Beliefrevision.org]
* [http://plato.stanford.edu/entries/reasoning-defeasible/#4.3 Defeasible Reasoning: 4.3 Belief Revision Theory] at [[Stanford Encyclopedia of Philosophy]]

[[Category:Belief revision| ]]
[[Category:Belief]]
[[Category:Formal epistemology]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Logic programming]]</text>
      <sha1>s94nkucl2eowe8xhx9arbrigdbgnnfv</sha1>
    </revision>
  </page>
  <page>
    <title>SERVQUAL</title>
    <ns>0</ns>
    <id>2755912</id>
    <revision>
      <id>761417820</id>
      <parentid>758350469</parentid>
      <timestamp>2017-01-22T21:59:01Z</timestamp>
      <contributor>
        <username>BronHiggs</username>
        <id>29331605</id>
      </contributor>
      <comment>add marketing template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28349" xml:space="preserve">{{marketing}}

'''SERVQUAL''' is a multi-dimensional research instrument, designed to capture consumer expectations and perceptions of a service along the five dimensions that are believed to represent service quality. SERVQUAL is built on the expectancy-disconfirmation paradigm, which in simple terms means that service quality is understood as the extent to which consumers' pre-consumption expectations of quality are confirmed or disconfirmed by their actual perceptions of the service experience. When the SERVQUAL questionnaire was first published in 1988 by a team of academic researchers, A. Parasurman, [[Valarie Zeithaml]] and [[Leonard Berry (professor)|Leonard L. Berry]]  to measure quality in the service sector,&lt;ref&gt;Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vo. 62,  no. 1, 1988, pp 12-40 &lt;online:https://www.researchgate.net/publication/225083802_SERVQUAL_A_multiple-_Item_Scale_for_measuring_consumer_perceptions_of_service_quality&gt;&lt;/ref&gt; it represented a breakthrough in the measurement methods used for service quality research. The diagnostic value of the instrument is supported by the ''model of service quality'' which forms the conceptual framework for the development of the scale (i.e. instrument or questionnaire). The instrument has been widely applied in a variety of contexts and cultural settings and found to be relatively robust. It has become the dominant measurement scale in the area of service quality. In spite of the long-standing interest in SERVQUAL and its myriad of context-specific applications, it has attracted some criticism from researchers.

==The SERVQUAL instrument==

[[File:Measuring service quality using SERVQUAL model (Kumar et al, 2009).png|thumb|The five dimensions of service quality)]]

SERVQUAL is a multidimensional instrument (i.e. questionnaire or measurement scale) designed to measure service quality by capturing respondents’ expectations and perceptions along the five dimensions of service quality.&lt;ref&gt;Parasuraman, A., Berry, L.L. and Zeithaml, V.A.,  “Refinement and Reassessment of the SERVQUAL scale,” ''Journal of Retailing,'' Vol. 67, no. 4, 1991, pp 57-67&lt;/ref&gt;  The questionnaire consists of matched pairs of items; 22 expectation items and 22 perceptions items, organised into five dimensions which are believed to  align with the consumer’s mental map of service quality dimensions. Both the expectations component and the perceptions component of the questionnaire consist a total of 22 items, comprising 4 items to capture tangibles, 5 items to capture reliabiility, 4 items for responsiveness, 5 items for assurance and 5 items to capture empathy.&lt;ref&gt;Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, p. 25&lt;/ref&gt; The questionnaire is designed to be administered in a face-to-face interview and requires a moderate to large size sample for statistical reliability. In practice, it is customary to add additional items such as the respondent's demographics, prior experience with the brand or category and behavioural intentions (intention to revisit/ repurchase, loyalty intentions and propensity to give word-of-mouth referrals). Thus, the final questionnaire may have up to 60 items and typically takes at least one hour, per respondent, to administer. The length of the questionnaire combined with sample size requirements contribute to substantial costs in administration and data analysis.


{| class="wikitable"
|+ Summary of SERVQUAL items &lt;ref&gt;Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, p. 22, 25 and 29&lt;/ref&gt;
|-
! Dimension
! No. of Items in Questionnaire
! Definition
|-
| '''Reliability'''
| colspan="1" style="text-align: center;" | 5 
| The ability to perform the promised service dependably and accurately 
|-
| '''Assurance'''
| colspan="1" style="text-align: center;" | 5 
| The knowledge and courtesy of  employees and their ability to convey trust and confidence
|-
| '''Tangibles'''
| colspan="1" style="text-align: center;" | 4 
| The appearance of physical facilities, equipment, personnel and communication materials 
|-
| '''Empathy'''
| colspan="1" style="text-align: center;" | 5 
| The provision of caring, individualized attention to customer
|-
| '''Responsiveness'''
| colspan="1" style="text-align: center;" | 4 
| The willingness to help customers and to provide prompt service
|}

The instrument was developed over a five year period; was tested, pre-tested and refined before appearing in its final form. The instrument's developers, claim that it is a highly reliable and valid instrument.&lt;ref&gt;Zeithaml, V., Parasuraman, A. and Berry, L.L., ''Delivering Service Quality: Balancing Customer Perceptions and Expectations,'' N.Y., The Free Press, 1990&lt;/ref&gt; Certainly, it has been widely used and adapted in service quality research for numerous industries and various geographic regions. In application, many researchers are forced to make minor modifications to the instrument as necessary for context-specific applications. Some researchers label their revised instruments with innovative labels such as EDUQUAL (educational context),&lt;ref&gt;Mahapatra, S.S. and Khan, M.S., "A Methodology for Evalution of Service Quality Using Neural Networks," in ''Proceedings of the International Conference on Global Manufacturing and Innovation,' July 27–29, 2006&lt;/ref&gt; HEALTHQUAL (hospital context) &lt;ref&gt;Lee, D., "HEALTHQUAL: a multi-item scale for assessing healthcare service quality," Service Business, 2016; pp 1-26, doi:10.1007/s11628-016-0317-2&lt;/ref&gt; and ARTSQUAL (art museum).&lt;ref&gt;Higgs, B., Polonsky M.J. and Hollick, M., “Measuring Expectations: Pre and Post Consumption: Does It Matter?” ''Journal of Retailing and Consumer Services'', vol. 12, no. 1, 2005&lt;/ref&gt;


{| class="wikitable"
|+ Examples of matched pairs of items in the SERVQUAL questionnaire &lt;ref&gt;Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, [Appendix: SERVQUAL questionnaire, pp 37-40&lt;/ref&gt;
|-
! Dimension
! Sample expectations item
! Sample perceptions item
|-
| '''Reliability'''
| When excellent telephone companies promise to do something by a certain time, they do so
| XYZ company provides it services at the promised time 
|-
| '''Assurance'''
| The behaviour of employees in excellent banks will instill confidence in customers
| The behaviour of employees in the XYZ bank instils confidence in you.
|-
| '''Tangibles'''
| Excellent telephone companies will have modern looking equipment
| XYZ company has modern looking equipment
|-
| '''Empathy'''
| Excellent banks will have operating hours convenient to customers
| XYZ bank has convenient operating hours
|-
| '''Responsiveness'''
| Employees of excellent telephone companies will never be too busy to help a customer
| XYZ employees are never too busy to help you
|}

The SERVQUAL questionnaire has been described as "the most popular standardized questionnaire to measure service quality." &lt;ref&gt;Caruanaa,A., Ewing, M.T and Ramaseshanc, B., "Assessment of the Three-Column Format SERVQUAL: An Experimental Approach," ''Journal of Business Research,'' Vol. 49, no. 1, July 2000, pp 57–65&lt;/ref&gt; It is widely used by service firms, most often in conjunction with other measures of service quality and customer satisfaction. The SERVQUAL instrument was developed as part of a broader conceptualisation of how customers understand service quality. This conceptualisation is known as the ''model of service quality'' or more popularly as the ''gaps model.''

==The model of service quality==

The model of service quality, popularly known as the ''gaps model'' was developed by a group of American authors, A. Parasuraman, [[Valarie Zeithaml|Valarie A. Zeithaml]] and [[Leonard Berry (professor)|Len Berry]], in a systematic research program carrie out between 1983 and 1988. The model identifies the principal dimensions (or components) of service quality; proposes a scale for measuring service quality (SERVQUAL) and suggests possible causes of service quality problems. The model's developers originally identified [[SERVQUAL#Determinants|ten dimensions of service quality]], but after testing and retesting, some of the dimensions were found to be autocorrelated and the total number of dimensions was reduced to five, namely - reliability, assurance, tangibles, empathy and responsiveness.  These five dimensions are thought to represent the dimensions of service quality across a range of industries and settings. &lt;ref&gt;Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," ''Journal of Marketing,'' Vol. 52, No. 2, 1988, pp. 35-48 &lt;/ref&gt; Among students of marketing, the memnonic,  '''RATER''', an acronym formed from the first letter of each of the five dimensions is often used as an aid to recall.

[[File:Servqual.jpg|thumb|A simplified model of service quality]]

Businesses use the SERVQUAL instrument (i.e. questionnaire) to measure potential service quality problems and the model of service quality to help diagnose possible causes of the problem. The model of service quality is built on the ''expectancy-confirmation paradigm'' which suggests that consumers perceive quality in terms of their perceptions of how well a given service delivery meets their expectations of that delivery.&lt;ref&gt;Oliver, R.L., Balakrishnan, P.V. S. and Barry, B., "Outcome Satisfaction in Negotiation: A Test of Expectancy Disconfirmation," ''Organizational Behavior and Human Decision Processes,'' Vol. 60, no. 2, 1994, Pages 252-275&lt;/ref&gt; Thus, service quality can be conceptualised as a simple equation:

'''SQ = P- E'''
: where;

: '''SQ''' is service quality
: '''P''' is the individual's perceptions of given service delivery
: '''E''' is the individual's expectations of a given service delivery

When customer expectations are greater than their perceptions of received delivery, service quality is deemed low. When perceptions exceed expectations then service quality is high. The model of service quality identifies five gaps that may cause customers to experience poor service quality. In this model, gap 5 is the service quality gap and is the ''only'' gap that can be directly measured. In other words, the SERVQUAL instrument was specifically designed to capture gap 5. In contrast, Gaps 1-4 cannot be measured, but have diagnostic value.


{| class="wikitable"
|+ Summary of Gaps with Diagnostic Indications &lt;ref&gt;Based on Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," ''Journal of Marketing,'' Vol. 52, No. 2, 1988, pp. 35-48 &lt;/ref&gt;
|-
! '''Gap'''
! '''Brief description'''
! '''Probable Causes
|-
| '''Gap 1'''
The Knowledge Gap
| Difference between the target market’s expected service and management’s perceptions of the target market’s expected service
| 
* Insufficient marketing research
* Inadequate upward communications
* Too many layers of management
|-
| '''Gap 2'''
The standards Gap 
| Difference between management’s perceptions of customer expectations and the translation into service procedures and specifications
| 
* Lack of management commitment to service quality
* Employee perceptions of infeasibility
* Inadequate goal setting
* Inadequate task standardisation
|-
|'''Gap 3''' 
The Delivery Gap
| Difference between service quality specifications and the service actually delivered
| 
* Technical breakdowns or malfunctions
*  Role conflict/ ambiguity
*  Lack of perceived control
* Poor employee-job fit
* Poor technology- fit
* Poor supervision or training
|-
| '''Gap 4''' 
The Communications Gap
| Difference between service delivery intentions and what is communicated to the customer
| 
*Lack of horizontal communications
* Poor communication with advertising agency
* Inadequate communications between sales and operations
* Differences in policies and procedures across branches or divisions of an entity
* Propensity to overpromise
|}

== Development of the model ==

The development of the model of service quality involved a systematic research undertaking which began in 1983 and after various refinements resulted in the publication of the SERVQUAL instrument in 1988.&lt;ref&gt;Parasuraman, A., Berry, L.L.,  Zeithaml, V. A., "Understanding Customer Expectations of Service," ''Sloan Management Review,'' Vol. 32, no. 3, 1991, p. 39&lt;/ref&gt; The model's developers began with an exhaustive literature search in order to identify items that were believed to impact on perceived service quality. This initial search identified some 100 items which were used in the first rounds of consumer testing. Prelimiary data analysis, using a data reduction technique known as [[factor analysis]] (also known as [[principal components analysis]]) revealed that these items loaded onto ten dimensions (or components) of service quality. The initial ten dimensions that were believed to represent service quality were:

# '''[[competence (human resources)|Competence]]''' is the possession of the required skills and knowledge to perform the service. For example, there may be competence in the knowledge and skill of contact personnel, knowledge and skill of operational support personnel and research capabilities of the organization.
# '''Courtesy''' is the consideration for the customer's property and a clean and neat appearance of contact personnel, manifesting as politeness, respect, and friendliness.
# '''Credibility''' includes factors such as trustworthiness, belief and honesty. It involves having the customer's best interests at prime position. It may be influenced by company name, company reputation and the personal characteristics of the contact personnel.
# '''Security''' enables the customer to feel free from danger, risk or doubt including physical safety, financial security and confidentiality.
# '''Access''' is approachability and ease of contact. For example, convenient office operation hours and locations.
# '''Communication''' means both informing customers in a language they are able to understand and also listening to customers. A company may need to adjust its language for the varying needs of its customers. Information might include for example, explanation of the service and its cost, the relationship between services and costs and assurances as to the way any problems are effectively managed.
# '''Knowing the customer''' means making an effort to understand the customer's individual needs, providing individualized attention, recognizing the customer when they arrive and so on. This in turn helps to delight the customers by rising above their expectations. 
# '''Tangibles''' are the physical evidence of the service, for instance, the  appearance of the physical facilities, tools and equipment used to provide the service; the appearance of personnel and communication materials and the presence of other customers in the service facility.
# '''Reliability'''  is the ability to perform the promised service in a dependable and accurate manner. The service is performed correctly on the first occasion, the accounting is correct, records are up to date and schedules are kept.
# '''Responsiveness''' is the readiness and willingness of employees to help customers by providing prompt timely services, for example, mailing a transaction slip immediately or setting up appointments quickly.

Further testing suggested that some of the ten preliminary dimensions of service quality were closely related or autocorrelated. Thus the ten initial dimensions were reduced and the labels amended to accurately reflect the revised dimensions. By the early 1990s, the authors had refined the model to five factors which in testing, appear to be relatively stable and robust.

# '''Reliability:''' the ability to perform the promised service dependably and accurately
# '''Assurance:''' the knowledge and courtesy of employees and their ability to convey trust and confidence
# '''Tangibles:''' the appearance of physical facilities, equipment, personnel and communication materials
# '''Empathy:''' the provision of caring, individualized attention to customers
# '''Responsiveness:''' the willingness to help customers and to provide prompt service

These are the five dimensions of service quality that form the basis of the individual items in the SERVQUAL research instrument (questionnaire). The acronym RATER, is often used to help students of marketing remember the five dimensions of quality explicitly mentioned in the research instrument.

Nyeck, Morales, Ladhari, and Pons (2002) stated the SERVQUAL measuring tool “appears to remain the most complete attempt to conceptualize and measure service quality” (p.&amp;nbsp;101). The SERVQUAL measuring tool has been used by many researchers across a wide range of service industries and contexts, such as healthcare, banking, financial services, and education (Nyeck, Morales, Ladhari, &amp; Pons, 2002).

== Criticisms of SERVQUAL and the model of service quality ==

Although the SERVQUAL instrument has been widely applied in a variety of industry and cross-cultural contexts, there are many criticisms of the approach. Francis Buttle published one of the most comprehensive criticisms of the model of service quality and the associated SERVQUAL instrument in 1996 in which both operational and theoretical concerns were identified.&lt;ref&gt;Buttle, F., “SERVQUAL: Review, Critique, Research Agenda,"  ''European Journal of Marketing,'' Vol. 30, no.  1, pp. 8-32 1996&lt;/ref&gt; Some of the more important criticisms include:

: ''Face validity'': The model of service quality has its roots in the expectancy-disconfimation paradigm that informs customer satisfaction.&lt;ref&gt;Oliver, R.L., ''Satisfaction: A Behavioural Perspective on the Consumer,'' Boston, MA, Irwin McGraw-Hill, 1996&lt;/ref&gt; A number of researchers have argued that the research instrument actually captures ''satisfaction'' rather than ''service quality''.&lt;ref&gt;Souca, Ma. L., "SERVQUAL - Thirty years of research on service quality with implications for customer satisfaction," in ''Marketing - from Information to Decision,'' [Proceedings of the International Conference], Cluj-Napoca: Babes Bolyai University, 2011, pp 420 -429&lt;/ref&gt; Other researchers have questioned the validity of conceptualising service quality as a gap.&lt;ref&gt;van Dyke, T.P.,   Kappelman, L.A. and Prybutok, V.R.,"Measuring Information Systems Service Quality: Concerns on the Use of the SERVQUAL Questionnaire," ''MIS Quarterly,'' Vol. 21, No. 2, 1997, pp. 195-208, &lt;Online:  http://www.jstor.org/stable/249419&gt;&lt;/ref&gt;

: ''Construct validity'': The model's developers tested and retested the SERVQUAL scale for reliability and validity. However, at the same time the model's developers recommended that applied use of the instrument should modify or adapt the for specific contexts. Any attempt to adapt or modify the scale will have implications for the validity of items with implications for the validity of the dimensions of reliability, assurance, tangibles, empathy and repsonsiveness.&lt;ref&gt;Smith, A.M., "Measuring Service Quality: Is SERVQUAL now redundant?  ''Journal of Marketing Management,'' [Special Issue: Marketing in the Services Sector],  Vol 11, no. 1, 1995, pp 257-276&lt;/ref&gt;
 
: ''Ambiguity of expectations construct'': SERVQUAL is designed to be administered after respondents have experienced a service. They are therefore asked to ''recall'' their pre-experience expectations. However,  recall is not always accurate, raising concerns about whether the research design accurately captures true pre-consumption expectations. In addition, studies show that expectations actually change over time. Consumers are continually modifying their expectations as they gain experience with a product category or brand.&lt;ref&gt;Parasuraman, A.; Berry, Leonard L.; Zeithaml, Valarie A., "Understanding Customer Expectations of Service," ''Sloan Management Review,'' Vol. 32, no. 3, 1991, pp 39 - 48&lt;/ref&gt; In light of these insights, concerns have been raised about whether the act of experiencing the service might colour respondents' expectations.

: The way that expectations has been operationalised also represents a concern for theorists investigating the validity of the gaps model. The literature identifies different types of expectations.&lt;ref&gt;Parasuraman, A., Zeithaml, V. A., Berry, L. L., "Reassessment of Expectations as a Comparison Standard in Measuring Service Quality: Implications for Further Research," ''Journal of Marketing,'' Vol. 58 January 1994, pp 111–124&lt;/ref&gt; Of these, there is an argument that only ''forecast expectations'' are true expectations. Yet, the SERVQUAL instrument appears to elicit ''ideal expectations''.&lt;ref&gt;Johnson, C. and Mathews, B.P. , "The influence of experience on service expectations", ''International Journal of Service Industry Management,'' Vol. 8 no. 4, pp 290-305&lt;/ref&gt;  Note the wording in the questionnaire in the preceding figure which grounds respondents in their expectations of what ''excellent'' companies will do. Subtle use of words can elicit different types of expectations.  Capturing true expectations is important because it has implications for service quality scores. When researchers elicit ideal expectations, overall service quality scores are likely to be lower, making it much more difficult for marketers to deliver on those expectations.&lt;ref&gt;Boulding, W., Kalra, A., Staelin, R. and Zeithaml, V. A., "Dynamic Process Model of Service Quality: From Expectations to Behavioral Intentions," ''Journal of Marketing Research,'' Vol. 30, no 1, 1993, pp 7-27&lt;/ref&gt;

:''Questionnaire length:''  The matched pairs design of the questionnaire (total of 22 expectation items plus 22 perception items= 44 total items) makes for a very long questionnaire. If researchers add demographic and other behavioural items such as prior experience with product or category and the standard battery of demographics including: age, gender, occupation, educational attainment etc. then the average questionnaire will have around 60 items. In practical terms, this means that the questionnaire would take more than one hour per respondent to administer in a face-to-face interview. Lengthy questionnaires are known to induce ''respondent fatigue'' which may have potential implications for data reliability. In addition, lengthy questionnaires add to the time and cost involved in data collection and data analysis. Coding, collation and interpretation of data is very time consuming and in the case of lengthy questionnaires administered across large samples, the findings cannot be used to address urgent quality-related problems. In some cases, it may be necessary to carry out 'quick and dirty' research while waiting for the findings of studies with superior research design.

: Some analysts have pointed out that the SERVPERF instrument, developed by Cronin and Taylor,&lt;ref&gt;Cronin, J. J. and Taylor, S. A., "Measuring Service Quality: A Re-examination and Extension," ''Journal of Marketing,'' Vol. 56, no. 3, 1992, pp 55-68.&lt;/ref&gt;&lt;ref&gt;Cronin J.J., Steven, J. and Taylor, A., "SERVPERF versus SERVQUAL: Reconciling performance based  and  perceptions-minus-expectations  measurement  of  service  quality," ''Journal  of  Marketing,''  Vol.  58, January, 1994, pp. 125-131&lt;/ref&gt; and which reduced the number of questionnaire items by half (22 perceptions items only), achieves results that correlate well with SERVQUAL, with no reduction in diagnostic power, improved data accuracy through reductions in respondent boredom and fatigue and savings in the form of reduced administration costs.

:''Dimensional instability'':  A number of studies have reported that the five dimensions of service quality implicit in the model (reliability, assurance, tangibles, empathy and responsiveness) do not hold up when the research is replicated in different countries, different industries, in different market segments or even at different time periods.&lt;ref&gt;Carman, J.M., "Consumer Perceptions of Service Quality: An assessment of the SERVQUAL dimensions," ''Journal of Retailing,'' Vol. 66, no 1, 1990&lt;/ref&gt;&lt;ref&gt;Lam, S. K and Woo, K. S.,  "Measuring Service Quality: A test-retest reliability investigation of SERVQUAL,"  ''Journal of the Market Research Society, '' Vol. 39, no. 2, 1997, pp 381-396&lt;/ref&gt; Some studies report that the SERVQUAL items do not always load onto the same factors. In some empirical research, the items load onto fewer dimensions, while other studies report tht the items load onto more than five dimensions of quality. In statistical terms, the robustness of the factor loadings is known as a model's ''dimensional stability.'' Across a wide range of empirical studies, the factors implicit in the SERVQUAL instrument have been shown to be unstable.&lt;ref&gt;Niedricha, R.W., Kiryanovab, E. and Black, W.C., "The Dimensional Stability of the Standards used in the Disconfirmation Paradigm," ''Journal of Retailing,'' Vol. 81, no. 1, 2005, pp 49–57&lt;/ref&gt; Problems associated  with  the stability of the factor loadings may be attributed, at least in part, to the  requirement  that  each  new  SERVQUAL  investigation needed  to  make  context-sensitive  modifications to  the  instrument  in  order  to  accommodate the  unique  aspects  of  the focal service setting or problem. However, it has also been hypothesised that the dimensions of service quality represented by the SERVQUAL research instrument fail to capture the true dimensionality of the service quality construct and that there may not be a universal set of service quality dimensions that are relevant across all service industries.&lt;ref&gt;Miller, R.E., Hardgrave, B.C. and Jones, R.W., "SERVQUAL Dimensionality: An investigation of presentation order effect," ''International Journal of Services and Standards,'' Vol. 7, no. 1 DOI: 10.1504/IJSS.2011.040639&lt;/ref&gt;

In spite of these criticisms, the SERVQUAL instrument, or any one of its variants (i.e. modified forms), dominates current research into service quality.&lt;ref&gt;Ladhari, R., "A review of twenty years of SERVQUAL research", ''International Journal of Quality and Service Sciences,'' Vol. 1 no. 2, pp.172 - 198&lt;/ref&gt;  In a review of more than 40 articles that made use of SERVQUAL. a team of researchers found that  “few researchers concern themselves with the validation of the measuring tool”.&lt;ref&gt;Nyeck, S., Morales, M., Ladhari, R., &amp; Pons, F., "10 Years of Service Quality  Measurement: Reviewing the use of the SERVQUAL Instrument," ''Cuadernos de Difusion,'' Vol. 7, no 13, pp 101-107.&lt;/ref&gt;  SERVQUAL is not only the subject of academic papers, but it is also widely used by industry practitioners.&lt;ref&gt;Asubonteng, P., McCleary, K.J. and Swan, J.E.,  "SERVQUAL revisited: a critical review of service quality", ''Journal of Services Marketing,'' Vol. 10, no 6, 1996, pp 62-81&lt;/ref&gt;

== See also==

* [[Customer satisfaction]]
* [[Customer satisfaction research]]
* [[Disconfirmed expectancy]]
* [[Quality management]]
* [[Service quality]]
* [[Services marketing]]

== References ==
{{Reflist}}

== External links ==

*[http://www.farrell-associates.com.au/BSOM/Papers/SERVQUAL.doc/ SERVQUAL questionnaire]  - Annotated copy of SERVQUAL questionnaire
*[http://www.kinesis-cem.com/pdf/ServQual.pdf/ SERVQUAL Instructions] - Detailed instructions for administering the SERVQUAL questionnaire

==Further reading==
* Luis Filipe Lages &amp; Joana Cosme Fernandes, 2005, "The SERPVAL scale: A multi-item instrument for measuring service personal values", ''Journal of Business Research,'' Vol.58, Issue 11, pp 1562–1572.
* Deborah McCabe, Mark S. Rosenbaum, and Jennifer Yurchisin (2007), “Perceived Service Quality and Shopping Motivations:  A Dynamic Relationship,” ''Services Marketing Quarterly,'' 29 (1), pp 1–21.

[[Category:Knowledge representation]]
[[Category:Quality management]]
[[Category:Service industries]]</text>
      <sha1>64fsdixdjbdf30ba43a5nxagp70835z</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic analysis (knowledge representation)</title>
    <ns>0</ns>
    <id>13645056</id>
    <revision>
      <id>743107116</id>
      <parentid>730580376</parentid>
      <timestamp>2016-10-07T21:27:33Z</timestamp>
      <contributor>
        <username>Klbrain</username>
        <id>11677590</id>
      </contributor>
      <comment>Removing stale merge proposal from June 2013; no support since case was made more than 3 years ago (see [[Talk:Semantic analysis (knowledge representation)#Semantic analysis (linguistics)]]); different topics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1310" xml:space="preserve">{{Cleanup|date=November 2008}}
{{context|date=February 2014}}
{{Vague|date=February 2009}}
'''Semantic analysis''' is a method for eliciting and representing [[knowledge]] about [[organisation]]s.{{Vague|date=February 2009}}&lt;ref&gt;[[Liu Kecheng]], (2000) Semiotics in [[information systems engineering]], Cambridge University Press.&lt;/ref&gt; 

Initially the problem must be defined by domain experts and passed to the project analyst(s). The next step is the generation of candidate affordances. This step will generate a list of semantic units that may be included in the schema. The candidate grouping follows where some of the semantic units that will appear in the schema are placed in simple groups. Finally the groups will be integrated together into an [[Ontology_(information_science)|ontology]] chart. 

Semantic analysis always starts from the problem definition which if not clear, require the analyst to employ relevant [[literature]], [[interview]]s with the [[Stakeholder (corporate)|stakeholders]] and other techniques towards collecting supplementary [[information]]. All assumptions made must be genuine and not limiting the system. 

== See also ==
* [[Semantic analysis (machine learning)]]
* [[Ontology chart]]

==References==
{{reflist}}

[[Category:Knowledge representation]]

{{Library-stub}}</text>
      <sha1>g4mc8lhahve376d9f63csabsw11cn7c</sha1>
    </revision>
  </page>
  <page>
    <title>Open Knowledge Base Connectivity</title>
    <ns>0</ns>
    <id>4720390</id>
    <revision>
      <id>559794202</id>
      <parentid>544344815</parentid>
      <timestamp>2013-06-13T23:17:53Z</timestamp>
      <contributor>
        <username>Disavian</username>
        <id>784547</id>
      </contributor>
      <comment>clarify source and sponsor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="829" xml:space="preserve">{{Unreferenced stub|auto=yes|date=December 2009}}
'''Open Knowledge Base Connectivity''' ('''OKBC''')  is a [[protocol (computer science)|protocol]] and an [[application programming interface|API]] for accessing knowledge in [[knowledge representation]] systems such as [[ontology (computer science)|ontology]] repositories and [[object-relational database]]s. It is somewhat complementary to the [[Knowledge Interchange Format]] that serves as a general representation language for knowledge. It is developed by [[SRI International]]'s [[Artificial Intelligence Center]] for [[DARPA]]'s High Performance Knowledge Base program (HPKB).

==External links==
* [http://www.ai.sri.com/~okbc/ Open Knowledge Base Connectivity Home Page]

[[Category:SRI International software]]
[[Category:Knowledge representation]]

{{Comp-sci-stub}}</text>
      <sha1>4608oy95hr1l0stdxuuqia3mffqkktj</sha1>
    </revision>
  </page>
  <page>
    <title>Vivification</title>
    <ns>0</ns>
    <id>15440345</id>
    <revision>
      <id>618789411</id>
      <parentid>591607328</parentid>
      <timestamp>2014-07-28T08:48:15Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>redlink [[Least common subsumer]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2202" xml:space="preserve">'''Vivification''' is an operation on a [[description logic]] knowledge base to improve performance of a [[semantic reasoner]].  Vivification replaces a [[Logical disjunction|disjunction]] of concepts &lt;math&gt;C_1 \sqcup C_2 \ldots \sqcup C_n&lt;/math&gt; by the ''[[least common subsumer]]'' of the concepts &lt;math&gt;C_1,C_2,\ldots C_n&lt;/math&gt;.

The goal of this operation is to improve the performance of the reasoner by replacing a complex set of concepts with a single concept which subsumes the original concepts. 

For example, consider the example given in (Cohen 92):  Suppose we have the concept &lt;math&gt;\textrm{PIANIST(Jill)} \vee \textrm{ORGANIST(Jill)}&lt;/math&gt;.  This concept can be vivified into a simpler concept &lt;math&gt;\textrm{KEYBOARD-PLAYER(Jill)}&lt;/math&gt;.  This summarization leads to an approximation that may not be exactly equivalent to the original.

== An approximation ==

[[Knowledge base]] vivification is not necessarily exact. If the reasoner is operating under the [[open world assumption]] we may get surprising results.  In the previous example, if we replace the disjunction with the vivified concept,  we will arrive at a surprising results.  

First, we find that the reasoner will no longer classify Jill as either a pianist or an organist.  Even though &lt;math&gt;\textrm{ORGANIST}&lt;/math&gt; and &lt;math&gt;\textrm{PIANIST}&lt;/math&gt; are the only two sub-classes, under the OWA we can no longer classify Jill as playing one or the other.  The reason is that there may be another keyboard instrument (e.g. a harpsichord) that Jill plays but which does not have a specific subclass.   

== References ==
# Cohen, W.W., Borgida, A., Hirsh, H., Computing Least Common Subsumers in Description Logics, In: Proc. AAAI-92, AAAI Press/The MIT Press, 1992, pages 754--760. [http://citeseer.ist.psu.edu/cohen92computing.html CiteSeer]
# Baader, F., Kusters, R., Wolter F., Extensions to Description Logics. In F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P.F. Patel-Schneider, editors, The Description Logic Handbook: Theory, Implementation, and Applications. Cambridge University Press, 2003. http://citeseer.ist.psu.edu/baader03basic.html
{{Wikt|vivification}}

[[Category:Knowledge representation]]</text>
      <sha1>amp6jqukgzdgjel7w76r1m4az5kkcv7</sha1>
    </revision>
  </page>
  <page>
    <title>New Classification Scheme for Chinese Libraries</title>
    <ns>0</ns>
    <id>17618735</id>
    <revision>
      <id>670755085</id>
      <parentid>657231421</parentid>
      <timestamp>2015-07-10T00:33:28Z</timestamp>
      <contributor>
        <username>Niceguyedc</username>
        <id>5288432</id>
      </contributor>
      <minor />
      <comment>[[:en:WP:CLEANER|WPCleaner]] v1.35 - Repaired 1 link to disambiguation page - [[WP:DPL|(You can help)]] - [[Serial publications]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4473" xml:space="preserve">{{Unreferenced|date=May 2009}}
The '''New Classification Scheme for Chinese Libraries''' is a system of [[library classification]] developed by Yung-Hsiang Lai since 1956.  It is modified from "[[:zh:中国图书分类法|A System of Book Classification for Chinese Libraries]]" of [[Liu Guojun]], which is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. 

The scheme is developed for Chinese books, and commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]]. 

==Main classes==
*000 Generalities
*100 [[Philosophy]]
*200 [[Religion]]
*300 [[Sciences]]
*400 [[Applied sciences]]
*500 [[Social sciences]]
*600-700 [[History]] and [[Geography]]
*800 [[Linguistics]] and [[Literature]]
*900 [[Arts]]

==Outline of the classification tables==
*'''000 Generalities'''
**000 Special collections
**010 [[Bibliography]]; [[Literacy]] ([[Documentation]])
**020 [[Library science|Library]] and [[information science]]; Archive management
**030 [[Sinology]]
**040 [[General encyclopedia]]
**050 [[Serial (publishing)|Serial publications]]; [[Periodicals]]
**060 General [[organization]]; [[Museology]]
**070 General collected [[essays]]
**080 General [[Book series|series]]
**090 Collected [[Chinese classics]]
*'''100 [[Philosophy]]'''
**100 Philosophy: general
**110 [[Thought]]; [[Learning]]
**120 [[Chinese philosophy]]
**130 [[Oriental philosophy]]
**140 [[Western philosophy]]
**150 [[Logic]]
**160 [[Metaphysics]]
**170 [[Psychology]]
**180 [[Esthetics]]
**190 [[Ethics]]
*'''200 [[Religion]]'''
**200 Religion: general
**210 Science of religion
**220 [[Buddhism]]
**230 [[Taoism]]
**240 [[Christianity]]
**250 [[Islam]] ([[Mohammedanism]])
**260 [[Judaism]]
**270 Other religions
**280 [[Mythology]]
**290 [[Astrology]]; [[Superstition]]
*'''300 [[Sciences]]'''
**300 Sciences: general
**310 [[Mathematics]]
**320 [[Astronomy]]
**330 [[Physics]]
**340 [[Chemistry]]
**350 [[Earth science]]; [[Geology]]
**360 [[Biological science]]
**370 [[Botany]]
**380 [[Zoology]]
**390 [[Anthropology]]
*'''400 [[Applied sciences]]'''
**400 Applied sciences: general
**410 [[Medical sciences]]
**420 [[Home economics]]
**430 [[Agriculture]]
**440 [[Engineering]]
**450 [[Mining]] and [[metallurgy]]
**460 [[Chemical engineering]]
**470 [[Manufacture]]
**480 [[Commerce]]: various business
**490 Commerce: [[Administration (business)|administration]] and [[management]]
*'''500 [[Social sciences]]'''
**500 Social sciences: general
**510 [[Statistics]]
**520 [[Education]]
**530 [[Rite]] and [[Convention (norm)|custom]]
**540 [[Sociology]]
**550 [[Economy]]
**560 [[Finance]]
**570 [[Political science]]
**580 [[Law]]; [[Jurisprudence]]
**590 [[Military science]]
*'''600-700 [[History]] and [[geography]]'''
**600 History and geography: General 
*'''History and geography of [[China]]'''
**610 General [[history of China]]
**620 Chinese history by period
**630 History of Chinese civilization
**640 Diplomatic history of China
**650 Historical sources
**660 [[Geography of China]]
**670 Local history
**680 Topical topography
**690 Chinese travels
*'''[[World history]] and geography'''
**710 World: general history and geography
**720 [[Oceans]] and [[sea]]s
**730 [[Asia]]: history and geography
**740 [[Europe]]: history and geography
**750 [[Americas|America]]: history and geography
**760 [[Africa]]: history and geography
**770 [[Oceania]]: history and geography
**780 [[Biography]]
**790 [[Antiquities]] and [[archaeology]]
*'''800 [[Linguistics]] and [[literature]]'''
**800 Linguistics: general
**810 Literature: general
**820 [[Chinese literature]]
**830 Chinese literature: general collections
**840 Chinese literature: individual works
**850 Various Chinese literature
**860 Oriental literature
**870 [[Western literature]]
**880 Other countries literatures
**890 [[Journalism]]
*'''900 [[Arts]]'''
**900 Arts: general
**910 [[Music]]
**920 [[Architecture]]
**930 [[Sculpture]]
**940 [[Drawing]] and [[painting]]; [[Calligraphy]]
**950 [[Photography]]; [[Computer art]]
**960 [[Decorative arts]]
**970 [[Arts and Crafts movement]]
**980 [[Theatre]]
**990 [[Recreation]] and [[leisure]]

==See also==
===Decimal systems===
*[[Dewey Decimal Classification]]
*[[Nippon Decimal Classification]]
*[[Korean Decimal Classification]] 
===Non-decimal systems===
*[[Library of Congress Classification]]
*[[Chinese Library Classification]]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>1d8bghjtrlskx5tvfw7i0td235ir37q</sha1>
    </revision>
  </page>
  <page>
    <title>ITools Resourceome</title>
    <ns>0</ns>
    <id>17727869</id>
    <revision>
      <id>723083957</id>
      <parentid>521403614</parentid>
      <timestamp>2016-05-31T22:10:34Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* top */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1292" xml:space="preserve">{{lowercase title}}
[[Image:Biositemap iTools NCBC.png|thumb|right|300px| NCBC iTools]]
'''iTools'''&lt;ref&gt;{{cite journal|vauthors=Dinov ID, Rubin D, Lorensen W, Dugan J, Ma J, Murphy S, Kirschner B, Bug W, Sherman M, Floratos A, Kennedy D, Jagadish HV, Schmidt J, Athey B, Califano A, Musen M, Altman R, Kikinis R, Kohane I, Delp S, Parker DS, Toga AW | title=iTools: A Framework for Classification, Categorization and Integration of Computational Biology Resources | journal=PLoS ONE |volume=3|issue=5|pages= e2265| doi=10.1371/journal.pone.0002265 | year=2008 |url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0002265 | pmid=18509477 | pmc=2386255}}&lt;/ref&gt; is a distributed infrastructure for managing, discovery, comparison and integration of computational biology resources. iTools employs [[Biositemap]] technology to retrieve and service meta-data about diverse bioinformatics data services, tools, and web-services. iTools is developed by the [[National Centers for Biomedical Computing]] as part of the [http://nihroadmap.nih.gov/ NIH Road Map Initiative].

==See also==
* [[Biositemaps]]

==References==
&lt;references/&gt;

== External links ==
* [http://iTools.ccb.ucla.edu Interactive iTools Server]

[[Category:Knowledge representation]]
[[Category:Bioinformatics]]</text>
      <sha1>360xzohfzms2si46t7wj19jsjkbmdg6</sha1>
    </revision>
  </page>
  <page>
    <title>Paradigm classification</title>
    <ns>0</ns>
    <id>21862082</id>
    <revision>
      <id>687523149</id>
      <parentid>666506511</parentid>
      <timestamp>2015-10-26T02:55:52Z</timestamp>
      <contributor>
        <username>I dream of horses</username>
        <id>9676078</id>
      </contributor>
      <minor />
      <comment>general clean up, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="875" xml:space="preserve">{{Multiple issues|
{{Orphan|date=October 2015}}
{{no footnotes|date=April 2013}}
}}

'''Paradigm classification''' in [[ontology]] is a two-dimensional classification scheme, such as a spreadsheet. It is a subset of [[faceted classification]].

== Overview ==
Paradigm classification deals with the large subset of faceted classification where an item may be classified within two dimensions. Examples might include [[genealogy]], where individuals are classified by their gender and relations with other individuals.

== References ==
{{reflist|2}}

== External links ==
{{Commons category|Ontology}}
*[http://www.miskatonic.org/library/facet-web-howto.html How to Make a Faceted Classification and Put It On the Web]
*[http://annotalia.com/philosophy/ontology Ontology]

{{Philosophy topics}}

[[Category:Knowledge representation]]
[[Category:Ontology]]


{{Ontology-stub}}</text>
      <sha1>fvr437ohrpcnd5m2gjrt824rdxm9lcm</sha1>
    </revision>
  </page>
  <page>
    <title>Sears Subject Headings</title>
    <ns>0</ns>
    <id>22257394</id>
    <redirect title="Minnie Earl Sears" />
    <revision>
      <id>281347285</id>
      <parentid>281346842</parentid>
      <timestamp>2009-04-02T19:24:51Z</timestamp>
      <contributor>
        <username>EmphasisMine</username>
        <id>13429</id>
      </contributor>
      <minor />
      <comment>categories that are less appropriate for the target page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="146" xml:space="preserve">#REDIRECT [[Minnie Earl Sears]] {{R with possibilities}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>bh14b3cww5grwex3813t6odjce0aion</sha1>
    </revision>
  </page>
  <page>
    <title>Chinese Library Classification</title>
    <ns>0</ns>
    <id>5625552</id>
    <revision>
      <id>753476984</id>
      <parentid>753457845</parentid>
      <timestamp>2016-12-07T11:43:00Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{FACT}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="19952" xml:space="preserve">The '''Chinese Library Classification''' ({{Zh|c = 中国图书馆分类法|s = |t = }}; CLC), also known as '''Classification for Chinese Libraries''' (CCL){{FACT|date=December 2016}}, is effectively the national [[library classification]] scheme in [[China]]. It is used in almost all primary and secondary schools, universities, academic institutions, as well as public [[libraries]]. It is also used by publishers to classify all books published in China.

The '''Book Classification of Chinese Libraries''' (BCCL) was first published in 1975, under the auspices of China's Administrative Bureau of Cultural Affairs. Its fourth edition (1999) was renamed CLC. In September 2010, the fifth edition was published by National Library of China Publishing House.
CLC has twenty-two top-level categories, and inherits a [[Marxist]] orientation from its earlier editions.&lt;ref&gt;[http://research.dils.tku.edu.tw/joemls/41/41-1/1-22.pdf Zhang, Wenxian (2003). ''Classification for Chinese Libraries (CCL): Histories, accomplishments, problems and its comparisons''. Journal of Educational Media &amp; Library Sciences, vol. 41, nr. 1, p. 1-22.] (PDF)&lt;/ref&gt; (For instance, category A is [[Marxism]], [[Leninism]], [[Maoism]] &amp; [[Deng Xiaoping Theory]].) It contains a total of 43600 categories, many of which are recent additions, meeting the needs of a rapidly changing nation.&lt;ref&gt;''The Standardization of Chinese Library Classification'', Xiaochun Liu, [[Cataloging &amp; Classification Quarterly]], Volume 16, Issue 2, ISSN 0163-9374, Pub Date: 8/13/1993
&lt;/ref&gt;

== The CLC System ==
The 22 top categories and selected sub-categories of CLC (5th Edition) are as follows:

=== A.  [[Marxism]], [[Leninism]], [[Maoism]] &amp; [[Deng Xiaoping Theory]] ===
* A1 The Works of [[Karl Marx]] and [[Friedrich Engels]]
* A2 The Works of [[Vladimir Lenin]]
* A3 The Works of [[Joseph Stalin]]
* A4 The Works of [[Mao Zedong]]
**   A49 The works of [[Deng Xiaoping]]
* A5 The Symposium/Collection of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping
* A7 The biobibliography and [[biography]] of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping
* A8 Study and Research of Marxism, Leninism, Maoism &amp; Deng Xiaoping Theory

=== B.  [[Philosophy]] and [[Religions]] ===
* B-4 Education and dissemination of philosophy
** B-49 Learners' book und popular literature of philosophy
* B0 theory of philosophy
** B0-0 Marxist philosophy
** B01 Basic problems of philosophy
*** B014 Object, purpose and method of philosophy
*** B015 [[Materialism]] and [[idealism]]
*** B016 [[Ontology]]
**** B016.8 [[Cosmology]]
**** B016.9 Time-space-theory
*** B017 [[Epistemology]]
**** B017.8 [[Determinism]] and [[Indeterminism]]
**** B017.9 Self Theory
*** B018 [[Axiology]]
**** B019.1 Materialism
***** B019.11 Naive materialism
***** B019.12 Metaphysical materialism
***** B019.13 [[Dialectical materialism]]
**** B019.2 [[Idealism]]
** B02 [[Dialectical materialism]]
*** B024 [[Materialist dialectics]]
*** B025 Categories of materialist dialectics
*** B026 [[Methodology]]
*** B027 Application of dialectical materialism
*** B028 [[Natural philosophy]]
*** B029 [[Dialectics of nature]]
** B03 [[Historical materialism]]
*** B031 Social material requirements of life
*** B032 Basic social conflict
**** B032.1 [[Productive forces]] und [[relations of production]]
**** B032.2 [[Base and superstructure]]
*** B033 [[Marxian Class Theory|Class Theory]]
*** B034 [[Marxism#Revolution|Theory of Revolution]]
*** B035 Theory of country
*** B036 [[Social being]] and [[social consciousness]]
*** B037 [[On Contradiction#Basics of Contradiction and its History|Contradictions among the People]]
*** B038 Role of the people in historical development
** B08 [[Philosophical schools]] and research
*** B081 [[Idealism]]
**** B081.1 [[Metaphysics]]
**** B081.2 Epistemology of idealism, apriorism
*** B082 [[Positivism]], [[Machism]]
*** B083 [[Voluntarism (metaphysics)|Voluntarism]] and [[philosophy of life]]
*** B084 [[Neo-Kantianism]] and [[Neohegelianism]]
*** B085 [[New realism (philosophy)|Neorealism]], [[logical positivism]] (new positivism, logical empiricism)
*** B086 [[Existentialism]] ([[survivalism]])
*** B087 [[Pragmatism]]
*** B088 [[Neo-Thomism]] (new scholasticism)
*** B089 Other philosophical schools
**** B089.1 [[Western Marxism]]
**** B089.2 [[Philosophical hermeneutics]]
**** B089.3 [[Philosophical anthropology]]
*  B1 Philosophy (Worldwide)
*  B2 Philosophy in China
**    B22 Pre-[[Qin Dynasty]] Philosophy (~before 220 BC)
***       B222 The Confucian School
****           B222.2 [[Confucius]] (Kǒng Qiū, 551-479 BC)
*  B3 Philosophy in Asia
*  B4 [[African philosophy|Philosophy in Africa]]
*  B5 Philosophy in Europe
*  B6 Philosophy in Australasia
*  B7 [[Philosophy in America]]
*  B8 [[Cognitive science]]
*  B9 Religions
**    B91 [[Sociology of Religion]], [[Religion]] and [[Science]]
**    B92 [[Philosophy of religion|Philosophy]] and [[History of Religion]]
**    B93 [[Mythology]] and [[Animism|Primitive religion]]
**    B94 [[Buddhism]]
**    B95 [[Taoism]]
**    B96 [[Islam]]
**    B97 [[Christianity]]
***       B971 [[Bible]]
****           B971.1 [[Old Testament]]
****           B971.2 [[New Testament]]
***       B972 [[Doctrine]], [[Theology]]
***       B975 [[Evangelism]], [[Sermon]]
***       B976 [[Christian Denomination]]
****           B976.1 [[Roman Catholic Church]]
****           B976.2 Orthodox Christianity ([[Eastern Orthodox Church|Eastern Orthodoxy]], [[Oriental Orthodoxy]])
****           B976.3 [[Protestantism]] ([[Protestant Reformation]])
***       B977 [[Ecclesiastical polity]]
***       B978 Research on Christianity
***       B979 [[History of Christianity]]
****           B979.9 Biography
**    B98 Other Religions
**    B99 [[Augury]], [[Superstition]]

=== C.  [[Social Sciences]] ===
*  C0 Social Scientific Theory and Methodology
*  C1 Present and Future of Social Sciences
*  C2 Organisations, Groups, Conferences
*  C3 Method of Research in Social Sciences
*  C4 Education and Popularization of Social Sciences
*  C5 Serials, [[Anthology|Anthologies]], Periodicals in Social Sciences
*  C6 Reference Materials in Social Sciences
*  C7 (no longer used)
*  C8 Statistics in Social Sciences
*  C9 [[Sociology]]

=== D.  [[Politics]] and [[Law]] ===
*  D0 [[Political theory]]
*  D1 International Campaign of [[Communism]]
*  D2 [[Communist Party of China]]
*  D3 [[Communist Parties]] of other Countries
*  D4 Labor, Peasant, Youth, Female Organizations and Movements
*  D5 Politics (worldwide)
*  D6 Politics in China
*  D7 Politics in individual [[Countries]]
*  D8 [[Diplomacy]], [[International relations]]
*  D9 [[Law]]

=== E.  [[Military Science]] ===
*  E0 Military Theory
*  E1 Military (worldwide)
*  E2 [[Military in China]]
*  E3 Military in Asia
*  E4 Military in Africa
*  E5 Military in Europe
*  E6 Military in Australasia
*  E7 Military in America
*  E8 Strategies, Tactics, and Battles
*  E9 [[Military technology|Military Technology]]

=== F.  [[Economics]] ===
*  F0 [[Economics]]
*  F1 [[Economics]], [[Economic history]] and [[Economic geography]] of individual countries
*  F2 Economic Planning and Management
*  F3 [[Agricultural Economics]]
*  F4 Industrial Economics
*  F5 Economics of [[Transport]]
*  F6 Economics of Postal and Cable Services
*  F7 Economics of [[Commerce]]
*  F8 [[Finance]], [[Banking]]

=== G.  [[Culture]], [[Science]], [[Education]] and [[Sports]] ===
*  G0 Philosophy of Culture
*  G1 Culture
*  G2 Knowledge transmission
*  G3 [[Science]], [[Scientific Research]]
*  G4 [[Education]]
*  G5 Education in individual [[Countries]]
*  G6 Education (Primary, Secondary, Tertiary)
*  G7 Education (specialized)
*  G8 Sports

=== H.  [[Languages]] and [[Linguistics]] ===
*  H0 Linguistics
**    H01 [[Phonetics]]
***      H109 Method of Recitation, Oratory of Speech
**    H02 [[Grammatology]]
**    H03 [[Semantics]], [[Lexicology]] and Meaning of words
***      H033 [[Idiom]]
***      H034 [[Adage]]
**    H04 [[Syntax]]
**    H05 Study of writing, [[Rhetoric]]
***      H059 Study of translation
**    H06 [[Lexicography]]
***      H061 [[Dictionary]]
*  H1 [[Chinese language]]
**    H10&lt;!--to be filled--&gt;
***      H102 Regulation, Standardisation of Chinese language, Promotion of [[Putonghua]]
***      H109&lt;!--to be filled--&gt;
****        H109.2 Ancient Chinese language
****        H109.4 Modern Chinese language
**    H11 [[Phone (phonetics)|Phone]] ([[Historical Chinese phonology]])
**    H12 Grammatology
*  H2  [[Languages of China|Languages of China's ethnic minorities]]
*  H3 Commonly Used Foreign Languages
**    H31 [[English language]]
**    H32 [[French language]]
**    H33 [[German language]]
**    H34 [[Spanish language]]
**    H35 [[Russian language]]
**    H36 [[Japanese language]]
**    H37 [[Arabic language]]
*  H4 Family of [[Sino-Tibetan languages]] ([[China]], [[Tibet]] and [[Burma]])
*  H5 Family of [[Altaic languages]] ([[Turkic languages|Turkic]], [[Mongolian language|Mongolian]] and [[Tungusic languages|Tungusic]])
*  H6 [[Language family|Language families]] in other areas of the World
**    H61 [[Austroasiatic languages]] and [[Tai languages]] ([[Southeast Asia|Mainland Southeast Asia]]))
**    H62 [[Dravidian languages]] ([[South India]])
**    H63 [[Austronesian languages]] ([[Malayo-Polynesian]])
**    H64 [[Paleosiberian languages]] ([[Siberia]])
**    H65 [[Ibero-Caucasian languages]] ([[Caucasus Mountains]])
**    H66 [[Uralic languages]]
**    H67 [[Afroasiatic languages]] ([[Southwest Asia]], [[Arabian Peninsula]], [[North Africa]])
*  H7 [[Indo-European languages]]
*  H8 [[Language family|Language families]] on other Continents
**    H81 [[African languages]]
**    H83 [[Indigenous languages of the Americas|American languages]]
**    H84 [[Papuan languages]]
*  H9 International Auxiliary Languages ([[Interlingua]], [[Ido (language)|Ido]], [[Esperanto]], etc.)

=== I.  [[Literature]] ===
*  I0 [[Literary Theory]]
*  I1 Literature (worldwide)
*  I2 Literature in China
*  I3 Literature in Asia
*  I4 Literature in Africa
*  I5 Literature in Europe
*  I6 Literature in Australasia
*  I7 Literature in America

=== J.  Art ===
*  J0 Theory of [[Fine Art]]
*  J1 Fine Art of the World
*  J2 [[Painting]]
*  J3 [[Sculpture]]
*  J4 [[Photography]]
*  J5 [[Applied arts]]
*  J6 [[Music]]
*  J7 [[Dance]]
*  J8 [[Drama]]
*  J9 [[Cinematography]], [[Television]]

=== K.  [[History]] and [[Geography]] ===
* K0 Historical Theory
* K1 [[History of the World]]
* K2 [[History of China]]
* K3 [[History of Asia]]
* K4 [[History of Africa]]
* K5 [[History of Europe]]
* K6 [[History of Australasia]]
* K7 [[History of the Americas|History of America]]
* K8 Biography, [[Archaeology]]
* K9 Geography

=== N.  [[Natural Science]] ===
*  N0 Theory and Methodology
*  N1 Present state
*  N2 Organisations, Groups, Conferences
*  N3 Research Methodology
*  N4 Education and Popularization
*  N5 Serials, Anthologies, Periodicals
*  N6 Reference Materials
*  N8 Field Surveys
*  N9 Minor Sciences

=== O.  Mathematics, Physics and Chemistry ===
* O1 [[Mathematics]]
* O2 [[Applied Mathematics]]
* O3 [[Mechanics]]
* O4 [[Physics]]
* O6 [[Chemistry]]
* O7 [[Crystallography]]

=== P.  [[Astronomy]] and [[Geoscience]] ===
*  P1 [[Astronomy]]
*  P2 [[Geodesy]]
*  P3 [[Geophysics]]
*  P4 [[Meteorology]]
*  P5 [[Geology]]
*  P6 [[Mineralogy]]
*  P7 [[Oceanography]]
*  P9 [[Physiography]]

=== Q.  [[Life Sciences]] ===
*  Q1 [[Biology|General Biology]]
*  Q2 [[Cell biology|Cytology]]
*  Q3 [[Genetics]]
*  Q4 [[Physiology]]
*  Q5 [[Biochemistry]]
*  Q6 [[Biophysics]]
*  Q7 [[Molecular Biology]]
*  Q8 [[Bioengineering]]
*  Q9 [[Zoology]] and [[Botany]]

=== R.  [[Medicine]] and [[Health Sciences]] ===
*  R1 [[Preventive Medicine]], [[Public health]]
*  R2 [[Traditional Chinese Medicine]]
*  R3 [[Human anatomy]], [[Physiology]], [[Pathology]], [[Microbiology]], [[Parasitology]]
*  R4 [[Clinical Medicine]]
*  R5 [[Internal medicine]]
*  R6 [[Surgery]]
*  R7 [[:Category:Medical specialties|Medical Specialties]]
** R71 [[Obstetrics]], [[Gynecology]]
** R72 [[Pediatrics]]
** R73 [[Oncology]]
** R74 [[Neurology]], [[Psychiatry]]
** R75 [[Dermatology]], [[Venereology]]
** R76 [[Otolaryngology]]
** R77 [[Ophthalmology]]
** R78 [[Dentistry]]
** R79 Non-Chinese [[Traditional medicine|Traditional Medicine]]
*  R8 [[Radiology]], [[Sport medicine]], [[Diving medicine]], [[Aerospace medicine]]
*  R9 [[Pharmacology]], [[Pharmacy]]

=== S.  [[Agricultural Science]] ===
*  S1 Fundamental Agricultural Science
*  S2 [[Agricultural Engineering]]
*  S3 [[Agronomy]]
*  S4 [[Phytopathology]]
*  S5 [[Crop|Individual Crops]]
*  S6 [[Horticulture]]
*  S7 [[Forestry]]
*  S8 [[Animal Husbandry]], [[Veterinary medicine]], [[Hunting]], [[Sericulture]], [[Apiculture]]
*  S9 [[Aquaculture]], [[Fishery]]

=== T.  Industrial Technology ===
*  TB General Industrial Technology
*  TD [[Mining Engineering]]
*  TE [[Petroleum]], [[Natural Gas]]
*  TF [[Extractive metallurgy]], [[Smelting]]
*  TG [[Metallurgy]], [[Metalworking]]
*  TH [[Machinery]], [[Instrumentation]]
*  TJ [[Military technology and equipment|Military Technology]]
*  TK [[Power Plant]]
*  TL [[Nuclear technology]]
*  TM [[Electrical Engineering]]
*  TN [[Electronic Engineering]], [[Telecommunication|Telecommunication Engineering]]
*  TP [[Automation]], [[Computer Engineering]]
*  TQ [[Chemical Engineering]]
*  TS [[Light industry|Light Industry]], [[Handicraft]]
*  TU [[Construction Engineering]]
*  TV [[Water Resources]], [[Hydraulics|Hydraulic Engineering]]

=== U.  [[Transportation]] ===
*  U1 General [[Transport]]
*  U2 [[Railway]] Transport
*  U4 [[Highway]] Transport
*  U6 [[Ship transport|Marine Transport]]

=== V.  [[Aviation]] and Aerospace ===
*  V1 Research and Exploration of Aviation and Aerospace Technology
*  V2 Aviation
*  V4 [[Aerospace]] ([[Spaceflight]])

=== X.  [[Environmental Science]] ===
*  X1 Fundamental Environmental Science
*  X2 Environmental Research
*  X3 [[Environmental Protection]] and Management
*  X4 Disaster Protection
*  X5 [[Pollution|Pollution Control]]
*  X7 [[Waste Management]] and [[Recycling]]
*  X8 Environmental Quality Monitoring
*  X9 [[Occupational safety and health]]

=== Z.  General Works ===
*  Z1 Collectanea/Generalia ([[Book series]])
** Z12 Collectanea of China
*** Z121 General Collectanea
**** Z121.2 Song Dynasty
**** Z121.3 Yuan Dynasty
**** Z121.4 Ming Dynasty
**** Z121.5 Qing Dynasty
**** Z121.6 Republic period
**** Z121.7 Modern
*** Z122 Collectanea of a particular locality
*** Z123 Collectanea by members of a particular family
*** Z124 Collectanea by individual writers
*** Z125 Collectanea of lost books
*** Z126 Collectanea of [[Chinese Classics]]
**** Z126.1 Collection of [[Confucian Classics]]
**** Z126.2 Collection of treatises
***** Z126.21 General Collection
***** Z126.22 Remake of lost books
***** Z126.23 Collection of a particular theme
***** Z126.24 [[Timeline of Chinese history|Chronological tables]], tablets, illustrated works
***** Z126.25 Works on [[phonetics]], [[semantics]] and [[Verisimilitude|authenticity]]
***** Z126.27 Research, critics and proves
** Z13 Collectanea and Book series of Asia
** Z14 Book series of Africa
** Z15 Book series of Europe
** Z16 Book series of Oceania
** Z17 Book series of America
*  Z2 [[Encyclopedia]] and Chinese Encyclopedia (Leishu)
** Z22 Chinese Encyclopedia
*** Z221 Tang Dynasty
*** Z222 Song Dynasty
*** Z223 Yuan Dynasty
*** Z224 Ming Dynasty
*** Z225 Qing Dynasty
*** Z226 Republic
*** Z227 Modern
*** Z228 General popular Literature 
**** Z228.1 Children's book
**** Z228.2 Popular Youth Book
**** Z228.3 Elders' book
**** Z228.4 Women's reader
**** Z228.5 Men's reader
** Z23 Encyclopedia of Asia
** Z24 Encyclopedia of Africa
** Z25 Encyclopedia of Europe
** Z26 Encyclopedia of Oceania
** Z27 Encyclopedia of America
** Z28 Encyclopedia of a particular field
*  Z3 [[Dictionary]]
*  Z4 [[Symposium]], [[Anthologies]], Selected Works, [[Essay]]
*  Z5 [[Almanac]]
*  Z6 [[Serial (literature)|Serial]], [[Periodicals]]
*  Z8 Catalogue, Abstract, Index

== Other classifications ==
The other library classifications in China are:

* [[Library Classification of the People’s University of China]] (LCPUC)
* [[Library Classification of the Chinese Academy of Sciences]] (LCCAS)
* [[Library Classification for Medium and Small Libraries]] (MSL)
* [[Library Classification of Wuhan University]] (LCWU)

The other library classifications for Chinese materials outside mainland China are:
* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books devised by Profs. Haloun and P. van der Loon for Cambridge University, UK.
* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])
* [[Harvard-Yenching Classification]] System
* [[New Classification Scheme for Chinese Libraries]] (commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]].)

== See also ==
* [[Libraries in the People's Republic of China]]

==References==
{{reflist}}

== External links ==
* [http://clc.nlc.gov.cn/ Official website ]
* [http://www.ifla.org/IV/ifla62/62-qiyz.htm Contemporary Classification Systems and Thesauri in China, Zhang Qiyu, Liu Xiangsheng, Wang Dongbo, 62nd IFLA General Conference - Conference Proceedings - August 25-31, 1996]
* [http://www.nlc.gov.cn/old/old/newpages/english/org/clce.htm Chinese Library Classification Editorial Board]
* [http://www.zju.edu.cn/jzus/download/clc.pdf Abridged third (obsolete) edition of CLC ]{{Zh icon}}
* [http://www.33tt.com/tools/ztf/ CLC Online ]{{Zh icon}}
* [http://journals.sfu.ca/dcpapers/2004/Paper_12.pdf Research on Interoperability of Metadata in Classification Schemes-construction of automatic mapping system between CLC and DDC, Jianbo Dai, Hanqing Hou, Ling Cao, Dept. of Libr. &amp; Inform. Sci., Nanjing Agri. Univ., Nanjing, China 210095]
* [ftp://ext-ftp.fao.org/GI/Agris/aims/publications/workshops/AOS_5/ppt/3-3.pdf Construction of Knowledge Base for Automatic Indexing and Classification based on CLC, Hanqing Hou, Chunxiang Xue, Nanjing Agri. Univ., Nanjing, China 210095]
* [http://www.fao.org/Agris/AOS/ConferencesW/FifthAOS_China04/AOS_Proceedings/docs/4-1.pdf#search='chinese%20library%20classification' An Intelligent Retrieval System for Chinese Agricultural Literature indexed by Chinese Classification System, Ping Qian, Xiaolu Su, Chinese Academy of Agricultural Sciences, China]
* [http://www.freewebs.com/yahnkim/East%20Asian%20Library%20Classification%20Systems%5B1%5D.doc East Asian Library Classification Systems], [http://archive.is/20121209181441/http://webcache.googleusercontent.com/search?q=cache:z3hxqowOTFoJ:www.freewebs.com/yahnkim/East%2520Asian%2520Library%2520Classification%2520Systems%255B1%255D.doc+chinese+library+classification&amp;hl=en&amp;gl=hk&amp;ct=clnk&amp;cd=410 archived]
* [http://www.nii.ac.jp/publications/CJK-WS3/cjk3-04a.pdf The Development of Authority Database in National Library of China (NLC), March 2002, Beixin Sun of NLC] NLC's classification subject thesaurus database based on CLC.
* [http://www.ifla.org/IV/ifla72/papers/109-Gu-en.pdf National Bibliographies: the Chinese Experience, 72nd IFLA Conference at Seoul in Korea, August 2006, Ben Gu of NLC] An overview of the current situation of the National Bibliography and classification systems in China.
* [http://pubs.nrc-cnrc.gc.ca/jchla/jchla26/c05-018.pdf A month at the Shanghai Library, November 2004, Helen Michael, University of Toronto] A librarian from Canada shared her experience of working in a library of China.

{{Library classification systems}}

[[Category:1975 introductions]]
[[Category:1975 establishments in China]]
[[Category:Library cataloging and classification]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Chinese culture]]</text>
      <sha1>6vwper4rwv8j81mblpmdx7kzluyo3x3</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Minimum Information Standards</title>
    <ns>14</ns>
    <id>23474509</id>
    <revision>
      <id>389242289</id>
      <parentid>376492840</parentid>
      <timestamp>2010-10-07T03:57:17Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Merging catmore1/catmore2 per [[Wikipedia:Templates for discussion/Log/2010 September 10|TFD]], and renaming catmore/catmore2 per [[Template talk:cat main|discussion]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="112" xml:space="preserve">{{Cat main|Minimum Information Standards}}

[[Category:Knowledge representation]]
[[Category:Standards by type]]</text>
      <sha1>ctsudpv127aivs1av1r0x9nfpnj2co9</sha1>
    </revision>
  </page>
  <page>
    <title>Reification (computer science)</title>
    <ns>0</ns>
    <id>232423</id>
    <revision>
      <id>713271172</id>
      <parentid>711384159</parentid>
      <timestamp>2016-04-03T01:04:13Z</timestamp>
      <contributor>
        <username>MindlessXD</username>
        <id>481404</id>
      </contributor>
      <minor />
      <comment>Place images on the right as per [[WP:IMGLOC]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17153" xml:space="preserve">{{Other uses|Reification (disambiguation)}}

'''Reification''' is the process by which an abstract idea about a [[computer program]] is turned into an explicit [[data model]] or other object created in a [[programming language]]. A computable/addressable object — a resource — is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification is often referred to as "making something a [[first-class citizen]]" within the scope of a particular system. Some aspect of a system can be reified at ''language design time'', which is related to [[Reflection (computer science)|reflection]] in programming languages. It can be applied as a stepwise refinement at ''system design time''. Reification is one of the most frequently used techniques of [[conceptual analysis]] and [[knowledge representation]].

== Reification and reflective programming languages ==
In the context of [[programming language]]s, reification is the process by which a user program or any aspect of a programming language that was implicit in the translated program and the run-time system, are  expressed in the language itself. This process makes it available to the program, which can inspect all these aspects as ordinary [[data]]. In [[Reflection (computer science)|reflective languages]], reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect. Reification data is often said to be made a [[first class object]]. Reification, at least partially, has been experienced in many languages to date: in early [[Lisp (programming language)|Lisp dialects]] and in current [[Prolog| Prolog dialects]], programs have been treated as data, although the causal connection has often been left to the responsibility of the programmer. In [[Smalltalk]]-80, the compiler from the source text to bytecode has been part of the run-time system since the very first implementations of the language.&lt;ref&gt;J. Malenfant, M. Jacques and F.-N. Demers, [http://www2.parc.com/csl/groups/sda/projects/reflection96/docs/malenfant/ref96/ref96.html A Tutorial on Behavioral Reflection and its Implementation]&lt;/ref&gt;

* The [[C (programming language)|C programming language]] reifies the low-level detail of [[memory address]]es.{{paragraph break}}Many programming language designs encapsulate the details of memory allocation in the compiler and the run-time system. In the design of the C programming language, the memory address is reified and is available for direct manipulation by other language constructs. For example, the following code may be used when implementing a memory-mapped device driver. The buffer pointer is a proxy for the memory address 0xB800000.{{paragraph break}}&lt;source lang="c"&gt;
 char* buffer = (char*) 0xB800000;
 buffer[0] = 10; 
&lt;/source&gt;
* [[Functional programming languages]] based on [[lambda-calculus]] reify the concept of a procedure abstraction and procedure application in the form of the [[Lambda calculus#Lambda calculus and programming languages|Lambda expression]].
* The [[Scheme (programming language)|Scheme]] programming language reifies [[continuations]] (approximately, the call stack).
* In [[C Sharp (programming language)|C#]], reification is used to make [[parametric polymorphism]] implemented as generics as a first-class feature of the language.
* In the [[Java (programming language)|Java]] programming language, there exist "reifiable types" that are "completely available at run time" (i.e. their information is not erased during compilation).&lt;ref&gt;[http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.7 The Java Language Specification, section 4.7], Java SE 7 Edition&lt;/ref&gt;
* [[REBOL]] reifies code as data and vice versa.
* Many languages, such as [[Lisp (programming language)|Lisp]], [[JavaScript]], and [[Curl (programming language)|Curl]], provide an [[eval|&lt;code&gt;eval&lt;/code&gt; or &lt;code&gt;evaluate&lt;/code&gt; procedure]] that effectively reifies the language interpreter.
* The [[Logtalk]] framework for [[Prolog]] offers a means to explore reification in the context of [[logic programming]].
* [[Smalltalk]] and [[Actor model|Actor languages]] permit the reification of blocks and [[message passing|messages]],&lt;ref&gt;{{cite web|url=http://c2.com/cgi/wiki?SmalltalkBlocksAndClosures |title=Smalltalk Blocks And Closures |publisher=C2.com |date=2009-10-15 |accessdate=2010-10-09}}&lt;/ref&gt; which are equivalent of lambda expressions in Lisp, and [[thisContext]] which is a reification of the current executing block.
* [[Homoiconicity|Homoiconic languages]] reify the syntax of the language itself in the form of an [[abstract syntax tree]], typically together with &lt;code&gt;eval&lt;/code&gt;.

== Data reification vs. data refinement ==
Data reification ([[stepwise refinement]]) involves finding a more concrete representation of the [[abstract data type]]s used in a [[formal specification]].

Data reification is the terminology of the [[Vienna Development Method]] (VDM) that most other people would call data refinement. An example is taking a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language, such as sets, by one that does have a counterpart (such as maps with fixed domains that can be implemented by arrays), or at least one that is closer to having a counterpart, such as sequences. The VDM community prefers the word "reification" over "refinement", as the process has more to do with concretising an idea than with refining it.&lt;ref&gt;[https://www.cs.tcd.ie/FME/original/FAQ/vdm/part13.html Formal Methods Europe, Frequently Asked Questions, part 13].&lt;/ref&gt;

For similar usages, see [[Reification (linguistics)]].

== Reification in conceptual modeling ==
Reification is widely used in [[Conceptual model (computer science)|conceptual modeling]].&lt;ref&gt;Antoni Olivé, Conceptual Modeling of Information Systems, Springer Verlag, 2007.&lt;/ref&gt; Reifying a relationship means viewing it as an entity. The purpose of reifying a relationship is to make it explicit, when additional information needs to be added to it. Consider the relationship type ''&lt;code&gt;IsMemberOf(member:Person, Committee)&lt;/code&gt;''. An instance of ''&lt;code&gt;IsMemberOf&lt;/code&gt;'' is a relationship that represents the fact that a person is a member of a committee. The figure below shows an example population of ''&lt;code&gt;IsMemberOf&lt;/code&gt;'' relationship in tabular form. Person ''P1'' is a member of committees ''C1'' and ''C2''. Person ''P2'' is a member of committee ''C1'' only. [[File:reification example1.png|500px|thumb|Example population of &lt;code&gt;IsMemberOf&lt;/code&gt; relationship in tabular form. Person P1 is a member of committees C1 and C2. Person P2 is a member of committee C1 only.]]

The same fact, however, could also be viewed as an entity. Viewing a relationship as an entity, one can say that the entity reifies the relationship. This is called reification of a relationship. Like any other entity, it must be an instance of an entity type. In the present example, the entity type has been named &lt;code&gt;Membership&lt;/code&gt;. For each instance of ''&lt;code&gt;IsMemberOf&lt;/code&gt;'', there is one and only one instance of ''&lt;code&gt;Membership&lt;/code&gt;'', and vice versa. Now, it becomes possible to add more information to the original relationship. As an example, we can express the fact that "person p1 was nominated to be the member of committee c1 by person p2". Reified relationship ''&lt;code&gt;Membership&lt;/code&gt;'' can be used as the source of a new relationship ''&lt;code&gt;IsNominatedBy(Membership, Person)&lt;/code&gt;''.

For related usages see [[Reification (knowledge representation)]].

== Reification in Unified Modeling Language (UML) ==
[[File:reification example2.png|400px|thumb|The UML [[class diagram]] for the Membership example.]] [[Unified Modeling Language|UML]] provides an ''association class'' construct for defining reified relationship types. The association class is a single model element that is both a kind of association and a kind of a class.&lt;ref&gt;''Unified Modeling Language, UML superstructure'', Object Management Group, 2007-11-02.&lt;/ref&gt; The association and the entity type that reifies are both the same model element. Note that attributes cannot be reified.

== Reification on Semantic Web ==

=== RDF and OWL ===
In [[Semantic Web]] languages, such as [[Resource Description Framework]] (RDF) and [[Web Ontology Language]] (OWL), a statement is a binary relation. It is used to link two individuals or an individual and a value. Applications sometimes need to describe other RDF statements, for instance, to record information like when statements were made, or who made them, which is sometimes called "[[provenance]]" information. As an example, we may want to represent properties of a relation, such as our certainty about it, severity or strength of a relation, relevance of a relation, and so on.

The example from the conceptual modeling section describes a particular person with &lt;code&gt;URIref person:p1&lt;/code&gt;, who is a member of the &lt;code&gt;committee:c1&lt;/code&gt;. The RDF triple from that description is
&lt;source lang="sparql"&gt;
  person:p1   committee:isMemberOf   committee:c1 .
&lt;/source&gt;
Consider to store two further facts: (i) to record who nominated this particular person to this committee (a statement about the membership itself), and (ii) to record who added the fact to the database (a statement about the statement).

The first case is a case of classical reification like above in UML: reify the membership and store its attributes and roles etc.:

&lt;source lang="sparql"&gt;
 committee:Membership        rdf:type              owl:Class .
 committee:membership12345   rdf:type              committee:Membership .
 committee:membership12345   committee:ofPerson    person:p1 .
 committee:membership12345   committee:inCommittee committee:c1 .
 person:p2                   committee:nominated   committee:membership12345 .  
&lt;/source&gt;

Additionally, RDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the type &lt;code&gt;rdf:Statement&lt;/code&gt;, and the properties &lt;code&gt;rdf:subject&lt;/code&gt;, &lt;code&gt;rdf:predicate&lt;/code&gt;, and &lt;code&gt;rdf:object&lt;/code&gt;.&lt;ref name="rdf"&gt;{{cite web|url=http://www.w3.org/TR/2004/REC-rdf-primer-20040210/#reification |title=RDF Primer |publisher=W3.org |date= |accessdate=2010-10-09}}&lt;/ref&gt;

Using the reification vocabulary, a reification of the statement about the person's membership would be given by assigning the statement a URIref such as &lt;code&gt;committee:membership12345&lt;/code&gt; so that describing statements can be written as follows:
&lt;source lang="sparql"&gt;
 committee:membership12345Stat   rdf:type        rdf:Statement .
 committee:membership12345Stat   rdf:subject     person:p1 .
 committee:membership12345Stat   rdf:predicate   committee:isMemberOf . 
 committee:membership12345Stat   rdf:object      committee:c1 .
&lt;/source&gt;
These statements say that the resource identified by the &lt;code&gt;URIref committee:membership12345Stat&lt;/code&gt; is an RDF statement, that the subject of the statement refers to the resource identified by &lt;code&gt;person:p1&lt;/code&gt;, the predicate of the statement refers to the resource identified by &lt;code&gt;committee:isMemberOf&lt;/code&gt;, and the object of the statement refers to the resource &lt;code&gt;committee:c1&lt;/code&gt;. Assuming that the original statement is actually identified by &lt;code&gt;committee:membership12345&lt;/code&gt;, it should be clear by comparing the original statement with the reification that the reification actually does describe it. The conventional use of the RDF reification vocabulary always involves describing a statement using four statements in this pattern. Therefore, they are sometimes referred to as the "reification quad".&lt;ref name="rdf"/&gt;

Using reification according to this convention, we could record the fact that &lt;code&gt;person:p3&lt;/code&gt; added the statement to the
database by
&lt;source lang="sparql"&gt;
  person:p3    committee:addedToDatabase    committee:membership12345Stat .
&lt;/source&gt;
It is important to note that in the conventional use of reification, the subject of the reification triples is assumed to identify a particular instance  of a triple in a particular RDF document, rather than some arbitrary triple having the same subject, predicate, and object. This particular convention is used because reification is intended for expressing properties such as dates of composition and source information, as in the examples given already, and these properties need to be applied to specific instances of triples. 
Note that the described triple &lt;code&gt;(subject predicate object)&lt;/code&gt; itself is not implied by such a reification quad (and it is not necessary that it actually exists in the database). This allows also to use this mechanism to express which triples do ''not'' hold.

The power of the reification vocabulary in RDF is restricted by the lack of a built-in means for assigning URIrefs to statements, so in order to express "provenance" information of this kind in RDF, one has to use some mechanism (outside of RDF) to assign URIs to individual RDF statements, then make further statements about those individual statements, using their URIs to identify them.&lt;ref name="rdf"/&gt;

=== Reification in Topic Maps ===
In an [[Topic Maps|XML Topic Map]] (XTM), only a topic can have a name or play a role in an association. One may use an association to make an assertion about a topic, but one cannot directly make assertions about that assertion. However, it is possible to create a topic that reifies a non-topic construct in a map, thus enabling the association to be named and treated as a topic itself.&lt;ref&gt;[http://www.techquila.com/practical_intro.html Practical Introduction into Topic Maps].&lt;/ref&gt;

=== Reification and n-ary relations ===
In Semantic Web languages, such as RDF and OWL, a property is a binary relation used to link two individuals or an individual and a value. However, in some cases, the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value. These relations are called [[n-ary relations]]. Examples are representing relations among multiple individuals, such as a committee, a person who is a committee member and another person who has nominated the first person to become the committee member, or a buyer, a seller, and an object that was bought when describing a purchase of a book.

A more general approach to reification is to create an explicit new class and n new properties to represent an n-ary relation, making an instance of the relation linking the n individuals an instance of this class. This approach can also be used to represent provenance information and other properties for an individual relation instance.&lt;ref&gt;{{cite web|url=http://www.w3.org/TR/swbp-n-aryRelations/ |title=W3C Defining N-ary relations on Semantic Web |publisher=W3.org |date= |accessdate=2010-10-09}}&lt;/ref&gt;
&lt;source lang="turtle"&gt;
 :p1
      a       :Person ;
      :has_membership _:membership_12345 .
 _:membership_12345
      a       :Membership ;
      :committee :c1;
      :nominated_by :p2 .
&lt;/source&gt;

=== Reification vs. quotation ===
It is also important to note that the reification described here is not the same as "quotation" found in other languages. Instead, the reification describes the relationship between a particular instance of a triple and the resources the triple refers to. The reification can be read intuitively as saying "this RDF triple talks about these things", rather than (as in quotation) "this RDF triple has this form." For instance, in the reification example used in this section, the triple:
&lt;source lang="sparql"&gt;
  committee:membership12345   rdf:subject   person:p1 .
&lt;/source&gt;
describing the &lt;code&gt;rdf:subject&lt;/code&gt; of the original statement says that the subject of the statement is the resource (the person) identified by the URIref &lt;code&gt;person:p1&lt;/code&gt;. It does not state that the subject of the statement is the URIref itself (i.e., a string beginning with certain characters), as quotation would.

==See also==
{{Wiktionary|reification}}
* [[Denotational semantics]]
* [[Formal semantics of programming languages]]
* [[Meta-circular evaluator]]
* [[Metamodeling]]
* [[Metaobject]]
* [[Metaprogramming]]
* [[Normalization by evaluation]]
* [[Operational semantics]]
* [[Reflection (computer science)]]
* [[Resource Description Framework]]
* [[Self-interpreter]]
* [[Topic Maps]]

==References==
{{reflist}}

&lt;!--Interwikies--&gt;

{{DEFAULTSORT:Reification (Computer Science)}}
&lt;!--Categories--&gt;
[[Category:Object-oriented programming]]
[[Category:Formal methods terminology]]
[[Category:Knowledge representation]]

[[de:Reifikation#Informatik]]
[[fr:Réification]]</text>
      <sha1>murjlqzlmq73rpkf0yh6c0lszavmzed</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology (information science)</title>
    <ns>0</ns>
    <id>49681</id>
    <revision>
      <id>758611062</id>
      <parentid>756733648</parentid>
      <timestamp>2017-01-06T14:00:59Z</timestamp>
      <contributor>
        <username>JamesBWatson</username>
        <id>1909773</id>
      </contributor>
      <comment>Removing link(s) to "CA ERwin Data Modeler": Removing links to deleted page CA ERwin Data Modeler. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="53862" xml:space="preserve">{{redirect|Knowledge graph|the Google knowledge base|Knowledge Graph||Knowledge engine (disambiguation)}}
{{About|ontology in information science|the study of the nature of being|Ontology}}
{{Information science}}

In [[computer science]] and [[information science]], an '''ontology''' is a formal naming and definition of the types, properties, and interrelationships of the [[entities]] that really or fundamentally exist for a particular [[domain of discourse]]. It is thus a practical application of philosophical [[ontology]], with a [[taxonomy (general)|taxonomy]].

An ontology compartmentalizes the variables needed for some set of computations and establishes the relationships between them.&lt;ref name="TRG93"&gt;{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199–220 |doi=10.1006/knac.1993.1008}}&lt;/ref&gt;&lt;ref&gt;{{cite web |first1=F. |last1=Arvidsson |first2=A. |last2=Flycht-Eriksson |url=http://www.ida.liu.se/~janma/SemWeb/Slides/ontologies1.pdf |title=Ontologies I |format=PDF |accessdate=26 November 2008}}&lt;/ref&gt;

The fields of [[artificial intelligence]], the [[Semantic Web]], [[systems engineering]], [[software engineering]], [[biomedical informatics]], [[library science]], [[enterprise bookmarking]], and [[information architecture]] all create ontologies to limit complexity and to organize information. The ontology can then be applied to [[problem solving]].

==Etymology and definition==
The term ''[[ontology]]'' has its origin in [[philosophy]] and has been applied in many different ways. The word element ''[[wiktionary:onto-|onto-]]'' comes from the [[Greek language|Greek]] ''[[wiktionary:ὤν|ὤν]], ὄντος'', ("being", "that which is"), present participle of the verb ''[[wiktionary:εἰμί|εἰμί]]'' ("be"). The core meaning within [[computer science]] is a model for describing the world that consists of a set of types, properties, and relationship types. There is also generally an expectation that the features of the model in an ontology should closely resemble the real world (related to the object).&lt;ref&gt;{{cite web |first=L. M. |last=Garshol |year=2004 |url=http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html#N773 |title=Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all'' |accessdate=13 October 2008 }}&lt;/ref&gt;

==Overview==
What many ontologies have in common in both computer science and in philosophy is the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. In both fields, there is considerable work on problems of [[Confirmation holism|ontological relativity]] (e.g., [[Willard Van Orman Quine|Quine]] and [[Saul Kripke|Kripke]] in philosophy, [[John F. Sowa|Sowa]] and [[Nicola Guarino|Guarino]] in computer science),&lt;ref&gt;{{cite journal |first=J. F. |last=Sowa |title=Top-level ontological categories
|journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 (November/December)
|year=1995 |pages=669–85 |doi=10.1006/ijhc.1995.1068 }}&lt;/ref&gt; and debates concerning whether a [[normative]] ontology is viable (e.g., debates over [[foundationalism]] in philosophy, and over the [[Cyc]] project in AI). Differences between the two are largely matters of focus. Computer scientists are more concerned with establishing fixed, controlled vocabularies, while philosophers are more concerned with first principles, such as whether there are such things as [[Essence|fixed essences]] or whether enduring objects must be ontologically more primary than processes.

Other fields make ontological assumptions that are sometimes explicitly elaborated and explored.  For instance, the [[philosophy and economics#Definition and ontology of economics|definition and ontology of economics]] (also sometimes called the [[political economy]]) is hotly debated especially in [[Marxist economics]]&lt;ref&gt;{{cite journal|first=Giulio |last=Palermo|url=http://cje.oxfordjournals.org/content/31/4/539.short |title=The ontology of economic power in capitalism: mainstream economics and Marx |journal=Cambridge Journal of Economics |volume=31 |issue=4 |pages=539–561 |date=10 January 2007 |doi=10.1093/cje/bel036 |accessdate=16 June 2013 |via=Oxford Journals }}&lt;/ref&gt; where it is a primary concern, but also in other subfields.&lt;ref&gt;{{cite web|author=Zuniga, Gloria L. |url=https://ideas.repec.org/p/pra/mprapa/5566.html |title=An Ontology Of Economic Objects |website=Ideas |publisher=Research Division of the Federal Reserve Bank of St. Louis |date=1999-02-02 |accessdate=2013-06-16}}&lt;/ref&gt;  Such concerns intersect with those of information science when a simulation or model is intended to enable decisions in the economic realm; for example, to determine what [[capital asset]]s are at risk and if so by how much (see [[risk management]]).  Some claim all social sciences have explicit ontology issues because they do not have hard [[falsifiability]] criteria like most models in physical sciences and that indeed the lack of such widely accepted hard falsification criteria is what defines a social or soft science.{{Citation needed|date=March 2012}}

==History==
Historically, ontologies arise out of the branch of [[philosophy]] known as [[metaphysics]], which deals with the nature of reality&amp;nbsp;– of what exists. This fundamental branch is concerned with analyzing various types or modes of existence, often with special attention to the relations between [[particular]]s and [[Universal (metaphysics)|universals]], between [[Intrinsic and extrinsic properties (philosophy)|intrinsic and extrinsic properties]], and between [[essence]] and [[existence]]. The traditional goal of ontological inquiry in particular is to divide the world "at its joints" to discover those fundamental categories or kinds into which the world’s objects naturally fall.&lt;ref name="PCB94"&gt;{{cite web |first1=Perakath C. |last1=Benjamin |first2=Christopher P. |last2=Menzel |first3=Richard J. |last3=Mayer |first4=Florence |last4=Fillion |first5=Michael T. |last5=Futrell |first6=Paula S. |last6=deWitte |first7=Madhavi |last7=Lingineni |date=September 21, 1994 |url=http://www.idef.com/pdf/Idef5.pdf |format=PDF |title=IDEF5 Method Report |publisher=Knowledge Based Systems, Inc.}}&lt;/ref&gt;

During the second half of the 20th century, philosophers extensively debated the possible methods or approaches to building ontologies without actually ''building'' any very elaborate ontologies themselves. By contrast, computer scientists were building some large and robust ontologies, such as [[WordNet]] and [[Cyc]], with comparatively little debate over ''how'' they were built.

Since the mid-1970s, researchers in the field of [[artificial intelligence]] (AI) have recognized that capturing knowledge is the key to building large and powerful AI systems.  AI researchers argued that they could create new ontologies as [[computational model]]s that enable certain kinds of [[automated reasoning]]. In the 1980s, the AI community began to use the term ''ontology'' to refer to both a theory of a modeled world and a component of knowledge systems. Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.&lt;ref name="TG08"&gt;{{cite book |first=T. |last=Gruber |authorlink=Tom Gruber |year=2008 |url=http://tomgruber.org/writing/ontology-definition-2007.htm  |title=Ontology |work=Encyclopedia of Database Systems |editor1-first=Ling |editor1-last=Liu |editor2-first=M. Tamer |editor2-last=Özsu |publisher=Springer-Verlag|isbn=978-0-387-49616-0}}&lt;/ref&gt;

In the early 1990s, the widely cited Web page and paper "Toward Principles for the Design of Ontologies Used for Knowledge Sharing" by [[Tom Gruber]]&lt;ref name="TRG95"&gt;{{cite journal |first=T. |last=Gruber |authorlink=Tom Gruber |title=Toward Principles for the Design of Ontologies Used for Knowledge Sharing |journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 |pages=907–928 |year=1995 |doi=10.1006/ijhc.1995.1081}}&lt;/ref&gt; is credited with a deliberate definition of ''ontology'' as a technical term in [[computer science]]. Gruber introduced the term to mean a specification of a conceptualization: &lt;blockquote&gt;An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.&lt;ref name="TRG01"&gt;{{cite web |first=T. |last=Gruber |authorlink=Tom Gruber |year=2001 |url=http://www-ksl.stanford.edu/kst/what-is-an-ontology.html |title=What is an Ontology? |publisher=[[Stanford University]] |accessdate=2009-11-09}}&lt;/ref&gt;&lt;/blockquote&gt;

According to Gruber (1993): &lt;blockquote&gt;Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to [[Conservative extension|conservative definitions]]&amp;nbsp;&amp;mdash; that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.&lt;ref&gt;{{cite book
|first=H. B. |last=Enderton |authorlink=Herbert Enderton |date=1972-05-12 |title=A Mathematical Introduction to Logic |location=San Diego, CA |publisher=Academic Press |edition=1 |page=295 |isbn=978-0-12-238450-9 |postscript=&amp;nbsp;2nd edition; January 5, 2001, ISBN 978-0-12-238452-3}}&lt;/ref&gt; To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.&lt;ref name="TRG93"/&gt;&lt;/blockquote&gt;

== Components ==
{{Main article|Ontology components}}
Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  As mentioned above, most ontologies describe individuals (instances), classes (concepts), attributes, and relations.  In this section each of these components is discussed in turn.

Common components of ontologies include:
; Individuals
: Instances or objects (the basic or "ground level" objects)
; [[Class (set theory)|Class]]es&lt;!-- This deliberately links to the disambiguation page --&gt;
: Sets, collections, concepts, [[Class (computer science)|classes in programming]], [[Class (philosophy)|types of objects]], or kinds of things
; [[Attribute (computing)|Attribute]]s
: Aspects, properties, features, characteristics, or parameters that objects (and classes) can have
; [[Relation (mathematics)|Relations]]
: Ways in which classes and individuals can be related to one another
; Function terms
: Complex structures formed from certain relations that can be used in place of an individual term in a statement
; Restrictions
: Formally stated descriptions of what must be true in order for some assertion to be accepted as input
; Rules
: Statements in the form of an if-then (antecedent-consequent) sentence that describe the logical inferences that can be drawn from an assertion in a particular form
; Axioms
: Assertions (including rules) in a [[logical form]] that together comprise the overall theory that the ontology describes in its domain of application.  This definition differs from that of "axioms" in [[generative grammar]] and [[formal logic]].  In those disciplines, axioms include only statements asserted as ''a priori'' knowledge.  As used here, "axioms" also include the theory derived from axiomatic statements
; [[Event (philosophy)|Events]]&lt;!-- this links  to the philosophy sense of 'Events' as that is currently the only article describing the issues around defining events in the ontology community--&gt;
: The changing of attributes or relations

Ontologies are commonly encoded using [[ontology language]]s.

== Types ==

=== Domain ontology&lt;!--linked from 'Domain ontology'--&gt; ===
A domain ontology (or domain-specific ontology) represents concepts which belong to part of the world. Particular meanings of terms applied to that domain are provided by domain ontology. For example, the word ''[[:wikt:card|card]]'' has many different meanings. An ontology about the domain of [[poker]] would model the "[[playing card]]" meaning of the word, while an ontology about the domain of [[computer hardware]] would model the "[[punched card]]" and "[[video card]]" meanings.

Since domain ontologies represent concepts in very specific and often eclectic ways, they are often incompatible. As systems that rely on domain ontologies expand, they often need to merge domain ontologies into a more general representation.  This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).

At present, merging ontologies that are not developed from a common foundation ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same foundation ontology to provide a set of basic elements with which to specify the meanings of the domain ontology elements can be merged automatically. There are studies on generalized techniques for merging ontologies,&lt;ref name="Dynamic Ontology Repair"&gt;{{cite web |url=http://dream.inf.ed.ac.uk/projects/dor/ |title=Project: Dynamic Ontology Repair |publisher= University of Edinburgh Department of Informatics|accessdate=2 January 2012}}&lt;/ref&gt; but this area of research is still largely theoretical.

=== Upper ontology ===
{{Main article|Upper ontology}}

An [[Upper ontology (computer science)|upper ontology]] (or foundation ontology) is a model of the common objects that are generally applicable across a wide range of domain ontologies. It usually employs a [[core glossary]] that contains the terms and associated object descriptions as they are used in various relevant domain sets.

There are several standardized upper ontologies available for use, including [[Basic Formal Ontology|BFO]], [[BORO method]], [[Dublin Core]], [[General Formal Ontology|GFO]], [[OpenCyc]]/[[ResearchCyc]], [[Suggested Upper Merged Ontology|SUMO]], the Unified Foundational Ontology (UFO),&lt;ref&gt;{{cite web|last=Giancarlo Guizzardi &amp; Gerd Wagner|url=http://ceur-ws.org/Vol-125/paper2.pdf|accessdate=31 March 2014|title=A Unified Foundational Ontology and some Applications of it in Business Modeling}}&lt;/ref&gt; and [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]].&lt;ref name="DOLCE"&gt;{{cite web |url=http://www.loa-cnr.it/DOLCE.html |title=Laboratory for Applied Ontology - DOLCE |publisher=Laboratory for Applied Ontology (LOA)|accessdate=10 February 2011}}&lt;/ref&gt;&lt;ref name="DOLCE-OWL"&gt;{{cite web |url=http://www.ontologydesignpatterns.org/ont/dul/DUL.owl |title=OWL version of DOLCE+DnS  |publisher=Semantic Technology Lab|accessdate=21 February 2013}}&lt;/ref&gt; [[WordNet]], while considered an upper ontology by some, is not strictly an ontology. However, it has been employed as a linguistic tool for learning domain ontologies.&lt;ref&gt;{{cite journal |first1=Roberto |last1=Navigli |authorlink1=Roberto Navigli |first2=Paola |last2=Velardi |authorlink2=Paola Velardi |year=2004 |url=http://www.mitpressjournals.org/doi/pdf/10.1162/089120104323093276 |format=PDF |title=Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites |journal=[[Computational Linguistics (journal)|Computational Linguistics]] |volume=30 |issue=2 |publisher=MIT Press |pages=151–179 |doi=10.1162/089120104323093276}}&lt;/ref&gt;

=== Hybrid ontology ===

The [[Gellish]] ontology is an example of a combination of an upper and a domain ontology.

== Visualization ==
A survey of ontology visualization techniques is presented by Katifori et al.&lt;ref&gt;{{cite journal |last1=Katifori |first1=A. |last2=Halatsis |first2=C. |last3=Lepouras |first3=G. |last4=Vassilakis |first4=C. |last5=Giannopoulou |first5=E. |title=Ontology Visualization Methods - A Survey |journal=ACM Computing Surveys |volume=39 |issue=4 |page=10 |date=2007 |url=http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-url=http://web.archive.org/web/20160304203317/http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-date=4 March 2016 |format=PDF }}&lt;/ref&gt; An evaluation of two most established ontology visualization techniques: indented tree and graph is discussed in.&lt;ref&gt;{{cite conference |first1=Bo |last1=Fu |first2=Natalya F. |last2=Noy |first3=Margaret-Anne |last3=Storey |title=Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation |book-title=The Semantic Web – ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21–25, 2013, Proceedings, Part I |series=Lecture Notes in Computer Science |volume=8218 |pages=117–134 |url=http://link.springer.com/chapter/10.1007/978-3-642-41335-3_8 |doi=10.1007/978-3-642-41335-3_8 |isbn=978-3-642-41335-3 |publisher=Springer |location=Berlin |date=2013 |via=SpringerLink }}&lt;/ref&gt; A visual language for ontologies represented in [[Web Ontology Language|OWL]] is specified by the ''Visual Notation for OWL Ontologies (VOWL)''.&lt;ref&gt;{{cite web |last1=Negru |first1=Stefan |last2=Lohmann |first2=Steffen |last3=Haag |first3=Florian |date=7 April 2014 |title=VOWL: Visual Notation for OWL Ontologies: Specification of Version 2.0 |website=Visual Data Web |url=http://vowl.visualdataweb.org/v2/ }}&lt;/ref&gt;

== Engineering ==
{{Main article|Ontology engineering}}
[[Ontology engineering]] (or ontology building) is a subfield of [[knowledge engineering]]. It studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.&lt;ref name="PFC04"&gt;{{cite book |first1=Ascunion |last1=Gómez-Pérez |authorlink1=Ascunion Gómez-Pérez |first2=Mariano |last2=Fernández-López |authorlink2=Mariano Fernández-López |first3=Oscar |last3=Corcho |authorlink3=Oscar Corcho |year=2004 |title=Ontological Engineering: With Examples from the Areas of Knowledge Management, E-commerce and the Semantic Web |publisher=Springer |isbn=978-1-85233-551-9 |page=403 |edition=1 }}&lt;/ref&gt;&lt;ref name="DMN"&gt;{{cite journal |first1=Antonio |last1=De Nicola |authorlink1=Antonio De Nicola |first2=Michele |last2=Missikoff |authorlink2=Michele Missikoff |first3=Roberto |last3=Navigli |authorlink3=Roberto Navigli |year=2009 |url=http://www.dsi.uniroma1.it/~navigli/pubs/De_Nicola_Missikoff_Navigli_2009.pdf |format=PDF |title=A Software Engineering Approach to Ontology Building |journal=[[Information Systems (journal)|Information Systems]] |volume=34 |issue=2 |publisher=Elsevier |pages=258–275 | doi = 10.1016/j.is.2008.07.002 }}&lt;/ref&gt;

Ontology engineering aims to make explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. Ontology engineering offers a direction towards solving the interoperability problems brought about by semantic obstacles, such as the obstacles related to the definitions of business terms and software classes. Ontology engineering is a set of tasks related to the development of ontologies for a particular domain.&lt;ref name="PIS00"&gt;{{cite journal |first1=Line  |last1=Pouchard |authorlink1=Line Pouchard |first2=Nenad |last2=Ivezic |authorlink2=Nenad Ivezic |first3=Craig |last3=Schlenoff |authorlink3=Craig Schlenoff |date=March 2000 |url=http://www.mel.nist.gov/msidlibrary/doc/AISfinal2.pdf |format=PDF |title=Ontology Engineering for Distributed Collaboration in Manufacturing |work=Proceedings of the AIS2000 conference }}&lt;/ref&gt;

Known challenges with ontology engineering include:
# Ensuring the ontology is ''current'' with domain knowledge and term use
# Providing ''sufficient specificity and concept coverage'' for the domain of interest, thus minimizing the [[content completeness problem]]
# Ensuring the ontology can support its use cases

=== Editor ===
'''Ontology editors''' are applications designed to assist in the creation or  manipulation of [[ontology (computer science)|ontologies]]. They often express ontologies in one of many [[ontology language (computer science)|ontology languages]]. Some provide export to other ontology languages however.

Among the most relevant criteria for choosing an ontology editor are the degree to which the editor abstracts from the actual [[Ontology language (computer science)|ontology representation language]] used for [[persistence (computer science)|persistence]] and the visual navigation possibilities within the [[knowledge model]]. Next come built-in [[inference engine]]s and [[information extraction]] facilities, and the support of meta-ontologies such as [[OWL-S]], [[Dublin Core]], etc. Another important feature is the ability to import &amp; export foreign [[knowledge representation]] languages for [[ontology matching]]. Ontologies are developed for a specific purpose and application.

*a.k.a. software (Ontology, taxonomy and thesaurus management software available from The Synercon Group)
*Anzo for Excel (Includes an RDFS and OWL ontology editor within Excel; generates ontologies from Excel spreadsheets)
*Chimaera (Other web service by Stanford)
*CmapTools Ontology Editor (COE) (Java based ontology editor from the Florida Institute for Human and Machine Cognition. Supports numerous formats)
*[[dot15926]] Editor (Open source ontology editor for data compliant to engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] scripting and pattern-based data analysis. Supports extensions.)
*EMFText OWL2 Manchester Editor, Eclipse-based, open-source, Pellet integration
* Enterprise Architect, along with [[Unified Modeling Language|UML]] modeling, supports [[Object Management Group|OMG's]] [[Ontology Definition MetaModel]] which includes [[Web Ontology Language|OWL]] and [[Resource Description Framework|RDF]].
*[[Fluent Editor]], a comprehensive ontology editor for OWL and SWRL with Controlled Natural Language (Controlled English). Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[Description Logic|DL]] and Functional rendering, unlimited imports and built-in reasoning services.
*[[HOZO]] (Java-based graphical editor especially created to produce heavy-weight and well thought out ontologies, from [[Osaka University]] and Enegate Co, ltd.)
*Java Ontology Editor (JOE)  (1998)
*[[KAON]] (single user and server based solutions possible, open source, from FZI/AIFB Karlsruhe)
*KMgen (Ontology editor for the KM language.&amp;nbsp;km: The Knowledge Machine)
*Knoodl (Free web application/service that is an ontology editor, [[wiki]], and [[Digital repository|ontology registry]].  Supports creation of communities where members can collaboratively import, create, discuss, document and publish ontologies.  Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[RDF Schema|RDFS]], and [[SPARQL]] queries.  Available since early Nov 2006 from Revelytix, Inc..)
*Model Futures IDEAS AddIn (free) A plug-in for Sparx Systems [[Enterprise Architect (software)|Enterprise Architect]] that allows [[IDEAS Group]] [[4D ontology|4D ontologies]] to be developed using a [[Unified Modeling Language|UML]] profile
*Model Futures OWL Editor (Free) Able to work with very large OWL files (e.g. [[Cyc]]) and has extensive import and export capabilities (inc. [[Unified Modeling Language|UML]], Thesaurus Descriptor, [[MS Word]], CA ERwin Data Modeler, CSV, etc.)
*myWeb (Java-based, mySQL connection, bundled with applet that allows online browsing of ontologies (including OBO))
*Neologism (Web-based, open source, supports RDFS and a subset of OWL, built on [[Drupal]])
*[[NeOn Toolkit (software)|NeOn Toolkit]]  (Eclipse-based, open source, OWL support, several import mechanisms, support for reuse and management of networked ontologies, visualization, etc.…from NeOn Project)
*OBO-Edit (Java-based, downloadable, open source, developed by the [[Gene Ontology Consortium]] for editing biological ontologies)
*OntoStudio (Eclipse-based, downloadable, support for RDF(S), OWL and F-Logic, graphical rule editor, visualizations, from ontoprise)
*Ontolingua (Web service offered by Stanford University)
* [[Open Semantic Framework]] (OSF), an integrated software stack using semantic technologies for knowledge management, which includes an ontology editor
*OWLGrEd (A graphical ontology editor, easy-to-use)
*PoolParty Thesaurus Server (Commercial ontology, taxonomy and thesaurus management software available from Semantic Web Company, fully based on standards like RDFS, SKOS and SPARQL, integrated with [[Virtuoso Universal Server]])
*[[Protégé (software)|Protégé]] (Java-based, downloadable, Supports OWL, open source, many sample ontologies, from Stanford University)
*ScholOnto (net-centric representations of research)
*Semantic Turkey (Firefox extension - also based on Java - for managing ontologies and acquiring new knowledge from the Web; developed at University of Rome, Tor Vergata )
*[[Sigma knowledge engineering environment]] is a system primarily for development of the [[Suggested Upper Merged Ontology]]
*Swoop (Java-based, downloadable, open source, OWL Ontology browser and editor from the University of Maryland)
*Semaphore Ontology Manager  (Commercial ontology, taxonomy and thesaurus management software available from [[Smartlogic Semaphore Limited]]. Intuitive tool to manage the entire "build - enhance - review - maintain" ontology lifecycle.)
*Synaptica  (Ontology, taxonomy and thesaurus management software available from Synaptica, LLC. Web based, supports [[Web Ontology Language|OWL]] and [[SKOS]].)
*TopBraid Composer  (Eclipse-based, downloadable, full support for RDFS and OWL, built-in inference engine, SWRL editor and SPARQL queries, visualization, import of XML and UML, from TopQuadrant)
*Transinsight (The editor is especially designed for creating text mining ontologies and part of GoPubMed.org)
*WebODE (Web service offered by the Technical University of Madrid)
*TwoUse Toolkit (Eclipse-based, open source, model-driven ontology editing environment especially designed for software engineers)
*Be Informed Suite (Commercial tool for building large ontology based applications. Includes visual editors, inference engines, export to standard formats)
*Thesaurus Master (Manages creation and use of ontologies for use in data management and semantic enrichment by enterprise, government, and scholarly publishers.)
*[[Tool for Ontology Development and Editing (TODE)|TODE]] (A Dot Net-based Tool for Ontology Development and Editing)
*VocBench (Collaborative Web Application for SKOS/SKOS-XL Thesauri Management - developed on a joint effort between University of Rome, Tor Vergata and the Food and Agriculture Organization of the United Nations: FAO )
*OBIS (Web based user interface that allows to input ontology instances in a user friendly way that can be accessed via SPARQL endpoint)
*[[Menthor Editor]] (An ontology engineering tool for dealing with [[OntoUML]]. It also includes OntoUML syntax validation, [[Alloy Analyzer|Alloy]] simulation, [[Anti-pattern|Anti-Pattern]] verification, and transformations from [[OntoUML]] to [[Web Ontology Language|OWL]], [[Semantics of Business Vocabulary and Business Rules|SBVR]] and [[Brazilian Portuguese|Natural Language (Brazilian Portuguese)]])

=== Learning ===
{{Main article|Ontology learning}}

Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process. 
Information extraction and text mining methods have been explored to automatically link ontologies to documents, e.g. in the context of the BioCreative challenges.&lt;ref&gt;{{Cite journal
 | pmid = 22438567
| year = 2012
| author1 = Krallinger
| first1 = M
| title = How to link ontologies and protein-protein interactions to literature: Text-mining approaches and the Bio ''Creative'' experience
| journal = Database
| volume = 2012
| pages = bas017
| last2 = Leitner
| first2 = F
| last3 = Vazquez
| first3 = M
| last4 = Salgado
| first4 = D
| last5 = Marcelle
| first5 = C
| last6 = Tyers
| first6 = M
| last7 = Valencia
| first7 = A
| last8 = Chatr-Aryamontri
| first8 = A
| doi = 10.1093/database/bas017
| pmc = 3309177
}}&lt;/ref&gt;

== Languages ==
{{Main article|Ontology language}}
An [[ontology language]] is a [[formal language]] used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:
* [[Common Algebraic Specification Language]] is a general logic-based specification language developed within the IFIP working group 1.3 "Foundations of System Specifications" and functions as a de facto standard in the area of software specifications. It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.
* [[Common logic]] is ISO standard 24707, a specification for a family of ontology languages that can be accurately translated into each other.
* The [[Cyc]] project has its own ontology language called [[CycL]], based on [[first-order predicate calculus]] with some higher-order extensions.
* [[DOGMA]] (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.
* The [[Gellish]] language includes rules for its own extension and thus integrates an ontology with an ontology language.
* [[IDEF5]] is a [[software engineering]] method to develop and maintain usable, accurate, domain ontologies.
* [[Knowledge Interchange Format|KIF]] is a syntax for [[first-order logic]] that is based on [[S-expression]]s.  SUO-KIF is a derivative version supporting the [[Suggested Upper Merged Ontology]].
* [[Meta-Object Facility|MOF]] and [[Unified Modeling Language|UML]] are standards of the [[Object Management Group|OMG]]
* [[Olog]] is a [[Category theory|category theoretic]] approach to ontologies, emphasizing translations between ontologies using [[functor]]s. 
* [[Open Biomedical Ontologies|OBO]], a language used for biological and biomedical ontologies.
* [[OntoUML]] is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.
* [[Web Ontology Language|OWL]] is a language for making ontological statements, developed as a follow-on from [[Resource Description Framework|RDF]] and [[RDFS]], as well as earlier ontology language projects including [[Ontology Inference Layer|OIL]], [[DARPA Agent Markup Language|DAML]], and [[DAMLplusOIL|DAML+OIL]]. OWL is intended to be used over the [[World Wide Web]], and all its elements (classes, properties and individuals) are defined as RDF [[web resource|resource]]s, and identified by [[Uniform Resource Identifier|URI]]s.
* [[Rule Interchange Format]] (RIF) and [[F-Logic]] combine ontologies and rules.
* [[Semantic Application Design Language]] (SADL)&lt;ref&gt;{{cite web |url=http://sadl.sourceforge.net/sadl.html |title=SADL |work=[[Sourceforge]] |accessdate=10 February 2011}}&lt;/ref&gt; captures a subset of the expressiveness of [[Web Ontology Language|OWL]], using an English-like language entered via an [[Eclipse (software)|Eclipse]] Plug-in.
* [[SBVR]] (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.
* [[TOVE Project]], TOronto Virtual Enterprise project

== Published examples ==
* AURUM - Information Security Ontology,&lt;ref&gt;{{cite web |url=http://www.securityontology.com |title=AURUM - Information Security Ontology |accessdate=29 January 2016}}&lt;/ref&gt; An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. It may serve as a basis for automated information security risk and compliance management.
* [[BabelNet]], a very large multilingual semantic network and ontology, lexicalized in many languages
* Basic Formal Ontology,&lt;ref&gt;{{cite web |url=http://www.ifomis.org/bfo/ |title=Basic Formal Ontology (BFO)
|publisher=[[Institute for Formal Ontology and Medical Information Science]] (IFOMIS) |accessdate=}}&lt;/ref&gt; a formal upper ontology designed to support scientific research
* BioPAX,&lt;ref&gt;{{cite web |url=http://biopax.org |title=BioPAX |accessdate=10 February 2011}}&lt;/ref&gt; an ontology for the exchange and interoperability of biological pathway (cellular processes) data
* BMO,&lt;ref&gt;{{cite journal |first1=Alexander |last1=Osterwalder |first2=Yves |last2=Pigneur | author-link2= Yves Pigneur | url=http://129.3.20.41/eps/io/papers/0202/0202004.pdf |title=An e-Business Model Ontology for Modeling e-Business |location=[[Bled eConference|15th Bled eConference]], [[Slovenia]] |date=June 17–19, 2002}}&lt;/ref&gt; an e-Business Model Ontology based on a review of enterprise ontologies and business model literature
* SSBMO,&lt;ref&gt;{{cite journal |first1=Antony |last1=Upward |first2=Peter |last2=Jones |url=https://www.academia.edu/14461116 |title=An Ontology for Strongly Sustainable Business Models: Defining an Enterprise Framework Compatible with Natural and Social Science |journal=Organization &amp; Environment |volume=29 |issue=1 |pages=97-123 |doi=10.1177/1086026615592933}}&lt;/ref&gt; a Strongly Sustainable Business Model Ontology based on a review of the systems based natural and social science literature (including business).  Includes critique of and significant extensions to the Business Model Ontology (BMO).
* CCO and GexKB,&lt;ref&gt;{{cite web|title=About CCO and GexKB|url=http://www.semantic-systems-biology.org/apo/|publisher=Semantic Systems Biology}}&lt;/ref&gt; Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)
* CContology (Customer Complaint Ontology),&lt;ref&gt;{{cite web |url=http://www.jarrar.info/CContology/ |title=CContology |accessdate=10 February 2011}}&lt;/ref&gt; an e-business ontology to support online customer complaint management
* [[CIDOC Conceptual Reference Model]], an ontology for [[cultural heritage]]&lt;ref&gt;{{cite web |url=http://www.cidoc-crm.org/ |title=The CIDOC Conceptual Reference Model (CRM) |accessdate=10 February 2011}}&lt;/ref&gt;
* COSMO,&lt;ref&gt;{{cite web |url=http://micra.com/COSMO/ |title=COSMO |publisher=MICRA Inc.|accessdate=10 February 2011}}&lt;/ref&gt; a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity.  It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases.  It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the [[Longman Dictionary of Contemporary English|Longman dictionary]] [[defining vocabulary]].
* [[Cyc]], a large Foundation Ontology for formal representation of the universe of discourse
* [[Disease Ontology]],&lt;ref&gt;{{cite journal |pmid=19594883}}&lt;/ref&gt; designed to facilitate the mapping of diseases and associated conditions to particular medical codes
* [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]], a Descriptive Ontology for Linguistic and Cognitive Engineering&lt;ref name="DOLCE"/&gt;&lt;ref name="DOLCE-OWL"/&gt;
* [[Drammar]], ontology of drama
* [[Dublin Core]], a simple ontology for documents and publishing
* Foundational, Core and Linguistic Ontologies&lt;ref&gt;{{cite web |url=http://www.loa-cnr.it/Ontologies.html
|title=Foundational, Core and Linguistic Ontologies |accessdate=10 February 2011}}&lt;/ref&gt;
* [[Foundational Model of Anatomy]],&lt;ref&gt;{{cite web |url=http://sig.biostr.washington.edu/projects/fm/AboutFM.html |title=Foundational Model of Anatomy |accessdate=10 February 2011}}&lt;/ref&gt; an ontology for human anatomy
* [[FOAF (software)|Friend of a Friend]], an ontology for describing persons, their activities and their relations to other people and objects
* [[Gene Ontology]] for [[genomics]]
* [[Gellish English dictionary]], an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement.
* [[Geopolitical ontology]], an ontology describing geopolitical information created by [[Food and Agriculture Organization]](FAO). The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. In addition, FAO provides web services of geopolitical ontology and a module maker to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). See more information at [[FAO Country Profiles]].
* GOLD,&lt;ref&gt;{{cite web |url=http://www.linguistics-ontology.org/gold.html |title=GOLD |accessdate=10 February 2011}}&lt;/ref&gt; General Ontology for [[descriptive linguistics|Linguistic Description]]
* GUM (Generalized Upper Model),&lt;ref&gt;{{cite web |url=http://www.fb10.uni-bremen.de/anglistik/langpro/webspace/jb/gum/index.htm |title=Generalized Upper Model |accessdate=10 February 2011}}&lt;/ref&gt; a linguistically motivated ontology for mediating between clients systems and natural language technology
* [[IDEAS Group]],&lt;ref&gt;{{cite web |url=http://www.ideasgroup.org |title=The IDEAS Group Website |accessdate=10 February 2011}}&lt;/ref&gt; a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.
* Linkbase,&lt;ref&gt;{{cite web |url=http://www.landcglobal.com/pages/linkbase.php |title=Linkbase |accessdate=10 February 2011}}&lt;/ref&gt; a formal representation of the biomedical domain, founded upon [http://www.ifomis.org/bfo/ Basic Formal Ontology].
* LPL, Lawson Pattern Language
* NCBO Bioportal,&lt;ref&gt;{{cite web|title=Bioportal|url=http://www.bioontology.org/tools/portal/bioportal.html|publisher=National Center for Biological Ontology (NCBO)}}&lt;/ref&gt; biological and biomedical ontologies and associated tools to search, browse and visualise
* [[NIFSTD]] Ontologies from the [[Neuroscience Information Framework]]: a modular set of ontologies for the neuroscience domain.
* OBO-Edit,&lt;ref&gt;{{cite web|title=Ontology browser for most of the Open Biological and Biomedical Ontologies|url=http://oboedit.org/?page=index|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}&lt;/ref&gt; an ontology browser for most of the Open Biological and Biomedical Ontologies
* [[OBO Foundry]],&lt;ref&gt;{{cite web|title=The Open Biological and Biomedical Ontologies|url=http://www.obofoundry.org/|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}&lt;/ref&gt; a suite of interoperable reference ontologies in biology and biomedicine
* OMNIBUS Ontology,&lt;ref&gt;{{cite web |url=http://edont.qee.jp/omnibus/ |title=OMNIBUS Ontology |accessdate=10 February 2011}}&lt;/ref&gt; an ontology of learning, instruction, and instructional design
* [[Ontology for Biomedical Investigations]], an open access, integrated ontology for the description of biological and clinical investigations
* ONSTR,&lt;ref&gt;{{cite web |url= https://nbsdc.org/onstr.php |title=ONSTR |accessdate=16 April 2014}}&lt;/ref&gt; Ontology for Newborn Screening Follow-up and Translational Research, Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta.
* Plant Ontology&lt;ref name="Plant Ontology"&gt;{{cite web |url=http://www.plantontology.org/ |title=Plant Ontology |accessdate=10 February 2011}}&lt;/ref&gt; for plant structures and growth/development stages, etc.
* POPE, Purdue Ontology for Pharmaceutical Engineering
* PRO,&lt;ref&gt;{{cite web |url=http://pir.georgetown.edu/pro/ |title=PRO |accessdate=10 February 2011}}&lt;/ref&gt; the Protein Ontology of the Protein Information Resource, Georgetown University
* Program abstraction taxonomy
* Protein Ontology&lt;ref&gt;{{cite web |url=http://pir.georgetown.edu/pro/ |title=Protein Ontology |accessdate=10 February 2011}}&lt;/ref&gt; for [[proteomics]]
* [[RXNO Ontology]], for [[name reaction]]s in chemistry
* [[Sequence Ontology]],&lt;ref&gt;{{cite journal |vauthors= Eilbeck K, Lewis SE, Mungall CJ, Yandell M, Stein L, Durbin R, Ashburner M |title= The Sequence Ontology: a tool for the unification of genome annotations |journal= Genome Biology |volume= 6 |issue= 5 |pages= R44 |year= 2005 |pmid= 15892872 |pmc= 1175956 |doi= 10.1186/gb-2005-6-5-r44 |authorlink5= Lincoln Stein |authorlink6= Richard M. Durbin |authorlink7= Michael Ashburner |authorlink2= Suzanna Lewis}}&lt;/ref&gt; for representing genomic feature types found on [[Sequence (biology)|biological sequences]]
* [[SNOMED CT]] (Systematized Nomenclature of Medicine—Clinical Terms)
* [[Suggested Upper Merged Ontology]], a formal upper ontology
* [[Systems Biology Ontology]] (SBO), for computational models in biology
* SWEET,&lt;ref&gt;{{cite web |url=http://sweet.jpl.nasa.gov/ |title=SWEET |accessdate=10 February 2011}}&lt;/ref&gt; Semantic Web for Earth and Environmental Terminology
* [[ThoughtTreasure]] ontology
* [[TIME-ITEM]], Topics for Indexing Medical Education
* [[Uberon]],&lt;ref&gt;{{cite journal|pmid=22293552}}&lt;/ref&gt; representing [[metazoa|animal]] anatomical structures
* [[UMBEL]], a lightweight reference structure of 20,000 subject concept classes and their relationships derived from [[Opencyc|OpenCyc]]
* [[WordNet]], a lexical reference system
* YAMATO,&lt;ref&gt;{{cite web |url=http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/upperOnto.htm |title=YAMATO |accessdate=10 February 2011}}&lt;/ref&gt; Yet Another More Advanced Top-level Ontology

The W3C [[Linked data#Linking Open Data community project|Linking Open Data community project]] coordinates attempts to converge different ontologies into worldwide [[Semantic Web]].

== Libraries ==
The development of ontologies for the Web has led to the emergence of services providing lists or directories of ontologies with search facility. Such directories have been called ontology libraries.

The following are libraries of human-selected ontologies.
* COLORE&lt;ref&gt;{{cite web |url=http://stl.mie.utoronto.ca/colore/ |title=COLORE |accessdate=4 May 2011}}&lt;/ref&gt; is an open repository of first-order ontologies in [[Common Logic]] with formal links between ontologies in the repository.
* DAML Ontology Library&lt;ref&gt;{{cite web |url=http://www.daml.org/ontologies/ |title=DAML Ontology Library |accessdate=10 February 2011}}&lt;/ref&gt; maintains a legacy of ontologies in DAML.
* Ontology Design Patterns portal&lt;ref&gt;{{cite web |url=http://www.ontologydesignpatterns.org |title=ODP Library |accessdate=21 February 2013}}&lt;/ref&gt; is a wiki repository of reusable components and practices for ontology design, and also maintains a list of ''exemplary ontologies''. 
* Protégé Ontology Library&lt;ref&gt;{{cite web
|url=http://protegewiki.stanford.edu/index.php/Protege_Ontology_Library |title=Protege Ontology Library |accessdate=10 February 2011}}&lt;/ref&gt; contains a set of OWL, Frame-based and other format ontologies.
* SchemaWeb&lt;ref&gt;{{cite web |url=http://www.schemaweb.info/ |title=SchemaWeb |accessdate=10 February 2011}}&lt;/ref&gt; is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.

The following are both directories and search engines. They include crawlers searching the Web for well-formed ontologies.
* [[OBO Foundry]] is a suite of interoperable reference ontologies in biology and biomedicine.&lt;ref&gt;{{cite web |url=http://www.obofoundry.org/ |title=OBO Foundry|accessdate=10 February 2011}}&lt;/ref&gt;&lt;ref name="pmid17989687"&gt;{{Cite journal 
| last1 = Smith | first1 = B. 
| authorlink1 = Barry Smith (ontologist)
| last2 = Ashburner | first2 = M. 
| authorlink2 = Michael Ashburner
| last3 = Rosse | first3 = C. 
| last4 = Bard | first4 = J. 
| last5 = Bug | first5 = W. 
| last6 = Ceusters | first6 = W. 
| last7 = Goldberg | first7 = L. J. 
| last8 = Eilbeck | first8 = K. 
| last9 = Ireland | first9 = A. 
| last10 = Mungall 
| doi = 10.1038/nbt1346 | first10 = C. J. 
| last11 = Leontis | first11 = N. 
| last12 = Rocca-Serra | first12 = P. 
| last13 = Ruttenberg | first13 = A. 
| last14 = Sansone | first14 = S. A. 
| last15 = Scheuermann | first15 = R. H. 
| last16 = Shah | first16 = N. 
| last17 = Whetzel | first17 = P. L. 
| last18 = Lewis | first18 = S. | authorlink18 = Suzanna Lewis
| title = The OBO Foundry: Coordinated evolution of ontologies to support biomedical data integration 
| journal = [[Nature Biotechnology]] 
| volume = 25 
| issue = 11 
| pages = 1251–1255 
| year = 2007 
| pmid = 17989687 
| pmc =2814061 
}} {{open access}}&lt;/ref&gt;
* Bioportal (ontology repository of NCBO)
* OntoSelect&lt;ref&gt;{{cite web |url=http://olp.dfki.de/OntoSelect/ |title=OntoSelect |accessdate=10 February 2011}}&lt;/ref&gt; Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.
* Ontaria&lt;ref&gt;{{cite web |url=http://www.w3.org/2004/ontaria/ |title=Ontaria |accessdate=10 February 2011}}&lt;/ref&gt; is a "searchable and browsable directory of semantic web data" with a focus on RDF vocabularies with OWL ontologies. (NB Project "on hold" since 2004).
* [[Swoogle]] is a directory and search engine for all RDF resources available on the Web, including ontologies.
* Open Ontology Repository initiative
* ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.

== Examples of applications ==
In general, ontologies can be used beneficially in 
* enterprise applications.&lt;ref&gt;{{cite journal |first=Daniel |last=Oberle |title=How ontologies benefit enterprise applications |journal=Semantic Web Journal |volume=5 |issue=6 |pages=473–491 |publisher=IOS Press |date=2014 |doi=10.3233/SW-130114 |url=http://www.semantic-web-journal.net/system/files/swj212_2.pdf |format=PDF }}&lt;/ref&gt; A more concrete example is [[SAPPHIRE (Health care)]] or ''Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines'' which is a [[semantics]]-based [[health information system]] capable of tracking and evaluating situations and occurrences that may affect [[public health]].
* [[geographic information systems]] bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.&lt;ref&gt;{{cite journal|first=Andrew U. |last=Frank|title=Tiers of ontology and consistency constraints in geographical information systems|journal=International Journal of Geographical Information Science|volume=15|issue=7|year=2001|pages=667–678|doi=10.1080/13658810110061144}}&lt;/ref&gt;

== See also ==
{{div col||25em}}
* [[Commonsense knowledge bases]]
* [[Controlled vocabulary]]
* [[Folksonomy]]
* [[Formal concept analysis]]
* [[Formal ontology]]
* [[Gene Ontology]]
* [[General formal ontology]]
* [[Lattice (order)|Lattice]]
* [[Ontology]]
* [[Ontology alignment]]
* [[Ontology chart]]
* [[Open Biomedical Ontologies]]
* [[Open Semantic Framework]]
* [[Soft ontology]]
* [[Terminology extraction]]
* [[Weak ontology]]
* [[Web Ontology Language]]
{{div col end}}

;Related philosophical concepts
* [[Alphabet of human thought]]
* [[Characteristica universalis]]
* [[Interoperability]]
* [[Metalanguage]]
* [[Natural semantic metalanguage]]

==References==
{{Reflist|2}}

==Further reading==
* Oberle, D., Guarino, N., &amp; Staab, S. (2009) [http://userpages.uni-koblenz.de/~staab/Research/Publications/2009/handbookEdition2/what-is-an-ontology.pdf What is an ontology?]. In: "Handbook on Ontologies". Springer, 2nd edition, 2009.
* Fensel, D., van Harmelen, F., Horrocks, I., McGuinness, D. L., &amp; Patel-Schneider, P. F. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920598 "OIL: an ontology infrastructure for the Semantic Web"]. In: ''Intelligent Systems''. IEEE, 16(2): 38&amp;ndash;45.
* Gangemi A., Presutti V. (2009). [http://hem.hj.se/~blev/HandbookChapter_ODPs.pdf Ontology Design Patterns].{{dead link|date=September 2016}} In Staab S. et al. (eds.): Handbook on Ontologies (2nd edition), Springer, 2009.
* Maria Golemati, Akrivi Katifori, Costas Vassilakis, George Lepouras, Constantin Halatsis (2007). [http://oceanis.mm.di.uoa.gr/pened/papers/11-onto-user-final.pdf "Creating an Ontology for the User Profile: Method and Applications"]. In: ''Proceedings of the First IEEE International Conference on Research Challenges in Information Science (RCIS)'', Morocco 2007.
* Mizoguchi, R. (2004). [http://www.ei.sanken.osaka-u.ac.jp/pub/miz/Part3V3.pdf "Tutorial on ontological engineering: part 3: Advanced course of ontological engineering"]. In: ''New Generation Computing''. Ohmsha &amp; Springer-Verlag, 22(2):198-220.
* [[Tom Gruber|Gruber, T. R.]] 1993. [http://tomgruber.org/writing/ontolingua-kaj-1993.pdf "A translation approach to portable ontology specifications"]. In: ''Knowledge Acquisition''. 5: 199&amp;ndash;199.
* Maedche, A. &amp; Staab, S. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920602 "Ontology learning for the Semantic Web"]. In: ''Intelligent Systems''. IEEE, 16(2): 72&amp;ndash;79.
* Natalya F. Noy and [[Deborah McGuinness|Deborah L. McGuinness]]. [http://www-ksl.stanford.edu/people/dlm/papers/ontology-tutorial-noy-mcguinness-abstract.html Ontology Development 101: A Guide to Creating Your First Ontology]. Stanford Knowledge Systems Laboratory Technical Report KSL-01-05 and Stanford Medical Informatics Technical Report SMI-2001-0880, March 2001.
* Prabath Chaminda Abeysiriwardana, Saluka R Kodituwakku, [http://www.ijorcs.org/manuscript/id/51/prabath-chaminda-abeysiriwardana-saluka-r-kodituwakku/ontology-based-information-extraction-ford-disease-intelligence "Ontology Based Information Extraction for Disease Intelligence"]. International Journal of Research in Computer Science, 2 (6): pp.&amp;nbsp;7–19, November 2012. doi:10.7815/ijorcs.26.2012.051
* Razmerita, L., Angehrn, A., &amp; Maedche, A. 2003. [http://www.springerlink.com/index/THW9RMVMVKLX9HAC.pdf "Ontology-Based User Modeling for Knowledge Management Systems"]. In: ''Lecture Notes in Computer Science'': 213&amp;ndash;17.
* Soylu, A., De Causmaecker, Patrick. 2009.[http://dx.doi.org/10.1109/ISCIS.2009.5291915 Merging model driven and ontology driven system development approaches pervasive computing perspective]. in Proc 24th Intl Symposium on Computer and Information Sciences. pp 730–735.
* Smith, B. [http://precedings.nature.com/documents/2027/version/2 Ontology (Science)], in C. Eschenbach and [[Michael Gruninger|M. Gruninger]] (eds.), Formal Ontology in Information Systems. Proceedings of FOIS 2008, Amsterdam/New York: ISO Press, 21&amp;ndash;35.
* [[Uschold, Mike]] &amp; [[Michael Gruninger|Gruninger, M.]] (1996). [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.5903&amp;rep=rep1&amp;type=pdf Ontologies: Principles, Methods and Applications]. Knowledge Engineering Review, 11(2).
* W. Pidcock, [http://infogrid.org/wiki/Reference/PidcockArticle ''What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?'']
* Yudelson, M., Gavrilova, T., &amp; Brusilovsky, P. 2005. [http://www.springerlink.com/index/3n0ekp8dgm4v3pr2.pdf Towards User Modeling Meta-ontology]. Lecture Notes in Computer Science, 3538: 448.
* Movshovitz-Attias, Dana and Cohen, William W. (2012) [http://www.cs.cmu.edu/~dmovshov/papers/dma_bioNELL_bioNLP2012.pdf Bootstrapping Biomedical Ontologies for Scientific Text using NELL]. BioNLP in NAACL, Association for Computational Linguistics, 2012.

==External links==
{{Commons category|Ontology}}
* [http://www.dmoz.org/Reference/Knowledge_Management/Knowledge_Representation/ Knowledge Representation] at Open Directory Project
* [http://protegewiki.stanford.edu/wiki/Protege_Ontology_Library Library of ontologies]
* [http://www.GoPubMed.com GoPubMed] using Ontologies for searching
* [http://ontolog.cim3.net/wiki ONTOLOG] (a.k.a. "[http://ontolog.cim3.net/forum/ontolog-forum/ Ontolog Forum]") - an Open, International, Virtual Community of Practice on Ontology, Ontological Engineering and Semantic Technology
* [http://trimc-nlp.blogspot.com/2013/08/nlp-driven-ontology-modeling-for.html Use of Ontologies in Natural Language Processing]
* [http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit Ontology Summit] - an annual series of events (first started in 2006) that involves the ontology community and communities related to each year's theme chosen for the summit.
&lt;!--
***********************************************************************************************

      This section shouldn't contain external links to specific ontologies,
      or to specific new subjects.

***********************************************************************************************
--&gt;
* [http://kore-nordmann.de/talks/09_04_standardization_of_ontologies_paper.pdf Standardization of Ontologies]

{{Semantic Web}}
{{Software engineering}}
{{computable knowledge}}

{{Authority control}}

{{DEFAULTSORT:Ontology (Information Science)}}
[[Category:Knowledge engineering]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)| ]]
[[Category:Knowledge representation]]
[[Category:Knowledge bases]]</text>
      <sha1>83jxg0obhnun9zg07dg81z2iryha2gm</sha1>
    </revision>
  </page>
  <page>
    <title>Basic Formal Ontology</title>
    <ns>0</ns>
    <id>18025074</id>
    <revision>
      <id>746536505</id>
      <parentid>724945879</parentid>
      <timestamp>2016-10-28T01:04:27Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v1.2.6)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4244" xml:space="preserve">The '''Basic Formal Ontology''' ([http://www.ifomis.org/bfo BFO]) is a formal ontological framework developed by [[Barry Smith (ontologist)|Barry Smith]] and his associates that consists in a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: '''continuant''' (or snapshot) ontologies, comprehending continuant entities such as three-dimensional enduring objects, and '''occurrent''' ontologies, comprehending processes conceived as extended through (or as spanning) time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. Each continuant ontology is an inventory of all entities existing at a time. Each occurrent ontology is an inventory (processory) of all the processes unfolding through a given interval of time. Both types of ontology serve as basis for a series of sub-ontologies, each of which can be conceived as a window on a certain portion of reality at a given level of granularity.

==Applications of BFO==

BFO has been adopted as a foundational ontology by many [http://www.ifomis.org/bfo/users projects], principally in the areas of biomedical ontology and security and defense (intelligence) ontology. An example application of BFO can be seen in the [[Ontology for Biomedical Investigations]] (OBI).

==References==

*Arp, R., Smith, B., and Spear, A. D. ''[http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology]'', Cambridge, MA: MIT Press, August 2015, xxiv + 220pp.
* Grenon, P. and Smith, B. (2004) [http://ontology.buffalo.edu/smith/articles/SNAP_SPAN.pdf “SNAP and SPAN: Towards Dynamic Spatial Ontology”], Spatial Cognition and Computation, 4:1, 69-103.
* Smith, B. and Grenon, P. (2004) [http://ontology.buffalo.edu/smith/articles/cornucopia.pdf “The Cornucopia of Formal-Ontological Relations”], Dialectica, 58:3, 279-296.
*http://www.ifomis.uni-saarland.de/bfo/
*https://github.com/bfo-ontology/BFO/wiki
*http://ncorwiki.buffalo.edu/index.php/Basic_Formal_Ontology_2.0
==See also==

* [[Formal Ontology]]
* [[Upper ontology]]

==External links==

* [http://www.ifomis.org/bfo Basic Formal Ontology at IFOMIS]
*Katherine Munn, Barry Smith (Eds.): [http://www.ontosverlag.com/index.php?page=shop.product_details&amp;flypage=flypage.tpl&amp;product_id=108&amp;category_id=13&amp;option=com_virtuemart&amp;Itemid=1&amp;lang=en ''Applied Ontology: An Introduction''], Ontos Verlag.
*Ludger Jansen: "[http://ontology.buffalo.edu/bfo/Tendencies.pdf Tendencies and other Realizables in Medical Information Sciences]"
*Fabian Neuhaus, Pierre Grenon, Barry Smith: "[http://ontology.buffalo.edu/bfo/SQU.pdf A Formal Theory of Substances, Qualities, and Universals]"
*Luc Schneider: "[http://www.ifomis.org/bfo/documents/schneider-fois2010.pdf Revisiting the Ontological Square]"
*Lars Vogt: "[http://www.biomedcentral.com/1471-2105/11/289 Spatio-structural granularity of biological material entities]"
*Barry Smith, Werner Ceusters, Bert Klagges, Jacob Köhler, Anand Kumar, Jane Lomax, Chris Mungall, Fabian Neuhaus, Alan Rector and Cornelius Rosse: "[http://genomebiology.com/2005/6/5/R46 Relations in Biomedical Ontologies]", Genome Biology (2005), 6 (5), R46
*Thomas Bittner, Maureen Donnelly and Barry Smith: "[http://www.acsu.buffalo.edu/~bittner3/Publications_files/Bittner-NA-2006-28.pdf A Spatio-Temporal Ontology for Geographic Information Integration]", International Journal for Geographical Information Science, 23 (6), 2009, 765-798
* [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0012258 Realism in Biology]
* Smith, B. and Ceusters, W. (2010) “[http://iospress.metapress.com/content/1551884412214u67/fulltext.pdf Ontological Realism as a Methodology for Coordinated Evolution of Scientific Ontologies]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}”, Applied Ontology, 5 (2010), 139–188.


[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Ontology]]
[[Category:Ontology (information science)]]</text>
      <sha1>fkdd8y7dnihwyzyy4qvp816xxuyy936</sha1>
    </revision>
  </page>
  <page>
    <title>Reason maintenance</title>
    <ns>0</ns>
    <id>849986</id>
    <revision>
      <id>732073166</id>
      <parentid>712416772</parentid>
      <timestamp>2016-07-29T12:22:03Z</timestamp>
      <contributor>
        <ip>2001:4898:8010:0:0:0:0:4FE</ip>
      </contributor>
      <comment>/* Other references */ Corrected author's name.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6769" xml:space="preserve">{{more footnotes|date=September 2009}}

'''Reason maintenance'''&lt;ref name="insNouts"&gt;Doyle, J.: The ins and outs of reason maintenance.&lt;/ref&gt;&lt;ref name="originalTR"&gt;Doyle, J.: Truth maintenance systems for problem solving. Tech. Rep. AI-TR-419,
Dep. of Electrical Engineering and Computer Science of MIT (1978)&lt;/ref&gt; is a [[knowledge representation]] approach to efficient handling of inferred information that is explicitly stored. Reason maintenance distinguishes between base facts, which can be [[Defeasible reasoning|defeated]], and derived facts. As such it differs from [[belief revision]] which, in its basic form, assumes that all facts are equally important. Reason maintenance was originally developed as a technique for implementing problem solvers.&lt;ref name="originalTR"/&gt; It encompasses a variety of techniques that share a common architecture:&lt;ref name="mcAllesterInterface"&gt;McAllester, D.A.: Truth maintenance. AAAI90 (1990)&lt;/ref&gt; two components - a reasoner and a reason maintenance system - communicate with each other via an interface. The reasoner uses the reason maintenance system to record its inferences and justifications of ("reasons" for) the inferences. The reasoner also informs the reason maintenance system which are the currently valid base facts (assumptions). The reason maintenance system uses the information to compute the truth value of the stored derived facts and to restore consistency if an inconsistency is derived.

A '''truth maintenance system''', or '''TMS''', is a [[knowledge representation]] method for representing both beliefs and their dependencies and an algorithm called the "truth maintenance algorithm" that manipulates and maintains the dependencies. The name ''truth maintenance'' is due to the ability of these systems to restore consistency.   

It is also termed as a belief revision system, a truth maintenance system maintains consistency between old believed knowledge and current believed knowledge in the knowledge base (KB) through revision. If the current believed statements contradict the knowledge in the KB, then the KB is updated with the new knowledge. It may happen that the same data will again come into existence, and the previous knowledge will be required in the KB. If the previous data is not present, it is required for new inference. But if the previous knowledge was in the KB, then no retracing of the same knowledge was needed. Hence the use of TMS to avoid such retracing; it keeps track of the contradictory data with the help of a dependency record. This record reflects the retractions and additions which makes the inference engine (IE) aware of its current belief set.

Each statement having at least one valid justification is made a part of the current belief set. When a contradiction is found, the statement(s) responsible for the contradiction are identified and an appropriate is retraced. This process is called dependency-directed backtracking.

The TMS algorithm maintains the records in the form of a dependency network. The nodes in the network are one of the entries in the KB (a premise, antecedent, or inference rule etc.) Each arc of the network represent the inference steps from which the node was derived.

A premise is a fundamental belief which is assumed to be always true. They do not need justifications. Considering premises are base from which justifications for all other nodes will be stated.

There are two types of justification for each node. They are:

# Support List [SL]
# Conditional Proof (CP)

Many kinds of truth maintenance systems exist.   Two major types are single-context and multi-context truth maintenance.   
In single context systems, consistency is maintained among all facts in memory (database) and relates to the notion of consistency found in [[classical logic]]. Multi-context systems support [[paraconsistency]] by allowing consistency to be  relevant to a subset of facts in memory (a context) according to the history of logical inference.  This is achieved by tagging each fact or deduction with its logical history. Multi-agent truth maintenance systems perform truth maintenance across multiple memories, often located on different machines. de Kleer's assumption-based truth maintenance system (ATMS, 1986) was utilized in systems based upon [[AI winter#The fall of expert systems|KEE]] on the [[Lisp Machine]]. The first multi-agent TMS was created by Mason and Johnson. It was a multi-context system.  Bridgeland and Huhns created the first single-context multi-agent system.

==See also==
* [[Knowledge representation]]
* [[Artificial intelligence]]
* [[Belief revision]]
* [[Knowledge acquisition]]

==References==
&lt;references /&gt;

==Other references==
* Bridgeland, D. M. &amp; Huhns, M. N.,  Distributed Truth Maintenance. Proceedings of. AAAI–90: Eighth National Conference on Artificial Intelligence, 1990.
* J. de Kleer (1986). An assumption-based TMS. ''Artificial Intelligence'', 28:127–162.
* J. Doyle. A Truth Maintenance System. AI. Vol. 12. No 3, pp.&amp;nbsp;251–272. 1979.
* U. Junker and K. Konolige (1990). Computing the extensions of autoepistemic and default logics with a truth maintenance system. In ''Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI'90)'', pages 278–283. [[MIT Press]].
* Mason, C. and Johnson, R. DATMS: A Framework for Assumption Based Reasoning, in Distributed Artificial Intelligence, Vol. 2, [[Morgan Kaufmann Publishers]], Inc., 1989.
* D. A. McAllester. A three valued maintenance system. [[Massachusetts Institute of Technology]], Artificial Intelligence Laboratory. AI Memo 473. 1978.
* G. M. Provan (1988). A complexity analysis of assumption-based truth maintenance systems. In B. Smith and G. Kelleher, editors, ''Reason Maintenance Systems and their Applications'', pages 98–113. Ellis Horwood, New York.
* G. M. Provan (1990). The computational complexity of multiple-context truth maintenance systems. In ''Proceedings of the Ninth European Conference on Artificial Intelligence (ECAI'90)'', pages 522–527.
* R. Reiter and J. de Kleer (1987). Foundations of assumption-based truth maintenance systems: Preliminary report. In ''Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI'87)'', pages 183–188. [http://www2.parc.com/spl/members/dekleer/Publications/Foundations%20of%20Assumption-Based%20Truth%20Maintenance%20Systems.pdf  PDF]

==External links==
* [http://scholar.google.com/scholar?q=Truth+maintenance+system&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en&amp;btnG=Search Google Scholar on TMSs]
* [http://plato.stanford.edu/entries/logic-ai/#3.2.1 Belief Revision and TMSs] at [[Stanford Encyclopedia of Philosophy]]

[[Category:Belief revision]]
[[Category:Knowledge representation]]
[[Category:Information systems]]</text>
      <sha1>ofbk0oqkmzseyotmtmkkzzamjw2kz8d</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise interoperability</title>
    <ns>0</ns>
    <id>24007403</id>
    <revision>
      <id>692606278</id>
      <parentid>692605929</parentid>
      <timestamp>2015-11-26T23:30:08Z</timestamp>
      <contributor>
        <ip>108.221.18.208</ip>
      </contributor>
      <comment>Removed a little gibberish (translation errors?)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7361" xml:space="preserve">'''Enterprise interoperability''' is the ability of an enterprise—a company or other large organization—to functionally link activities, such as [[product design]], [[supply chains]], manufacturing, in an efficient and competitive way. 

The research in interoperability of enterprise practised in is various domains itself ([[Enterprise Modelling]], [[Ontologies]], [[Information systems]], Architectures and Platforms) which it is a question of positioning.&lt;ref name="Doumei2008"/&gt;

== Enterprise interoperability topics ==
===Interoperability in enterprise architecture===
Enterprise architecture (EA) presents a high level design of enterprise capabilities that defines successful IT projects in coherence with enterprise principals and business related requirements. EA covers mainly (i) the business capabilities analysis and validation; (ii) the development of business, application, data and technical architectures and solutions, and finally (iii) the control of programme and project implementation and governance. The application of EA methodology feeds the enterprise repository reference frame with sets of building blocks used to compose the targeted system.

The interoperability can be considered either as a principal, requirement or constraint that impact the definition of patterns to compose building blocks in the definition of targeted architectural roadmap. In this scope, EA within the TOGAF perspective,&lt;ref name="TOGAF2011"/&gt; aims to reconcile interoperability requirements with potential solutions that make developed systems interoperable.
So as to maintain the interoperability challenge quite present in the next steps of system’s lifecycle, several models and Frameworks are developed under the topic enterprise interoperability.

===Enterprise interoperability frameworks===
To preserve interoperability, several [[Enterprise Interoperability Framework|enterprise interoperability frameworks]] can be identified in the literature:
* 2003: IDEAS:&lt;ref name="IDEAS"/&gt; Interoperability Developments for Enterprise Application and Software.
* 2004: EIF:&lt;ref name="EIF"/&gt; The European Interoperability Framework
* 2004: e-GIF:&lt;ref name="eGIF"/&gt; e-Government Interoperability Framework
* 2006: FEI:&lt;ref name="FEI"/&gt; The Framework for Enterprise Interoperability
* 2006: C4IF:&lt;ref name="Peristeras2006"/&gt; Connection, Communication, Consolidation, Collaboration Interoperability Framework
* 2007: AIF:&lt;ref name="ATHENA"/&gt; Athena Interoperability Framework
* 2007:&lt;ref name="EntSysArch2007"/&gt;  Enterprise Architecture Framework for Agile and Interoperable Virtual Enterprises

The majority of these frameworks considers enterprise at several aspects, viewpoints or abstraction levels: business, process, knowledge, application, technology, data, technic, etc. and proposes guidelines to support modeling and connection capabilities between these levels. The semantic challenge is considered as transversal to all these abstraction levels.
Setting up and applying guidelines and methodologies developed within these frameworks requires modeling efforts that identify and connect artifacts.

===Interoperability in software engineering===
The evolution of IT technologies aims to outsource IT capabilities to vendors to manage for use on demand. The evolution pathway starts form packaged solutions and goes through Infrastructure as a service (Iaas), Platform as a service (Paas), Software as a service (Saas) and recently the Cloud. Interoperability efforts are still mainly expected among these levels:
* strategy to business 
* business to processes
* processes to application

Dealing with business process definition, alignment, collaboration and interoperability, several international standards propose methodologies and guidelines in these perspectives:
* ISO 15704—Requirements for enterprise-reference architectures and methodologies
* CEN-ISO DIS 19439—Framework for Enterprise Modeling
* CEN-ISO WD 19440—Constructs for Enterprise Modeling
* ISO 18629—Process specification language
* ISO/IEC 15414—ODP Reference Model—Enterprise Language 
In addition, recent standards (BPMN, BPEL, etc.) and their implementation technologies propose relevant integration capabilities. Furthermore, model driven-engineering &lt;ref name="MDAguide"/&gt; provides capabilities that connect, transform and refine models to support interoperability.

===Metrics for interoperability maturity assessment===
The following approaches propose some metrics to assess the interoperability maturity,&lt;ref name="Ford2008"/&gt;&lt;ref name="GUEDRIA"/&gt; 
* LISI: Levels of Information Systems Interoperability
* OIM: Organizational Interoperability Model
* NMI: NC3TA reference Model for Interoperability
* LCIM: Levels of Conceptual Interoperability Model
* EIMM: Enterprise Interoperability Maturity Model
* Smart Grid Interoperability Maturity Model Rating System

For the several interoperability aspects identified previously, the listed maturity approaches define interoperability categories (or dimensions) and propose qualitative as well as qualitative cross cutting issues to assess them. While interoperability aspects are not covered by a single maturity approach, some propositions go deeply in the definition of metric dimensions at one interoperability aspect such as the business interoperability measurement proposed by Aneesh.&lt;ref name="Zutshi"/&gt;

== See also ==
* [[INTEROP-VLab]]

== References ==
{{reflist|
refs=
&lt;ref name=Doumei2008&gt;Chen, D., [[Guy Doumeingts|Doumeingts]], G., and [[François Vernadat|Vernadat, F.]] 2008. Architectures for enterprise integration and interoperability: Past, present and future. ''Comput. Ind.'' 59, 7 (Sep. 2008), 647–659. {{en icon}} [http://dx.doi.org/10.1016/j.compind.2007.12.016 : DOI]&lt;/ref&gt;
&lt;ref name=TOGAF2011&gt;TOGAF® 9 Certified, 2nd edition. The Open Group, 2011.&lt;/ref&gt;
&lt;ref name=IDEAS&gt;“A Contribution to Enterprise Interoperability Maturity Assessment”&lt;/ref&gt;
&lt;ref name=EIF&gt;EIF 2.0 http://ec.europa.eu/idabc/servlets/Docb0db.pdf&lt;/ref&gt;
&lt;ref name=eGIF&gt;http://edina.ac.uk/projects/interoperability/e-gif-v6-0_.pdf&lt;/ref&gt;
&lt;ref name=FEI&gt;http://chen33.free.fr/M2/Elearning/CIGI2009.Chen.final.pdf&lt;/ref&gt;
&lt;ref name=Peristeras2006&gt;Peristeras, V., and K. Tarabanis (2006): The Connection, Communication, Consolidation, Collaboration Interoperability Framework (C4IF) for Information Systems Interoperability, International Journal of Interoperability in Business Information Systems (IBIS), Vol. 1, No. 1, pp. 61-72.&lt;/ref&gt;
&lt;ref name=ATHENA&gt;http://www.asd-ssg.org/html/ATHENA/Deliverables/Deliverables%20provided%20to%20EC%206th%206%20Months/070306_ATHENA_DA82_V10.pdf&lt;/ref&gt;
&lt;ref name=EntSysArch2007&gt;Handbook of Enterprise Systems Architecture in Practice, 2007&lt;/ref&gt;
&lt;ref name=MDAguide&gt;http://www.omg.org/cgi-bin/doc?omg/03-06-01.pdf&lt;/ref&gt;
&lt;ref name=Ford2008&gt;Ford T., et al. Measuring System Interoperability: An i-Score Improvement. Proceedings of the 6th Annual Conference on Systems Engineering Research. Los Angeles, CA, April 4–5, 2008&lt;/ref&gt;
&lt;ref name=GUEDRIA&gt;GUEDRIAhttp://ori-oai.u-bordeaux1.fr/pdf/2012/GUEDRIA_WIDED_2012.pdf&lt;/ref&gt;
&lt;ref name=Zutshi&gt;http://run.unl.pt/bitstream/10362/2646/1/Zutshi_2010.pdf&lt;/ref&gt;
}}

== External links ==
* [http://www.interop-vlab.eu INTEROP-VLab]

[[Category:Interoperability]]
[[Category:Enterprise modelling]]
[[Category:Knowledge representation]]</text>
      <sha1>8qu979rhik3qxnf2niug0tly9wzrssa</sha1>
    </revision>
  </page>
  <page>
    <title>Linguistic value</title>
    <ns>0</ns>
    <id>15299080</id>
    <revision>
      <id>690594418</id>
      <parentid>690557883</parentid>
      <timestamp>2015-11-14T11:53:38Z</timestamp>
      <contributor>
        <ip>108.183.102.223</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1619" xml:space="preserve">:''For the similar but unrelated term in linguistics see '' [[Linguistic variable]]

In [[artificial intelligence]], [[operations research]], and related fields, a '''Linguistic value''', (for some authors '''Linguistic variable''') is a [[natural language]] term which is derived using quantitative or qualitative reasoning such as with probability and statistics or [[fuzzy sets and systems]].&lt;ref&gt;''Fuzzy Logic for Business and Industry'' Earl Cox, Charles River Media, pp188,214,302,306,352  1995 ISBN 1-886801-01-0&lt;/ref&gt;&lt;ref&gt;''The Fuzzy Systems Handbook, Second Edition'' Earl Cox, Academic Press, 1999 ISBN 0-12-194455-7 Ch 6 Fuzzy Reasoning, &amp;sect; 1 The Role of Linguistic Variables&lt;/ref&gt;
&lt;ref&gt;''On the Modeling of Linguistic Information using Random Sets'' Hung T. Nguyen p. 242 in Readings in Fuzzy Sets for Intelligent Systems. Morgan Kaufmann 1993. Dubois, Prade, and Yager eds. &lt;/ref&gt;
&lt;ref&gt;''Fuzzy Sets And The Social Nature of Truth'' J. Goguen. CS UCLA p. 49-67 in Advances in Fuzzy Sets and Systems, North Holland, 1979. &amp;sect; 2.3 ''Linguistic Truth Values''. ISBN 0-444-85372-3&lt;/ref&gt;

==Example of Linguistic Value==

For example, if a shuttle heat shield is deemed of having a linguistic value of a "very low" percentage of damage in re-entry, based upon knowledge from experts in the field, that probability would be given a value of say, 5%.  From there on out, if it were to be used in an equation, the variable of percentage of damage will be at 5% if it deemed very low percentage.

==References==
{{Reflist}}

{{DEFAULTSORT:Linguistic Value}}
[[Category:Knowledge representation]]


{{AI-stub}}</text>
      <sha1>sc5pqvdi127rfd6amm5f6kv7fn0wbnw</sha1>
    </revision>
  </page>
  <page>
    <title>Completeness (knowledge bases)</title>
    <ns>0</ns>
    <id>25154746</id>
    <revision>
      <id>418198636</id>
      <parentid>327195102</parentid>
      <timestamp>2011-03-10T22:07:22Z</timestamp>
      <contributor>
        <ip>129.93.162.155</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="909" xml:space="preserve">A [[knowledge base]] KB is '''complete''' ''if'' there is no formular α such that KB ⊭ α and KB ⊭ ¬α.

Example of knowledge base with incomplete knowledge:

KB := { A ∨ B }

Then we have KB ⊭ A and KB ⊭ ¬A.

In some cases, you can make a [[Consistency (knowledge bases)|consistent knowledge base]] complete with the [[closed world assumption]] - that is, adding all not-entailed literals as negations to the knowledge base. In the above example though, this would not work because it would make the knowledge base inconsistent:

KB' = { A ∨ B, ¬A, ¬B }

In the case you have KB := { P(a), Q(a), Q(b) }, you have KB ⊭ P(b) and KB ⊭ ¬P(b), so with the closed world assumption you would get KB' = { P(a), ¬P(b), Q(a), Q(b) } where you have KB' ⊨ ¬P(b).

See also:

* [[Vivid knowledge]]

{{computable knowledge}}

[[Category:Knowledge representation]]
{{logic-stub}}
{{database-stub}}</text>
      <sha1>772grdrxxl9j3uze7d0fptigpp61zn1</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology alignment</title>
    <ns>0</ns>
    <id>4696039</id>
    <revision>
      <id>757961504</id>
      <parentid>757958751</parentid>
      <timestamp>2017-01-02T18:34:32Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Clarify}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11736" xml:space="preserve">'''Ontology alignment''', or '''ontology matching''', is the process of determining correspondences between [[concept]]s in [[ontologies]]. A set of correspondences is also called an alignment. The phrase takes on a slightly different meaning, in [[computer science]], [[cognitive science]] or [[philosophy]].

==Computer Science==

For [[computer scientist]]s, concepts are expressed as labels for data.  Historically, the need for ontology alignment arose out of the need to [[data integration|integrate]] heterogeneous [[database]]s, ones developed independently and thus each having their own data vocabulary.  In the [[Semantic Web]] context involving many actors providing their own [[ontology (information science)|ontologies]], ontology matching has taken a critical place for helping heterogeneous resources to interoperate. Ontology alignment tools find classes of data that are "[[semantic equivalence|semantically equivalent]]," for example, "Truck" and "Lorry."  The classes are not necessarily logically identical.  According to Euzenat and Shvaiko (2007),&lt;ref name="Euzenat Shvaiko"&gt;Jérôme Euzenat and Pavel Shvaiko. 2007. [http://book.ontologymatching.org Ontology matching], Springer-Verlag, 978-3-540-49611-3.&lt;/ref&gt; there are three major dimensions for similarity: syntactic, external, and semantic.  Coincidentally, they roughly correspond to the dimensions identified by Cognitive Scientists below.  A number of tools and frameworks have been developed for aligning ontologies, some with inspiration from Cognitive Science and some independently.

Ontology alignment tools have generally been developed to operate on [[database schema]]s,&lt;ref&gt;J. Berlin and A. Motro. 2002. [http://www.dit.unitn.it/~accord/RelatedWork/Matching/Berlin_caise02.pdf Database Schema Matching Using Machine Learning with Feature Selection]. Proc. of the 14th International Conference on Advanced Information Systems Engineering, pp. 452-466&lt;/ref&gt; [[XML schema]]s,&lt;ref name="coma"&gt;D. Aumueller, H. Do, S. Massmann, E. Rahm. 2005. [http://www.dit.unitn.it/~p2p/RelatedWork/Matching/COMA++-SIGMOD05.pdf Schema and ontology matching with COMA++]. Proc. of the 2005 International Conference on Management of Data, pp. 906-908&lt;/ref&gt; [[Taxonomy (general)|taxonomies]],&lt;ref&gt;S. Ponzetto, R. Navigli. 2009. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf "Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia"]. Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009), Pasadena, California, pp. 2083-2088.&lt;/ref&gt; [[formal language]]s, [[entity-relationship model]]s,&lt;ref&gt;A. H. Doan, A. Y. Halevy. [http://pages.cs.wisc.edu/~anhai/papers/si-survey-db-community.pdf Semantic integration research in the database community: A brief survey]. AI magazine, 26(1), 2005&lt;/ref&gt; [[dictionary|dictionaries]], and other label frameworks. They are usually converted to a graph representation before being matched. 
Since the emergence of the Semantic Web, such graphs can be represented in the [[Resource Description Framework]] line of languages by triples of the form &lt;subject, predicate, object&gt;, as illustrated in the [[Notation 3]] syntax.
In this context, aligning ontologies is sometimes referred to as "ontology matching".

The problem of Ontology Alignment has been tackled recently by trying to compute matching first and mapping (based on the matching) in an automatic fashion. Systems like [[DSSim]], X-SOM&lt;ref name="curino-xsom2007"&gt;{{cite journal|author=Carlo A. Curino and Giorgio Orsi and Letizia Tanca |title=X-SOM: A Flexible Ontology Mapper |url=http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |journal=International Workshop on Semantic Web Architectures for Enterprises (SWAE'07) in conjunction with the 18th International Conference on Database and Expert Systems Applications (DEXA'07) |year=2007 |format= |deadurl=yes |archiveurl=https://web.archive.org/web/20120213104823/http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |archivedate=February 13, 2012 }}&lt;/ref&gt; or COMA++ obtained at the moment very high precision and recall.&lt;ref name="coma" /&gt; The [http://oaei.ontologymatching.org Ontology Alignment Evaluation Initiative] aims to evaluate, compare and improve the different approaches.

More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

=== Formal Definition ===
Given two ontologies &lt;math&gt;i=\langle C_{i}, R_{i}, I_{i}, A_{i}\rangle&lt;/math&gt; and &lt;math&gt;j=\langle C_{j}, R_{j}, I_{j}, A_{j}\rangle&lt;/math&gt;{{clarify|Give a (link to a) formal definition of 'ontology' first, such that the meaning of C, R, I, and A is explained.|date=January 2017}} we can define different type of (inter-ontology) relationships among their terms. 
Such relationships will be called, all together, alignments and can be categorized among different dimensions:

* similarity vs logic: this is the difference between matchings (predicating about the [[semantic similarity|similarity]] of ontology terms), and mappings ([[logical axiom]]s, typically expressing [[logical equivalence]] or inclusion among ontology terms)
* atomic vs complex: whether the alignments we considered are [[bijection|one-to-one]], or can involve more terms in a query-like formulation (e.g., [[data integration|LAV/GAV]] mapping)
* homogeneous vs heterogeneous: do the alignments predicate on terms of the same type (e.g., classes are related only to classes, individuals to individuals, etc.) or we allow heterogeneity in the relationship?
* type of alignment: the semantics associated to an alignment. It can be [[Hierarchy#Subsumptive containment hierarchy|subsumption]], [[logical equivalence|equivalence]], [[disjointness]], [[part-of]] or any user-specified relationship.

Subsumption, atomic, homogeneous alignments are the building blocks to obtain richer alignments, and have a well defined semantics in every Description Logic. 
Let's now introduce more formally ontology matching and mapping.

An atomic homogeneous '''matching''' is an alignment that carries a similarity degree &lt;math&gt;s\in [0,1]&lt;/math&gt;, describing the similarity of two terms of the input ontologies &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;.
Matching can be either ''computed'', by means of heuristic algorithms, or ''[[inference|inferred]]'' from other matchings.

Formally we can say that, a matching is a quadruple &lt;math&gt;m=\langle id, t_{i}, t_{j}, s\rangle&lt;/math&gt;, where &lt;math&gt;t_{i}&lt;/math&gt; and &lt;math&gt;t_{j}&lt;/math&gt; are homogeneous ontology terms, &lt;math&gt;s&lt;/math&gt; is the similarity degree of &lt;math&gt;m&lt;/math&gt;. 
A (subsumption, homogeneous, atomic) mapping is defined as a pair &lt;math&gt;\mu=\langle t_{i}, t_{j}\rangle&lt;/math&gt;, where &lt;math&gt;t_{i}&lt;/math&gt; and &lt;math&gt;t_{j}&lt;/math&gt; are homogeneous ontology terms.

==Cognitive Science==

For [[cognitive scientist]]s interested in ontology alignment, the "concepts" are nodes in a [[semantic network]] that reside in brains as "conceptual systems."  The focal question is: if everyone has unique experiences and thus different semantic networks, then how can we ever understand each other?  This question has been addressed by a model called ABSURDIST (Aligning Between Systems Using Relations Derived Inside Systems for Translation). Three major dimensions have been identified for similarity as equations for "internal similarity, external similarity, and mutual inhibition."&lt;ref&gt;R. Goldstone and B. Rogosky. 2002. [http://courses.media.mit.edu/2003spring/mas963/goldstone.pdf Using relations within conceptual systems to translate across conceptual systems]. Cognition 84, pp. 295–320.&lt;/ref&gt;

Ontology alignment is closely related to [[analogy formation]], where "concepts" are variables in logic expressions.

==Ontology alignment methods==
Two sub research fields have emerged in recent years in ontology mapping, namely monolingual ontology mapping and cross-lingual ontology mapping. The former refers to the mapping of ontologies in the same natural language, whereas the latter refers to "the process of establishing relationships among ontological resources from two or more independent ontologies where each ontology is labelled in a different natural language".&lt;ref&gt;Bo Fu, Rob Brennan, Declan O'Sullivan, A Configurable Translation-Based Cross-Lingual Ontology Mapping System to adjust Mapping Outcomes. Journal of Web Semantics, Volume 15, 15-36, ISSN 1570-8268, 2012 [http://www.sciencedirect.com/science/article/pii/S1570826812000704].&lt;/ref&gt; Existing matching methods in monolingual ontology mapping are discussed in Euzenat and Shvaiko (2007).&lt;ref name="Euzenat Shvaiko"/&gt; Current approaches to cross-lingual ontology mapping are presented in Fu et al. (2011).&lt;ref&gt;Fu B., Brennan R., O'Sullivan D., Using Pseudo Feedback to Improve Cross-Lingual Ontology Mapping [http://www.springerlink.com/content/a214858426kgm750/]. In Proceedings of the 8th Extended Semantic Web Conference (ESWC 2011), LNCS 6643, pp.336-351, Heraklion, Greece, May 2011.&lt;/ref&gt;

==Philosophy==

For philosophers, much like cognitive scientists, the interest is in the nature of "understanding."  The roots of discourse, however, may be traced to [[radical interpretation]].

==Visualization Tools (links obsolete)==
*[http://www.mondeca.com/content/download/718/6964/file/ITM_ALIGN_en.pdf ITM Align: semi-automated ontology alignment]
*[http://cs.uga.edu/~uthayasa/Optima/Optima.html Optima: A Visual Ontology Alignment Tool]
*[http://www.stanford.edu/~sfalc/cogz/cogz.html CogZ: Cognitive Support and Visualization for Human-Guided Mapping Systems]
*[http://agreementmaker.org AgreementMaker: Efficient Matching for Large Real-World Schemas and Ontologies]
*[http://bio-mixer.appspot.com/ Biomixer]: A Web-based Collaborative Ontology Visualization Tool.
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6882028 SDI(Semantic Data Integration) Tool]: A Semantic Mapping Representation and Generation Tool Using UML for System Engineers

==See also==
* [[Ontology (computer science)]]
* [[Rule Interchange Format]] (RIF)
* [[Data conversion]]
* [[Semantic Integration]]
* [[Semantic matching]]
* [[Minimal mappings|Minimal Mappings]]
* [[Semantics|Interpretation]] "An interpretation can be the part of a presentation or portrayal of information altered in order to conform to a specific set of symbols."
* [[Graph isomorphism]]
* [[Unification (computer science)]] (as [[Semantic unification]])
* [[Semantic integration]]

==References==
{{Reflist|colwidth=35em}}

== Further reading ==
*[http://www.ontologymatching.org/publications.html Collection of surveys and research papers related to ontology mapping, matching, and alignment]
* [http://www.atl.external.lmco.com/projects/ontology/ The Ontology Alignment Source]
* [http://cognitrn.psych.indiana.edu/rgoldsto/pdfs/cogsci2002.pdf ABSURDIST]
* [http://ontologymatching.org/publications.html Ontologymatching.org: Surveys, Approaches, and Themes]
* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data]
* [http://sites.google.com/site/bschopman/master-thesis/master.pdf Instance-based Ontology Matching by Instance Enrichment]
* Noy, N. F. (2004). "Semantic integration: a survey of ontology-based approaches." SIGMOD Rec. 33(4): 65-70.

[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Knowledge representation]]</text>
      <sha1>rtk9ywu7w3r0ahtwds0pxy5mq8jamwu</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic data model</title>
    <ns>0</ns>
    <id>19558680</id>
    <revision>
      <id>741600527</id>
      <parentid>700444944</parentid>
      <timestamp>2016-09-28T14:36:01Z</timestamp>
      <contributor>
        <ip>168.215.73.114</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8765" xml:space="preserve">[[File:A2 4 Semantic Data Models.svg|thumb|320px|Semantic data models.&lt;ref name ="FIPS184"&gt;[http://www.itl.nist.gov/fipspubs/idef1x.doc FIPS Publication 184] released of IDEF1X by the Computer Systems Laboratory of the National Institute of Standards and Technology (NIST). 21 December 1993.&lt;/ref&gt;]]
A '''semantic data model''' in [[software engineering]] has various meanings: 
# It is a [[conceptual data model]] in which semantic information is included. This means that the model describes the meaning of its instances. Such a semantic [[data model]] is an abstraction that defines how the stored [[symbol]]s (the instance data) relate to the real world.&lt;ref name ="FIPS184"/&gt;
# It is a [[conceptual data model]] that includes the capability to express information that enables parties to the information exchange to interpret meaning (semantics) from the instances, without the need to know the meta-model. Such semantic models are fact oriented (as opposed to object oriented). Facts are typically expressed by [[binary relations]] between [[data]] elements, whereas higher order relations are expressed as collections of binary relations. Typically binary relations have the form of triples: Object-RelationType-Object. For example: the Eiffel Tower &lt;is located in&gt; Paris.
Typically the instance data of semantic data models explicitly include the kinds of relationships between the various data elements, such as &lt;is located in&gt;. To interpret the meaning of the facts from the instances it is required that the meaning of the kinds of relations (relation types) be known. Therefore, semantic data models typically standardise such relation types. This means that the second kind of semantic data models enable that the instances express facts that include their own meaning. 
The second kind of semantic data models are usually meant to create semantic databases. The ability to include meaning in semantic databases facilitates building [[distributed database]]s that enable applications to interpret the meaning from the content. This implies that semantic databases can be integrated when they use the same (standard) relation types. This also implies that in general they have a wider applicability than relational or object oriented databases.

== Overview ==
The logical data structure of a [[database management system]] (DBMS), whether [[Hierarchical model|hierarchical]], [[Network model|network]], or [[Relational model|relational]], cannot totally satisfy the [[Requirements analysis|requirements]] for a conceptual definition of data, because it is limited in scope and biased toward the implementation strategy employed by the DBMS. Therefore, the need to define data from a [[Three schema approach|conceptual view]] has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure. The real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.&lt;ref name ="FIPS184"/&gt;

According to Klas and Schrefl (1995), the "overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the [[Artificial Intelligence]] field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations".&lt;ref&gt;Wolfgang Klas, Michael Schrefl (1995). "Semantic data modeling" In: ''Metaclasses and Their Application''. Book Series Lecture Notes in Computer Science. Publisher Springer Berlin / Heidelberg. Volume Volume 943/1995.&lt;/ref&gt;

== History ==
The need for semantic data models was first recognized by the U.S. Air Force in the mid-1970s as a result of the [[Integrated Computer-Aided Manufacturing]] (ICAM) Program. The objective of this program was to increase manufacturing productivity through the systematic application of computer technology. The ICAM Program identified a need for better analysis and communication techniques for people involved in improving manufacturing productivity. As a result, the ICAM Program developed a series of techniques known as the IDEF (ICAM Definition) Methods which included the following:&lt;ref name ="FIPS184"/&gt;
* [[IDEF0]] used to produce a “function model” which is a structured representation of the activities or processes within the environment or system.
* [[IDEF1]] used to produce an “information model” which represents the structure and semantics of information within the environment or system.
** [[IDEF1X]] is a semantic data modeling technique. It is used to produce a graphical information model which represents the structure and semantics of information within an environment or system. Use of this standard permits the construction of semantic data models which may serve to support the management of data as a resource, the integration of information systems, and the building of computer databases.
* [[IDEF2]] used to produce a “dynamics model” which represents the time varying behavioral characteristics of the environment or system.

During the 1990s the application of semantic modelling techniques resulted in the semantic data models of the second kind. An example of such is the semantic data model that is standardised as [[ISO 15926]]-2 (2002), which is further developed into the semantic modelling language [[Gellish]] (2005). The definition of the Gellish language is documented in the form of a semantic data model. Gellish itself is a semantic modelling language, that can be used to create other semantic models. Those semantic models can be stored in Gellish Databases, being semantic databases.

== Applications ==
A semantic data model can be used to serve many purposes. Some key objectives include:&lt;ref name ="FIPS184"/&gt;
* Planning of Data Resources: A preliminary data model can be used to provide an overall view of the data required to run an enterprise. The model can then be analyzed to identify and scope projects to build shared data resources.
* Building of Shareable Databases: A fully developed model can be used to define an application independent view of data which can be validated by users and then transformed into a physical database design for any of the various DBMS technologies. In addition to generating databases which are consistent and shareable, development costs can be drastically reduced through data modeling.
* Evaluation of Vendor Software: Since a data model actually represents the infrastructure of an organization, vendor software can be evaluated against a company’s data model in order to identify possible inconsistencies between the infrastructure implied by the software and the way the company actually does business.
* Integration of Existing Databases: By defining the contents of existing databases with semantic data models, an integrated data definition can be derived. With the proper technology, the resulting conceptual schema can be used to control transaction processing in a distributed database environment. The U.S. Air Force Integrated Information Support System (I2S2) is an experimental development and demonstration of this type of technology applied to a heterogeneous DBMS environment.

== See also ==
* [[Conceptual schema]]
* [[Object-role modeling]]
* [[Entity-relationship model]]
* [[Information model]]
* [[Relational Model/Tasmania]]
* [[Three schema approach]]
* [[QuakeSim]]

== References ==
{{NIST-PD}}
{{reflist}}

== Further reading ==
* [http://hpdrc.cs.fiu.edu/library/books/datades-book/ Database Design - The Semantic Modelling Approach]
* Johan ter Bekke (1992). ''Semantic Data Modeling''. Prentice Hall.
* Alfonso F. Cardenas and Dennis McLeod (1990). ''Research Foundations in Object-Oriented and Semantic Database Systems''. Prentice Hall.
* Peter Gray, Krishnarao G. Kulkarni and, Norman W. Paton (1992). ''Object-Oriented Databases: A Semantic Data Model Approach''. Prentice-Hall International Series in Computer Science.
* Michael Hammer and Dennis McLeod (1978). "The Semantic Data Model: a Modeling Mechanism for Data Base Applications." In: ''Proc. ACM SIGMOD Int’l. Conf. on Management of Data''. Austin, Texas, May 31 - June 2, 1978, pp.&amp;nbsp;26–36.

== External links ==
* [http://www.jhterbekke.net/SemanticDataModeling.html Semantic Data Modeling] Johan ter Bekke tribute site.

{{Data model}}

{{DEFAULTSORT:Semantic Data Model}}
[[Category:Data modeling]]
[[Category:Systems analysis]]
[[Category:Knowledge representation]]</text>
      <sha1>hsepseqn59qia362hscd2phj4wf8az5</sha1>
    </revision>
  </page>
  <page>
    <title>AGROVOC</title>
    <ns>0</ns>
    <id>5465574</id>
    <revision>
      <id>697217849</id>
      <parentid>656596890</parentid>
      <timestamp>2015-12-29T01:40:04Z</timestamp>
      <contributor>
        <username>CaliViking</username>
        <id>11006619</id>
      </contributor>
      <comment>/* External links */  - Fixed broken link to VoCBench</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9442" xml:space="preserve">'''AGROVOC''' (a portmanteau of agriculture and vocabulary) is a multilingual controlled vocabulary covering all areas of interest to the [[Food and Agriculture Organization of the United Nations]] (FAO), including food, nutrition, agriculture, fisheries, forestry and the environment. The vocabulary consists of over 32,000 concepts with up to 40,000 terms in 23 languages: Arabic, Chinese, Czech, English, French, German, Hindi, Hungarian, Italian, Japanese, Korean, Lao, Malay, Persian, Polish, Portuguese, Russian, Slovak, Spanish, Telugu, Thai, Turkish and Ukrainian. It is a collaborative effort, edited by a community of experts and coordinated by FAO.

AGROVOC is made available by FAO as an [[Resource Description Framework|RDF]]/[[SKOS]]-XL concept scheme and published as a [[linked data]] set aligned to 16 other vocabularies.

==History==
AGROVOC was first published at the beginning of the 1980s by FAO in English, Spanish and French to serve as a controlled vocabulary to index publications in agricultural science and technology, especially for [[AGRIS]].

In the 1990s, AGROVOC abandoned paper printing and went digital with data storage handled by a relational database. In 2004, preliminary experiments with expressing AGROVOC into the [[Web Ontology Language]](OWL) took place. At the same time a web based editing tool was developed, then called WorkBench, nowadays VocBench. In 2009 AGROVOC became an SKOS resource.

Today, AGROVOC is available in 23 languages as an SKOS-XL concept scheme and published as a Linked Open Data (LOD) set aligned to 16 other data sets related to agriculture.

==Users==
AGROVOC is used by researchers, librarians and information managers for indexing, retrieving and organizing data in agricultural information systems and web pages.&lt;ref&gt;[http://aims.fao.org/standards/agrovoc/uses-agrovoc AGROVOC Uses]&lt;/ref&gt; Within the context of the [[Semantic Web]] also new users are emerging, like software developers and ontology builders.

==Access==
AGROVOC is accessible in various ways:

* Online: Search &lt;ref&gt;[http://aims.fao.org/standards/agrovoc/functionalities/search AGROVOC search]&lt;/ref&gt; and browse AGROVOC on the [[Agricultural Information Management Standards]] (AIMS) website. 
* Download: RDF-SKOS (AGROVOC only or AGROVOC LOD).&lt;ref&gt;[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC Releases]&lt;/ref&gt; 
* Live: SPARQL endpoint &lt;ref&gt;[http://202.45.139.84:10035/catalogs/fao/repositories/agrovoc AGROVOC SPARQL endpoint]&lt;/ref&gt; and the AGROVOC Web services.&lt;ref&gt;[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC releases]&lt;/ref&gt;

==Maintenance==
The AGROVOC team, located at FAO Headquarter, coordinates the editorial activities related to the maintenance of AGROVOC. The actual maintenance is carried out by a community of editors and institutions&lt;ref&gt;[http://aims.fao.org/standards/agrovoc/community AGROVOC people]&lt;/ref&gt; for each of the 23 language versions.

The tool used by the community to edit and maintain AGROVOC is Vocbench, which was designed to meet the needs of the Semantic Web and linked data environments. VocBench provides tools and functionalities that facilitate both collaborative editing and multilingual terminology. It also includes administration and group management features that permit flexible roles for maintenance, validation and quality assurance.

FAO also facilitates the technical maintenance of AGROVOC, including its publication as a LOD resource. Technical support is provided by the University of Tor Vergata&lt;ref&gt;[http://art.uniroma2.it/ Tor Vergata University]&lt;/ref&gt; (Rome, Italy) which leads the technical development of VocBench. The technical infrastructure for the online publication of AGROVOC is hosted by MIMOS Berhad&lt;ref&gt;[http://www.mimos.my/ MIMOS Berhad]&lt;/ref&gt; (Kuala Lumpur, Malaysia).

==Structure==
All 32,000+ concepts of the AGROVOC thesaurus are hierarchically organized under 25 top concepts. AGROVOC top concepts are very general and high level concepts, like “activities”, “organisms”, “location”, “products” etc. More than half of the total number of concepts (20,000+) fall under the top concept “organism”, which confirms how AGROVOC is largely oriented towards the agricultural sector.

AGROVOC is an RDF/SKOS-XL concept scheme, meaning the conceptual and terminological level are separated. The basic notions for such a concept scheme are: concepts, their labels and relations.

*'''Concepts''' 
Concepts are anything we want to represent or “talk about” in our domain. Concepts are represented by terms. A concept could also be considered as the set of all terms used to express it in various languages.
In SKOS, concepts are formalized as skos:Concept, identified by dereferenceable URIs (= URL). For example, the AGROVOC concept with URI http://aims.fao.org/aos/agrovoc/c_12332 is for ''maize''.

*'''Terms''' 
Terms are the actual terms used to name a concept. For example ''maize'', ''maïs'', ''玉米'', ''ข้าวโพด'' are all terms used to refer to the same concept in English, French, Chinese and Hindi respectively.

AGROVOC terms are expressed by means of the SKOS extension for labels, SKOS-XL. The predicates used are:
skosxl:prefLabel, used for preferred terms (“descriptors” in thesaurus terminology), and 
skosxl:altLabel, used for non- preferred terms.

*'''Relations'''
In SKOS, hierarchical relations between concepts are expressed by the predicates skos:broader, skos:narrower. They correspond to the classical thesaurus relations broader/narrower (BT/NT).

Non-hierarchical relations express a notion of “relatedness” between concepts. AGROVOC uses the SKOS relation skos:related (corresponding to the classical thesaurus RT), and a specific vocabulary of relations called Agrontology.&lt;ref&gt;[https://aims-fao.atlassian.net/wiki/display/AGV/Agrontology Agrontology]&lt;/ref&gt;

AGROVOC also allows for relations between labels (i.e. terms), thanks to the SKOS-XL extension to SKOS.

==Linked data==
AGROVOC is available as a linked data set and is aligned (linked) with 16 vocabularies related to agriculture (see table down below). The linked data version of AGROVOC is exposed as RDF and HTML, through a content-negotiation mechanism. It is also exposed through a SPARQL endpoint.

The advantage of having a thesaurus like AGROVOC published as LOD is that once thesauri are linked, the resources they index are linked as well. A good example is AGRIS, a mash-up web application that links the AGRIS bibliographic repository (indexed with AGROVOC) to related web resources (indexed with vocabularies linked to AGROVOC).

{| class="wikitable"
|-
! Resource !! Topics !! Linked concpets !! Languages !! Linked data !! Type of link
|-
| ASFA || Fisheries|| 1784|| || || skos:closeMatch
|-
| FAO Biotechnology Glossary || Biotechnologies|| 810|| EN, ES, FR, +3 more|| Yes|| skos:closeMatch
|-
| Chinese Agriculture Thesaurus (CAT)|| Agriculture|| || || Yes|| skos:closeMatch
|-
| EARTh|| Environment|| 1363 || EN+|| Yes|| skos:closeMatch
|-
| EUROVOC|| General EU || 1,297 || EN, ES, FR + 21 more || Yes || skos:exactMatch
|-
| GEMET || Environment || 1,191 || EN, ES, FR + 30 more || Yes|| skos:exactMatch
|-
| Library of Congress Subject Headings (LCSH)|| General || 1,093 || EN || Yes || skos:exactMatch
|-
| NAL Thesaurus || Agriculture || 13,390 || EN,ES || Yes || skos:exactMatch
|-
| RAMEAU Répertoire d'autorité-matière encyclopedique et alphabetique unifie  || General || 686 || FR || Yes || skos:exactMatch
|-
| STW - Thesaurus for Economics || Economy || 1,136 || EN, DE || Yes || skos:exactMatch
|-
| TheSoz - Thesaurus for the Social Sciences || Social sciences || 846 || EN,DE || Yes || skos:exactMatch
|-
| Geopolical Ontology || Geopolitical entities || 253 || AR, CH, EN, ES, FR, RU || Yes || skos:exactMatch
|-
| Dewey Decimal Classification (DDC) || General || 409 || EN, ES, FR + 8 more || Yes || skos:exactMatch
|-
| DBpedia || General || 10,989 || EN, ES, FR + 8 more || Yes || skos:exactMatch
skos:closeMatch
|-
| SWD (Schlagwortnormdatei)|| General || 6,245 || DE || Yes || skos:exactMatch
skos:closeMatch
skos:broadMatch
skos:narrowMatch
|-
| GeoNames || Geographical entities || 212 || EN, ES, FR + 63 more || Yes || skos:exactMatch
|}

==Copyright and license==
The copyright for the AGROVOC thesaurus content in English, French, Russian and Spanish stays with FAO and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.&lt;ref&gt;[http://creativecommons.org/licenses/by-sa/3.0/ Creative Commons Attribution ShareAlike 3.0 Unported]&lt;/ref&gt; For any other language, the copyright rests with the institution responsible for its production.

==Related links==
* [[Agricultural Information Management Standards]]
* [[AGRIS]]
* [[Food and Agriculture Organization]]
* [[Geopolitical ontology]]

==External links==
* [http://agris.fao.org/ AGRIS]
* [http://aims.fao.org/standards/agrovoc AGROVOC]
* [http://aims.fao.org/ AIMS]
* [http://www.fao.org/home/en/ FAO]
* [https://www.w3.org/2001/sw/wiki/VocBench VocBench/Agricultural Ontology Server]

==Further reading==
* [http://aims.fao.org/standards/agrovoc/publications AGROVOC Publications]

==References==
{{reflist}}

{{DEFAULTSORT:Agrovoc}}
[[Category:Agricultural databases]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Food and Agriculture Organization]]
[[Category:Thesauri]]</text>
      <sha1>4ptfmztxv5ejhnkh4rrnc6ykmwvvcxt</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic parameterization</title>
    <ns>0</ns>
    <id>18922270</id>
    <revision>
      <id>735037147</id>
      <parentid>735036985</parentid>
      <timestamp>2016-08-18T07:16:30Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor />
      <comment>/* Introduction with Example */ ''p''</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6435" xml:space="preserve">'''Semantic parameterization''' is a conceptual modeling process for expressing natural language descriptions of a domain in first-order predicate logic.&lt;ref&gt;Travis D. Breaux and Annie I. Antón (2004). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2004-36.pdf Deriving Semantic Models from Privacy Policies]. North Carolina State University Computer Science Technical Report TR-2004-36.&lt;/ref&gt;&lt;ref&gt;Travis D. Breaux and Annie I. Antón (2008). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2005-31.pdf "Mining Rule Semantics to Understand Legislative Compliance"]. North Carolina State University Computer Science Technical Report TR-2005-31.&lt;/ref&gt;&lt;ref name="Breaux"&gt;T.D. Breaux, A.I. Anton, J. Doyle, [http://www4.ncsu.edu/~tdbreaux/publications/tdbreaux-tosem09.pdf "Semantic parameterization: a process for modeling domain descriptions"], ''ACM Transactions on Software Engineering Methodology'', vol. 18, no. 2, Article 5, 2008.&lt;/ref&gt; The process yields a formalization of natural language sentences in [[Description Logic]] to answer the ''who,'' ''what'' and ''where'' questions in the Inquiry-Cycle Model (ICM) developed by Colin Potts and his colleagues at the Georgia Institute of Technology.&lt;ref name="Potts"&gt;C. Potts, K. Takahashi, and A.I. Anton, "Inquiry-based requirements analysis", ''IEEE Software'' 11(2): 21–32, 1994.&lt;/ref&gt; The parameterization process complements the Knowledge Acquisition and autOmated Specification (KAOS) method,&lt;ref&gt;A. Dardenne, A. van Lamsweerde and S. Fickas, "Goal-Directed Requirements Acquisition", ''Science of Computer Programming'' v. 20, North Holland, 1993, pp. 3-50.&lt;/ref&gt; which formalizes answers to the ''when'', ''why'' and ''how'' ICM questions in [[Temporal Logic]], to complete the ICM formalization. The artifacts used in the parameterization process include a dictionary that aligns the domain lexicon with unique concepts, distinguishing between [[synonyms]] and [[polysemes]], and several natural language patterns that aid in mapping common domain descriptions to formal specifications.

== Relationship to other theories ==

Semantic Parameterization defines a meta-model consisting of eight roles that are domain-independent and reusable. Seven of these roles correspond to Jeffrey Gruber's [[thematic relations]]&lt;ref&gt;J. Gruber, ''Lexical Structures in Syntax and Semantics'', North Holland, New York, 1976.&lt;/ref&gt; and [[case role]]s in Charles Fillmore's [[case grammar]]:&lt;ref&gt;C. Fillmore, "The Case for Case", ''Universals in Linguistic Theory'', Holt, Rhinehart and Winston, New York, 1968.&lt;/ref&gt;

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ Meta-model Mapping to Case Frames and Thematic Relations
! Breaux's Meta-model
! Fillmore's Case Roles
! Thematic Relations
|-
| Subject
| Agentive
| Agent
|-
| Action
|
|-
| Object
| Objective/ Factitive
| Theme/ Patient
|-
| Target
| Dative
| Goal
|-
| Source
| Source
| Source
|-
| Instrument
| Instrumental
| Instrument
|-
| Purpose
|
| Purposive
|-
| Location
| Locative
| Location
|-
|
| Comitative
| Accompaniment
|}

The Inquiry-Cycle Model (ICM) was introduced to drive elicitation between engineers and stakeholders in requirements engineering.&lt;ref name="Potts" /&gt; The ICM consists of ''who'', ''what'', ''where'', ''why'', ''how'' and ''when'' questions. All but the ''when'' questions, which require a [[Temporal Logic]] to represent such phenomena, have been aligned with the meta-model in semantic parameterization using [[Description Logic]] (DL).

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ Mapping from DL roles to questions in the Inquiry-Cycle Model
! DL Role in Meta-model
! ICM Question
|-
| isSubjectOf.Activity
| Who performs the action?
|-
| isObjectOf.Activity
| Upon what is the action performed?
|-
| isTargetOf.Activity
| With whom is the transaction performed?
|-
| isPurposeOf.Activity
| Why is the action performed?
|-
| isInstrumentOf.Activity
| How is the action performed?
|-
| isLocationOf.Activity
| Where is the action performed?
|}

== Introduction with Example ==

The semantic parameterization process is based on [[Description Logic]], wherein the TBox is composed of words in a ''dictionary'', including nouns, verbs, and adjectives, and the ABox is partitioned into two sets of assertions: 1) those assertions that come from words in the natural language statement, called the ''grounding'', and 2) those assertions that are inferred by the (human) modeler, called the ''meta-model''. Consider the following unstructured natural language statement (UNLS) (see Breaux et al.&lt;ref name="Breaux" /&gt; for an extended discussion):

;UNLS&lt;sub&gt;1.0&lt;/sub&gt;: The customer&lt;sub&gt;1,1&lt;/sub&gt; must not share&lt;sub&gt;2,2&lt;/sub&gt; the access-code&lt;sub&gt;3,3&lt;/sub&gt; of the customer&lt;sub&gt;1,1&lt;/sub&gt; with someone&lt;sub&gt;4,4&lt;/sub&gt; who is not the provider&lt;sub&gt;5,4&lt;/sub&gt;.

The modeler first identifies intensional and extensional polysemes and synonyms, denoted by the subscripts: the first subscript uniquely refers to the intensional index, i.e., the same first index in two or more words refer to the same concept in the TBox; the second subscript uniquely refers to the extensional index, i.e., two same second index in two or more words refer to the same individual in the ABox. This indexing step aligns words in the statement and concepts in the dictionary. Next, the modeler identifies concepts from the dictionary to compose the meta-model. The following table illustrates the complete DL expression that results from applying semantic paramterization.

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ The grounding {{mvar|G}} and meta-model {{mvar|M}} derived from UNLS&lt;sub&gt;1.0&lt;/sub&gt;
! Grounding ({{mvar|G}})
! Meta-model ({{mvar|M}})
|-valign="top"
| {{math|Customer(''p''&lt;sub&gt;1&lt;/sub&gt;) &lt;br /&gt;
⨅ Share(''p''&lt;sub&gt;2&lt;/sub&gt;) &lt;br /&gt;
⨅ isAccessCodeOf(''p''&lt;sub&gt;3&lt;/sub&gt;, ''p''&lt;sub&gt;1&lt;/sub&gt;) &lt;br /&gt;
⨅ Someone(''p''&lt;sub&gt;4&lt;/sub&gt;) &lt;br /&gt;
⨅ Provider(''p''&lt;sub&gt;4&lt;/sub&gt;)}}
| {{math|Activity(''p''&lt;sub&gt;5&lt;/sub&gt;) &lt;br /&gt;
⨅ hasSubject(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;1&lt;/sub&gt;) &lt;br /&gt;
⨅ hasAction(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;2&lt;/sub&gt;) &lt;br /&gt;
⨅ hasObject(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;3&lt;/sub&gt;) &lt;br /&gt;
⨅ hasTarget(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;4&lt;/sub&gt;) &lt;br /&gt;
⨅ isRefrainmentOf(''p''&lt;sub&gt;5&lt;/sub&gt;, ''p''&lt;sub&gt;1&lt;/sub&gt;)}}
|}

== References ==
{{Reflist}}

{{DEFAULTSORT:Semantic Parameterization}}
[[Category:Knowledge representation]]</text>
      <sha1>is7613ml77t7zuh1vi3pkj8giuuplzr</sha1>
    </revision>
  </page>
  <page>
    <title>AgMES</title>
    <ns>0</ns>
    <id>5465589</id>
    <revision>
      <id>742711864</id>
      <parentid>727374110</parentid>
      <timestamp>2016-10-05T09:19:19Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7951" xml:space="preserve">{{Refimprove|date=January 2015}}
The '''AgMES''' (Agricultural Metadata Element set) initiative was developed by the [[Food and Agriculture Organization]] (FAO) of the [[United Nations]] and aims to encompass issues of semantic standards in the domain of agriculture with respect to description, resource discovery, interoperability and data exchange for different types of information resources.

There are numerous other metadata schemas for different types of information resources. The following list contains a list of a few examples:

* Document-like Information Objects (DLIOs): [[Dublin Core]], Agricultural Metadata Element Set (AgMES)
* Events: [[VCalendar]]
* Geographic and Regional Information: Geographic information—Metadata ISO/IEC 11179 Standards&lt;ref&gt;{{cite web|url=http://isotc.iso.org/livelink/livelink/fetch/2000/2489/Ittf_Home/PubliclyAvailableStandards.htm |title=Freely Available Standards |publisher=Isotc.iso.org |date= |accessdate=2013-07-10}}&lt;/ref&gt;
* Persons: [[FOAF (software)|Friend-of-a-friend]] (FOAF), [[vCard]]
* Plant Production and Protection: Darwin Core (1.0 and 2.0) (DwC)

AgMES as a namespace is designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]], AGLS&lt;ref&gt;http://www.naa.gov.au/recordkeeping/gov_online/agls/summary.html&lt;/ref&gt; etc. Thus to be used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc., it will have to be used in conjunction with the standard namespaces mentioned before.  The AgMES initiative strives to achieve improved interoperability between information resources in agricultural domain by enabling means for exchange of information.

Describing a DLIO with AgMES means exposing its major characteristics and contents in a standard way that can be reused easily in any information system. The more institutions and organizations in the agricultural domain that use AgMES to describe their DLIOs, the easier it will be to interchange data in between information systems like digital libraries and other repositories of agricultural information.

== Use of AgMES ==
Metadata on agricultural Document-like Information Objects (DLIOs) can be created and stored in various formats:
* embedded in a web site (in the manor as with the HTML meta tag)
* in a separate metadata database
* in an XML file
* in an RDF file

AgMES defines elements that can be used to describe a DLIO that can be used together with other metadata standards such as the Dublin Core, the Australian Government Locator Service. A complete list of all elements, refinements and schemes endorsed by AgMES is available from the AgMES website.&lt;ref&gt;{{cite web|url=http://aims.fao.org/standards/agmes/namespace-specification |title=AgMES 1.1 Namespace Specification &amp;#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt;

=== Creating application profiles ===
[[Application profile]]s are defined as schemas which consist of data elements drawn from one or more namespaces, combined by implementers, and optimized for a particular local application. Application profiles share the following four characteristics:
* They draw upon existing pool of metadata definition standards to extract suitable application- or requirement oriented elements.
* An application profile cannot create new elements.
* Application profiles specify the application specific details such as the schemes or controlled vocabularies. An application profile also contains information such as the format for the element value, cardinality or [[data type]].
* Lastly, an application profile can refine standardized definitions as long as it is "semantically narrower or more specific". This capability of application profiles caters to situations where a domain specific terminology is needed to replace a more general one.

=== Sample application profiles using AgMES ===
* The AGRIS Application Profile&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt; is a standard created specifically to enhance the description, exchange and subsequent retrieval of agricultural Document-like Information Objects (DLIOs). It is a format that allows sharing of information across dispersed bibliographic systems and is based on well-known and accepted metadata standards.
* The Event Application Profile&lt;ref&gt;{{cite web|url=http://www.fao.org/aims/ap_applied.jsp |title=Agricultural Information Management Standards (AIMS) &amp;#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt; is a standard created to allow members of the Agricultural community to 'know' about an upcoming event and guide them to the event Web site where they can find further information. The information communicated is thus minimum yet interoperable across domains and organizations.

== AgMES and the semantic web ==

One of the advantages of the AgMES metadata schema is the ability to link between the [[Data element|metadata element]] and [[Controlled vocabulary|controlled vocabularies]]. The use of controlled vocabulary provides a "known" set of options to the indexer (and the search programmer) as to how the field can be filled out. Often the values may come from a specific thesaurus (e.g. [[AGROVOC]]) or classification schemes (e.g. the AGRIS/CARIS classification scheme) etc.&lt;ref&gt;{{cite web|url=http://www.fao.org/aims/index_en.jsp?callingPage=ag_classifschemes.jsp |title=Agricultural Information Management Standards (AIMS) &amp;#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt;

Thanks to the possibility to use controlled vocabularies for metadata elements, the user is provided with the most precise information. In this context, work is also being carried out on exploiting the power of controlled vocabularies expressed as using URIs and machine-understandable semantics.  In this context, FAO is promoting the [[Agricultural Ontology Service]] (AOS) initiative with the objective of expressing more semantics within the traditional thesaurus AGROVOC and build a Concept Server&lt;ref&gt;{{cite web|url=http://www.fao.org/aims/cs.htm |title=Agricultural Information Management Standards (AIMS) &amp;#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt; as a repository from which it will be always possible to extract traditional KOS.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AGRIS]]
* [[AGROVOC]]

==References==
{{Reflist}}

== External links ==
* {{Official website|https://web.archive.org/web/20060519100855/http://www.fao.org:80/aims/agmes_intro.jsp }}
* [http://dublincore.org/ Dublin Core Metadata Initiative]
* [http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]
* [http://xml.coverpages.org/ni2003-05-12-a.html FAO's AgMES Project Releases a New Application Profile for Encoding Metadata.] (''Cover Pages'', May 2003)
* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&amp;SERIES=339 Agricultural Information and Knowledge Management Papers]
* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&amp;status=10&amp;dateto=31/12/2006&amp;lang=en&amp;sites=1 RSS feed of news and events]
* [https://web.archive.org/web/20060617132639/http://www.dgroups.org:80/groups/fao/agstandards/index.cfm?op=dsp_join Agstandards Discussion List]: This is a forum established for discussing metadata standards and the development of multilingual thesauri and ontologies.

{{DEFAULTSORT:Agmes}}
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Semantic Web]]
[[Category:Food and Agriculture Organization]]</text>
      <sha1>ro6m4aty4xfhnleky7v4668ff9hqb0t</sha1>
    </revision>
  </page>
  <page>
    <title>Mental mapping</title>
    <ns>0</ns>
    <id>8090717</id>
    <revision>
      <id>756921340</id>
      <parentid>739537802</parentid>
      <timestamp>2016-12-27T17:40:40Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor />
      <comment>/* Background */ cite repair;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9052" xml:space="preserve">{{about|the geographical concept|the diagram|Mind map}}
In [[behavioral geography]], a '''mental map''' is a person's [[Perspective (cognitive)|point-of-view]] perception of their area of interaction. Although this kind of subject matter would seem most likely to be studied by fields in the [[social sciences]], this particular subject is most often studied by modern day [[geographers]]. They study it to determine [[Subjectivity|subjective]] qualities from the public such as personal preference and practical uses of geography like driving directions. [[Mass media]] also have a virtually direct effect on a person's mental map of the geographical world.&lt;ref&gt;[http://mentalmaps.info Mental Maps Resource Site]&lt;/ref&gt; The perceived geographical dimensions of a foreign nation (relative to one's own nation) may often be heavily influenced by the amount of time and relative news coverage that the news media may spend covering news events from that foreign region. For instance, a person might perceive a small island to be nearly the size of a continent, merely based on the amount of news coverage that he or she is exposed to on a regular basis.&lt;ref&gt;[http://geography.about.com/cs/culturalgeography/a/mentalmaps.htm Mental Maps on About.com]&lt;/ref&gt;

In [[Experimental psychology|psychology]], the term names the information maintained in the mind of an organism by means of which it may plan activities, select routes over previously traveled territories, etc. The rapid traversal of a familiar [[maze]] depends on this kind of mental map if scents or other markers laid down by the subject are eliminated before the maze is re-run.

==Background==
Mental maps are an outcome of the field of behavioral geography. The imagined maps are considered one of the first studies that intersected geographical settings with human action.&lt;ref name="Gregory"&gt;{{cite book|last=Gregory|first=Derek|title=Dictionary of Human Geography: Mental maps/Cognitive Maps|year=2009|publisher=Wiley-Blackwell|location=Hoboken|edition=5th |author2=Johnston, Rom |author3=Pratt, Geraldine |page=455}}&lt;/ref&gt;  The most prominent contribution and study of mental maps was in the writings of [[Kevin A. Lynch|Kevin Lynch]]. In ''[[The Image of the City]]'', Lynch used simple sketches of maps created from memory of an urban area to reveal five elements of the city; nodes, edges, districts, paths and landmarks.&lt;ref&gt;{{cite book|last=Lynch|first=Kevin|title=The Image of the City|year=1960|publisher=MIT Press|location=Cambridge MA}}&lt;/ref&gt;  Lynch claimed that “Most often our perception of the city is not sustained, but rather partial, fragmentary, mixed with other concerns. Nearly every sense is in operation, and the image is the composite of them all.” (Lynch, 1960, p 2.) The creation of a mental map relies on memory as opposed to being copied from a preexisting map or image. In ''The Image of the City'', Lynch asks a participant to create a map as follows: “Make it just as if you were making a rapid description of the city to a stranger, covering all the main features. We don’t expect an accurate drawing- just a rough sketch.” (Lynch 1960, p 141) In the field of human geography mental maps have led to an emphasizing of social factors and the use of social methods versus quantitative or positivist methods.&lt;ref name="Gregory" /&gt; Mental maps have often led to revelations regarding social conditions of a particular space or area. Haken and Portugali (2003) developed an information view, which 
argued that the face of the city is its information &lt;ref&gt;{{cite journal|last=Haken|first=Herman|author2=Portugali, Juval|title=The face of the city is its information|journal=Journal of Environmental Psychology|date=August 2003|volume=23|pages=385–408|doi=10.1016/s0272-4944(03)00003-3}}&lt;/ref&gt;
. Bin Jiang (2012) argued that the image of the city (or mental map) arises out of the scaling of city artifacts and locations.&lt;ref&gt;{{cite journal|last=Jiang|first=Bin|title=The image of the city out of the underlying scaling of city artifacts or locations|journal=Annals of the Association of American Geographers|year=2012|volume=103|pages= 1552–1566| doi = 10.1080/00045608.2013.779503 |arxiv=1209.1112}}&lt;/ref&gt; He addressed that why the image of city can be formed 
&lt;ref&gt;{{cite journal|last=Jiang|first=Bin|title=Why can the image of the city be formed| arxiv=1212.3703}}&lt;/ref&gt;
, and he even suggested ways of computing the image of the city, or more precisely the kind of collective image of the city, using increasingly available geographic information such as Flickr and Twitter
&lt;ref&gt;{{cite journal|last=Jiang|first=Bin|title=Computing the image of the city| arxiv=1212.0940}}&lt;/ref&gt;
.

==Research applications==
Mental maps have been used in a collection of spatial research. Many studies have been performed that focus on the quality of an environment in terms of feelings such as fear, desire and stress. A study by Matei et al. in 2001 used mental maps to reveal the role of media in shaping urban space in Los Angeles. The study used Geographic Information Systems (GIS) to process 215 mental maps taken from seven neighborhoods across the city. The results showed that people’s fear perceptions in Los Angeles are not associated with high crime rates but are instead associated with a concentration of certain ethnicities in a given area.&lt;ref&gt;{{cite journal|last=Matei|first=Sorin |author2=Ball-Rokeach, Sandra |author3=Qiu Linchuan, Jack|title=Fear and Misperception of Los Angeles Urban Space: A Spatial-Statistical Study of Communication-Shaped Mental Maps|journal=Communication Research|date=August 2001|volume=28|issue=4|pages=429–463|accessdate=4 November 2012|url=http://mentalmap.org/files/matei_fear_CR.pdf|doi=10.1177/009365001028004004}}&lt;/ref&gt;  The mental maps recorded in the study draw attention to these areas of concentrated ethnicities as parts of the urban space to avoid or stay away from. 
	
Mental maps have also been used to describe the urban experience of children. In a 2008 study by Olga den Besten mental maps were used to map out the fears and dislikes of children in Berlin and Paris. The study looked into the absence of children in today’s cities and the urban environment from a child’s perspective of safety, stress and fear.&lt;ref&gt;{{cite journal|last=Den Besten|first=Olga den|title=Local belonging and ‘geographies of emotions’: Immigrant children’s experience of their neighbourhoods in Paris and Berlin|journal=Childhood|date=May 2010|volume=17|issue=2|pages=181–195|url=http://chd.sagepub.com/content/17/2/181|accessdate=4 November 2012|doi=10.1177/0907568210365649}}&lt;/ref&gt;

Peter Gould and Rodney White have performed prominent analyses in the book “Mental Maps.” The book is an investigation into people’s spatial desires. The book asks of its participants: “Suppose you were suddenly given the chance to choose where you would like to live- an entirely free choice that you could make quite independently of the usual constraints of income or job availability. Where would you choose to go?” (Gould, 1974, p 15) Gould and White use their findings to create a surface of desire for various areas of the world. The surface of desire is meant to show people’s environmental preferences and regional biases.&lt;ref&gt;{{cite book|last=Gould|first=Peter|title=Mental Maps|year=1993|publisher=Rutledge|location=New York|author2=White, Rodney|page=93}}&lt;/ref&gt;

In an experiment done by [[Edward C. Tolman]], the development of a mental map was seen in rats.&lt;ref&gt;Goldstein, B. (2011). ''Cognitive Psychology: Connecting Mind, Research, and Everyday Experience--with coglab manual. (3rd ed.).'' Belmont, CA: Wadsworth.&lt;/ref&gt; A rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial mental map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.

The idea of mental maps is also used in strategic analysis. David Brewster, an Australian strategic analyst, has applied the concept to strategic conceptions of South Asia and Southeast Asia.  He argues that popular mental maps of where regions begin and end can have a significant impact on the strategic behaviour of states.&lt;ref&gt;{{cite web|author=David Brewster|url=https:// www.academia.edu/7697999/Dividing_Lines_Evolving_Mental_Maps_of_the_Bay_of_Bengal|title=Dividing Lines: Evolving Mental Maps of the Bay of Bengal. Retrieved 21 September 2014}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Knowledge representation]]
[[Category:Cognitive psychology]]
[[Category:Psychology terminology]]</text>
      <sha1>8x393tlyfzwqzcihnthgol145mgvm6g</sha1>
    </revision>
  </page>
  <page>
    <title>General Architecture for Text Engineering</title>
    <ns>0</ns>
    <id>11270152</id>
    <revision>
      <id>755401228</id>
      <parentid>748380685</parentid>
      <timestamp>2016-12-17T20:31:33Z</timestamp>
      <contributor>
        <username>Catlemur</username>
        <id>13510414</id>
      </contributor>
      <comment>Filled in 8 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8648" xml:space="preserve">{{Infobox software
| name = GATE
| screenshot = [[Image:GATE5 main window.png|250px]]
| caption = GATE Developer v5 main window
| developer = [http://gate.ac.uk/people GATE research team], [http://www.dcs.shef.ac.uk/ Dept. Computer Science, University of Sheffield]
| released = {{start date and age |1995}}
| frequently_updated = yes&lt;!-- Release version update? Don't edit this page, just click on the version number! --&gt;
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| language = English
| genre = [[Text mining]] [[Information Extraction]]
| license = [[LGPL]]
| website = {{url|http://gate.ac.uk}}
}}
'''General Architecture for Text Engineering''' or '''GATE''' is a [[Java (programming language)|Java]] suite of tools originally developed at the [[University of Sheffield]] beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many [[natural language processing]] tasks, including [[information extraction]] in many languages.&lt;ref&gt;Languages mentioned on http://gate.ac.uk/gate/plugins/ include Arabic, Bulgarian, Cebuano, Chinese, French, German, Hindi, Italian, Romanian and Russian.&lt;/ref&gt;

GATE has been compared to [[NLTK]], [[R (programming language)|R]] and [[RapidMiner]].&lt;ref&gt;{{cite web|url=http://www.b-eye-network.com/view/9516|title=Open Source Text Analytics by Seth Grimes - BeyeNETWORK|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; As well as being widely used in its own right, it forms the basis of the KIM semantic platform.&lt;ref&gt;{{cite journal|url=https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitlekim-a-semantic-platform-for-information-extraction-and-retrievaldiv/7249CC61F5AB25CBC7AAE182509DFEDE|title=KIM – a semantic platform for information extraction and retrieval|first1=Borislav|last1=Popov|first2=Atanas|last2=Kiryakov|first3=Damyan|last3=Ognyanoff|first4=Dimitar|last4=Manov|first5=Angel|last5=Kirilov|date=1 September 2004|publisher=|volume=10|issue=3-4|pages=375–392|accessdate=17 December 2016|via=Cambridge Core|doi=10.1017/S135132490400347X}}&lt;/ref&gt;

GATE community and research has been involved in several European research projects including [[Transitioning Applications to Ontologies|TAO]], [[SEKT]], NeOn, Media-Campaign, Musing, [[Service-Finder]], LIRICS and [[KnowledgeWeb Project|KnowledgeWeb]], as well as many other projects.

As of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from [[SourceForge]] are recorded since the project moved to SourceForge in 2005.&lt;ref&gt;{{cite web|url=http://sourceforge.net/projects/gate/|title=GATE|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; The paper "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications"&lt;ref&gt;[http://gate.ac.uk/sale/acl02/acl-main.pdf "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications", by Cunningham H., Maynard D., Bontcheva K. and Tablan V. (In proc. of the 40th Anniversary Meeting of the Association for Computational Linguistics, 2002)]&lt;/ref&gt; has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide,&lt;ref&gt;{{cite web|url=http://gate.ac.uk/userguide/|title=GATE.ac.uk  - sale/tao/split.html|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; include "Building Search Applications: Lucene, LingPipe, and Gate", by Manu Konchady,&lt;ref&gt;Konchady, Manu. [https://books.google.com/books?id=mcM-OAAACAAJ&amp;dq=Building+Search+Applications:+Lucene,+LingPipe,+and+Gate&amp;hl=en&amp;ei=avbDTczPJITqrQfk1IXQBA&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CDEQ6AEwAA Building Search Applications: Lucene, LingPipe, and Gate]. Mustru Publishing. 2008.&lt;/ref&gt; and "Introduction to Linguistic Annotation and Text Analytics", by Graham Wilcock.&lt;ref&gt;{{cite web|url=https://books.google.com/books?id=TDQJb1UgVywC&amp;dq=Introduction%20to%20Linguistic%20Annotation%20and%20Text%20Analytics&amp;printsec=frontcover&amp;source=bl&amp;ots=bAF26ZQSTx&amp;sig=TbxZ_-3tRy3IeDBKFofeVN6bAIc&amp;hl=en&amp;ei=vc0gS7PlLo-64QaSgqnfCQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=2&amp;ved=0CBcQ6AEwAQ#v=onepage&amp;q=&amp;f=false|title=Introduction to Linguistic Annotation and Text Analytics|first=Graham|last=Wilcock|date=1 January 2009|publisher=Morgan &amp; Claypool Publishers|accessdate=17 December 2016|via=Google Books}}&lt;/ref&gt;

== Features ==

GATE includes an [[information extraction]] system called '''ANNIE''' ('''A Nearly-New Information Extraction System''') which is a set of modules comprising a [[Lexical analysis|tokenizer]], a [[gazetteer]], a [[Sentence boundary disambiguation|sentence splitter]], a [[Part-of-speech tagging|part of speech tagger]], a [[Named entity recognition|named entities]] transducer and a [[coreference]] tagger. ANNIE can be used as-is to provide basic [[information extraction]] functionality, or provide a starting point for more specific tasks.

Languages currently handled in GATE include [[English language|English]], [[Mandarin Chinese|Chinese]], [[Arabic]], [[Bulgarian language|Bulgarian]], [[French language|French]], [[German language|German]], [[Hindi]], [[Italian language|Italian]], [[Cebuano language|Cebuano]], [[Romanian language|Romanian]], [[Russian language|Russian]], [[Danish language|Danish]].

Plugins are included for [[machine learning]] with [[Weka (machine learning)|Weka]], RASP, MAXENT, SVM Light, as well as a [[LIBSVM]] integration and an in-house [[perceptron]] implementation, for managing [[Ontology (information science)|ontologies]] like [[WordNet]], for querying [[search engines]] like [[Google]] or [[Yahoo]], for [[part of speech tagging]] with [[Brill tagger|Brill]] or TreeTagger, and many more. Many external plugins are also available, for handling e.g. [[Twitter|tweets]].&lt;ref&gt;{{cite web|url=https://gate.ac.uk/wiki/twitie.html|title=GATE.ac.uk  - wiki/twitie.html|publisher=|accessdate=17 December 2016}}&lt;/ref&gt;

GATE accepts input in various formats, such as [[Text file|TXT]], [[HTML]], [[XML]], [[DOC (computing)|Doc]], [[PDF]] documents, and [[Serialization|Java Serial]], [[PostgreSQL]], [[Lucene]], [[Oracle database|Oracle]] Databases with help of [[RDBMS]] storage over [[JDBC]].

[[JAPE (linguistics)|JAPE]] transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide.&lt;ref&gt;{{cite web|url=http://gate.ac.uk/userguide/chap:jape|title=GATE.ac.uk  - sale/tao/splitch8.html|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; A tutorial has also been written by Press Association Images.&lt;ref&gt;{{cite web|url=http://realizingsemanticweb.blogspot.com/2009/07/jape-grammar-tutorial.html|title=Realizing Semantic Web: JAPE grammar tutorial|first=Dhavalkumar|last=Thakker|date=17 July 2009|publisher=|accessdate=17 December 2016}}&lt;/ref&gt;

== GATE Developer ==

[[Image:GATE5 main window.png|thumb|400px|GATE 5 main window.]]

The screenshot shows the document viewer used to display a document and its annotations. In pink are &lt;A&gt; hyperlink annotations from an [[Hypertext Markup Language|HTML]] file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.

== GATE Mímir ==
&lt;!-- re-written to remove any lingering copyright worries --&gt;
 Generate vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and [[SPARQL]].

==See also==
{{Portal|Free software}}
* [[Unstructured Information Management Architecture]] (UIMA)
* [[OpenNLP]]
* [[List of natural language processing toolkits]]
* [[Pheme (project)|Pheme]], a major EU project managed by the GATE group on early detection of false information in social media

==References==
&lt;references/&gt;

{{DEFAULTSORT:General Architecture For Text Engineering}}
[[Category:Data mining and machine learning software]]
[[Category:Free computer libraries]]
[[Category:Free science software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free integrated development environments]]
[[Category:Knowledge representation]]
[[Category:Natural language processing toolkits]]
[[Category:Ontology editors]]</text>
      <sha1>lk8mr4h5qtnk6d56pbf8bls6qjy2drx</sha1>
    </revision>
  </page>
  <page>
    <title>Linear belief function</title>
    <ns>0</ns>
    <id>24134105</id>
    <revision>
      <id>758833049</id>
      <parentid>758832978</parentid>
      <timestamp>2017-01-07T21:00:15Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* Knowledge representation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23222" xml:space="preserve">{{Notability|date=January 2010}}

'''Linear belief function''' is an extension of the [[Dempster–Shafer theory]] of [[belief functions]] to the case when variables of interest are [[Continuous function|continuous]]. Examples of such variables include financial asset prices, portfolio performance, and other antecedent and consequent variables. The theory was originally proposed by [[Arthur P. Dempster]]&lt;ref&gt;A. P. Dempster, "Normal belief functions and the [[Kalman filter]]," in ''Data Analysis from Statistical Foundations'', A. K. M. E. Saleh, Ed.: Nova Science Publishers, 2001, pp. 65–84.&lt;/ref&gt; in the context of Kalman Filters and later was reelaborated, refined, and applied to knowledge representation in artificial intelligence and decision making in finance and accounting by Liping Liu.&lt;ref&gt;Liu, Liping, Catherine Shenoy, and Prakash P. Shenoy, “Knowledge Representation and Integration for Portfolio Evaluation Using Linear Belief Functions,” IEEE Transactions on Systems, Man, and Cybernetics, Series A, vol. 36 (4), 2006, pp. 774–785.&lt;/ref&gt;

== Concept ==

A linear belief function intends to represent our belief regarding the location of the true value as follows: We are certain that the truth is on a so-called certainty [[hyperplane]] but we do not know its exact location; along some dimensions of the certainty hyperplane, we believe the true value could be anywhere from –∞ to +∞ and the probability of being at a particular location is described by a [[normal distribution]]; along other dimensions, our knowledge is [[vacuous]], i.e., the true value is somewhere from –∞ to +∞ but the associated probability is unknown. A [[belief function]] in general is defined by a [[mass function]] over a class of [[focal elements]], which may have nonempty intersections. A linear belief function is a special type of [[belief function]] in the sense that its [[focal elements]] are exclusive, parallel sub-hyperplanes over the certainty hyperplane and its [[mass function]] is a [[normal distribution]] across the sub-hyperplanes.

Based on the above geometrical description, Shafer&lt;ref&gt;G. Shafer, "A note on Dempster's Gaussian belief functions," School of Business, University of Kansas, Lawrence, KS, Technical Report 1992.&lt;/ref&gt; and Liu&lt;ref&gt;L. Liu, "A theory of Gaussian belief functions," ''International Journal of Approximate Reasoning'', vol. 14, pp. 95–126, 1996&lt;/ref&gt;  propose two mathematical representations of a LBF: a wide-sense inner product and a linear functional in the variable space, and as their duals over a hyperplane in the sample space. Monney &lt;ref&gt;P. A. Monney, ''A Mathematical Theory of Arguments for Statistical Evidence''. New York, NY: Springer, 2003.&lt;/ref&gt; proposes still another structure called Gaussian hints. Although these representations are mathematically neat, they tend to be unsuitable for knowledge representation in expert systems.

== Knowledge representation ==

A linear belief function can represent both logical and probabilistic knowledge for three types of variables: deterministic such as an observable or controllable, random whose distribution is normal, and vacuous on which no knowledge bears. Logical knowledge is represented by linear equations, or geometrically, a certainty hyperplane. Probabilistic knowledge is represented by a normal distribution across all parallel focal elements.

In general, assume X is a vector of multiple normal variables with mean μ and covariance Σ. Then, the multivariate normal distribution can be equivalently represented as a moment matrix:

: &lt;math&gt;
M(X) = \left( \begin{array}{*{20}c}
   \mu \\
   \Sigma
\end{array} \right).
&lt;/math&gt;

If the distribution is non-degenerate, i.e., Σ has a full rank and its inverse exists, the moment matrix can be fully swept:

: &lt;math&gt; 
M(\vec X) = \left( \begin{array}{*{20}c}
   \mu \Sigma^{-1} \\
   -\Sigma^{-1}
\end{array} \right)
&lt;/math&gt;

Except for normalization constant, the above equation completely determines the normal density function for ''X''. Therefore,  &lt;math&gt;M(\vec X)&lt;/math&gt; represents the probability distribution of ''X'' in the potential form.

These two simple matrices allow us to represent three special cases of linear belief functions. First, for an ordinary normal probability distribution M(X) represents it. Second, suppose one makes a direct observation on X and obtains a value μ. In this case, since there is no uncertainty, both variance and covariance vanish, i.e., Σ = 0. Thus, a direct observation can be represented as:

: &lt;math&gt;M(X) = \left( \begin{array}{*{20}c}
   \mu \\
   0
\end{array} \right)
&lt;/math&gt;

Third, suppose one is completely ignorant about X. This is a very thorny case in Bayesian statistics since the density function does not exist. By using the fully swept moment matrix, we represent the vacuous linear belief functions as a zero matrix in the swept form follows:

: &lt;math&gt;M(\vec X) = \left[ \begin{array}{*{20}c}
   0 \\
   0
\end{array} \right]
&lt;/math&gt;

One way to understand the representation is to imagine complete ignorance as the limiting case when the variance of X approaches to ∞, where one can show that Σ&lt;sup&gt;−1&lt;/sup&gt; = 0 and hence &lt;math&gt;M(\vec X)&lt;/math&gt; vanishes. However, the above equation is not the same as an improper prior or normal distribution with infinite variance. In fact, it does not correspond to any unique probability distribution. For this reason, a better way is to understand the vacuous linear belief functions as the neutral element for combination (see later).

To represent the remaining three special cases, we need the concept of partial sweeping. Unlike a full sweeping, a partial sweeping is a transformation on a subset of variables. Suppose X and Y are two vectors of normal variables with the joint moment matrix:

: &lt;math&gt;M(X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   \mu_1 \\
   \Sigma _{11} \\
   \Sigma_{21}
\end{array} &amp; \begin{array}{*{20}c}
   \mu_2 \\
   \Sigma _{12} \\
   \Sigma_{22}
\end{array}
\end{array} \right]&lt;/math&gt;

Then M(X, Y) may be partially swept. For example, we can define the partial sweeping on X as follows:

: &lt;math&gt; M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   \mu _1 (\Sigma_{11})^{-1} \\
   -(\Sigma_{11})^{-1} \\
   \Sigma_{21} (\Sigma_{11})^{-1}
\end{array} &amp; \begin{array}{*{20}c}
   \mu_2  - \mu_1 (\Sigma_{11} )^{-1} \Sigma_{12} \\
   (\Sigma_{11} )^{-1} \Sigma_{12} \\
   \Sigma_{22} - \Sigma_{21} (\Sigma_{11})^{-1} \Sigma_{12}
\end{array}
\end{array} \right]
&lt;/math&gt;

If ''X'' is one-dimensional, a partial sweeping replaces the variance of ''X'' by its negative inverse and multiplies the inverse with other elements. If ''X'' is multidimensional, the operation involves the inverse of the covariance matrix of ''X'' and other multiplications. A swept matrix obtained from a partial sweeping on a subset of variables can be equivalently obtained by a sequence of partial sweepings on each individual variable in the subset and the order of the sequence does not matter. Similarly, a fully swept matrix is the result of partial sweepings on all variables.

We can make two observations. First, after the partial sweeping on&amp;nbsp;''X'', the mean vector and covariance matrix of ''X'' are respectively &lt;math&gt; \mu_1 (\Sigma _{11} )^{-1} &lt;/math&gt; and &lt;math&gt; -(\Sigma_{11} )^{-1} &lt;/math&gt;, which are the same as that of a full sweeping of the marginal moment matrix of&amp;nbsp;''X''. Thus, the elements corresponding to X in the above partial sweeping equation represent the marginal distribution of X in potential form. Second, according to statistics, &lt;math&gt; \mu_2  - \mu_1 (\Sigma_{11} )^{-1} \Sigma_{12} &lt;/math&gt;is the conditional mean of ''Y'' given ''X''&amp;nbsp;=&amp;nbsp;0; &lt;math&gt; \Sigma_{22} - \Sigma_{21} (\Sigma_{11} )^{-1} \Sigma_{12} &lt;/math&gt;  is the conditional covariance matrix of ''Y'' given ''X''nbsp;=&amp;nbsp;0; and  &lt;math&gt;(\Sigma_{11} )^{-1} \Sigma_{12} &lt;/math&gt; is the slope of the regression model of ''Y'' on&amp;nbsp;''X''. Therefore, the elements corresponding to Y indices and the intersection of ''X'' and ''Y'' in &lt;math&gt; M(\vec X,Y)&lt;/math&gt;represents the conditional distribution of ''Y'' given&amp;nbsp;''X''&amp;nbsp;=&amp;nbsp;0.

These semantics render the partial sweeping operation a useful method for manipulating multivariate normal distributions. They also form the basis of the moment matrix representations for the three remaining important cases of linear belief functions, including proper belief functions, linear equations, and linear regression models.

=== Proper linear belief functions ===
For variables ''X'' and ''Y'', assume there exists a piece of evidence justifying a normal distribution for variables ''Y'' while bearing no opinions for variables&amp;nbsp;''X''. Also, assume that ''X'' and ''Y'' are not perfectly linearly related, i.e., their correlation is less than&amp;nbsp;1. This case involves a mix of an ordinary normal distribution for Y and a vacuous belief function for&amp;nbsp;''X''. Thus, we represent it using a partially swept matrix  as follows:

: &lt;math&gt;M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   0
\end{array} &amp; \begin{array}{*{20}c}
   \mu_2 \\
   0  \\
   \Sigma_{22} \\
\end{array}
\end{array} \right]
&lt;/math&gt;

This is how we could understand the representation. Since we are ignorant on&amp;nbsp;''X'', we use its swept form and set  &lt;math&gt; \mu_1 (\Sigma_{11})^{-1} = 0&lt;/math&gt; and &lt;math&gt; - (\Sigma_{11})^{-1} = 0&lt;/math&gt;. Since the correlation between ''X'' and ''Y'' is less than&amp;nbsp;1, the regression coefficient of ''X'' on ''Y'' approaches to 0 when the variance of ''X'' approaches to&amp;nbsp;∞. Therefore,  &lt;math&gt;(\Sigma_{11})^{-1} \Sigma_{12} = 0&lt;/math&gt;. Similarly, one can prove that &lt;math&gt;\mu_1 (\Sigma_{11})^{-1} \Sigma_{12}  = 0&lt;/math&gt; and  &lt;math&gt; \Sigma_{21} (\Sigma_{11})^{-1} \Sigma_{12} = 0&lt;/math&gt;.

=== Linear equations ===
Suppose X and Y are two row vectors, and Y = XA + b, where A and b are the coefficient matrices. We represent the equation using a partially swept matrix as follows:

: &lt;math&gt;M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   A^T
\end{array} &amp; \begin{array}{*{20}c}
   b  \\
   A  \\
   0
\end{array}
\end{array} \right]
&lt;/math&gt;

We can understand the representation based on the fact that a linear equation contains two pieces of knowledge: (1) complete ignorance about all variables; and (2) a degenerate conditional distribution of dependent variables given independent variables. Since X is an independent vector in the equation, we are completely ignorant about it. Thus, &lt;math&gt; \mu_1 (\Sigma _{11})^{-1} = 0&lt;/math&gt; and &lt;math&gt; -(\Sigma_{11})^{-1} = 0&lt;/math&gt;. Given ''X'' = 0, ''Y'' is completely determined to be b. Thus, the conditional mean of Y is b and the conditional variance is 0. Also, the regression coefficient matrix is A.

Note that the knowledge to be represented in linear equations is very close to that in a proper linear belief functions, except that the former assumes a perfect correlation between X and Y while the latter does not. This observation is interesting; it characterizes the difference between partial ignorance and linear equations in one parameter — correlation.

=== Linear regression models ===
A linear regression model is a more general and interesting case than previous ones. Suppose X and Y are two vectors and Y = XA + b + E, where A and b are the appropriate coefficient matrices and E is an independent white noise satisfying E ~ N(0, Σ). We represent the model as the following partially swept matrix:

: &lt;math&gt;
M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   A^T
\end{array} &amp; \begin{array}{*{20}c}
   b  \\
   A \\
   \Sigma
\end{array}
\end{array} \right]
&lt;/math&gt;

This linear regression model may be considered as the combination of two pieces of knowledge (see later), one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, Σ). Alternatively, one may consider it similar to a linear equation, except that, given X = 0, Y is not completely determined to be b. Instead, the conditional mean of Y is b while the conditional variance is Σ. Note that, in this alternative interpretation, a linear regression model forms a basic building block for knowledge representation and is encoded as one moment matrix. Besides, the noise term E does not appear in the representation. Therefore, it makes the representation more efficient.

From representing the six special cases, we see a clear advantage of the moment matrix representation, i.e., it allows a unified representation for seemingly diverse types of knowledge, including linear equations, joint and conditional distributions, and ignorance. The unification is significant not only for knowledge representation in artificial intelligence but also for statistical analysis and engineering computation. For example, the representation treats the typical logical and probabilistic components in statistics — observations, distributions, improper priors (for Bayesian statistics), and linear equation models — not as separate concepts, but as manifestations of a single concept. It allows one to see the inner connections between these concepts or manifestations and to interplay them for computational purposes.

== Knowledge operations ==

There are two basic operations for making inferences in [[expert system]]s using linear belief functions: combination and marginalization. Combination corresponds to the integration of knowledge whereas marginalization corresponds to the coarsening of knowledge. Making an inference involves combining relevant knowledge into a full body of knowledge and then projecting the full body of knowledge to a partial domain, in which an inference question is to be answered.

=== Marginalization ===
Marginalization projects a linear belief function into one with fewer variables. Expressed as a moment matrix, it is simply the restriction of a nonswept moment matrix to a submatrix corresponding to the remaining variables. For example, for the joint distribution M(X, Y), its marginal to Y is:

: &lt;math&gt;
M^{\downarrow Y} (X,Y) = \left[ {\begin{array}{*{20}c}
   \mu_2  \\
   \Sigma_{22}
\end{array}} \right]
&lt;/math&gt;

When removing a variable, it is important that the variable has not been swept on in the corresponding moment matrix, i.e., it does not have an arrow sign above the variable. For example, projecting the matrix &lt;math&gt;M(\vec X,Y)&lt;/math&gt; to Y produces:

: &lt;math&gt; M^{ \downarrow Y} (\vec X,Y) = \left[ {\begin{array}{*{20}c}
   \mu _2  - \mu _1 (\Sigma _{11} )^{-1} \Sigma _{12} \\
   \Sigma_{22} - \Sigma_{21} (\Sigma _{11})^{-1} \Sigma_{12}
\end{array}} \right]
&lt;/math&gt;

which is not the same linear belief function of Y. However, it is easy to see that removing any or all variables in Y from the partially swept matrix will still produce the correct result — a matrix representing the same function for the remaining variables.

To remove a variable that has been already swept on, we have to reverse the sweeping using partial or full reverse sweepings. Assume  &lt;math&gt;M(\vec X)&lt;/math&gt; is a fully swept moment matrix,

: &lt;math&gt;
M(\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu }  \\
   {\bar \Sigma }  \\
\end{array}} \right)
&lt;/math&gt;

Then a full reverse sweeping of &lt;math&gt;M(\vec X)&lt;/math&gt; will recover the moment matrix M(X) as follows:

: &lt;math&gt;
M(X) = \left( {\begin{array}{*{20}c}
   { - \bar \mu \bar \Sigma ^{ - 1} }  \\
   { - \bar \Sigma ^{ - 1} }  \\
\end{array}} \right)
&lt;/math&gt;

If a moment matrix is in a partially swept form, say

: &lt;math&gt;
M(\vec X,Y) = \left[ {\begin{array}{*{20}c}
   {\begin{array}{*{20}c}
   {\bar \mu _1 }  \\
   {\bar \Sigma _{11} }  \\
   {\bar \Sigma _{21} }  \\
\end{array}} &amp; {\begin{array}{*{20}c}
   {\bar \mu _2 }  \\
   {\bar \Sigma _{12} }  \\
   {\bar \Sigma _{22} }  \\
\end{array}}  \\
\end{array}} \right]
&lt;/math&gt;

its partially reverse sweeping on X is defined as follows:

: &lt;math&gt; M(X,Y) = \left[ {\begin{array}{*{20}c}
   {\begin{array}{*{20}c}
   { - \bar \mu _1 (\bar \Sigma _{11} )^{ - 1} }  \\
   { - (\bar \Sigma _{11} )^{ - 1} }  \\
   { - \bar \Sigma _{21} (\bar \Sigma _{11} )^{ - 1} }  \\
\end{array}} &amp; {\begin{array}{*{20}c}
   {\bar \mu _2  - \bar \mu _1 (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
   { - (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
   {\bar \Sigma _{22}  - \bar \Sigma _{21} (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
\end{array}}  \\
\end{array}} \right]
&lt;/math&gt;

Reverse sweepings are similar to those of forward ones, except for a sign difference for some multiplications. However, forward and reverse sweepings are opposite operations. It can be easily shown that applying the fully reverse sweeping to &lt;math&gt;M(\vec X)&lt;/math&gt;   will recover the initial moment matrix  M(X). It can also be proved that applying a partial reverse sweeping on X to the matrix  &lt;math&gt; M(\vec X,Y)&lt;/math&gt; will recover the moment matrix M(X,Y). As a matter of fact, Liu&lt;ref&gt;L. Liu, "Local Computation of Gaussian Belief Functions," ''International Journal of Approximate Reasoning'', vol. 22, pp. 217–248, 1999&lt;/ref&gt; proves that a moment matrix will be recovered through a reverse sweeping after a forward sweeping on the same set of variables. It can be also recovered through a forward sweeping after a reverse sweeping. Intuitively, a partial forward sweeping factorizes a joint into a marginal and a conditional, whereas a partial reverse sweeping multiplies them into a joint.

=== Combination ===
According to [[Dempster’s rule]], the combination of belief functions may be expressed as the intersection of focal elements and the multiplication of probability density functions. [[Liping Liu]] applies the rule to linear belief functions in particular and obtains a formula of combination in terms of density functions. Later he proves a claim by [[Arthur P. Dempster]] and reexpresses the formula as the sum of two fully swept matrices. Mathematically, assume &lt;math&gt;M_1 (\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _1 }  \\
   {\bar \Sigma _1 }  \\
\end{array}} \right)
&lt;/math&gt;  and &lt;math&gt; M_2 (\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _2 }  \\
   {\bar \Sigma _2 }  \\
\end{array}} \right)
&lt;/math&gt;  are two LBFs for the same vector of variables X. Then their combination is a fully swept matrix:

: &lt;math&gt; M(\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _1  + \bar \mu _2 }  \\
   {\bar \Sigma _1  + \bar \Sigma _2 }  \\
\end{array}} \right)
&lt;/math&gt;

This above equation is often used for multiplying two normal distributions. Here we use it to define the combination of two linear belief functions, which include normal distributions as a special case. Also, note that a vacuous linear belief function (0 swept matrix) is the neutral element for combination. When applying the equation, we need to consider two special cases. First, if two matrices to be combined have different dimensions, then one or both matrices must be vacuously extended, i.e., assuming ignorance on the variables that are no present in each matrix. For example, if M&lt;sub&gt;1&lt;/sub&gt;(X,Y)  and M&lt;sub&gt;2&lt;/sub&gt;(X,Z)  are to be combined, we will first extend them into &lt;math&gt; M_1 (X,Y,\vec Z)&lt;/math&gt;  and &lt;math&gt; M_2 (X,\vec Y,Z)&lt;/math&gt;  respectively such that  &lt;math&gt; M_1 (X,Y,\vec Z)&lt;/math&gt; is ignorant about Z and  &lt;math&gt; M_2 (X,\vec Y,Z)&lt;/math&gt; is ignorant about Y. The vacuous extension was initially proposed by Kong &lt;ref&gt;A. Kong, "Multivariate belief functions and graphical models," in Department of Statistics. Cambridge, MA: Harvard University, 1986&lt;/ref&gt; for discrete belief functions. Second, if a variable has zero variance, it will not permit a sweeping operation. In this case, we can pretend the variance to be an extremely small number, say ε, and perform the desired sweeping and combination. We can then apply a reverse sweeping to the combined matrix on the same variable and let ε approaches 0. Since zero variance means complete certainty about a variable, this ε-procedure will vanish ε terms in the final result.

In general, to combine two linear belief functions, their moment matrices must be fully swept. However, one may combine a fully swept matrix with a partially swept one directly if the variables of the former matrix have been all swept on in the later. We can use the linear regression model — Y = XA + b + E — to illustrate the property. As we mentioned, the regression model may be considered as the combination of two pieces of knowledge: one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, Σ). Let  &lt;math&gt;M_1 (\vec X,\vec {\rm E},Y) = \left[ {\begin{array}{*{20}c}
   0 &amp; 0 &amp; b  \\
   0 &amp; 0 &amp; A  \\
   0 &amp; 0 &amp; I  \\
   {A^T } &amp; I &amp; 0  \\
\end{array}} \right]
&lt;/math&gt; and &lt;math&gt; M_2 (\vec {\rm E}) = \left[ {\begin{array}{*{20}c}
   0  \\
   { - \Sigma ^{ - 1} }  \\
\end{array}} \right]
&lt;/math&gt;  be their moment matrices respectively. Then the two matrices can be combined directly without sweeping &lt;math&gt; M_1 (\vec X,\vec {\rm E},Y)
&lt;/math&gt;  on Y first. The result of the combination is a partially swept matrix as follows:

: &lt;math&gt; M(\vec X,\vec {\rm E},Y) = \left[ {\begin{array}{*{20}c}
   0 &amp; 0 &amp; b  \\
   0 &amp; 0 &amp; A  \\
   0 &amp; { - \Sigma ^{ - 1} } &amp; I  \\
   {A^T } &amp; I &amp; 0  \\
\end{array}} \right]
&lt;/math&gt;

If we apply a reverse sweeping on E and then remove E from the matrix, we will obtain the same representation of the regression model.

== Applications ==

We may use an audit problem to illustrate the three types of variables as follows. Suppose we want to audit the ending balance of accounts receivable (''E''). As we saw earlier, ''E'' is equal to the beginning balance (''B'') plus the sales (''S'') for the period minus the cash receipts (''C'') on the sales plus a residual (''R'') that represents insignificant sales returns and cash discounts. Thus, we can represent the logical relation as a linear equation:

: &lt;math&gt;E=B+S-C+R&lt;/math&gt;
	
Furthermore, if the auditor believes ''E'' and ''B'' are 100 thousand dollars on the average with a standard deviation 5 and the covariance 15, we can represent the belief as a multivariate normal distribution. If historical data indicate that the residual R is zero on the average with a standard deviation of 0.5 thousand dollars, we can summarize the historical data by normal distribution ''R''&amp;nbsp;~&amp;nbsp;N(0,&amp;nbsp;0.5&lt;sup&gt;2&lt;/sup&gt;).  If there is a direct observation on cash receipts, we can represent the evidence as an equation say, C = 50 (thousand dollars). If the auditor knows nothing about the beginning balance of accounts receivable, we can represent his or her ignorance by a vacuous LBF. Finally, if historical data suggests that, given cash receipts&amp;nbsp;''C'', the sales ''S'' is on the average 8''C''&amp;nbsp;+&amp;nbsp;4 and has a standard deviation 4 thousand dollars, we can represent the knowledge as a linear regression model ''S''&amp;nbsp;~&amp;nbsp;N(4&amp;nbsp;+&amp;nbsp;8''C'',&amp;nbsp;16).

==References==
&lt;references/&gt;

{{DEFAULTSORT:Linear Belief Function}}
[[Category:Knowledge representation]]</text>
      <sha1>sd11h71trh5fpbmkus9tmmldbc74dd9</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Lexical databases</title>
    <ns>14</ns>
    <id>31484297</id>
    <revision>
      <id>423805300</id>
      <timestamp>2011-04-13T04:47:41Z</timestamp>
      <contributor>
        <username>4th-otaku</username>
        <id>7579047</id>
      </contributor>
      <comment>created</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="141" xml:space="preserve">{{catmain|Lexical database}}

[[Category:Translation databases]]
[[Category:Computational linguistics]]
[[Category:Knowledge representation]]</text>
      <sha1>lozpxzugc6h3nqf9kvdonkx766oa85c</sha1>
    </revision>
  </page>
  <page>
    <title>Folksonomy</title>
    <ns>0</ns>
    <id>23219749</id>
    <revision>
      <id>756231871</id>
      <parentid>756218811</parentid>
      <timestamp>2016-12-22T21:29:11Z</timestamp>
      <contributor>
        <username>McFarlandDana</username>
        <id>27259913</id>
      </contributor>
      <minor />
      <comment>/* Social tagging for knowledge acquisition */ applied hdl=free</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24865" xml:space="preserve">A '''folksonomy''' is a system in which users apply public [[Tag (metadata)|tags]]  to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a [[Taxonomy (general)|taxonomic]] classification specified by the owners of the content when it is published.&lt;ref&gt;{{cite news
 | title = Folksonomies. Indexing and Retrieval in Web 2.0.
 | url = https://books.google.com/books?id=Aeib_wy18gkC&amp;printsec=frontcover&amp;dq=folksonomies.+Indexing+and+Retrieval+in+Web+2.0#v=onepage&amp;q&amp;f=false
 | first = Isabella
 | last = Peters
 | work = Berlin: De Gruyter Saur
 | year = 2009
 }}&lt;/ref&gt;&lt;ref&gt;{{cite news
 | title = Folksonomy
 | first = Daniel H.
 | last = Pink
 | authorlink =
 | url = http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html
 | work = New York Times
 | date = 11 December 2005
 | accessdate = 14 July 2009
 }}&lt;/ref&gt; This practice is also known as '''collaborative tagging''',&lt;ref&gt;Lambiotte, R, and M Ausloos. 2005. Collaborative tagging as a tripartite network. http://arxiv.org/abs/cs.DS/0512090.&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of Association for Information Science and Technology|publisher=ASIS&amp;T|accessdate=26 May 2016}}&lt;/ref&gt; '''social classification''', '''social indexing''', and '''social tagging'''. However, these terms have slightly different meanings than folksonomy. Folksonomy was originally “the result of personal free tagging of information [...] for one’s own retrieval.”.&lt;ref&gt;Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from &lt;nowiki&gt;http://www.vanderwal.net/folksonomy.html&lt;/nowiki&gt;&lt;/ref&gt; '''Social tagging''' is the application of tags in an open online environment where the tags of other users are available to others. '''Collaborative tagging''' (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.

The term was coined by [[Thomas Vander Wal]] in 2004&lt;ref&gt;{{cite news
 | title = Folksonomy Coinage and Definition
 | url = http://www.vanderwal.net/folksonomy.html
 | first = Tomas
 | last = Vander Wal
 | date = 11 December 2005
 }}&lt;/ref&gt;&lt;ref&gt;Vander Wal, T. (2005). "[http://www.vanderwal.net/random/category.php?cat=153 Off the Top: Folksonomy Entries]." Visited November 5, 2005. See also: Smith, Gene. "[https://web.archive.org/web/20040828035712/http://atomiq.org/archives/2004/08/folksonomy_social_classification.html Atomiq: Folksonomy: social classification]." Aug 3, 2004. Retrieved January 1, 2007.&lt;/ref&gt;&lt;ref&gt;http://vanderwal.net/folksonomy.html Origin of the term&lt;/ref&gt; as a [[portmanteau]] of ''[[Volk (German word)|folk]]'' and ''[[Taxonomy (general)|taxonomy]]''. Folksonomies became popular as part of [[social software]] applications such as [[social bookmarking]] and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include [[tag cloud]]s as a way to visualize tags in a folksonomy.&lt;ref&gt;{{Cite journal
 | last1 = Lamere | first1 = Paul
 | title = Social Tagging And Music Information Retrieval
 | journal = Journal of New Music Research
 | volume = 37
 | issue = 2
 | pages = 101–114
 | date = June 2008
 | url = http://www.informaworld.com/smpp/content~db=all~content=a906001732
 | doi = 10.1080/09298210802479284 }}&lt;/ref&gt;

Folksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.

==Benefits and disadvantages==
Folksonomies are a trade-off between traditional centralized classification and no classification at all,&lt;ref&gt;Gupta, M., et al., ''An Overview of Social Tagging and Applications, in Social Network Data Analytics'', C.C. Aggarwal, Editor. 2011, Springer. p. 447-497.&lt;/ref&gt; and have several advantages:&lt;ref&gt;Quintarelli, E., ''Folksonomies: power to the people''. 2005.&lt;/ref&gt;&lt;ref&gt;Mathes, A., ''Folksonomies - Cooperative Classification and Communication Through Shared Metadata''. 2004.&lt;/ref&gt;&lt;ref&gt;Wal, T.V. ''Folksonomy''. 2007&lt;/ref&gt;
* tagging is easy to understand and do, even without training and previous knowledge in classification or indexing
* the vocabulary in a folksonomy directly reflects the user’s vocabulary
* folksonomies are flexible, in the sense that the user can add or remove tags
* tags consist of both popular content and long-tail content, enabling users to browse and discover new content even in narrow topics
* tags reflect the user’s conceptual model without cultural, social, or political bias
* enable the creation of communities, in the sense that users who apply the same tag have a common interest
* folksonomies are multi-dimensional, in the sense that users can assign any number and combination of tags to express a concept

There are several disadvantages with the use of tags and folksonomies as well,&lt;ref&gt;Kipp, M. and D.G. Campbell, ''Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices''. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.&lt;/ref&gt; and some of the advantages (see above) can lead to problems. For example, the simplicity in tagging can result in poorly applied tags.&lt;ref&gt;Hayman, S., ''Folksonomies and Tagging: New developments in social bookmarking'', in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.&lt;/ref&gt; Further, while controlled vocabularies are exclusionary by nature,&lt;ref&gt;Kroski, E., The Hive Mind: ''Folksonomies and User-Based Tagging. 2005''&lt;/ref&gt; tags are often ambiguous and overly personalized.&lt;ref&gt;Guy, M. and E. Tonkin, ''Folksonomies: Tidying up Tags?'' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.&lt;/ref&gt; Users apply tags to documents in many different ways and tagging systems also often lack mechanisms for handling synonyms, acronyms and homonyms, and they also often lack mechanisms for handling spelling variations such as misspellings, singular/plural form, conjugated and compound words. Some tagging systems do not support tags consisting of multiple words, resulting in tags like “viewfrommywindow”. Sometimes users choose specialized tags or tags without meaning to others.

==Elements and types==
A folksonomy emerges when users tag content or information, such web pages, photos, videos, podcasts, tweets, scientific papers and others. Strohmaier et al.&lt;ref&gt;Strohmaier, M., C. Körner, and R. Kern, ''Understanding why users tag: A survey of tagging motivation literature and results from an empirical study''. Web Semantics: Science, Services and Agents on the World Wide Web, 2012. 17: p. 1-11.&lt;/ref&gt; elaborate the concept: the term “tagging” refers to a "voluntary activity of users who are annotating resources with term-so-called 'tags' – freely chosen from an unbounded and uncontrolled vocabulary". Others explain tags as an unstructured textual label &lt;ref&gt;Ames, M.N.M., ''Why We Tag: Motivations for Annotation in Mobile and Online Media'', in SIGCHI conference on Human factors in computing systems. 2007, ACM Press: New York, NY, USA. p. 971-980.&lt;/ref&gt; or keywords,&lt;ref&gt;Guy, M. and E. Tonkin, Folksonomies: ''Tidying up Tags?'' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.&lt;/ref&gt; and that they appear as a simple form of metadata.&lt;ref&gt;Brooks, C.H. and N. Montanez, ''Improved annotation of the blogosphere via autotagging and hierarchical clustering'', in WWW '06: Proceedings of the 15th international conference on World Wide Web. 2006, ACM Press: New York, NY, USA. p. 625-632.&lt;/ref&gt;

Folksonomies consist of three basic entities: users, tags, and resource. Users create tags to mark resources such as: web pages, photos, videos, and podcasts. These tags are used to manage, categorize and summarize online content. This collaborative tagging system also uses these tags as a way to index information, facilitate searches and navigate resources. Folksonomy also includes a set of URLs that are used to identify resources that have been referred to by users of different websites. These systems also include category schemes that have the ability to organize tags at different levels of granularity.&lt;ref name="Berlin, B. 1992"&gt;Berlin, B. (1992). Ethnobiological Classification. Princeton: Princeton University Press.&lt;/ref&gt;

Vander Wal identifies two types of folksonomy: broad and narrow.&lt;ref name="Vander Wal"&gt;{{cite web |title=Explaining and Showing Broad and Narrow Folksonomies |url=http://www.vanderwal.net/random/entrysel.php?blog=1635 | last = Vander Wal | first=Thomas |accessdate= 2013-03-05}}&lt;/ref&gt;  A broad folksonomy arises when multiple users can apply the same tag to an item, providing information about which tags are the most popular. A narrow folksonomy occurs when users, typically fewer in number and often including the item's creator, tag an item with tags that can each be applied only once.  While both broad and narrow folksonomies enable the searchability of content by adding an associated word or phrase to an object, a broad folksonomy allows for sorting based on the popularity of each tag, as well as the tracking of emerging trends in tag usage and developing vocabularies.&lt;ref name="Vander Wal"/&gt;

An example of a broad folksonomy is [[Delicious (website)|del.icio.us]],  a website where users can tag any online resource they find relevant with their own personal tags. The photo-sharing website [[Flickr]] is an oft-cited example of a narrow folksonomy.

==Folksonomy vs. taxonomy==
'Taxonomy' refers to a hierarchical categorization in which relatively well-defined classes are nested under broader categories. A ''folksonomy'' establishes categories (each tag is a category) without stipulating or necessarily deriving a hierarchical structure of parent-child relations among different tags. (Work has been done on techniques for deriving at least loose hierarchies from clusters of tags.&lt;ref&gt;{{cite journal|last1=Laniado|first1=David|title=Using WordNet to turn a folksonomy into a hierarchy of concepts|journal=CEUR Workshop Proceedings|volume=314|issue=51|url=http://ceur-ws.org/Vol-314/51.pdf|accessdate=7 August 2015}}&lt;/ref&gt;)

Supporters of folksonomies claim that they are often preferable to taxonomies because folksonomies democratize the way information is organized, they are more useful to users because they reflect current ways of thinking about domains, and they express more information about domains.&lt;ref&gt;{{cite web|last1=Weinberger|first1=David|title=Folksonomy as Symbol|url=http://www.hyperorg.com/blogger/?p=6254|website=Joho the Blog|accessdate=7 August 2015}}&lt;/ref&gt; Critics claim that folksonomies are messy and thus harder to use, and can reflect transient trends that may misrepresent what is known about a field.

An empirical analysis of the complex dynamics of tagging systems, published in 2007,&lt;ref name="WWW07-ref" &gt;Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proc. International Conference on World Wide Web, ACM Press, 2007.&lt;/ref&gt; has shown that consensus around stable distributions and shared vocabularies does emerge, even in the absence of a central [[controlled vocabulary]]. For content to be searchable, it should be categorized and grouped. While this was believed to require commonly agreed on sets of content describing tags (much like keywords of a journal article), some research has found that in large folksonomies common structures also emerge on the level of categorizations.&lt;ref name="TWEB-ref" &gt;V. Robu, H. Halpin, H. Shepherd [http://portal.acm.org/citation.cfm?id=1594173.1594176 Emergence of consensus and shared vocabularies in collaborative tagging systems], ACM Transactions on the Web (TWEB), Vol. 3(4), art. 14, 2009.&lt;/ref&gt;
Accordingly, it is possible to devise mathematical [[models of collaborative tagging]] that allow for translating from personal tag vocabularies (personomies) to the vocabulary shared by most users.&lt;ref&gt;Robert Wetzker, Carsten Zimmermann, Christian Bauckhage, and Sahin Albayrak [http://portal.acm.org/citation.cfm?id=1718487.1718497 I tag, you tag: translating tags for advanced user models], Proc. International Conference on Web Search and Data Mining, ACM Press, 2010.&lt;/ref&gt;

Folksonomy is unrelated to [[folk taxonomy]], a cultural practice that has been widely documented in anthropological and [[folkloristics|folkloristic]] work. Folk taxonomies are culturally supplied, intergenerationally transmitted, and relatively stable classification systems that people in a given culture use to make sense of the entire world around them (not just the [[Internet]]).&lt;ref name="Berlin, B. 1992"/&gt;

The study of the structuring or classification of folksonomy is termed ''folksontology''.&lt;ref&gt;{{cite web | url=http://www.heppnetz.de/files/vandammeheppsiorpaes-folksontology-semnet2007-crc.pdf | title=FolksOntology: An Integrated Approach for Turning Folksonomies into Ontologies | accessdate=April 20, 2012 | author=Van Damme, Céline|display-authors=etal}}&lt;/ref&gt; This branch of [[ontology (information science)|ontology]] deals with the intersection between highly structured taxonomies or hierarchies and loosely structured folksonomy, asking what best features can be taken by both for a system of classification. The strength of flat-tagging schemes is their ability to relate one item to others like it. Folksonomy allows large disparate groups of users to collaboratively label massive, dynamic information systems. The strength of taxonomies are their browsability: users can easily start from more generalized knowledge and target their queries towards more specific and detailed knowledge.&lt;ref&gt;Trattner, C., Körner, C., Helic, D.: [http://www.christophtrattner.info/pubs/iknow2011.pdf Enhancing the Navigability of Social Tagging Systems with Tag Taxonomies]. In Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies, ACM, New York, NY, USA, 2011&lt;/ref&gt; Folksonomy looks to categorize tags and thus create browsable spaces of information that are easy to maintain and expand.

== Social tagging for knowledge acquisition ==
Social tagging for knowledge acquisition is the specific use of tagging for finding and re-finding specific content for an individual or group. Social tagging systems differ from traditional taxonomies in that they are community-based systems lacking the traditional hierarchy of taxonomies. Rather than a top-down approach, social tagging relies on users to create the folksonomy from the bottom up.&lt;ref name=":0"&gt;Held, C., &amp; Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.&lt;/ref&gt;

Common uses of social tagging for knowledge acquisition include personal development for individual use and collaborative projects. Social tagging is used for knowledge acquisition in secondary, post-secondary, and graduate education as well as personal and business research. The benefits of finding/re-finding source information are applicable to a wide spectrum of users. Tagged resources are located through search queries rather than searching through a more traditional file folder system.&lt;ref&gt;Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229–238. ACM, New York.&lt;/ref&gt; The social aspect of tagging also allows users to take advantage of metadata from thousands of other users.&lt;ref name=":0" /&gt;

Users choose individual tags for stored resources. These tags reflect personal associations, categories, and concepts. All of which are individual representations based on meaning and relevance to that individual. The tags, or keywords, are designated by users. Consequently, tags represent a user’s associations corresponding to the resource. Commonly tagged resources include videos, photos, articles, websites, and email.&lt;ref name=":1"&gt;Kimmerle, J., Cress, U., &amp; Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research &amp; Practice, 8(1), 33-44.&lt;/ref&gt; Tags are beneficial for a couple of reasons. First, they help to structure and organize large amounts of digital resources in a manner that makes them easily accessible when users attempt to locate the resource at a later time. The second aspect is social in nature, that is to say that users may search for new resources and content based on the tags of other users. Even the act of browsing through common tags may lead to further resources for knowledge acquisition.&lt;ref name=":0" /&gt;

Tags that occur more frequently with specific resources are said to be more strongly connected. Furthermore, tags may be connected to each other. This may be seen in the frequency in which they co-occur. The more often they co-occur, the stronger the connection. Tag clouds are often utilized to visualize connectivity between resources and tags. Font size increases as the strength of association increases.&lt;ref name=":1" /&gt;

Tags show interconnections of concepts that were formerly unknown to a user. Therefore, a user’s current cognitive constructs may be modified or augmented by the metadata information found in aggregated social tags. This process promotes knowledge acquisition through cognitive irritation and equilibration. This theoretical framework is known as the co-evolution model of individual and collective knowledge.&lt;ref name=":1" /&gt;

The co-evolution model focuses on cognitive conflict in which a learner’s prior knowledge and the information received from the environment are dissimilar to some degree.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt; When this incongruence occurs, the learner must work through a process cognitive equilibration in order to make personal cognitive constructs and outside information congruent. According to the coevolution model, this may require the learner to modify existing constructs or simply add to them.&lt;ref name=":0" /&gt; The additional cognitive effort promotes information processing which in turn allows individual learning to occur.&lt;ref name=":1" /&gt;

A Canadian university study of instructors' use of folksonomy tools in a learning objects repository identified critical success factors, and affirmed the applicability of Zipf's [[Principle of least effort|Principle of Least Effort]], concluding that a major benefit of "the folksonomical approach to knowledge management... is the fact that it is developed and maintained by the users of that body of knowledge," fostering "the dual outcome of creating a more viable knowledge management tool while strengthening the bonds of the user community."&lt;ref&gt;{{Cite book|url=http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-368-5.ch045|title=Critical Success Factors in the Development of Folksonomy-Based Knowledge Management Tools|last=Owen|first=Kenneth|last2=Willis|first2=Robert|date=2010|publisher=IGI Global|year=|isbn=|location=|pages=509–518|language=English|doi=10.4018/978-1-60566-368-5.ch045|hdl=http://hdl.handle.net/10613/3176|quote=|via=VIUSpace|hdl-access=free}}&lt;/ref&gt;

==Examples of folksonomies==

* [[Twitter]] [[hashtag]]s
* Many libraries' online catalogs&lt;ref&gt;Steele, T. (2009).  The new cooperative cataloging.  Library Hi Tech, 27 (1), 68-77&lt;/ref&gt;&lt;ref&gt;Corey A. Harper and [[Barbara B. Tillett]], [https://scholarsbank.uoregon.edu/dspace/bitstream/1794/3269/1/ccq_s Library of Congress controlled vocabularies and their application to the Semantic Web]&lt;/ref&gt;
* [[Delicious (website)|del.icio.us]]: public tagging service
* [[Flickr]]: shared photos
* [[Steam (software)|Steam]] video game store
* [[Mendeley]]: social reference management software
* [[StumbleUpon]]: content discovery engine
* [[Diigo]]: [[social bookmarking]] website
*  The [[World Wide Web Consortium]]'s [[Annotea]] project with user-generated tags in 2002.
* [[Instagram]]: online photo-sharing and social networking service
* [[WordPress]]: blogging tool and Content Management System
* [[Pinterest]]: photosharing and publishing website

==See also==
{{div col|colwidth=30em}}
* [[Collective intelligence]]
* [[Enterprise bookmarking]]
* [[Faceted classification]]
* [[Semantic similarity]]
* [[Thesaurus]]
* [[Weak ontology]]
* [[Wiki]]
{{div col end}}

==References==
{{Reflist|30em|refs = Bateman, S., Brooks, C., McCalla, G., &amp; Brusilovsky, P. (2007, May). Applying collaborative tagging to e-learning. In Proceedings of the 16th International World Wide Web Conference (WWW2007).

Civan, A., Jones, W., Klasnja, P., &amp; Bruce, H. (2008). Better to organize personal information by folders or by tags?: The devil is in the details.Proceedings of the American Society for Information Science and Technology,45(1), 1-13.
 
Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229–238. ACM, New York.

Guy, M. and E. Tonkin, Folksonomies: Tidying up Tags? D-Lib Magazine, 2006. 12(Number 1): p. 1-15.

Gupta, M., et al., An Overview of Social Tagging and Applications, in Social Network Data Analytics, C.C. Aggarwal, Editor. 2011, Springer. p. 447-497
 
Held, C., &amp; Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.

Hayman, S., Folksonomies and Tagging: New developments in social bookmarking, in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.
 
Kimmerle, J., Cress, U., &amp; Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research &amp; Practice, 8(1), 33-44.

Kipp, M. and D.G. Campbell, Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.

Kroski, E., The Hive Mind: Folksonomies and User-Based Tagging. 2005.
 
Lavoué, É. (2011). Social tagging to enhance collaborative learning. In Advances in Web-Based Learning-ICWL 2011 (pp. 92-101). Springer Berlin Heidelberg.

Mathes, A., Folksonomies - Cooperative Classification and Communication Through Shared Metadata. 2004.

Quintarelli, E., Folksonomies: power to the people. 2005.

Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from http://www.vanderwal.net/folksonomy.html

Weinberger, D. (2007). Everything is miscellaneous: The power of the new digital disorder. Times Books, New York.
 
}}

==Additional references==
* [http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html "Folksonomy"], [[The New York Times]], 2005-12-11
* [http://www.wired.com/science/discoveries/news/2005/02/66456 "Folksonomies Tap People Power"], [[Wired News]], 2005-02-01
* {{cite journal | journal = [[Information Services &amp; Use]] | title = Folksonomy and science communication | issue = 27 | year = 2007 | pages = 97–103 | url = http://wwwalt.phil-fak.uni-duesseldorf.de/infowiss/admin/public_dateien/files/1/1194272247inf_servic.pdf }}{{spaced ndash}} Folksonomies as a tool for professional scientific databases
* [http://www.hyperorg.com/blogger/misc/taxonomies_and_tags.html "The Three Orders"]: 2005 explanation of tagging and folksonomies

==External links==
* [http://www.socialtagging.org/ SocialTagging.org] provides short definitions of key terms related to tagging and folksonomies
* [http://www.vanderwal.net/folksonomy.html Vanderwal's definition of folksonomy]
* [http://www.vanderwal.net/random/entrysel.php?blog=1750 Vanderwal's take on Wikipedia's definition of folksonomy]
*[http://er.educause.edu/articles/2011/9/classroom-collaboration-using-social-bookmarking-service-diigo Classroom Collaboration Using Social Bookmarking Service Diigo]

{{Web syndication}}
{{Semantic Web}}

[[Category:Collective intelligence]]
[[Category:Folksonomy| ]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Social bookmarking]]
[[Category:Taxonomy]]
[[Category:Web 2.0 neologisms]]
[[Category:Sociology of knowledge]]</text>
      <sha1>b4pt0gknezyskcl56r4mrh3gu6u0fa6</sha1>
    </revision>
  </page>
  <page>
    <title>Agricultural Information Management Standards</title>
    <ns>0</ns>
    <id>5465644</id>
    <revision>
      <id>727374332</id>
      <parentid>711272018</parentid>
      <timestamp>2016-06-28T15:31:38Z</timestamp>
      <contributor>
        <username>Yaron K.</username>
        <id>2276977</id>
      </contributor>
      <comment>/* See also */ Simplified</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12595" xml:space="preserve">{{Infobox website
|name=Agricultural Information Management Standards (AIMS)
|logo =[[File:Agricultural Information Management Standards (AIMS) logo.jpg|100px|AIMS logo]]
|slogan = ''Standards, Tools, Services &amp; Advice''
|url ={{URL|http://aims.fao.org}}
|type = [[Community of Practice]]
|commercial      = No
|registration    = Optional
|language        = English
|launch date = 2006
|current status = Online
|screenshot     = 
}}

[http://aims.fao.org/ '''Agricultural Information Management Standards'''], abbreviated to '''AIMS''' is a space for accessing and discussing agricultural information management standards, tools and methodologies connecting information workers worldwide to build a global community of practice. Information management standards, tools and good practices can be found on AIMS:

* to support the implementation of structured and linked information and knowledge to enable institutions and individuals from different technical backgrounds to build open and interoperable information systems;
* to provide advice on how to best manage, disseminate, share and exchange agricultural scientific information;
* to promote good practices widely applicable and easy to implement, and;
* to foster communities of practices centered on interoperability, reusability and cooperation.

== Users ==

AIMS is primarily intended for information workers—librarians, information managers, software developers—but is also of interest to those who are simply passionate about knowledge and information sharing. The success of AIMS depends upon its communities reaching a critical mass to show that the investment in interoperability standards has a return.

== Community ==

AIMS holds [http://aims.fao.org/communities-aims 9 communities of practice]. They are intended to discuss and share information about the different ongoing initiatives under the AIMS umbrella. AIMS supports collaboration through forums and blogs amongst institutions and individuals that wish to share expertise on how to use tools, standards and methodologies. Moreover, news and events are published on AIMS as part of its ‘one-stop” access to interoperability and reusability of information resources. The AIMS communities are aimed at the global agricultural community, including information providers, from research institutes, academic institutions, educational and extension institutions and also the private sector.

== Content ==

=== Vocabularies ===

* [[AGROVOC]] is a comprehensive multilingual vocabulary that contains close to 40,000 concepts in over 20 languages covering subject fields in agriculture, forestry and fisheries together with cross-cutting themes such as land use, rural livelihoods and food security.&lt;ref&gt;{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai144e/ai144e00.pdf |title=Basic Guidelines for Managing AGROVOC|year=2008 |accessdate=2011-08-01}}&lt;/ref&gt; It standardizes data description to enable a set of core integration goals: interoperability, reusability and cooperation.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/008/af238e/af238e04.htm |title=Agricultural Information Systems and Common Exchange Standards|year=2005 |accessdate=2011-08-01}}&lt;/ref&gt; In this spirit of collaboration, [[AGROVOC]] also works with other organizations that are using [[Linked Open Data]] techniques to connect vocabularies and build the backbone of the next generation of internet data; data that is marked up not just for style but for meaning. It is maintained by a global community of librarians, terminologists, information managers and software developers&lt;ref&gt;{{cite web|url=http://aims.fao.org/standards/agrovoc/community |title=AGROVOC Community |accessdate=2011-08-01}}&lt;/ref&gt; using [http://aims.fao.org/tools/vocbench-2 VocBench], a multilingual, web-based vocabulary editor and workflow management tool that allows for simultaneous, distributed editing.&lt;ref&gt;{{cite web|url=http://aims.fao.org/tools/vocbench-2 |title=VocBench Homepage |accessdate=2011-08-01}}&lt;/ref&gt;
* In addition to AGROVOC, AIMS provides access to other vocabularies like the [[Geopolitical ontology]] and [http://aims.fao.org/standards/agvocabularies/fisheries-ontology Fisheries Ontologies]. The [[Geopolitical ontology]] is used to facilitate data exchange and sharing in a standardized manner among systems managing information about countries and/or regions. The network of fisheries ontologies was created as a part of the [http://www.neon-project.org/nw/Welcome_to_the_NeOn_Project NeOn Project] and it covers the following areas: Water areas: for statistical reporting, jurisdictional ([[EEZ]]), environmental (LME), Species: taxonomic classification, ISSCAAP commercial classification, Aquatic resources, Land areas, Fisheries commodities, Vessel types and size, Gear types, [[AGROVOC]], ASFA.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/field/009/ai254e/ai254e00.htm |title=Revised and enhanced fisheries ontologies |accessdate=2011-08-01}}&lt;/ref&gt;
* [[AgMES]] is as a namespace designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]] or [[Australian Government Locator Service|AGLS]], used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc.&lt;ref&gt;{{cite web|url=ftp://193.43.36.44/gi/gil/gilws/aims/publications/workshops/coherence0/ppt/agmes.pdf |title=Agricultural Metadata Element Set: Standardization and Information Dissemination|accessdate=2011-08-01}}&lt;/ref&gt;
* Linked Open Data (LOD) - Enabled Bibliographic Data [http://aims.fao.org/lode/bd (LODE-BD) Recommendations 2.0] are a reference tool that assists bibliographic data providers in selecting appropriate encoding strategies according to their needs in order to facilitate metadata exchange by, for example, constructing crosswalks between their local data formats and widely used formats or even with a [[Linked Data]] representation

=== Tools ===
* [http://aims.fao.org/tools/agridrupal AgriDrupal] is both a suite of solutions for agricultural information management and a community of practice around these solutions.  The AgriDrupal community is made up of people who work in the community of agricultural information management specialists and have been experimenting with IM solutions in [[Drupal]].&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/article/am642e.pdf |title=AgriDrupal: repository management integrated into a content management system |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://aims.fao.org/agriocean-dspace AgriOcean DSpace] is a joint initiative of the [[United Nations]] agencies of [[FAO]] and [[UNESCO]]-IOC/IODE to provide a customized version of [[DSpace]]. It uses standards for [[metadata]], [[thesauri]] and other [[controlled vocabularies]] for [[oceanography]], [[marine science]], food, agriculture, development, [[fisheries]], [[forestry]], [[natural resources]] and other related sciences.&lt;ref&gt;{{cite web|url=http://eprints.rclis.org/handle/10760/15812 |title=AgriOcean DSpace : FAO and UNESCO/IOC-IODE Combine Efforts in their Support of Open Access |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://aims.fao.org/tools/vocbench-2 VocBench] is a web-based multilingual vocabulary management tool developed by [[FAO]] and hosted by [[MIMOS Berhad]]. It transforms thesauri, authority lists and glossaries into [[SKOS]]/[[Resource Description Framework|RDF]] concept schemes for use in a linked data environment. VocBench also manages the workflow and editorial processes implied by vocabulary evolution such as user rights/roles, validation and versioning. VocBench  supports a growing set of user communities, including the global, distributed group of terminologists who manage [[AGROVOC]].&lt;ref&gt;{{cite web|url=http://semtech2011.semanticweb.com/uploads/handouts/MON_600_Jaques_3910.pdf |title=VocBench: vocabulary editing and workflow management |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://aims.fao.org/tools/webagris-2 WebAGRIS] is a multilingual Web-based system for distributed data input, processing and dissemination (through the Internet or on CD-Rom), of agricultural bibliographic information. It is based on common standards of data input and dissemination formats ([[XML]], [[Html|HTML]], ISO2709), as well as subject categorization schema and [[AGROVOC]].&lt;ref&gt;{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai161e/ai161e00.pdf |title=FAO’s experience in metadata exchange from CDS/ISIS bibliographic databases using XML format, compliant to Dublin Core standard |accessdate=2011-08-01}}&lt;/ref&gt;

=== Services ===
* [http://www.agrifeeds.org/ AgriFeeds] is a service that allows users to search and filter news and events from several agricultural information sources and to create custom feeds based on the filters applied.&lt;ref&gt;{{cite web|url=ftp://ftp.fao.org/docrep/fao/011/ak182e/ak182e00.pdf |title=AgriFeeds: The Agricultural News and Events Aggregator |accessdate=2011-08-01}}&lt;/ref&gt; AgriFeeds was designed in the context on [http://www.ciard.net/ CIARD] (Coherence in Information for Agricultural Research for Development). Within CIARD, the partners who designed and implemented AgriFeeds are [[FAO]] and [[Global Forum on Agricultural Research|GFAR]]. AgriFeeds is currently maintained by [[FAO]].
* [[AGRIS]] is a global public domain database with nearly 3 million structured bibliographical records on agricultural science and technology. The database is maintained by [[FAO]], with the content provided by more than 100 participating institutions from 65 countries.&lt;ref&gt;{{cite web|url=http://agris.fao.org/knowledge-and-information-sharing-through-agris-network |title=Knowledge and information sharing through the AGRIS Network |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://ring.ciard.net/ CIARD Routemap to Information Nodes and Gateways (RING)] is a project implemented within CIARD and is led by [[Global Forum on Agricultural Research|GFAR]]. The RING is a global registry of web-based services that give access to any kind of information pertaining to agricultural research for development (ARD). It allows information providers to register their services in various categories and so facilitate the discovery of sources of agriculture-related information across the world.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/012/al207e/al207e00.pdf |title=The CIARD RING, an infrastructure for interoperability of agricultural research information services |year=2010 |accessdate=2011-08-01}}&lt;/ref&gt;
* Since January 2011, AIMS supports [[E-lis|E-LIS]], the international electronic [[Open Archives Initiative|archive]] for [[Library science|library and information science]] (LIS). E-LIS is established, managed and maintained by an international team of 73 librarians and information scientists from 47 countries and support for 22 languages. It is freely accessible, aligned with the [[Open access (publishing)|Open Access]] (OA) movement and is a voluntary enterprise. Currently it is the largest international repository in the LIS field. Searching or browsing E-LIS is a kind of multilingual, multicultural experience, an example of what could be accomplished through open access archives to bring the people of the world together.&lt;ref&gt;{{cite web|url=http://eprints.rclis.org/handle/10760/6634 |title=E-LIS: an international open archive towards building open digital libraries |year=2005 |accessdate=2011-08-02}}&lt;/ref&gt;
* [http://aims.fao.org/vest-registry VEST Registry] is a catalog of controlled vocabularies (such as authority files, classification systems, [[concept maps]], controlled lists, dictionaries, [[ontologies]] or subject headings); [[metadata]] sets ([[metadata]] element sets, namespaces and application profiles); and tools (such as library management software, content management systems or document repository software). It is concerned primarily with collecting and maintaining a consistent set of [[metadata]] for each resource. The scope of the VEST Registry is to provide a clearing house for tools, [[metadata]] sets and vocabularies used in food, [[agriculture]], development, [[fisheries]], [[forestry]] and [[natural resources]] [[information management]] context.

==See also==
* [[AGRIS]]
* [[AGROVOC]]
* [[E-LIS]]
* [[IMARK]]
* [[Geopolitical ontology]]

== References ==

{{Reflist|2}}

== External links ==
* [http://aims.fao.org/home Agricultural Information Management Standards]

[[Category:Agriculture]]
[[Category:Food and Agriculture Organization]]
[[Category:Standards]]
[[Category:Information science]]
[[Category:Knowledge]]
[[Category:Knowledge representation]]
[[Category:Library science]]</text>
      <sha1>bycpu2156tett01plagopzshj2vofiw</sha1>
    </revision>
  </page>
  <page>
    <title>Document classification</title>
    <ns>0</ns>
    <id>1331441</id>
    <revision>
      <id>757804927</id>
      <parentid>754790580</parentid>
      <timestamp>2017-01-01T21:19:13Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2016 July 2|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11467" xml:space="preserve">'''Document classification''' or '''document categorization''' is a problem in [[library science]], [[information science]] and [[computer science]]. The task is to assign a [[document]] to one or more [[Class (philosophy)|classes]] or [[Categorization|categories]]. This may be done "manually" (or "intellectually") or [[algorithmically]]. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.

The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.

Documents may be classified according to their [[Subject (documents)|subjects]] or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.

=="Content-based" versus "request-based" classification==
'''Content-based classification''' is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned.&lt;ref&gt;Library of Congress (2008). The subject headings manual. Washington, DC.: Library of Congress, Policy and Standards Division. (Sheet H 180: "Assign headings only for topics that comprise at least 20% of the work.")&lt;/ref&gt; In automatic classification it could be the number of times given words appears in a document.

'''Request-oriented classification''' (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p.&amp;nbsp;230&lt;ref&gt;Soergel, Dagobert (1985). Organizing information: Principles of data base and retrieval systems. Orlando, FL: Academic Press.&lt;/ref&gt;).

Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as ''policy-based classification'': The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.

==Classification versus indexing==
Sometimes a distinction is made between assigning documents to classes ("classification") versus assigning [[Subject (documents)|subjects]] to documents ("[[subject indexing]]") but as [[Frederick Wilfrid Lancaster]] has argued, this distinction is not fruitful. "These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p.&amp;nbsp;21&lt;ref&gt;Lancaster, F. W. (2003). Indexing and abstracting in theory and practice. Library Association, London.&lt;/ref&gt;). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a [[thesaurus]] and vice versa (cf., Aitchison, 1986,&lt;ref&gt;Aitchison, J. (1986). "A classification as a source for thesaurus: The Bibliographic Classification of H. E. Bliss as a source of thesaurus terms and structure." Journal of Documentation, Vol. 42 No. 3, pp. 160-181.&lt;/ref&gt; 2004;&lt;ref&gt;Aitchison, J. (2004). "Thesauri from BC2: Problems and possibilities revealed in an experimental thesaurus derived from the Bliss Music schedule." Bliss Classification Bulletin, Vol. 46, pp. 20-26.&lt;/ref&gt; Broughton, 2008;&lt;ref&gt;Broughton, V. (2008). "A faceted classification as the basis of a faceted terminology: Conversion of a classified structure to thesaurus format in the Bliss Bibliographic Classification (2nd Ed.)." Axiomathes, Vol. 18 No.2, pp. 193-210.&lt;/ref&gt; Riesthuis &amp; Bliedung, 1991&lt;ref&gt;Riesthuis, G. J. A., &amp; Bliedung, St. (1991). "Thesaurification of the UDC." Tools for knowledge organization and the human interface, Vol. 2, pp. 109-117. Index Verlag, Frankfurt.&lt;/ref&gt;). Therefore, is the act of labeling a document (say by assigning a term from a [[controlled vocabulary]] to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).

==Automatic document classification (ADC)==
Automatic document classification tasks can be divided into three sorts: '''supervised document classification''' where some external mechanism (such as human feedback) provides information on the correct classification for documents, '''unsupervised document classification''' (also known as [[document clustering]]), where the classification must be done entirely without reference to external information, and '''semi-supervised document classification''',&lt;ref&gt;
Rossi, R. G., Lopes, A. d. A., and Rezende, S. O. (2016). Optimization and label propagation in bipartite heterogeneous networks to improve transductive classification of texts.
Information Processing &amp; Management, 52(2):217–257.
&lt;/ref&gt; where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.&lt;ref&gt;[http://www.ling.ohio-state.edu/~kbaker/Automatic_Interactive_Document_Classification.pdf An Interactive Automatic Document Classification Prototype]&lt;/ref&gt;&lt;ref&gt;[https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An Interactive Automatic Document Classification Prototype] {{wayback|url=https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An |date=20150424122349 }}&lt;/ref&gt;&lt;ref&gt;[http://www.artsyltech.com/da_classification.htmlAutomatic Document Classification - Artsyl]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;&lt;ref&gt;[http://www.abbyy.com/ocr_sdk_windows/what_is_new/classification/ ABBYY FineReader Engine 11 for Windows]&lt;/ref&gt;

== Techniques ==
Automatic document classification techniques include:
* [[Expectation maximization]] (EM)
* [[Naive Bayes classifier]]
* [[tf–idf]]
* [[Instantaneously trained neural networks]]
* [[Latent semantic indexing]]
* [[Support vector machines]] (SVM)
* [[Artificial neural network]]
* [[k-nearest neighbor algorithm|K-nearest neighbour algorithms]]
* [[Decision tree learning|Decision trees]] such as [[ID3 algorithm|ID3]] or [[C4.5 algorithm|C4.5]]
* [[Concept Mining]]
* [[Rough set]]-based classifier
* [[Soft set]]-based classifier
* [[Multiple-instance learning]]
* [[Natural language processing]] approaches

== Applications ==
Classification techniques have been applied to
* [[spam filter]]ing, a process which tries to discern [[E-mail spam]] messages from legitimate emails
* email [[routing]], sending an email sent to a general address to a specific address or mailbox depending on topic&lt;ref&gt;Stephan Busemann, Sven Schmeier and Roman G. Arens (2000). Message classification in the call center. In Sergei Nirenburg, Douglas Appelt, Fabio Ciravegna and Robert Dale, eds., Proc. 6th Applied Natural Language Processing Conf. (ANLP'00), pp. 158-165, ACL.&lt;/ref&gt;
* [[language identification]], automatically determining the language of a text
* genre classification, automatically determining the genre of a text&lt;ref&gt;{{Citation|last = Santini| first = Marina | last2 = Rosso| first2 = Mark| title = Testing a Genre-Enabled Application: A Preliminary Assessment| url = http://www.bcs.org/upload/pdf/ewic_fd08_paper7.pdf| series = BCS IRSG Symposium: Future Directions in Information Access| place = London, UK | pages= 54–63| year = 2008 }}&lt;/ref&gt;
* [[Readability|readability assessment]], automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger [[text simplification]] system
* [[sentiment analysis]], determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.
* Article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.&lt;ref&gt;{{Cite journal
 | pmid = 18834495
| year = 2008
| author1 = Krallinger
| first1 = M
| title = Overview of the protein-protein interaction annotation extraction task of Bio ''Creative'' II
| journal = Genome Biology
| volume = 9 Suppl 2
| pages = S4
| last2 = Leitner
| first2 = F
| last3 = Rodriguez-Penagos
| first3 = C
| last4 = Valencia
| first4 = A
| doi = 10.1186/gb-2008-9-s2-s4
| pmc = 2559988
}}&lt;/ref&gt;

== See also ==
{{colbegin}}
* [[Categorization]]
* [[Classification (disambiguation)]]
* [[Compound term processing]]
* [[Concept-based image indexing]]
* [[Content-based image retrieval]]
* [[Document]]
* [[Supervised learning]], [[unsupervised learning]]
* [[Document retrieval]]
* [[Document clustering]]
* [[Information retrieval]]
* [[Knowledge organization]]
* [[Knowledge Organization System]]
* [[Library classification]]
* [[Machine learning]]
* [[Native Language Identification]]
* [[String metrics]]
* [[Subject (documents)]]
* [[Subject indexing]]
* [[Text mining]], [[web mining]], [[concept mining]]
{{colend}}

== Further reading ==
* Fabrizio Sebastiani. [http://arxiv.org/pdf/cs.ir/0110053 Machine learning in automated text categorization]. ACM Computing Surveys, 34(1):1–47, 2002.
* Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, 2010.

==References==
{{Reflist}}

== External links ==
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node11.html Introduction to document classification]
* [http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html Bibliography on Automated Text Categorization]
* [http://liinwww.ira.uka.de/bibliography/Ai/query-classification.html Bibliography on Query Classification]
* [http://www.gabormelli.com/RKB/Text_Classification_Task Text Classification] analysis page
* [http://www.nltk.org/book/ch06.html Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python] (available online)
* [http://techtc.cs.technion.ac.il TechTC - Technion Repository of Text Categorization Datasets]
* [http://www.daviddlewis.com/resources/testcollections/ David D. Lewis's Datasets]
* [http://www.biocreative.org/tasks/biocreative-iii/ppi/ BioCreative III ACT (article classification task) dataset]

[[Category:Information science]]
[[Category:Natural language processing]]
[[Category:Knowledge representation]]
[[Category:Data mining]]
[[Category:Machine learning]]</text>
      <sha1>t6n9nih5otbwrayqiruc73uhw9opbhw</sha1>
    </revision>
  </page>
  <page>
    <title>LEAP (programming language)</title>
    <ns>0</ns>
    <id>911672</id>
    <revision>
      <id>467661561</id>
      <parentid>458626698</parentid>
      <timestamp>2011-12-25T20:14:23Z</timestamp>
      <contributor>
        <username>AHMartin</username>
        <id>3932984</id>
      </contributor>
      <comment>Add: description of triples; Ext link: SAIL User Manual; Cat: Knowledge representation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1375" xml:space="preserve">{{Unreferenced|date=November 2007}}
'''LEAP''' is an extension to the [[ALGOL 60]] '''[[programming language]]''' which provides an associative memory of triples. The three items in a triple denote the association that an Attribute of an Object has a specific Value.  LEAP was created by Jerome Feldman (University of California Berkeley) and Paul Rovner (MIT Lincoln Lab) in 1967.  LEAP was also implemented in [[SAIL (programming language)|SAIL]].

==References==
* Feldman, Jerome A. and Paul D. Rovner (Jan, 1968).  "The LEAP language and data structure", MIT Lincoln Laboratory, Lexington, MA.  In abbreviated form, ''Proc. 1968 IFIP Congress'', Brandon Systems Press, Princeton, NJ.
* Feldman, Jerome A. and Paul D. Rovner (Aug, 1969).  [http://portal.acm.org/citation.cfm?doid=363196.363204 "An ALGOL-based associative language"], ''Communications of the ACM'', 12:8, pp 439 - 449.
* Rovner, Paul D (Dec, 1968).  "The LEAP users manual", MIT Lincoln Laboratory, Lexington, MA.
* [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/73/373/CS-TR-73-373.pdf VanLehn, Kurt A. (Jul, 1973).  "SAIL User Manual", Stanford Artificial Intelligence Laboratory, Stanford, CA.]

[[Category:Structured programming languages]]
[[Category:Procedural programming languages]]
[[Category:Programming languages created in 1967]]
[[Category:Knowledge representation]]


{{compu-lang-stub}}</text>
      <sha1>gi0f3jyiez9bla5k7qloa4nzdjdiz6f</sha1>
    </revision>
  </page>
  <page>
    <title>POSC Caesar</title>
    <ns>0</ns>
    <id>23872172</id>
    <revision>
      <id>723650021</id>
      <parentid>708161192</parentid>
      <timestamp>2016-06-04T09:07:29Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <comment>[[User:Green Cardamom/WaybackMedic|WaybackMedic]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16664" xml:space="preserve">{{advert|date=September 2011}}

{{Infobox Organization
|name         = POSC Caesar Association
|image        = POSC Caesar logo.gif
|size         = 200
|alt          = Logo for POSC Caesar Association.
|caption      = Logo for POSC Caesar Association.
|abbreviation = PCA
|formation    = 1997-11-14
|status       = Association
|purpose      = Promote the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters
|location     = Bærum, Norway
|region_served = Worldwide
|membership   = 36
|language     = English
|leader_title = General Manager
|leader_name  = Nils Sandsmark
|main_organ   = Board of Directors
|affiliations = &lt;!-- if any --&gt;
|num_staff    =
|num_volunteers =
|budget       =
|website      = http://www.posccaesar.org
}}
'''POSC Caesar Association''' (PCA) is an international, open, [[Non-profit organization|not-for-profit]], member organization that promotes the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters.

PCA is the initiator of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities" and is committed to its maintenance and enhancement.

Nils Sandsmark has been the General Manager of POSC Caesar Association since 1999&lt;ref name="BioSandsmarkDaraTech"&gt;
{{cite web
|url=http://www.daratech.com/plant2007/bios/sandsmark_nils.html
|title=Biography Dr. Ing. Nils Sandsmark
|author=Daratech
|accessdate=2009-08-11
}}&lt;/ref&gt; and Thore Langeland, [[Norwegian Oil Industry Association]] ({{lang-no|Oljeindustriens Landsforening}}, OLF), is the Chairman of the Board.

== History ==

=== Caesar Offshore ===
The first predecessor of POSC Caesar Association, the '''Caesar Offshore program''', started in 1993.&lt;ref name="PCAHis"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/History
|title=History of POSC Caesar
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-05
}}&lt;/ref&gt;&lt;ref name=POSCStatus98&gt;{{cite web
  | title = POSC/CAESAR project - Information and Status - January 1998
  | publisher = Petrotechnical Open Software Corporation (POSC)
  | date = 1998-03-17
  | url = http://posc.org/caesar/caesar_info.html
  | accessdate = 2009-08-06}}&lt;/ref&gt;&lt;ref name=Pein&gt;{{cite book
  | last = Pein
  | first = Martin
  | title = On data models, their transformations and consistency preserving programming interfaces
  | publisher = Books on Demand GmbH
  | year = 2002
  | location = Norderstedt, Germany
  | pages = 228
  | url = http://www.amazon.com/transformations-consistency-preserving-programming-interfaces/dp/3831139288/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1249538783&amp;sr=1-1
  | isbn = 3-8311-3928-8}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
  | title = Data warehouse manages offshore project information
  | journal = Oil &amp; Gas Journal
  | volume = 96
  | issue = 18
  | pages = 94
  | url = http://www.ogj.com/index/current-issue/s-oil-gas-journal/s-volume-96/s-issue-18.html
  | publisher = Pennwell Corporation
  | location = Tulsa, OK
  | date = 1998-05-04
  | issn = 0030-1388
}}&lt;/ref&gt;&lt;ref name=Stanley&gt;{{cite web
  | last = Port
  | first = Stanley
  | title = Plant Information Management at Statoil Norway
  | date = 1998-04-13
  | url = http://www.hydrocarbononline.com/article.mvc/Plant-Information-Management-at-Statoil-Norwa-0001
  | accessdate = }}&lt;/ref&gt;
The original focus was on standardizing technical data definitions for capital intensive projects at the handover from the [[EPC (contract)|EPC]] contractor to the owner/operators of onshore and offshore oil and gas production facilities. The program was sponsored by [[The Research Council of Norway]], two [[EPC (contract)|EPC]] contractors ([[Aker Maritime]] and [[Kværner]]), three owners/operators ([[Norsk Hydro]], [[Saga Petroleum]] and [[Statoil]]) and [[DNV]] as service provider and project owner.

=== POSC Caesar project ===
During the period 1994-96, Caesar Offshore Program was defined as a project of [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]), and changed its name to the '''POSC Caesar Project'''.&lt;ref name="PCAHis"/&gt;&lt;ref name=POSCStatus98/&gt;&lt;ref name="Pein"/&gt;&lt;ref name="Stanley"/&gt;

In 1995 the project was joined by [[BP]], [[Brown and Root]] and [[Elf Aquitaine]] and in 1997 by [[Intergraph]], [[IBM]], [[Oracle Corporation|Oracle]], [[Lloyd's]], [[Royal Dutch Shell|Shell]], [[ABB Group|ABB]] and [http://www.umoe.no/ UMOE Technologies].&lt;ref name=POSCStatus98/&gt;

During that time, POSC Caesar also became a member of European Process Industries STEP Technical Liaison Executive (EPISTLE) where it collaborates with PISTEP (UK), and USPI-NL (The Netherlands) on the development of [[ISO 10303]], also known as "Standard for the Exchange of Product model data (STEP)."

=== POSC Caesar Association ===
In 1997, POSC Caesar Association was founded as an independent, global, non-profit, member organization. POSC Caesar Association serves an international membership and collaborates with other international organizations. It has its main office in Norway.

Albeit the name of POSC Caesar Association still hints to its past as a project within the [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]&lt;ref name="POSCEnergistics"&gt;
{{cite web
|url=http://www.energistics.org/posc/NewsBot.asp?MODE=VIEW&amp;ID=606&amp;SnID=2
|title=POSC Rebrands as Energistics - Press release
|author=Energistics
|accessdate=2009-09-01
|date=2006-11-09
}}&lt;/ref&gt;), from 1997 onwards, the organization has been independent. Energistics and POSC Caesar Association do collaborate, and are formally member in each other's organization.&lt;ref name="EnergisticsMembers"&gt;
{{cite web
|url=http://www.energistics.org//assnfe/companydirectory.asp?MODE=FINDRESULTS&amp;SEARCH_TYPE=1&amp;COMPANY_TYPE=2,3,6&amp;SnID=1941691677
|title=Energistics Member Directory
|author=Energistics
|accessdate=2009-08-11
}}&lt;/ref&gt;&lt;ref name="PCAMembers"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Membership
|title=POSC Caesar Association Membership
|author=POSC Caesar Association
|accessdate=2009-08-11
}}&lt;/ref&gt;

== Membership ==
POSC Caesar Association has with its current 36 members&lt;ref name="PCAMem"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Membership
|title=POSC Caesar Association - Membership
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-13
}}&lt;/ref&gt; from around the world established an international footprint (with a strong membership in Norway) that includes a wide range from academia, solution providers to engineering contractors and owners/operators. The members are (subdivided by organization type):
* Associations: Energistics (USA) and [[Norwegian Oil Industry Association|The Norwegian Oil Industry Association (OLF, Norway)]],
* [[Universities]] and [[Research institute|Research Institutes]]: International Research Institute of Stavanger (IRIS, Norway), [[Norwegian University of Science and Technology|Norwegian University of Science and Technology (NTNU, Norway)]], [[KAIST|Korea Advanced Institute of Science and Technology (KAIST, Korea)]], [[SINTEF|SINTEF (Norway)]], [[University of Bergen|University of Bergen (Norway)]], [[University of Oslo|University of Oslo (Norway)]], [[University of Stavanger|University of Stavanger (Norway)]], [[University of Tromsø|University of Tromsø (Norway)]] and Western Norway Research Institute (Norway),
* [[Petroleum Industry|Oil and Gas Companies]]: [[BP|BP (UK)]], [[Petronas|Petronas (Malaysia)]] and [[Statoil|Statoil (Norway)]],
* Engineering contractors and consultants:  Akvaplan-niva (Norway), [[Aker Solutions|Aker Solutions (Norway)]], Asset Life Cycle Information Management (ALCIM, Malaysia), CAESAR systems (USA), [[Bechtel|Bechtel (USA)]], [[Det Norske Veritas|Det Norske Veritas (DNV, Norway)]], Information Logic (USA) and iXIT Engineering Technology (Germany), Phusion IM Ltd (UK).&lt;ref&gt;https://www.posccaesar.org/wiki/PCA/Membership&lt;/ref&gt;
* Solution providers: [[Aveva|Aveva (UK)]], [[Bentley Systems|Bentley Systems (USA)]], Jotne EPM Technology (Norway), Epsis (Norway), Eurostep (Sweden), [[IBM|International Business Machines Corporation (IBM, USA)]], Siemens - Comos Industry Solutions (before Innotec) (Germany), [[Intergraph|Intergraph (USA)]], Invenia (Norway), Keel Solution (Denmark), Noumenon (UK), NRX (Canada), Octaga (Norway) and Tektonisk (Norway).

In general, the organization holds three membership meetings a year;&lt;ref name="PCAAgenda"&gt;{{cite web|url=http://www.posccaesar.org/wiki/PCA/Agenda |title=POSC Caesar Association - Agenda |author=POSC Caesar Association (PCA) |accessdate=2009-08-13 }}{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}&lt;/ref&gt; one in January / February in North-America (typically USA), one in April / May in Europe (typically Norway) and one in October in Asia (typically Malaysia).

== Activities and services ==

=== Initiator and custodian of ISO 15926 ===

In consultation with the other EPISTLE members and the [[International Organization for Standardization|International Organization for Standardization (ISO)]], it was decided in 2003 (some say already in 1997{{Citation needed|date=August 2009}}) that for modeling-technical reasons it was better to discontinue the development of [[ISO 10303]]&lt;ref name="SC4Legacy"&gt;
{{cite web
|url=http://www.tc184-sc4.org/SC4_Open/SC4%20Legacy%20Products%20(2001-08)/STEP_(10303)/
|title=ISO - SC4- Legacy products - STEP (ISO 10303)
|author=ISO
|accessdate=2009-08-05
}}&lt;/ref&gt; and to initiate the development of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities."

Over the years, the scope of the standard has increased from the initial capital-intensive projects in the [[Upstream (oil industry)|upstream oil and gas industry]], to include also relevant terminology for [[Downstream (oil industry)|downstream oil and gas industry]] applications and to deal with real-time data related to the actual oil and gas production.

[[ISO 15926]] has also over the years evolved from a dictionary (a list of terms with definitions), over a [[Taxonomy (general)|taxonomy]] (added hierarchy) to an [[Ontology (information science)|ontology]] (a formal representation of a set of concepts within a domain and the relationships between those concepts). [[ISO 15926]] is therefore sometimes nicknamed the "Oil and Gas Ontology.",&lt;ref name="OLFIOOntology"&gt;
{{cite web
|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf
|title=Integrated Operations and the Oil &amp; Gas Ontology
|author=[[Norwegian Oil Industry Association]] (OLF)
|accessdate=2009-08-05
}}&lt;/ref&gt; for some considered to be an essential prerequisite together with [[Semantic Web]] technologies&lt;ref name="W3C"&gt;
{{cite web
|url=http://www.w3.org/2008/12/ogws-report
|title=W3C workshop on Semantic Web in Oil and Gas Industry - Report
|author=[[W3C]]
|date=2009-01-13
|accessdate=2009-08-05
}}&lt;/ref&gt;
to get to better interoperability, an optimal use of all available data across boundaries and an increase in efficiency. This is what some call the next generation of [[Integrated Operations]].&lt;ref name="OLFIOOntology"/&gt;

=== Reference data services ===
Placeholders:
* Flow scheme of WIP - RDS - ISO and role of SIGs
* RDS
* Standards in database pilot (ISO)

=== Special interest groups ===
Placeholders:
* Overview of SIGs
* Drilling and Completion
* Reservoir and Production
* Operations and Maintenance

== Projects ==
There are a number of projects (co-)organized by POSC Caesar Association working on the extension of the [[ISO 15926]] standard in different application areas.

=== Capital intensive projects application domain ===
The following projects are running at the moment (August 2009):

* The ADI Project of FIATECH, to build the tools (which will then be made available in the public domain)
* The IDS Project of POSC Caesar Association, to define product models required for data sheets
* A joint collaboration project between FIATECH POSC Caesar Association is the ADI-IDS project is the [[ISO 15926 WIP]]

=== Upstream oil and gas industry application domain ===
The following projects are currently running (August 2009):

* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].&lt;ref&gt;
{{cite web
|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html
|title=Norway takes a leading role in next generation Integrated Operations
|author=The [[Norwegian Oil Industry Association]] (OLF)
|accessdate=2009-08-05
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="Rigzone"&gt;
{{cite web
|url=http://www.rigzone.com/news/article.asp?a_id=65883
|title=Norway Takes Reign to Provide Next Generation Integrated Operations
|author=Rigzone E&amp;P News
|accessdate=2009-08-05
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="DEJ"&gt;
{{cite web
|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&amp;PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7
|title=Norway developing next generation Integrated Operations
|author=Digital Energy Journal
|accessdate=2009-08-05
|date=2008-08-27
}}&lt;/ref&gt;&lt;ref name="EPMag"&gt;
{{cite web
|url=http://www.epmag.com/Magazine/2008/12/item24047.php
|title=Offshore R&amp;D pushes the limits
|author=E&amp;P Magazine
|accessdate=2009-08-05
|date=2008-12-02
}}&lt;/ref&gt;
* The Environment Web project to include environmental reporting terms and definitions as used in EPIM's EnvironmentWeb in ISO 15926.

Finalised projects include:

* The Integrated Information Platform (IIP) project working on establishing a real-time information pipeline based on open standards. It worked among others on:
** Daily Drilling Report (DDR) to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008&lt;ref&gt;{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-08-05 }}&lt;/ref&gt; for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and Safety Authority Norway (PSA). NPD says that the quality of the reports has improved considerably since.
** Daily Production Report (DPR) to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and Åsgard ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]’ [[PRODML]] standard.

== Conferences and events ==

=== Semantic Days ===
{{Empty section|date=January 2011}}

=== Sogndal academic network meeting ===
{{Empty section|date=January 2011}}

== Collaborations ==

POSC Caesar is collaborating with a number of standardization bodies,&lt;ref name="PCAColl"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Collaboration
|title=POSC Caesar - Collaboration
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-05
}}&lt;/ref&gt; including:
* Mimosa: collaboration on open information standards for Operations and Maintenance mainly for the [[Downstream (oil industry)|downstream oil and gas industry]],
* FIATECH: collaboration on open information standards for life cycle data of capital projects&lt;ref name="IDSADI"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/IdsAdiProject
|title=POSC Caesar / FIATECH IDS-ADI Projects
|author=FIATECH &amp; POSC Caesar Association
|accessdate=2009-08-05
}}&lt;/ref&gt;&lt;ref name="iRing"&gt;
{{cite press release
|url=http://fiatech.org/press-releases/364-iring-version100.html
|title=iRING Version 1.0.0 Available Now
|publisher=FIATECH &amp; POSC Caesar Association
|accessdate=2009-08-05
|date=2009-06-05
}}&lt;/ref&gt;
* Energistics: collaboration on information standards for the [[Upstream (oil industry)|upstream oil and gas industry]], including [[WITSML]] and [[PRODML]]
* OASIS: collaboration on e-business standards,
* [[ISO TC 184/SC 4|ISO TC184/SC4]]: the host of the ISO 15926 standard.

== See also ==
* [[ISO 15926]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://www.posccaesar.org/ POSC Caesar Association] website

[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Standards organizations]]</text>
      <sha1>eqmoynir86it45s5c9oxvq1xwvkuh7r</sha1>
    </revision>
  </page>
  <page>
    <title>GermaNet</title>
    <ns>0</ns>
    <id>33768132</id>
    <revision>
      <id>745608935</id>
      <parentid>663113959</parentid>
      <timestamp>2016-10-22T05:10:40Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4768" xml:space="preserve">{{primary sources|date=November 2011}}
'''GermaNet''' is a lexical-semantic net for the [[German language]] that relates [[noun]]s, [[verb]]s, and [[adjective]]s semantically by grouping lexical units that express the same concept into ''[[synset]]s'' and by defining [[semantic]] relations between these synsets.&lt;ref name="Storjohann2010"&gt;{{cite book|author=Petra Storjohann|title=Lexical-semantic relations: theoretical and practical perspectives|url=https://books.google.com/books?id=OYBWObJ547AC&amp;pg=PA165|accessdate=16 November 2011|date=23 June 2010|publisher=John Benjamins Publishing Company|isbn=978-90-272-3138-3|pages=165–}}&lt;/ref&gt; GermaNet has much in common with the English [[WordNet]] and can be viewed as an on-line [[thesaurus]] or a light-weight [[ontology (information science)|ontology]]. GermaNet has been developed and maintained within various projects at the research group for General and Computational Linguistics, [[University of Tübingen]] since 1997. It has been integrated into the [[EuroWordNet]], a multilingual lexical-semantic database.&lt;ref name="homepage"&gt;[http://www.sfs.uni-tuebingen.de/lsd/index.shtml GermaNet homepage]&lt;/ref&gt;

==Database==

===Contents===
GermaNet  partitions the lexical space into a set of concepts that are interlinked by semantic relations. A semantic concept is modeled by a ''[[synset]]''. A synset is a set of words (called lexical units) where all the words are taken to have (almost) the same meaning. Thus a synset is a set-representation of the semantic relation of synonymy, which means that it consists of a list of lexical units and a definition (paraphrase). The lexical units in turn have frames (which specify syntactic valence) and examples of their use.&lt;ref name="GernEdiT"&gt;V. Henrich, E. Hinrichs. 2010. [http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf GernEdiT - The GermaNet Editing Tool]. In: ''Proceedings of the Seventh Conference on International Language Resources and Evaluation''.&lt;/ref&gt;
Just as in WordNet, for each word category the semantic space is divided into a number of [[semantic field]]s closely related to major nodes in the semantic network: ''Ort'', or "location", ''Körper'', or "body", etc.&lt;ref name="homepage"/&gt;

The following is an up-to-date statistics of GermaNet's version 6.0 contents (release April 2011):
 
*Number of synsets: 69594
**Of which adjectives: 5991
**Of which nouns: 53753
**Of which verbs: 9850
*Number of lexical units: 93407
**Of which adjectives: 8582
**Of which nouns: 71844
**Of which verbs: 12981 &lt;ref name="homepage"/&gt;

===Format===
All GermaNet data is stored in a relational [[PostgreSQL]] 5 database. The database model follows the internal structure of GermaNet: there are tables to store synsets, lexical units, conceptual and lexical relations, etc.&lt;ref name="GernEdiT"/&gt; The distribution format of all GermaNet data is [[XML]]. The two types of files, one for synsets and the other for relations, represent all data that is available in the GermaNet database.

==Interfaces==
There are several [[Application Programming Interface]]s (API) available for [[Java (programming language)|Java]]&lt;ref name="api"&gt;[http://www.sfs.uni-tuebingen.de/lsd/tools.shtml GermaNet APIs in Java]&lt;/ref&gt; and for [[Perl]]. These APIs are distributed freely and provide easy access to all information in various versions of GermaNet.

==Licenses==
GermaNet 6.0 (released April 2011) can be distributed under one of the following types of [[software license agreement|license agreements]]: ''Academic Research Agreement'', ''Research and Development Agreement'', or ''Commercial Agreement''. GermaNet is free for academic use.

==Applications==
GermaNet has been used for a variety of applications, including semantic analysis, shallow recognition of implicit document structure, compound analysis;&lt;ref&gt;Manuela Kunze and Dietmar Rösner. 2004. Issues in Exploiting GermaNet as a Resource in Real Applications.&lt;/ref&gt; for analyzing selectional preferences,&lt;ref&gt;Sabine Schulte im Walde, 2004. GermaNet Synsets as Selectional Preferences in Semantic Verb Clustering.&lt;/ref&gt; for word sense disambiguation,&lt;ref&gt;Saito et al., 2002. Evaluation of GermanNet: Problems Using GermaNet for Automatic Word Sense Disambiguation.&lt;/ref&gt; etc.
== See also==
* [[Hyponym]]
* [[Is-a]]
* [[Machine-readable dictionary]]
* [[Ontology (information science)]]
* [[Semantic network]]
* [[Semantic Web]]
* [[Synonym Ring]]
* [[Taxonomy (general)|Taxonomy]]
* [[ThoughtTreasure]]
* [[UBY-LMF]]
* [[Word sense disambiguation]]

==References==
{{Reflist}}

{{Authority control}}
[[Category:German language]]
[[Category:Thesauri]]
[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Online dictionaries]]</text>
      <sha1>hwob5wucbvcfg0nomqpdp24gzznin6c</sha1>
    </revision>
  </page>
  <page>
    <title>BabelNet</title>
    <ns>0</ns>
    <id>37291130</id>
    <revision>
      <id>759335612</id>
      <parentid>749197036</parentid>
      <timestamp>2017-01-10T15:52:34Z</timestamp>
      <contributor>
        <username>Jona</username>
        <id>330665</id>
      </contributor>
      <minor />
      <comment>/* Applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7445" xml:space="preserve">{{Infobox software
 |name = BabelNet
 |logo = [[File:BabelNet_logo.svg|140px|BabelNet logo.]]
 |screenshot =
 |caption = Wikipedia Extraction
 |developer = 
 |released = 
 |latest_release_version = BabelNet 3.7
 |latest_release_date = August 2016
 |operating_system = {{Flatlist|
* [[Virtuoso Universal Server]]
* [[Lucene]]
}}
 |genre = {{Flatlist|
* [[Machine-readable dictionary|Multilingual encyclopedic dictionary]]
* [[Linked data]]
}}
 |programming language = 
 |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]
 |website = {{URL|babelnet.org}}
 |alexa   = 
}}

'''BabelNet''' is a [[Multilinguality|multilingual]] lexicalized [[semantic network]] and [[Ontology (information science)|ontology]] developed at the Linguistic Computing Laboratory in the Department of Computer Science of the [[Sapienza University of Rome]].&lt;ref name="NavigliPonzetto12"&gt;R. Navigli and S. P Ponzetto. 2012. [http://dx.doi.org/10.1016/j.artint.2012.07.001 BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network]. Artificial Intelligence, 193, Elsevier, pp. 217-250.&lt;/ref&gt;&lt;ref&gt;R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11–16, 2010, pp. 216–225.&lt;/ref&gt; BabelNet was automatically created by linking Wikipedia, to the most popular computational [[Dictionary|lexicon]] of the [[English language]], [[WordNet]]. The integration is performed by means of an automatic mapping and by filling in lexical gaps in resource-poor [[language]]s with the aid of [[statistical machine translation]]. The result is an "encyclopedic dictionary" 
that provides [[concept]]s and [[Named entity|named entities]] [[Lexicalization|lexicalized]] in many languages and connected with large amounts of [[Semantic relation#Relationships|semantic relations]]. Additional lexicalizations and definitions are added by linking to free-license wordnets, [[OmegaWiki]], the English [[Wiktionary]], [[Wikidata]], [[FrameNet]], [[VerbNet]] and others. Similarly to WordNet, BabelNet groups [[word]]s in different languages into sets of [[synonyms]], called ''Babel [[synsets]]''. For each Babel synset, BabelNet provides short definitions (called [[Definition|glosses]]) in many languages harvested from both WordNet and Wikipedia.

[[File:The BabelNet structure.png|thumb|600px|BabelNet is a multilingual semantic network obtained as an integration of WordNet and Wikipedia.]]

==Statistics of BabelNet==

{{As of|2016|08}}, BabelNet (version 3.7) covers 271 [[language]]s, including all European languages, most [[Asian language]]s, and [[Latin]]. BabelNet 3.7 contains almost 14 million synsets and about 746 million [[word sense]]s (regardless of their language). Each Babel synset contains 2 synonyms per language, i.e., word senses, on average. The semantic network includes all the lexico-semantic relations from WordNet ([[Hyponymy|hypernymy and hyponymy]], [[meronymy]] and [[holonymy]], [[antonymy]] and [[synonymy]], etc., totaling around 364,000 relation edges) as well as an underspecified relatedness relation from Wikipedia (totaling around 380 million relation edges).&lt;ref name="NavigliPonzetto12" /&gt; Version 3.7 also associates about 11 million images with Babel synsets and provides a Lemon [[Resource Description Framework|RDF]] encoding of the resource,&lt;ref&gt;M. Ehrmann, F. Cecconi, D. Vannella, J. McCrae, P. Cimiano, R. Navigli. [http://www.lrec-conf.org/proceedings/lrec2014/pdf/810_Paper.pdf Representing Multilingual Data as Linked Data: the Case of BabelNet 2.0]. Proc. of the 9th Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, 26–31 May 2014.&lt;/ref&gt; available via a [[SPARQL endpoint]]. 2.67 million synsets are assigned domain labels.

==Applications==

BabelNet has been shown to enable multilingual [[Natural Language Processing]] applications. The lexicalized [[knowledge]] available in BabelNet has been shown to obtain state-of-the-art results in:

* [[semantic relatedness]]&lt;ref&gt;R. Navigli and S. Ponzetto. 2012. [http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/download/5112/5126 BabelRelate! A Joint Multilingual Approach to Computing Semantic Relatedness]. Proc. of the 26th AAAI Conference on Artificial Intelligence (AAAI 2012), Toronto, Canada, pp. 108-114.&lt;/ref&gt;&lt;ref&gt;J. Camacho-Collados, M. T. Pilehvar and R. Navigli. [http://aclweb.org/anthology/N/N15/N15-1059.pdf NASARI: a Novel Approach to a Semantically-Aware Representation of Items]. Proc. of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2015), Denver, Colorado (US), 31 May-5 June 2015, pp. 567-577.&lt;/ref&gt;
* multilingual [[Word Sense Disambiguation]]&lt;ref&gt;R. Navigli and S. Ponzetto. [http://www.aclweb.org/anthology/D12-1128 Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation]. Proc. of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP 2012), Jeju, Korea, July 12–14, 2012, pp. 1399-1410.&lt;/ref&gt;
* multilingual Word Sense Disambiguation and [[Entity Linking]] with the [[Babelfy]] system&lt;ref&gt;A. Moro, A. Raganato, R. Navigli. [http://www.transacl.org/wp-content/uploads/2014/05/54.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.&lt;/ref&gt;
* [[game with a purpose|video games with a purpose]]&lt;ref&gt;D. Jurgens, R. Navigli. {{webarchive|url=http://web.archive.org/web/20150103085712/http://www.transacl.org/wp-content/uploads/2014/10/421-camera-ready.pdf |date=January 3, 2015 |title=It's All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation}}. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 449-464, 2014.&lt;/ref&gt;

==Prizes and acknowledgments==
BabelNet received the [http://www.meta-net.eu/meta-prize META prize] 2015 for "groundbreaking work in overcoming language barriers through a multilingual lexicalised semantic network and ontology making use of heterogeneous data sources". 

BabelNet featured prominently in a [[TIME magazine]]'s article&lt;ref&gt;Katy Steinmetz. [http://time.com/4327440/redefining-the-modern-dictionary/ Redefining the modern dictionary], TIME magazine, vol. 187, 23 maggio 2016, pp. 20-21.&lt;/ref&gt; about the new age of innovative and up-to-date lexical knowledge resources available on the Web. The article describes in some detail how BabelNet is playing a leading role in the 21st century scenario.

==See also==
* [[Babelfy]]
* [[EuroWordNet]]
* [[Knowledge acquisition]]
* [[Linguistic Linked Open Data]]
* [[OmegaWiki]]
* [[Semantic network]]
* [[Semantic relatedness]]
* [[Wikidata]]
* [[Wiktionary]]
* [[Word sense disambiguation]]
* [[Word sense induction]]
* [[UBY]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://babelnet.org}}
* [http://babelnet.org/sparql SPARQL endpoint]
* [http://www.meta-net.eu/meta-prize META prize]

[[Category:Lexical databases]]
[[Category:Knowledge bases]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Artificial intelligence]]
[[Category:Online dictionaries]]
[[Category:Multilingualism]]</text>
      <sha1>bpdwdc9uxj2vfrpe73qhdby58hkkunx</sha1>
    </revision>
  </page>
  <page>
    <title>Composite Capability/Preference Profiles</title>
    <ns>0</ns>
    <id>4476270</id>
    <revision>
      <id>544325318</id>
      <parentid>535501541</parentid>
      <timestamp>2013-03-15T09:25:47Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2990630]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1836" xml:space="preserve">'''Composite Capability/Preference Profiles''' ('''CC/PP''') is a specification for defining capabilities and preferences (also known as 'delivery context') of [[user agents]]. CC/PP is a [[vocabulary extension]] of the [[Resource Description Framework]] (RDF). Delivery context can be used to guide the process of tailoring content for a [[user agent]].

The CC/PP specification is maintained by the [[World Wide Web Consortium|W3C]]'s [[UWAWG|Ubiquitous Web Applications Working Group (UWAWG)]] Working Group.

== History ==
* Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0 became a W3C recommendation on 15 January 2004.
* A "Last-Call Working-Draft" of CC/PP 2.0 was issued in April 2007

== See also ==
* [[Resource Description Framework|Resource Description Framework (RDF)]]
* [[UAProf|User Agent Profile (UAProf)]]
* [[WURFL|Wireless Universal Resource File (WURFL)]]

== External links ==
* [http://www.w3.org/Mobile/CCPP/ W3C CC/PP Information Page]
* [http://www.w3.org/TR/CCPP-struct-vocab2/ Newest version of CC/PP: Structure and Vocabularies]
* [http://www.w3.org/TR/2004/REC-CCPP-struct-vocab-20040115/ Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0]
* [http://www.w3.org/2001/di/ W3C Device Independence working group]
* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite/ CC/PP: Structure and Vocabularies Test Suite]
* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite-20030827/implementation-report.html CC/PP: Structure and Vocabularies Implementation Report]
* [http://www.w3.org/Consortium/Offices/Presentations/CCPP/ CC/PP presentation]
* [http://java.sun.com/j2ee/ccpp/ Sun J2EE CC/PP Processing Tools]

{{DEFAULTSORT:Composite Capability Preference Profiles}}
[[Category:Knowledge representation]]
[[Category:World Wide Web Consortium standards]]</text>
      <sha1>4gedybyq2vokvq5wvhiel7cp1qfgqah</sha1>
    </revision>
  </page>
  <page>
    <title>Conceptualization (information science)</title>
    <ns>0</ns>
    <id>38982174</id>
    <revision>
      <id>725295976</id>
      <parentid>722584061</parentid>
      <timestamp>2016-06-14T19:50:31Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <comment>refs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11584" xml:space="preserve">[[File:Ontological commitments.png|thumb|200px|Chart showing the relation between a conceptualization in information science, its various ontologies (each with its own specialized language), and their shared ontological commitment.&lt;ref name=CZ&gt;
This figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&amp;pg=PA7&amp;lpg=PA7#v=onepage&amp;q&amp;f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].
&lt;/ref&gt;]]
In [[information science]] a '''conceptualization''' is an abstract simplified view of some selected part of the world, containing the objects, concepts, and other entities that are presumed of interest for some particular purpose and the relationships between them.&lt;ref name=Gruber/&gt;&lt;ref name=Smith/&gt; An explicit specification of a conceptualization is an [[ontology (information science)|ontology]], and it may occur that a conceptualization can be realized by several distinct ontologies.&lt;ref name=Gruber/&gt;  An ''[[ontological commitment]]'' in describing ontological comparisons is taken to refer to that subset of elements of an ontology shared with all the others.&lt;ref name=Audi/&gt;&lt;ref name=Ceccaroni1/&gt;  "An ontology is ''language-dependent''", its objects and interrelations described within the language it uses,  while a conceptualization is always the same, more general, its concepts existing "independently of the language used to describe it".&lt;ref name=Guarino/&gt; The relation between these terms is shown in the figure to the right.

Not all workers in [[knowledge engineering]] use the term ‘conceptualization’, but instead refer to the conceptualization itself, or to the ontological commitment of all its realizations, as an overarching ontology.&lt;ref name=Ceccaroni/&gt;

==Purpose and implementation==
As a higher level abstraction, a conceptualization facilitates the discussion and comparison of its various ontologies, facilitating knowledge sharing and reuse.&lt;ref name=Ceccaroni/&gt;&lt;ref name=Harmelen/&gt; Each ontology based upon the same overarching conceptualization maps the conceptualization into specific elements and their relationships.

The question then arises as to how to describe the 'conceptualization' in terms that can encompass multiple ontologies. This issue has been called the '[[Tower of Babel]]' problem, that is, how can persons used to one ontology talk with others using a different ontology?&lt;ref name=Smith/&gt;&lt;ref name=Harmelen/&gt; This problem is easily grasped, but a general resolution is not at hand. It can be a 'bottom-up' or a 'top-down' approach, or something in between.&lt;ref name=Alignment/&gt;

However, in more artificial situations, such as information systems, the idea of a 'conceptualization' and the 'ontological commitment' of various ontologies that realize the 'conceptualization' is possible.&lt;ref name=Guarino/&gt;&lt;ref name=Guarino1/&gt; The formation of a conceptualization and its ontologies involves these steps:&lt;ref name=Hadzic/&gt;
* specification of the conceptualization
* ontology concepts: every definition involves the definitions of other terms
* relationships between the concepts: this step maps conceptual relationships onto the ontology structure
* groups of concepts: this step may lead to the creation of sub-ontologies
* formal description of ontology commitments, for example, to make them computer readable

An example of moving conception into a language leading to a variety of ontologies is the expression of a process in [[pseudocode]] (a strictly structured form of ordinary language) leading to implementation in several different formal computer languages like [[Lisp (programming language)|Lisp]] or [[Fortran]]. The pseudocode makes it easier to understand the instructions and compare implementations, but the formal languages make possible the compilation of the ideas as computer instructions. {{citation needed|date=August 2013}}

Another example is mathematics, where a very general formulation (the analog of a conceptualization) is illustrated with 'applications' that are more specialized examples. For instance, aspects of a [[function space]] can be illustrated using a [[vector space]] or a [[topological space]] that introduce interpretations of the 'elements' of the conceptualization and additional relationships between them but preserve the connections required in the [[function space]]. {{citation needed|date=August 2013}}

==See also==
*[[Knowledge representation and reasoning]]
*[[Ontology alignment]]
*[[Ontology (computer science)]]
*[[Semantic integration]]
*[[Semantic matching]]
*[[Semantic translation]]

==References==
{{reflist |30em|refs=
&lt;ref name=Alignment&gt;
In information science, one approach to finding a conceptualization (or avoiding it and using an automated comparison) is called 'ontology alignment' or 'ontology matching'. See for example, {{cite book |title=Ontology Matching |url=https://books.google.com/books?id=qYVpA2t2EtQC&amp;printsec=frontcover  |author1=Jérôme. Euzenat |author2=Pavel Shvaiko |isbn=3540496122 |year=2007 |publisher=Springer}}
&lt;/ref&gt;

&lt;ref name=Audi&gt;
{{cite book |title=The Cambridge Dictionary of Philosophy |edition=Paperback 2nd |page= 631 |chapter=Ontological commitment |isbn=0521637228 |author= Roger F. Gibson |editor=Robert Audi  |year=1999 |url=https://books.google.com/books?id=kQQNBTW_hoAC&amp;pg=PT1537}} A shortened version of that definition is as follows:
:The ''ontological commitments'' of a theory are those things which occur in all the ''ontologies'' of that theory. To explain further, the [[ontology]] of a theory consists of the objects the theory makes use of. A dependence of a theory upon an object is indicated if the theory fails when the object is omitted. However, the ontology of a theory is not necessarily unique. A theory is ''ontologically committed'' to an object only if that object occurs in ''all'' the ontologies of that theory. A theory also can be ''ontologically committed'' to a class of objects if that class is populated (not necessarily by the same objects) in all its ontologies. [italics added]
&lt;/ref&gt;

&lt;!-- Unused ref &lt;ref name=Aydede&gt;
{{cite web |title=The language of thought hypothesis |first=Murat|last=Aydede |work= The Stanford Encyclopedia of Philosophy (Fall 2010 Edition) |editor=Edward N. Zalta |url= http://plato.stanford.edu/archives/fall2010/entries/language-thought/  |date=September 17, 2010}}
&lt;/ref&gt; --&gt;

&lt;ref name=Ceccaroni&gt;
For example, see {{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf |journal=Proceedings of the workshop AAMAS |year=2002}}
&lt;/ref&gt;

&lt;ref name=Ceccaroni1&gt;
{{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf  |journal=Proceedings of the workshop AAMAS |year=2002}} A quotation follows:
:“Researchers...come from different areas of study and have different perspectives on modeling, but significantly they pledged to adopt the same ''ontological commitment''. That is, they agree to adopt common, predefined ontologies...to express general categories, even if they do not completely agree on the modeling behind the ontological representations. Where ontological commitment is lacking, it is difficult to converse clearly about a domain and to benefit from knowledge representations developed by others... Ontological commitment is thus an integral aspect of ontological engineering.” [italics added]
&lt;/ref&gt;

&lt;!--ref name=CZ&gt;
This figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&amp;pg=PA7&amp;lpg=PA7#v=onepage&amp;q&amp;f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].
&lt;/ref--&gt;

&lt;ref name=Guarino&gt;
{{cite book |title=Formal Ontology in Information Systems (Proceedings of FOIS '98, Trento, Italy) |first=Nicola|last=Guarino |pages=3 ''ff'' |chapter=Formal Ontology in Information Systems |editor=Nicola Guarino |isbn=978-90-5199-399-8 |year=1998 |publisher=IOS Press |url=http://books.google.ca/books?id=Wf5p3_fUxacC&amp;pg=PA7&amp;lpg=PA7}}
&lt;/ref&gt;

&lt;ref name=Gruber&gt;
{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199–220 |doi=10.1006/knac.1993.1008}}
&lt;/ref&gt;

&lt;ref name=Guarino1&gt;
{{cite journal |title=Formalizing ontological commitments |author1=Nicola Guarino |author2=Massimiliano Carrara |author3=Pierdaniele Giaretta |journal=AAAI |volume=94 |pages=560–567 |year=1994 |url=http://www.mit.bme.hu/system/files/oktatas/targyak/7412/Formalizing_Ontological_Commitments.pdf}} 
&lt;/ref&gt;

&lt;ref name=Hadzic&gt;
{{cite book |title=Ontology-Based Multi-Agent Systems |chapter=Chapter 7: Design methodology for integrated systems - Part I (Ontology design) |pages=111 ''ff'' |author1=Maja Hadzic |author2=Pornpit Wongthongtham |author3=Elizabeth Chang |author4=Tharam Dillon |isbn=364201903X |year=2009 |publisher=Springer |url=https://books.google.com/books?id=kRoA_vxUwvQC&amp;pg=PA111}}
&lt;/ref&gt;

&lt;ref name=Harmelen&gt;
{{cite web |title=Ontology mapping: a way out of the medical tower of babel |author=Frank van Harmelen |url=http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf}}
&lt;/ref&gt;

&lt;ref name=Smith&gt;
{{cite book |chapter= Chapter 11: Ontology |first=Barry|last=Smith |url=http://ontology.buffalo.edu/smith/articles/ontology_PIC.pdf |title=Blackwell Guide to the Philosophy of Computing and Information |publisher=Blackwell |year=2003 |pages=155–166 |editor=Luciano Floridi |isbn=0631229183 }}
&lt;/ref&gt;

}}

==Further reading==

*{{cite book |chapter=On Ontology, ontologies, conceptualizations, modeling languages and (meta)models |first=G|last=Guizzardi |title=Frontiers in artificial intelligence and applications, databases and information systems IV |editor=Olegas Vaselicas |editor2=Johan Edler |editor3=Albertas Caplinskas, eds |isbn=978-1-58603-715-4|publisher=IOS Press |year=2007 |url=http://www.loa.istc.cnr.it/Guizzardi/FAIA.pdf }}
*{{cite book |title=Applied ontology: an introduction |editor1=Katherine Munn |editor2=Barry Smith |url=https://books.google.com/books?id=vuYLID7IfqEC&amp;printsec=frontcover |isbn=3938793988 |publisher=Ontos Verlag |year=2008}}

==External links==
*{{cite web |url=http://www.obitko.com/tutorials/ontologies-semantic-web/specification-of-conceptualization.html |title=Specification of conceptualization |work=Ontologies and the semantic web |first=Marek|last=Obitko |year=2006–2007}}
{{Citizendium|Ontological commitment#Conceptualization}}

[[Category:Information science]]
[[Category:Ontology]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)| ]]
[[Category:Semantic Web]]
[[Category:Technical communication]]</text>
      <sha1>1x4vnd56vz4mdptz5lj381q5ygbsfpk</sha1>
    </revision>
  </page>
  <page>
    <title>Harvard–Yenching Classification</title>
    <ns>0</ns>
    <id>8311927</id>
    <revision>
      <id>730571750</id>
      <parentid>708107281</parentid>
      <timestamp>2016-07-19T21:39:47Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12315" xml:space="preserve">[[Alfred Kaiming Chiu]] &lt;ref&gt;{{zh|t=裘開明 | s=裘开明 |p=Qiú Kāimíng|w=Ch'iu&lt;sup&gt;2&lt;/sup&gt; K'ai&lt;sup&gt;1&lt;/sup&gt;-Ming&lt;sup&gt;2&lt;/sup&gt;}}.&lt;/ref&gt; (1898–1977) was a pioneer of establishing a library classification system for Chinese language materials in the United States of America.  The system devised by him was known as '''Harvard–Yenching Classification System'''.  The system was primarily created for the classification of Chinese language materials in the [[Harvard-Yenching Library]] which was founded in 1927 at the [[Harvard-Yenching Institute]].&lt;ref&gt;Eugene W. Wu, "The Founding of the Harvard-Yenching Library," ''Journal of East Asian Libraries'' 101.1  (1993):  65-69.     [http://scholarsarchive.byu.edu/jeal/vol1993/iss101/16/]&lt;/ref&gt;

During that early period other systems, such as the early edition of the [[Library of Congress Classification]], did not consist of appropriate subject headings to classify the Chinese language materials, particularly the ancient published materials.  As many American libraries started to collect the ancient and contemporary published materials from China, a number of American libraries subsequently followed Harvard University to adopt Harvard–Yenching classification system, such as the East Asian Library of the University of California in Berkeley, Columbia University, The University of Chicago, Washington University in St. Louis etc.

In addition to American libraries, the libraries of other universities in the world including England, Australia, New Zealand, Hong Kong, Singapore etc. also followed Harvard University to adopt the system.  During the period from the 1930s to the 1970s, the use of the system became popular for classifying not only Chinese language materials but also other East Asian materials including Korean and Japanese language materials.

During the period from the 1970s to the 1980s, a comprehensive subset of subject headings for Chinese language materials was gradually established in the Library of Congress Classification System so that almost a full spectrum of ancient and contemporary Chinese topics can be widely covered. As a result of this, the Library of Congress Classification System eventually replaced the Harvard–Yenching Classification System for all Chinese language materials acquired after the 1970s in many American Libraries.

Though the system has largely been phased out, the system is still being used in some libraries for Chinese language materials acquired prior to the Library of Congress update. Such previously acquired books are normally stored in separate stacks in libraries. However, some of the university libraries in the Commonwealth countries of the United Kingdom such as England, Australia and New Zealand still continue to use the Harvard-Yenching system; for example, the Institute for Chinese Studies Library of the University of Oxford, University of Sydney, University of Melbourne, and University of Auckland.

== The Harvard–Yenching classification system ==
The key classes of the system are listed as follows:

===Key classes===
** 0100–0999 Chinese Classics
** 1000–1999 Philosophy and Religion
** 2000–3999 Historical Sciences
** 4000–4999 Social Sciences
** 5000–5999 Language and Literature
** 6000–6999 Fine and Recreative Arts
** 7000–7999 Natural Sciences
** 8000–8999 Agriculture and Technology
** 9000–9999 Generalia and Bibliography

===Subjects of sub-classes===
{{Empty section|date=January 2011}}

===0100 to 0999 Chinese Classics===
** 0100–0199 Chinese classics in general
** 0200–0299 I Ching
** 0300–0399 Shu Ching
** 0400–0499 Shih Ching
** 0500–0669 San Li
** 0680–0799 Ch’un Ch’iu
** 0800–0849 Hsiao Ching
** 0850–0999 Ssu Shu

===1000 to 1999 Philosophy and Religion===
** 1000–1008 Philosophy &amp; religion in general
** 1010–1429 Chinese philosophy
** 1470–1499 Hindu philosophy
** 1500–1539 Occident philosophy
** 1540–1569 Philosophical problems and systems
** 1570–1609 Logic
** 1610–1649 Metaphysics
** 1650–1699 Ethics
** 1700–1729 Religion in general
** 1730–1738 Mythology
** 1739–1749 Occultism numerology
** 1750–1779 History of religions
** 1780–1799 Chinese state cults
** 1800–1919 Buddhism
** 1920–1939 Taoism
** 1975–1987 Christianity
** 1988–1999 Other religions

===2000 to 3999 Historical Sciences===
** 2000–2049 Archaeology, Antiquities in general
** 2060–2159 China archaeology
** 2200–2249 Ethnology, ethnography
** 2250–2299 Genealogy and biography
** 2300–2349 World history
** 2350–2399 World geography
** 2400–2440 Asian history and geography
** 2450–2459 History of China in general
** 2461–2469 Chinese historiography
** 2470–2479 History of Chinese civilisation
** 2480–2509 Diplomatic history of China
** 2510–2519 General China history
** 2520–2533 Ancient history of China in general
** 2535 Ch’in, Han and 3 Kingdom in general
** 2536–2543 Ch’in Dynasty
** 2545–2559 Han Dynasty
** 2560–2567 The Three Kingdom
** 2570 Chin Dynasty and the Southern / Northern Dynasties
** 2571–2578 Chin Dynasty (265–420)
** 2581–2588 The Southern Dynasties
** 2590–2599 The Northern Dynasties
** 2605–2618 Sui, T’ang &amp; the Five Dynasties in general
** 2605–2619 Sui Dynasty
** 2620–2639 T’ang Dynasty
** 2640–2649 Epoch of the Five Dynasties (North)
** 2650–2660 The Ten Kingdoms (South)
** 2662 Sung, Liao, Chin and Yuan Dynasties in general
** 2665–2684 Sung Dynasty (960–1279)
** 2685–2688 The Liao Kingdom (916–1201)
** 2690 The Chin Kingdom (1115–1234)
** 2695 The [[Hsi Hsia Kingdom]] (982–1227)
** 2700–2713 Yuan Dynasty (1280–1268)
** 2718 Ming and Ching Dynasties in general
** 2720–2739 Ming Dynasty
** 2740–2969 Ch’ing Dynasty
** 2970 Period of Republic, 1912
** 3000–3019 China: geography &amp; history in general
** 3020–3031 General system treatises
** 3032–3049 Special works of geography: China
** 3507–3079 China: local description and travel
** 3080–3109 Maps, Atlas of China
** 3110–3299 Gazetteers of China
** 3300–3479 [[Japanese history]]
** 3400–3479 [[Geography of Japan|Japanese geography]]
** 3480–3489 [[Korean history]]
** 3490–3499 [[Hong Kong]], [[Macau]] history and geography
** 3500–3599 Other counties in Asia: history and geography
** 3600–3799 Europe: history and geography
** 3800–3899 America: history and geography
** 3900–3999 Africa, Oceania: history and geography

===4000 to 4999 Social Sciences===
** 4000–4019 Social sciences in general
** 4020–4099 Statistics
** 4100–4299 Sociology
** 4300–4599 Economics
** 4600–4899 Politics and Law
** 4900–4999 Education

===5000 to 5999 Language and Literature===
** 5000–5039 Linguistics in general
** 5040–5059 Literature in general
** 5060–5069 Chinese language in general
** 5070–5089 Semantic studies
** 5090–5119 Graphic studies
** 5120–5139 Phonological Studies
** 5140–5149 Grammar
** 5150–5159 Dialects
** 5160–5169 Texts: learning the language
** 5170–5199 Lexicography dictionaries
** 5200–5209 Chinese literature in general
** 5210–5217 Chinese literature: literary criticism
** 5218–5229 Chinese literature: history &amp; biography
** 5230–5235 Chinese literature: collection of individual complete works
** 5236–5241 Chinese literature: general anothlogies
** 5242–5569 Collected Chinese literart works of individual authors
** 5570–5649 Tz’u
** 5650–5730 Lyrical works and drama
** 5731–5769 Chinese Fiction
** 5770–5779 Letters
** 5780–5799 Miscellany: proverbs, fables, juv. lit.
** 5800–5809 Minor languages in China
** 5810–5859 Japanese language
** 5860–5959 Japanese literature
** 5973 Korean language and literature
** 5975–5993 Indo-European language and literature
** 5994–5999 Other language and literature

===6000 to 6999 Fine and Recreative Arts===
** 6000–6019 Fine and recreative arts in general
** 6020–6029 Aesthetics
** 6030–6069 History of arts
** 6070–6289 Chinese &amp; Japanese Calligraphy and painting
** 6290–6299 Materials &amp; instruments
** 6300–6349 Western painting
** 6350–6359 Engraving Prints
** 6360–6399 Photography
** 6400–6499 Sculpture
** 6500–6599 Architecture
** 6600–6699 Industrial arts
** 6700–6799 Music
** 6800–6899 Amusements &amp; games
** 6900–6999 Physical training &amp; sports

===7000 to 7999 Natural Sciences===
** 7000–7019 Natural science in general
** 7020–7099 [[Mathematics]]
** 7100–7199 [[Astronomy]]
** 7200–7299 [[Physics]]
** 7300–7399 [[Chemistry]]
** 7400–7499 [[Geology|Geological science]]
** 7500–7599 [[Natural history]]
** 7600–7699 [[Botany]]
** 7700–7799 [[Zoology]]
** 7800–7869 Anthropology (Physical)
** 7870–7899 Psychology
** 7900–7999 Medical science

===8000 to 8999 Agriculture and Technology===
** 8000–8009 Agriculture &amp; technology in general
** 8020–8239 Agriculture
** 8240–8289 Home economics (Domestic)
** 8290–8299 Technology in general
** 8300–8349 Handicrafts &amp; artisan trades
** 8400–8499 Manufactures
** 8500–8599 Chemical technology
** 8600–8699 Mining &amp; Metallurgy
** 8700–8899 Engineering
** 8900–8999 Military &amp; Naval science

===9000 to 9999 Generalia and Bibliography===
** 9000–9007 Generalia and bibliography in general
** 9100 Chinese general series of composite nature
** 9101–9109 Chinese general series of a special type
** 9110 Chinese series of particular locality
** 9111–9120 Chinese family &amp; individual author
** 9130–9163 Sinology
** 9164–9179 Japanese general series
** 9180–9199 Japanese individual polygraphic books
** 9200–9229 General periodicals &amp; society publications
** 9230–9289 General congresses &amp; museums
** 9290–9339 General encyclopedias and reference books
** 9401–9409 Bibliography in general
** 9410–9510 Bibliography
** 9511–9519 Subject bibliographies
** 9520–9539 Chinese collective bibliographies
** 9540–9549 Other general bibliographies of various countries
** 9550–9559 Reading lists &amp; best books, periodical index
** 9562–9569 Special bibliographies
** 9570–9579 Bibliographies of critical reviews
** 9600–9629 Library catalogues
** 9630–9639 Dealers’ &amp; publishers’ catalogues
** 9640–9684 Japanese bibliographies
** 9696–9699 Bibliographies of Western countries
** 9700–9929 Librarianship
** 9930–9999 Journalism, newspapers

== See also ==
The official library classification in China is:

* [[Chinese Library Classification]] (CLC)

The other library classifications for Chinese materials outside China are:
* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books drawn up by Profs. Haloun and P. van der Loon for Cambridge University, UK.
* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])

== Notes==

&lt;references /&gt;

==References==
* [http://www.lib.unimelb.edu.au/collections/asian/Harvard-Yenching.html Harvard-Yenching Classification in the University of Melbourne] - Subject headings in both Chinese and English.
* [http://research.dils.tku.edu.tw/joemls/41/41-2/139-162.pdf PDF] - Brief history of the Harvard–Yenching Classification System, and an overview of the collections of Chinese language materials in Columbia University, Cornell University, Harvard University, the Library of Congress, Princeton University and Yale University.
* [http://www.news.harvard.edu/gazette/2003/10.30/19-yenching.html Yenching, The singular history of a singular library, Ken Gewertz, Harvard News Office] - A brief history of the Harvard-Yenching Library and the Harvard–Yenching Classification System.

== External links ==
* [https://www.lib.uchicago.edu/e/easia/shelf.html Examples of the Harvard-Yenching classification system]
* [http://www.library.ucla.edu/libraries/eastasian/collect.htm East Asian Library of the University of California in Los Angeles]

{{Library classification systems}}

{{DEFAULTSORT:Harvard-Yenching Classification}}
[[Category:Library cataloging and classification]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Chinese culture]]
[[Category:Harvard University]]
[[Category:Yenching University]]</text>
      <sha1>eot4p2xptbhzvnkoqwxpbch6cs3tu03</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic reasoner</title>
    <ns>0</ns>
    <id>13536810</id>
    <revision>
      <id>753805031</id>
      <parentid>753804987</parentid>
      <timestamp>2016-12-09T08:09:02Z</timestamp>
      <contributor>
        <username>Shvahabi</username>
        <id>8307062</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6630" xml:space="preserve">{{Redirect|Reasoner}}
A '''semantic reasoner''', '''reasoning engine''', '''rules engine''', or simply a '''reasoner''', is a piece of software able to infer [[logical consequence]]s from a set of asserted facts or [[axioms]]. The notion of a semantic reasoner generalizes that of an [[inference engine]], by providing a richer set of mechanisms to work with. The [[inference rules]] are commonly specified by means of an [[ontology language]], and often a [[description logic]] language.  Many reasoners use [[first-order predicate logic]] to perform reasoning; [[inference]] commonly proceeds by [[forward chaining]] and [[backward chaining]]. There are also examples of probabilistic reasoners, including Pei Wang's [[non-axiomatic reasoning system]],&lt;ref name=Wang&gt;{{cite web|last1=Wang|first1=Pei|title=Grounded on Experience Semantics for intelligence, Tech report 96|url=http://www.cogsci.indiana.edu/pub/wang.semantics.ps|website=http://www.cogsci.indiana.edu/|publisher=CRCC|accessdate=13 April 2015}}&lt;/ref&gt; and [[probabilistic logic network]]s.&lt;ref name=Goertzel2008&gt;{{cite book|last1=Goertzel|first1=Ben|last2=Iklé|first2=Matthew|last3=Goertzel|first3=Izabela Freire|last4=Heljakka|first4=Ari|title=Probabilistic Logic Networks: A Comprehensive Framework for Uncertain Inference|date=2008|publisher=Springer Science &amp; Business Media|isbn=9780387768724|page=42}}&lt;/ref&gt;

==List of semantic reasoners==

Existing semantic reasoners and related software:

===Commercial software===
* Bossam (software), an RETE-based rule engine with native supports for reasoning over [[Web Ontology Language|OWL]] ontologies, SWRL rules, and RuleML rules.
* RacerPro

===Free to use (Closed Source)===
* [[Cyc]] inference engine, a forward and backward chaining inference engine with numerous specialized modules for high-order logic. ([http://research.cyc.com/] ResearchCyc) ([http://opencyc.org/] OpenCyc)
* [[KAON2]] is an infrastructure for managing [[OWL-DL]], [[Semantic Web Rule Language|SWRL]], and [[F-Logic]] ontologies.
* [[ Internet Business Logic (software)]]—A reasoner designed for end-user app authors. Automatically generates and runs complex networked SQL queries. Explains the results in English at the end-user level.

===Free software (open source)===
* [[Cwm (software)|Cwm]], a forward-chaining reasoner used for querying, checking, transforming and filtering information. Its core language is RDF, extended to include rules, and it uses RDF/XML or N3 serializations as required. ([http://www.w3.org/2000/10/swap/doc/cwm.html CWM],  W3C software license)
* [[Drools]], a forward-chaining inference-based rules engine which uses an enhanced implementation of the [[Rete algorithm]]. ([http://www.jboss.org/drools/ Drools], Apache license 2.0)
* [http://owl.cs.manchester.ac.uk/tools/fact/ FaCT++ Reasoner], a tableaux-based reasoner for expressive Description Logics (DL), covering OWL and OWL 2 but lacking support for key constraints and some datatypes. Written in C++. (LGPL)
* [[Flora-2]], an object-oriented, rule-based knowledge-representation and reasoning system. ([http://flora.sourceforge.net Flora-2], Apache 2.0)
* [https://gndf.io/ Gandalf], open-source decision rules engine on PHP (GPL).
* [[Prova]], a semantic-web rule engine which supports data integration via SPARQL queries and type systems (RDFS, OWL ontologies as type system). ([http://prova.ws Prova], GNU GPL v2, commercial option available)
* [https://github.com/stardog-union/pellet Pellet], OWL 2 DL reasoner (AGPL, commercial option available)
* [http://www.hermit-reasoner.com/ HermiT], OWL 2 DL reasoner (LGPL)
* [https://github.com/liveontologies/elk-reasoner ELK], OWL 2 EL reasoner (Apache 2)
* [https://lat.inf.tu-dresden.de/systems/cel CEL], OWL 2 EL reasoner (Apache 2)
* [https://github.com/julianmendez/jcel jcel], OWL 2 EL reasoner (LGPL / Apache 2)
* [https://github.com/ha-mo-we/Racer RACER], OWL 2 DL reasoner (BSD-3)
* [[Jena (framework)]], an open-source semantic-web framework for Java which includes a number of different semantic-reasoning modules. ([http://jena.apache.org/ Apache Jena], Apache License 2.0)
* [[RDFSharp]], an open source semantic web framework for .NET which includes a semantic extension implementing RDFS/OWL-DL/custom rule-based reasoning. ([http://rdfsharp.codeplex.com/ RDFSharp], Apache License 2.0)

=== Applications that contain reasoners ===
* [[Apache Marmotta]] includes a rule-based reasoner in its KiWi [[triple store]].
* [http://techinvestlab.ru/dot15926Editor dot15926 Editor]—Ontology management framework initially designed for engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] rule scripting and pattern-based data analysis. Supports extensions.

==See also==
{{portal|Software}}
* [[Business rules engine]]
* [[Expert systems]]
* [[Doxastic logic]]
* [[Method of analytic tableaux]]
*[[Logic Programming]]

==References==
{{reflist}}

==External links==
* [https://www.w3.org/2001/sw/wiki/OWL/Implementations OWL 2 Reasoners listed on W3C SW Working Group homepage]
* [http://www.w3.org/TR/rdf-sparql-query/ SPARQL Query Language for RDF]
* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics DL course] by Enrico Franconi, Faculty of Computer Science, [[Free University of Bolzano]], Italy
* [http://trimc-nlp.blogspot.com/2013/04/owl-properties.html ''Inference using OWL 2.0 Semantics''] by Craig Trim (IBM).
* Marko Luther, Thorsten Liebig, Sebastian Böhm, Olaf Noppens: [http://dx.doi.org/10.1007/978-3-642-02121-3_9 Who the Heck Is the Father of Bob?]. ESWC 2009: 66-80
* Jurgen Bock, Peter Haase, Qiu Ji, Raphael Volz. [http://www.aifb.uni-karlsruhe.de/WBS/pha/publications/owlbenchmark_07_2007.pdf Benchmarking OWL Reasoners]. In ARea2008 - Workshop on Advancing Reasoning on the Web: Scalability and Commonsense (June 2008)
* Tom Gardiner, Ian Horrocks, Dmitry Tsarkov. [http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-189/submission_23.pdf Automated Benchmarking of Description Logic Reasoners]. Description Logics Workshop 2006
* [http://www2009.org/proceedings/pdf/p601.pdf OpenRuleBench] Senlin Liang, Paul Fodor, Hui Wan, Michael Kifer. OpenRuleBench: An Analysis of the Performance of Rule Engines. 2009.  Latest benchmarks at [http://rulebench.projects.semwebcentral.org/ OpenRuleBench website].

{{Semantic Web}}
{{Computable knowledge}}

{{DEFAULTSORT:Semantic Reasoner}}
[[Category:Rule engines| ]]
[[Category:Knowledge representation]]
[[Category:Knowledge engineering]]
[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Reasoning]]</text>
      <sha1>hzqzetc3v8arkw5emm361qxkitjc1ew</sha1>
    </revision>
  </page>
  <page>
    <title>DOAP</title>
    <ns>0</ns>
    <id>4842020</id>
    <revision>
      <id>704970903</id>
      <parentid>704970730</parentid>
      <timestamp>2016-02-14T19:23:00Z</timestamp>
      <contributor>
        <ip>108.24.111.89</ip>
      </contributor>
      <comment>/* External links */ better formatting</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2428" xml:space="preserve">{{one source|date=October 2012}}

'''DOAP''' ('''Description of a Project''') is an [[RDF Schema]] and [[XML]] vocabulary to describe  software projects, in particular [[free and open source software]].

It was created and initially developed by [[Edd Dumbill]] to convey semantic information associated with open source software projects.

== Adoption ==

There are currently generators, [[validator]]s, viewers, and converters to enable more projects to be able to be included in the [[semantic web]]. [[Freshmeat]]'s 43 000 projects are now available published with DOAP.&lt;ref&gt;{{ cite web | url = http://fgiasson.com/blog/index.php/2007/08/04/freshmeatnet-now-available-in-doap-43-000-new-doap-projects/ | title = Freshmeat.net now available in DOAP: 43 000 new DOAP projects | first = Frederick | last = Giasson | accessdate = 2010-04-08 }}&lt;/ref&gt; It is currently used in the [[Mozilla Foundation]]'s project page and in several other software repositories, notably the [[Python Package Index]].

Major properties include: doap:homepage, doap:developer, doap:programming-language, doap:os

== Examples ==

The following is an example in RDF/XML:

&lt;source lang="xml"&gt;
 &lt;rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:doap="http://usefulinc.com/ns/doap#"&gt;
  &lt;doap:Project&gt;
   &lt;doap:name&gt;Example project&lt;/doap:name&gt;
   &lt;doap:homepage rdf:resource="http://example.com" /&gt;
   &lt;doap:programming-language&gt;javascript&lt;/doap:programming-language&gt;
   &lt;doap:license rdf:resource="http://example.com/doap/licenses/gpl"/&gt;
  &lt;/doap:Project&gt;
 &lt;/rdf:RDF&gt;
&lt;/source&gt;

Other properties include &lt;code&gt;Implements specification, anonymous root, platform, browse, mailing list, category, description, helper, tester, short description, audience, screenshots, translator, module, documenter, wiki, repository, name, repository location, language, service endpoint, created, download mirror, vendor, old homepage, revision, download page, license, bug database, maintainer, blog, file-release&lt;/code&gt; and &lt;code&gt;release.&lt;/code&gt;{{citation needed|date=January 2013}}

==References==

{{reflist}}

==External links==
* {{github|edumbill/doap/|Doap Project}}
* [http://www.oss-watch.ac.uk/resources/doap.xml OSS Watch DOAP Briefing Note]
* [http://crschmidt.net/semweb/doapamatic/ doapamatic]: DOAP generator

{{Semantic Web}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]</text>
      <sha1>b7nh0l2clc2mzre08odcdjmyuez9ll0</sha1>
    </revision>
  </page>
  <page>
    <title>Transaction logic</title>
    <ns>0</ns>
    <id>12817496</id>
    <revision>
      <id>747984977</id>
      <parentid>668546553</parentid>
      <timestamp>2016-11-05T16:55:17Z</timestamp>
      <contributor>
        <username>Narky Blert</username>
        <id>22041646</id>
      </contributor>
      <comment>DN tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6103" xml:space="preserve">'''Transaction Logic''' is an extension of [[predicate logic]] that accounts in a clean and declarative way for the phenomenon of state changes in [[logic program]]s and [[database]]s. This extension adds connectives specifically designed for combining simple actions into complex transactions and for providing control over their execution. The logic has a natural [[model theory]] and a sound and complete [[proof theory]]. Transaction Logic has a [[Horn clause]] subset, which has a procedural as well as a declarative semantics. The important features of the logic include hypothetical and committed updates, dynamic constraints on transaction execution, non-determinism, and bulk updates.  In this way, Transaction Logic is able to declaratively capture a number of non-logical phenomena, including [[procedural knowledge]] in [[artificial intelligence]], [[active database]]s, and methods with side effects in [[object database]]s.

Transaction Logic was originally proposed in &lt;ref name="tr-iclp1993"&gt;A.J. Bonner and M. Kifer (1993), ''Transaction Logic Programming'', International Conference on Logic Programming (ICLP), 1993.&lt;/ref&gt; by [http://www.cs.toronto.edu/~bonner/ Anthony Bonner] and [http://www.cs.stonybrook.edu/~kifer/ Michael Kifer] and later described in more detail in &lt;ref&gt;A.J. Bonner and M. Kifer (1994), ''An Overview of Transaction Logic'', Theoretical Computer Science, 133:2, 1994.&lt;/ref&gt; and.&lt;ref&gt;A.J. Bonner and M. Kifer (1998), [http://www.cs.sunysb.edu/~kifer/TechReports/tr-chomicki.pdf ''Logic Programming for Database Transactions''] in Logics for Databases and Information Systems, J. Chomicki and G. Saake (eds.), Kluwer Academic Publ., 1998.&lt;/ref&gt; The most comprehensive description appears in.&lt;ref&gt;A.J. Bonner and M. Kifer (1995), [http://www.cs.sunysb.edu/~kifer/TechReports/transaction-logic.pdf ''Transaction Logic Programming (or A Logic of Declarative and Procedural Knowledge)'']. Technical Report CSRI-323, November 1995, Computer Science Research Institute, University of Toronto.&lt;/ref&gt;

In later years, Transaction Logic was extended in various ways, including [[concurrency]]{{dn|date=November 2016}},&lt;ref name="concurrentTR"&gt;A.J. Bonner and M. Kifer (1996), [http://www.cs.sunysb.edu/~kifer/TechReports/concurrent-trans-logic.pdf ''Concurrency and communication in Transaction Logic''], Joint Intl. Conference and Symposium on Logic Programming, Bonn, Germany, September 1996&lt;/ref&gt; [[defeasible reasoning]],&lt;ref&gt;P. Fodor and M. Kifer (2011), [http://drops.dagstuhl.de/opus/volltexte/2011/3159/ ''Transaction Logic with Defaults and Argumentation Theories'']. In Technical communications of the 27th International Conference on Logic Programming (ICLP), July 2011.&lt;/ref&gt; partially defined actions,&lt;ref&gt;M. Rezk and M. Kifer (2012), [http://link.springer.com/article/10.1007%2Fs13740-012-0007-8 ''Transaction Logic with Partially Defined Actions'']. Journal on Data Semantics, August 2012, vol. 1, no. 2, Springer.&lt;/ref&gt; and other features.&lt;ref&gt;H. Davulcu, M. Kifer and I.V. Ramakrishnan (2004), [http://www.www2004.org/proceedings/docs/2p144.pdf CTR-S: A Logic for Specifying Contracts in Semantic Web Services'']. Proceedings of the 13-th World Wide Web Conference (WWW2004), May 2004.&lt;/ref&gt;&lt;ref&gt;P. Fodor and M. Kifer (2010), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.6968 ''Tabling for Transaction Logic'']. In Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming (PPDP), July 2010.&lt;/ref&gt;

In 2013, the original paper on Transaction Logic &lt;ref name="tr-iclp1993"/&gt; has won the 20-year Test of Time Award as the most influential paper from the proceedings of [http://www.informatik.uni-trier.de/~ley/db/conf/iclp/iclp93.html ICLP 1993 conference] in the preceding 20 years.{{citation needed|date=February 2014}}

== Examples ==

Graph coloring. Here &lt;tt&gt;tinsert&lt;/tt&gt; denotes the elementary update operation of ''transactional insert''. The connective ⊗ is called ''serial conjunction''.
 colorNode &lt;-  // color one node correctly
     node(N) ⊗ &amp;neg; colored(N,_) ⊗ color(C)
     ⊗ ¬(adjacent(N,N2) ∧ colored(N2,C))
     ⊗ tinsert(colored(N,C)).
 colorGraph &lt;- ¬uncoloredNodesLeft.
 colorGraph &lt;- colorNode ⊗ colorGraph.

Pyramid stacking. The elementary update &lt;tt&gt;tdelete&lt;/tt&gt; represents the ''transactional delete'' operation.
 stack(N,X) &lt;- N&gt;0 ⊗ move(Y,X) ⊗ stack(N-1,Y).
 stack(0,X).
 move(X,Y) &lt;- pickup(X) ⊗ putdown(X,Y).
 pickup(X) &lt;- clear(X) ⊗ on(X,Y) ⊗
              ⊗ tdelete(on(X,Y)) ⊗ tinsert(clear(Y)).
 putdown(X,Y) &lt;-  wider(Y,X) ⊗ clear(Y) 
                  ⊗ tinsert(on(X,Y)) ⊗ tdelete(clear(Y)).

Hypothetical execution. Here &lt;tt&gt;&amp;lt;&amp;gt;&lt;/tt&gt; is the modal operator of possibility: If both &lt;tt&gt;action1&lt;/tt&gt; and &lt;tt&gt;action2&lt;/tt&gt; are possible, execute &lt;tt&gt;action1&lt;/tt&gt;. Otherwise, if only &lt;tt&gt;action2&lt;/tt&gt; is possible, then execute it.
  execute &lt;- &lt;&gt;action1 ⊗ &lt;&gt;action2 ⊗ action1.
  execute &lt;- ¬&lt;&gt;action1 ⊗ &lt;&gt;action2 ⊗ action2.

Dining philosophers. Here | is the logical connective of parallel conjunction of Concurrent Transaction Logic.&lt;ref name="concurrentTR"/&gt;
 diningPhilosophers &lt;- phil(1) | phil(2) | phil(3) | phil(4).

== Implementations ==

A number of implementations of Transaction Logic exist. The original implementation is available [http://www.cs.toronto.edu/~bonner/transaction-logic.html here]. An implementation of Concurrent Transaction Logic is available [http://www.cs.toronto.edu/~bonner/ctr/index.html here]. Transaction Logic enhanced with [[tabling]] is available [http://flora.sourceforge.net/tr-interpreter-suite.tar.gz here]. An implementation of Transaction Logic has also been incorporated as part of the [[Flora-2]] knowledge representation and reasoning system. All these implementations are [[open source]].

Additional papers on Transaction Logic can be found on the [http://flora.sourceforge.net Flora-2 Web site].

== References ==
{{Reflist}}

[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]</text>
      <sha1>sjzstulfw3hl9xm8hbdhan3lc78u29k</sha1>
    </revision>
  </page>
  <page>
    <title>ERIL</title>
    <ns>0</ns>
    <id>42325907</id>
    <revision>
      <id>725131878</id>
      <parentid>628302717</parentid>
      <timestamp>2016-06-13T19:35:42Z</timestamp>
      <contributor>
        <username>Cabayi</username>
        <id>6561336</id>
      </contributor>
      <minor />
      <comment>Added {{[[Template:COI|COI]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3945" xml:space="preserve">{{COI|date=June 2016}}
{{Orphan|date=April 2014}}

[[File:A simple example ERIL diagram.png|thumb|right|An example ERIL diagram with 3 classes and 3 one-to-many relationships.]]

'''ERIL''' ('''Entity-Relationship and Inheritance Language''') is a [[visual language]] for representing the data structure of a computer system.
As its name suggests, ERIL is based on [[Entity–relationship model|entity-relationship]] diagrams and [[class diagram]]s.
ERIL combines the [[Relational data model|relational]] and [[Object-oriented programming|object-oriented]] approaches to [[data model]]ing.

== Overview ==
ERIL can be seen as a set of guidelines aimed at improving the readability of structure diagrams.
These guidelines were borrowed from [[DRAKON]], a variant of [[flowchart]]s created within the Russian space program.
ERIL itself was developed by Stepan Mitkin.

The ERIL guidelines for drawing diagrams:
* Lines must be straight, either strictly vertical or horizontal.
* Vertical lines mean ownership ([[Object composition|composition]]).
* Horizontal lines mean peer relationships ([[Object composition#Aggregation|aggregation]]).
* Line intersections are not allowed.
* It is not recommended to fit the whole data model on a single diagram. Draw many simple diagrams instead.
* The same class (table) can appear several times on the same diagram.    
* Use the following standard symbols to indicate the type of the relationship.
** One-to-one: a simple line.
** One-to-many, two-way: a line with a "paw".
** One-to-many, one-way: an arrow.
** Many-to-many: a line with two "paws".    
* Do not lump together inheritance and data relationships.&lt;ref&gt;[http://drakon-editor.sourceforge.net/eril.html ERIL: a Visual Language for Data Modelling]&lt;/ref&gt;

== Indexes ==
A class (table) in ERIL can have several indexes.
Each index in ERIL can include one or more fields, similar to indexes in [[relational database]]s.
ERIL indexes are logical. They can optionally be implemented by real data structures.

== Links ==
Links between classes (tables) in ERIL are implemented by the so-called "link" fields.
Link fields can be of different types according to the link type:
* reference;
* collection of references.
    
Example: there is a one-to-many link between ''Documents'' and ''Lines''. One ''Document'' can have many ''Lines''. Then the ''Document.Lines'' field is a collection of references to the lines that belong to the document. ''Line.Document'' is a reference to the document that contains the line.

Link fields are also logical. They may or may not be implemented physically in the system.

== Usage ==

ERIL is supposed to model any kind of data regardless of the storage. 
The same ERIL diagram can represent data stored in a [[relational database]], in a [[Nosql|NoSQL]] database, [[Xml|XML]] file or in the memory.

ERIL diagrams serve two purposes.
The primary purpose is to explain the data structure of an existing or future system or component.
The secondary purpose is to automatically generate source code from the model.
Code that can be generated includes specialized collection classes, hash and comparison functions, data retrieval and modification procedures, [[Data definition language|SQL data-definition]] code, etc. Code generated from ERIL diagrams can ensure referential and uniqueness [[data integrity]].
Serialization code of different kinds can also be automatically generated.
In some ways ERIL can be compared to [[object-relational mapping]] frameworks.

== See also ==
* [[Model-driven engineering]]
* [[Unified Modeling Language|UML]]
* [[Entity–relationship model]]
* [[Flowchart]]s
* [[Class diagram]]
* [[DRAKON]]

== Notes ==
{{Reflist}}


[[Category:Architecture description language]]
[[Category:Data modeling languages]]
[[Category:Data modeling diagrams]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Specification languages]]
[[Category:Software modeling language]]</text>
      <sha1>h3ais36eetyq4dwd79i8utndn0v89k9</sha1>
    </revision>
  </page>
  <page>
    <title>Enactive interfaces</title>
    <ns>0</ns>
    <id>5510317</id>
    <revision>
      <id>757030560</id>
      <parentid>744779542</parentid>
      <timestamp>2016-12-28T09:50:14Z</timestamp>
      <contributor>
        <ip>82.158.187.111</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13424" xml:space="preserve">[[File:Enactive Human Machine Interface.png|thumb|300px|Enactive human-machine interface translating the aspects of a knowledge base into modalities of perception for a human operator. The auditory, visual, and tactile presentations by the system respond to tactile input from the operator, which user input in turn depends upon the auditory, visual, and tactile feedback from the system.&lt;ref name=Bordegoni/&gt;&lt;ref name=Fukuda/&gt;]]
'''Enactive interfaces''' are interactive systems that allow organization and transmission of knowledge obtained through action. Examples are interfaces that couple a human with a machine to do things usually done unaided, such as shaping a three-dimensional object using multiple modality interactions with a data base,&lt;ref name=Fukuda/&gt; or using interactive video to allow a student to visually engage with mathematics concepts.&lt;ref name=Held/&gt; Enactive interface design can be approached through the idea of raising awareness of [[affordances]], that is, optimization of the awareness of possible actions available to someone using the enactive interface.&lt;ref name=Stoffregen/&gt; This optimization involves visibility, affordance, and feedback.&lt;ref name=Stone/&gt;&lt;ref name=Zudilova/&gt;

The enactive interface in the figure interprets manual input and provides a response in perceptual terms in the form of images, sounds, and haptic (tactile) feedback. The system is called enactive because of the feedback loop in which the system response is decided by the user input, and the user input is driven by the perceived system responses.&lt;ref name=Bordegoni/&gt;

Enactive interfaces are new types of [[Human–computer interaction|human-computer interface]] that express and transmit the enactive knowledge by integrating different sensory aspects. The driving concept of enactive interfaces is then the fundamental role of motor action for storing and acquiring knowledge (action driven interfaces). Enactive interfaces are then capable of conveying and understanding gestures of the user, in order to provide an adequate response in perceptual terms. Enactive interfaces can be considered a new step in the development of the human-computer interaction because they are characterized by a closed loop between the natural gestures of the user (efferent component of the system) and the perceptual modalities activated (afferent component). Enactive interfaces can be conceived to exploit this direct loop and the capability of recognizing complex gestures.

The development of such interfaces requires the creation of a common vision between different research areas like [[computer vision]], [[Haptic perception|haptic]] and sound processing, giving more attention on the motor action aspect of interaction. An example of prototypical systems that are able to introduce enactive interfaces are reactive robots, robots that are always in contact with the human hand (like current play console controllers, [[Wii Remote]]) and are capable of interpreting the human movements and guiding the human for the completion of a manipulation task.

==Enactive knowledge==
Enactive knowledge is information gained through perception–action interaction in the environment. In many aspects the enactive knowledge is more natural than the other forms both in terms of the learning process and in the way it is applied in the world. Such knowledge is inherently [[multimodal interaction|multimodal]] because it requires the co-ordination of the various senses. Two key characteristics of enactive knowledge are that it is ''experential'': it relates to doing and depends on the user's experience, and it is ''cultural'': the way of doing is itself dependent upon social aspects, attitudes, values, practices, and legacy.&lt;ref name=Bordegoni/&gt;

Enactive interfaces are related to a fundamental interaction concept that often is not exploited by existing [[Human–computer interaction|human-computer interface]] technologies. As stated by cognitive psychologist [[Jerome Bruner]], the traditional interaction with the information mediated by a computer is mostly based on symbolic or iconic knowledge, and not on enactive knowledge.&lt;ref name=Slee/&gt; While in the symbolic way of learning knowledge is stored as words, mathematical symbols or other symbol systems, in the iconic stage knowledge is stored in the form of visual images, such as diagrams and illustrations that can accompany verbal information. On the other hand, enactive knowledge is a form of knowledge based on active participation, knowing by doing, by living rather than thinking.&lt;ref name=Slee2/&gt;
:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"&lt;ref name=Bruner/&gt;

A particular form of knowledge is a ''[[skill]]'', juggling being a simple example, and the acquisition of a skill is one area where enactive knowledge is evident. The sensorimotor and cognitive activities involved in acquiring skills are tabulated by the SKILLS FP6 European skills project.&lt;ref name=Bardy&gt;
{{cite journal |title=An enactive approach to perception-action and skill acquisition in virtual reality environments |author1=B Bardy |author2=D Delignières |author3=J Lagarde |author4=D Mottet |author5=G Zelic |journal= Third International Conference on Applied Human Factors and Ergonomics |location=Miami |date=July 2010 |url=http://didier.delignieres.perso.sfr.fr/Colloques-docs/Bardy%20et%20al.%20%282010%29%20Skills%20apprentissage.pdf }}
&lt;/ref&gt;

==Multimodal interfaces==
Multimodal interfaces are a good candidate for the creation of ''Enactive interfaces'' because of their coordinated use of [[Haptic perception|haptic]], sound and vision. Such research is the main objective of the ENACTIVE [[Framework Programmes for Research and Technological Development|Network of Excellence]], a European consortium of more than 20 research laboratories that are joining their research effort for the definition, development and exploitation of enactive interfaces.

==ENACTIVE Network of Excellence==
The research on enactive knowledge and enactive interfaces is the objective of the ENACTIVE Network of Excellence. A Network of Excellence is a [[European Economic Community|European Community]] research instrument that provides fundings for the integration of the research activities of different research laboratories and institutions. The ENACTIVE NoE started in 2004 with more than 20 partners with the objective of ''the creation of a multidisciplinary research community with the aim of structuring the research on a new generation of human-computer interfaces called Enactive Interfaces.''. The aim of this NoE is not only the research on enactive interfaces by itself, but also the integration of the partners through a Virtual Laboratory and the spreading of the expertise and knowledge of the Network.

Since 2004, the partners, coordinated by the PERCRO laboratory, have improved both the theoretical aspects of enaction, through seminars and the creation of a [[lexicon]], and the technological aspects necessary for the creation of enactive interfaces. Every year the status of the ENACTIVE NoE is presented through an international conference.&lt;ref name=PERCRO/&gt;

== See also ==
* [[Enactivism]]

==References==
{{reflist|refs=

&lt;ref name=Bordegoni&gt;
{{cite book |title=Emotional Engineering: Service Development |editor=Shuichi Fukuda |url=https://books.google.com/books?id=ow-UFDj15rUC&amp;pg=PA76 |page=76 |chapter=§4.4.2: PDP [Product Development Process] scenario based on user-centered design |author=Monica Bordegoni |isbn=9781849964234 |publisher=Springer |year=2010}}
&lt;/ref&gt;

&lt;ref name=Bruner&gt;
{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}}. Quoted in {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&amp;pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor &amp; Francis |isbn= 0415326982 |edition=Paperback}}
&lt;/ref&gt;

&lt;ref name=Fukuda&gt;
{{cite book |title=Emotional Engineering: Service Development |chapter=§4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&amp;pg=PA78 |pages=78 ''ff'' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}
&lt;/ref&gt;

&lt;ref name=Held&gt;
{{cite book |title=Research on Technology and the Teaching and Learning of Mathematics  |editor1=Mary Kathleen Heid |editor2=Glendon W. Blume |url=https://books.google.com/books?id=RGqFJ9inaQQC&amp;pg=PA213 |pages=213 ''ff'' |chapter=Enactive control |authors=D Tall, D Smith, C Piez |isbn=9781931576192 |year=2008 |publisher=Information Age Publishing Inc }}
&lt;/ref&gt;

&lt;ref name=PERCRO&gt;
{{cite web |title=Research on haptic interfaces and virtual environments |url=http://www.percro.org/node/24 |publisher=PERCRO Perceptual Robotics Laboratory |accessdate=April 30, 2014}}
&lt;/ref&gt;

&lt;ref name=Slee&gt;
Bruner's list of six characteristics of iconic knowledge is found in {{cite book |chapter=Iconic representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&amp;pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}
&lt;/ref&gt;

&lt;ref name=Slee2&gt;
{{cite book |chapter=Enactive representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&amp;pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}
&lt;/ref&gt;

&lt;ref name=Stoffregen&gt;
{{cite journal |url=http://link.springer.com/article/10.1007/s10055-006-0025-7 |title=Affordances in the design of enactive systems |author1=TA Stoffregen |author2=BG Bardy |author3=B Mantel |journal=Virtual Reality |volume=10 |issue=1 |year=2006 |pages=4–10 |doi=10.1007/s10055-006-0025-7}}
&lt;/ref&gt;

&lt;ref name=Stone&gt;
{{cite book |author1=Debbie Stone |author2=Caroline Jarrett |author3=Mark Woodroffe |author4=Shailey Minocha Morgan Kaufmann |year=2005 |title=User Interface Design and Evaluation |publisher=Morgan Kaufmann |isbn=9780080520322 |url=https://books.google.com/books?id=VvSoyqPBPbMC&amp;pg=PA97 |pages=97 ''ff'' |chapter=Chapter 5; §3: Three principles from experience: visibility, affordance, and feedback}}
&lt;/ref&gt;

&lt;ref name=Zudilova&gt;
{{cite book |title=Trends in Interactive Visualization: State-of-the-Art Survey |pages=166 ''ff'' |chapter=Perceptual and design principles for effective interactive visualizations |author1=Elena Zudilova-Seinstra |author2=Tony Adriaansen |author3=Robert van Liere |year=2008 |publisher=Springer |isbn=9781848002692 |url=https://books.google.com/books?id=mFtS7uN8ybsC&amp;pg=PA166}}
&lt;/ref&gt;

}}

==External links==
* [http://vimeo.com/79179138 Vimeo], video of a three-dimensional dynamic interactive graphical display allowing a human operator to visualize and manipulate data.

==Additional reading==
*{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&amp;pg=PA118&amp;lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118}} "The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela's sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself." 
*{{cite journal |title=The systemics of dialogism: On the prevalence of the self in HCI design |author=Colin T Schmidt |journal=Journal of the American society for information science |volume=48 |issue=11 |pages=1073–1081 |year=1997 |url=http://www.researchgate.net/publication/220433804_The_Systemics_of_Dialogism_On_the_Prevalence_of_the_Self_in_HCI_Design |doi=10.1002/(sici)1097-4571(199711)48:11&lt;1073::aid-asi9&gt;3.0.co;2-t}} Autopoiesic systems.
*{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-Jörg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&amp;rct=j&amp;q=&amp;esrc=s&amp;sa=X&amp;ei=N-h_U6HtHIiEogSy_oHAAw&amp;ved=0CCcQgAMoADAA&amp;usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&amp;cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 ''ff'' |doi=10.1016/s0007-8506(07)62129-5}}

[[Category:Enactive cognition]]
[[Category:Knowledge representation]]
[[Category:Educational psychology]]
[[Category:Motor cognition]]
[[Category:User interface techniques]]</text>
      <sha1>08nw6zx0ynqhko9vgrf2u3qgg55et5c</sha1>
    </revision>
  </page>
  <page>
    <title>Closed-world assumption</title>
    <ns>0</ns>
    <id>2526582</id>
    <revision>
      <id>751601040</id>
      <parentid>651094943</parentid>
      <timestamp>2016-11-26T18:49:13Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9923" xml:space="preserve">{{more footnotes|date=August 2011}}
The '''closed-world assumption''' (CWA), in a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], is the presumption that a statement that is true is also known to be true. Therefore, conversely, what is not currently known to be true, is false. The same name also refers to a [[formal logic|logical]] formalization of this assumption by [[Raymond Reiter]]. The opposite of the closed-world assumption is the [[open-world assumption]] (OWA), stating that lack of knowledge does not imply falsity. Decisions on CWA vs. OWA determine the understanding of the actual semantics of a conceptual expression with the same notations of concepts. A successful formalization of natural language semantics usually cannot avoid an explicit revelation of whether the implicit logical backgrounds are based on CWA or OWA.

[[Negation as failure]] is related to the closed-world assumption, as it amounts to believing false every predicate that cannot be proved to be true.

== Example ==

In the context of [[knowledge management]], the closed-world assumption is used in at least two situations: (1) when the knowledge base is known to be complete (e.g., a corporate database containing records for every employee), and (2) when the knowledge base is known to be incomplete but a "best" definite answer must be derived from incomplete information. For example, if a [[database]] contains the following table reporting editors who have worked on a given article, a query on the people not having edited the article on Formal Logic is usually expected to return "Sarah Johnson".

{| border="1" cellspacing="0" cellpadding="2" align="center"
! colspan="2" style="background:#ffdead;" | Edit
|-
! style="background:#efefef;" | Editor
! style="background:#efefef;" | Article
|-
| John Doe || Formal Logic
|-
| Joshua A. Norton || Formal Logic
|-
| Sarah Johnson || Introduction to Spatial Databases
|-
| Charles Ponzi || Formal Logic
|-
| Emma Lee-Choon || Formal Logic
|}
&lt;br /&gt; &lt;!-- The preceding tag creates a whitespace line separating the table above from the text paragraph below --&gt;
In the closed-world assumption, the table is assumed to be [[Complete knowledge base|complete]] (it lists all editor-article relationships), and Sarah Johnson is the only editor who has not edited the article on Formal Logic. In contrast, with the open-world assumption the table is not assumed to contain all editor-article tuples, and the answer to who has not edited the Formal Logic article is unknown. There is an unknown number of editors not listed in the table, and an unknown number of articles edited by Sarah Johnson that are also not listed in the table.

==Formalization in logic==

The first formalization of the closed-world assumption in [[logic|formal logic]] consists in adding to the knowledge base the negation of the literals that are not currently [[logical consequence|entailed]] by it. The result of this addition is always [[consistent]] if the knowledge base is in [[Horn clause|Horn form]], but is not guaranteed to be consistent otherwise. For example, the knowledge base
:&lt;math&gt;\{English(Fred) \vee Irish(Fred)\}&lt;/math&gt;
entails neither &lt;math&gt;English(Fred)&lt;/math&gt; nor &lt;math&gt;Irish(Fred)&lt;/math&gt;.

Adding the negation of these two literals to the knowledge base leads to
:&lt;math&gt;\{English(Fred) \vee Irish(Fred), \neg English(Fred), \neg Irish(Fred)\}&lt;/math&gt;
which is inconsistent. In other words, this formalization of the closed-world assumption sometimes turns a consistent knowledge base into an inconsistent one. The closed-world assumption does not introduce an inconsistency on a knowledge base &lt;math&gt;K&lt;/math&gt; exactly when the intersection of all [[Herbrand model]]s of &lt;math&gt;K&lt;/math&gt; is also a model of &lt;math&gt;K&lt;/math&gt;; in the propositional case, this condition is equivalent to &lt;math&gt;K&lt;/math&gt; having a single minimal model, where a model is minimal if no other model has a subset of variables assigned to true.

Alternative formalizations not suffering from this problem have been proposed. In the following description, the considered knowledge base &lt;math&gt;K&lt;/math&gt; is assumed to be propositional. In all cases, the formalization of the closed-world assumption is based on adding to &lt;math&gt;K&lt;/math&gt; the negation of the formulae that are “free for negation” for &lt;math&gt;K&lt;/math&gt;, i.e., the formulae that can be assumed to be false. In other words, the closed-world assumption applied to a [[propositional formula]] &lt;math&gt;K&lt;/math&gt; generates the formula:
:&lt;math&gt;K \wedge \{\neg f ~|~ f \in F\}&lt;/math&gt;.
The set &lt;math&gt;F&lt;/math&gt; of formulae that are free for negation in &lt;math&gt;K&lt;/math&gt; can be defined in different ways, leading to different formalizations of the closed-world assumption. The following are the definitions of &lt;math&gt;f&lt;/math&gt; being free for negation in the various formalizations.

; CWA (closed-world assumption) : &lt;math&gt;f&lt;/math&gt; is a positive literal not entailed by &lt;math&gt;K&lt;/math&gt;;

; GCWA (generalized CWA) : &lt;math&gt;f&lt;/math&gt; is a positive literal such that, for every positive clause &lt;math&gt;c&lt;/math&gt; such that &lt;math&gt;K \not\vdash c&lt;/math&gt;, it holds &lt;math&gt;K \not\vdash c \vee f&lt;/math&gt;;&lt;ref&gt;{{Citation
 | first = Jack | last = Minker
 | author-link = Jack Minker
 | title = On indefinite databases and the closed world assumption
 | publisher = [[Springer Berlin Heidelberg]]
 | series = Lecture Notes in Computer Science
 | volume = 138
 | year = 1982
 | pages = 292–308
 | url = http://link.springer.com/chapter/10.1007/BFb0000066
 | doi = 10.1007/BFb0000066
 | isbn = 978-3-540-11558-8 }}&lt;/ref&gt;

; EGCWA (extended GCWA): same as above, but &lt;math&gt;f&lt;/math&gt; is a conjunction of positive literals;

; CCWA (careful CWA): same as GCWA, but a positive clause is only considered if it is composed of positive literals of a given set and (both positive and negative) literals from another set;

; ECWA (extended CWA): similar to CCWA, but &lt;math&gt;f&lt;/math&gt; is an arbitrary formula not containing literals from a given set.

The ECWA and the formalism of [[Circumscription (logic)|circumscription]] coincide on propositional theories. The complexity of query answering (checking whether a formula is entailed by another one under the closed-world assumption) is typically in the second level of the [[polynomial hierarchy]] for general formulae, and ranges from [[P (complexity)|P]] to [[coNP]] for [[Horn clause|Horn formulae]]. Checking whether the original closed-world assumption introduces an inconsistency requires at most a logarithmic number of calls to an [[Oracle machine|NP oracle]]; however, the exact complexity of this problem is not currently known.

==See also==

* [[Open-world assumption]]
* [[Non-monotonic logic]]
* [[Circumscription (logic)]]
* [[Negation as failure]]
* [[Default logic]]
* [[Stable model semantics]]
* [[Unique name assumption]]

==References==
{{Reflist}}
*{{cite journal |last1=Cadoli |first1=Marco |last2=Lenzerini |first2=Maurizio |title=The complexity of propositional closed world reasoning and circumscription |journal=Journal of Computer and System Sciences |date=April 1994 |volume=48 |issue=2 |pages=255–310 |doi=10.1016/S0022-0000(05)80004-2 |url=http://www.sciencedirect.com/science/article/pii/S0022000005800042 |accessdate=20 February 2013 |issn=0022-0000}}
*{{cite journal |last1=Eiter |first1=Thomas |last2=Gottlob |authorlink2=Georg Gottlob |first2=Georg |title=Propositional circumscription and extended closed-world reasoning are &lt;math&gt;\Pi^p_2&lt;/math&gt;-complete |journal=Theoretical Computer Science |date=June 1993 |volume=114 |issue=2 |pages=231–245 |doi=10.1016/0304-3975(93)90073-3 |url=http://www.sciencedirect.com/science/article/pii/0304397593900733 |accessdate=20 February 2013 |issn=0304-3975}}
*{{cite journal |last1=Rajasekar |first1=Arcot |last2=Lobo |first2=Jorge |last3=Minker |first3=Jack |authorlink3=Jack Minker |title=Weak Generalized Closed World Assumption |journal=Journal of Automated Reasoning |date=September 1989 |volume=5 |issue=3 |pages=293–307 |doi=10.1007/BF00248321 |url=http://link.springer.com/article/10.1007/BF00248321 |accessdate=20 February 2013 |publisher=Kluwer Academic Publishers |issn=0168-7433}}
*{{cite journal |last=Lifschitz |first=Vladimir |authorlink=Vladimir Lifschitz |title=Closed-world databases and circumscription |journal=Artificial Intelligence |date=November 1985 |volume=27 |issue=2 |pages=229–235 |doi=10.1016/0004-3702(85)90055-4 |url=http://www.sciencedirect.com/science/article/pii/0004370285900554 |accessdate=20 February 2013 |issn=0004-3702}}
*{{cite book |last=Reiter |first=Raymond |authorlink=Raymond Reiter |editor1-last=Gallaire |editor1-first=Hervé |editor2-last=Minker |editor2-first=Jack |editor2-link=Jack Minker |title=Logic and Data Bases |year=1978 |publisher=Plenum Press |isbn=9780306400605 |url=http://www.springer.com/computer/security+and+cryptology/book/978-0-306-40060-5 |accessdate=21 February 2013 |chapter=On Closed World Data Bases |pages=119–140}}
*{{cite journal |last1=Duan |first1=Yucong |last2=Cruz |first2=Christophe |title=Formalizing Semantic of Natural Language through Conceptualization from Existence |journal=International Journal of Innovation, Management and Technology |date=February 2011 |volume=2 |issue=1 |pages=37–42 |doi=10.7763/IJIMT.2011.V2.100 |url=http://ijimt.org/abstract/100-E00187.htm |accessdate=21 February 2013 |issn=2010-0248}}

==External links==
* https://web.archive.org/web/20090624113015/http://www.betaversion.org:80/~stefano/linotype/news/91/
* [http://owl1-1.googlecode.com/svn-history/r374/trunk/www.webont.org/owled/2005/sub12.pdf Closed World Reasoning in the Semantic Web through Epistemic Operators]
* [http://books.hammerpig.com/the-closed-world-assumption-of-databases.html Excerpt from Reiter's 1978 talk on the closed world assumption]

{{DEFAULTSORT:Closed-world assumption}}
[[Category:Logic programming]]
[[Category:Knowledge representation]]</text>
      <sha1>t2ic0nllkgwvhup9gvcmbcpcnsswq6n</sha1>
    </revision>
  </page>
  <page>
    <title>Deductive classifier</title>
    <ns>0</ns>
    <id>43342432</id>
    <revision>
      <id>730453007</id>
      <parentid>714726433</parentid>
      <timestamp>2016-07-19T03:08:05Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8117" xml:space="preserve">A '''deductive classifier''' is a type of [[artificial intelligence]] [[inference engine]]. It takes as input a set of declarations in a [[frame language]] about a domain such as medical research or molecular biology. For example, the names of [[Class hierarchy|classes, sub-classes]], properties, and restrictions on allowable values.  The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to [[Automated theorem proving|theorem provers]] in that they take as input and produce output via [[First Order Logic]]. Classifiers originated with [[KL-ONE]] [[Frame language]]s. They are increasingly significant now that they form a part in the enabling technology of the [[Semantic Web]]. Modern classifiers leverage the [[Web Ontology Language]]. The models they analyze and generate are called [[Ontologies (computer science)|ontologies]].&lt;ref&gt;{{cite journal|last=Berners-Lee|first=Tim|first2=James|last2=Hendler|first3=Ora|last3=Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}&lt;/ref&gt;

== History ==
A classic problem in [[knowledge representation]] for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.&lt;ref&gt;{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}&lt;/ref&gt;

As a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on [[modus ponens]], i.e. IF-THEN rules. [[Rule-based systems]] were the predominate knowledge representation mechanism for virtually all early [[expert systems]]. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.&lt;ref&gt;{{cite book|last=Hayes-Roth|first=Frederick|pages=6–7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|first2=Donald|last2=Waterman|first3=Douglas|last3=Lenat}}&lt;/ref&gt;

However, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an [[Ontology (computer science)|ontology]]) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The [[Loom (ontology)|LOOM language]] from ISI was heavily influenced by KL-ONE.  LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.&lt;ref&gt;{{cite journal|last1=MacGregor|first1=Robert|title=A Descriptive Classifier for the Predicate Calculus|journal=AAAI - 94 Proceedings|date=1994|url=http://www.aaai.org/Papers/AAAI/1994/AAAI94-033.pdf|accessdate=17 July 2014}}&lt;/ref&gt;

== Implementations ==
[[File:Protégé 3.4.3.png|500px|thumbnail|right|Protege Ontology Editor]]
The earliest versions of classifiers were [[Automated theorem proving|logic theorem provers]]. The first classifier to work with a [[Frame language]] was the [[KL-ONE]] classifier.&lt;ref&gt;{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers &amp; Mathematics with Applications | volume = 23 | issue = 2–5 | pages = 133–177 | year = 1992 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171–216 | year = 1985 | pmid =  | pmc = }}&lt;/ref&gt; A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language.&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}&lt;/ref&gt; In the Semantic Web the [[Protégé (software)|Protege]] tool from [[Stanford University|Stanford]] provides classifiers (also known as reasonsers) as part of the default environment.&lt;ref&gt;{{cite web|title=Protege Wiki: Reasoners that integrate with Protege|url=http://protegewiki.stanford.edu/wiki/Reasoning|publisher=Stanford University|accessdate=19 July 2014}}&lt;/ref&gt;

== External links ==
* [http://owl.man.ac.uk/factplusplus/ Fact++ Reasoner]
* [http://hermit-reasoner.com/ HermiT Reasoner]
* [http://protege.stanford.edu/ Protege Ontology Editor]

==References==
{{Reflist}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Ontology languages]]
[[Category:Classification algorithms]]</text>
      <sha1>37030torfyuzkmliwbbha1ye4nkpkse</sha1>
    </revision>
  </page>
  <page>
    <title>Flint toolkit</title>
    <ns>0</ns>
    <id>43763321</id>
    <revision>
      <id>684958210</id>
      <parentid>624928560</parentid>
      <timestamp>2015-10-09T21:37:27Z</timestamp>
      <contributor>
        <username>ASammourBot</username>
        <id>20828417</id>
      </contributor>
      <minor />
      <comment>bot: Test of replace stub category to stub template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1935" xml:space="preserve">{{COI|date=September 2014}}
Flint is a software toolkit which supports various modes of uncertainty handling, namely, [[Fuzzy_logic| Fuzzy]], [[Bayesian_inference| Bayesian]] and [[Expert_system#Certainty_factors| Certainty Theory]].

Along with [[Flex_expert_system| Flex]], Flint was licensed to the Open University as part of T396: 'Artificial intelligence for technology'.

Much of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.&lt;ref name = "AI Toolkit"&gt;{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists}} by Adrian Hopgood &lt;/ref&gt;

Flint is produced by [[Logic Programming Associates|LPA]] and runs on PCs and web servers.

==External links==
*[http://www.generation5.org/content/2003/lpainference.asp The Shape of Inference] by Clive Spenser &amp; Charles Langley, Generation5
*[http://www.generation5.org/content/2003/probmodulation.asp Probability Modulation and Non-linearity in Bayesian Networks] by Clive Spenser &amp; Charles Langley, Generation5
*[http://www.generation5.org/content/2004/defuzz.asp Defuzzification Options in Flex] by Clive Spenser &amp; Charles Langley, Generation5
*[http://www.degruyter.com/view/j/pomr.2007.14.issue-3/v10012-007-0012-2/v10012-007-0012-2.xml Application of fuzzy inference to assessment of degree of hazard to ship power plant operator] by Tomasz Kowalewski, Antoni Podsiadło &amp; Wiesław Tarełko
*[http://www.slaai.lk/proc/2007/2007.pdf#page=42 Computational Modeling in Conceptual Models: Widening Scope of Artificial Life] by Mendis, Asoka. S. Karunananda, Samaratunga
*[http://www.lpa.co.uk/fln.htm Flint Overview], LPA

== References ==
{{Reflist}}

[[Category:Expert_systems]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}</text>
      <sha1>lylypzekx98cjlrulwqdhe0dw152do1</sha1>
    </revision>
  </page>
  <page>
    <title>VisiRule</title>
    <ns>0</ns>
    <id>43833827</id>
    <revision>
      <id>650617219</id>
      <parentid>650617087</parentid>
      <timestamp>2015-03-09T15:17:33Z</timestamp>
      <contributor>
        <ip>91.142.235.80</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2453" xml:space="preserve">{{COI|date=September 2014}}

'''VisiRule''' is a graphical tool for non-programmers to develop and deliver rule-based and expert systems simply by drawing their decision logic.

VisiRule is designed for building regulatory compliance systems, financial and legal decision-making systems, and machine
diagnostic and medical systems.

VisiRule generates executable rules in the form of the [[Flex expert system|Flex expert system toolkit]] which was developed by [[Logic Programming Associates|LPA]] in 1989.

LPA set up a dedicated website for VisiRule in 2015 at http://www.visirule.co.uk

VisiRule is used as part of expert systems and decision support courses in Universities such as Uniten.&lt;ref name = "VisiRule Lab"&gt;{{citation |url=http://metalab.uniten.edu.my/~zaihisma/dss/lab/CISB434-module1.ppt | title=VisiRule Slides from Uniten}}&lt;/ref&gt;

==Academic Uses==
In RSA-Expert, VisiRule is used as a decision support tool, in which the rules are basically and precisely presented using a Logic Programming model. RSA-Expert aims to assist researchers in making a decision about utilizing suitable statistical data analysis in research.&lt;ref name = "RSA-Expert"&gt;{{citation |url=http://iiste.org/Journals/index.php/IKM/article/view/4740 | title=Use of a Rule Tool in Data Analysis Decision Making}}&lt;/ref&gt;

TPA-EXPERT is a legal expert system which deals with transfer of property act of Indian legal domain. TPA-EXPERT has a simple representation structure which combine time tested rule based and case based approach.&lt;ref name = "TPA-EXPERT"&gt;{{citation |url=http://www.ijcaonline.org/archives/volume33/number9/4045-5494 | title=TPA-EXPERT: A Hybrid Legal Knowledge Based System for Indian Legal domain}}&lt;/ref&gt;

==External links==
* [http://www.lpa.co.uk/vsr.htm VisiRule Overview, LPA website]
* [http://www.visirule.co.uk/ VisiRule website]
* [http://www.pcai.com/18.3_sample_issue/18.3%20sample%20PDF/PCAI_LPA-pg.29-30-Sample_Issue.pdf "The Visual Development of Rule-Based Systems", PCAI Magazine]
* [http://www.academicpub.com/map/items/2999654.html "Drawing on your Knowledge with VisiRule", C.Spenser, IEEE]

==See also==
* [[Expert system]]
* [[Inference engine]]
* [[Knowledge base]]
* [[Knowledge-based system]]
* [[Knowledge representation]]

== References ==
{{Reflist}}

[[Category:Artificial intelligence stubs]]
[[Category:Expert systems]]
[[Category:Rule engines]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]</text>
      <sha1>acu25fqb5nxa1ezg9iy6xg9v5n6vahu</sha1>
    </revision>
  </page>
  <page>
    <title>WYSIWYM (interaction technique)</title>
    <ns>0</ns>
    <id>15998635</id>
    <revision>
      <id>725284704</id>
      <parentid>724260923</parentid>
      <timestamp>2016-06-14T18:26:04Z</timestamp>
      <contributor>
        <username>Kvng</username>
        <id>910180</id>
      </contributor>
      <comment>[[WP:DEPROD]] see talk for details</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2901" xml:space="preserve">{{for|the editing paradigm|WYSIWYM}}

'''What you see is what you meant''' ('''WYSIWYM''') is a text editing [[interaction technique]] that emerged from two projects at [[University of Brighton]]. It allows users to create abstract [[knowledge representation]]s such as those required by the [[Semantic Web]] using a natural language interface. [[Natural language understanding]] (NLU) technology is not employed. Instead,  [[natural language generation]] (NLG) is used in a highly interactive manner.

The text editor accepts repeated refinement of a selected span of text as it becomes increasingly less vacuous of authored semantics. Using a mouse, a text property held in the evolving text can be further refined by a set of options derived by NLG from a built-in [[ontology]]. An invisible representation of the semantic knowledge is created which can be used for multilingual document generation, formal knowledge formation, or any other task that requires formally specified information.&lt;ref&gt;{{Cite web|url = http://oro.open.ac.uk/39116/1/Thesis_Final.pdf|title = Generating Natural Language Explanations For Entailments In Ontologies|date = 2013|accessdate = 10 November 2014|website = Open Research Online|publisher = The Open University|last = Nguyen|first = Tu}}&lt;/ref&gt;

The two projects at Brighton worked in the field of Conceptual Authoring to lay a foundation for further research and development of a Semantic Web Authoring Tool (SWAT). This tool has been further explored as a means for developing a knowledge base by those without prior experience with Controlled Natural Language tools.&lt;ref&gt;{{Cite journal|url = http://oro.open.ac.uk/40385/|title = How easy is it to learn a controlled natural language for building a knowledge base?|last = Williams|first = Sandra|date = 13 June 2014|journal = Fourth Workshop on Controlled Natural Language, 20–22 August 2014, Galway, Ireland (forthcoming), Springer International Publishing AG.|accessdate = 10 November 2014|doi = |pmid = }}&lt;/ref&gt;

==See also==
*[[Semantic markup]]
* [[Web Ontology Language|OWL [Web Ontology Language]]]
* [[WYSIWYM]]
* [[Protégé (software)]]

== References ==
&lt;references /&gt;

==External links==
* Nguyen, Tu (2013).  ''[http://oro.open.ac.uk/39116/ Generating Natural Language Explanations For Entailments In Ontologies.]  ''PhD thesis The Open University.
* [http://mcs.open.ac.uk/nlg/research/Conceptual_Authoring.html Conceptual Authoring] at Natural Language Generation group of the Open University
* [http://mcs.open.ac.uk/nlg/SWAT/ SWAT: Semantic Web Authoring Tool] research project
* [http://mcs.open.ac.uk/nlg/old_projects/wysiwym/ WYSIWYM home page]

{{DEFAULTSORT:WYSIWYM (interaction technique)}}
[[Category:Knowledge representation]]
[[Category:Natural language generation]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Semantic Web]]


{{software-stub}}</text>
      <sha1>r5b8okhr3z8mlxshjk9brtdb2x4b8z8</sha1>
    </revision>
  </page>
  <page>
    <title>Argument map</title>
    <ns>0</ns>
    <id>6190251</id>
    <revision>
      <id>762093638</id>
      <parentid>758094953</parentid>
      <timestamp>2017-01-26T16:27:12Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>/* Key features of an argument map */Typo fixing, replaced: The the → The using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="42966" xml:space="preserve">[[File:Argument Map.png|thumb|A schematic argument map showing a contention (or conclusion), supporting arguments and objections, and an inference objection.]]
In [[informal logic]] and [[philosophy]], an '''argument map''' or '''argument diagram''' is a visual representation of the structure of an [[argument]]. An argument map typically includes the key components of the argument, traditionally called the ''[[Logical consequence|conclusion]]'' and the ''[[premise]]s'', also called ''[[Main contention|contention]]'' and ''[[Reason (argument)|reason]]s''.&lt;ref&gt;{{harvnb|Freeman|1991|pp=49–90}}&lt;/ref&gt; Argument maps can also show [[co-premise]]s, [[Objection (argument)|objection]]s, [[counterargument]]s, [[rebuttal]]s, and [[Lemma (logic)|lemma]]s. There are different styles of argument map but they are often functionally equivalent and represent an argument's individual claims and the relationships between them.

Argument maps are commonly used in the context of teaching and applying [[critical thinking]].&lt;ref&gt;For example: {{harvnb|Davies|2012}}; {{harvnb|Facione|2013|p=86}}; {{harvnb|Fisher|2004}}; {{harvnb|Kelley|2014|p=73}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}; {{harvnb|Walton|2013|p=10}}&lt;/ref&gt; The purpose of mapping is to uncover the logical structure of arguments, identify unstated assumptions, evaluate the support an argument offers for a conclusion, and aid understanding of debates. Argument maps are often designed to support deliberation of issues, ideas and arguments in [[wicked problem]]s.&lt;ref&gt;For example: {{harvnb|Culmsee|Awati|2013}}; {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Metcalfe|Sastrowardoyo|2013}}; Ricky Ohl, [//books.google.com/books?id=loa4BAAAQBAJ&amp;pg=PA360 "Computer supported argument visualisation: modelling in consultative democracy around wicked problems"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=361–380}}&lt;/ref&gt;

An argument map is not to be confused with a [[concept map]] or a [[mind map]], which are less strict in relating claims.

==Key features of an argument map==
A number of different kinds of argument map have been proposed but the most common, which Chris Reed and Glenn Rowe called the ''standard diagram'',&lt;ref name="ReedRowe64"&gt;{{harvnb|Reed|Rowe|2007|p=64}}&lt;/ref&gt; consists of a [[tree structure]] with each of the reasons leading to the conclusion. There is no consensus as to whether the conclusion should be at the top of the tree with the reasons leading up to it or whether it should be at the bottom with the reasons leading down to it.&lt;ref name="ReedRowe64"/&gt; Another variation diagrams an argument from left to right.&lt;ref&gt;For example: {{harvnb|Walton|2013|pp=18–20}}&lt;/ref&gt;

According to [[Doug Walton]] and colleagues, an argument map has two basic components: "One component is a set of circled numbers arrayed as points. Each number represents a proposition (premise or conclusion) in the argument being diagrammed. The other component is a set of lines or arrows joining the points. Each line (arrow) represents an inference. The whole network of points and lines represents a kind of overview of the reasoning in the given argument..."&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|p=2}}&lt;/ref&gt; With the introduction of software for producing argument maps, it has become common for argument maps to consist of boxes containing the actual propositions rather than numbers referencing those propositions.

There is disagreement on the terminology to be used when describing argument maps,&lt;ref&gt;{{harvnb|Freeman|1991|pp=49–90}}; {{harvnb|Reed|Rowe|2007}}&lt;/ref&gt; but the ''standard diagram'' contains the following structures:

'''Dependent premises''' or '''co-premises''', where at least one of the joined premises requires another premise before it can give support to the conclusion: An argument with this structure has been called a ''linked'' argument.&lt;ref&gt;{{harvnb|Harrell|2010|p=19}}&lt;/ref&gt;

[[File:Dependent premises.jpg|thumb|centre|100px|Statements 1 and 2 are dependent premises or co-premises]]

'''Independent premises''', where the premise can support the conclusion on its own: Although independent premises may jointly make the conclusion more convincing, this is to be distinguished from situations where a premise gives no support unless it is joined to another premise. Where several premises or groups of premises lead to a final conclusion the argument might be described as ''convergent''. This is distinguished from a ''divergent'' argument where a single premise might be used to support two separate conclusions.&lt;ref&gt;{{harvnb|Freeman|1991|pp=91–110}}; {{harvnb|Harrell|2010|p=20}}&lt;/ref&gt;

[[File:Independent premises.jpg|thumb|centre|150px|Statements 2, 3, 4 are independent premises]]

'''Intermediate conclusions''' or '''sub-conclusions''', where a claim is supported by another claim that is used in turn to support some further claim, i.e. the final conclusion or another intermediate conclusion: In the following diagram, statement '''4''' is an intermediate conclusion in that it is a conclusion in relation to statement '''5''' but is a premise in relation to the final conclusion, i.e. statement '''1'''. An argument with this structure is sometimes called a ''complex'' argument. If there is a single chain of claims containing at least one intermediate conclusion, the argument is sometimes described as a ''serial'' argument or a ''chain'' argument.&lt;ref&gt;{{harvnb|Beardsley|1950|pp=18–19}}; {{harvnb|Reed|Walton|Macagno|2007|pp=3–8}}; {{harvnb|Harrell|2010|pp=19–21}}&lt;/ref&gt;

[[File:Intermediate conclusion.jpg|thumb|centre|150px|Statement 4 is an intermediate conclusion or sub-conclusion]]

Each of these structures can be represented by the equivalent "box and line" approach to argument maps. In the following diagram, the ''contention'' is shown at the top, and the boxes linked to it represent supporting ''reasons'', which comprise one or more ''premises''. The green arrow indicates that the two ''reasons'' support the ''contention'':

[[File:A box and line diagram.png|thumb|center|A box and line diagram]]

Argument maps can also represent counterarguments. In the following diagram, the two ''objections'' weaken the ''contention'', while the ''reasons'' support the ''premise'' of the objection:

[[File:A sample argument using objections.png|thumb|center|A sample argument using objections]]

==Representing an argument as an argument map==
A written text can be transformed into an argument map by following a sequence of steps. [[Monroe Beardsley]]'s 1950 book ''Practical Logic'' recommended the following procedure:&lt;ref name="Beardsley"&gt;{{harvnb|Beardsley|1950}}&lt;/ref&gt;
#Separate statements by brackets and number them.
#Put circles around the logical indicators.
#Supply, in parenthesis, any logical indicators that are left out.
#Set out the statements in a diagram in which arrows show the relationships between statements.

[[File:Diagram using Beardsley's procedure.jpg|thumb|right|100px|A diagram of the example from Beardsley's ''Practical Logic'']]

Beardsley gave the first example of a text being analysed in this way:

:Though &lt;span style="color:red;"&gt;① [&lt;/span&gt;people who talk about the "social significance" of the arts don’t like to admit it&lt;span style="color:red;"&gt;]&lt;/span&gt;, &lt;span style="color:red;"&gt;② [&lt;/span&gt;music and painting are bound to suffer when they are turned into mere vehicles for propaganda&lt;span style="color:red;"&gt;]&lt;/span&gt;. &lt;span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;"&gt;For&lt;/span&gt; &lt;span style="color:red;"&gt;③ [&lt;/span&gt;propaganda appeals to the crudest and most vulgar feelings&lt;span style="color:red;"&gt;]&lt;/span&gt;: &lt;span style="color:red;"&gt;(for)&lt;/span&gt; &lt;span style="color:red;"&gt;④ [&lt;/span&gt;look at the academic monstrosities produced by the official Nazi painters&lt;span style="color:red;"&gt;]&lt;/span&gt;. What is more important, &lt;span style="color:red;"&gt;⑤ [&lt;/span&gt;art must be an end in itself for the artist&lt;span style="color:red;"&gt;]&lt;/span&gt;, &lt;span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;"&gt;because&lt;/span&gt; &lt;span style="color:red;"&gt;⑥ [&lt;/span&gt;the artist can do the best work only in an atmosphere of complete freedom&lt;span style="color:red;"&gt;]&lt;/span&gt;.

Beardsley said that the conclusion in this example is statement ②. Statement ④ needs to be rewritten as a declarative sentence, e.g. "Academic monstrosities [were] produced by the official Nazi painters." Statement ① points out that the conclusion isn't accepted by everyone, but statement ① is omitted from the diagram because it doesn't support the conclusion. Beardsley said that the logical relation between statement ③ and statement ④ is unclear, but he proposed to diagram statement ④ as supporting statement ③.

[[File:Using Harrell's procedure.jpg|thumb|right|200px|A box and line diagram of Beardsley's example, produced using Harrell's procedure]]

More recently, philosophy professor Maralee Harrell recommended the following procedure:&lt;ref&gt;{{harvnb|Harrell|2010|p=28}}&lt;/ref&gt;
#Identify all the claims being made by the author.
#Rewrite them as independent statements, eliminating non-essential words.
#Identify which statements are premises, sub-conclusions, and the main conclusion.
#Provide missing, implied conclusions and implied premises. (This is optional depending on the purpose of the argument map.)
#Put the statements into boxes and draw a line between any boxes that are linked.
#Indicate support from premise(s) to (sub)conclusion with arrows.

Argument maps are useful not only for representing and analyzing existing writings, but also for thinking through issues as part of a [[Problem structuring methods|problem-structuring process]] or [[writing process]]. The use of such argument analysis for thinking through issues has been called "reflective argumentation".&lt;ref&gt;For example: {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Hoffmann|2015}}&lt;/ref&gt;

==History==

===The philosophical origins and tradition of argument mapping===
[[File:Whatley.png|thumb|From Whately's Elements of Logic p467, 1852 edition]]
In the ''Elements of Logic'', which was published in 1826 and issued in many subsequent editions,&lt;ref&gt;{{harvnb|Whately|1834}} (first published 1826)&lt;/ref&gt; Archbishop [[Richard Whately]] gave probably the first form of an argument map, introducing it with the suggestion that "many students probably will find it a very clear and convenient mode of exhibiting the logical analysis of the course of argument, to draw it out in the form of a Tree, or Logical Division".

However, the technique did not become widely used, possibly because for complex arguments, it involved much writing and rewriting of the premises.

[[File:Wigmore chart.png|thumb|Wigmore evidence chart, from 1905]]
Legal philosopher and theorist [[John Henry Wigmore]] produced maps of legal arguments using numbered premises in the early 20th century,&lt;ref&gt;{{harvnb|Wigmore|1913}}&lt;/ref&gt; based in part on the ideas of 19th century philosopher [[Henry Sidgwick]] who used lines to indicate relations between terms.&lt;ref&gt;{{harvnb|Goodwin|2000}}&lt;/ref&gt;

===Anglophone argument diagramming in the 20th century===
Dealing with the failure of [[Formal system|formal]] reduction of informal argumentation, English speaking [[argumentation theory]] developed diagrammatic approaches to informal reasoning over a period of fifty years.

[[Monroe Beardsley]] proposed a form of argument diagram in 1950.&lt;ref name="Beardsley"/&gt; His method of marking up an argument and representing its components with linked numbers became a standard and is still widely used. He also introduced terminology that is still current describing ''convergent'', ''divergent'' and ''serial'' arguments.

[[File:Toulmindiag.png|thumb|A Toulmin argument diagram, redrawn from his 1959 ''Uses of Argument'']]
[[File:Toulmingeneral.png|thumb|A generalised Toulmin diagram]]
[[Stephen Toulmin]], in his groundbreaking and influential 1958 book ''The Uses of Argument'',&lt;ref&gt;{{harvnb|Toulmin|2003}} (first published 1958)&lt;/ref&gt; identified several elements to an argument which have been generalized. The Toulmin diagram is widely used in educational critical teaching.&lt;ref name="Simon06"&gt;{{harvnb|Simon|Erduran|Osborne|2006}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Böttcher|Meisert|2011}}; {{harvnb|Macagno|Konstantinidou|2013}}&lt;/ref&gt; Whilst Toulmin eventually had a significant impact on the development of [[informal logic]] he had little initial impact and the Beardsley approach to diagramming arguments along with its later developments became the standard approach in this field. Toulmin introduced something that was missing from Beardsley's approach. In Beardsley, "arrows link reasons and conclusions (but) no support is given to the implication itself between them. There is no theory, in other words, of inference distinguished from logical deduction, the passage is always deemed not controversial and not subject to support and evaluation".&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|p=8}}&lt;/ref&gt; Toulmin introduced the concept of ''warrant'' which "can be considered as representing the reasons behind the inference, the backing that authorizes the link".&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|p=9}}&lt;/ref&gt;

Beardsley's approach was refined by Stephen N. Thomas, whose 1973 book ''Practical Reasoning In Natural Language''&lt;ref&gt;{{harvnb|Thomas|1997}} (first published 1973)&lt;/ref&gt; introduced the term ''linked'' to describe arguments where the premises necessarily worked together to support the conclusion.&lt;ref name="SnoeckHenkemans"&gt;{{harvnb|Snoeck Henkemans|2000|p=453}}&lt;/ref&gt; However, the actual distinction between dependent and independent premises had been made prior to this.&lt;ref name="SnoeckHenkemans"/&gt; The introduction of the linked structure made it possible for argument maps to represent missing or "hidden" premises. In addition, Thomas suggested showing reasons both ''for'' and ''against'' a conclusion with the reasons ''against'' being represented by dotted arrows. Thomas introduced the term ''argument diagram'' and defined ''basic reasons'' as those that were not supported by any others in the argument and the ''final conclusion'' as that which was not used to support any further conclusion.

[[File:Scrivendiag.png|thumb|Scriven's argument diagram. The explicit premise 1 is conjoined with additional unstated premises a and b to imply 2.]]
[[Michael Scriven]] further developed the Beardsley-Thomas approach in his 1976 book ''Reasoning''.&lt;ref&gt;{{harvnb|Scriven|1976}}&lt;/ref&gt; Whereas Beardsley had said "At first, write out the statements...after a little practice, refer to the statements by number alone"&lt;ref&gt;{{harvnb|Beardsley|1950|p=21}}&lt;/ref&gt; Scriven advocated clarifying the meaning of the statements, listing them and then using a tree diagram with numbers to display the structure. Missing premises (unstated assumptions) were to be included and indicated with an alphabetical letter instead of a number to mark them off from the explicit statements. Scriven introduced counterarguments in his diagrams, which Toulmin had defined as rebuttal.&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|pp=10–11}}&lt;/ref&gt; This also enabled the diagramming of "balance of consideration" arguments.&lt;ref&gt;{{harvnb|van Eemeren|Grootendorst|Snoeck Henkemans|Blair|1996|p=175}}&lt;/ref&gt;

In the 1990s, [[Tim van Gelder]] and colleagues developed a series of computer software applications that permitted the premises to be fully stated and edited in the diagram, rather than in a legend.&lt;ref&gt;{{harvnb|van Gelder|2007}}&lt;/ref&gt; Van Gelder's first program, Reason!Able, was superseded by two subsequent programs, bCisive and Rationale.&lt;ref&gt;{{harvnb|Berg|van Gelder|Patterson|Teppema|2009}}&lt;/ref&gt;

Throughout the 1990s and 2000s, many other software applications were developed for argument visualization. By 2013, more than 60 such software systems existed.&lt;ref&gt;{{harvnb|Walton|2013|p=11}}&lt;/ref&gt; One of the differences between these software systems is whether collaboration is supported.&lt;ref name="Scheuer10"&gt;{{harvnb|Scheuer|Loll|Pinkwart|McLaren|2010}}&lt;/ref&gt; Single-user argumentation systems include Convince Me, iLogos, LARGO, Athena, [[Araucaria (software)|Araucaria]], and Carneades; small group argumentation systems include Digalo, QuestMap, [[Compendium (software)|Compendium]], Belvedere, and AcademicTalk; community argumentation systems include [[Debategraph]] and [[Collaboratorium]].&lt;ref name="Scheuer10"/&gt; For more software examples, see: {{section link||External links}}.

In 1998 a series of large-scale argument maps released by [[Robert E. Horn]] stimulated widespread interest in argument mapping.&lt;ref&gt;{{harvnb|Holmes|1999}}; {{harvnb|Horn|1998}} and Robert E. Horn, [http://www.stanford.edu/~rhorn/a/topic/arg/artclCmptrSpArgmttn.pdf "Infrastructure for navigating interdisciplinary debates: critical decisions for representing argumentation"], in {{harvnb|Kirschner|Buckingham Shum|Carr|2003|pp=165–184}}&lt;/ref&gt;

==Applications==
Argument maps have been applied in many areas, but foremost in educational, academic and business settings, including [[design rationale]].&lt;ref name="Applications"&gt;{{harvnb|Kirschner|Buckingham Shum|Carr|2003}}; {{harvnb|Okada|Buckingham Shum|Sherborne|2014}}&lt;/ref&gt; Argument maps are also used in [[forensic science]],&lt;ref&gt;For example: {{harvnb|Bex|2011}}&lt;/ref&gt; [[law]], and [[artificial intelligence]].&lt;ref&gt;For example: {{harvnb|Verheij|2005}}; {{harvnb|Reed|Walton|Macagno|2007}}; {{harvnb|Walton|2013}}&lt;/ref&gt; It has also been proposed that argument mapping has a great potential to improve how we understand and execute democracy, in reference to the ongoing evolution of [[e-democracy]].&lt;ref&gt;{{harvnb|Hilbert|2009}}&lt;/ref&gt;

===Difficulties with the philosophical tradition===
It has traditionally been hard to separate teaching critical thinking from the philosophical tradition of teaching [[logic]] and method, and most critical thinking textbooks have been written by philosophers. [[Informal logic]] textbooks are replete with philosophical examples, but it is unclear whether the approach in such textbooks transfers to non-philosophy students.&lt;ref name="Simon06" /&gt; There appears to be little statistical effect after such classes. Argument mapping, however, has a measurable effect according to many studies.&lt;ref&gt;{{harvnb|Twardy|2004}}; {{harvnb|Álvarez Ortiz|2007}}; {{harvnb|Harrell|2008}}; Yanna Rider and Neil Thomason, [//books.google.com/books?id=loa4BAAAQBAJ&amp;pg=PA112 "Cognitive and pedagogical benefits of argument mapping: LAMP guides the way to better thinking"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=113–134}}; {{harvnb|Dwyer|2011}}; {{harvnb|Davies|2012}}&lt;/ref&gt; For example, instruction in argument mapping has been shown to improve the critical thinking skills of business students.&lt;ref&gt;{{harvnb|Carrington|Chen|Davies|Kaur|2011}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}&lt;/ref&gt;

===Evidence that argument mapping improves critical thinking ability===
There is empirical evidence that the skills developed in argument-mapping-based critical thinking courses substantially transfer to critical thinking done without argument maps. Alvarez's meta-analysis found that such critical thinking courses produced gains of around 0.70 SD, about twice as much as standard critical-thinking courses.&lt;ref&gt;{{harvnb|Álvarez Ortiz|2007|pp=69–70 ''et seq''}}&lt;/ref&gt; The tests used in the reviewed studies were standard critical-thinking tests.

===How argument mapping helps with critical thinking===
The use of argument mapping has occurred within a number of disciplines, such as philosophy, management reporting, military and intelligence analysis, and public debates.&lt;ref name="Applications"/&gt;

''Logical structure'': Argument maps display an argument's logical structure more clearly than does the standard linear way of presenting arguments.

''Critical thinking concepts'': In learning to argument map, students master such key critical thinking concepts as "reason", "objection", "premise", "conclusion", "inference", "rebuttal", "unstated assumption", "co-premise", "strength of evidence", "logical structure", "independent evidence", etc. Mastering such concepts is not just a matter of memorizing their definitions or even being able to apply them correctly; it is also understanding why the distinctions these words mark are important and using that understanding to guide one's reasoning.

''Visualization'': Humans are highly visual and argument mapping may provide students with a basic set of visual schemas with which to understand argument structures.

''More careful reading and listening'': Learning to argument map teaches people to read and listen more carefully, and highlights for them the key questions "What is the logical structure of this argument?" and "How does this sentence fit into the larger structure?" In-depth cognitive processing is thus more likely.

''More careful writing and speaking'': Argument mapping helps people to state their reasoning and evidence more precisely, because the reasoning and evidence must fit explicitly into the map's logical structure.

''Literal and intended meaning'': Often, many statements in an argument do not precisely assert what the author meant. Learning to argument map enhances the complex skill of distinguishing literal from intended meaning.

''Externalization'': Writing something down and reviewing what one has written often helps reveal gaps and clarify one's thinking. Because the logical structure of argument maps is clearer than that of linear prose, the benefits of mapping will exceed those of ordinary writing.

''Anticipating replies'': Important to critical thinking is anticipating objections and considering the plausibility of different rebuttals. Mapping develops this anticipation skill, and so improves analysis.

==Standards==

===Argument Interchange Format===
The Argument Interchange Format, AIF, is an international effort to develop a representational mechanism for exchanging argument resources between research groups, tools, and domains using a semantically rich language.&lt;ref&gt;See the [http://www.arg.dundee.ac.uk/people/chris/publications/2006/aif_final.pdf AIF original draft description] (2006) and the [http://www.argdf.org/source/ArgDF_Ontology.rdfs full AIF-RDF ontology specifications] in [[RDFS]] format.&lt;/ref&gt; AIF-RDF is the extended ontology represented in the [[Resource Description Framework]] Schema (RDFS) semantic language. Though AIF is still something of a moving target, it is settling down.&lt;ref&gt;{{harvnb|Bex|Modgil|Prakken|Reed|2013}}&lt;/ref&gt;

===Legal Knowledge Interchange Format===
The Legal Knowledge Interchange Format (LKIF),&lt;ref&gt;{{harvnb|Boer|Winkels|Vitali|2008}}&lt;/ref&gt; developed in the European ESTRELLA project,&lt;ref&gt;{{cite web |title=Estrella project website |url=http://www.estrellaproject.org/ |website=estrellaproject.org |archiveurl=https://web.archive.org/web/20160212103522/http://www.estrellaproject.org/ |archivedate=2016-02-12 |accessdate=2016-02-24}}&lt;/ref&gt; is an XML schema for rules and arguments, designed with the goal of becoming a standard for representing and interchanging policy, legislation and cases, including their justificatory arguments, in the legal domain. LKIF builds on and uses the [[Web Ontology Language]] (OWL) for representing concepts and includes a reusable basic ontology of legal concepts.

== See also ==
{{Commons category|Argument maps}}
* [[Flow (policy debate)]]
* [[Informal fallacy]]
* [[Information graphics]]
* [[Natural deduction]], a logical system with argument map-like notation

== Notes ==
{{Reflist|colwidth=20em}}

== References ==
* {{cite thesis |type=M.A. thesis |last=Álvarez Ortiz |first=Claudia María |title=Does philosophy improve critical thinking skills? |url=http://images.austhink.com/pdf/Claudia-Alvarez-thesis.pdf |year=2007 |publisher=Department of Philosophy, [[University of Melbourne]] |oclc=271475715 |ref=harv}}
* {{cite book |last=Beardsley |first=Monroe C. |authorlink=Monroe Beardsley |date=1950 |title=Practical logic |location=New York |publisher=Prentice-Hall |oclc=4318971 |ref=harv}}
* {{cite book |last1=Berg |first1=Timo ter |last2=van Gelder |first2=Tim |authorlink2=Tim van Gelder |last3=Patterson |first3=Fiona |last4=Teppema |first4=Sytske |year=2009 |title=Critical thinking: reasoning and communicating with Rationale |location=Amsterdam |publisher=Pearson Education Benelux |isbn=9043018015 |oclc=301884530 |ref=harv}}
* {{cite book |last=Bex |first=Floris J. |date=2011 |title=Arguments, stories and criminal evidence: a formal hybrid theory |series=Law and philosophy library |volume=92 |location=Dordrecht; New York |publisher=Springer |isbn=9789400701397 |oclc=663950184 |doi=10.1007/978-94-007-0140-3 |ref=harv}}
* {{cite journal |last1=Bex |first1=Floris J. |last2=Modgil |first2=Sanjay |last3=Prakken |first3=Henry |last4=Reed |first4=Chris |title=On logical specifications of the Argument Interchange Format |journal=[[Journal of Logic and Computation]] |volume=23 |issue=5 |pages=951–989 |year=2013 |doi=10.1093/logcom/exs033 |url=http://www.arg.dundee.ac.uk/people/chris/publications/2013/AIF-ASPIC-JLC-Final.pdf |ref=harv}}
* {{cite book |last1=Boer |first1=Alexander |last2=Winkels |first2=Radboud |last3=Vitali |first3=Fabio |date=2008 |chapter=MetaLex XML and the Legal Knowledge Interchange Format |editor1-last=Casanovas |editor1-first=Pompeu |editor2-last=Sartor |editor2-first=Giovanni |editor3-last=Casellas |editor3-first=Núria |editor4-last=Rubino |editor4-first=Rossella |title=Computable models of the law: languages, dialogues, games, ontologies |series=Lecture notes in computer science |volume=4884 |location=Berlin; New York |publisher=Springer |pages=21–41 |isbn=9783540855682 |oclc=244765580 |doi=10.1007/978-3-540-85569-9_2 |chapterurl=https://www.researchgate.net/profile/Radboud_Winkels/publication/225708550_MetaLex_XML_and_the_Legal_Knowledge_Interchange_Format/links/02bfe510239cc79c8d000000.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Böttcher |first1=Florian |last2=Meisert |first2=Anke |title=Argumentation in science education: a model-based framework |journal=Science &amp; Education |volume=20 |issue=2 |pages=103–140 |date=February 2011 |doi=10.1007/s11191-010-9304-5 |ref=harv}}
* {{cite journal |last1=Carrington |first1=Michal |last2=Chen |first2=Richard |last3=Davies |first3=Martin |last4=Kaur |first4=Jagjit |last5=Neville |first5=Benjamin |title=The effectiveness of a single intervention of computer‐aided argument mapping in a marketing and a financial accounting subject |journal=Higher Education Research &amp; Development |volume=30 |issue=3 |pages=387–403 |date=June 2011 |doi=10.1080/07294360.2011.559197 |url=https://www.researchgate.net/profile/Martin_Davies/publication/232947753_The_effectiveness_of_a_single_intervention_of_computeraided_argument_mapping_in_a_marketing_and_a_financial_accounting_subject/links/0deec51d94fec63bdc000000.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite book |last1=Culmsee |first1=Paul |last2=Awati |first2=Kailash |date=2013 |origyear=2011 |chapter=Chapter 7: Visualising reasoning, and Chapter 8: Argumentation-based rationale |title=The heretic's guide to best practices: the reality of managing complex problems in organisations |location=Bloomington, IN |publisher=iUniverse, Inc. |pages=153–211 |isbn=9781462058549 |oclc=767703320 |chapterurl=https://books.google.com/books?id=Gb2uuT1zrAAC&amp;pg=PA153 |ref=harv}}
* {{cite journal |last=Davies |first=Martin |title=Computer-aided argument mapping as a tool for teaching critical thinking |journal=[[International Journal of Learning and Media]] |volume=4 |issue=3-4 |pages=79–84 |date=Summer 2012 |doi=10.1162/IJLM_a_00106 |url=http://www.mitpressjournals.org/doi/full/10.1162/IJLM_a_00106 |ref=harv}}
* {{cite thesis |type=Ph.D. thesis |last=Dwyer |first=Christopher Peter |title=The evaluation of argument mapping as a learning tool |url=http://aran.library.nuigalway.ie/xmlui/bitstream/handle/10379/2617/C.Dwyer-PhD%20Thesis%20Psychology.pdf |year=2011 |publisher=School of Psychology, National University of Ireland, Galway |oclc=812818648 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Grootendorst |first2=Rob |authorlink2=Rob Grootendorst |last3=Snoeck Henkemans |first3=A. Francisca |last4=Blair |first4=J. Anthony |last5=Johnson |first5=Ralph H. |authorlink5=Ralph Johnson (philosopher) |last6=Krabbe |first6=Erik C. W. |last7=Plantin |first7=Christian |last8=Walton |first8=Douglas N. |authorlink8=Doug Walton |last9=Willard |first9=Charles A. |authorlink9=Charles Arthur Willard |last10=Woods |first10=John |authorlink10=John Woods (logician) |date=1996 |title=Fundamentals of argumentation theory: a handbook of historical backgrounds and contemporary developments |location=Mahwah, NJ |publisher=[[Lawrence Erlbaum Associates]] |isbn=0805818618 |oclc=33970847 |doi=10.4324/9780203811306 |ref=harv}}
* {{cite book |last=Facione |first=Peter A. |title=THINK critically |year=2013 |origyear=2011 |edition=2nd |location=Boston |publisher=Pearson |isbn=0205490980 |oclc=770694200 |ref=harv}}
* {{cite book |last=Fisher |first=Alec |title=The logic of real arguments |url=https://books.google.com/books?id=Q2a_RtDZUGgC |year=2004 |origyear=1988 |edition=2nd |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521654815 |oclc=54400059 |doi=10.1017/CBO9780511818455 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Freeman |first=James B. |title=Dialectics and the macrostructure of arguments: a theory of argument structure |url=https://books.google.com/books?id=ScvV9riTOGsC |year=1991 |location=Berlin; New York |publisher=Foris Publications |isbn=3110133903 |oclc=24429943 |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last=van Gelder |first=Tim |authorlink=Tim van Gelder |title=The rationale for Rationale |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=23–42 |year=2007 |doi=10.1093/lpr/mgm032 |url=http://sites.google.com/site/timvangelder/publications-1/therationaleforrationale/TheRationaleforRationale.pdf |ref=harv}}
* {{cite journal |last=Goodwin |first=Jean |title=Wigmore's chart method |journal=Informal Logic |volume=20 |issue=3 |pages=223–243 |year=2000 |url=http://windsor.scholarsportal.info/ojs/leddy/index.php/informal_logic/article/view/2278 |ref=harv}}
* {{cite journal |last=Harrell |first=Maralee |title=No computer program required: even pencil-and-paper argument mapping improves critical-thinking skills |journal=[[Teaching Philosophy]] |volume=31 |issue=4 |pages=351–374 |date=December 2008 |doi=10.5840/teachphil200831437 |url=http://www.hss.cmu.edu/philosophy/harrell/HarrellTeachingPhilosophy2008.pdf |ref=harv}}
* {{cite web |last=Harrell |first=Maralee |title=Creating argument diagrams |date=August 2010 |url=http://www.academia.edu/772321/Creating_Argument_Diagrams |website=[[academia.edu]] |ref=harv}}
* {{cite journal |last=Hilbert |first=Martin |title=The maturing concept of e-democracy: from e-voting and online consultations to democratic value out of jumbled online chatter |journal=[[Journal of Information Technology and Politics]] |date=April 2009 |volume=6 |issue=2 |pages=87–110 |doi=10.1080/19331680802715242 |url=http://www.martinhilbert.net/e-democracyHilbertJITP.pdf |ref=harv}}
* {{cite journal |last=Hoffmann |first=Michael H. G. |date=November 2015 |title=Reflective argumentation: a cognitive function of arguing |journal=Argumentation |doi=10.1007/s10503-015-9388-9 |ref=harv}}
* {{cite journal |last1=Hoffmann |first1=Michael H. G. |last2=Borenstein |first2=Jason |date=February 2013 |title=Understanding ill-structured engineering ethics problems through a collaborative learning and argument visualization approach |journal=Science and Engineering Ethics |volume=20 |issue=1 |pages=261–276 |doi=10.1007/s11948-013-9430-y |pmid=23420467 |url=http://works.bepress.com/michael_hoffmann/39/ |ref=harv}}
* {{cite news |last=Holmes |first=Bob |title=Beyond words |url=http://www.newscientist.com/article/mg16321944.700-beyond-words.html |date=10 July 1999 |newspaper=New Scientist |issue=2194 |archiveurl=https://web.archive.org/web/20080928062226/http://www.newscientist.com/article/mg16321944.700-beyond-words.html |archivedate=28 September 2008 |ref=harv}}
* {{cite book |last=Horn |first=Robert E. |authorlink=Robert E. Horn |title=Visual language: global communication for the 21st century |year=1998 |location=Bainbridge Island, WA |publisher=MacroVU, Inc. |isbn=189263709X |oclc=41138655 |ref=harv}}
* {{cite book |last=Kelley |first=David |authorlink=David Kelley |title=The art of reasoning: an introduction to logic and critical thinking |year=2014 |origyear=1988 |edition=4th |location=New York |publisher=W. W. Norton &amp; Company |isbn=0393930785 |oclc=849801096 |ref=harv}}
* {{cite book |editor1-last=Kirschner |editor1-first=Paul Arthur |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Carr |editor3-first=Chad S |title=Visualizing argumentation: software tools for collaborative and educational sense-making |year=2003 |series=Computer supported cooperative work |location=New York |publisher=Springer |isbn=1852336641 |oclc=50676911 |doi=10.1007/978-1-4471-0037-9 |url=https://books.google.com/books?id=dNijwv-my_kC |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Kunsch |first1=David W. |last2=Schnarr |first2=Karin |last3=van Tyle |first3=Russell |title=The use of argument mapping to enhance critical thinking skills in business education |journal=Journal of Education for Business |volume=89 |issue=8 |pages=403–410 |date=November 2014 |doi=10.1080/08832323.2014.925416 |url= |ref=harv}}
* {{cite journal |last1=Macagno |first1=Fabrizio |last2=Konstantinidou |first2=Aikaterini |title=What students' arguments can tell us: using argumentation schemes in science education |journal=Argumentation |volume=27 |issue=3 |pages=225–243 |date=August 2013 |doi=10.1007/s10503-012-9284-5 |url=http://ssrn.com/abstract=2185945 |ref=harv}}
* {{cite journal |last1=Metcalfe |first1=Mike |last2=Sastrowardoyo |first2=Saras |date=November 2013 |title=Complex project conceptualisation and argument mapping |journal=International Journal of Project Management |volume=31 |issue=8 |pages=1129–1138 |doi=10.1016/j.ijproman.2013.01.004 |ref=harv}}
* {{cite book |editor1-last=Okada |editor1-first=Alexandra |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Sherborne |editor3-first=Tony |year=2014 |origyear=2008 |title=Knowledge cartography: software tools and mapping techniques |edition=2nd |series=Advanced information and knowledge processing |location=New York |publisher=Springer |isbn=9781447164692 |oclc=890438015 |doi=10.1007/978-1-4471-6470-8 |url=https://books.google.com/books?id=loa4BAAAQBAJ |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Reed |first1=Chris |last2=Rowe |first2=Glenn |title=A pluralist approach to argument diagramming |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=59–85 |year=2007 |doi=10.1093/lpr/mgm030 |url=http://lpr.oxfordjournals.org/content/6/1-4/59.full.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Reed |first1=Chris |last2=Walton |first2=Douglas |authorlink2=Doug Walton |last3=Macagno |first3=Fabrizio |title=Argument diagramming in logic, law and artificial intelligence |journal=The Knowledge Engineering Review |volume=22 |issue=1 |pages=1–22 |date=March 2007 |doi=10.1017/S0269888907001051 |url=https://www.academia.edu/2992281/Argument_Diagramming_in_Logic_Artificial_Intelligence_and_Law |ref=harv}}
* {{cite journal |last1=Scheuer |first1=Oliver |last2=Loll |first2=Frank |last3=Pinkwart |first3=Niels |last4=McLaren |first4=Bruce M. |title=Computer-supported argumentation: a review of the state of the art |journal=International Journal of Computer-Supported Collaborative Learning |volume=5 |issue=1 |pages=43–102 |year=2010 |doi=10.1007/s11412-009-9080-x |url=http://www.oliver-scheuer.info/publications/ScheuerEtAl-EdArgSystemsReview-IJCSCL-2010.pdf |ref=harv}}
* {{cite book |last=Scriven |first=Michael |authorlink=Michael Scriven |title=Reasoning |year=1976 |location=New York |publisher=McGraw-Hill |isbn=0070558825 |oclc=2800373 |ref=harv}}
* {{cite journal |last1=Simon |first1=Shirley |last2=Erduran |first2=Sibel |last3=Osborne |first3=Jonathan |title=Learning to teach argumentation: research and development in the science classroom |journal=International Journal of Science Education |volume=28 |issue=2-3 |pages=235–260 |year=2006 |doi=10.1080/09500690500336957 |url=http://cset.stanford.edu/sites/default/files/files/documents/publications/Simon-Learning%20to%20Teach%20Argumentation.pdf |ref=harv}}
* {{cite journal |last=Snoeck Henkemans |first=A. Francisca |date=November 2000 |title=State-of-the-art: the structure of argumentation |journal=Argumentation |volume=14 |issue=4 |pages=447–473 |doi=10.1023/A:1007800305762 |ref=harv}}
* {{cite book |last=Thomas |first=Stephen N. |date=1997 |origyear=1973 |title=Practical reasoning in natural language |edition=4th |location=Upper Saddle River, NJ |publisher=[[Prentice-Hall]] |isbn=0136782698 |oclc=34745923 |ref=harv}}
* {{cite book |last=Toulmin |first=Stephen E. |authorlink=Stephen Toulmin |title=The uses of argument |url=https://books.google.com/books?id=8UYgegaB1S0C |year=2003 |origyear=1958 |edition=Updated |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521534836 |oclc=57253830 |doi=10.1017/CBO9780511840005 |accessdate=2016-02-24 }}
* {{cite journal |last=Twardy |first=Charles R. |title=Argument maps improve critical thinking |journal=[[Teaching Philosophy]] |volume=27 |issue=2 |pages=95–116 |date=June 2004 |doi=10.5840/teachphil200427213 |url=http://cogprints.org/3008/1/reasonpaper.pdf |ref=harv}}
* {{cite book |last=Verheij |first=Bart |title=Virtual arguments: on the design of argument assistants for lawyers and other arguers |year=2005 |location=The Hague |publisher=T.M.C. Asser Press |series=Information technology &amp; law series |volume=6 |isbn=9789067041904 |oclc=59617214 |ref=harv}}
* {{cite book |last=Walton |first=Douglas N. |authorlink=Doug Walton |title=Methods of argumentation |url=https://books.google.com/books?id=vaU0AAAAQBAJ |year=2013 |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=1107677335 |oclc=830523850 |doi=10.1017/CBO9781139600187 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Whately |first=Richard |authorlink=Richard Whately |title=Elements of logic: comprising the substance of the article in the Encyclopædia metropolitana: with additions, &amp;c. |url=https://books.google.com/books?id=5mgAAAAAMAAJ |year=1834 |origyear=1826 |edition=5th |location=London |publisher=B. Fellowes |oclc=1739330 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Wigmore |first=John Henry |authorlink=John Henry Wigmore |title=The principles of judicial proof: as given by logic, psychology, and general experience, and illustrated in judicial trials |url=https://books.google.com/books?id=4ho-AAAAIAAJ |year=1913 |location=Boston |publisher=Little Brown |oclc=1938596 |ref=harv |accessdate=2016-02-24}}

== Further reading ==
* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Garssen |first2=Bart |last3=Krabbe |first3=Erik C. W. |last4=Snoeck Henkemans |first4=A. Francisca |last5=Verheij |first5=Bart |last6=Wagemans |first6=Jean H. M. |date=2014 |title=Handbook of argumentation theory |location=New York |publisher=Springer |isbn=9789048194728 |oclc=871004444 |doi=10.1007/978-90-481-9473-5 |ref=harv}}
* {{cite book|last1=Facione |first1=Peter A. |last2=Facione |first2=Noreen C. |title=Thinking and reasoning in human decision making: the method of argument and heuristic analysis |year=2007 |location=Milbrae, CA |publisher=California Academic Press |isbn=1891557580 |oclc=182039452 |ref=harv}}
* {{cite web |last=van Gelder |first=Tim |authorlink=Tim van Gelder |url=http://timvangelder.com/2009/02/17/what-is-argument-mapping/ |title=What is argument mapping? |publisher=timvangelder.com |date=17 February 2009 |accessdate=12 January 2015 |ref=harv}}
* {{cite book |last=van Gelder |first=Tim |authorlink=Tim van Gelder |date=2015 |chapter=Using argument mapping to improve critical thinking skills |editor1-last=Davies |editor1-first=Martin |editor2-last=Barnett |editor2-first=Ronald |title=The Palgrave handbook of critical thinking in higher education |location=New York |publisher=[[Palgrave Macmillan]] |pages=183–192 |isbn=9781137378033 |oclc=894935460 |doi=10.1057/9781137378057_12 |ref=harv}}
* {{cite journal |last=Harrell |first=Maralee |title=Using argument diagramming software in the classroom |journal=[[Teaching Philosophy]] |volume=28 |issue=2 |pages=163–177 |date=June 2005 |doi=10.5840/teachphil200528222 |url=http://www.hss.cmu.edu/philosophy/harrell/ArgumentDiagramsInClassroom.pdf |ref=harv}}
* {{cite journal |last1=Schneider |first1=Jodi |last2=Groza |first2=Tudor |last3=Passant |first3=Alexandre |date=April 2013 |title=A review of argumentation for the social semantic web |journal=Semantic Web |volume=4 |issue=2 |pages=159–218 |url=http://semantic-web-journal.org/sites/default/files/swj138_0.pdf |ref=harv}}

== External links ==

===Argument mapping software===
*[http://araucaria.computing.dundee.ac.uk/ Araucaria] (open source, cross platform/Java)
*[http://sourceforge.net/projects/argumentative/ Argumentative] (open source, Windows); supports single-user, graphical argumentation
*[http://www.argunet.org/editor/ Argunet] (open source, cross platform)
*[http://compendiuminstitute.net/ Compendium] (open source, cross platform/Java)
*[http://www.phil.cmu.edu/projects/argument_mapping/ iLogos] (cross platform/Java)
*[http://ova.arg-tech.org/ OVA] (Web based, Online Visualisation of Argument)
*[http://www.cs.ie.niigata-u.ac.jp/Research/PIRIKA/PIRIKA.html PIRIKA (PIlot for the RIght Knowledge and Argument)] (open source, Linux, Windows)
*[http://www.vangeldermonk.com/reasoningapp/ The Reasoning PowerPoint App] ([[PowerPoint]] add-in, Windows 2007 or later)

===Online, collaborative software===
*[http://agora.gatech.edu/ AGORA-net] (user interface in English, German, Spanish, Chinese, and Russian)
*[http://arguman.org/ Arguman] (Open source, English, Turkish, and Chinese)
*[http://www.bcisiveonline.com/ bCisiveOnline]
*[http://carneades.github.io/ Carneades] (open source, argument (re)construction, evaluation, mapping and interchange)
*[https://code.google.com/p/collam/ Collam] ([[JavaScript]] library for visualizing argument maps)
*[http://www.debategraph.org/ Debategraph]
*[http://www.truthmapping.com/ TruthMapping]

[[Category:Argument mapping]]
[[Category:Arguments]]
[[Category:Critical thinking]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Problem structuring methods]]</text>
      <sha1>e3g63ewjo08cpvujyl3dxerikbdvivi</sha1>
    </revision>
  </page>
  <page>
    <title>Class (knowledge representation)</title>
    <ns>0</ns>
    <id>46926920</id>
    <revision>
      <id>666234941</id>
      <parentid>666144276</parentid>
      <timestamp>2015-06-09T20:11:30Z</timestamp>
      <contributor>
        <username>TomT0m</username>
        <id>11109968</id>
      </contributor>
      <minor />
      <comment>TomT0m moved page [[Class (Knowledge representation)]] to [[Class (knowledge representation)]]: typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2473" xml:space="preserve">{{Main|Ontology components#Classes|l1 = Classes on the ontology component page}}

In  [[Knowledge representation and reasoning|knowledge representation]], a '''class''' is a collection of individuals or objects.&lt;ref name="DLs"&gt;{{cite proceedings|url=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=110B072B7265573684AB8D4F0D6B2306?doi=10.1.1.177.2787&amp;rep=rep1&amp;type=pdf|title=Description Logics: Foundations for Class-based Knowledge Representation|author1=Diego Calvanese|author2=Giuseppe De Giacomo|author3=Maurizio Lenzerini|conference=[[Logic in Computer Science]]|year=2002}}&lt;/ref&gt; A class can be defined either by [[Extensional definition|extension]], or by [[Intensional definition|intension]], using what is called in some ontology languages like [[Web Ontology Language|OWL]]. If we follow the [[Type–token distinction]], the ontology is divided into individuals, who are real worlds objects, or events, and types, or classes, who are sets of real world objects. Class expressions or definitions gives the properties that the individuals must fulfill to be members of the class. Individuals that fulfill the property are called [[Instance (computer science)|Instances]].

== Relationships ==

=== Instantiation ===

The instantiation [[relation (mathematics)|relationship]] is a relation between objects and classes. We say that an object O, say ''Harry the eagle'' is an instance of a class, say ''Eagle''. ''Harry the eagle'' has all the properties that we can attribute to an eagle, for example his parents were eagles, he's a bird, he's a meat eater and so on. It's a special kind of [[is a]] relationship. It's noted [[Concept assertion]] (&lt;math&gt; : &lt;/math&gt;) in [[Description logic]]s, a family of logic based on classes, class assertion &lt;ref name="owlclass"&gt;{{cite web|url=http://www.w3.org/TR/owl2-syntax/#Class_Assertions|title=owl2 syntax}}&lt;/ref&gt;

=== Subsumption ===
Classes can [[is a|subsume]] each other. We say usually that if &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are classes, and all &lt;code&gt;A&lt;/code&gt; instances are also &lt;code&gt;B&lt;/code&gt; instances, then B subsumes A, or A is a subclass of B, for example in the OWL Language it's called subclassof.&lt;ref name="owlclass"/&gt;

==References==
&lt;references/&gt;

== See also ==
* [[Metaclass (Semantic Web)]]
* [[Ontology (information science)|Ontology]]
* [[Ontology components]]
* [[Description logic]]

[[Category:Knowledge representation]]
[[Category:Semantic Web Ontology]]


{{Information-science-stub}}</text>
      <sha1>fx325ir9o10aeasvogjnqra6vjcan5u</sha1>
    </revision>
  </page>
  <page>
    <title>WordNet</title>
    <ns>0</ns>
    <id>33955</id>
    <revision>
      <id>763075132</id>
      <parentid>760466563</parentid>
      <timestamp>2017-02-01T05:10:15Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor />
      <comment>/* Knowledge structure */ {{tree list}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33042" xml:space="preserve">'''WordNet''' is a [[lexical database]] for the [[English language]].&lt;ref&gt;G. A. Miller, R. Beckwith, C. D. Fellbaum, D. Gross, K. Miller. 1990. WordNet: An online lexical database. Int. J. Lexicograph. 3, 4, pp. 235–244.&lt;/ref&gt; It groups English [[word]]s into sets of [[synonyms]] called ''[[synsets]]'', provides short definitions and usage examples, and records a number of relations among  these synonym sets or their members. WordNet can thus be seen as a combination of [[dictionary]] and [[thesaurus]]. While it is  accessible to human users via a [[web browser]],&lt;ref name="WordNet Search"&gt;{{cite web|url=http://wordnetweb.princeton.edu/perl/webwn|title=WordNet Search - 3.1}}&lt;/ref&gt; its primary use is in automatic [[natural language processing|text analysis]] and [[artificial intelligence]] applications. The [[database]] and [[software]] tools have been released under a [[BSD License|BSD style license]] and are freely available for download from the WordNet website. Both the lexicographic data (''lexicographer files'') and the compiler (''called grind'') for producing the distributed database are available.
[[File:WordNet.PNG|thumb|This is a snapshot of WordNet's definition of itself.]]

== History and team members ==
WordNet was created in the [[Cognitive Science]] Laboratory of [[Princeton University]] under the direction of  [[psychology]] [[professor]] [[George Armitage Miller]] starting in 1985 and has been directed in recent years by [[Christiane Fellbaum]]. The project received funding from government agencies including the [[National Science Foundation]], [[DARPA]], the [[Disruptive Technology Office]] (formerly the Advanced Research and Development Activity), and REFLEX. George Miller and Christiane Fellbaum were awarded the 2006 [[European Language Resources Association#Antonio Zampolli Prize|Antonio Zampolli Prize]] for their work with WordNet.

== Database contents ==
[[File:Hamburger WordNet.png|thumb|Example entry "Hamburger" in WordNet]]

As of November 2012 WordNet's latest Online-version is 3.1.&lt;ref&gt;{{cite web|url=http://wordnet.princeton.edu/wordnet/download/current-version/ |title=Current WordNet version |publisher=Wordnet.princeton.edu |date=2012-11-09 |accessdate=2014-03-11}}&lt;/ref&gt; The  database contains 155,287 words organized in 117,659 [[synsets]] for a total of 206,941 word-sense pairs; in [[data compression|compressed]] form, it is about 12 [[megabyte]]s in size.&lt;ref&gt;{{cite web|url=http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html |title=WordNet Statistics |publisher=Wordnet.princeton.edu |date= |accessdate=2014-03-11}}&lt;/ref&gt;

WordNet includes the lexical categories [[noun]]s, [[verb]]s, [[adjective]]s and [[adverb]]s but ignores [[preposition]]s, [[determiner (linguistics)|determiner]]s and other function words.

Words from the same lexical category that are roughly synonymous are grouped into [[synsets]]. Synsets include simplex words as well as [[collocation]]s like "eat out" and "car pool." The different senses of a [[polysemous]] word form are assigned to different synsets. The meaning of a synset is further clarified with a short defining ''gloss'' and one or more usage examples. An example adjective synset is:

: good, right, ripe – (most suitable or right for a particular purpose; "a good time to plant tomatoes"; "the right time to act"; "the time is ripe for great sociological changes")

All synsets are connected to other synsets by means of semantic relations. These relations, which are not all shared by all lexical categories, include:
* [[Noun]]s
**''[[hypernym]]s'': ''Y'' is a hypernym of ''X'' if every ''X'' is a (kind of) ''Y'' (''canine'' is a hypernym of ''[[dog]]'')
**''[[hyponym]]s'': ''Y'' is a hyponym of ''X'' if every ''Y'' is a (kind of) ''X'' (''dog'' is a hyponym of ''canine'')
**''coordinate terms'': ''Y'' is a coordinate term of ''X'' if ''X'' and ''Y'' share a hypernym (''wolf'' is a coordinate term of ''dog'', and ''dog'' is a coordinate term of ''wolf'')
**''[[meronymy|meronym]]'': ''Y'' is a meronym of ''X'' if ''Y'' is a part of ''X'' (''window'' is a meronym of ''building'')
**''[[holonymy|holonym]]'': ''Y'' is a holonym of ''X'' if ''X'' is a part of ''Y'' (''building'' is a holonym of ''window'')
* [[Verb]]s
**''hypernym'': the verb ''Y'' is a hypernym of the verb ''X'' if the activity ''X'' is a (kind of) ''Y'' (''to perceive'' is an hypernym of ''to listen'')
**''[[troponym]]'': the verb ''Y'' is a troponym of the verb ''X'' if the activity ''Y'' is doing ''X'' in some manner (''to lisp'' is a troponym of ''to talk'')
**''[[entailment]]'': the verb ''Y'' is entailed by ''X'' if by doing ''X'' you must be doing ''Y'' (''to sleep'' is entailed by ''to snore'')
**''coordinate terms'': those verbs sharing a common hypernym (''to lisp'' and ''to yell'')

These semantic relations hold among all members of the linked synsets. Individual synset members (words) can also be connected with  lexical relations. For example, (one sense of) the noun "director" is linked to (one sense of) the verb "direct" from which it is derived via a "morphosemantic" link.

The morphology functions of the software distributed with the database try to deduce the [[Lemma (morphology)|lemma]] or [[stem (linguistics)|stem]] form of a [[word]] from the user's input. Irregular forms are stored in a list, and looking up "ate" will return "eat," for example.

== Knowledge structure ==
Both nouns and verbs are organized into hierarchies, defined by [[hypernym]] or ''[[is-a|IS A]]'' relationships. For instance, one sense of the word ''dog'' is found following hypernym hierarchy; the words at the same level represent synset members.  Each set of synonyms has a unique index.
{{tree list}}
* {{Tree list/final branch}}dog, domestic dog, Canis familiaris
** {{Tree list/final branch}}canine, canid
*** {{Tree list/final branch}}carnivore
**** {{Tree list/final branch}}placental, placental mammal, eutherian, eutherian mammal
***** {{Tree list/final branch}}mammal
****** {{Tree list/final branch}}vertebrate, craniate
******* {{Tree list/final branch}}chordate
******** {{Tree list/final branch}}animal, animate being, beast, brute, creature, fauna
********* {{Tree list/final branch}}...
{{tree list/end}}
At the top level, these hierarchies are organized into 25 beginner "trees" for nouns and 15 for verbs (called ''lexicographic files'' at a maintenance level).  All are linked to a unique beginner synset, "entity."
Noun hierarchies are far deeper than verb hierarchies

Adjectives are not organized into hierarchical trees. Instead, two "central" antonyms such as "hot" and "cold" form binary poles, while 'satellite' synonyms such as "steaming" and "chilly" connect to their respective poles via a "similarity" relations. The adjectives can be visualized in this way as "dumbbells" rather than as "trees."

== Psycholinguistic aspects of WordNet ==

The initial goal of the WordNet project was to build a lexical database that would be consistent with theories of human semantic memory developed in the late 1960s.  Psychological experiments indicated that speakers organized their knowledge of concepts in an economic, hierarchical fashion. Retrieval time required to access conceptual knowledge seemed to be directly related to the number of hierarchies the speaker needed to "traverse" to access the knowledge. Thus, speakers could more quickly verify that ''canaries can sing'' because a canary is a songbird ("sing" is a property stored on the same level as "canary"), but required slightly more time to verify that ''canaries can fly'' (where they had to access the concept "bird" on the superordinate level) and even more time to verify ''canaries have skin'' (requiring look-up across multiple levels of hyponymy, up to "animal").&lt;ref&gt;Collins A., Quillian M. R. 1972. Experiments on Semantic Memory and Language Comprehension. In ''Cognition in Learning and Memory''. Wiley, New York.&lt;/ref&gt;
While such experiments and the underlying theories have been subject to criticism, some of WordNet's organization is consistent with experimental evidence. For example, [[anomic aphasia]] selectively affects speakers' ability to produce words from a specific semantic category, a WordNet hierarchy. Antonymous adjectives (WordNet's central adjectives in the dumbbell structure) are found to co-occur far more frequently than chance, a fact that has been found to hold for many languages.

== WordNet as a lexical ontology ==

WordNet is sometimes called an ontology, a persistent claim that its creators do not make. The hypernym/hyponym relationships among the noun synsets can be interpreted as specialization relations among conceptual categories. In other words, WordNet can be interpreted and used as a lexical [[ontology (computer science)|ontology]] in the [[computer science]] sense. However, such an ontology should normally be corrected before being used since it contains hundreds of basic semantic inconsistencies such as (i) the existence of common specializations for exclusive categories and (ii) redundancies in the specialization hierarchy. Furthermore, transforming WordNet into a lexical ontology usable for knowledge representation should normally also involve (i)&amp;nbsp;distinguishing the specialization relations into ''subtypeOf'' and ''instanceOf'' relations, and (ii)&amp;nbsp;associating intuitive unique identifiers to each category. Although such corrections and transformations have been performed and documented as part of the integration of WordNet&amp;nbsp;1.7 into the cooperatively updatable knowledge base of WebKB-2,&lt;ref&gt;{{cite web|author=http://www.phmartin.info |url=http://www.webkb.org/doc/wn/ |title=Integration of WordNet 1.7 in WebKB-2|publisher=Webkb.org |date= |accessdate=2014-03-11}}&lt;/ref&gt; most projects claiming to re-use WordNet for knowledge-based applications (typically, knowledge-oriented information retrieval) simply re-use it directly.

WordNet has also been converted to a formal specification, by means of a hybrid bottom-up top-down methodology to automatically extract association relations from WordNet, and interpret these associations in terms of a set of conceptual relations, formally defined in the [[Upper ontology (computer science)#DOLCE and DnS|DOLCE foundational ontology]].&lt;ref&gt;{{cite book |first1=A. |last1=Gangemi |first2=R. |last2=Navigli |first3=P. |last3=Velardi |url=http://www.w3.org/2001/sw/BestPractices/WNET/ODBASE-OWN.pdf |format=PDF |title=The OntoWordNet Project: Extension and Axiomatization of Conceptual Relations in WordNet |work= Proc. of International Conference on Ontologies, Databases and Applications of SEmantics (ODBASE 2003) |location=Catania, Sicily (Italy) |year=2003 |pages= 820–838}}&lt;/ref&gt;

In most works that claim to have integrated WordNet into ontologies, the content of WordNet has not simply been corrected when it seemed necessary; instead, WordNet has been heavily re-interpreted and updated whenever suitable. This was the case when, for example, the top-level ontology of WordNet was re-structured&lt;ref&gt;{{cite conference | first1 = A. | last1 = Oltramari | first2 = A. | last2 = Gangemi | first3 = N. | last3 = Guarino | first4 = C. | last4 = Masolo | date = 2002 | title = Restructuring WordNet's Top-Level: The OntoClean approach | citeseerx = 10.1.1.19.6574 | conference = OntoLex'2 Workshop, Ontologies and Lexical Knowledge Bases (LREC 2002) | location = Las Palmas, Spain | pages = 17–26 }}&lt;/ref&gt; according to the [[OntoClean]] based approach or when WordNet was used as a primary source for constructing the lower classes of the SENSUS ontology.

== Limitations ==

WordNet does not include information about the [[etymology]] or the pronunciation of words and it contains only limited information about usage.
WordNet aims to cover most of everyday English and does not include much domain-specific terminology.

WordNet is the most commonly used computational lexicon of English for [[word sense disambiguation]] (WSD), a task aimed to assigning the context-appropriate meanings (i.e. synset members) to words in a text.&lt;ref&gt;R. Navigli. [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ''ACM Computing Surveys'', 41(2), 2009, pp. 1–69&lt;/ref&gt; However, it has been argued that WordNet encodes sense distinctions that are too fine-grained. This issue prevents WSD systems from achieving a level of performance comparable to that of humans, who do not always agree when confronted with the task of selecting a sense from a dictionary that matches a word in a context. The granularity issue has been tackled by proposing [[cluster analysis|clustering]] methods that automatically group together similar senses of the same word.&lt;ref&gt;E. Agirre, O. Lopez. 2003.
Clustering WordNet Word Senses. In ''Proc. of the Conference on Recent Advances on Natural Language (RANLP’03)'', Borovetz, Bulgaria, pp. 121–130.&lt;/ref&gt;&lt;ref&gt;R. Navigli. [http://acl.ldc.upenn.edu/P/P06/P06-1014.pdf Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance], In ''Proc. of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL 2006)'', Sydney, Australia, July 17-21st, 2006, pp. 105–112.&lt;/ref&gt;&lt;ref&gt;R. Snow, S. Prakash, D. Jurafsky, A. Y. Ng. 2007. [http://www.aclweb.org/anthology/D/D07/D07-1107.pdf Learning to Merge Word Senses], ''In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)'', Prague, Czech Republic, pp. 1005–1014.&lt;/ref&gt;

=== Licensed vs. Open WordNets ===
Some wordnets were subsequently created for other languages. A 2012 survey lists the wordnets and their availability.&lt;ref&gt;Francis Bond and Kyonghee Paik 2012a. [http://web.mysites.ntu.edu.sg/fcbond/open/pubs/2012-gwc-wn-license.pdf A survey of wordnets and their licenses]. In Proceedings of the 6th Global WordNet Conference (GWC 2012). Matsue. 64–71&lt;/ref&gt; In an effort to propagate the usage of WordNets, the Global WordNet community had been slowly re-licensing their WordNets to an open domain where researchers and developers can easily access and use WordNets as language resources to provide [[ontology|ontological]] and [[lexicon|lexical]] knowledge in [[Natural Language Processing]] tasks.

The Open Multilingual WordNet&lt;ref&gt;http://compling.hss.ntu.edu.sg/omw/&lt;/ref&gt; provides access to [[Open-source license|open licensed]] wordnets in a variety of languages, all linked to the Princeton Wordnet of English (PWN). The goal is to make it easy to use wordnets in multiple languages.

== Applications ==

WordNet has been used for a number of different purposes in information systems, including [[word-sense disambiguation]], [[information retrieval]], [[Document classification|automatic text classification]], [[Automatic summarization|automatic text summarization]], [[machine translation]] and even automatic crossword puzzle generation.

A common use of WordNet is to determine the [[semantic similarity|similarity]] between words. Various algorithms have been proposed, and these include measuring the distance among the words and synsets in WordNet's graph structure, such as by counting the number of edges among synsets. The intuition is that the closer two words or synsets are, the closer their meaning. A number of WordNet-based word similarity algorithms are implemented in a [[Perl]] package called WordNet::Similarity,&lt;ref&gt;{{cite web|url=http://www.d.umn.edu/~tpederse/similarity.html |title=Ted Pedersen - WordNet::Similarity |publisher=D.umn.edu |date=2008-06-16 |accessdate=2014-03-11}}&lt;/ref&gt; and in a [[Python (programming language)|Python]] package called [[NLTK]].
Other more sophisticated WordNet-based similarity techniques include ADW,&lt;ref&gt;M. T. Pilehvar, D. Jurgens and R. Navigli.  [http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2013_Pilehvar_Jurgens_Navigli.pdf Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity.]. Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), Sofia, Bulgaria, August 4–9, 2013, pp. 1341-1351.&lt;/ref&gt; whose implementation is available in [[Java (programming language)|Java]]. WordNet can also be used to inter-link other vocabularies.&lt;ref&gt;{{cite journal  |vauthors=Ballatore A, etal |volume=20|issue=2| arxiv=1404.5372| journal=Annals of GIS |title=Linking geographic vocabularies through WordNet |publisher= |date=2014}}&lt;/ref&gt;

== Interfaces ==
Princeton maintains a list of related projects&lt;ref&gt;{{cite web|url=http://wordnet.princeton.edu/wordnet/related-projects/ |title=Related projects - WordNet - Related projects |publisher=Wordnet.princeton.edu |date=2014-01-06 |accessdate=2014-03-11}}&lt;/ref&gt; that includes links to some of the widely used [[application programming interface]]s available for accessing WordNet using various programming languages and environments.

== Related projects and extensions ==
WordNet is connected to several databases of the [[Semantic Web]]. WordNet is also commonly re-used via mappings between the WordNet synsets and the categories from ontologies. Most often, only the top-level categories of WordNet are mapped.

===Global WordNet Association===
The Global WordNet Association (GWA)&lt;ref&gt;{{cite web|author=The Global WordNet Association |url=http://www.globalwordnet.org/ |title=globalwordnet.org |publisher=globalwordnet.org |date=2010-02-04 |accessdate=2014-03-11}}&lt;/ref&gt; is a public and non-commercial organization that provides a platform for discussing, sharing and connecting wordnets for all languages in the world. The GWA also promotes the standardization of wordnets across different languages to ensure its uniformity in enumerating the different synsets in human languages. The GWA keeps a list of wordnets developed around the world.&lt;ref&gt;{{cite web|title=Wordnets in the World|url=http://www.globalwordnet.org/gwa/wordnet_table.html|archiveurl=https://web.archive.org/web/20111021114613/http://www.globalwordnet.org/gwa/wordnet_table.html |archivedate=2011-10-21}}&lt;/ref&gt;

===Other languages===
* [[Arabic WordNet]]:&lt;ref&gt;Black W., Elkateb S., Rodriguez H., Alkhalifa M., Vossen P., Pease A., Bertran M., Fellbaum C., (2006) The Arabic WordNet Project, Proceedings of LREC 2006&lt;/ref&gt;&lt;ref&gt;Lahsen Abouenour, Karim Bouzoubaa, Paolo Rosso (2013) On the evaluation and improvement of Arabic WordNet coverage and usability, Language Resources and Evaluation 47(3) pp 891–917&lt;/ref&gt; WordNet for Arabic language.
* [[Malayalam WordNet]] , developed by [[Cochin University of Science and Technology|Cochin University Of Science and Technology]]
* CWN (Chinese Wordnet or 中文詞彙網路) supported by [[National Taiwan University]].&lt;ref&gt;[http://lope.linguistics.ntu.edu.tw/cwn/ Chinese Wordnet (中文詞彙網路) official page] at National Taiwan University&lt;/ref&gt;
* WOLF (WordNet Libre du Français), a French version of WordNet.&lt;ref&gt;S. Benoît, F. Darja. 2008. [http://alpage.inria.fr/~sagot/pub/Ontolex08.pdf Building a free French wordnet from multilingual resources]. In ''Proc. of Ontolex 2008'', Marrakech, Maroc.&lt;/ref&gt;
* JAWS (Just Another WordNet Subset), another French version of WordNet&lt;ref&gt;C. Mouton, G. de Chalendar. 2010.[http://www.iro.umontreal.ca/~felipe/TALN2010/Xml/Papers/all/taln2010_submission_71.pdf JAWS : Just Another WordNet Subset]. In ''Proc. of TALN 2010''.&lt;/ref&gt; built using the Wiktionary and semantic spaces
* The [[IndoWordNet]]&lt;ref name="PushpakBhattacharyya"&gt;Pushpak Bhattacharyya, IndoWordNet, Lexical Resources Engineering Conference 2010 (LREC 2010), Malta, May, 2010.&lt;/ref&gt; is a linked lexical knowledge base of wordnets of 18 scheduled languages of India.
* The MultiWordNet project,&lt;ref&gt;E. Pianta, L. Bentivogli, C. Girardi. 2002. [http://multiwordnet.itc.it/paper/MWN-India-published.pdf MultiWordNet: Developing an aligned multilingual database]. In ''Proc. of the 1st International Conference on Global WordNet'', Mysore, India, pp. 21–25.&lt;/ref&gt; a multilingual WordNet aimed at producing an Italian WordNet strongly aligned with the Princeton WordNet.
* The [[EuroWordNet]] project&lt;ref&gt;P. Vossen, Ed. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer, Dordrecht, The Netherlands.&lt;/ref&gt; has produced WordNets for several European languages and linked them together; these are not freely available however. The Global Wordnet project attempts to coordinate the production and linking of "wordnets" for all languages.&lt;ref&gt;{{cite web|url=http://www.globalwordnet.org/ |title=The Global WordNet Association |publisher=Globalwordnet.org |date=2010-02-04 |accessdate=2014-01-05}}&lt;/ref&gt; [[Oxford University Press]], the publisher of the [[Oxford English Dictionary]], has voiced plans to produce their own online competitor to WordNet.{{Citation needed|date=May 2009}}
* The BalkaNet project&lt;ref&gt;D. Tufis, D. Cristea, S. Stamou. 2004. [http://www.racai.ro/~tufis/papers/Tufis-CS-ROMJIST2004.pdf Balkanet: Aims, methods, results and perspectives. A general overview]. ''Romanian J. Sci. Tech. Inform. (Special Issue on Balkanet)'', 7(1-2), pp. 9–43.&lt;/ref&gt; has produced WordNets for six European languages (Bulgarian, Czech, Greek, Romanian, Turkish and Serbian). For this project, a freely available XML-based WordNet editor was developed. This editor – VisDic – is not in active development anymore, but is still used for the creation of various WordNets. Its successor, DEBVisDic, is client-server application and is currently used for the editing of several WordNets (Dutch in Cornetto project, Polish, Hungarian, several African languages, Chinese).
* UWN is an automatically constructed multilingual lexical knowledge base extending WordNet to cover over a million words in many different languages.&lt;ref&gt;{{cite web|url=http://www.mpi-inf.mpg.de/yago-naga/uwn |title=UWN: Towards a Universal Multilingual Wordnet - D5: Databases and Information Systems (Max-Planck-Institut für Informatik) |publisher=Mpi-inf.mpg.de |date=2011-08-14 |accessdate=2014-01-05}}&lt;/ref&gt;
* Such projects as BalkaNet and EuroWordNet made it feasible to create standalone wordnets linked to the original one. One of such projects is Russian WordNet patronized by [[Petersburg State University of Means of Communication]]&lt;ref&gt;{{cite web|url=http://www.pgups.ru/abitur/inostrancam/inter/ruwordnet/ |title=Русский WordNet |publisher=Pgups.ru |date= |accessdate=2014-01-05}}&lt;/ref&gt; or Russnet&lt;ref&gt;{{cite web|url=http://project.phil.spbu.ru/RussNet/index_ru.shtml |title=RussNet: Главная страница |publisher=Project.phil.spbu.ru |date= |accessdate=2014-03-11}}&lt;/ref&gt; by [[Saint Petersburg State University]]
* FinnWordNet is a Finnish version of the WordNet where all entries of the original English WordNet were translated.&lt;ref&gt;{{cite web|url=http://www.ling.helsinki.fi/en/lt/research/finnwordnet/ |title=FinnWordNet – The Finnish WordNet - Department of General Linguistics |publisher=Ling.helsinki.fi |date= |accessdate=2014-01-05}}&lt;/ref&gt;
* [[GermaNet]] is a German version of the WordNet developed by the University of Tübingen.&lt;ref&gt;{{cite web|url=http://www.sfs.uni-tuebingen.de/lsd/index.shtml |title=GermaNet |publisher=Sfs.uni-tuebingen.de |date= |accessdate=2014-03-11}}&lt;/ref&gt;
* OpenWN-PT  is a Brazilian Portuguese version of the original WordNet freely available for download under CC-BY-SA license.&lt;ref&gt;{{cite web|url=https://github.com/arademaker/openWordnet-PT |title=arademaker/openWordnet-PT — GitHub |publisher=Github.com |date= |accessdate=2014-01-05}}&lt;/ref&gt;
* [[plWordNet]]&lt;ref&gt;http://plwordnet.pwr.wroc.pl/wordnet/ official webpage&lt;/ref&gt; is a Polish-language version of WordNet developed by [[Wrocław University of Technology]].
* PolNet&lt;ref&gt;http://www.ltc.amu.edu.pl/polnet/ official webpage&lt;/ref&gt; is a Polish-language version of WordNet developed by [[Adam Mickiewicz University in Poznań]] (distributed under CC BY-NC-ND 3.0 license).
* [[BulNet]] is a Bulgarian version of the WordNet developed at the Department of Computational Linguistics of the [[Institute for Bulgarian Language]], Bulgarian Academy of Sciences.&lt;ref&gt;{{cite web|url=http://dcl.bas.bg/BulNet/general_en.html |title=BulNet |publisher=dcl.bas.bg |date= |accessdate=2015-05-07}}&lt;/ref&gt;

===Linked data===

* [[BabelNet]],&lt;ref&gt;R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11–16, 2010, pp. 216–225.&lt;/ref&gt; a very large multilingual [[semantic network]] with millions of concepts obtained from an integration of WordNet and Wikipedia based on an automatic mapping algorithm.
* The [[Suggested Upper Merged Ontology|SUMO]] ontology&lt;ref&gt;A. Pease, I. Niles, J. Li. 2002. [https://www.aaai.org/Papers/Workshops/2002/WS-02-11/WS02-11-011.pdf The suggested upper merged ontology: A large ontology for the Semantic Web and its applications]. In ''Proc. of the AAAI-2002 Workshop on Ontologies and the Semantic Web'', Edmonton, Canada.&lt;/ref&gt; has produced a mapping between all of the WordNet synsets, (including nouns, verbs, adjectives and adverbs), and [[SUMO class]]es.  The most recent addition of the mappings provides links to all of the more specific terms in the MId-Level Ontology (MILO), which extends SUMO.
* [[OpenCyc]],&lt;ref&gt;S. Reed and D. Lenat. 2002. [http://www.cyc.com/doc/white_papers/mapping-ontologies-into-cyc_v31.pdf Mapping Ontologies into Cyc]. In ''Proc. of AAAI 2002 Conference Workshop on Ontologies For The Semantic Web'', Edmonton, Canada, 2002&lt;/ref&gt; an open [[ontology (information science)|ontology]] and [[knowledge base]] of everyday common sense knowledge, has 12,000 terms linked to WordNet synonym sets.
* [[Descriptive Ontology for Linguistic and Cognitive Engineering|DOLCE]],&lt;ref&gt;Masolo, C., Borgo, S., Gangemi, A., Guarino, N., Oltramari, A., Schneider, L.S. 2002. [http://www.loa-cnr.it/Papers/WonderWebD17V2.0.pdf WonderWeb Deliverable D17. The WonderWeb Library of Foundational Ontologies and the DOLCE ontology]. Report (ver. 2.0, 15-08-2002)&lt;/ref&gt; is the first module of the WonderWeb Foundational Ontologies Library (WFOL). This upper-ontology has been developed in light of rigorous ontological principles inspired by the philosophical tradition, with a clear orientation toward language and cognition. OntoWordNet&lt;ref&gt;Gangemi, A., Guarino, N., Masolo, C., Oltramari, A. 2003 [http://www.loa-cnr.it/Papers/AIMag24-03-003.pdf Sweetening WordNet with DOLCE]. In AI Magazine 24(3): Fall 2003, pp. 13–24&lt;/ref&gt; is the result of an experimental effort to align WordNet's upper level with DOLCE. It is suggested that such alignment could lead to an "ontologically sweetened" WordNet, meant to be conceptually more rigorous, cognitively transparent, and efficiently exploitable in several applications.
* [[DBpedia]],&lt;ref&gt;C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, [http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/Bizer-etal-DBpedia-CrystallizationPoint-JWS-Preprint.pdf DBpedia – A crystallization point for the Web of Data]. Web Semantics, 7(3), 2009, pp. 154–165&lt;/ref&gt; a database of structured information, is also linked to WordNet.
* The [[eXtended WordNet]]&lt;ref&gt;S. M. Harabagiu, G. A. Miller, D. I. Moldovan. 1999. [http://www.ldc.upenn.edu/acl/W/W99/W99-0501.pdf WordNet 2 – A Morphologically and Semantically Enhanced Resource]. In ''Proc. of the ACL SIGLEX Workshop: Standardizing Lexical Resources'', pp. 1–8.&lt;/ref&gt; is a project at the [[University of Texas at Dallas]] which aims to improve WordNet by semantically parsing the glosses, thus making the information contained in these definitions available for automatic knowledge processing systems. It is also freely available under a license similar to WordNet's.
* The [[GCIDE]] project produced a dictionary by combining a [[public domain]] ''[[Webster's Dictionary]]'' from 1913 with some WordNet definitions and material provided by volunteers. It was released under the [[copyleft]] license [[GNU General Public License|GPL]].
* [[ImageNet]] is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.&lt;ref&gt;J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei. [https://nlpainter.googlecode.com/svn-history/r16/trunk/papers/ImageNet__cvpr09.pdf ImageNet: A Large-Scale Hierarchical Image Database]. In ''Proc. of 2009 IEEE Conference on Computer Vision and Pattern Recognition''&lt;/ref&gt; Currently it has an average of over five hundred images per node.
* BioWordnet, a biomedical extension of wordnet was abandoned due to issues about stability over versions.&lt;ref&gt;M. Poprat, E. Beisswanger, U. Hahn. 2008. [http://www.aclweb.org/anthology/W/W08/W08-0507.pdf Building a BIOWORDNET by Using WORDNET’s Data Formats and WORDNET’s Software Infrastructure – A Failure Story]. In ''Proc. of the Software Engineering, Testing, and Quality Assurance for Natural Language Processing Workshop'', pp. 31–39.&lt;/ref&gt;
* WikiTax2WordNet, a mapping between WordNet synsets and [[Wikipedia:Categorization|Wikipedia categories]].&lt;ref&gt;S. Ponzetto, R. Navigli. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia], In ''Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009)'', Pasadena, California, July 14-17th, 2009, pp. 2083–2088.&lt;/ref&gt;
* WordNet++, a resource including over millions of semantic edges harvested from Wikipedia and connecting pairs of WordNet synsets.&lt;ref&gt;S. P. Ponzetto, R. Navigli. [http://aclweb.org/anthology-new/P/P10/P10-1154.pdf Knowledge-rich Word Sense Disambiguation rivaling supervised systems]. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010, pp. 1522–1531.&lt;/ref&gt;
* SentiWordNet, a resource for supporting opinion mining applications obtained by tagging all the WordNet 3.0 synsets according to their estimated degrees of positivity, negativity, and neutrality.&lt;ref&gt;S. Baccianella, A. Esuli and F. Sebastiani. [http://nemis.isti.cnr.it/sebastiani/Publications/LREC10.pdf SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining]. In Proceedings of the 7th Conference on Language Resources and Evaluation (LREC'10), Valletta, MT, 2010, pp. 2200–2204.&lt;/ref&gt;
* ColorDict, is an Android application to mobiles phones that use Wordnet database and others, like Wikipedia.
* [[UBY-LMF]] a database of 10 resources including WordNet.

===Related projects===
* [[FrameNet]] is a lexical database that shares some similarities with, and refers to, WordNet. 
* [[Lexical markup framework]] (LMF) is an ISO standard specified within [[ISO/TC37]] in order to define a common standardized framework for the construction of lexicons, including WordNet. The subset of LMF for Wordnet is called Wordnet-LMF. An instantiation has been made within the KYOTO project.&lt;ref&gt;Piek Vossen, Claudia Soria, Monica Monachini: Wordnet-LMF: a standard representation for multilingual wordnets, in ''LMF Lexical Markup Framework'', edited by Gil Francopoulo ISTE / Wiley 2013 (ISBN 978-1-84821-430-9)&lt;/ref&gt;
* [[Universal Networking Language|UNL Programme]] is a project under the auspices of [[United Nations|UNO]] aimed to consolidate lexicosemantic data of many languages to be used in machine translation and information extraction systems.

==Distributions==
* [[Babylon (software)|Babylon]]&lt;ref&gt;{{cite web|url=http://www.babylon.com/free-dictionaries/reference/encyclopedias/WordNet-2.0/42406.html |title=Babylon WordNet |publisher=Babylon.com |date= |accessdate=2014-03-11}}&lt;/ref&gt;
WordNet Database is distributed as a dictionary package (usually a single file) for the following software:
* [[GoldenDict]]&lt;ref&gt;{{cite web|url=http://sourceforge.net/projects/goldendict/files/dictionaries |title=GoldenDict - Browse /dictionaries at Sourceforge.net |publisher=Sourceforge.net |date=2010-12-01 |accessdate=2014-01-05}}&lt;/ref&gt;
* [[Lingoes (program)|Lingoes]]&lt;ref&gt;{{cite web|url=http://www.lingoes.net/en/dictionary/dict_down.php?id=12D98EC3940843498672A92149455292 |title=Lingoes WordNet |publisher=Lingoes.net |date=2007-11-16 |accessdate=2014-03-11}}&lt;/ref&gt;

== See also ==
* [[Lexical Markup Framework]]
* [[Machine-readable dictionary]]
* [[Synonym Ring]]
* [[Taxonomy (general)|Taxonomy]]
* [[ThoughtTreasure]]

== References ==
{{Reflist|2}}

== External links ==
* {{Official website|http://wordnet.princeton.edu/ }}
{{Lexicography}}

{{Authority control}}

{{DEFAULTSORT:Wordnet}}
[[Category:English dictionaries]]
[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Online dictionaries]]
[[Category:Open data]]
[[Category:Thesauri]]</text>
      <sha1>to33p5kze2a2v6cvak8molnq88o88h3</sha1>
    </revision>
  </page>
  <page>
    <title>Information Coding Classification</title>
    <ns>0</ns>
    <id>47525166</id>
    <revision>
      <id>744377201</id>
      <parentid>739355021</parentid>
      <timestamp>2016-10-14T20:32:15Z</timestamp>
      <contributor>
        <username>Frietjes</username>
        <id>13791031</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18263" xml:space="preserve">The '''Information Coding Classification''' ('''ICC''') is a [[classification system]] covering almost all extant 6500 knowledge fields ([[knowledge domain]]s). Its [[conceptualization]] goes beyond the scope of the well known  library classification systems, such as [[Dewey Decimal Classification]] (DCC), [[Universal Decimal Classification]] (UDC), and [[Library of Congress Classification]] (LCC), by extending also to [[knowledge system]]s that so far have not afforded to  classify  [[literature]].  ICC actually presents a flexible universal ordering system for both literature and other kinds of [[information]], set out as knowledge fields. From a methodological point of view, ICC differs from the above-mentioned systems along the following three lines:
# Its main classes are not based on [[discipline]]s but on nine live stages of development, so-called [[ontic]]al levels.
# It breaks them roughly down into hierarchical steps by further nine [[Categorization|categories]] which makes decimal number coding possible.
# The contents of a knowledge field is earmarked via a digital position scheme, which makes the first hierarchical step refer to the nine ontical levels  (object areas as subject categories), and the second hierarchical step refer to nine functionally ordered form categories.
Respective knowledge fields permit to step down by the same principle to a third and  forth level, and even further to a fifth and sixth level. Finally, knowledge field subdivisions will have to conform to said digital position scheme.
Hence, for a given knowledge field identical codes will mark identical categories under respective numbers of the coding system. This  [[mnemotechnics|mnemotechnical]]  aspect of the  system helps memorizing and straightaway retrieving the whereabouts of respective [[interdisciplinary]] and [[transdisciplinary]] fields.

The first two hierarchical levels may be regarded as a top- or [[upper ontology]] for ontologies and other applications.

The terms of the first three hierarchical levels were set out in German and English in ''Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft'',&lt;ref name="WEAAZ2014"&gt;{{citation/core|Surname1=[[Ingetraut Dahlberg]]|EditorSurname1=Deutsche Sektion der Internationalen Gesellschaft für Wissensorganisation e.V. (ISKO)|Periodical=Textbooks for Knowledge Organization|Title=Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft |Volume=Vol.3|Publisher=Ergon Verlag|PublicationPlace=Würzburg|Year=2014|At=pp.&amp;nbsp;1-175|ISBN=978-3-95650-065-7|Date=  2014|language=German|AccessDate=2015-07-16}}&lt;/ref&gt; on pp.&amp;nbsp;82 to 100. It was published in 2014 and available so far only in German. In the meantime, also the French terms of the knowlwdge fields have been collected.
Competence for maintenance and further development rests with the German Chapter of the  
[[International Society for Knowledge Organization]] (ISKO) e.V.

== Historical development ==
At the end of 1970, Prof. Alwin Diemer, Univ.of Düsseldorf proposed to [[Ingetraut Dahlberg]] to undertake a philosophical [[dissertation]] on ''The universal classification system of knowledge, its ontological, epistemological, and information theoretical foundations''. Diemer had in mind an innovating ontological approach for such a system based on the whole spectrum of kinds of being and complying with [[epistemological]] requirements. The third requirement had already been taken up somehow in the Indian [[Colon Classification]], yet it still called for explanations and additions. In 1974, the dissertation was published in German entitled ''Grundlagen universaler Wissensordnung''.&lt;ref name="GuWo"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Deutsche Gesellschaft für Dokumentation e.V.|Title=Grundlagen universaler Wissensordnung. Probleme und Möglichkeiten eines universalen Klassifikationssystems des Wissens. : im Antiquariat noch erhältlich sonst als Print on Demand bei deGruyter|Publisher=Verlag Dokumentation|PublicationPlace=Pullach bei München|Year=1974|ISBN=978-3111412672|Date=  1974|language=German|AccessDate=2015-05-13}}&lt;/ref&gt; It started with conceptual clarifications, and why and how the term „universal“ was linked to knowledge, including knowledge fields, such as commodity science, artefacts,  statistics, patents, standardization, communication, utility services et al. In chapter 3, six universal classification systems (DDC, UDC, LCC, BC, CC and BBK) were presented,  analyzed and compared.

While preparing the dissertation, Dahlberg started with elaborating the new universal system by first gleaning a lot of extant designations of knowledge fields from whatever available reference works. This was funded by the German Documentation Society (DGD) (1971-2) under the title of ''Order system of knowledge fields''. In addition, the [[syllabus]]es of German universities and polytechniques were explored for relevant terms and documented (1975).  Thereafter, it seemed necessary to add definitions from special dictionaries and encyclopediae; it soon appeared that the 12.500 terms included numerous synonyms, so that the whole collection boiled down to about 6.500 concept designations (Project Logstruktur, supported by the German Science Foundation (DFG) 1976-78).

The outcome of this work &lt;ref name="GuWo" /&gt; was the formulation of 30 theses which ended up in 12 principles for the new system, published 40 years later under.&lt;ref name="WEAAZ2014" /&gt; These principles refer not only to theoretical foundations but also to structure and other organizational aspects of the whole array of knowledge fields. In 1974, the digital position scheme for field subdivision had already been developed  to allow for classifying classification literature in the bibliographical section of the first issue of the Journal International Classification. In 1977, the entire ICC  was ready for presentation at a seminar in Bangalore, India.&lt;ref&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Sarada Ranganathan Endowment for Library Science|Title=Ontical Structures and Universal Classification |PublicationPlace=Bangalore|Year=1978|Date=  1978|language=German|AccessDate=2014-10-06}}&lt;/ref&gt; A publication of the first three hierarchical levels appeared however only in 1982.&lt;ref name="ICC-PSAP"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=International Classification|Title=ICC – Information Coding Classification. Principles, structure and application possibilities |Volume=2|Year=1982|At=pp.&amp;nbsp;98-103|Date=  1982|language=German}}&lt;/ref&gt; It was applied to the bibliography of classification systems and [[thesauri]] in vol.1 of the International Classification and Indexing Bibliography;&lt;ref&gt;{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=International Classification and Indexing Bibliography (ICIB 1): Classification systems and thesauri 1950-1982 |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1982|ISSN=0943-7444|Date=  1982|language=German|AccessDate=2015-05-13}}&lt;/ref&gt; it has been updated.&lt;ref name="WEAAZ2014" /&gt;

== Governing principles of ICC ==
These were published in full length in the book ''Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft''&lt;ref name="WEAAZ2014" /&gt; and the  article ''Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches'',&lt;ref name="ICC2010"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Marlies Ockenfeld|Periodical=Information, Wissenschaft &amp; Praxis|Title=Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches |Volume=61, Heft 8|Publisher=De Gruyter|Year=2010|At=pp.&amp;nbsp;449-454|ISSN=1619-4292|Date=  2010|language=German|AccessDate=2015-07-16}}&lt;/ref&gt; hence it suffices to just mention their topics with some necessary additions.

* Principle 1: Concept theoretical approaches. Concepts are the contents of ICC, they are understood as being units of knowledge. The „birth“ of a concept. Where do the characteristics, the knowledge elements come from? How do conceptual relations arise?
* Principle 2: The four kinds of concept relations and their applications.
* Principle 3: Decimal numbers form the ICC codes as its universal language.
* Principle 4: The nine ontical levels of ICC. They were grouped under three captions: Prolegomena (1-3), life sciences (4-6) and human output (7-9):
:# Structure and form
:# Matter and energy
:# Cosmos and earth
:# Biosphere
:# Anthroposphere
:# Sociosphere
:# Material products (economics and technology)
:# Intellectual products (knowledge and information)
:# Spiritual products (products of mind and culture)

* Principle 5: Knowledge fields are structured by categories, based on the Aristotelian form-categories, under a digital position scheme, a kind of scaling rule for subdividing a given field as follows:
:# General area: problems, theories, principles (axiom and structure)
:# Object area: objects, kinds, parts, properties of objects
:# Activity area: methods, processes, activities
:# Field properties or first characterization   
:# Persons or secondary characterization
:# Societies or tertiary characterization
:# Influences from outside
:# Applications of the field to other fields
:# Field information and synthesizing tasks
:The digital position scheme, called Systematifier, has  also been used for structuring the entire system via the categories figuring on the upper zero level.

An example of its application is the structure of the classification system for knowledge organization literature [http://www.isko.org/scheme.php Gliederung der Klassifikationsliteratur]. (A simplified version with an additional introduction is given in,&lt;ref name="WEAAZ2014" /&gt; p.&amp;nbsp;71)

* Principle 6: The ontical levels outlined under principle 4 conform to the „integrative level theory“ which means that every level is integrated in the following one. In addition, each knowledge area presumes the following one.
* Principle 7: The combination potential of knowledge fields (interdisciplinarity and transdisciplinarity)is determined by the digital position scheme. (Examples are given in,&lt;ref name="WEAAZ2014" /&gt; p.&amp;nbsp;103-4)
* Principle 8: The categories of the zero-level are general concepts, their possible subdivisions could once be used for classificatory statements. (These subdivisions still need elaboration)
* Principle 9 and 10: These relate to the combination potential of classificatory statements with space and time concepts. (Still to be elaborated)
* Principle 11: The system's mnemotechnical aspect relies on the fixed system position codes and on the 3x3 form- and subject-categories.
* Principle 12: The combination potential of system position 1, 8 and 9 make ICC to a self-networking system which complies with the present scientific development.

== ICC in Matrix Form ==

The first two levels of ICC can be represented by following matrix.

[[File:ICC as a Matrix.png|maxi|840px|ICC as a Matrix with the first two hierarchical levels]]

The first hierarchical level of the 9 subject categories results from the first vertical array under codes 1-9.  The second hierarchical level of subject categories is structured by the 9 functionally ordered form categories, listed in the first horizontal line under codes 01-09. Some exceptions are mentioned in principle 7.

== Research work with ICC ==

=== Exploration of automatic classification with ICC ===
For classifying web documents as conceived by Jens Hartmann, University of Karlsruhe, Prof.Walter Koch, University of Graz, has explored in his Institute for Applied Information Technology Research Society (AIT) the application of ICC to automatically classifying metadata of some 350.000 documents. This was facilitated by data generated within the framework of an EU-supported project [http://www.europeana-local.at "EuropeanaLocal"] . For this exploration, three ICC hierarchical levels have been used for some 5000 terms. The result is described in the report of Christoph Mak.&lt;ref&gt;{{citation/core|Surname1=Christian Mak|Periodical=Bericht des Instituts "Angewandte Informationstechnik Forschungsgesellschaft mbh" (AIT)|Title=Kategorisierung des Datenbestandes der EuropeanaLocal-Österreich anhand der ICC |PublicationPlace=Graz|Year=2011|Date=  2011|language=German}}&lt;/ref&gt; Prof.Koch regarded a classification degree of almost 50% as a good result, considering that only a shortened version of ICC had been used. In order to reach a better result one would have needed 1–2 years. Also an index of all terms with their codes could be achieved under these explorations.

=== Data Linkage with ICC ===
Motivated by the work of an Italian research Group in Trento on ''Revising the Wordnet Domains Hierarchy: semantics, coverage and balancing'',&lt;ref name="FGinTrnto"&gt;{{citation/core|Surname1=Luisa Bentivogli|Surname2=Pamela Forner|Surname3=Bernardo Magnini|Surname4=Emanuele Pianta|Periodical=Proceedings of COLING 2004 Workshop on "Multilingual Linguistic Resources|Title=[http://multiwordnet.fbk.eu/paper/coling04-ws-WDH.pdf Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing] |PublicationPlace=Geneva, Switzerland|Year=2004|At=pp.&amp;nbsp;101-108|Date=  2004|language=German}}&lt;/ref&gt; by which the DDC codes were used, Prof. Ernesto DeLuca et al. showed in a study that for such case the use of ICC could lead to essentially better results. This was shown in two contributions: ''Including knowledge domains from the ICC into the Multilingual Lexical Linked Data Cloud (LLD)'' &lt;ref name=" IKDfICC"&gt;{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Knowledge Organization in the 21st Century. Between Historical Patterns and Future Prospects. Proc.13th Int. ISKO Conf.|Title=Including Knowledge Domains from the ICC into the Multilingual Lexical Linked Data Cloud |PublicationPlace=Krakau, Polen|Year=2014|At=pp.&amp;nbsp;258-365|Date=  2014|language=German}}&lt;/ref&gt; and ''Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung?'',&lt;ref name="MLLDC"&gt;{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Information, Wissenschaft &amp; Praxis|Title=Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung? |Volume=65, Heft 4-5|Publisher=De Gruyter|Year=2014|At=pp.&amp;nbsp;279-287|ISSN=1619-4292|Date=  2014|language=German}}&lt;/ref&gt; in which the LLD was used in a meta-model which contains all ressources with the possibility of retrieval and navigation of data from different aspects. By this, the existing work about many thousand knowledge fields (of  ICC) can be combined with the Multilingual Lexical Linked Data Cloud, based on RDF/OWL representation of EuroWordNet and similar integrated lexical ressources (MultiWordNet, MEMODATA and the Hamburg Metapher BD).

=== Semantic Web structuring with ICC ===
In October 2013, the computer scientist Hermann Bense, Dortmund, explored the possibilities for structuring the Semanic Web with ICC codes. He developed two approaches for a pictorial presentation of knowledge fields with their possible subdivisions. A graphic representation of those knowledge fields pertaining to the first two levels can be found under [http://www.ontology4.us/english/Ontologies/Science%2520Ontology/index.html Ontology4]. The inclusion of the third hierarchical level has been envisaged as the next step.

== Some potential applications of ICC in its present form ==
# Possibility to roughly structure documents, especially bibliographies and reference works.
# Structuring personal repertories, e.g. a [[Who’s Who]] in ''Who’s Who in Classification and Indexing''&lt;ref name="whoiswho"&gt;{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=Who's Who in Classification and Indexing |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1983|Date=  1983|language=German}}&lt;/ref&gt;
# Supporting the recollection of statistics by knowledge fields, e.g. also concerning university professors, statistics of academies, of institutions, of teachers in special education
# Publishing houses could take up ICC codes for their products to help later sorting by knowledge fields.
# As a standard classification ICC may be used in many cases, especially in industry,  [[knowledge management]] and [[knowledge engineering]].
# With the definition of all its terms a lexicon of knowledge fields could be published. This could also be used for such lexica in other languages.&lt;ref name="lexikon"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=Knowledge Organisation 39, No.2|Title=A systematic new lexicon of all knowledge fields based on the Information Coding Classification. |Year=2012|At=pp.&amp;nbsp;142-150|Date=  2012|language=German}}&lt;/ref&gt;
# As an example, ICC could be used to compare ongoing scientific activities on a European or world-wide scale.
# ICC can also be an appropriate tool for switching between extant universal classification systems.&lt;ref name="switch"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Green, R.|Periodical=Knowledge organization &amp; change. Proc.4th Int.ISKO Conf., Washington,D.C.|Title=Library catalogs and the Internet. Switching for future subject access. |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1996|At=pp.&amp;nbsp;155-165|Date=  1996|language=German}}&lt;/ref&gt;
# ICC can also be a suitable „hang-up system“ for special classification systems, e.g. for special terminological concept systems.
# ICC in its three hierarchies and corresponding explanations might also be used in higher education to supply the youngsters with an overview of knowledge fields and an understanding of the relationships in the whole of human knowledge.
# Similar to the [[Unified Medical Language System]] (UMLS) for medicine such a Unified System of Knowledge Fields could be held available in many languages and thus reach a global understanding of knowledge fields.
# The alphabetical index to all knowledge field concepts could be used for comparisons with other such indexes to help in finding the missing fields in the different universal classification systems.   
 
== References ==
{{Reflist|30em}}

{{Authority control}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Ontology]]
[[Category:Data coding framework]]</text>
      <sha1>5gqt9kj4pivblmj56u1p8me5aykn2nv</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical model</title>
    <ns>0</ns>
    <id>20590</id>
    <revision>
      <id>756102611</id>
      <parentid>756102141</parentid>
      <timestamp>2016-12-22T00:54:50Z</timestamp>
      <contributor>
        <username>Frode54</username>
        <id>28691415</id>
      </contributor>
      <minor />
      <comment>/* Elements of a mathematical model */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32079" xml:space="preserve">{{Distinguish2|the same term used in [[model theory]], a branch of [[mathematical logic]].}}
{{Refimprove|date=May 2008}} 
A '''mathematical model''' is a description of a [[system]] using [[mathematics|mathematical]] concepts and [[Language of mathematics|language]]. The process of developing a mathematical model is termed '''mathematical modeling'''. Mathematical models are used in the [[natural science]]s (such as [[physics]], [[biology]], [[earth science]], [[meteorology]]) and [[engineering]] disciplines (such as [[computer science]], [[artificial intelligence]]), as well as in the [[social sciences]] (such as [[economics]], [[psychology]], [[sociology]], [[political science]]).  [[Physicist]]s, [[engineer]]s, [[statistician]]s, [[operations research]] analysts, and [[economist]]s use mathematical models most extensively. A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.

== Elements of a mathematical model ==

Mathematical models can take many forms, including [[dynamical systems]], [[statistical model]]s, [[differential equations]], or [[Game theory|game theoretic models]].  These and other types of models can overlap, with a given  model involving a variety of abstract structures. In general, mathematical models may include [[model theory|logical models]].  In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments.  Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.

In the [[physical sciences]], the traditional mathematical model contains four major elements. These are
# [[Governing equation]]s
# [[Defining equation (physics)|Defining equation]]s
# [[Constitutive equation]]s
# [[Constraint (mathematics)|Constraint]]s

== Classifications ==
Mathematical models are usually composed of relationships and ''[[variable (mathematics)|variables]]''. Relationships can be described by ''[[Operator (mathematics)|operators]]'', such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system [[parameter (computer programming)|parameters]] of interest, that can be [[Quantification (science)|quantified]]. Several classification criteria can be used for mathematical models according to their structure:
* '''Linear vs. nonlinear:''' If all the operators in a mathematical model exhibit [[linear]]ity, the resulting mathematical model is defined as linear. A model is considered to be nonlinear otherwise. The definition of linearity and nonlinearity is dependent on context, and linear models may have nonlinear expressions in them.  For example, in a [[Linear model|statistical linear model]], it is assumed that a relationship is linear in the parameters, but it may be nonlinear in the predictor variables.  Similarly, a differential equation is said to be linear if it can be written with linear [[differential operator]]s, but it can still have nonlinear expressions in it.  In a [[Optimization (mathematics)|mathematical programming]] model, if the objective functions and constraints are represented entirely by [[linear equation]]s, then the model is regarded as a linear model.  If one or more of the objective functions or constraints are represented with a [[nonlinearity|nonlinear]] equation, then the model is known as a nonlinear model.&lt;br&gt;Nonlinearity, even in fairly simple systems, is often associated with phenomena such as [[Chaos theory|chaos]] and [[irreversibility]].  Although there are exceptions, nonlinear systems and models tend to be more difficult to study than linear ones.  A common approach to nonlinear problems is [[linearization]], but this can be problematic if one is trying to study aspects such as irreversibility, which are strongly tied to nonlinearity.
* '''Static vs. dynamic:''' A ''dynamic'' model  accounts for time-dependent changes in the state of the system, while a ''static'' (or steady-state) model calculates the system in equilibrium, and thus is time-invariant.  Dynamic models typically are represented by [[differential equation]]s or [[difference equation]]s.
* '''Explicit vs. implicit:''' If all of the input parameters of the overall model are known, and the output parameters can be calculated by a finite series of computations, the model is said to be ''explicit''. But sometimes it is the ''output'' parameters which are known, and the corresponding inputs must be solved for by an iterative procedure, such as [[Newton's method]] (if the model is linear) or [[Broyden's method]] (if non-linear). In such a case the model is said to be ''implicit''. For example, a [[jet engine]]'s physical properties such as turbine and nozzle throat areas can be explicitly calculated given a design [[thermodynamic cycle]] (air and fuel flow rates, pressures, and temperatures) at a specific flight condition and power setting, but the engine's operating cycles at other flight conditions and power settings cannot be explicitly calculated from the constant physical properties.
* '''Discrete vs. continuous:''' A [[discrete modeling|discrete model]] treats objects as discrete, such as the particles in a [[molecular model]] or the states in a [[statistical model]]; while a [[continuous model]] represents the objects in a continuous manner, such as the velocity field of fluid in pipe flows, temperatures and stresses in a solid, and electric field that applies continuously over the entire model due to a point charge.
* '''Deterministic vs. probabilistic (stochastic):''' A [[deterministic system|deterministic]] model is one in which every set of variable states is uniquely determined by parameters in the model and by sets of previous states of these variables; therefore, a deterministic model always performs the same way for a given set of initial conditions. Conversely, in a stochastic model—usually called a "[[statistical model]]"—randomness is present, and variable states are not described by unique values, but rather by [[probability]] distributions.
* '''Deductive, inductive, or floating:''' A deductive model is a logical structure based on a theory. An inductive model arises from empirical findings and generalization from them. The floating model rests on neither theory nor observation, but is merely the invocation of expected structure. Application of mathematics in social sciences outside of economics has been criticized for unfounded models.&lt;ref&gt;{{cite book |authorlink=Stanislav Andreski |first=Stanislav |last=Andreski |year=1972 |title=Social Sciences as Sorcery |publisher=[[St. Martin’s Press]] |isbn=0-14-021816-5 }}&lt;/ref&gt; Application of [[catastrophe theory]] in science has been characterized as a floating model.&lt;ref&gt;{{cite book |authorlink=Clifford Truesdell |first=Clifford |last=Truesdell |year=1984 |title=An Idiot’s Fugitive Essays on Science |pages=121–7 |publisher=Springer |isbn=3-540-90703-3 }}&lt;/ref&gt;

== Significance in the natural sciences ==
Mathematical models are of great importance in the natural sciences, particularly in [[physics]]. Physical [[theory|theories]] are almost invariably expressed using mathematical models.

Throughout history, more and more accurate mathematical models have been developed. [[Newton's laws of motion|Newton's laws]] accurately describe many everyday phenomena, but at certain limits [[relativity theory]] and [[quantum mechanics]] must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the [[speed of light]]. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the [[de Broglie wavelength]] of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.

It is common to use idealized models in physics to simplify things. Massless ropes, point particles, [[ideal gases]] and the [[particle in a box]] are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, [[Maxwell's equations]] and the [[Schrödinger equation]]. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by [[molecular orbital]] models that are approximate solutions to the Schrödinger equation. In [[engineering]], physics models are often made by mathematical methods such as [[finite element analysis]].

Different mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. [[Euclidean geometry]] is much used in classical physics, while [[special relativity]] and [[general relativity]] are examples of theories that use [[geometry|geometries]] which are not Euclidean.

== Some applications==&lt;!--whatever this section is, it isn't background (original heading), and isn't general--&gt;
Since [[prehistory|prehistorical times]] simple models such as [[map]]s and [[Mathematical diagram|diagrams]] have been used.

Often when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in [[simulation]]s.

A mathematical model usually describes a system by a [[Set (mathematics)|set]] of variables and a set of equations that establish relationships between the variables. Variables may be of many types; [[Real number|real]] or [[integer]] numbers, [[Boolean data type|boolean]] values or [[String (computing)|strings]], for example. The variables represent some properties of the system, for example, measured system outputs often in the form of [[Signal (electronics)|signals]], [[Chronometry|timing data]], counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.

== Building blocks ==
In [[business]] and [[engineering]], mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: [[decision theory|decision variables]], [[state variable]]s, [[Exogeny|exogenous]] variables, and [[random variable]]s.

Decision variables are sometimes known as independent variables.  Exogenous variables are sometimes known as [[parameter]]s or [[constant (mathematics)|constant]]s.
The variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables.  Furthermore, the output variables are dependent on the state of the system (represented by the state variables).

[[Goal|Objective]]s and [[constraint (mathematics)|constraint]]s of the system and its users can be represented as [[function (mathematics)|function]]s of the output variables or state variables.  The [[objective function]]s will depend on the perspective of the model's user.  Depending on the context, an objective function is also known as an ''index of performance'', as it is some measure of interest to the user.  Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.

For example, [[economist]]s often apply [[linear algebra]] when using [[input-output model]]s. Complicated mathematical models that have many variables may be consolidated by use of [[vector space|vectors]] where one symbol represents several variables.

== A priori information ==
[[File:Blackbox3D-withGraphs.png|thumb|480px|To analyse something with a typical "black box approach", only  the behavior of the stimulus/response will be accounted for, to infer the (unknown) ''box''. The usual representation of this ''black box system''  is a [[data flow diagram]] centered in the box.]]

Mathematical modeling problems are often classified into [[black box]] or [[White box (software engineering)|white box]] models, according to how much [[a priori (philosophy)|a priori]] information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.

Usually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an [[exponential decay|exponentially decaying]] function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.

In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are [[neural networks]] which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of [[nonlinear system identification]] &lt;ref name="SAB1"&gt;Billings S.A. (2013), ''Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains'', Wiley.&lt;/ref&gt; can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.

=== Subjective information ===
Sometimes it is useful to incorporate subjective information into a mathematical model.  This can be done based on [[Intuition (knowledge)|intuition]], [[experience]], or [[expert opinion]], or based on convenience of mathematical form.  [[Bayesian statistics]] provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a [[prior probability distribution]] (which can be subjective), and then update this distribution based on empirical data.

An example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads.  After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use.  Incorporation of such subjective information might be important to get an accurate estimate of the probability.

== Complexity ==
[[File:Mathematical models for complex systems.jpg|300px|right|thumb|This is a schematic representation of three types of mathematical models of complex systems with the level of their mechanistic understanding.]]
In general, model complexity involves a trade-off between simplicity and accuracy of the model.  [[Occam's razor]] is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable.  While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including [[numerical instability]].  [[Thomas Kuhn]] argues that as science progresses, explanations tend to become more complex before a [[paradigm shift]] offers radical simplification.

For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, [[Isaac Newton|Newton's]] [[classical mechanics]] is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the [[speed of light]], and we study macro-particles only.

== Training ==
Any model which is not pure white-box contains some [[parameter]]s that can be used to fit the model to the system it is intended to describe. If the modeling is done by a [[neural network]] or other [[machine learning]], the optimization of parameters is called ''training'', while the optimization of model hyperparameters is called ''tuning'' and often uses [[cross-validation]]. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by ''[[curve fitting]]''.

== Model evaluation ==
A crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately.  This question can be difficult to answer as it involves several different types of evaluation.

=== Fit to empirical data ===
Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data.  In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters.  An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as [[cross-validation (statistics)|cross-validation]] in statistics.

Defining a [[Metric (mathematics)|metric]] to measure distances between observed and predicted data is a useful tool of assessing model fit.  In statistics, decision theory, and some [[economic model]]s, a [[loss function]] plays a similar role.

While it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model.  In general, more mathematical tools have been developed to test the fit of [[statistical model]]s than models involving [[differential equations]].  Tools from [[non-parametric statistics]] can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.

=== Scope of the model ===
Assessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward.  If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a "typical" set of data.

The question of whether the model describes well the properties of the system between data points is called [[interpolation]], and the same question for events or data points outside the observed data is called [[extrapolation]].

As an example of the typical limitations of the scope of a model, in evaluating Newtonian [[classical mechanics]], we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light.  Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.

=== Philosophical considerations ===
Many types of modeling implicitly involve claims about [[causality]].  This is usually (but not always) true of models involving differential equations.  As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.

An example of such criticism is the argument that the mathematical models of [[Optimal foraging theory]] do not offer insight that goes beyond the common-sense conclusions of [[evolution]] and other basic principles of ecology.&lt;ref&gt;{{Cite journal | last1 = Pyke | first1 = G. H. | doi = 10.1146/annurev.es.15.110184.002515 | title = Optimal Foraging Theory: A Critical Review | journal = Annual Review of Ecology and Systematics | volume = 15 | pages = 523–575 | year = 1984 | pmid =  | pmc = }}&lt;/ref&gt;

== Examples ==
* One of the popular examples in [[computer science]] is the mathematical models of various machines, an example is the [[deterministic finite automaton]] which is defined as an abstract mathematical concept, but due to the deterministic nature of a DFA, it is implementable in hardware and software for solving various specific problems. For example, the following is a DFA M with a binary alphabet, which requires that the input contains an even number of 0s.

[[File:DFAexample.svg|right|thumb|250px|The [[state diagram]] for ''M'']]
''M'' = (''Q'', Σ, δ, ''q''&lt;sub&gt;0&lt;/sub&gt;, ''F'') where
*''Q'' = {''S''&lt;sub&gt;1&lt;/sub&gt;, ''S''&lt;sub&gt;2&lt;/sub&gt;},
*Σ = {0, 1},
*''q&lt;sub&gt;0&lt;/sub&gt;'' = ''S''&lt;sub&gt;1&lt;/sub&gt;,
*''F'' = {''S''&lt;sub&gt;1&lt;/sub&gt;}, and
*δ is defined by the following [[state transition table]]:
:{| border="1" cell padding="1" cell spacing="0"
| || &lt;center&gt;'''0'''&lt;/center&gt; || &lt;center&gt;'''1'''&lt;/center&gt;
|-
|'''''S''&lt;sub&gt;1&lt;/sub&gt;''' || ''S''&lt;sub&gt;2&lt;/sub&gt; || ''S''&lt;sub&gt;1&lt;/sub&gt;
|-
|'''''S''&lt;sub&gt;2&lt;/sub&gt;''' || ''S''&lt;sub&gt;1&lt;/sub&gt; || ''S''&lt;sub&gt;2&lt;/sub&gt;
|}

The state ''S''&lt;sub&gt;1&lt;/sub&gt; represents that there has been an even number of 0s in the input so far, while ''S''&lt;sub&gt;2&lt;/sub&gt; signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, ''M'' will finish in state ''S''&lt;sub&gt;1&lt;/sub&gt;, an accepting state, so the input string will be accepted.

The language recognized by ''M'' is the [[regular language]] given by the [[regular expression]] 1*( 0 (1*) 0 (1*) )*, where "*" is the [[Kleene star]], e.g., 1* denotes any non-negative number (possibly zero) of symbols "1".

* Many everyday activities carried out without a thought are uses of mathematical models. A geographical [[map projection]] of a region of the earth onto a small, plane surface  is a model&lt;ref&gt;[http://www.landinfo.com/resources_dictionaryMP.htm landinfo.com, definition of map projection]&lt;/ref&gt; which can be used for many purposes such as planning travel.
* Another simple activity is predicting the position of a vehicle from its initial position, direction and speed of travel, using the equation that distance traveled is the product of time and speed. This is known as [[dead reckoning]] when used more formally. Mathematical modeling in this way does not necessarily require formal mathematics; animals have been shown to use dead reckoning.&lt;ref&gt;{{cite book |last=Gallistel |first= |title=The Organization of Learning |location=Cambridge |publisher=The MIT Press |year=1990 |ISBN=0-262-07113-4 }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Whishaw | first1 = I. Q. | last2 = Hines | first2 = D. J. | last3 = Wallace | first3 = D. G. | doi = 10.1016/S0166-4328(01)00359-X | title = Dead reckoning (path integration) requires the hippocampal formation: Evidence from spontaneous exploration and spatial learning tasks in light (allothetic) and dark (idiothetic) tests | journal = Behavioural Brain Research | volume = 127 | issue = 1–2 | pages = 49–69 | year = 2001 | pmid =  11718884| pmc = }}&lt;/ref&gt;
* ''[[Population]] Growth''. A simple (though approximate) model of population growth is the [[Malthusian growth model]]. A slightly more realistic and largely used population growth model is the [[logistic function]], and its extensions. 
* ''Individual-based cellular automata models of [[population]] growth''
[[File:Logical deterministic individual-based cellular automata model of single species population growth.gif|left|thumb|150px]]
&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
* ''Model of a particle  in a potential-field''. In this model we consider a particle as being a point of mass which describes a trajectory in space which is modeled by a function giving its coordinates in space as a function of time. The potential field is given by a function &lt;math&gt;V\!:\mathbb{R}^3\!\rightarrow\mathbb{R}&lt;/math&gt; and the trajectory, that is a function &lt;math&gt;\mathbf{r}\!:\mathbb{R}\rightarrow\mathbb{R}^3&lt;/math&gt;, is the solution of the [[differential equation]]:

::&lt;math&gt; -\frac{\mathrm{d}^2\mathbf{r}(t)}{\mathrm{d}t^2}m=\frac{\partial V[\mathbf{r}(t)]}{\partial x}\mathbf{\hat{x}}+\frac{\partial V[\mathbf{r}(t)]}{\partial y}\mathbf{\hat{y}}+\frac{\partial V[\mathbf{r}(t)]}{\partial z}\mathbf{\hat{z}}, &lt;/math&gt;

that can be written also as:

::&lt;math&gt; m\frac{\mathrm{d}^2\mathbf{r}(t)}{\mathrm{d}t^2}=-\nabla V[\mathbf{r}(t)]. &lt;/math&gt;

:Note this model assumes  the particle is a point mass, which is certainly known to be false in many cases in which we use this model; for example, as a model of planetary motion.

* ''Model of rational behavior for a consumer''.  In this model we assume a consumer faces a choice of ''n'' commodities labeled 1,2,...,''n'' each with a market price ''p''&lt;sub&gt;1&lt;/sub&gt;, ''p''&lt;sub&gt;2&lt;/sub&gt;,..., ''p''&lt;sub&gt;''n''&lt;/sub&gt;. The consumer is assumed to have a ''cardinal'' utility function ''U'' (cardinal in the sense that it assigns numerical values to utilities), depending on the amounts of commodities ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,..., ''x''&lt;sub&gt;''n''&lt;/sub&gt; consumed.  The model further assumes that the consumer has a budget ''M'' which is used to purchase a vector ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,..., ''x''&lt;sub&gt;''n''&lt;/sub&gt; in such a way as to maximize ''U''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;).  The problem of rational behavior in this model then becomes an [[Optimization (mathematics)|optimization]] problem, that is:
:: &lt;math&gt; \max U(x_1,x_2,\ldots, x_n) &lt;/math&gt;
:: subject to:
:: &lt;math&gt; \sum_{i=1}^n p_i x_i \leq M.&lt;/math&gt;
:: &lt;math&gt; x_{i} \geq 0   \; \; \; \forall i \in \{1, 2, \ldots, n \} &lt;/math&gt;
: This model has been used in [[general equilibrium theory]], particularly to show existence and [[Pareto efficiency]] of economic equilibria.  However, the fact that this particular formulation assigns ''numerical values'' to levels of satisfaction is the source of criticism (and even ridicule).  However, it is not an essential ingredient of the theory and again this is an idealization.
* ''[[Neighbour-sensing model]]'' explains the [[mushroom]] formation from the initially chaotic [[fungus|fungal]] network.
* ''[[Computer science]]'': models in Computer Networks, data models, surface model,...
* ''[[Mechanics]]'': movement of rocket model,...

Modeling requires selecting and identifying relevant aspects of a situation in the real world.

== See also ==
{{Portal|Mathematics}}
{{div col|2}}
* [[Agent-based model]]
* [[Cliodynamics]]
* [[Computer simulation]]
* [[Conceptual model]]
* [[Decision engineering]]
* [[Grey box model]]
* [[Mathematical biology]]
* [[Mathematical diagram]]
* [[Mathematical psychology]]
* [[Mathematical sociology]]
* [[Model inversion]]
* [[Microscale and macroscale models]]
* [[Statistical Model]]
* [[System identification]]
* [[TK Solver]] - Rule Based Modeling
{{div col end}}

== References ==
{{Reflist}}

== Further reading ==

===Books===
* Aris, Rutherford [ 1978 ] ( 1994 ). ''Mathematical Modelling Techniques'', New York: Dover. ISBN 0-486-68131-9
* Bender, E.A. [ 1978 ] ( 2000 ). ''An Introduction to Mathematical Modeling'', New York: Dover. ISBN 0-486-41180-X
* Gershenfeld, N. (1998) ''The Nature of Mathematical Modeling'', [[Cambridge University Press]] ISBN 0-521-57095-6 .
* Lin, C.C. &amp; Segel, L.A. ( 1988 ). ''Mathematics Applied to Deterministic Problems in the Natural Sciences'', Philadelphia: SIAM. ISBN 0-89871-229-7

===Specific applications===
* [[Korotayev]] A., Malkov A., Khaltourina D. (2006). [http://cliodynamics.ru/index.php?option=com_content&amp;task=view&amp;id=124&amp;Itemid=70 ''Introduction to Social Macrodynamics: Compact Macromodels of the World System Growth'']. Moscow: [http://urss.ru/cgi-bin/db.pl?cp=&amp;lang=en&amp;blang=en&amp;list=14&amp;page=Book&amp;id=34250 Editorial URSS] ISBN 5-484-00414-4 .
* {{Cite journal | last1 = Peierls | first1 = R. | doi = 10.1080/00107518008210938 | title = Model-making in physics | journal = Contemporary Physics | volume = 21 | pages = 3–17 | year = 1980 | pmid =  | pmc = |bibcode = 1980ConPh..21....3P }}
* ''[http://anintroductiontoinfectiousdiseasemodelling.com/ An Introduction to Infectious Disease Modelling]'' by Emilia Vynnycky and Richard G White.

== External links ==

;General reference

* Patrone, F. [http://www.fioravante.patrone.name/mat/u-u/en/differential_equations_intro.htm Introduction to modeling via differential equations], with critical remarks.
* [http://plus.maths.org/issue44/package/index.html Plus teacher and student package: Mathematical Modelling.] Brings together all articles on mathematical modeling from ''[[Plus Magazine]]'', the online mathematics magazine produced by the Millennium Mathematics Project at the University of Cambridge.

;Philosophical

* Frigg, R. and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science], in: The Stanford Encyclopedia of Philosophy, (Spring 2006 Edition)
* Griffiths, E. C. (2010) [https://sites.google.com/a/ncsu.edu/emily-griffiths/whatisamodel.pdf What is a model?]

{{Authority control}}

{{DEFAULTSORT:Mathematical Model}}
[[Category:Applied mathematics]]
[[Category:Collective intelligence]]
[[Category:Conceptual models]]
[[Category:Knowledge representation]]
[[Category:Mathematical modeling| ]]
[[Category:Mathematical terminology]]</text>
      <sha1>rynmrsy8u9m5zy4j74jlcegwy2ifo09</sha1>
    </revision>
  </page>
  <page>
    <title>Categorization</title>
    <ns>0</ns>
    <id>72717</id>
    <revision>
      <id>759688568</id>
      <parentid>759685697</parentid>
      <timestamp>2017-01-12T17:02:42Z</timestamp>
      <contributor>
        <username>Fyrael</username>
        <id>6675779</id>
      </contributor>
      <comment>This currently reads as though it's an entire article of original research</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8344" xml:space="preserve">{{Refimprove|date=January 2017}}
{{selfref|For information about Wikipedia's article categories, see [[Help:Category]].}}
{{for|particular uses|Category (disambiguation)}}
{{Information science}}
'''Categorization''' is the process in which ideas and objects are recognized, differentiated, and understood.&lt;ref&gt;Cohen, H., &amp; Lefebvre, C. (Eds.). (2005).[https://books.google.com/books?id=5WDfl14RgKMC ''Handbook of Categorization in Cognitive Science'']. Elsevier.&lt;/ref&gt; Categorization implies that objects are grouped into categories, usually for some specific purpose. Ideally, a category illuminates a [[Binary relation|relationship]] between the [[subject (philosophy)|subject]]s and [[object (philosophy)|object]]s of knowledge. Categorization is fundamental in language, prediction, [[inference]], decision making and in all kinds of environmental interaction. It is indicated that categorization plays a major role in [[computer programming]].&lt;ref&gt;Frey, T., Gelhausen, M., &amp; Saake (2011).[http://ecs.victoria.ac.nz/twiki/pub/Events/PLATEAU/Program/plateau2011-frey.pdf '' Categorization of Concerns – A Categorical Program Comprehension Model. In Proceedings of the Workshop on Evaluation and Usability of Programming Languages and Tools (PLATEAU) at the ACM Onward! and SPLASH Conferences. October, 2011. Portland, Oregon, USA''].&lt;/ref&gt;

There are many categorization theories and techniques. In a broader historical view, however, three general approaches to categorization may be identified:
* Classical categorization
* Conceptual clustering
* Prototype theory

==The classical view==
{{main|Categories (Aristotle)}}
'''Classical categorization''' first appears in the context of [[Western Philosophy]] in the work of [[Plato]], who, in his [[Statesman (dialogue)|Statesman]] dialogue, introduces the approach of grouping objects based on their similar [[Property (philosophy)|properties]]. This approach was further explored and systematized by [[Aristotle]] in his [[Categories (Aristotle)|Categories]] treatise, where he analyzes the differences between [[Class (philosophy)|class]]es and [[Object (philosophy)|object]]s. Aristotle also applied intensively the classical categorization scheme in his approach to the classification of living beings (which uses the technique of applying successive narrowing questions such as "Is it an animal or vegetable?", "How many feet does it have?", "Does it have fur or feathers?", "Can it fly?"...), establishing this way the basis for natural [[Taxonomy (biology)|taxonomy]].

The classical [[Aristotelianism|Aristotelian]] view claims that categories are discrete entities characterized by a set of properties which are shared by their members. In [[analytic philosophy]], these properties are assumed to establish the conditions which are both [[necessary and sufficient condition]]s to capture meaning. 

According to the classical view, categories should be clearly defined, mutually exclusive and collectively exhaustive. This way, any entity of the given classification universe belongs unequivocally to one, and only one, of the proposed categories.

==Conceptual clustering==
{{main|Conceptual clustering}}
'''Conceptual clustering''' is a modern variation of the classical approach, and derives from attempts to explain how knowledge is represented. In this approach, [[Class (philosophy)|class]]es (clusters or entities) are generated by first formulating their conceptual descriptions and then classifying the entities according to the descriptions. 

Conceptual clustering developed mainly during the 1980s, as a machine paradigm for [[unsupervised learning]]. It is distinguished from ordinary [[Cluster analysis|data clustering]] by generating a concept description for each generated category. 

Categorization tasks in which category labels are provided to the learner for certain objects are referred to as supervised classification, [[supervised learning]], or [[concept learning]]. Categorization tasks in which no labels are supplied are referred to as unsupervised classification, [[unsupervised learning]], or [[Cluster analysis|data clustering]]. The task of supervised classification involves extracting information from the labeled examples that allows accurate prediction of class labels of future examples. This may involve the [[abstraction]] of a rule or concept relating observed object features to category labels, or it may not involve abstraction (e.g., [[Exemplar theory|exemplar model]]s). The task of clustering involves recognizing inherent structure in a data set and grouping objects together by similarity into classes. It is thus a process of ''generating'' a classification structure. 

Conceptual clustering is closely related to [[fuzzy set]] theory, in which objects may belong to one or more groups, in varying degrees of fitness.

==Prototype theory==
{{main|Prototype theory}}
Since the research by [[Eleanor Rosch]] and [[George Lakoff]] in the 1970s, categorization can also be viewed as the process of grouping things based on [[prototype]]s—the idea of necessary and sufficient conditions is almost never met in categories of naturally occurring things. It has also been suggested that categorization based on prototypes is the basis for human development, and that this learning relies on learning about the world via [[embodied cognition|embodiment]].

A [[cognition|cognitive]] approach accepts that natural categories are graded (they tend to be fuzzy at their boundaries) and inconsistent in the status of their constituent members.

Systems of categories are not objectively "out there" in the world but are rooted in people's experience. Conceptual categories are not identical for different cultures, or indeed, for every individual in the same culture. 

Categories form part of a hierarchical structure when applied to such subjects as [[Taxonomy (biology)|taxonomy]] in [[biological classification]]: higher level: life-form level, middle level: generic or [[genus]] level, and lower level: the [[species]] level. These can be distinguished by certain traits that put an item in its distinctive category. But even these can be arbitrary and are subject to revision.

Categories at the middle level are perceptually and conceptually the more salient. The generic level of a category tends to elicit the most responses and richest images and seems to be the psychologically basic level. Typical taxonomies in zoology for example exhibit categorization at the [[embodied cognition|embodied]] level, with similarities leading to formulation of "higher" categories, and differences leading to differentiation within categories.

== Miscategorization ==
Miscategorization can be a [[Fallacy|logical fallacy]] in which diverse and dissimilar objects, concepts, entities, etc. are grouped together based upon illogical common denominators, or common denominators that virtually any concept, object or entity have in common. A common way miscategorization occurs is through an over-categorization of concepts, objects or entities, and then miscategorization based upon characters that virtually all things have in common.

== See also ==
{{too many see alsos|date=October 2013}}
{{columns-list|3| 
* [[Lumpers and splitters]]
* [[Artificial neural network]]
* [[Category learning]]
* [[Categorical perception]]
* [[Classification in machine learning]]
* [[Family resemblance]]
* [[Fuzzy concept]]
* [[Language acquisition]]
* [[Library classification]]
* [[Machine learning]]
* [[Multi-label classification]]
* [[Natural kind]]
* [[Ontology]]
* [[Pattern recognition]]
* [[Perceptual learning]]
* [[Semantics]]
* [[Socrates]]
* [[Sortal]]
* [[Structuralism]]
* [[Symbol grounding]]
* [[Taxonomy (general)]]
}}

== References ==
{{Reflist}}

==External links==
{{Wiktionary}}
* [http://eprints.ecs.soton.ac.uk/11725/ To Cognize is to Categorize: Cognition is Categorization]
*[http://toolserver.org/~dapete/catgraph/ Wikipedia Categories Visualizer]
*[http://www.revue-emulations.net/archives/n8/categentretien Interdisciplinary Introduction to Categorization: Interview with Dvora Yanov (political sciences), Amie Thomasson (philosophy) and Thomas Serre (artificial intelligence)]

{{philosophy of language}}

[[Category:Knowledge representation]]
[[Category:Concepts in epistemology]]
[[Category:Semantics]]
[[Category:Cognition]]</text>
      <sha1>d6up0gikqv0q2346b0kmulxecpn4zyu</sha1>
    </revision>
  </page>
  <page>
    <title>Group concept mapping</title>
    <ns>0</ns>
    <id>1539290</id>
    <revision>
      <id>732078977</id>
      <parentid>732040771</parentid>
      <timestamp>2016-07-29T13:23:54Z</timestamp>
      <contributor>
        <username>Pinkbeast</username>
        <id>11291690</id>
      </contributor>
      <comment>/* External links */ two spamlinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9778" xml:space="preserve">'''Group concept mapping''' is a structured methodology for organizing the ideas of a group on any topic of interest and representing those ideas visually in a series of interrelated maps.&lt;ref name="ref1"&gt;Kane M, Trochim WM (2007). Concept mapping for planning and evaluation. Thousand Oaks, CA: Sage Publications.&lt;/ref&gt;&lt;ref name="ref2"&gt;Trochim W (1989). An introduction to concept mapping for evaluation and planning. Evaluation and Program Planning, 12(1), 1-16.&lt;/ref&gt;  It is a type of integrative mixed method,&lt;ref&gt;Caracelli VW, Greene JC (1993). Data analysis strategies for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 15(2), 195-207.&lt;/ref&gt;&lt;ref&gt;Greene JC, Caracelli VJ, Graham WF (1989). Toward a conceptual framework for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 11, 255-274.&lt;/ref&gt; combining qualitative and quantitative approaches to [[data collection]] and [[analysis]].  Group concept mapping allows for a collaborative group process with groups of any size, including a broad and diverse array of participants.&lt;ref name="ref1" /&gt;  Since its development in the late 1980s by William M.K. Trochim at [[Cornell University]], it has been applied to various fields and contexts, including community and public health,&lt;ref&gt;Rao JK, Alongi J, Anderson LA, Jenkins L, Stokes GA, Kane M (2005). Development of public health priorities for end-of-life initiatives. American Journal of Preventive Medicine, 29(5), 453-460.&lt;/ref&gt;&lt;ref&gt;Risisky D, Hogan VK, Kane M, Burt B, Dove C, Payton M (2008). Concept mapping as a tool to engage a community in health disparity identification. Ethnicity &amp; Disease, 18, 77-83.&lt;/ref&gt;&lt;ref&gt;Trochim W, Milstein B, Wood B, Jackson S, Pressler V (2004). Setting objectives for community and systems change: an application of concept mapping for planning a statewide health improvement initiative. Health Promotion Practice, 5, 8–19.&lt;/ref&gt;&lt;ref&gt;Trochim WM, Cabrera DA, Milstein B, Gallagher RS, Leischow SJ (2006). Practical challenges of systems thinking and modeling in public health. American Journal of Public Health, 96(3), 538-546.&lt;/ref&gt; social work,&lt;ref&gt;Petrucci C, Quinlan KM (2007). Bridging the research-practice gap: concept mapping as a mixed-methods strategy in practice-based research and evaluation. Journal of Social Science Research, 34(2), 25-42.&lt;/ref&gt;&lt;ref&gt;Ridings JW, Powell DM, Johnson JE, Pullie CJ, Jones CM, Jones RL, Terrell KJ (2008). Using concept mapping to promote community building: the African American Initiative at Roseland. Journal of Community Practice, 16(1), 39-63.&lt;/ref&gt; health care,&lt;ref&gt;Trochim WM, Kane M (2005). Concept mapping: an introduction to structured conceptualization in health care. International Journal for Quality in Health Care, 7(3),187-191.&lt;/ref&gt; human services,&lt;ref&gt;Pammer W, Haney M, Wood BM, Brooks RG, Morse K, Hicks P, Handler EG, Rogers H, Jennett P (2001). Use of telehealth technology to extend child protection team services. Pediatrics, 108(3), 584-590.&lt;/ref&gt;&lt;ref&gt;Paulson BL, Truscott D, Stuart J (1999). Clients’ perceptions of helpful experiences in counseling. Journal of Counseling Psychology, 46(3), 317-324.&lt;/ref&gt; and biomedical research and evaluation.&lt;ref&gt;Kagan JM, Kane M, Quinlan KM, Rosas S, Trochim WMK (2009). Developing a conceptual framework for an evaluation system for the NIAID HIV/AIDS clinical trials networks. Health Research Policy and Systems, 7, 12.&lt;/ref&gt;&lt;ref&gt;Robinson JM, Trochim WMK (2007). An examination of community members’, researchers’ and health professionals’ perceptions of barriers to minority participation in medical research: an application of concept mapping. Ethnicity &amp; Health, 12(5), 521-539.&lt;/ref&gt;&lt;ref&gt;Trochim WM, Markus SE, Masse LC, Moser RP, and Weld PC (2008). The evaluation of large research initiatives: a participatory integrative mixed-methods approach. American Journal of Evaluation, 29(1), 8–28.&lt;/ref&gt;

==Overview==
Group concept mapping integrates qualitative group processes with [[multivariate analysis]] to help a group organize and visually represent its ideas on any topic of interest through a series of related maps.&lt;ref name="ref1" /&gt;&lt;ref name="ref2" /&gt;  It combines the ideas of diverse participants to show what the group thinks and values in relation to the specific topic of interest. It is a type of structured conceptualization used by groups to develop a conceptual framework, often to help guide evaluation and planning efforts.&lt;ref name="ref2" /&gt;  Group concept mapping is participatory in nature, allowing participants to have an equal voice and to contribute through various methods.&lt;ref name="ref1" /&gt; A group concept map visually represents all the ideas of a group and how they relate to each other, and depending on the scale, which ideas are more relevant, important, or feasible.

==Process==

Group concept mapping involves a structured multi-step process, including [[brainstorming]], sorting and rating, [[multidimensional scaling]] and [[cluster analysis]], and the generation and interpretation of multiple maps.&lt;ref name="ref1" /&gt;&lt;ref name="ref2" /&gt;  The first step requires participants to brainstorm a large set of statements relevant to the topic of interest, usually in response to a focus prompt.  Participants are then asked to individually sort those statements into categories based on their perceived similarity and rate each statement on one or more scales, such as importance or feasibility.

The data is then analyzed using The Concept System® software, which creates a series of interrelated maps using [[multidimensional scaling]] (MDS) of the sort data, [[hierarchical clustering]] of the MDS coordinates applying [[Ward's method|Ward’s method]], and the computation of average ratings for each statement and cluster of statements.&lt;ref name="ref3"&gt;Rosas SR, Camphausen (2007).  The use of concept mapping for scale development and validation in evaluation.  Evaluation and Program Planning, 30, 125-135.&lt;/ref&gt;  The resulting maps display the individual statements in two-dimensional space with more similar statements located closer to each other, and grouped into clusters that partition the space on the map.  The Concept System® software also creates other maps that show the statements in each cluster rated on one or more scales, and absolute or relative cluster ratings between two cluster sets.  As a last step in the process, participants are led through a structured interpretation session to better understand and label all the maps.

==History==
Group concept mapping was developed as a methodology in the late 1980s by William M.K. Trochim at [[Cornell University]].  Trochim is considered to be a leading evaluation expert, and he has taught evaluation and research methods at Cornell since 1980.&lt;ref name="ref4"&gt;Cornell University, College of Human Ecology (2012).  William Trochim biographical statement. http://www.human.cornell.edu/bio.cfm?netid=wmt1.&lt;/ref&gt;  Originally called "concept mapping", the methodology has evolved since its inception with the maturation of the field and the continued advancement of the software, which is now a Web application.

==Uses==
Group concept mapping can be used with any group for any topic of interest.  It is often used by government agencies, academic institutions, national associations, not-for-profit and community-based organizations, and private businesses to help turn the ideas of the group into measurable actions.  This includes in the areas of organizational development, strategic planning, needs assessment, curriculum development, research, and evaluation.&lt;ref name="ref1" /&gt;  Group concept mapping is well-documented, well-established methodology, and it has been used in hundreds of published papers.

==Group concept mapping versus concept mapping and mind mapping==
Concept mapping is any process used for visually representing relationships between ideas in pictures or maps.&lt;ref name="ref1" /&gt;  The technique was originally developed in the 1970s by [[Joseph D. Novak]] at [[Cornell University]]. &lt;ref name="ref5"&gt;Novak JD, Gowin DB (1984). Learning How to Learn. Cambridge: Cambridge University Press.&lt;/ref&gt;  A [[concept map]] is typically a diagram of multiple ideas, often represented as boxes or circles, linked in a hierarchical structure through arrows and words where each idea is connected to each other and linked back to the original idea.&lt;ref name="ref5" /&gt;  Concept mapping tends to be more free form, and may involve an individual or group.  Unlike other forms of concept mapping, group concept mapping is purposefully designed to work with groups and has a more structured process for organizing and visually representing the ideas of a group through a series of specific steps.&lt;ref name="ref1" /&gt;

A [[mind map]] is a diagram used to visually represent information, centering on one word or idea with categories and sub-categories radiating off of it.&lt;ref&gt;Buzan T (2010).  The Mind Map Book: Unlock Your Creativity, Boost Your Memory, Change Your Life. Essex: British Broadcasting Company.&lt;/ref&gt;  Popularized by [[Tony Buzan]] in the 1970s, mind mapping is often a spontaneous exercise done by an individual or group to gather information about what they think around a single topic.  In contrast to mind mapping, group concept mapping represents multiple ideas and it has a less flexible and more structured process.  Group concept mapping is also specific to groups.

==See also==
* [[Knowledge representation and reasoning]]
* [[List of concept- and mind-mapping software]]

==References==
{{reflist}}

==External links==
* [http://www.socialresearchmethods.net/mapping/mapping.htm Concept mapping research guide]

[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Survey methodology]]</text>
      <sha1>osxf88ox9ikmlrrex5tgcq48ffe2w7l</sha1>
    </revision>
  </page>
  <page>
    <title>Consistency (knowledge bases)</title>
    <ns>0</ns>
    <id>25154802</id>
    <revision>
      <id>731053736</id>
      <parentid>347480606</parentid>
      <timestamp>2016-07-22T16:50:53Z</timestamp>
      <contributor>
        <username>Yaron K.</username>
        <id>2276977</id>
      </contributor>
      <comment>Changed category back to "Knowledge representation" - nothing web-related here</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="508" xml:space="preserve">A [[knowledge base]] KB is '''consistent''' ''[[iff]]'' its negation is not a [[Tautology (logic)|tautology]].

I.e., a knowledge base KB is inconsistent (not consistent) [[iff]] there is no [[Interpretation (logic)|interpretation]] which [[entailment|entails]] KB.

Example of an inconsistent knowledge base:

KB := { a, ¬a }

Consistency in terms of knowledge bases is mostly the same as the natural understanding of [[consistency]].

[[Category:Knowledge representation]]
{{logic-stub}}
{{database-stub}}</text>
      <sha1>dtbhlzru95r0t4mdp4ysewf5in0k6gq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computational fields of study</title>
    <ns>14</ns>
    <id>52242291</id>
    <revision>
      <id>750534442</id>
      <parentid>750259546</parentid>
      <timestamp>2016-11-20T10:06:25Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Inductive reasoning]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="706" xml:space="preserve">Computational fields of study are areas of research in an existing field using the power of computation, and which are usually named for that. Computational fields of study as a group are sometimes also be referred to as [[Computational X]]&lt;ref&gt;[http://blog.stephenwolfram.com/2016/09/how-to-teach-computational-thinking/ How to Teach Computational Thinking] by [[Stephen Wolfram]], Stephen Wolfram Blog, September 7, 2016.&lt;/ref&gt;.

[[Category:Computer science]]
[[Category:Knowledge representation]]
[[Category:Applied mathematics]]
[[Category:Big data]]
[[Category:Systems theory]]
[[Category:Computing and society]]
[[Category:Systems thinking]]
[[Category:Futurology]]
[[Category:Theories of deduction]]</text>
      <sha1>23fsujue2h4bd7m7z462hf67b493etd</sha1>
    </revision>
  </page>
  <page>
    <title>Hallin's spheres</title>
    <ns>0</ns>
    <id>31244175</id>
    <revision>
      <id>761583402</id>
      <parentid>752889039</parentid>
      <timestamp>2017-01-23T19:17:47Z</timestamp>
      <contributor>
        <ip>149.63.0.134</ip>
      </contributor>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7239" xml:space="preserve">'''Hallin's spheres''' is a theory of media objectivity posited by journalism historian [[Daniel C. Hallin]] in his book ''The Uncensored War'' to explain the coverage of the Vietnam war.&lt;ref name="DH"&gt;{{cite book|last=Hallin|first=Daniel|title=The Uncensored War: The Media and Vietnam.|year=1986|publisher=Oxford University press|location=New York|pages=116–118|isbn=978-0-19-503814-9}}&lt;/ref&gt; Hallin divides the world of political discourse into three concentric spheres: consensus, legitimate controversy, and deviance. In the sphere of consensus, journalists assume everyone agrees. The sphere of legitimate controversy includes the standard political debates, and journalists are expected to remain neutral. The sphere of deviance falls outside the bounds of legitimate debate, and journalists can ignore it. These boundaries shift, as public opinion shifts.&lt;ref&gt;[http://www.cjr.org/analysis/trump_inspires_murrow_moment_for_journalism.php For journalists covering Trump, a Murrow moment], By David Mindich, Columbia Journalism Review, July 15, 2016&lt;/ref&gt;

Hallin's spheres, which deals with the media, are similar to the [[Overton window]], which deals with public opinion generally, and posits a sliding scale of public opinion on any given issue ranging from conventional wisdom to unacceptable. 

Hallin used the concept of [[Framing (social sciences)|framing]] to describe the presentation and reception of issues in public. For example, framing the use of drugs as criminal activity can encourage the public to consider that behavior anti-social. Hallin also used the concept of an [[opinion corridor]], in which the range of public opinion narrows, and opinion outside that corridor moves from legitimate controversy into deviance. 

== Description ==

=== Sphere of consensus ===
This sphere contains those topics on which there is widespread agreement, or at least the perception thereof. Within the sphere of consensus, 'journalists feel free to invoke a generalized "we" and to take for granted shared values and shared assumptions'&lt;ref&gt;Schudson 2002, p. 40&lt;/ref&gt; Example include such things as free speech, the abolition of slavery, or human rights.  For topic in this sphere "journalists do not feel compelled to present an opposing view point or to remain disinterested observers."&lt;ref name="DH"/&gt;

=== Sphere of legitimate controversy ===
For topics in this sphere rational and informed people hold differing views. These topics are therefore the most important to cover, and also ones upon which journalists are obliged to remain disinterested reporters, rather than advocating for or against a particular view.&lt;ref&gt;Hallin, 1986, p. 116;&lt;/ref&gt; Schudson notes that Hallin, in his influential study of the US media during the Vietnam War, argues that journalism's commitment to objectivity has always been compartmentalized. That is, within a certain sphere—the sphere of legitimate controversy—journalists seek conscientiously to be balanced and objective.&lt;ref&gt;Schudson, M (2002) 'What's unusual about covering politics as usual', in Zelizer, B., &amp; Allan, S. (Eds.). Journalism after September 11. London: Routledge, p. 40&lt;/ref&gt;

=== Sphere of deviance ===
Topics in this sphere are rejected by journalists as being unworthy of general consideration.  Such views are perceived as being either unfounded, taboo, or of such minor consequence that they are not news worthy.  Hallin argues that in the sphere of deviance, 'journalists also depart from standard norms of objective reporting and feel authorized to treat as marginal, laughable, dangerous, or ridiculous individuals and groups who fall far outside a range of variation taken as legitimate.'&lt;ref&gt;Schudson 2002, 40&lt;/ref&gt; For example, a person claiming that aliens are manipulating college basketball scores might have difficulty finding media coverage for such a claim.&lt;ref&gt;Hallin, 1986, p. 117&lt;/ref&gt;

== Uses of the terms ==
Craig Watkins (2001, pp.&amp;nbsp;92–4) makes use of the Hallin's spheres in a paper examining ABC, CBS, and NBC television network television news coverage of the "Million Man March", a demonstration that took place in Washington, DC on October 16, 1995. Watkins analyzes the dominant framing practices-problem definition, rhetorical devices, use of sources, and images-employed by journalists to make sense of this particular expression of political protest. He argues that Hallins three spheres are a way for media framing practices to develop specific reportorial contexts, each sphere develops its own distinct style of news reporting resources by different rhetorical tropes and discourses.&lt;ref&gt;Watkins, S. C. (2001). Framing protest: News media frames of the Million Man March. Critical Studies in Media Communication, 18(1), 83-101.&lt;/ref&gt;

Piers Robinson (2001, p.&amp;nbsp;536) uses the concept in relation to debate that have emerged over the extent to which the mass media serves elite interests or, alternatively, plays a powerful role in shaping political outcomes. His articles reviews Hallin's spheres as an example of media-state relations, that highlights theoretical and empirical shortcomings in the 'manufacturing consent' thesis (Chomsky McChesney).&lt;ref&gt;Herman, E. S., &amp; Chomsky, N. (2010). Manufacturing consent: The political economy of the mass media. Random House.&lt;/ref&gt; Robinson argues that a more nuanced and bi-directional understanding is needed of the direction of influence between media and the state that builds upon, rather than rejecting, existing theoretical accounts.&lt;ref&gt;Robinson, P. (2001). Theorizing the Influence of Media on World Politics Models of Media Influence on Foreign Policy. European Journal of Communication, 16(4), 523-544.&lt;/ref&gt;

Hallin's theory assumed a relatively homogenized media environment, where most producers were trying to reach most consumers. A more fractured media landscape can challenge this assumption.&lt;ref&gt;{{cite web|title=Does NPR Have A Liberal Bias?|url=http://www.onthemedia.org/2012/sep/14/|work=On The Media from NPR|publisher=WNYC|accessdate=11 February 2013}}&lt;/ref&gt; because different audiences may place topics in different spheres, a concept related to the [[filter bubble]], which posits that many members of the public choose to limit their media consumption to the areas of consensus and deviance that they personally prefer.

==See also==
* [[Ambit claim]]
* [[Argument to moderation]]
* [[Creeping normality]]
* [[Cultural hegemony]]
* [[Door-in-the-face technique]]
* [[Political suicide]]
* [[Slippery slope]]
* [[Spiral of silence]]
* [[Third rail of politics]]

== References ==
{{reflist}}

==External links==
* [http://archive.pressthink.org/2009/01/12/atomization.html Audience Atomization Overcome: Why the Internet Weakens the Authority of the Press], Rosen, Jay. PressThink.org, January 12, 2009
* [http://wnymedia.net/smith/2009/03/the-sphere-of-deviance/ The Sphere of Deviance] Smith, Christopher. WNYMedia.net, 2009.
* [http://www.onthemedia.org/2012/sep/14/ Does NPR have a Liberal Bias?], On The Media from NPR. Retrieved 11 February 2013

{{Media culture}}
{{Media manipulation}}
{{Propaganda}}

[[Category:Framing (social sciences)]]
[[Category:Knowledge representation]]
[[Category:Propaganda techniques]]
[[Category:Journalism]]</text>
      <sha1>pduoifdz93dvxozjp6mzsd0o2mfuznr</sha1>
    </revision>
  </page>
  </mediawiki>