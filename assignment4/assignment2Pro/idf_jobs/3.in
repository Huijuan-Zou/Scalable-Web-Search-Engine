<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Internet search</title>
    <ns>14</ns>
    <id>8321034</id>
    <revision>
      <id>666703046</id>
      <parentid>547928112</parentid>
      <timestamp>2015-06-13T01:34:55Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval, tidy up comment</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="228" xml:space="preserve">{{Cat main|Internet search}}

[[Category:Web services]]
[[Category:Information retrieval]]
[[Category:World Wide Web|Search]] &lt;!-- searching is a web function. Note that [[Internet search]] redirects to [[Web search engine]] --&gt;</text>
      <sha1>6bp9rbz93mtxeowt780j0keu5nkf5cq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval organizations</title>
    <ns>14</ns>
    <id>46964829</id>
    <revision>
      <id>747326238</id>
      <parentid>666703545</parentid>
      <timestamp>2016-11-01T19:13:47Z</timestamp>
      <contributor>
        <ip>50.53.1.33</ip>
      </contributor>
      <comment>[[:Category:Organizations by subject]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="72" xml:space="preserve">[[Category:Information retrieval]]
[[Category:Organizations by subject]]</text>
      <sha1>6oxv3bk2f696b15yla517vsopfxwpr9</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval systems</title>
    <ns>14</ns>
    <id>46964839</id>
    <revision>
      <id>666703925</id>
      <timestamp>2015-06-13T01:42:50Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34" xml:space="preserve">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Metadirectory</title>
    <ns>0</ns>
    <id>943530</id>
    <revision>
      <id>669335889</id>
      <parentid>669335659</parentid>
      <timestamp>2015-06-30T14:08:36Z</timestamp>
      <contributor>
        <username>Ronz</username>
        <id>7862</id>
      </contributor>
      <comment>/* Open source software */ no independent sources</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1180" xml:space="preserve">A '''metadirectory''' system provides for the flow of data between one or more [[directory service]]s and [[database]]s, in order to maintain synchronization of that data, and is an important part of [[identity management]] systems. The data being synchronized typically are collections of entries that contain user profiles and possibly authentication or policy information. Most metadirectory deployments synchronize data into at least one [[Lightweight Directory Access Protocol|LDAP]]-based directory server, to ensure that LDAP-based applications such as [[single sign-on]] and portal servers have access to recent data, even if the data is mastered in a non-LDAP data source.

Metadirectory products support filtering and transformation of data in transit.

Most [[identity management]] suites from commercial vendors include a metadirectory product, or a [[provisioning#User provisioning|user provisioning]] product.

== See also ==
* [[Virtual directory]]
* [[Identity correlation]]
* [[Microsoft Identity Integration Server]]
* [[Novell Identity Manager]]
* [[Critical Path, Inc.|Critical Path Metadirectory]]

[[Category:Directory services]]
[[Category:Data management]]</text>
      <sha1>g2r3einul3mmaoab7n9uv66xc7cd9ll</sha1>
    </revision>
  </page>
  <page>
    <title>Thomas write rule</title>
    <ns>0</ns>
    <id>217343</id>
    <revision>
      <id>732152254</id>
      <parentid>694446583</parentid>
      <timestamp>2016-07-30T00:18:26Z</timestamp>
      <contributor>
        <ip>197.215.241.230</ip>
      </contributor>
      <comment>Removed plural form of transaction in "For example a transaction..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2829" xml:space="preserve">In [[computer science]], particularly the field of [[database]]s, the '''Thomas write rule''' is a rule in [[timestamp-based concurrency control]].  It can be summarized as ''ignore outdated writes''.

It states that, if a more recent transaction has already written the value of an object, then a less recent transaction does not need perform its own write since it will eventually be overwritten by the more recent one.  

The Thomas write rule is applied in situations where a predefined '''logical''' order is assigned to transactions when they start.  For example a transaction might be assigned a monotonically increasing timestamp when it is created.  The rule prevents changes in the order in which the transactions are executed from creating different outputs: The outputs will always be consistent with the predefined logical order.

For example consider a database with 3 variables (A, B, C), and two atomic operations C := A (T1), and C := B (T2).  Each transaction involves a read (A or B), and a write (C).  The only conflict between these transactions is the write on C.  The following is one possible schedule for the operations of these transactions:

:&lt;math&gt;\begin{bmatrix}
T_1 &amp; T_2 \\
&amp; Read(A) \\
Read(B) &amp;   \\
 &amp;Write(C)   \\
Write(C) &amp;  \\
Commit &amp; \\
&amp; Commit \end{bmatrix} \Longleftrightarrow
\begin{bmatrix}
T_1 &amp; T_2 \\
&amp; Read(A) \\
Read(B) &amp; \\
&amp; Write(C) \\
 &amp; \\
Commit &amp; \\
&amp; Commit\\
\end{bmatrix}
&lt;/math&gt;

If (when the transactions are created) T1 is assigned a timestamp that precedes T2 (i.e., according to the logical order, T1 comes first), then only T2's write should be visible.  If, however, T1's write is executed after T2's write, then we need a way to detect this and discard the write.

One practical approach to this is to label each value with a write timestamp (WTS) that indicates the timestamp of the last transaction to modify the value.  Enforcing the Thomas write rule only requires checking to see if the write timestamp of the object is greater than the time stamp of the transaction performing a write.  If so, the write is discarded   

In the example above, if we call TS(T) the timestamp of transaction T, and WTS(O) the write timestamp of object O, then T2's write sets WTS(C) to TS(T2).  When T1 tries to write C, it sees that TS(T1) &lt; WTS(C), and discards the write.  If a third transaction T3 (with TS(T3) &gt; TS(T2)) were to then write to C, it would get TS(T3) &gt; WTS(C), and the write would be allowed.

==References==
*{{Cite journal | author=Robert H. Thomas | title=A majority consensus approach to concurrency control for multiple copy databases | journal=ACM Transactions on Database Systems | year=1979 | volume=4 | issue=2 | pages= 180&#8211;209 | doi=10.1145/320071.320076 }}

[[Category:Data management]]
[[Category:Transaction processing]]


{{compu-sci-stub}}</text>
      <sha1>6w3cax51sr45pdzipfbf9gxqkrvushu</sha1>
    </revision>
  </page>
  <page>
    <title>Rollback (data management)</title>
    <ns>0</ns>
    <id>1015240</id>
    <revision>
      <id>761417861</id>
      <parentid>757600594</parentid>
      <timestamp>2017-01-22T21:59:24Z</timestamp>
      <contributor>
        <username>ImperfectlyInformed</username>
        <id>5106682</id>
      </contributor>
      <comment>/* See also */ add [[schema migration]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2696" xml:space="preserve">{{Other uses|Rollback (disambiguation)}}
{{Selfref|For the Wikipedia tool, see [[Wikipedia:Rollback feature]].}}
{{no footnotes|date=June 2009}}

In [[database]] technologies, a '''rollback''' is an operation which returns the database to some previous state. Rollbacks are important for database [[data integrity|integrity]], because they mean that the database can be restored to a clean copy even after erroneous operations are performed. They are crucial for recovering from database server crashes; by rolling back any [[Database transaction|transaction]] which was active at the time of the crash, the database is restored to a consistent state.

The rollback feature is usually implemented with a [[Database log|transaction log]], but can also be implemented via [[multiversion concurrency control]].

==Cascading rollback==
A ''cascading rollback'' occurs in database systems when a transaction (T1) causes a failure and a rollback must be performed. Other transactions dependent on T1's actions must also be rollbacked due to T1's failure, thus causing a cascading effect. That is, one transaction's failure causes many to fail.

Practical database recovery techniques guarantee cascadeless rollback, therefore a cascading rollback is not a desirable result.

==SQL==
In [[SQL]], &lt;code&gt;ROLLBACK&lt;/code&gt; is a command that causes all data changes since the last &lt;code&gt;[[Begin work (SQL)|BEGIN WORK]]&lt;/code&gt;, or &lt;code&gt;[[Start transaction (SQL)|START TRANSACTION]]&lt;/code&gt; to be discarded by the [[relational database management systems]] (RDBMS), so that the state of the data is "rolled back" to the way it was before those changes were made.

A &lt;code&gt;ROLLBACK&lt;/code&gt; statement will also release any existing [[savepoint]]s that may be in use.

In most SQL dialects, &lt;code&gt;ROLLBACK&lt;/code&gt;s are connection specific.  This means that if two connections are made to the same database, a &lt;code&gt;ROLLBACK&lt;/code&gt; made in one connection will not affect any other connections.  This is vital for proper [[Concurrent programming|concurrency]].

==See also==
*[[Savepoint]]
*[[Commit (data management)|Commit]]
*[[Undo]]
*[[Schema migration]]

==References==
*{{cite book |author = [[Ramez Elmasri]] |title= Fundamentals of Database Systems |publisher= [[Pearson Addison Wesley]] |year= 2007|isbn= 0-321-36957-2 }}
*[http://msdn2.microsoft.com/en-us/library/ms181299.aspx "ROLLBACK Transaction"], Microsoft SQL Server.
*[http://www.pantz.org/software/mysql/mysqlcommands.html "Sql Commands"], MySQL.

{{Databases}}
{{Web syndication}}

[[Category:Data management]]
[[Category:Database theory]]
[[Category:SQL]]
[[Category:Transaction processing]]
[[Category:Reversible computing]]


{{compu-prog-stub}}</text>
      <sha1>m6x3r76p4ngwpki24vjfexqc0wltt0f</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise Objects Framework</title>
    <ns>0</ns>
    <id>59561</id>
    <revision>
      <id>760026911</id>
      <parentid>687688793</parentid>
      <timestamp>2017-01-14T14:56:57Z</timestamp>
      <contributor>
        <username>&#28436;&#27468;&#12499;&#12491;&#12540;&#12523;</username>
        <id>23956730</id>
      </contributor>
      <minor />
      <comment>"Mac OS X" -&gt; "macOS"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9613" xml:space="preserve">The '''Enterprise Objects Framework''', or more commonly simply '''EOF''', was introduced by [[NeXT]] in 1994 as a pioneering [[object-relational mapping]] product for its [[NeXTSTEP]] and [[OpenStep]] development platforms. EOF abstracts the process of interacting with a [[relational database]], mapping database rows to [[Java (programming language)|Java]] or [[Objective-C]] [[Object (computer science)|objects]]. This largely relieves developers from writing low-level [[SQL]] code. EOF enjoyed some niche success in the mid-1990s among financial institutions who were attracted to the rapid application development advantages of NeXT's object-oriented platform. Since [[Apple Inc]]'s merger with NeXT in 1996, EOF has evolved into a fully integrated part of [[WebObjects]], an application server also originally from NeXT.

== History ==
In the early 1990s [[NeXT]] Computer recognized that connecting to databases was essential to most businesses and yet also potentially complex.  Every data source has a different data-access language (or [[Application programming interface|API]]), driving up the costs to learn and use each vendor's product. The NeXT engineers wanted to apply the advantages of [[object-oriented programming]], by getting objects to "talk" to relational databases. As the two technologies are very different, the solution was to create an abstraction layer, insulating developers from writing the low-level procedural code ([[SQL]]) specific to each data source.

The first attempt came in 1992 with the release of Database Kit (DBKit), which wrapped an object-oriented framework around any database. Unfortunately, [[NEXTSTEP]] at the time was not powerful enough and DBKit had serious design flaws.

NeXT's second attempt came in 1994 with the Enterprise Objects Framework (EOF) version 1, a [[Rewrite (programming)|complete rewrite]] that was far more modular and [[OpenStep]] compatible. EOF 1.0 was the first product released by [[NeXT]] using the Foundation Kit and introduced autoreleased objects to the developer community. The development team at the time was only four people: Jack Greenfield, Rich Williamson, Linus Upson and Dan Willhite. EOF 2.0, released in late 1995, further refined the architecture, introducing the editing context. At that point, the development team consisted of Dan Willhite, [[Craig Federighi]], Eric Noyau and Charly Kleissner.

EOF achieved a modest level of popularity in the financial programming community in the mid-1990s, but it would come into its own with the emergence of the [[World Wide Web]] and the concept of [[web application]]s. It was clear that EOF could help companies plug their legacy databases into the Web without any rewriting of that data. With the addition of frameworks to do state management, load balancing and dynamic HTML generation, NeXT was able to launch the first object-oriented Web application server, [[WebObjects]], in 1996, with EOF at its core.

In 2000, Apple Inc. (which had merged with NeXT) officially dropped EOF as a standalone product, meaning that developers would be unable to use it to create desktop applications for the forthcoming [[macOS|Mac OS X]].  It would, however, continue to be an integral part of a major new release of WebObjects.  WebObjects 5, released in 2001, was significant for the fact that its frameworks had been ported from their native [[Objective-C]] programming language to the [[Java (programming language)|Java]] language.  Critics of this change argue that most of the power of EOF was a side effect of its Objective-C roots, and that EOF lost the beauty or simplicity it once had.  Third-party tools, such as [[EOGenerator]], help fill the deficiencies introduced by Java (mainly due to the loss of [[Objective-C#Categories|categories]]).

The Objective-C code base was re-introduced with some modifications to desktop application developers as [[Core Data]], part of Apple's [[Cocoa (API)|Cocoa API]], with the release of [[Mac OS X Tiger]] in April 2005.

==How EOF works==

Enterprise Objects provides tools and frameworks for object-relational mapping. The technology specializes in providing mechanisms to retrieve data from various data sources, such as relational databases via JDBC and JNDI directories, and mechanisms to commit data back to those data sources. These mechanisms are designed in a layered, abstract approach that allows developers to think about data retrieval and commitment at a higher level than a specific data source or data source vendor.

&lt;!--  Commented out because image was deleted: [[Image:EoModeler.png|frame|right|EOModeler application icon (Mac OS X)]] --&gt;Central to this mapping is a model file (an "EOModel") that you build with a visual tool &amp;mdash; either EOModeler, or the EOModeler plug-in to [[Xcode]]. The mapping works as follows:

* Database tables are mapped to classes.
* Database columns are mapped to class attributes.
* Database rows are mapped to objects (or class instances).

You can build data models based on existing data sources or you can build data models from scratch, which you then use to create data structures (tables, columns, joins) in a data source. The result is that database records can be transposed into Java objects.

The advantage of using data models is that applications are isolated from the idiosyncrasies of the data sources they access. This separation of an application's business logic from database logic allows developers to change the database an application accesses without needing to change the application.

EOF provides a level of database transparency not seen in other tools and allows the same model to be used to access different vendor databases and even allows relationships across different vendor databases without changing source code.  

Its power comes from exposing the underlying data sources as managed graphs of persistent objects.  In simple terms, this means that it organizes the application's model layer into a set of defined in-memory data objects.  It then tracks changes to these objects and can reverse those changes on demand, such as when a user performs an undo command.  Then, when it is time to save changes to the application's data, it archives the objects to the underlying data sources.

===Using Inheritance===

In designing Enterprise Objects developers can leverage the object-oriented feature known as [[Inheritance (computer science)|inheritance]]. A Customer object and an Employee object, for example, might both inherit certain characteristics from a more generic Person object, such as name, address, and phone number. While this kind of thinking is inherent in object-oriented design, relational databases have no explicit support for inheritance. However, using Enterprise Objects, you can build data models that reflect object hierarchies. That is, you can design database tables to support inheritance by also designing enterprise objects that map to multiple tables or particular views of a database table.

==What is an Enterprise Object (EO)? ==

An Enterprise Object is analogous to what is often known in object-oriented programming as a [[Business object (computer science)|business object]] &amp;mdash; a class which models a physical or [[conceptual object]] in the business domain (e.g. a customer, an order, an item, etc.). What makes an EO different from other objects is that its instance data maps to a data store. Typically, an enterprise object contains key-value pairs that represent a row in a relational database. The key is basically the column name, and the value is what was in that row in the database. So it can be said that an EO's properties persist beyond the life of any particular running application.

More precisely, an Enterprise Object is an instance of a class that implements the com.webobjects.eocontrol.EOEnterpriseObject interface.

An Enterprise Object has a corresponding model (called an EOModel) that defines the mapping between the class's object model and the database schema. However, an enterprise object doesn't explicitly know about its model. This level of abstraction means that database vendors can be switched without it affecting the developer's code. This gives Enterprise Objects a high degree of reusability.

== EOF and Core Data ==

Despite their common origins, the two technologies diverged, with each technology retaining a subset of the features of the original Objective-C code base, while adding some new features.

=== Features Supported Only by EOF ===

EOF supports custom SQL; shared editing contexts; nested editing contexts; and pre-fetching and batch faulting of relationships, all features of the original Objective-C implementation not supported by Core Data.  Core Data also does not provide the equivalent of an EOModelGroup&#8212;the NSManagedObjectModel class provides methods for merging models from existing models, and for retrieving merged models from bundles.

=== Features Supported Only by Core Data ===

Core Data supports fetched properties; multiple configurations within a managed object model; local stores; and store aggregation (the data for a given entity may be spread across multiple stores); customization and localization of property names and validation warnings; and the use of predicates for property validation.  These features of the original Objective-C implementation are not supported by the Java implementation.

== External links ==
* [http://www.linuxjournal.com/article.php?sid=7101&amp;mode=thread&amp;order=0&amp;thold=0 article in linuxjournal about GDL2]

[[Category:Data management]]
[[Category:NeXT]]
[[Category:Apple Inc. software]]</text>
      <sha1>1l0lmu5f9v0ud3bptxs3rb9uatgy2ar</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Metadata</title>
    <ns>14</ns>
    <id>2388558</id>
    <revision>
      <id>731303057</id>
      <parentid>544085047</parentid>
      <timestamp>2016-07-24T13:28:36Z</timestamp>
      <contributor>
        <username>Uanfala</username>
        <id>11049176</id>
      </contributor>
      <comment>+UDC classification</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="155" xml:space="preserve">{{Infobox library classification|CC = |DDC = |LCC = |UDC = 001.103.2}}
{{catdiffuse}}
{{Cat main|Metadata}}

[[Category:Data management]]
[[Category:Data]]</text>
      <sha1>jrht02f6061ji5glfhwz7a9t3j3ykkn</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise information integration</title>
    <ns>0</ns>
    <id>773166</id>
    <revision>
      <id>757143791</id>
      <parentid>757143029</parentid>
      <timestamp>2016-12-29T01:35:08Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>cols, integrate/rm items linked already, combine tags</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7242" xml:space="preserve">{{multiple issues|{{refimprove|date=February 2015}}
{{POV|date=February 2011}}}}
'''Enterprise information integration''' ('''EII''') is the ability to support a unified view of data and information for an entire organization.  In a [[data virtualization]] application of EII, a process of [[information integration]], using [[data abstraction]] to provide a unified interface (known as [[uniform data access]]) for viewing all the data within an organization, and a single set of structures and naming conventions (known as [[uniform information representation]]) to represent this data; the goal of EII is to get a large set of [[heterogeneous]] data sources to appear to a user or system as a single, homogeneous data source.

== Overview ==
[[Data]] within an [[Enterprise architecture|enterprise]] can be stored in heterogeneous formats, including [[relational database]]s (which themselves come in a large number of varieties), text files, [[XML]] files, [[spreadsheet]]s and a variety of proprietary [[data storage device|storage]] methods, each with their own [[index (database)|index]]ing and [[data access]] methods.

Standardized data access [[application programming interface|API]]s have emerged, that offer a specific set of commands to retrieve and modify data from a generic data source. Many applications exist that implement these APIs' commands across various data sources, most notably relational databases. Such APIs include [[ODBC]], [[JDBC]], [[XQJ]], [[OLE DB]], and more recently [[ADO.NET]].

There are also standard formats for representing data within a file, that are very important to information integration. The best-known of these is XML, which has emerged as a standard universal representation format. There are also more specific XML "grammars" defined for specific types of data, such as [[Geography Markup Language]] for expressing geographical features, and [[Directory Service Markup Language]], for holding directory-style information. In addition, non-XML standard formats exist, such as [[iCalendar]], for representing calendar information, and [[vCard]], for [[business card]] information.

Enterprise Information Integration (EII) applies [[data integration]] commercially.  Despite the theoretical problems described above, the private sector shows more concern with the problems of data integration as a viable product.&lt;ref name="refthree"&gt;{{cite conference | author=Alon Y. Halevy | title=Enterprise information integration: successes, challenges and controversies | booktitle=SIGMOD 2005 | year=2005 | pages=778&#8211;787 | url=http://www.cs.washington.edu/homes/alon/files/eiisigmod05.pdf|display-authors=etal}}&lt;/ref&gt;
EII emphasizes neither on correctness nor tractability, but speed and simplicity. An EII industry has emerged, but many professionals{{Who|date=June 2009}} believe it does not perform to its full potential.  Practitioners cite the following major issues which  EII must address for the industry to become mature:{{Citation needed|date=June 2009}}

; Combining disparate data sets : Each data source is disparate and as such is not designed to support EII.  Therefore, data virtualization as well as [[Federated database system|data federation]] depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.

:  One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.
; Simplicity of understanding : Answering queries with views arouses interest from a theoretical standpoint, but difficulties in understanding how to incorporate it as an "enterprise solution".{{Citation needed|date=June 2009}}  Some developers{{Who|date=June 2009}} believe it should be merged with [[Enterprise application integration|EAI]].  Others{{Who|date=June 2009}} believe it should be incorporated with ETL systems, citing customers' confusion over the differences between the two services.{{Citation needed|date=June 2009}}
; Simplicity of deployment : Even if recognized as a solution to a problem, EII {{as of | 2009 | lc = on}} currently takes time to apply and offers complexities in deployment.  People have proposed a variety of schema-less solutions such as "Lean Middleware",&lt;ref name="reffour"&gt;{{cite conference | author=David A. Maluf | title=Lean middleware | booktitle=SIGMOD 2005 | year=2005 | pages=788&#8211;791 | url=http://portal.acm.org/citation.cfm?id=1066157.1066247&amp;coll=portal&amp;dl=ACM&amp;type=series&amp;idx=1066157&amp;part=Proceedings&amp;WantType=Proceedings&amp;title=International%20Conference%20on%20Management%20of%20Data&amp;CFID=15151515&amp;CFTOKEN=6184618|display-authors=etal}}&lt;/ref&gt; but ease-of-use and speed of employment appear inversely proportional to the generality of such systems.{{Citation needed|date=June 2009}}  Others{{Who|date=June 2009}} cite the need for standard data interfaces to speed and simplify the integration process in practice.
; Handling higher-order information : Analysts experience difficulty&#8212;even with a functioning information integration system&#8212;in determining whether the sources in the database will  satisfy a given application.  Answering these kinds of questions about a set of repositories requires semantic information like [[metadata]] and/or ontologies.  The few commercial tools{{Which|date=June 2009}} that leverage this information remain in their infancy.

== Applications ==
EII products enable [[loose coupling]] between [[wiktionary:Homogeneous|homogeneous]]-data consuming client applications and services and heterogeneous-data stores.  Such client applications and services include Desktop Productivity Tools (spreadsheets, [[word processor]]s, presentation software, etc.), [[Integrated development environment|development environment]]s and [[Software framework|framework]]s ([[Java EE]], [[Microsoft .NET|.NET]], [[Mono (software)|Mono]], [[SOAP]] or [[Representational State Transfer|REST]]ful [[Web service]]s, etc.), [[business intelligence]] (BI), [[business activity monitoring]] (BAM) software, [[enterprise resource planning]] (ERP), [[Customer relationship management]] (CRM), [[business process management]] (BPM and/or BPEL) Software, and [[web content management]] (CMS).

==  Example technology vendors ==
*[[Capsenta]]
*[[Composite Software]]
*[[Denodo]]
*[[MetaMatrix]]
*[[XAware]]

== Data access technologies ==
*[[XQuery]] and [[XQuery API for Java]]
*[[Service Data Objects]] (SDO) for Java, C++ and .Net clients and any type of data source

== See also ==
{{div col|3}}
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Data warehouse]]
* [[Disparate system]]
* [[Enterprise integration]]
* [[Federated database system]]
* [[Resource Description Framework]]
* [[Semantic heterogeneity]]
* [[Semantic integration]]
* [[Semantic Web]]
* [[Web 2.0]]
* [[Web services]]
{{div col end}}

==References==

&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>9pyzz7i9s3j77airwg93yyzcpcku5ru</sha1>
    </revision>
  </page>
  <page>
    <title>IMS VDEX</title>
    <ns>0</ns>
    <id>5446251</id>
    <revision>
      <id>734815266</id>
      <parentid>671588320</parentid>
      <timestamp>2016-08-16T22:02:47Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>Removing link(s) to "IMS Global": rm redlink (deleted). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9168" xml:space="preserve">{{Multiple issues|
{{refimprove|date=December 2010}}
{{primary sources|date=December 2010}}
{{notability|date=December 2010}}
{{context|date=December 2010}}
}}

'''IMS VDEX''', which stands for '''IMS Vocabulary Definition Exchange''', is a mark-up language &#8211; or grammar &#8211; for [[Controlled vocabulary|controlled vocabularies]] developed by IMS Global as an open specification, with the Final Specification being approved in February 2004.

IMS VDEX allows the exchange and expression of simple machine-readable lists of human language terms, along with information that may assist a human in understanding the meaning of the various terms, i.e. a flat list of values, a hierarchical tree of values, a thesaurus, a taxonomy, a glossary or a dictionary.

Structural a vocabulary has an identifier, title and a list of terms. Each term has a unique key, titles and (optional) descriptions. A term may have nested terms, thus a hierarchical structure can be created. It is possible to define relationships between terms and add custom metadata to terms.

IMS VDEX support multilinguality. All values supposed to be read by a human, i.e. titles, can be defined in one or more languages.

== Purposes ==
VDEX was designed to supplement other IMS specifications and the IEEE LOM standard by giving additional semantic control to tool developers. IMS VDEX could be used for the following purposes. It is used in practice for other purposes as well.

* ''Interfaces providing pre-defined choices'' &#8211; providing radio buttons and drop-down menus for interfaces such as metadata editors or a repository browse tool, based on the vocabulary allowed in the metadata profile used
* ''Distributing vocabularies among many users'' &#8211; achieved by simple XML file sharing, or possibly a searchable [[Repository Open Service Interface Definition|repository]] or registry of vocabularies
* ''XML stylesheets used to select and generate different views'' &#8211; selecting an overview of an entire vocabulary as an [[HTML]] or [[PDF]] file, for example; providing scope notes for catalogues; or storing a glossary of terms which are called upon by hyperlinks within a document
* ''Validation of metadata instances'' &#8211; validated against an application profile, by comparison of the vocabulary terms used in certain metadata elements with those of the machine readable version of the vocabularies specified by the application profile.
* ''Controlled terms for other IMS specifications and IEEE LOM'' &#8211; both may contain elements where controlled terms should be used. These elements are often specified as being of a vocabulary data type, and a definition of the permitted terms and their usage may be expressed using VDEX.

== Technical details ==
[[Image:VDEX model.PNG|350px|right|thumb|simplified VDEX data model]]
The VDEX Information Model is represented in the diagram. A VDEX file describing a vocabulary comprises a number of information elements, most of which are relatively simple, such as a string representation of the default (human) language or a [[URI]] identifying the value domain (or vocabulary). Some of the elements are &#8216;containers&#8217; &#8211; such as a ''term'' &#8211; that contain additional elements.

Elements may be required or optional, and in some cases, repeatable. Within a term, for example, a ''description'' and ''caption'' may be defined. Multiple language definitions can be used inside a description, by using a ''langstring'' element, where the description is paired with the language to be used. Additional elements within a term include ''media descriptors'', which are one or more media files to supplement a term&#8217;s description; and ''metadata'', which is used to describe the vocabulary further.

The ''relationship'' container defines a relationship between terms by identifying the two terms and the specifying type or relationship, such as a term being broader or narrower than another. The term used to specify the type of relationship may conform to the ISO standards for thesauri.

''Vocabulary identifiers'' are unique, persistent URIs, whereas term or relationship identifiers are locally unique strings. VDEX also allows for a ''default language'' and ''vocabulary name'' to be given, and for whether the ordering of terms within the vocabulary is significant (''order significance'') to be specified.

A ''profile type'' is specified to describe the type of vocabulary being expressed; different features of the VDEX model are permitted depending on the profile type, providing a common grammar for several classes of vocabulary. For example, it is possible, in some profile types, for terms to be contained within one another and be nested, which is suited to the expression of hierarchical vocabularies. Five profile types exist: ''lax'', ''thesaurus'', ''hierarchicalTokenTerms'', &#8216;glossaryOrDictionary&#8217; and ''flatTokenTerms''. The lax profile is the least restrictive and offers the full VDEX model, whereas the flatTokenTerms profile is the most restrictive and lightweight.

VDEX also offers some scope for complex vocabularies, assuming the existence of a well-defined application profile (for exchange interoperability). Some examples are:
* ''Faceted schemes'' &#8211; faceted vocabularies are possible with the definition of appropriate relationships
* ''Multi-lingual thesauri'' &#8211; metadata could be used within a relationship to achieve multilingual thesauri
* ''Polyhierarchical taxonomies'' &#8211; can be expressed using the source/target value pairs in the relationship.

Identifiers in VDEX data should be persistent, unique, resolvable, transportable and URI-compliant. Specifically, vocabulary identifiers should be unique URIs, whereas term and relationship identifiers should be locally unique strings.

== Implementations ==
* [http://aloha2.netera.ca/ ALOHA Metadata Tagging Tool] &#8212; Java-based software project that can read IMS VDEX files.
* [http://www.ivimeds.org/news/demonstrator.html IVIMEDS 1G v1.0] &#8211; from The International Virtual Medical School &#8211; includes VDEX instances in curriculum maps. Partners can create their own maps in VDEX format and use these to help students search the repository.
* [http://www.elframework.org/projects/spws/view Skills Profiling Web Service] &#8212; project implemented and demonstrated use of a skills profiling web service using open standards in a medical context. IMS VDEX files were used in the representation of the SPWS hierarchy skills framework.
* [http://www.scottishdoctor.org/ Scottish Doctors] &#8212; project used VDEX as a format for expressing curricular outcome systems.
* [http://prs.heacademy.ac.uk/technical/vdex_scripts.html VDEX XSLT scripts] &#8212; developed by The Higher Education Academy Centre for Philosophical and Religious Studies to convert VDEX to XHTML and PostgreSQL .
* [http://www.icbl.hw.ac.uk/vdex VDEX Implementation Project] &#8212; carried out by the Institute for Computer Based Learning at Heriot-Watt University, with a primary objective of creating a tool for editing vocabularies in VDEX format. The project, which ended in January 2004, was based on the Public Draft (not the current Final Specification).
* [http://sourceforge.net/projects/vdex-j/ VDEX Java Binding] &#8212; implementation neutral Java interface for VDEX, as well as providing a default implementation of that interface, and XML marshalling functionality.
* [https://pypi.python.org/pypi/imsvdex imsvdex Python egg] &#8212;  API for VDEX XML-files. It is free software written in [[Python (programming language)|Python]].
* [http://plone.org/products/atvocabularymanager ATVocabularyManager] &#8212; addon for [[Plone]] CMS uses VDEX as a possible format to define vocabularies.
* [https://pypi.python.org/pypi/collective.vdexvocabulary collective.vdexvocabulary] &#8212; implements IMS VDEX as standard [[Zope]] vocabulary which can also be used in [[Plone]] CMS, written in [[Python (programming language)|Python]].
* [https://pypi.python.org/pypi/vdexcsv/ vdexcsv] &#8212; offers a commandline converter from [[Comma-separated values|CSV]] to VDEX. It is written in [[Python (programming language)|Python]].

== See also ==
*IMS Global
*[[Learning object metadata]]

==References==
#{{note|coillie}} Marc van Coillie [http://www.eife-l.org/publications/standards/interop/europasscv/europassCV-IMS-AP/usingvdex Using IMS VDEX for the EDS AP - EIfEL]
#{{note|sarasa}} Antonio Sarasa, Jose Manuel Canabal, Juan Carlos Sacristan, Raquel Jimenez [http://online-journals.org/i-jet/article/view/806 Using IMS VDEX in Agrega]

== External links ==
* [http://www.imsglobal.org/vdex IMS VDEX] &#8212; official resources by IMS global
* [http://wiki.cetis.ac.uk/What_is_IMS_VDEX What is IMS VDEX] &#8212; JISC CETIS
* [http://metadata.cetis.ac.uk/ CETIS Metadata and Digital Repository Special Interest Group (SIG)] &#8212; mailing list for those in UK Higher and Further Education  interested in creating, storing and serving educational metadata.

[[Category:Data management]]
[[Category:Educational technology standards]]
[[Category:Knowledge representation]]
[[Category:Library science]]
[[Category:Metadata]]
[[Category:Standards]]
[[Category:Standards organizations]]
[[Category:Technical communication]]</text>
      <sha1>ttrwfaqccd2ra89fkg7wveh0y0qlqzv</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed data store</title>
    <ns>0</ns>
    <id>870094</id>
    <revision>
      <id>748872239</id>
      <parentid>733885752</parentid>
      <timestamp>2016-11-10T22:01:10Z</timestamp>
      <contributor>
        <username>Kevinjenscox</username>
        <id>9894519</id>
      </contributor>
      <minor />
      <comment>/* Distributed non-relational databases */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6150" xml:space="preserve">{{Essay-like|date=May 2012}}

A '''distributed data store''' is a [[computer network]] where information is stored on more than one [[Node (networking)|node]], often in a [[Replication (computing)|replicated]] fashion.&lt;ref&gt;{{Citation
|author = Yaniv Pessach
|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations
|title = Distributed Storage
|edition = Distributed Storage: Concepts, Algorithms, and Implementations
}}&lt;/ref&gt; It is usually specifically used to refer to either a [[distributed database]] where users store information on a ''number of nodes'', or a [[computer network]] in which users store information on a ''number of peer network nodes''.

==Distributed databases==
[[Distributed database]]s are usually [[non-relational database]]s that make a quick access to data over a large number of nodes possible. Some distributed databases expose rich query abilities while others are limited to a [[key-value store]] semantics. Examples of limited distributed databases are [[Google]]'s [[BigTable]], which is much more than a [[distributed file system]] or a [[peer-to-peer network]],&lt;ref&gt;{{cite web
| accessdate = 2011-04-05
| location = http://the-paper-trail.org/
| publisher = Paper Trail
| title = BigTable: Google's Distributed Data Store
| quote = Although GFS provides Google with reliable, scalable distributed file storage, it does not provide any facility for structuring the data contained in the files beyond a hierarchical directory structure and meaningful file names. It&#8217;s well known that more expressive solutions are required for large data sets. Google&#8217;s terabytes upon terabytes of data that they retrieve from web crawlers, amongst many other sources, need organising, so that client applications can quickly perform lookups and updates at a finer granularity than the file level. [...] The very first thing you need to know about BigTable is that it isn&#8217;t a relational database. This should come as no surprise: one persistent theme through all of these large scale distributed data store papers is that RDBMSs are hard to do with good performance. There is no hard, fixed schema in a BigTable, no referential integrity between tables (so no foreign keys) and therefore little support for optimised joins.
| url = http://the-paper-trail.org/blog/?p=86}}&lt;/ref&gt; [[Amazon.com|Amazon]]'s [[Dynamo (storage system)|Dynamo]]&lt;ref&gt;{{cite web
| accessdate = 2011-04-05
| author = Sarah Pidcock
| date = 2011-01-31
| location = http://www.cs.uwaterloo.ca/
| page = 2/22
| publisher = WATERLOO &#8211; CHERITON SCHOOL OF COMPUTER SCIENCE
| title = Dynamo: Amazon&#8217;s Highly Available Key-value Store
| quote = Dynamo: a highly available and scalable distributed data store
| url = http://www.cs.uwaterloo.ca/~kdaudjee/courses/cs848/slides/sarah1.pdf}}&lt;/ref&gt;
and [[Azure Services Platform|Windows Azure Storage]].&lt;ref&gt;{{cite web 
| url= http://www.microsoft.com/windowsazure/features/storage/ |title= Windows Azure Storage |author= |date=2011-09-16 |work= |publisher= |accessdate=6 November 2011}}&lt;/ref&gt;

As the ability of arbitrary querying is not as important as the [[availability]], designers of distributed data stores have increased the latter at an expense of consistency. But the high-speed read/write access results in reduced consistency, as it is not possible to have both [[Consistency (database systems)|consistency]], availability, and partition tolerance of the network, as it has been proven by the [[CAP theorem]].

==Peer network node data stores==
In peer network data stores, the user can usually reciprocate and allow other users to use their computer as a storage node as well. Information may or may not be accessible to other users depending on the design of the network.

Most [[peer-to-peer]] networks do not have distributed data stores in that the user's data is only available when their node is on the network. However, this distinction is somewhat blurred in a system such as [[BitTorrent (protocol)|BitTorrent]], where it is possible for the originating node to go offline but the content to continue to be served. Still, this is only the case for individual files requested by the redistributors, as contrasted with a network such as [[Freenet]] where all computers are made available to serve all files.

Distributed data stores typically use an [[error detection and correction]] technique.
Some distributed data stores (such as [[Parchive]] over NNTP) use [[forward error correction]] techniques to recover the original file when parts of that file are damaged or unavailable.
Others try again to download that file from a different mirror.

==Examples==

===Distributed non-relational databases===
* [[Aerospike database|Aerospike]]
* [[Apache Cassandra]], former data store of [[Facebook]]
* [[BigTable]], the data store of [[Google]]
* [[CrateIO]]
* [[Druid (open-source data store)]], used by [[Netflix]], [[Yahoo]] and others
* [[Dynamo (storage system)|Dynamo]] of [[Amazon.com|Amazon]]
* [[Hazelcast]]
* [[HBase]], current data store of Facebook's Messaging Platform
* [[Couchbase]], data store used by [[LinkedIn]], [[Paypal]], [[Ebay]] and others.
* [[MongoDB]]
* [[Riak]]
* [[Hypertable]], from [[Baidu]]
* [[Voldemort (distributed data store)|Voldemort]], data store used by [[LinkedIn]]

===Peer network node data stores===
* [[BitTorrent (protocol)|BitTorrent]]
* [[Blockchain (database)]]
* [[Chord project]]
* [[GNUnet]]
* [[Freenet]]
* Unity, of the software [[Perfect Dark (P2P)|Perfect Dark]]
* [[Mnet (Computer program)|Mnet]]
* [[Network News Transfer Protocol|NNTP]] (the distributed data storage protocol used for [[Usenet]] news)
* [[Storage@home]]
* [[Tahoe-LAFS]]

==See also==
{{Portal|Computer Science}}
* [[Data store]]
* [[Distributed file system]]
* [[Keyspace (distributed data store)|Keyspace]], the DDS [[Schema (database)|schema]]
* [[Peer-to-peer]]
* [[Distributed hash table]]
* [[Distributed cache]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Distributed data storage| ]]
[[Category:Distributed data stores| ]]

[[ja:&#20998;&#25955;&#12501;&#12449;&#12452;&#12523;&#12471;&#12473;&#12486;&#12512;#&#20998;&#25955;&#12487;&#12540;&#12479;&#12473;&#12488;&#12450;]]</text>
      <sha1>k2xfjmp63utf1ytzykhzhzryxkg9wfv</sha1>
    </revision>
  </page>
  <page>
    <title>DAMA</title>
    <ns>0</ns>
    <id>1390371</id>
    <revision>
      <id>710362641</id>
      <parentid>695541045</parentid>
      <timestamp>2016-03-16T14:35:41Z</timestamp>
      <contributor>
        <ip>83.104.230.249</ip>
      </contributor>
      <comment>/* The Data Management Body of Knowledge (DMBOK) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4767" xml:space="preserve">{{about|the association||Dama (disambiguation)}}

== About DAMA ==

'''DAMA''' (the [http://dama.org Data Management Association]) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and [[data resource management]] (DRM).

DAMA's primary purpose is to promote the understanding, development and practice of managing information and data as a key enterprise asset. The group is organized as a set of more than 40 chapters and members-at-large around the world, with an International Conference held every year. 

== Chapters, Chapter Structure, and Central Membership ==

DAMA International is organised through a Chapter structure, with each Chapter being a separate legal entity that formally affiliates with DAMA International. There are over 40 chapters established in over 16 countries around the world. The United States is disproportionately represented in the number of Chapters due to the number of city-based chapters as opposed to country-level in other jurisdictions.

In 2015 DAMA introduced a "Central" membership to help support and develop member services in a more consistent manner internationally and to provide a clear rallying point for members world wide who may lack a local chapter structure.

A full listing of Chapters can be found on the [http://www.dama.org/browse-chapters DAMA International website]. This list does not include "chapters in formation" which have yet to meet the criteria for recognition as Chapters and formal affiliation with DAMA-I.

== The Data Management Body of Knowledge (DMBOK) ==

The DAMA Guide to the [[Data management|Data Management]] Body of Knowledge" (DAMA-DMBOK Guide) was first published in April 5th, 2009.  

It defines ten knowledge domains which are at the core of Information and Data Management. 
* Data Governance (the central knowledge domain that connects all the others) 
* Data Architecture Management 
* Data Development 
* Data Operations Management 
* Data Security Management 
* Reference and Master Data Management 
* Data Warehousing and Business Intelligence 
* Document and Content Management 
* Metadata Management 
* Data Quality Management 
The DMBOK is copyright DAMA International.

== CDMP ==

DAMA International is the owner of the [[Certified Data Management Professional]] certification. This Certification is based on a range of learning objectives derived from the DMBOK.

In October 2015, DAMA International terminated its relationship with ICCP who had provided administrative services for the delivery of the CDMP certification. 

== DAMA Awards ==

From its inception in 1989 through to 2002, The DAMA Individual Achievement Awards have recognized a data professional who has made significant, demonstrable contributions to the information resource management industry consistent with DAMA International's vision.  From 2003 to 2015, DAMA created additional categories to recognize more people who have made special contributions to the world of data management.  The awards categories are:
* Academic Achievement Award: To a member from academia for outstanding research or theoretical contributions in the area of IRM/DRM
* DAMA Community Award : To a member of the DAMA community who has gone beyond the call of volunteer service to enhance the efforts of providing exceptional benefits to the DAMA Membership.
* Government Achievement Award : To a member of the leadership populace for instituting the inclusion and adherence to DRM/IRM principles.
* Professional Achievement Award : To a member from the &#8216;industry&#8217; (business, discipline, specialist) who has made significant, demonstrable contributions to the IRM/DRM.
* Lifetime Achievement and Contribution Award : This special award has been presented to John Zachman in 2002, Michael Brackett in 2006 and Catherine Nolan in 2015. 

Starting in 2016, DAMA International created the DAMA International Award for Data Management Excellence.  This award will be presented to organizations or individuals who have made contributions to data management principles.  The first awards under this new structure will be given in April, 2016.

=== List of Award Winners ===

A list of award winners can be found on the [https://www.dama.org/content/award-results DAMA website].

=== Speakers Bureau ===
DAMA International provides a speakers bureau service to connect conference and event organisers with internationally regarded expert speakers.

A full listing of speakers can be found [http://www.dama.org/speakers here].

== References ==
&lt;references /&gt;

==External links==
* [http://www.dama.org/ DAMA International]

{{prof-assoc-stub}}
[[Category:Data management]]</text>
      <sha1>g0wf75cuq9whkgvsmpk3s4n8sjmuk07</sha1>
    </revision>
  </page>
  <page>
    <title>Scriptella</title>
    <ns>0</ns>
    <id>8303455</id>
    <revision>
      <id>712747548</id>
      <parentid>711485899</parentid>
      <timestamp>2016-03-30T22:14:23Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]] using [[Project:AWB|AWB]] (11971)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3665" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=March 2012}}
{{Notability|Products|date=March 2012}}
}}

{{Infobox Software
| name = Scriptella
| logo = [[File:Scriptella logo.png|160px|Scriptella logo]]
| latest_release_version = 1.1
| latest_release_date = 28 December 2012
| operating_system = [[Cross-platform]]
| genre = [[Extract transform load|ETL]], [[Data migration]] and [[SQL]].
| license = [[Apache Software License]]
| website = [http://scriptella.org http://scriptella.org]
}}

'''Scriptella''' is an open source [[Extract transform load|ETL (Extract-Transform-Load)]] and script execution tool written in Java. Its primary focus is simplicity. It doesn't require the user to learn another complex XML-based language to use it, but allows the use of SQL or another scripting language suitable for the data source to perform required transformations. Scriptella does not offer any [[graphical user interface]].

==Typical use==
* Database migration.
* Database creation/update scripts.
* Cross-database ETL operations, import/export.
* Alternative for Ant &lt;sql&gt; task.
* Automated database schema upgrade.

==Features==
* '''Simple XML syntax''' for scripts. Add dynamics to your existing SQL scripts by creating a thin wrapper XML file:&lt;source lang="xml"&gt;
      &lt;!DOCTYPE etl SYSTEM "http://scriptella.javaforge.com/dtd/etl.dtd"&gt;
      &lt;etl&gt;
          &lt;connection driver="$driver" url="$url" user="$user" password="$password"/&gt;
          &lt;script&gt;
              &lt;include href="PATH_TO_YOUR_SCRIPT.sql"/&gt;
              -- And/or directly insert SQL statements here
          &lt;/script&gt;
      &lt;/etl&gt;
&lt;/source&gt;
* Support for '''multiple datasources''' (or multiple connections to a single database) in an ETL file.
* Support for many useful '''[[Java Database Connectivity|JDBC]] features''', e.g. parameters in SQL including file blobs and JDBC escaping.
* '''Performance.''' Performance and low memory usage are one of the primary goals.
* Support for '''evaluated expressions and properties''' (JEXL syntax)
* Support for '''cross-database ETL scripts''' by using &lt;dialect&gt; elements
* '''Transactional execution'''
* '''Error handling''' via &lt;onerror&gt; elements
* '''Conditional scripts/queries execution''' (similar to Ant if/unless attributes but more powerful)
* '''Easy-to-Use''' as a standalone tool or Ant task. No deployment/installation required.
* '''Easy-To-Run''' ETL files directly from Java code.
* '''Built-in adapters for popular databases''' for a tight integration. Support for any database with JDBC/[[Open Database Connectivity|ODBC]] compliant driver.
* Service Provider Interface (SPI) for interoperability with non-JDBC DataSources and integration with scripting languages. Out of the box support for [[JSR 223|JSR 223 (Scripting for the Java Platform)]] compatible languages.
* Built-In [[Comma-separated values|CSV]], TEXT, [[XML]], [[Lightweight Directory Access Protocol|LDAP]], [[Apache Lucene|Lucene]], [[Apache Velocity|Velocity]], JEXL and Janino providers. Integration with [[Java EE]], [[Spring framework|Spring Framework]], [[Java Management Extensions|JMX]] and [[JNDI]] for enterprise ready scripts.

==External links==
* [http://scriptella.org Scriptella ETL Site]
* [https://github.com/scriptella/scriptella-etl GitHub Page]
* [http://groups.google.com/group/scriptella/ Discussion forum]
* [http://www.javaforge.com/proj/forum/browseForum.do?forum_id=3126 Discussion forum(deprecated)]
* [http://jroller.com/page/ejboy Scriptella ETL Author's Blog]
* {{Ohloh project|id=4526|name=Scriptella ETL}}

[[Category:Extract, transform, load tools]]
[[Category:Data warehousing products]]
[[Category:Data management]]


{{compu-stub}}</text>
      <sha1>l58xu5xyo58tdujaeuqu8ctd0wjr8oh</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Transaction processing</title>
    <ns>14</ns>
    <id>11300280</id>
    <revision>
      <id>586756500</id>
      <parentid>544805793</parentid>
      <timestamp>2013-12-19T07:35:25Z</timestamp>
      <contributor>
        <username>Codename Lisa</username>
        <id>16847332</id>
      </contributor>
      <comment>removed [[Category:System software]]; added [[Category:Utility software by type]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="159" xml:space="preserve">{{Cat main|Transaction processing}}

[[Category:Concurrency control]]
[[Category:Data management]]
[[Category:Utility software by type]]
[[Category:Databases]]</text>
      <sha1>kbyik55mn8agrequ2q5cr877v09b9p5</sha1>
    </revision>
  </page>
  <page>
    <title>Global concurrency control</title>
    <ns>0</ns>
    <id>12380968</id>
    <revision>
      <id>544873458</id>
      <parentid>461118993</parentid>
      <timestamp>2013-03-17T07:23:19Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q5570820]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2824" xml:space="preserve">{{POV|Commitment ordering|date=November 2011}}
'''Global concurrency control''' typically pertains to the [[concurrency control]] of a system comprising several components, each with its own concurrency control. The overall concurrency control of the whole system, the ''Global concurrency control'', is determined by the concurrency control of its components, [[Modular programming|module]]s. In this case also the term '''Modular concurrency control''' is used.

In many cases a system may be distributed over a communication network. In this case we deal with [[distributed concurrency control]] of the system, and the two terms sometimes overlap. However, distributed concurrency control typically relates to a case where the distributed system's components do not have each concurrency control of its own, but rather are involved with a concurrency control mechanism that spans several components in order to operate. For example, as typical in a [[distributed database]].

In ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') global concurrency control relates to the concurrency control of a ''multidatabase system'' (for example, a [[Federated database]]; other examples are [[Grid computing]] and [[Cloud computing]] environments). It deals with the properties of the ''global [[schedule (computer science)|schedule]]'', which is the unified schedule of the multidatabase system, comprising all the individual schedules of the [[database system]]s and possibly other [[transactional object]]s in the system. A major goal for global concurrency control is ''[[Global serializability]]'' (or ''Modular serializability''). The problem of achieving global serializability in a [[heterogeneous]] environment had been [[open problem|open]] for many years, until an effective solution based on [[Commitment ordering]] (CO) has been proposed (see [[Global serializability]]). Global concurrency control deals also with [[global serializability#Relaxing global serializability|relaxed]] forms of global serializability which compromise global serializability (and in many applications also correctness, and thus are avoided there). While local (to a database system) [[Serializability#Relaxing serializability|relaxed serializability methods]] compromise serializability for performance gain (utilized when the application allows), it is unclear that the various proposed relaxed global serializability methods provide any performance gain over CO, which guarantees global serializability.


==See also==
*[[Concurrency control]]
*[[Global serializability]]
*[[Commitment ordering]]
*[[Distributed concurrency control]]

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]</text>
      <sha1>6xf9m467sigpbh9kqq0ewf6tlbdzrxk</sha1>
    </revision>
  </page>
  <page>
    <title>Data governance</title>
    <ns>0</ns>
    <id>6222875</id>
    <revision>
      <id>755902747</id>
      <parentid>755866414</parentid>
      <timestamp>2016-12-20T20:55:31Z</timestamp>
      <contributor>
        <username>RLB2016</username>
        <id>29857789</id>
      </contributor>
      <minor />
      <comment>/* Data governance drivers */ added link to GDPR - a newer external regulation driving data governance</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13781" xml:space="preserve">{{Governance}}

'''Data governance''' is a [[Control (management)|control]] that ensures that the [[data]] entry by an operations team member or by automated processes meets precise standards, such as a business rule, a data definition and data integrity constraints in the data model. The data governor uses data quality monitoring against production data to communicate errors in data back to operational team members, or to the technical support team, for corrective action.  Data governance is used by organizations to exercise control over processes and methods used by their [[data stewards]] and [[data custodian]]s in order to improve data quality.

Data governance is a set of processes that ensures that important data assets are formally managed throughout the enterprise. Data governance ensures that data can be trusted and that people can be made accountable for any adverse event that happens because of low data quality. It is about putting people in charge of fixing and preventing issues with data so that the enterprise can become more efficient. Data governance also describes an evolutionary process for a company, altering the company&#8217;s way of thinking and setting up the processes to handle information so that it may be utilized by the entire organization. It&#8217;s about using technology when necessary in many forms to help aid the process. When companies desire, or are required, to gain control of their data, they empower their people, set up processes and get help from technology to do it.&lt;ref name="sarsfield"&gt;Sarsfield, Steve (2009). "The Data Governance Imperative", IT Governance.&lt;/ref&gt;

According to one vendor, data governance is a [[quality control]] discipline for assessing, managing, using, improving, monitoring, maintaining, and protecting organizational information. It is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.&lt;ref name="The DGI Data Governance Framework"&gt;{{cite web|url=http://www.datagovernance.com/wp-content/uploads/2014/11/dgi_framework.pdf|title=The DGI Data Governance Framework}}&lt;/ref&gt;

== Overview ==
Data governance encompasses the people, processes, and [[information technology]] required to create a consistent and proper handling of an organization's data across the business enterprise.  Goals may be defined at all levels of the enterprise and doing so may aid in acceptance of processes by those who will use them.  Some goals include

* Increasing consistency and confidence in [[decision making]]
* Decreasing the risk of regulatory fines
* Improving [[information security|data security]], also defining and verifying the requirements for data distribution policies&lt;ref&gt;Gianni, D., (2015, Jan). Data Policy Definition and Verification for System of Systems Governance, in Modeling and Simulation Support for System of Systems Engineering [http://onlinelibrary.wiley.com/doi/10.1002/9781118501757.ch5/summary]&lt;/ref&gt; 
* Maximizing the income generation potential of data
* Designating accountability for information quality
* Enable better planning by supervisory staff
* Minimizing or eliminating re-work
* Optimize staff effectiveness
* Establish process performance baselines to enable improvement efforts
* Acknowledge and hold all gain

These goals are realized by the implementation of Data governance programs, or initiatives using Change Management techniques

==Data governance drivers==
While data governance initiatives can be driven by a desire to improve data quality, they are more often driven by C-Level leaders responding to external regulations. Examples of these regulations include [[Sarbanes-Oxley]], [[Basel I]], [[Basel II]], [[HIPAA]], [[General Data Protection Regulation|GDPR]] and a number of data privacy regulations. To achieve compliance with these regulations, business processes and controls require formal management processes to govern the data subject to these regulations.&lt;ref&gt;[http://www.rimes.com/rimes-data-governance-handbook 'Rimes Data Governance Handbook'] [[RIMES]]&lt;/ref&gt; Successful programs identify drivers meaningful to both supervisory and executive leadership.

Common themes among the external regulations center on the need to manage risk. The risks can be financial misstatement, inadvertent release of sensitive data, or poor data quality for key decisions. Methods to manage these risks vary from industry to industry. Examples of commonly referenced best practices and guidelines include [[COBIT]], [[ISO/IEC 38500]], and others. The proliferation of regulations and standards creates challenges for data governance professionals, particularly when multiple regulations overlap the data being managed. Organizations often launch data governance initiatives to address these challenges.

== Data governance initiatives (Dimensions)==
Data governance initiatives improve [[data quality]] by assigning a team responsible for data's accuracy, accessibility, consistency, and completeness, among other metrics.  This team usually consists of executive leadership, [[project management]], [[line function|line-of-business managers]], and [[data steward]]s. The team usually employs some form of methodology for tracking and improving enterprise data, such as [[Six Sigma]], and tools for [[data mapping]], [[data profiling|profiling]], cleansing, and monitoring data.

Data governance initiatives may be aimed at achieving a number of objectives including offering better visibility to internal and external customers (such as [[supply chain]] management), compliance with [[compliance (regulation)|regulatory law]], improving operations after rapid company growth or [[mergers and acquisitions|corporate mergers]], or to aid the efficiency of enterprise [[knowledge worker]]s by reducing confusion and error and increasing their scope of knowledge. Many data governance initiatives are also inspired by past attempts to fix information quality at the departmental level, leading to incongruent and redundant data quality processes. Most large companies have many applications and databases that can't easily share information. Therefore, knowledge workers within large organizations often don't have access to the information they need to best do their jobs. When they do have access to the data, the [[data quality]] may be poor. By setting up a data governance practice or [[corporate data|Corporate Data]] Authority, these problems can be mitigated.

The structure of a data governance initiative will vary not only with the size of the organization, but with the desired objectives or the 'focus areas' &lt;ref name="focus areas"&gt;{{cite web|url=http://datagovernance.com/fc_focus_areas_for_data_governance.html |title=Data Governance Focus Areas |deadurl=yes |archiveurl=https://web.archive.org/web/20081006152845/http://www.datagovernance.com/fc_focus_areas_for_data_governance.html |archivedate=2008-10-06 |df= }}&lt;/ref&gt; of the effort.

== Implementation ==
Implementation of a Data Governance initiative may vary in scope as well as origin. Sometimes, an executive mandate will arise to initiate an enterprise wide effort, sometimes the mandate will be to create a pilot project or projects, limited in scope and objectives, aimed at either resolving existing issues or demonstrating value. Sometimes an initiative will originate lower down in the organization&#8217;s hierarchy, and will be deployed in a limited scope to demonstrate value to potential sponsors higher up in the organization.  The initial scope of an implementation can vary greatly as well, from review of a one-off IT system, to a cross-organization initiative.

== Data governance tools ==
Leaders of successful data governance programs declared in December 2006 at the Data Governance Conference in Orlando, Fl, that data governance is between 80 and 95 percent communication."&lt;ref&gt;{{cite web
 |url=http://www.dmreview.com/issues/2007_48/10001356-1.html 
 |title=Data Governance: One Size Does Not Fit All 
 |last=Hopwood 
 |first=Peter 
 |authorlink=Peter Hopwood 
 |publisher=[[DM Review Magazine]] 
 |date=June 2008 
 |accessdate=2008-10-02 
 |archiveurl=http://www.webcitation.org/5bGHaz1gA?url=http://www.dmreview.com/issues/2007_48/10001356-1.html 
 |archivedate=2008-10-02 
 |quote=At the inaugural Data Governance Conference in Orlando, Florida, in December 2006, leaders of successful data governance programs declared that in their experience, data governance is between 80 and 95 percent communication. Clearly, data governance is not a typical IT project. 
 |deadurl=yes 
 |df= 
}}&lt;/ref&gt; That stated, it is a given that many of the objectives of a Data Governance program must be accomplished with appropriate tools. Many vendors are now positioning their products as Data Governance tools; due to the different focus areas of various data governance initiatives, any given tool may or may not be appropriate, in addition, many tools that are not marketed as governance tools address governance needs.&lt;ref&gt;{{cite web
 |url=http://www.datagovernancesoftware.com 
 |title=DataGovernanceSoftware.com 
 |publisher=[[The Data Governance Institute]] 
 |accessdate=2008-10-02 
 |archiveurl=http://www.webcitation.org/5bGI3dfHV?url=http://www.datagovernancesoftware.com/ 
 |archivedate=2008-10-02 
 |quote= 
 |deadurl=yes 
 |df= 
}}&lt;/ref&gt;

== Data governance organizations ==
;DAMA International&lt;ref&gt;[http://www.dama.org/i4a/pages/index.cfm?pageid=1 DAMA International]&lt;/ref&gt;
:[[DAMA]] (the Data Management Association) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and data resource management (DRM).

;Data Governance Professionals Organization (DGPO)&lt;ref&gt;
[http://www.dgpo.org/ Data Governance Professionals Organization&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
:The Data Governance Professionals Organization (DGPO) is a non-profit, vendor neutral, association of business, IT and data professionals dedicated to advancing the discipline of data governance.  The objective of the DGPO is to provide a forum that fosters discussion and networking for members and to encourage, develop and advance the skills of members working in the data governance discipline.

;The Data Governance Society &lt;ref&gt;[http://www.datagovernancesociety.org Data Governance Society]&lt;/ref&gt;
:The Data Governance Society, Inc. is dedicated to fostering a new paradigm for the effective use and protection of information in which Data is governed and leveraged as a unique corporate asset.

;The Data Governance Council &lt;ref&gt;[https://www-935.ibm.com/services/uk/cio/pdf/leverage_wp_data_gov_council_maturity_model.pdf Data Governance Council]&lt;/ref&gt; 
:The Data Governance Council is an organization formed by IBM consisting of companies, institutions and technology solution providers with the stated objective to build consistency and quality control in governance, which will help companies better protect critical data."

;IQ International -- the International Association for Information and Data Quality&lt;ref&gt;[http://iaidq.org/ IQ International, the International Association for Information and Data Quality]&lt;/ref&gt;
:IQ International is a not-for-profit, vendor neutral, professional association formed in 2004, dedicated to building the information and data quality profession.

== Data governance conferences ==
A number of major conferences relevant to data governance are held annually:
;Data Governance and Information Quality Conference&lt;ref&gt;[http://dgiq-conference.com/ Data Governance and Information Quality Conference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 
:Commercial conferences held each year in the USA

;Data Governance Conference Europe,&lt;ref&gt;[http://www.irmuk.co.uk/ Data Governance Conference Europe&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
:Commercial conferences held annually in London, England .
;Information and Data Quality Conference&lt;ref&gt;[http://idq-conference.com/ Information and Data Quality Conference]&lt;/ref&gt;
:Not for profit conference run by IQ International in the USA
;Master Data Management &amp; Data Governance Conferences&lt;ref&gt;[http://www.tcdii.com/events/cdimdmsummitseries.html MDM SUMMIT Conference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 
:Six major conferences are run annually by the MDM Institute in London, San Francisco, Sydney, Toronto, Madrid, Frankfurt, and New York City.
;Financial Information Summit series of conferences&lt;ref&gt;[http://www.financialinformationsummit.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 
;Hosted by Inside Reference Data magazine in New York, London, Hong Kong, Toronto, Chicago, Frankfurt, Paris and Tokyo.

==See also==
* [[Information Architecture]]
* [[Information technology governance]]
* [[Semantics of Business Vocabulary and Business Rules]]
* [[Master data management]]
* [[COBIT]]
* [[ISO/IEC 38500]]
* [[ISO/TC 215]]
* [[Operational risk management]]
* [[Basel II Accord]]
* [[HIPAA]]
* [[Sarbanes-Oxley Act]]
* [[Information technology controls]]
* [[Data Protection Directive]] (EU)
* [[Universal Data Element Framework]]
* [[Asset Description Metadata Schema]]

==References==
&lt;!--to cite a web resource, use this template
&lt;ref&gt;{{cite web
  | url = MANDATORY
  | title = MANDATORY
  | last =
  | first =
  | authorlink =
  | coauthors =
  | work =
  | publisher =
  | date =
  | format =
  | language=
  | doi =
  | accessdate =  
  | archiveurl = SHOULD BE USED ON PAGES ALLOWING ARCHIVING - USE A SERVICE LIKE webcitation.org or archive.org
  | archivedate = MANDATORY IF archiveurl
  | quote = 
 }}&lt;/ref&gt;
--&gt;
{{reflist}}

[[Category:Information technology governance]]
[[Category:Data management]]</text>
      <sha1>akoh8z7astqpjvcww56lgcnc4zx685g</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data modeling</title>
    <ns>14</ns>
    <id>1116481</id>
    <revision>
      <id>761378463</id>
      <parentid>728513127</parentid>
      <timestamp>2017-01-22T17:37:49Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* top */Cleaning up... using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="417" xml:space="preserve">{{Commons cat|Data modeling}}
In information system design, '''[[data modeling]]''' is the analysis and design of the information in the system, concentrating on the logical entities and the logical dependencies between these entities

{{catdiffuse}}

&lt;!--  --&gt;

[[Category:Computer-aided software engineering tools]]
[[Category:Data management|Modeling]]
[[Category:Scientific modeling]]
[[Category:Software design]]</text>
      <sha1>3xkpenlemii03c7usxmw2pg9zq68uht</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data-centric programming languages</title>
    <ns>14</ns>
    <id>925067</id>
    <revision>
      <id>547353776</id>
      <parentid>459422864</parentid>
      <timestamp>2013-03-28T00:10:45Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363819]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="318" xml:space="preserve">{{Cat main|Data-centric programming language}}

This [[Wikipedia:Category|category]] lists those [[programming languages]] that are data-centric, with significant built-in functionality for data storage and manipulation.

[[Category:Data management|Programming languages]]
[[Category:Persistent programming languages]]</text>
      <sha1>pnfo0y5f2cia3c1j913fot0np0xcsjn</sha1>
    </revision>
  </page>
  <page>
    <title>Virtual facility</title>
    <ns>0</ns>
    <id>13444144</id>
    <revision>
      <id>650168862</id>
      <parentid>597578169</parentid>
      <timestamp>2015-03-06T17:13:15Z</timestamp>
      <contributor>
        <username>Fraulein451</username>
        <id>16206019</id>
      </contributor>
      <comment>/* References */ added reflist</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3277" xml:space="preserve">{{Advert|article|date=May 2013}}
[[Image:VirtualFacility.jpg|thumb|250px|A Virtual Facility snapshot created with 6SigmaDC software.|right]]
A '''Virtual Facility''' (VF) is a highly realistic digital representation of a [[data center]] (primarily). The term virtual in Virtual Facility refers to the use of the word as in [[Virtual reality|Virtual Reality]] rather than the abstraction of computer resources as in [[platform virtualization]]. The VF mirrors the characteristics of the physical facility over time and allows modeling all relevant characteristics of a physical data center with a high degree of precision. 

==VF Model includes==

* Three-dimensional physical facility layout
* Network connectivity of facility equipment
* Full inventory of facility equipment, including electronics and electrical systems such as [[Power distribution unit|Power Distribution Units]] (PDU&#8217;s) and [[Uninterruptible power supplies|Uninterruptible Power Supplies]] (UPS&#8217;s)
* Full air conditioning system (ACU&#8217;s) and controls within the room

The term Virtual Facility was introduced by Future Facilities, a data centre design consultancy focused on delivering Design and Operational solutions to address the emerging environmental problems facing the modern Mission Critical Facility (MCF). The concept is in essence a convergence of the fields of [[Virtual reality|Virtual Reality]] (VR), [[Computer simulation|Computer Simulation]] and [[Expert systems|Expert Systems]], applied to the specific domain of facilities.

The VF type of computer simulation allows detailed analysis and prototyping of air flow in the data center by making use of [[Computational fluid dynamics|Computational Fluid Dynamics]] (CFD) techniques. This in turn allows the air flow and temperatures of the facility to be analyzed visually ([[Scientific visualization|Scientific Visualisation]]) and numerically to study and predict what will happen in the real facility. The importance of scientific methods in design of mission critical facilities has become a necessity, since the performance gains predicted by [[Moore&#8217;s law|Moore's Law]] go hand in hand with a rise in power and heat dissipated by equipment. Rules of thumb have proven to be no longer adequate.

==VF design purposes==

* Green field design
* Asset management
* Troubleshooting existing data centers
* Making existing data centers more resilient
* Making existing data centers more energy efficient
* Cost prediction
* Staff training
* Capacity planning
* Load growth management
 
The VF is now being employed by many large organizations as a way of virtually assessing a situation before having to spend huge sums of money trying to solve a problem in the real facility.

It is essential to know whether adding new equipment or changing equipment will cause a logistical or thermal problem.  The VF allows the designer or operator to assess the best course of action and gives in depth understanding on unintuive behaviours.

==References==
{{reflist}}
* {{Citation
  | last = Seymour
  | first = Mark
  | title = Virtual Data Centre Design. A blueprint for success
  | url=http://www.futurefacilities.com/newsarticles/articles/commerzbankarticlesummerZDTjournal.pdf
  | accessdate = 2007-09-26}}

[[Category:Data management]]</text>
      <sha1>b2iwp7wbtpckn3dvms3k82vdhjxd55i</sha1>
    </revision>
  </page>
  <page>
    <title>Cognos ReportNet</title>
    <ns>0</ns>
    <id>16942788</id>
    <revision>
      <id>754691302</id>
      <parentid>751717308</parentid>
      <timestamp>2016-12-14T00:10:13Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>punct using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6185" xml:space="preserve">{{Infobox Software
| name = Cognos ReportNet
| logo = 
| screenshot = 
| caption = 
| author = [[Cognos]], an [[IBM]] Company
| developer = 
| released = September, 2003
| latest release version = Cognos ReportNet 1.3
| latest release date = 
| latest preview version = 
| latest preview date = 
| operating system = Multiple
| platform = Multiple
| language = Multi-lingual
| status = '''Inactive'''&lt;ref&gt;[http://www-01.ibm.com/software/analytics/cognos/products/reportnet/ IBM.com, ''Cognos ReportNet : now part of Cognos Enterprise'', consult&#233; le 12 f&#233;vrier 2014]&lt;/ref&gt;
| genre = [[Business Intelligence]]
| license = 
| website = [http://www-01.ibm.com/software/data/cognos/products/reportnet/ IBM.com]
}}

'''Cognos ReportNet (CRN)''' is a web-based [[software]] product for creating and managing [[ad hoc]] and custom-made reports. ReportNet is developed by the [[Ottawa]]-based  company [[Cognos]] (formerly Cognos Incorporated), an [[IBM]] company. The web-based reporting tool was launched in September 2003. Since IBM's acquisition of Cognos, ReportNet has been renamed ''IBM Cognos ReportNet'' like all other Cognos products.

ReportNet uses web services standards such as [[XML]] and [[Simple Object Access Protocol]] and also supports dynamic [[HTML]] and [[Java (programming language)|Java]].&lt;ref&gt;[https://web.archive.org/web/20080312025955/http://www.vnunet.com/vnunet/news/2123232/bear-sterns-chooses-cognos-reportnet Cognos ReportNet in news]&lt;/ref&gt; ReportNet is compatible with multiple databases including [[Oracle Database|Oracle]], [[SAP AG|SAP]], [[Teradata]], [[Microsoft SQL server]], [[IBM DB2|DB2]] and [[Sybase]].&lt;ref&gt;[http://www.cognos.com/solutions/data/ibm/advantages.html Data sources]&lt;/ref&gt;&lt;ref&gt;[http://support.cognos.com/en/support/products/crn101_software_environments.html CRN Environment details]&lt;/ref&gt; The product provides interface in over 10 languages,&lt;ref&gt;[http://www.cognos.com/products/business_intelligence/reporting/features.html CRN Features]&lt;/ref&gt; has Web Services architecture to meet the needs of multi-national, diversified enterprises and helps reduce total cost of ownership. Multiple versions of Cognos ReportNet have since been released by the company. Cognos ReportNet was awarded the [[Software and Information Industry Association]] (SIIA) 2005 [[Codie Awards]] for the "Best Business Intelligence or Knowledge Management Solution" category.&lt;ref&gt;[http://www.mywire.com/pubs/PRNewswire/2005/06/08/885642?extID=10051 Cognos ReportNet wins award]&lt;/ref&gt; CRN's capabilities have been further used in [[IBM Cognos 8 Business Intelligence|IBM Cognos 8 BI (2005)]], the latest reporting tool.&lt;ref&gt;[http://www.cognos.com/products/cognos8businessintelligence Cognos 8 BI]&lt;/ref&gt; CRN comes with its own [[software development kit]] (SDK).

==Launch==
Early adopters of Cognos ReportNet for their corporate reporting needs included [[Bear Stearns]], [[BMW]] and [[Alfred Publishing]]. Around this same time of launch, Cognos competitor [[Business Objects]] released version 6.1 of its enterprise reporting tool. Cognos ReportNet has been successful since its launch, raising revenues in 2004 from licensing fees.&lt;ref&gt;[http://www.highbeam.com/doc/1G1-131525446.html Cognos ReportNet delivers $30 Million in License Revenue in one Quarter]&lt;/ref&gt; Subsequently, other major corporations like [[McDonald's]] adopted Cognos ReportNet.&lt;ref&gt;[http://www.ebizq.net/news/5538.html ReportNet and fries]&lt;/ref&gt;

==Controversy==
Cognos rival [[Business Objects (company)|Business Objects]] announced in 2005 that BusinessObjects XI significantly outperformed Cognos ReportNet in benchmark tests conducted by VeriTest, an independent software testing firm. The tests performed showed Cognos ReportNet performed poorly when processing styled reports, complex business reports and combination of both.&lt;ref&gt;[http://www.crm2day.com/news/crm/114773.php BO XI Vs Cognos ReportNet]&lt;/ref&gt; The tests reported a massive 21 times higher report throughput for BusinessObjects XI than Cognos ReportNet at capacity loads.&lt;ref&gt;[http://goliath.ecnext.com/coms2/summary_0199-4404821_ITM BO XI outperforms Cognos ReportNet]&lt;/ref&gt; Cognos soon dismissed the claims by stating Business Objects dictated the environment and testing criteria and Cognos did not provide the software to participate in benchmark test.&lt;ref&gt;[http://www.cognos.com/news/releases/2005/0624_3.html Cognos dismisses the Test results]&lt;/ref&gt; Cognos later performed their own test to demonstrate Cognos ReportNet capabilities.&lt;ref&gt;[http://www.cognos.com/pdfs/whitepapers/wp_cognos_reportnet_scalability_benchmakrs_ms_windows.pdf Cognos scalability results]&lt;/ref&gt;

==Components==
* Cognos Report Studio &#8211; A Web-based product for creating complex professional looking reports.&lt;ref&gt;[http://web.princeton.edu/sites/datamall/documents/ug_cr_rptstd.pdf Refer definition in introduction page]&lt;/ref&gt;
* Cognos Query Studio - A Web-based product for creating ad-hoc reports.&lt;ref&gt;[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer Introduction page]&lt;/ref&gt;
* Cognos Framework Manager &#8211; A [[metadata modeling]] tool to create BI metadata for reporting and dashboard applications.&lt;ref&gt;[http://www.cognos.com/products/framework_services Framework Manager Services] {{webarchive |url=https://web.archive.org/web/20080417030129/http://www.cognos.com/products/framework_services |date=April 17, 2008 }}&lt;/ref&gt;
* Cognos Connection &#8211; Main [[Enterprise portal|portal]] used to access reports, schedule reports and perform administrator activities.&lt;ref&gt;[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer page9]&lt;/ref&gt;

==Versions==
* Cognos ReportNet 1.1 &#8211; [[Java EE]]-style professional web-based authoring tool. (base version)
* Cognos ReportNet IBM Special Edition &#8211; comes with an embedded version of [[IBM WebSphere]] as its application server and [[IBM DB2]] as its data store.
* Cognos Linux &#8211; for Intel-based [[Linux]] platforms.&lt;ref&gt;[http://www.ebizq.net/news/5688.html ReportNet on Linux]&lt;/ref&gt;

==See also==
*[[IBM Cognos Business Intelligence]]

==References==
{{reflist|30em}}

[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:IBM software]]</text>
      <sha1>5qxneybl8ewy2g5p1b6i0bo36mntnrm</sha1>
    </revision>
  </page>
  <page>
    <title>Long-lived transaction</title>
    <ns>0</ns>
    <id>17866900</id>
    <revision>
      <id>721205323</id>
      <parentid>687016731</parentid>
      <timestamp>2016-05-20T09:48:43Z</timestamp>
      <contributor>
        <username>R'n'B</username>
        <id>2300502</id>
      </contributor>
      <minor />
      <comment>Disambiguating links to [[Versioning]] (link changed to [[Version control]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1596" xml:space="preserve">{{Multiple issues|
{{Orphan|date=February 2009}}
{{No sources|date=October 2015}}
}}

A '''long-lived transaction''' is a [[Database transaction|transaction]] that spans multiple database transactions. The transaction is considered "long-lived" because its boundaries must, by necessity of business logic, extend past a single database transaction. A long-lived transaction can be thought of as a sequence of database transactions grouped to achieve a single atomic result.

A common example is a multi-step sequence of requests and responses of an interaction with a user through a web client.

A long-lived transaction creates challenges of [[concurrency control]] and [[scalability]].

A chief strategy in designing long-lived transactions is [[optimistic concurrency control]] with [[Version control|versioning]].

So much research work related to these long lived transactions was carried out by several professors from the Oxford University and Michigan State University and the Central University of Hyderabad. Dr. James from the Oxford University created several hypotheses for long-lived transactions. Dr Copperfield of the Michigan State University was regarded highly for his contributions in this field. Dr A B Sagar of Hyderabad Central University has also done very creative work in relating long-lived transactions with financial transactions in Microfinance.

However the study is not complete and is still open to challenges and research issues.

==See also==
*[[Long-running transaction]]

[[Category:Data management]]
[[Category:Transaction processing]]


{{software-eng-stub}}</text>
      <sha1>sqm2cvhumht8rgrrscfgeiz0aji8z0j</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Semantic Web</title>
    <ns>14</ns>
    <id>18014783</id>
    <revision>
      <id>601308308</id>
      <parentid>594005753</parentid>
      <timestamp>2014-03-26T05:41:52Z</timestamp>
      <contributor>
        <ip>99.65.176.161</ip>
      </contributor>
      <comment>del cat WWW.  Internet ages is a top level cat and having pages in both top level and one level down is redundant.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="244" xml:space="preserve">{{Cat main|Semantic Web}}
{{Commons cat|Semantic Web}}

[[Category:Internet ages|Web 3]]
[[Category:World Wide Web Consortium]] &lt;!-- the Semantic Web is a major W3C activity --&gt;
[[Category:Knowledge representation]]
[[Category:Data management]]</text>
      <sha1>mkcln7p7q7j6y333y8creg62oizg40t</sha1>
    </revision>
  </page>
  <page>
    <title>Vector-field consistency</title>
    <ns>0</ns>
    <id>18477184</id>
    <revision>
      <id>724046458</id>
      <parentid>623353624</parentid>
      <timestamp>2016-06-06T20:26:53Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2897" xml:space="preserve">'''Vector-Field Consistency'''&lt;ref group="nb"&gt;&lt;sub&gt;Designation coined by L. Veiga.&lt;/sub&gt;&lt;/ref&gt; is a [[consistency model]] for replicated data (for example, objects), initially described in a paper&lt;ref&gt;{{cite conference |author1=Nuno Santos |author2=Lu&#237;s Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~pjpf/middleware07vector.pdf | format=PDF}}&lt;/ref&gt; which was awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007. It has since been enhanced for increased scalability and fault-tolerance in a recent paper.&lt;ref&gt;{{cite conference |author1=Lu&#237;s Veiga |author2=Andr&#233; Negr&#227;o |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency 
| booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}&lt;/ref&gt;

== Description ==
This consistency model was initially designed for replicated [[data management]] in adhoc gaming in order to minimize bandwidth usage without sacrificing playability. Intuitively, it captures the notion that although players require, wish, and take advantage of information regarding the whole of the game world (as opposed to a restricted view to rooms, arenas, etc. of limited size employed in many [[multiplayer game]]s), they need to know information with greater freshness, frequency, and accuracy as other game entities are located closer and closer to the player's position.

It prescribes a multidimensional divergence bounding scheme, based on a [[vector field]] that employs consistency vectors k=(&#952;,&#963;,&#957;), standing for maximum allowed '''t'''ime - or replica staleness, '''s'''equence - or missing updates, and '''v'''alue&lt;ref group="nb"&gt;&lt;sub&gt;Since in the [[Greek alphabet]] there was no letter for the ''vee'' sound, the ''nu'' letter was preferred for its resemblance with the roman V, for ''v''alue, instead of &#946; (''beta'') for the ''vee'' sound in contemporary Greek speaking.&lt;/sub&gt;&lt;/ref&gt; - or user-defined measured replica divergence, applied to all space coordinates in game scenario or world.

The consistency vector-fields emanate from field-generators designated as pivots (for example, players) and [[Field strength|field intensity]] attenuates as distance grows from these pivots in concentric or square-like regions. This consistency model unifies locality-awareness techniques employed in message routing and consistency enforcement for multiplayer games, with divergence bounding techniques traditionally employed in replicated database and web scenarios.

== Notes ==
&lt;references group="nb"/&gt;

== References ==
&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>ts2a7eg5n1qp06glx07kez2aws2a4za</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise data management</title>
    <ns>0</ns>
    <id>2654483</id>
    <revision>
      <id>672129974</id>
      <parentid>672105695</parentid>
      <timestamp>2015-07-19T13:39:14Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <comment>Added {{[[Template:notability|notability]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5845" xml:space="preserve">{{notability|date=July 2015}}
{{No footnotes|date=November 2009}}
'''Enterprise Data Management''' ('''EDM''') is:

#A concept &#8211; referring to the ability of an organization to precisely define, easily integrate and effectively retrieve data for both internal applications and external communication.
#A business objective &#8211; focused on the creation of accurate, consistent and transparent content.  EDM emphasizes data precision, granularity and meaning and is concerned with how the content is integrated into [[business application]]s as well as how it is passed along from one [[business process]] to another.

EDM arose to address circumstances where users within organizations independently source, model, manage and [[Computer data storage|store data]].  Uncoordinated approaches by various segments of the organization can result in [[data conflict]]s and quality inconsistencies &#8211; lowering the trustworthiness of the data as it is used for operations and [[Business reporting|reporting]]. 

The goal of EDM is trust and confidence in data assets.  Its components are:

==Strategy and governance==
EDM requires a strategic approach to choosing the right processes, technologies and resources (i.e. data owners, governance, stewardship, [[data analyst]]s and [[data architect]]s).  EDM is a challenge for organizations because it requires alignment among multiple stakeholders (including IT, operations, finance, strategy and [[end-user]]s) and relates to an area (creation and use of common data) that has not traditionally had a clear &#8220;owner.&#8221;  

The governance challenge can be a big obstacle to the implementation of an effective EDM because of the difficulties associated with providing a [[business case]] on the benefits of data management.  The core of the challenge is due to the fact that data quality has no intrinsic value.  It is an enabler of other processes and the true benefits of effective data management are systematic and intertwined with other processes.  This makes it hard to quantify all the downstream implications or upstream improvements.  

The difficulties associated with quantification of EDM benefits translate into challenges with the positioning of EDM as an organizational priority.  Achieving organizational alignment on the importance of data management (as well as managing data as an ongoing area of focus) is the domain of [[governance]].  In recent years the establishment of an EDM and the EDM governance practice has become commonplace despite these difficulties. 

==Program implementation==
Implementation of an EDM program encompasses many processes &#8211; all of which need to be coordinated throughout the organization and managed while maintaining operational continuity.  Below are some of the major components of EDM implementation that should be given consideration:

===Stakeholder requirements===
EDM requires alignment among multiple stakeholders (at the right level of authority) who all need to understand and support the EDM objectives.  EDM begins with a thorough understanding of the requirements of the end users and the organization as a whole.  Managing stakeholder requirements is a critical, and ongoing, process based in an understanding of [[workflow]], data dependencies and the tolerance of the organization for operational disruption.  Many organizations use formal processes such as [[service level agreement]]s to specify requirements and establish EDM program objectives.

===Policies and procedures===
Effective EDM usually includes the creation, documentation and enforcement of operating policies and procedures associated with [[change management]], (i.e. [[data model]], business [[glossary]], master data shared domains, [[data cleansing]] and [[data normalization|normalization]]), data [[stewardship]], security constraints and dependency rules.  In many cases, these policies and procedures are documented for the first time as part of the EDM initiative.

===Data definitions and tagging===

One of the core challenges associated with EDM is the ability to compare data that is obtained from multiple internal and external sources.  In many circumstances, these sources use inconsistent terms and definitions to describe the data content itself &#8211; making it hard to compare data, hard to automate business processes, hard to feed complex applications and hard to exchange data.  This frequently results in a difficult process of [[data mapping]] and cross-referencing.  Normalization of all the terms and definitions at the data attribute level is referred to as the [[metadata]] component of EDM and is an essential prerequisite for effective data management.

===Platform requirements===

Even though EDM is fundamentally a data content challenge, there is a core technology dimension that must be addressed.  Organizations need to have a functional storage platform, a comprehensive data model and a robust messaging infrastructure.  They must be able to integrate data into applications and deal with the challenges of the existing (i.e. legacy) technology infrastructure.  Building the platform or partnering with an established technology provider on how the data gets stored and integrated into business applications is an essential component of the EDM process.

Enterprise data management as an essential business requirement has emerged as a priority for many organizations.  The objective is confidence and trust in data as the glue that holds business strategy together.

==See also==
* [[Master data management]]
* [[Master Data]]

==References==
{{Reflist}}

;General
* Enterprise Data Management Council http://www.edmcouncil.org
* [http://www.thegoldensource.com/files/EDM_Finextra_Report_Final.pdf Issues in Enterprise Data Management: A Survey Report, 12/06]

[[Category:Data management]]
[[Category:Product lifecycle management]]</text>
      <sha1>0bzmrtfnp92am5fbv7d09214eadn3zc</sha1>
    </revision>
  </page>
  <page>
    <title>Data exchange</title>
    <ns>0</ns>
    <id>10231058</id>
    <revision>
      <id>748877729</id>
      <parentid>748877673</parentid>
      <timestamp>2016-11-10T22:44:03Z</timestamp>
      <contributor>
        <username>Quercus solaris</username>
        <id>7034620</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10778" xml:space="preserve">{{more footnotes|date=February 2014}}
'''Data exchange''' is the process of taking [[data]] structured under a ''source'' [[Database schema|schema]] and transforming it into data structured under a ''target'' schema, so that the target data is an accurate representation of the source data.&lt;ref&gt;A. Doan, A. Halevy, and Z. Ives. "Principles of data integration", Morgan Kaufmann, 2012 pp. 276&lt;/ref&gt; Data exchange allows data to be [[cross-platform|shared between]] different [[computer program]]s. It is similar to the related concept of [[data integration]] except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an [[Instance (computer science)|instance]] given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.

== Single-domain data exchange ==

Often there are a few dozen different source and target schema (proprietary data formats) in some specific domain.
Often people develop an '''exchange format''' or '''interchange format''' for some single domain, and then write a few dozen different routines to (indirectly) translate each and every source schema to each and every target schema by using the interchange format as an intermediate step.
That requires a lot less work than writing and debugging the hundreds of different routines that would be required to directly translate each and every source schema directly to each and every target schema.
(For example,
[[Standard Interchange Format]] for geospatial data,
[[Data Interchange Format]] for spreadsheet data,
[[GPS eXchange Format]] or [[Keyhole Markup Language]] for indicating GPS coordinates on the globe,
[[Quicken Interchange Format]] for financial data,
[[GDSII]] for integrated circuit layout,
etc.){{Citation needed|date=September 2016}}

== Data exchange languages == &lt;!-- redirect target, if you change this, fix the redirect, too! --&gt;
{{merge to|Modeling language|date=May 2016}}
A data exchange language{{citation needed|date=May 2016}} is a language that is domain-independent and can be used for any kind of data. Its semantic expression capabilities and qualities are largely determined by comparison with the capabilities of natural languages. The term is also applied to any [[file format]] that can be read by more than one program, including proprietary formats such as [[Microsoft Office]] documents. However, a file format is not a real language as it lacks a grammar and vocabulary.

Practice has shown that certain types of [[formal language]]s are better suited for this task than others, since their specification is driven by a formal process instead of a particular software implementation needs. For example, [[XML]] is a [[markup language]] that was designed to enable the creation of dialects (the definition of domain-specific sublanguages) and a popular choice now in particular on the internet. However, it does not  contain domain specific dictionaries or fact types. Beneficial to a reliable data exchange is the availability of standard dictionaries-taxonomies and tools libraries such as [[parser]]s, schema [[validator]]s and transformation tools.{{Citation needed|date=September 2016}}

=== Popular languages used for data exchange ===
The following is a partial list of popular generic languages used for data exchange in multiple domains.

&lt;!-- this currently is very rough and ad-hoc - feel free to extend and change it! --&gt;
&lt;!-- Please verify definitions for the column headers of the table! --&gt;
{| class="wikitable sortable" style="font-size: 85%; text-align: center; width: auto;"
!
! Schemas
! Flexible
! Semantic verification
! Dictionary
! Information Model
! Synonyms and homonyms
! Dialecting
! Web standard
! Transformations
! Lightweight
! Human readable
! Compatibility
|-
| {{rh}}|[[Resource Description Framework|RDF]]
| {{yes}}{{Ref label|feat-rdf|1}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{partial}}
| Subset of [[Semantic web]]
|-
| {{rh}}|[[XML]]
| {{yes}}{{Ref label|feat-schema|1}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| subset of [[SGML]], [[HTML]]
|-
| {{rh}}|[[Atom (file format)|Atom]]
| {{yes}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| [[XML]] dialect
|-
| {{rh}}|[[JSON]]
| {{no}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| subset of [[YAML]]
|-
| {{rh}}|[[YAML]]
| {{no}}{{Ref label|feat-ext|2}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{no}}
| {{no}}
| {{no}}{{Ref label|feat-ext|2}}
| {{yes}}
| {{yes}}{{Ref label|feat-yaml-readable|3}}
| superset of [[JSON]]
|-
| {{rh}}|[[REBOL]]
| {{yes}}{{Ref label|feat-rebol-parse|6}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}{{Ref label|feat-rebol-parse|6}}
| {{yes}}
| {{yes}}{{Ref label|feat-rebol-readable|4}}
| 
|-
| {{rh}}|[[Gellish]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}{{Ref label|feat-gellish-dict|7}}
| {{no}}
| {{yes}}
| {{yes}}
| ISO
| {{no}}
| {{yes}}
| {{partial}}{{Ref label|feat-gellish-readable|5}}
| SQL, RDF/XML, OWL
|}

'''Nomenclature'''
* Schemas - Whether the language definition is available in a computer interpretable form.
* Flexible - Whether the language enables extension of the semantic expression capabilities without modifying the schema.
* Semantic verification - Whether the language definition enables semantic verification of the correctness of expressions in the language.
* Dictionary-Taxonomy - Whether the language includes a dictionary and a taxonomy (subtype-supertype hierarchy) of concepts with inheritance.
* Synonyms and homonyms - Whether the language includes and supports the use of synonyms and homonyms in the expressions.
* Dialecting - Whether the language definition is available in multiple natural languages or dialects.
* Web or ISO standard - Organization that endorsed the language as a standard.
* Transformations - Whether the language includes a translation to other standards.
* Lightweight - Whether a lightweight version is available, in addition to a full version.
* Human readable - Whether expressions in the language are [[human-readable]]&#8212;readable by humans without training.{{Citation needed|date=September 2016}}
* Compatibility - Which other tools are possible or required when using the language.{{Citation needed|date=September 2016}}

'''Notes:'''

# {{note|feat-rdf}} RDF is a schema flexible language.
# {{note|feat-schema}} The schema of XML contains a very limited grammar and vocabulary.
# {{note|feat-ext}} Available as extension.
# {{note|feat-yaml-readable}} in the default format, not the compact syntax.
# {{note|feat-rebol-readable}} the syntax is fairly simple (the language was designed to be human readable); the dialects may require domain knowledge.
# {{note|feat-gellish-readable}} the standardized fact types are denoted by standardized English phrases, which interpretation and use needs some training.
# {{note|feat-rebol-parse}} the [[REBOL#parse|Parse dialect]] is used to specify, validate, and transform dialects.
# {{note|feat-gellish-dict}} the English version includes a Gellish English Dictionary-Taxonomy that also includes standardized fact types (= kinds of relations).

=== XML for data exchange ===
The popularity of [[XML]] for data exchange on the [[World Wide Web]] has several reasons. First of all, it is closely related to the preexisting standards [[Standard Generalized Markup Language]] (SGML) and [[Hypertext Markup Language]] (HTML), and as such a parser written to support these two languages can be easily extended to support XML as well. For example, [[XHTML]] has been defined as a format that is formal XML, but understood correctly by most (if not all) HTML parsers. This led to quick adoption of XML support in web browsers and the toolchains used for generating web pages.{{Citation needed|date=September 2016}}

=== YAML for data exchange ===
[[YAML]] is a language that was designed to be human-readable (and as such to be easy to edit with any standard text editor). Its notion often is similar to [[reStructuredText]] or a Wiki syntax, who also try to be readable both by humans and computers. YAML 1.2 also includes a shorthand notion that is compatible with JSON, and as such any JSON document is also valid YAML; this however does not hold the other way.{{Citation needed|date=September 2016}}

=== REBOL for data exchange ===

[[REBOL]] is a language that was designed to be human-readable and easy to edit using any standard text editor. To achieve that it uses a simple free-form syntax with minimal punctuation, and a rich set of datatypes. REBOL datatypes like URLs, e-mails, date and time values, tuples, strings, tags, etc. respect the common standards. REBOL is designed to not need any additional meta-language, being designed in a metacircular fashion. The metacircularity of the language is the reason why e.g. the Parse dialect used (not exclusively) for definitions and transformations of REBOL dialects is also itself a dialect of REBOL. REBOL was used as a source of inspiration by the designer of JSON.{{Citation needed|date=September 2016}}

=== Gellish for data exchange ===
[[Gellish English]] is a formalized subset of natural English, which includes a simple grammar and a large extensible [[English dictionary|English Dictionary-Taxonomy]] that defines the general and domain specific terminology (terms for concepts), whereas the concepts are arranged in a subtype-supertype hierarchy (a Taxonomy), which supports inheritance of knowledge and requirements. The Dictionary-Taxonomy also includes standardized fact types (also called relation types). The terms and relation types together can be used to create and interpret expressions of facts, knowledge, requirements and other information. Gellish can be used in combination with [[SQL]], [[RDF/XML]], [[Web Ontology Language|OWL]] and various other meta-languages. The Gellish standard is being adopted as ISO 15926-11.{{Citation needed|date=September 2016}}

== See also ==
* [[Atom (file format)]]
* [[Lightweight markup language]]
* [[RSS]]

== References ==

{{reflist}}

{{refbegin}}

*R. Fagin, P. Kolaitis, R. Miller, and L. Popa. "Data exchange: semantics and query answering." Theoretical Computer Science, 336(1):89&#8211;124, 2005.
*P. Kolaitis. "Schema mappings, data exchange, and metadata management." Proceedings of the twenty- fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 61&#8211;75, 2005
{{refend}}

{{Data Exchange}}

[[Category:Data management]]</text>
      <sha1>gbnjtk7ij0h7y1vcfaj0fm2apzi9gs8</sha1>
    </revision>
  </page>
  <page>
    <title>UI data binding</title>
    <ns>0</ns>
    <id>18644154</id>
    <revision>
      <id>753190483</id>
      <parentid>753187205</parentid>
      <timestamp>2016-12-05T18:45:26Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Cn}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2309" xml:space="preserve">{{Refimprove|date=February 2015}}

'''UI data binding''' is a [[Design pattern (computer science)|software design pattern]] to simplify development of [[GUI]] applications.  UI [[data binding]] binds UI elements to an application [[domain model]]. Most frameworks employ the [[Observer pattern]] as the underlying binding mechanism.  To work efficiently, UI data binding has to address [[Data validation|input validation]] and data type mapping.

A ''bound control'' is a [[GUI widget|widget]] whose value is tied or [[data binding|bound]] to a field in a [[recordset]] (e.g., a [[column (database)|column]] in a [[row (database)|row]] of a [[table (database)|table]]).  Changes made to data within the control are automatically saved to the database when the control's exit [[event trigger]]s.

== Data binding frameworks and tools ==

=== [[Embarcadero Delphi|Delphi]] ===
* [[DSharp (data binding)|DSharp]] 3rd party Data Binding tool{{cn|date=December 2016}}
* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd party Visual Data Binding tool

=== Java ===
* [[JFace]] Data Binding
* [[JavaFX]] Property&lt;ref&gt;https://docs.oracle.com/javafx/2/binding/jfxpub-binding.htm&lt;/ref&gt;

=== .NET ===
* [[Windows Forms]] data binding overview
* [[Windows Presentation Foundation|WPF]] data binding overview
* Unity 3D data binding framework (available in modifications for NGUI, iGUI and EZGUI libraries){{cn|date=December 2016}}

=== JavaScript ===
* [[AngularJS]]
* [[Backbone.js]]
* [[Ember.js]]
* Datum.js&lt;ref&gt;{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}&lt;/ref&gt;
* [[knockout.js]]
* [[Meteor (web framework)|Meteor]], via its ''Blaze'' live update engine&lt;ref&gt;{{cite web|title=Meteor Blaze|url=https://www.meteor.com/blaze|quote=Meteor Blaze is a powerful library for creating live-updating user interfaces. Blaze fulfills the same purpose as Angular, Backbone, Ember, React, Polymer, or Knockout, but is much easier to use. We built it because we thought that other libraries made user interface programming unnecessarily difficult and confusing.}}&lt;/ref&gt;
* [[OpenUI5]]
* [[React (JavaScript library)|React]]

==See also==
*[[Data binding]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Software design patterns]]


{{compu-prog-stub}}
{{database-stub}}</text>
      <sha1>1iptk4z55om8ou4ij93r3p24awfm7og</sha1>
    </revision>
  </page>
  <page>
    <title>Metadata controller</title>
    <ns>0</ns>
    <id>21423528</id>
    <revision>
      <id>732166611</id>
      <parentid>623254040</parentid>
      <timestamp>2016-07-30T02:33:43Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>about SAN</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="585" xml:space="preserve">'''Metadata controller''' (or MDC) is a [[storage area network]] (SAN) technology for managing [[file locking]], space allocation and data access authorization.
This is needed when several clients are given block level access to the same disk volume, [[Computer data storage|data storage]] sharing.

The abstract for the patent describing this technology can be read [http://www.freepatentsonline.com/7448077.html here]

[[Category:Data management]]
[[Category:Telecommunications engineering]]
[[Category:Storage area networks]]
[[Category:Local area networks]]

{{compu-storage-stub}}</text>
      <sha1>mte5e3v0huy7ajvpu96kg4rhb2q6p3b</sha1>
    </revision>
  </page>
  <page>
    <title>Parchive</title>
    <ns>0</ns>
    <id>526495</id>
    <revision>
      <id>757771215</id>
      <parentid>749546109</parentid>
      <timestamp>2017-01-01T17:14:55Z</timestamp>
      <contributor>
        <ip>70.36.223.208</ip>
      </contributor>
      <comment>Multipar dev now considers multipar.eu malicious, unauthorized changes https://www.livebusinesschat.com/smf/index.php?topic=6108.0</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18906" xml:space="preserve">{{Merge from|QuickPAR|date=March 2014}}

{{Infobox file format
| name = Parchive
| extension = .par, .par2, .p??, (.par3 future)
| mime =
| owner =
| creatorcode =
| genre = [[Erasure code]]
| containerfor =
| containedby =
| extendedfrom =
| extendedto =
}}

'''Parchive''' (a [[portmanteau]] of '''parity archive''', and formally known as '''Parity Volume Set Specification'''&lt;ref&gt;[https://www.livebusinesschat.com/smf/index.php?topic=5736.msg38234#msg38234 Re: Correction to Parchive on Wikipedia], reply #3, by Yutaka Sawada: "Their formal title are "Parity Volume Set Specification 1.0" and "Parity Volume Set Specification 2.0."&lt;/ref&gt;) is an [[erasure code]] system that produces '''par''' files for [[checksum]] verification of [[data integrity]], with the capability to perform [[data recovery]] operations that can repair or regenerate corrupted or missing data. 

Parchive was originally written to solve the problem of reliable file sharing on [[Usenet]],&lt;ref&gt;{{cite web
| url         = http://parchive.sourceforge.net/#desc
| title       = Parchive: Parity Archive Volume Set
| accessdate  = 2009-10-29
| quote       = The original idea behind this project was to provide a tool to apply the data-recovery capability concepts of RAID-like systems to the posting and recovery of multi-part archives on Usenet.
}}&lt;/ref&gt; but it is now commonly used for protecting any kind of data from [[data corruption]], [[disc rot]], [[data degradation|bit rot]], and accidental or malicious damage. Despite the name, Parchive uses more advanced techniques that do not utilize simplistic [[Parity bit|parity]] methods of [[error detection and correction]].

As of 2014, '''PAR1''' is obsolete, '''PAR2''' is mature for widespread use, and '''PAR3''' is an experimental version being developed by MultiPar author Yutaka Sawada.&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=5098.0 possibility of new PAR3 file]&lt;/ref&gt;&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=3339.0 Question about your usage of PAR3]&lt;/ref&gt;&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=5025.msg29912;topicseen#msg29912 Risk of undetectable intended modification]&lt;/ref&gt;&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=3527.msg8850;topicseen#msg8850 PAR3 specification proposal not finished as of April 2011]&lt;/ref&gt;  The original SourceForge Parchive project has been inactive since November 9, 2010.&lt;ref&gt;{{cite web |url = http://sourceforge.net/projects/parchive/ |title = Parchive: Parity Archive Tool |accessdate = 2012-09-02}}&lt;/ref&gt; 

== History ==
Parchive was intended to increase the reliability of transferring files via Usenet [[newsgroup]]s. Usenet was originally designed for informal conversations, and the underlying protocol, [[NNTP]] was not designed to transmit arbitrary binary data. Another limitation, which was acceptable for conversations but not for files, was that messages were normally fairly short in length and limited to 7-bit [[ASCII]] text.&lt;ref&gt;{{cite IETF
| title       = Network News Transfer Protocol
| rfc         = 977
| sectionname = Character Codes
| section     = 2.2
| page        = 5
| last1       = Kantor
| first1      = Brian
| authorlink1 =
| last2       = Lapsley
| first2      = Phil
| authorlink2 = Phil Lapsley
| year        = 1986
| month       = February
| publisher   = [[Internet Engineering Task Force|IETF]]
| accessdate  = 2009-10-29
}}&lt;/ref&gt;

Various techniques were devised to send files over Usenet, such as [[uuencode|uuencoding]] and [[Base64]]. Later Usenet software allowed  8 bit [[Extended ASCII]], which permitted new techniques like [[yEnc]]. Large files were broken up to reduce the effect of a corrupted download, but the unreliable nature of Usenet remained.

With the introduction of Parchive, parity files could be created that were then uploaded along with the original data files. If any of the data files were damaged or lost while being propagated between Usenet servers, users could download parity files and use them to reconstruct the damaged or missing files. Parchive included the construction of small index files (*.par in version 1 and *.par2 in version 2) that do not contain any recovery data. These indexes contain [[hash function|file hash]]es that can be used to quickly identify the target files and verify their integrity.

Because the index files were so small, they minimized the amount of extra data that had to be downloaded from Usenet to verify that the data files were all present and undamaged, or to determine how many parity volumes were required to repair any damage or reconstruct any missing files. They were most useful in version 1 where the parity volumes were much larger than the short index files. These larger parity volumes contain the actual recovery data along with a duplicate copy of the information in the index files (which allows them to be used on their own to verify the integrity of the data files if there is no small index file available).

In July 2001, Tobias Rieper and Stefan Wehlus proposed the Parity Volume Set specification, and with the assistance of other project members, version 1.0 of the specification was published in October 2001.&lt;ref&gt;{{cite web|url=http://sourceforge.net/docman/display_doc.php?docid=7273&amp;group_id=30568 |title=Parchive: Parity Volume Set specification 1.0 |accessdate=2009-04-07 |last=Nahas |first=Michael |date=2001-10-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20081220184024/http://sourceforge.net/docman/display_doc.php?docid=7273&amp;group_id=30568 |archivedate=December 20, 2008 }}&lt;/ref&gt; Par1 used [[Reed&#8211;Solomon error correction]] to create new recovery files. Any of the recovery files can be used to rebuild a missing file from an incomplete [[download]].

Version 1 became widely used on Usenet, but it did suffer some limitations:
* It was restricted to handle at most 255 files.
* The recovery files had to be the size of the largest input file, so it did not work well when the input files were of various sizes. (This limited its usefulness when not paired with the proprietary RAR compression tool.)
* The recovery algorithm had a bug, due to a flaw&lt;ref&gt;{{cite web
| url         = http://web.eecs.utk.edu/~plank/plank/papers/CS-03-504.html
| title       = Note: Correction to the 1997 Tutorial on Reed-Solomon Coding
| accessdate  = 2009-10-29
| last        = Plank
| first       = James S.
|author2=Ding, Ying
|date=April 2003
}}&lt;/ref&gt; in the academic paper&lt;ref&gt;{{cite web
| url         = http://web.eecs.utk.edu/~plank/plank/papers/SPE-9-97.html
| title       = A Tutorial on Reed-Solomon Coding for Fault-Tolerance in RAID-like Systems
| accessdate  = 2009-10-29
| last        = Plank
| first       = James S.
|date=September 1997
}}&lt;/ref&gt; on which it was based.
* It was strongly tied to Usenet and it was felt that a more general tool might have a wider audience.

In January 2002, Howard Fukada proposed that a new Par2 specification should be devised with the significant changes that data verification and repair should work on blocks of data rather than whole files, and that the algorithm should switch to using 16 bit numbers rather than the 8 bit numbers that PAR 1 used. Michael Nahas and Peter Clements took up these ideas in July 2002, with additional input from Paul Nettle and Ryan Gallagher (who both wrote Par1 clients). Version 2.0 of the Parchive specification was published by Michael Nahas in September 2002.&lt;ref&gt;{{cite web
| url         = http://parchive.sourceforge.net/docs/specifications/parity-volume-spec/article-spec.html
| title       = Parity Volume Set Specification 2.0
| accessdate  = 2009-10-29
| last        = Nahas
| first       = Michael |author2=Clements, Peter |author3=Nettle, Paul |author4=Gallagher, Ryan
| date        = 2003-05-11
}}&lt;/ref&gt;

Peter Clements then went on to write the first two Par2 implementations, [[QuickPar]] and par2cmdline. Abandoned since 2004, Paul Houle created phpar2 to supersede par2cmdline. Yutaka Sawada created MultiPar to supersede QuickPar. Sawada maintains par2cmdline to use as MultiPar's PAR engine backend.

On May 10, 2014, Sawada reported a hash collision security problem in par2cmdline (the backend for MultiPar):&lt;ref name="livebusinesschat.com"&gt;[https://www.livebusinesschat.com/smf/index.php?topic=5579.0 v1.2.5.3 is public]&lt;/ref&gt;

&lt;blockquote&gt;I'm not sure this problem can be used for DoS attack against automated Par2 usage. If someone has a skill to forge CRC-32, it is possible to make a set of source file and Par2 file, which freeze a Par2 client for several hours.&lt;/blockquote&gt;

== Versions ==
Versions 1 and 2 of the [[file format]] are incompatible. (However, many clients support both.)

=== Parity Volume Set Specification 1.0 ===
For Par1, the files ''f1'', ''f2'', ..., ''fn'', the Parchive consists of an index file (''f.par''), which is CRC type file with no recovery blocks, and a number of "parity volumes" (''f.p01'', ''f.p02'', etc.). Given all of the original files except for one (for example, ''f2''), it is possible to create the missing ''f2'' given all of the other original files and any one of the parity volumes. Alternatively, it is possible to recreate two missing files from any two of the parity volumes and so forth.&lt;ref&gt;{{cite book
| last        = Wang
| first       = Wallace
| authorlink  =
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&amp;pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 &#8211; 167
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}&lt;/ref&gt;

Par1 supports up to 256 recovery files. Each recovery file must be the size of the largest input file.

=== Parity Volume Set Specification 2.0 ===
Par2 files generally use this naming/extension system: ''filename.vol000+01.PAR2'', ''filename.vol001+02.PAR2'', ''filename.vol003+04.PAR2'', ''filename.vol007+06.PAR2'', etc. The +01, +02, etc. in the filename indicates how many blocks it contains, and the vol000, vol001, vol003 etc. indicates the number of the first recovery block within the PAR2 file. If an index file of a download states that 4 blocks are missing, the easiest way to repair the files would be by downloading ''filename.vol003+04.PAR2''. However, due to the redundancy, ''filename.vol007+06.PAR2'' is also acceptable. There is also an index file ''filename.PAR2'', it is identical in function to the small index file used in PAR1.

Par2 supports up to 65536 (2&lt;sup&gt;16&lt;/sup&gt;) recovery blocks (however, par2cmdline, the official PAR2 implementation, it limited to 32767 blocks at once). Input files are split into multiple equal-sized blocks so that recovery files do not need to be the size of the largest input file.

Although [[Unicode]] is mentioned in the PAR2 specification as an option, most PAR2 implementations do not support unicode.&lt;ref&gt;[http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 QuickPar forum posting] {{webarchive |url=https://web.archive.org/web/20120302104523/http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 |date=March 2, 2012 }}&lt;/ref&gt;

Directory support is included in the PAR2 specification, but most or all implementations do not support it.

=== Parity Volume Set Specification 3.0 ===
Par3 is a planned improvement over Par2.&lt;ref&gt;{{cite web|url=http://hp.vector.co.jp/authors/VA021385/|title=MultiPar announcement|publisher=}}&lt;/ref&gt;&lt;ref&gt;[http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 QuickPar forum posting&amp;nbsp;&#8211; status PAR3] {{webarchive |url=https://web.archive.org/web/20101127125317/http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 |date=November 27, 2010 }}&lt;/ref&gt;&lt;ref&gt;[http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 QuickPar forum posting&amp;nbsp;&#8211; PAR3 specifications] {{webarchive |url=https://web.archive.org/web/20120316104813/http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 |date=March 16, 2012 }}&lt;/ref&gt;&lt;ref&gt;[http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm PAR3 proposal] {{webarchive |url=https://web.archive.org/web/20100911002706/http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm |date=September 11, 2010 }}&lt;/ref&gt; The authors intend to fix problems related to creating or repairing when the block count or block size is very high. Par3 also adds support for including directories (file folders) in a parchive and Unicode characters in file names. In addition, the authors plan to enable the Par3 algorithm to identify files that have been moved or renamed.&lt;ref&gt;http://www.livebusinesschat.com/smf/index.php?topic=4751.0 PAR3 move/rename brainstorming&lt;/ref&gt;

== Software ==

=== Windows ===
* MultiPar (freeware) &amp;nbsp;&#8212; Builds upon QuickPar's features and [[GUI]], and Yutaka Sawada's fork of par2cmdline as the PAR2 backend.&lt;ref name="livebusinesschat.com"/&gt; It has support for Par3, [[multithreading (software)|multithreading]], [[Symmetric multiprocessor system|multiple processors]], and the ability to recurse subfolders. MultiPar is able to add recovery data to [[Zip (file format)|ZIP]] and [[7-Zip]]&lt;ref&gt;{{cite web|url=https://sourceforge.net/p/sevenzip/feature-requests/1006/|title=7-Zip|publisher=}}&lt;/ref&gt; files, with a few minor caveats.&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=4922.0 How to add recovery record to ZIP or 7-Zip archive]&lt;/ref&gt; MultiPar is also verified to work with [[Wine (software)|Wine]] under [[TrueOS]], and may work with other operating systems too.&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=4902.0 MultiPar works with PCBSD 9.0]&lt;/ref&gt; Although the Par2 and Par3 components are (or will be) open source, the MultiPar GUI on top of them is currently not open source.&lt;ref&gt;[https://www.livebusinesschat.com/smf/index.php?topic=5402.0 contacted you, asking about sourcecode]&lt;/ref&gt;  Download from [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar forum]. 
* [[QuickPar]] (freeware)&amp;nbsp;&#8212; unmaintained since 2004, superseded by MultiPar.
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] ([[GNU General Public License|GPLv2]])&amp;nbsp;&#8212; a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]].
* Par-N-Rar ([[GNU General Public License|GPL]])
* [http://paulhoule.com/phpar2/index.php phpar2] &amp;nbsp;&#8212; advanced par2cmdline with multithreading and highly optimized assemblercode (about 66% faster than QuickPar 0.9.1)
* Rarslave ([[GNU General Public License|GPLv2]])
* [[SmartPAR]] (freeware) &amp;nbsp;&#8212; Unmaintained since 2002 and obsolete as this application written for Microsoft Windows only works with the original Par1 (PAR) Parchive format parity files. Superseded by QuickPar. It uses Reed&#8211;Solomon error correction to create new recovery files. SmartPAR is able to correct errors and recover missing parts of distributed files from PAR files.&lt;ref&gt;{{cite book
| last        = Wang
| first       = Wallace
| authorlink  = 
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&amp;pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 &#8211; 167 
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}&lt;/ref&gt; Last stable release 0.13d1 dated {{Start date and age|2002|01|22}}&lt;ref&gt;{{cite web |url=http://parchive.sourceforge.net/ |title=Parchive: Parity archive tool |accessdate=2009-09-26}}&lt;/ref&gt;
* [http://www.wehlus.de/mirror/index.html Mirror]&amp;nbsp;&#8212; First PAR implementation, unmaintained since 2001.
* [http://parchive.sourceforge.net/ Original par2cmdline]&amp;nbsp;&#8212; (obsolete).
* [https://github.com/Parchive/par2cmdline par2cmdline] by BlackIkeEagle.

=== Mac OS X ===
* [https://gp.home.xs4all.nl/Site/MacPAR_deLuxe.html MacPAR deLuxe 4.2]
* [http://www.unrarx.com/ UnRarX]
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.

=== [[Linux]] ===
* The [https://github.com/Parchive/par2cmdline par2] utility, which is a maintained fork of par2cmdline. 
* [http://pypar2.silent-blade.org/index.php?n=Main.HomePage PyPar2 1.4], a frontend for par2.
* [http://sourceforge.net/projects/parchive/ GPar2 2.03]
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.
* [https://github.com/jkansanen/par2cmdline-mt par2cmdline-mt] is another multithreaded version of par2cmdline using [[OpenMP]], [[GNU General Public License|GPLv2]], or later.

=== [[FreeBSD]] ===
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later. It is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline-tbb/ par2cmdline-tbb].
* [http://parchive.sourceforge.net/ par2cmdline] is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline/ par2cmdline].

=== [[POSIX]] ===
Software for POSIX conforming operating systems:
* [http://sourceforge.net/projects/ekpar2/ Par2 for KDE 4]

== See also ==
* [[Data degradation|Bit rot]]
* [[Disc rot]]
* [[Data corruption]]
* [[Checksum]]
* [[Comparison of file archivers]] &#8211; Some [[file archivers]] are capable of integrating parity data into their formats for error detection and correction:
* [[RAID]]&amp;nbsp;&#8211; RAID levels at and above RAID 5 make use of parity data to detect and repair errors.

== References ==
{{Reflist|30em}}

== External links ==
* [http://parchive.sourceforge.net/ Parchive project - full specifications and math behind it]
* [http://www.ydecode.com/page_articles003.htm Introduction to PAR and PAR2]
* [http://www.slyck.com/Newsgroups_Guide_PAR_PAR2_Files Slyck's Guide To The Usenet Newsgroups: PAR &amp; PAR2 Files]
* [http://www.warezfaq.com/allaboutpar.htm Another introduction to PAR and PAR2] and [http://www.warezfaq.com/more_info.htm more information from the same site]
* [http://www.binaries4all.com/quickpar/repair.php Guide to repair files using PAR2]
* [https://web.archive.org/web/20100912073937/http://chuchusoft.com/par2_tbb/ par2+tbb]
* [http://www.milow.net/public/projects/parnrar/parnrar.html Par-N-Rar]
* [http://www.irasnyder.com/devel/#rarslave Rarslave]

[[Category:Archive formats]]
[[Category:Data management]]
[[Category:Usenet]]</text>
      <sha1>nh1llz1q1kvr113izyhxpvsb2i1qmry</sha1>
    </revision>
  </page>
  <page>
    <title>Two-phase commit protocol</title>
    <ns>0</ns>
    <id>787850</id>
    <revision>
      <id>761695313</id>
      <parentid>761695308</parentid>
      <timestamp>2017-01-24T08:18:05Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/49.248.74.34|49.248.74.34]] to version by 37.228.230.157. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2908071) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14270" xml:space="preserve">{{Redirect|2PC|the play in American and Canadian football|Two-point conversion|the cryptographic protocol|Commitment scheme}}

In [[transaction processing]], [[database]]s, and [[computer networking]], the '''two-phase commit protocol''' ('''2PC''') is a type of [[Atomic commit|atomic commitment protocol]] (ACP). It is a [[distributed algorithm]] that coordinates all the processes that participate in a [[Distributed transaction|distributed atomic transaction]] on whether to ''[[Commit (data management)|commit]]'' or ''abort'' (''roll back'') the transaction (it is a specialized type of [[Consensus (computer science)|consensus]] protocol). The protocol achieves its goal even in many cases of temporary system failure (involving either process, network node, communication, etc. failures), and is thus widely used.&lt;ref name="bernstein1987"&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Chapter 7, Addison Wesley Publishing Company, ISBN 0-201-10715-5&lt;/ref&gt;&lt;ref name="weikum2001"&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Chapter 19, Elsevier, ISBN 1-55860-508-8&lt;/ref&gt;&lt;ref name=Bern2009&gt;Philip A. Bernstein, Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition], Chapter 8, Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4&lt;/ref&gt;
However, it is not resilient to all possible failure configurations, and in rare cases, user (e.g., a system's administrator) intervention is needed to remedy an outcome. To accommodate recovery from failure (automatic in most cases) the protocol's participants use [[Server log|logging]] of the protocol's states. Log records, which are typically slow to generate but survive failures, are used by the protocol's [[recovery procedure]]s. Many protocol variants exist that primarily differ in logging strategies and recovery mechanisms. Though usually intended to be used infrequently, recovery procedures compose a substantial portion of the protocol, due to many possible failure scenarios to be considered and supported by the protocol.

In a "normal execution" of any single [[distributed transaction]] ( i.e., when no failure occurs, which is typically the most frequent situation), the protocol consists of two phases:
#The ''commit-request phase'' (or ''voting phase''), in which a ''coordinator'' process attempts to prepare all the transaction's participating processes (named ''participants'', ''cohorts'', or ''workers'') to take the necessary steps for either committing or aborting the transaction and to ''vote'', either "Yes": commit (if the transaction participant's local portion execution has ended properly), or "No": abort (if a problem has been detected with the local portion), and
#The ''commit phase'', in which, based on ''voting'' of the cohorts, the coordinator decides whether to commit (only if ''all'' have voted "Yes") or abort the transaction (otherwise), and notifies the result to all the cohorts. The cohorts then follow with the needed actions (commit or abort) with their local transactional resources (also called ''recoverable resources''; e.g., database data) and their respective portions in the transaction's other output (if applicable).

Note that the two-phase commit (2PC) protocol should not be confused with the [[two-phase locking]] (2PL) protocol, a [[concurrency control]] protocol.

==Assumptions==
The protocol works in the following manner: one node is a designated '''coordinator''', which is the master site, and the rest of the nodes in the network are designated the '''cohorts'''. The protocol assumes that there is [[stable storage]] at each node with a [[Write ahead logging|write-ahead log]], that no node crashes forever, that the data in the write-ahead log is never lost or corrupted in a crash, and that any two nodes can communicate with each other. The last assumption is not too restrictive, as network communication can typically be rerouted. The first two assumptions are much stronger; if a node is totally destroyed then data can be lost.

The protocol is initiated by the coordinator after the last step of the transaction has been reached. The cohorts then respond with an '''agreement''' message or an '''abort''' message depending on whether the transaction has been processed successfully at the cohort.

==Basic algorithm==

===Commit request phase===
or '''voting phase'''

#The coordinator sends a '''query to commit''' message to all cohorts and waits until it has received a reply from all cohorts.
#The cohorts execute the transaction up to the point where they will be asked to commit.  They each write an entry to their ''undo log'' and an entry to their ''[[redo log]]''.
#Each cohort replies with an '''agreement''' message (cohort votes '''Yes''' to commit), if the cohort's actions succeeded, or an '''abort''' message (cohort votes '''No''', not to commit), if the cohort experiences a failure that will make it impossible to commit.

===Commit phase===
or '''Completion phase'''

====Success====
If the coordinator received an '''agreement''' message from ''all'' cohorts during the commit-request phase:
#The coordinator sends a '''commit''' message to all the cohorts.
#Each cohort completes the operation, and releases all the locks and resources held during the transaction.
#Each cohort sends an '''acknowledgment''' to the coordinator.
#The coordinator completes the transaction when all acknowledgments have been received.

====Failure====
If ''any'' cohort votes '''No''' during the commit-request phase (or the coordinator's timeout '''expires'''):
#The coordinator sends a '''rollback''' message to all the cohorts.
#Each cohort undoes the transaction using the undo log, and releases the resources and locks held during the transaction.
#Each cohort sends an '''acknowledgement''' to the coordinator.
#The coordinator undoes the transaction when all acknowledgements have been received.

====Message flow====
&lt;pre&gt;
Coordinator                                         Cohort
                              QUERY TO COMMIT
                --------------------------------&gt;
                              VOTE YES/NO           prepare*/abort*
                &lt;-------------------------------
commit*/abort*                COMMIT/ROLLBACK
                --------------------------------&gt;
                              ACKNOWLEDGMENT        commit*/abort*
                &lt;--------------------------------  
end
&lt;/pre&gt;
An * next to the record type means that the record is forced to stable storage.&lt;ref name="mohan1986"&gt;[[C. Mohan]], Bruce Lindsay and R. Obermarck (1986): [http://dl.acm.org/citation.cfm?id=7266  "Transaction management in the R* distributed database management system"],''ACM Transactions on Database Systems (TODS)'', Volume 11 Issue 4, Dec. 1986, Pages 378 - 396&lt;/ref&gt;

==Disadvantages==
The greatest disadvantage of the two-phase commit protocol is that it is a blocking protocol. If the coordinator fails permanently, some cohorts will never resolve their transactions: After a cohort has sent an '''agreement''' message to the coordinator, it will block until a '''commit''' or '''rollback''' is received.

==Implementing the two-phase commit protocol==

===Common architecture===
In many cases the 2PC protocol is distributed in a computer network. It is easily distributed by implementing multiple dedicated 2PC components similar to each other, typically named ''[[Transaction manager]]s'' (TMs; also referred to as ''2PC agents'' or Transaction Processing Monitors), that carry out the protocol's execution for each transaction (e.g., [[The Open Group]]'s [[X/Open XA]]). The databases involved with a distributed transaction, the ''participants'', both the coordinator and cohorts, ''register'' to close TMs (typically residing on respective same network nodes as the participants) for terminating that transaction using 2PC. Each distributed transaction has an ad hoc set of TMs, the TMs to which the transaction participants register. A leader, the coordinator TM, exists for each transaction to coordinate 2PC for it, typically the TM of the coordinator database. However, the coordinator role can be transferred to another TM for performance or reliability reasons. Rather than exchanging 2PC messages among themselves, the participants exchange the messages with their respective TMs. The relevant TMs communicate among themselves to execute the 2PC protocol schema above, "representing" the respective participants, for terminating that transaction. With this architecture the protocol is fully distributed (does not need any central processing component or data structure), and scales up with number of network nodes (network size) effectively.

This common architecture is also effective for the distribution of other [[atomic commitment protocol]]s besides 2PC, since all such protocols use the same voting mechanism and outcome propagation to protocol participants.&lt;ref name="bernstein1987" /&gt;&lt;ref name="weikum2001" /&gt;

===Protocol optimizations===
[[Database]] research has been done on ways to get most of the benefits of the two-phase commit protocol while reducing costs by ''protocol optimizations''&lt;ref name="bernstein1987" /&gt;&lt;ref name="weikum2001" /&gt;&lt;ref name="Bern2009" /&gt; and protocol operations saving under certain system's behavior assumptions.

====Presumed Abort and Presumed Commit====
''Presumed abort'' or ''Presumed commit'' are common such optimizations.&lt;ref name="weikum2001" /&gt;&lt;ref name=Bern2009/&gt;&lt;ref name="mohan1983"&gt;[[C. Mohan]], Bruce Lindsay (1985): [http://portal.acm.org/citation.cfm?id=850772  "Efficient commit protocols for the tree of processes model of distributed transactions"],''ACM SIGOPS Operating Systems Review'',
19(2),pp. 40-52 (April 1985)&lt;/ref&gt; An assumption about the outcome of transactions, either commit, or abort, can save both messages and logging operations by the participants during the 2PC protocol's execution. For example, when presumed abort, if during system recovery from failure no logged evidence for commit of some transaction is found by the recovery procedure, then it assumes that the transaction has been aborted, and acts accordingly. This means that it does not matter if aborts are logged at all, and such logging can be saved under this assumption. Typically a penalty of additional operations is paid during recovery from failure, depending on optimization type. Thus the best variant of optimization, if any, is chosen according to failure and transaction outcome statistics.

====Tree two-phase commit protocol====
The '''[[Tree (data structure)|Tree]] 2PC protocol'''&lt;ref name="weikum2001" /&gt; (also called ''Nested 2PC'', or ''Recursive 2PC'') is a common variant of 2PC in a [[computer network]], which better utilizes the underlying communication infrastructure. The participants in a distributed transaction are typically invoked in an order which defines a tree structure, the ''invocation tree'', where the participants are the nodes and the edges are the invocations (communication links). The same tree is commonly utilized to complete the transaction by a 2PC protocol, but also another communication tree can be utilized for this, in principle. In a tree 2PC the coordinator is considered the root ("top") of a communication tree (inverted tree), while the cohorts are the other nodes. The coordinator can be the node that originated the transaction (invoked recursively (transitively) the other participants), but also another node in the same tree can take the coordinator role instead. 2PC messages from the coordinator are propagated "down" the tree, while messages to the coordinator are "collected" by a cohort from all the cohorts below it, before it sends the appropriate message "up" the tree (except an '''abort''' message, which is propagated "up" immediately upon receiving it or if the current cohort initiates the abort).

The '''Dynamic two-phase commit''' (Dynamic two-phase commitment, D2PC) '''protocol'''&lt;ref name="weikum2001" /&gt;&lt;ref name="raz1995"&gt;[[Yoav Raz]] (1995): [http://www.springerlink.com/content/pv12p828kk616258/  "The Dynamic Two Phase Commitment (D2PC) protocol "],''Database Theory &#8212; ICDT '95'', ''Lecture Notes in Computer Science'', Volume 893/1995, pp. 162-176, Springer, ISBN 978-3-540-58907-5&lt;/ref&gt; is a variant of Tree 2PC with no predetermined coordinator. It subsumes several optimizations that have been proposed earlier. '''Agreement''' messages ('''Yes''' votes) start to propagate from all the leaves, each leaf when completing its tasks on behalf of the transaction (becoming ''ready''). An intermediate (non leaf) node sends when ''ready'' an '''agreement''' message to the last (single) neighboring node from which '''agreement''' message has not yet been received. The coordinator is determined dynamically by racing '''agreement''' messages over the transaction tree, at the place where they collide. They collide either at a transaction tree node, to be the coordinator, or on a tree edge. In the latter case one of the two edge's nodes is elected as a coordinator (any node). D2PC is time optimal (among all the instances of a specific transaction tree, and any specific Tree 2PC protocol implementation; all instances have the same tree; each instance has a different node as coordinator): By choosing an optimal coordinator D2PC commits both the coordinator and each cohort in minimum possible time, allowing the earliest possible release of locked resources in each transaction participant (tree node).

==See also==
*[[Atomic commit]]
*[[Commit (data management)]]
*[[Three-phase commit protocol]]
*[[X/Open XA|XA]]
*[[Paxos algorithm]]
*[[Two Generals' Problem]]

==References==
{{Reflist}}

==External links==
*[http://exploredatabase.blogspot.in/2014/07/two-phase-commit-protocol-in-pictures.html Two Phase Commit protocol explained in Pictures] by exploreDatabase

{{DEFAULTSORT:Two-Phase Commit Protocol}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>qet372qxkezgu2hevgtfi77b3fynucr</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic translation</title>
    <ns>0</ns>
    <id>2994894</id>
    <revision>
      <id>720900670</id>
      <parentid>627434220</parentid>
      <timestamp>2016-05-18T16:55:32Z</timestamp>
      <contributor>
        <username>Swpb</username>
        <id>1921264</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2871" xml:space="preserve">{{Unreferenced|date=December 2009}}
'''Semantic translation''' is the process of using [[semantic]] information to aid in the translation of data in one representation or [[data model]] to another representation or data model.  Semantic translation takes advantage of semantics that associate meaning with individual [[data element]]s in one [[data dictionary|dictionary]] to create an equivalent meaning in a second system.

An example of semantic translation is the conversion of [[XML]] data from one data model to a second data model using formal [[ontologies]] for each system such as the [[Web Ontology Language]] (OWL).  This is frequently required by [[intelligent agents]] that wish to perform searches on remote computer systems that use different data models to store their data elements.  The process of allowing a single user to search multiple systems with a single search request is also known as [[federated search]].

Semantic translation should be differentiated from [[data mapping]] tools that do simple one-to-one translation of data from one system to another without actually associating meaning with each data element.

Semantic translation requires that data elements in the source and destination systems have "semantic mappings" to a central registry or registries of data elements. The simplest mapping is of course where there is equivalence.
There are three types of [[Semantic equivalence]]:

* '''[[Class (computer science)|Class]] Equivalence'''{{Anchor|Class equivalence}} - indicating that class or "concepts" are equivalent.  For example: "Person" is the same as "Individual"
* '''[[Relation (mathematics)|Property]] Equivalence'''{{Anchor|Property equivalence}} - indicating that two properties are equivalent.  For example: "PersonGivenName" is the same as "FirstName"
* '''[[Instance (computer science)|Instance]] Equivalence'''{{Anchor|Instance equivalence}} - indicating that two individual instances of objects are equivalent.  For example: "Dan Smith" is the same person as "Daniel Smith"

Semantic translation is very difficult if the terms in a particular data model do not have direct one-to-one mappings to data elements in a foreign data model. In that situation an alternative approach must be used to find mappings from the original data to the foreign data elements.  This problem can be alleviated by centralized metadata registries that use the ISO-11179 standards such as the [[National Information Exchange Model]] (NIEM).

==See also==
* [[Data mapping]]
* [[Semantic heterogeneity]]
* [[Semantic mapper]]
* [[Federated search]]
* [[Intelligent agents]]
* [[ISO/IEC 11179]]
* [[National Information Exchange Model]]
* [[Semantic Web]]
* [[Vocabulary-based transformation]]
* [[Web Ontology Language]]

{{DEFAULTSORT:Semantic Translation}}
[[Category:Data management]]
[[Category:Enterprise application integration]]</text>
      <sha1>jnchv6yatg3xim2zff3kp8tt9atnwpb</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise information system</title>
    <ns>0</ns>
    <id>1010494</id>
    <revision>
      <id>744923318</id>
      <parentid>744923314</parentid>
      <timestamp>2016-10-18T08:01:50Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/43.245.120.161|43.245.120.161]] to version by Mild Bill Hiccup. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2800771) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4454" xml:space="preserve">An '''enterprise information system''' ('''EIS''') is any kind of [[information system]] which improves the functions of an enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of [[data]] and capable of supporting some large and possibly complex [[organization]] or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.&lt;ref name="eidvtai" /&gt;

The word ''enterprise'' can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become the latest corporate-speak [[buzzword]].{{Citation needed|date=June 2016}}

==Purpose==
Enterprise information systems provide a technology platform that enables organizations to [[Enterprise integration|integrate]] and coordinate their [[business processes]] on a robust foundation. An EIS is currently used in conjunction with [[customer relationship management]] and [[supply chain management]] to automate business processes.&lt;ref name="eidvtai" /&gt;  An enterprise information system provides a single system that is central to the organization that ensures information can be shared across all functional levels and management [[hierarchies]].

An EIS can be used to increase business [[productivity]] and reduce service cycles, [[product development]] cycles and marketing life cycles.&lt;ref name="eidvtai"&gt;{{cite book |title=Enterprise Information Systems: Contemporary Trends and Issues |last=Olson |first=David L. |author2=Subodh Kesharwani |year=2010 |publisher=World Scientific |isbn=9814273163 |pages=2, 13&#8211;16 |url=https://books.google.com/books?id=-AwDAp7Fe2UC |accessdate=20 August 2013}}&lt;/ref&gt;  It may be used to amalgamate existing applications. Other outcomes include higher [[operational efficiency]] and cost savings.&lt;ref name="eidvtai" /&gt;

Financial value is not usually a direct outcome from the implementation of an enterprise information system.&lt;ref name="eiscmta"&gt;{{cite book |title=Enterprise Information Systems: Concepts, Methodologies, Tools and Applications |author=Information Resources Management Association |year=2010 |publisher=Idea Group Inc |isbn=1616928530 |pages=38, 43 |url=https://books.google.com/books?id=hpc6-SfS2scC |accessdate=20 August 2013}}&lt;/ref&gt;

==Design stage==
At the design stage the main characteristic of EIS efficiency evaluation is the probability of timely delivery of various messages such as command, service, etc.&lt;ref&gt;{{cite journal |title=&#1054;&#1062;&#1045;&#1053;&#1050;&#1040; &#1061;&#1040;&#1056;&#1040;&#1050;&#1058;&#1045;&#1056;&#1048;&#1057;&#1058;&#1048;&#1050; &#1060;&#1059;&#1053;&#1050;&#1062;&#1048;&#1054;&#1053;&#1048;&#1056;&#1054;&#1042;&#1040;&#1053;&#1048;&#1071; &#1050;&#1054;&#1056;&#1055;&#1054;&#1056;&#1040;&#1058;&#1048;&#1042;&#1053;&#1067;&#1061; &#1048;&#1053;&#1060;&#1054;&#1056;&#1052;&#1040;&#1062;&#1048;&#1054;&#1053;&#1053;&#1067;&#1061; &#1057;&#1048;&#1057;&#1058;&#1045;&#1052; &#1057; &#1053;&#1045;&#1054;&#1044;&#1053;&#1054;&#1056;&#1054;&#1044;&#1053;&#1054;&#1049; &#1053;&#1040;&#1043;&#1056;&#1059;&#1047;&#1050;&#1054;&#1049; |trans-title=Efficiency Evaluation of Enterprise Information Systems with Non-uniform Load |language=ru |url=http://ntv.ifmo.ru/en/article/13881/ocenka_harakteristik_funkcionirovaniya_korporativnyhinformacionnyh_sistem_s_neodnorodnoy_nagruzkoy.htm |author1=Kalinin I.V. |author2=Maharevs E. |author3=Muravyeva-Vitkovskaya L.A. |journal=Scientific and Technical Journal of Information Technologies, Mechanics and Optics |volume=15 |issue=5 |pages=863&#8211;868 |year=2015 |doi=10.17586/2226-1494-2015-15-5-863-868}}&lt;/ref&gt;

==Information systems==
{{main|Information systems}}
Enterprise systems create a standard [[data structure]] and are invaluable in eliminating the problem of information fragmentation caused by multiple information systems within an organization.  An EIS differentiates itself from [[legacy system]]s in that it self-transactional, self-helping and adaptable to general and specialist conditions.&lt;ref name="eidvtai" /&gt;  Unlike an enterprise information system, legacy systems are limited to department wide communications.&lt;ref name="eiscmta" /&gt;

A typical enterprise information system would be housed in one or more [[data center]]s, would run [[enterprise software]], and could include applications that typically cross organizational borders such as [[content management systems]].

==See also==
{{Portal|Computing|Business}}
*[[Executive information system]]
*[[Management information system]]
*[[Enterprise planning systems]]
*[[Enterprise software]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Enterprise modelling]]
[[Category:Website management]]</text>
      <sha1>6pcqoeu98ktpsd3ip3zjpo27xztzest</sha1>
    </revision>
  </page>
  <page>
    <title>Modular concurrency control</title>
    <ns>0</ns>
    <id>24906259</id>
    <redirect title="Global concurrency control" />
    <revision>
      <id>323027606</id>
      <parentid>323024786</parentid>
      <timestamp>2009-10-31T01:36:44Z</timestamp>
      <contributor>
        <username>Comps</username>
        <id>1071011</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="191" xml:space="preserve">#REDIRECT [[Global concurrency control]]

Modular concurrency control

[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]</text>
      <sha1>n91o2igtd1n2mt1vgecy84cffvdoy24</sha1>
    </revision>
  </page>
  <page>
    <title>Disaster recovery</title>
    <ns>0</ns>
    <id>640655</id>
    <revision>
      <id>762835821</id>
      <parentid>754651773</parentid>
      <timestamp>2017-01-31T00:30:06Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14597" xml:space="preserve">{{About|1=[[business continuity planning]]|2=societal disaster recovery|3=emergency management}}
{{Other uses|DR (disambiguation)}}

{{merge to|business continuity|date=June 2015}}

'''Disaster recovery''' (DR) involves a set of policies and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a [[natural disaster|natural]] or [[man-made hazards|human-induced]] [[disaster]]. Disaster recovery focuses on the IT or [[technology systems]] supporting critical business functions,&lt;ref&gt;[http://continuity.georgetown.edu/dr/ ''Systems and Operations Continuity: Disaster Recovery.''] Georgetown University. University Information Services. Retrieved 3 August 2012.&lt;/ref&gt; as opposed to [[business continuity]], which involves keeping all essential aspects of a business functioning despite significant disruptive events.  Disaster recovery is therefore a subset of business continuity.&lt;ref&gt;[http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&amp;expand=true&amp;lc=en ''Disaster Recovery and Business Continuity, version 2011.''] {{webarchive |url=https://web.archive.org/web/20130111203921/http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&amp;expand=true&amp;lc=en |date=January 11, 2013 }} IBM. Retrieved 3 August 2012.&lt;/ref&gt;

==History==
Disaster recovery developed in the mid- to late 1970s as computer center managers began to recognize the dependence of their organizations on their computer systems. At that time, most systems were [[Batch processing|batch]]-oriented [[Mainframe computer|mainframe]]s which in many cases could be [[downtime|down]] for a number of days before significant damage would be done to the organization.

As awareness of the potential business disruption that would follow an IT-related disaster, the disaster recovery industry developed to provide backup computer centers, with Sun Information Systems (which later became Sungard Availability Services) becoming the first major US commercial hot site vendor, established in 1978 in Philadelphia.

During the 1980s and 90s, customer awareness and industry both grew rapidly, driven by the advent of open systems and [[Real-time computing|real-time processing]] which increased the dependence of organizations on their IT systems. Regulations mandating business continuity and disaster recovery plans for organizations in various sectors of the economy, imposed by the authorities and by business partners, increased the demand and led to the availability of commercial disaster recovery services, including mobile data centers delivered to a suitable recovery location by truck.

With the rapid growth of the [[Internet]] through the late 1990s and into the 2000s, organizations of all sizes became further dependent on the continuous [[availability]] of their IT systems, with some organizations setting objectives of 2, 3, 4 or 5 nines (99.999%) availability of critical systems.{{citation needed|date=March 2016}} This increasing dependence on IT systems, as well as increased awareness from large-scale disasters such as tsunami, earthquake, flood, and volcanic eruption, spawned disaster recovery-related products and services, ranging from [[high-availability]] solutions to [[hot-site]] facilities.  Improved networking meant critical IT services could be served remotely, hence on-site recovery became less important.

The rise of cloud computing since 2010 continues that trend: nowadays, it matters even less where computing services are physically served, just so long as the network itself is sufficiently reliable (a separate issue, and less of a concern since modern networks are highly resilient by design).  'Recovery as a Service' (RaaS) is one of the security features or benefits of cloud computing being promoted by the Cloud Security Alliance.&lt;ref&gt;[https://cloudsecurityalliance.org/download/secaas-category-9-bcdr-implementation-guidance/ ''SecaaS Category 9 // BCDR Implementation Guidance''] CSA, retrieved 14 July 2014.&lt;/ref&gt;

===Classification of disasters===
Disasters can be classified into two broad categories. The first is natural disasters such as floods, hurricanes, tornadoes or earthquakes. While preventing a natural disaster is impossible, risk management measures such as avoiding disaster-prone situations and good planning can help. The second category is man made disasters, such as hazardous material spills, infrastructure failure, bio-terrorism, and disastrous IT bugs or failed change implementations. In these instances, surveillance, testing and mitigation planning are invaluable.

==Importance of disaster recovery planning==
Recent research supports the idea that implementing a more holistic pre-disaster planning approach is more cost-effective in the long run. Every $1 spent on hazard mitigation(such as a [[disaster recovery plan]]) saves society $4 in response and recovery costs.&lt;ref&gt;{{cite web|first=Partnership for Disaster Resilience|title=Post-Disaster Recovery Planning Forum: How-To Guide|url=http://nthmp.tsunami.gov/Minutes/oct-nov07/post-disaster_recovery_planning_forum_uo-csc-2.pdf|publisher=University of Oregon's Community Service Center|accessdate=2013-05-23}}&lt;/ref&gt;

As [[Information technology|IT systems]] have become increasingly critical to the smooth operation of a company, and arguably the economy as a whole, the importance of ensuring the continued operation of those systems, and their rapid recovery, has increased. For example, of companies that had a major loss of business data, 43% never reopen and 29% close within two years. As a result, preparation for continuation or recovery of systems needs to be taken very seriously. This involves a significant investment of time and money with the aim of ensuring minimal losses in the event of a disruptive event.&lt;ref&gt;{{cite web|url=http://www.ready.gov/business/implementation/IT|title=IT Disaster Recovery Plan|date=25 October 2012|publisher=FEMA|accessdate=11 May 2013}}&lt;/ref&gt;

==Control measures==
Control measures are steps or mechanisms that can reduce or eliminate various threats for organizations. Different types of measures can be included in disaster recovery plan (DRP).

Disaster recovery planning is a subset of a larger process known as business continuity planning and includes planning for resumption of applications, data, hardware, electronic communications (such as networking) and other IT infrastructure. A business continuity plan (BCP) includes planning for non-IT related aspects such as key personnel, facilities, crisis communication and reputation protection, and should refer to the disaster recovery plan (DRP) for IT related infrastructure recovery / continuity.

IT disaster recovery control measures can be classified into the following three types:
# Preventive measures - Controls aimed at preventing an event from occurring.
# Detective measures - Controls aimed at detecting or discovering unwanted events.
# Corrective measures - Controls aimed at correcting or restoring the system after a disaster or an event.

Good disaster recovery plan measures dictate that these three types of controls be documented and exercised regularly using so-called "DR tests".

==Strategies==
Prior to selecting a disaster recovery strategy, a disaster recovery planner first refers to their organization's business continuity plan which should indicate the key metrics of [[recovery point objective]] (RPO) and [[recovery time objective]] (RTO) for various business processes (such as the process to run payroll, generate an order, etc.). The metrics specified for the business processes are then mapped to the underlying IT systems and infrastructure that support those processes.&lt;ref&gt;Gregory, Peter. CISA Certified Information Systems Auditor All-in-One Exam Guide, 2009. ISBN 978-0-07-148755-9. Page 480.&lt;/ref&gt;

Incomplete RTOs and RPOs can quickly derail a disaster recovery plan. Every item in the DR plan requires a defined recovery point and time objective, as failure to create them may lead to significant problems that can extend the disaster&#8217;s impact.&lt;ref&gt;{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |title=Five Mistakes That Can Kill a Disaster Recovery Plan |publisher=Dell.com |accessdate=2012-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |archivedate=2013-01-16 |df= }}&lt;/ref&gt; Once the RTO and RPO metrics have been mapped to IT infrastructure, the DR planner can determine the most suitable recovery strategy for each system.  The organization ultimately sets the IT budget and therefore the RTO and RPO metrics need to fit with the available budget. While most business unit heads would like zero data loss and zero time loss, the cost associated with that level of protection may make the desired high availability solutions impractical. A [[cost-benefit analysis]] often dictates which disaster recovery measures are implemented.

Traditionally, a disaster recovery system involved cutover or switch-over recovery systems.{{citation needed|date=April 2016}} Such measures would allow an organization to preserve its technology and information, by having a remote disaster recovery location that produced backups on a regular basis. However, this strategy proved to be expensive and time-consuming. Therefore, more affordable and effective cloud-based systems were introduced.

Some of the most common strategies for [[Data recovery|data protection]] include: 
* backups made to tape and sent off-site at regular intervals
* backups made to disk on-site and automatically copied to off-site disk, or made directly to off-site disk
* replication of data to an off-site location, which overcomes the need to restore the data (only the systems then need to be restored or synchronized), often making use of [[storage area network]] (SAN) technology
* Private Cloud solutions which replicate the management data (VMs, Templates and disks) into the storage domains which are part of the private cloud setup. These management data are configured as an xml representation called OVF (Open Virtualization Format), and can be restored once a disaster occurs.
* Hybrid Cloud solutions that replicate both on-site and to off-site data centers.  These solutions provide the ability to instantly fail-over to local on-site hardware, but in the event of a physical disaster, servers can be brought up in the cloud data centers as well.
* the use of high availability systems which keep both the data and system replicated off-site, enabling continuous access to systems and data, even after a disaster (often associated with [[cloud storage]])&lt;ref&gt;{{cite web|url=http://www.inc.com/guides/201106/how-to-use-the-cloud-as-a-disaster-recovery-strategy.html|title=How to Use the Cloud as a Disaster Recovery Strategy|last=Brandon|first=John|date=23 June 2011|publisher=Inc. |accessdate=11 May 2013}}&lt;/ref&gt;

In many cases, an organization may elect to use an outsourced disaster recovery provider to provide a stand-by site and systems rather than using their own remote facilities, increasingly via [[cloud computing]].

In addition to preparing for the need to recover systems, organizations also implement precautionary measures with the objective of preventing a disaster in the first place. These may include: 
* local mirrors of systems and/or data and use of disk protection technology such as [[RAID]]
* surge protectors &#8212; to minimize the effect of power surges on delicate electronic equipment
* use of an [[uninterruptible power supply]] (UPS) and/or backup generator to keep systems going in the event of a power failure
* fire prevention/mitigation systems such as alarms and fire extinguishers
* anti-virus software and other security measures

==See also==
* [[Backup site]]
* [[High availability]]
* [[Continuous data protection]]
* [[Data recovery]]
* [[Emergency management]]
* [[IT service continuity]]
* [[Remote backup service]]
* [[Seven tiers of disaster recovery]]
* [[Virtual tape library]]

==References==
{{reflist}}

==Further reading==
* ISO/IEC 22301:2012 (replacement of BS-25999:2007) Societal Security - Business Continuity Management Systems - Requirements
* ISO/IEC 27001:2013 (replacement of ISO/IEC 27001:2005 [formerly BS 7799-2:2002]) Information Security Management System
* ISO/IEC 27002:2013 (replacement of ISO/IEC 27002:2005 [renumbered ISO17799:2005]) Information Security Management - Code of Practice
* ISO/IEC 22399:2007 Guideline for incident preparedness and operational continuity management
* ISO/IEC 24762:2008 Guidelines for information and communications technology disaster recovery services
* IWA 5:2006 Emergency Preparedness&#8212;British Standards Institution --
* BS 25999-1:2006 Business Continuity Management Part 1: Code of practice
* BS 25999-2:2007 Business Continuity Management Part 2: Specification
* BS 25777:2008 Information and communications technology continuity management - Code of practice&#8212;Others --
* "A Guide to Business Continuity Planning" by James C. Barnes
* "Business Continuity Planning", A Step-by-Step Guide with Planning Forms on CDROM by Kenneth L Fulmer
* "Disaster Survival Planning: A Practical Guide for Businesses" by Judy Bell
* ICE Data Management (In Case of Emergency) made simple - by MyriadOptima.com
* Harney, J.(2004). Business continuity and disaster recovery: Back up or shut down.
* AIIM E-Doc Magazine, 18(4), 42-48.
* Dimattia, S. (November 15, 2001).Planning for Continuity. Library Journal,32-34.

==External links==
* [https://www.ready.gov/business/implementation/IT IT Disaster Recovery Plan from Ready.gov]
&lt;!--========================({{No More Links}})============================
 | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA |
 | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |
 | |
 | Excessive or inappropriate links WILL BE DELETED. |
 | See [[Wikipedia:External links]] &amp; [[Wikipedia:Spam]] for details. |
 | |
 | If there are already plentiful links, please propose additions or |
 | replacements on this article's discussion page, or submit your link |
 | to the relevant category at the Open Directory Project (dmoz.org) |
 | and link back to that category using the {{dmoz}} template. |
 =======================({{No More Links}})=============================--&gt;

{{Disasters}}

{{DEFAULTSORT:Disaster Recovery}}
[[Category:Disaster recovery]]
[[Category:Data management]]
[[Category:Backup]]
[[Category:IT risk management]]</text>
      <sha1>0l2r2dhyomkgwomfpdehhr5szvbngap</sha1>
    </revision>
  </page>
  <page>
    <title>Query language</title>
    <ns>0</ns>
    <id>494528</id>
    <revision>
      <id>755820936</id>
      <parentid>755667762</parentid>
      <timestamp>2016-12-20T11:21:31Z</timestamp>
      <contributor>
        <username>Feminist</username>
        <id>25530780</id>
      </contributor>
      <minor />
      <comment>/* Examples */[[Talk:Bing (search engine)#Requested move 13 December 2016]], replaced: [[Bing]] &#8594; [[Bing (search engine)|Bing]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6603" xml:space="preserve">{{redirect|Database language|other types of database languages|Database#Languages}}
{{Multiple issues|
{{prose|date=October 2010}}
{{refimprove|date=October 2010}}
}}

'''Query languages''' are [[computer language]]s used to make queries in [[database]]s and [[information system]]s.

==Types==
Broadly, query languages can be classified according to whether they are database query languages or [[information retrieval query language]]s. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.

==Examples==
Examples include:
* [[.QL]] is a proprietary object-oriented query language for querying [[relational database]]s; successor of Datalog;
* [[Contextual Query Language]] (CQL) a formal language for representing queries to [[information retrieval]] systems such as web indexes or bibliographic catalogues.
* CQLF (CODYASYL Query Language, Flat) is a query language for [[CODASYL]]-type databases;
* [[Concept-Oriented Query Language]] (COQL) is used in the concept-oriented model (COM). It is based on a novel [[data modeling]] construct, concept, and uses such operations as projection and de-projection for multi-dimensional analysis, analytical operations and inference;
* [[Cypher Query Language|Cypher]] is a query language for the [[Neo4j]] graph database;
* [[Data Mining Extensions|DMX]] is a query language for [[Data Mining]] models;
* [[Datalog]] is a query language for [[deductive database]]s;
* [[F-logic]] is a declarative object-oriented language for [[deductive database]]s and [[knowledge representation]].
* [[Facebook Query Language|FQL]] enables you to use a [[SQL]]-style interface to query the data exposed by the [[Graph API]]. It provides advanced features not available in the [[Graph API]].&lt;ref&gt;{{cite web|url=https://developers.facebook.com/docs/technical-guides/fql/|title=FQL Overview|work=Facebook Developers}}&lt;/ref&gt;
* [[Gellish English]] is a language that can be used for queries in Gellish English Databases, for dialogues (requests and responses) as well as for information modeling and [[knowledge modeling]];&lt;ref&gt;http://gellish.wiki.sourceforge.net/Querying+a+Gellish+English+database{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;
* [[Gremlin (programming language)|Gremlin]] is an [[Apache Software Foundation]] graph traversal language for OLTP and OLAP graph systems.
* [[HTSQL]] is a query language that translates [[HTTP]] queries to [[SQL]];
* [[ISBL]] is a query language for [[PRTV]], one of the earliest relational database management systems;
* [[LINQ]] query-expressions is a way to query various data sources from [[.NET Framework|.NET]] languages
* [[LDAP]] is an [[application protocol]] for querying and modifying [[directory services]] running over [[TCP/IP]];
* LogiQL is a variant of Datalog and is the query language for the LogicBlox system.
* [[Molecular Query Language|MQL]] is a [[cheminformatics]] query language for a [[substructure search]] allowing beside nominal properties also numerical properties;
* [[MultiDimensional eXpressions|MDX]] is a query language for [[OLAP]] databases;
* [[N1QL]] is a [[Couchbase, Inc.|Couchbase]]'s query language finding data in [[Couchbase Server]]s;
* [[Object Query Language|OQL]] is Object Query Language;
* [[Object Constraint Language|OCL]] (Object Constraint Language). Despite its name, OCL is also an object query language and an [[Object Management Group|OMG]] standard;
* [[OPath]], intended for use in querying [[WinFS]] ''Stores'';
* [[OttoQL]], intended for querying tables, [[XML]], and databases;
* [[Poliqarp Query Language]] is a special query language designed to analyze annotated text. Used in the [[Poliqarp]] search engine;
* [[PQL]] is a [[special-purpose programming language]] for managing [[process model]]s based on information about [[wiktionary:Scenario|scenarios]] that these models describe;
* [[QUEL query languages|QUEL]] is a [[relational database]] access language, similar in most ways to [[SQL]];
* [[RDQL]] is a [[Resource Description Framework|RDF]] query language;
* [[ReQL]] is a query language used in [http://rethinkdb.com/docs/introduction-to-reql/ RethinkDB];
* [[Smiles arbitrary target specification|SMARTS]] is the [[cheminformatics]] standard for a [[substructure search]];
* [[SPARQL]] is a query language for [[Resource Description Framework|RDF]] [[Graph (discrete mathematics)|graphs]];
* [[SPL (Search Processing Language)|SPL]] is a search language for machine-generated [[big data]], based upon Unix Piping and SQL.
* SCL is the Software Control Language to query and manipulate [[Endevor]] objects
* [[SQL]] is a well known query language and [[Data Manipulation Language]] for [[relational database]]s;
* [[SuprTool]] is a proprietary query language for SuprTool, a database access program used for accessing data in ''Image/SQL'' (formerly [[TurboIMAGE]]) and Oracle databases;
* [[TMQL]] Topic Map Query Language is a query language for [[Topic Maps]];
* TQL is a language used to [http://cmshelpcenter.saas.hp.com/CMS/10.21/ucmdb-docs/docs/eng/doc_lib/Content/modeling/Tql_c_Overview.htm query topology for HP products] 
* [[D (data language specification)|Tutorial D]] is a query language for [[Relational database management system|truly relational database management systems]] (TRDBMS);
* [[XQuery]] is a query language for [[XML database|XML data sources]];
* [[XPath]] is a declarative language for navigating XML documents;
* [[XSPARQL]] is an integrated query language combining XQuery with SPARQL to query both XML and RDF data sources at once;
* [[Yahoo! query language|YQL]] is an [[SQL]]-like query language created by [[Yahoo!]]
* Search engine query languages, e.g., as used by [[Google Search|Google]]&lt;ref&gt;
{{cite web
| title = Search operators
| url = https://support.google.com/websearch/answer/2466433?hl=en
| accessdate = August 22, 2015
| publisher = Google
}}&lt;/ref&gt; or [[Bing (search engine)|Bing]]&lt;ref&gt;
{{cite web
| title = Bing Query Language
| url = https://msdn.microsoft.com/en-us/library/ff795667.aspx
| accessdate = August 22, 2015
| publisher = Microsoft
}}&lt;/ref&gt;

== See also ==
* [[Data manipulation language]]

== References ==
{{Reflist}}

{{Database}}
{{Databases}}
{{Computer language}}
{{Query languages}}

{{Authority control}}

{{DEFAULTSORT:Query Language}}
[[Category:Computer languages]]
[[Category:Data management]]
[[Category:Query languages|*]]

[[no:Database#Sp&#248;rrespr&#229;k]]</text>
      <sha1>8w19fmrpyiy8mhdih5bhd7ci3460h3e</sha1>
    </revision>
  </page>
  <page>
    <title>White pages schema</title>
    <ns>0</ns>
    <id>1462050</id>
    <revision>
      <id>718116231</id>
      <parentid>688509110</parentid>
      <timestamp>2016-05-01T16:45:22Z</timestamp>
      <contributor>
        <username>Wiae</username>
        <id>3495083</id>
      </contributor>
      <minor />
      <comment>to log into -&gt; to log in to</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3746" xml:space="preserve">{{Unreferenced|date=December 2009}}
A '''white pages schema''' is a [[data model]], specifically a [[logical schema]], for organizing the data contained in entries in a [[directory service]], database, or application, such as an address book.  In a white pages directory, each entry typically represents an individual [[user (computing)|person that makes use of]] network resources, such as by receiving email or having an account to log in to a system.
In some environments, the schema may also include the representation of organizational divisions, roles, groups, and devices.  The term is derived from the [[white pages]], the listing of individuals in a [[telephone directory]], typically sorted by the individual's home location (e.g. city) and then by
their name.

While many [[Postal Telephone and Telegraph|telephone service providers]] have for decades published a list of their [[subscriber]]s in a [[telephone directory]], and similarly corporations published a list of their employees in an internal directory, it was not until the rise of [[electronic mail]] systems that a requirement for standards for the electronic exchange of [[subscriber]] information between different systems appeared.

A white pages schema typically defines, for each real-world object being represented:

* what attributes of that object are to be represented in the entry for that object
* what relationships of that object to other objects are to be represented
* how is the entry to be named in a [[Directory Information Tree|DIT]]
* how an entry is to be located by a client searching for it
* how similar entries are to be distinguished
* how are entries to be ordered when displayed in a list

One of the earliest attempts to standardize a white pages schema for electronic mail use was in [[X.520]] and [[X.521]], part of the [[X.500]] specifications,
that was derived from the addressing requirements of [[X.400]] and defined a [[Directory Information Tree]] that mirrored the international telephone system, with entries representing residential and organizational subscribers.  This evolved into the [[Lightweight Directory Access Protocol]] standard schema in RFC 2256.  One of the most widely deployed white pages schemas used in LDAP
for representing individuals in an organizational context is '''inetOrgPerson''', defined in RFC 2798, although versions of [[Active Directory]] require a different object class, '''User'''.  Many large organizations have
also defined their own white pages schemas for their employees or customers, as part of their [[Identity management]] architecture.  Converting between data bases and directories using different schemas is often the
function of a [[Metadirectory]], and data interchange standards such as [[Common Indexing Protocol]].

Some early directory deployments suffered due to poor design choices in their white pages schema, such as:

* attributes used for naming purposes were non-unique in large environments (such as a person's common name)
* attributes used for naming purposes were likely to change (such as surnames)
* attributes were included which could lead to [[Identity theft]], such as a [[Social security number]]
* users were required during [[provisioning]] to choose attributes which are unique but still memorable to them

Numerous other proposed schemas exist, both as standalone definitions suitable for use with general purpose
directories, or as embedded into network protocols.

Examples of other generic white pages schemas include [[vCard]], defined in RFC 2426, and [[FOAF (software)|FOAF]].

==See also==
* [[Recognition of human individuals]]

{{DEFAULTSORT:White Pages Schema}}
[[Category:Data modeling]]
[[Category:Data management]]
[[Category:Identity management]]</text>
      <sha1>qvjp24it97x8zxlzne09jkz6v0e94zp</sha1>
    </revision>
  </page>
  <page>
    <title>Content migration</title>
    <ns>0</ns>
    <id>26350658</id>
    <revision>
      <id>732797153</id>
      <parentid>732794863</parentid>
      <timestamp>2016-08-03T09:07:53Z</timestamp>
      <contributor>
        <username>Erik Kennedy</username>
        <id>1115816</id>
      </contributor>
      <comment>Fixed typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5537" xml:space="preserve">{{Multiple issues|
{{primary sources|date=March 2011}}
{{cleanup|date=March 2011}}
}}

'''Content Migration''' is the process of moving information stored on a [[Web content management system]]  (CMS), [[Digital asset management]] (DAM), [[Document management system]] (DMS), or flat HTML based system to a new system. Flat HTML content can entail HTML files, [[Active Server Pages]] (ASP), [[JavaServer Pages]] (JSP), [[PHP]], or content stored in some type of [[HTML]]/[[JavaScript]] based system and can be either static or dynamic content.   

Content Migrations can solve a number of issues ranging from:
* Consolidation from one or more CMS systems into one system to allow for more centralized control, governance of content, and better   Knowledge    management and sharing.
* Reorganizing content due to mergers and acquisitions to assimilate as much content from the source systems for a unified look and feel.
* Converting content that has grown organically either in a CMS or Flat HTML and standardizing the formatting so standards can be applied for a unified branding of the content.

There are many ways to access the content stored in a CMS.  Depending on the CMS vendor they offer either an  [[Application programming interface]] (API), [[Web services]], rebuilding a record by writing [[SQL]] queries, [[XML]] exports, or through the web interface.

# The API&lt;ref name="refname1"/&gt; requires a developer to read and understand how to interact with the source CMS&#8217;s API layer then develop an application that extracts the content and stores it in a database, XML file, or Excel. Once the content is extracted the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
# Most CMSs use a database to store and associate content so if no API exists the SQL programmer must reverse engineer the table structure.  Once the structure is reverse engineered, very complex SQL queries are written to pull all the content from multiple tables into an intermediate table or into some type of [[Comma-separated values]] (CSV) or XML file.   Once the developer has the files or database the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
# XML export creates XML files of the content stored in a CMS but after the files are exported they need to be altered to fit the new scheme of the target CMS system.  This is typically done by a developer by writing some code to do the transformation.
# HTML files, JSP, ASP, PHP, or other application server file formats are the most difficult.  The  structure for Flat HTML files are based on a culmination of  folder structure, HTML file structure, and image locations.  In the early days of content migration, the developer had to use programming languages to parse the html files and save it as structured database, XML or CSV. Typically PERL, JAVA, C++, or C# were used because of the regular expression handling capability.  JSP, ASP, PHP, ColdFusion, and other Application Server technologies usually rely on server side includes to help simplify development but makes it very difficult to migrate content because the content is not assembled until the user looks at it in their web browser.  This makes is very difficult to look at the files and extract the content from the file structure.
# Web Scraping allows users to access most of the content directly from the Web User Interface.  Since a web interface is visual (this is the point of a CMS) some Web Scrapers leverage the UI to extract content and place it into a structure like a Database, XML, or CSV formats.  All CMSs, DAMs, and DMSs use  web interfaces so extracting the content for one or many source sites is basically the same process.  In some cases it is possible to push the content into the new CMS using the web interface but some CMSs use JAVA applets, or Active X Control which are not supported by most web scrapers.  In that case the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
'''The basic content migration flow'''

1. Obtain an inventory of the content.&lt;br /&gt;
2. Obtain an inventory of Binary content like Images, PDFs, CSS files, Office Docs, Flash, and any binary objects.&lt;br /&gt;
3. Find any broken links in the content or content resources.&lt;br /&gt;
4. Determine the Menu Structure of the Content.&lt;br /&gt;
5. Find the parent/sibling connection to the content so the links to other content and resources are not broken when moving them.&lt;br /&gt;
6. Extract the Resources from the pages and store them into a Database or File structure.  Store the reference in a database or a File.&lt;br /&gt;
7. Extract the HTML content from the site and store locally.&lt;br /&gt;
8. Upload the resources to the new CMS either by using the API or the web interface and store the new location in a Database or XML.&lt;br /&gt;
9. Transform the HTML to meet the new CMSs standards and reconnect any resources.&lt;br /&gt;
10. Upload the transformed content into the new system.

== References ==
&lt;references&gt;
&lt;ref name="refname1"&gt;[http://msdn.microsoft.com/en-us/library/ms453426.aspx What the Content Migration APIs Are Not]&lt;/ref&gt;
&lt;/references&gt;

==External links==
* [http://www.cmswire.com/cms/web-publishing/no-small-task-migrating-content-to-a-new-cms-002437.php No Small Task: Migrating Content to a New CMS]

[[Category:Data management]]</text>
      <sha1>2ng6roje0im161ltyhe1e757gide2yi</sha1>
    </revision>
  </page>
  <page>
    <title>Data independence</title>
    <ns>0</ns>
    <id>1786411</id>
    <revision>
      <id>750567967</id>
      <parentid>750555610</parentid>
      <timestamp>2016-11-20T15:31:01Z</timestamp>
      <contributor>
        <username>KylieTastic</username>
        <id>2790592</id>
      </contributor>
      <comment>Reverted [[WP:AGF|good faith]] edits by [[Special:Contributions/182.64.133.11|182.64.133.11]] ([[User talk:182.64.133.11|talk]]): Revert asserting National varieties of English (see [[WP:ENGVAR]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5481" xml:space="preserve">{{multiple issues|
{{Cleanup|date=January 2008}}
{{Unreferenced|date=December 2009}}
}}

'''Data independence''' is the type of [[data]] transparency that matters for a centralised [[Database management system|DBMS]]. It refers to the immunity of user [[application software|applications]] to changes made in the definition and organization of data.

Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues, since there is no difference in the operation carried out against the data.

The data independence and operation independence together gives the feature of [[data abstraction]]. There are two levels of data independence.

==First Level of Data Independence==
The [[logical]] structure of the data is known as the 'schema definition'. In general, if a user application operates on a subset of the [[Attribute (computing)|attributes]] of a [[Relation (database)|relation]], it should not be affected later when new attributes are added to the same relation.
Logical data independence indicates that the conceptual schema can be changed without affecting the existing schemas.

==Second Level of Data Independence==
The physical structure of the data is referred to as "physical data description". Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues since, conceptually, there is no difference in the operations carried out against the data. There are three types of data independence:
# Logical data independence: The ability to change the logical (conceptual) schema without changing the External schema (User View) is called logical data independence. For example, the addition or removal of new entities, attributes, or relationships to the conceptual schema should be possible without having to change existing external schemas or having to rewrite existing application programs.
# Physical data independence: The ability to change the physical schema without changing the logical schema is called physical data independence. For example, a change to the internal schema, such as using different file organization or storage structures, storage devices, or indexing strategy, should be possible without having to change the conceptual or external schemas.
#View level data independence: always independent no effect, because there doesn't exist any other level above view level.

===Data Independence===

Data independence can be explained as follows: Each higher level of the data architecture is immune to changes of the next lower level of the architecture.

The logical scheme stays unchanged even though the storage space or type of some data is changed for reasons of optimization or reorganization. In this external schema does not change. In this internal schema changes may be required due to some physical schema were reorganized here.  Physical data independence is present in most databases and file environment in which hardware storage of encoding, exact location of data on disk,merging of records, so on this are hidden from user.

One of the biggest advantage of databases is data independence. It means we can change the conceptual schema at one level without affecting the data at another level. It also means we can change the structure of a database without affecting the data required by users and programs. This feature was not available in the file oriented approach.

==Data Independence Types==

The ability to modify schema definition in one level without affecting schema definition in the next higher level is called data independence. There are two levels of data independence, they are Physical data independence and Logical data independence.

# Physical data independence is the ability to modify the physical schema without causing application programs to be rewritten. Modifications at the physical level are occasionally necessary to improve performance. It means we change the physical storage/level without affecting the conceptual or external view of the data. The new changes are absorbed by mapping techniques.
# Logical data independence is the ability to modify the logical schema without causing application program to be rewritten. Modifications at the logical level are necessary whenever the logical structure of the database is altered (for example, when money-market accounts are added to banking system).  Logical Data independence means if we add some new columns or remove some columns from table then the user view and programs should not change. For example: consider two users A &amp; B. Both are selecting the fields "EmployeeNumber" and "EmployeeName". If user B adds a new column (e.g. salary) to his table, it will not effect the external view for user A, though the internal schema of the database has been changed for both users A &amp; B. 

Logical data independence is more difficult to achieve than physical data independence, since application programs are heavily dependent on the logical structure of the data that they access.

Physical data independence means we change the physical storage/level without affecting the conceptual or external view of the data. Mapping techniques absorbs the new changes.

==See also==
* [[Network transparency]]
* [[Replication transparency]]
* [[Codd's 12 rules]]
* [[ANSI-SPARC_Architecture]]


{{DEFAULTSORT:Data Independence}}
[[Category:Data management]]</text>
      <sha1>3ywft3bvsfhrckpsdzvtxmy0di1jztg</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed transaction</title>
    <ns>0</ns>
    <id>619053</id>
    <revision>
      <id>729608163</id>
      <parentid>729608081</parentid>
      <timestamp>2016-07-13T11:01:02Z</timestamp>
      <contributor>
        <ip>59.163.27.11</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4129" xml:space="preserve">{{POV|Commitment ordering|date=November 2011}}
A '''distributed transaction''' is a [[database transaction]] in which two or more network hosts are involved. Usually, hosts provide '''transactional resources''', while the '''transaction manager''' is responsible for creating and managing a global transaction that encompasses all operations against such resources. Distributed transactions, as any other [[Database transaction|transactions]], must have all four [[ACID|ACID (atomicity, consistency, isolation, durability)]] properties, where atomicity guarantees all-or-nothing outcomes for the unit of work (operations bundle).

Open Group, a vendor consortium, proposed the [[X/Open XA|X/Open Distributed Transaction Processing (DTP) Model]] (X/Open XA), which became a de facto standard for behavior of transaction model components.

Database are common transactional resources and, often, transactions span a couple of such databases. In this case, a distributed transaction can be seen as a [[database transaction]] that must be [[Synchronization|synchronized]] (or provide [[ACID]] properties) among multiple participating [[database]]s which are [[distributed computing|distributed]] among different physical locations. The [[isolation (computer science)|isolation]] property (the I of ACID) poses a special challenge for multi database transactions, since the (global) [[serializability]] property could be violated, even if each database provides it (see also [[global serializability]]). In practice most commercial database systems use [[Two phase locking|strong strict two phase locking (SS2PL)]] for [[concurrency control]], which ensures global serializability, if all the participating databases employ it. (see also [[commitment ordering]] for multidatabases.)

A common [[algorithm]] for ensuring [[correctness (computer science)|correct]] completion of a distributed transaction is the [[two-phase commit]] (2PC). This algorithm is usually applied for updates able to [[commit (data management)|commit]] in a short period of time, ranging from couple of milliseconds to couple of minutes.

There are also long-lived distributed transactions, for example a transaction to book a trip, which consists of booking a flight, a rental car and a hotel. Since booking the flight might take up to a day to get a confirmation, two-phase commit is not applicable here, it will lock the resources for this long. In this case more sophisticated techniques that involve multiple undo levels are used. The way you can undo the hotel booking by calling a desk and cancelling the reservation, a system can be designed to undo certain operations (unless they are irreversibly finished).

In practice, long-lived distributed transactions are implemented in systems based on [[Web Services]]. Usually these transactions utilize principles of [[Compensating transaction]]s, Optimism and Isolation Without Locking. X/Open standard does not cover long-lived DTP.

Several modern technologies, including [[Enterprise Java Beans]] (EJBs) and [[Microsoft Transaction Server]] (MTS) fully support distributed transaction standards.

==See also==
* [[Java Transaction API|Java Transaction API (JTA)]]
* [[Enduro/X|Enduro/X Open source X/Open XA and XATMI implementation]]

==References==
* {{cite web | title=Web-Services Transactions | work=Web-Services Transactions | url=http://xml.sys-con.com/read/43755.htm | accessdate=May 2, 2005 }}
* {{cite web | title=Nuts And Bolts Of Transaction Processing | work=Article about Transaction Management | url=http://www.subbu.org/articles/transactions/NutsAndBoltsOfTP.html
| accessdate=May 3, 2005 }}
* {{cite web | title=A Detailed Comparison of Enterprise JavaBeans (EJB) &amp; The Microsoft Transaction Server (MTS) Models
 | url=http://gsraj.tripod.com/misc/ejbmtscomp.html }}

==Further reading==
* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8

{{DEFAULTSORT:Distributed Transaction}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>623uoin2ix7w9v0uywdhsi9ebvmjwx6</sha1>
    </revision>
  </page>
  <page>
    <title>Storage area network</title>
    <ns>0</ns>
    <id>20444608</id>
    <revision>
      <id>753620028</id>
      <parentid>753516739</parentid>
      <timestamp>2016-12-08T06:57:07Z</timestamp>
      <contributor>
        <username>G&#252;nniX</username>
        <id>237572</id>
      </contributor>
      <comment>Undid revision 753516739 by [[Special:Contributions/203.123.36.183|203.123.36.183]] ([[User talk:203.123.36.183|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16606" xml:space="preserve">{{Distinguish|Network-attached storage}}
{{Use dmy dates|date=February 2013}}
{{Area networks}}

A '''storage area network''' ('''SAN''') &lt;ref&gt;{{cite web |url=http://cctvinstitute.co.uk/storage-area-network/|title=Storage Area Network by Noor Ul Mushtaq }}&lt;/ref&gt; is a network which provides access to consolidated, [[Block device|block level data storage]]. SANs are primarily used to enhance storage devices, such as [[disk array]]s, [[tape library|tape libraries]], and [[optical jukebox]]es, accessible to [[Server (computing)|server]]s so that the devices appear to the [[operating system]] as [[Direct-attached storage|locally attached devices]]. A SAN typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.

A SAN does not provide file abstraction, only block-level operations. However, [[file systems]] built on top of SANs do provide file-level access, and are known as [[shared-disk file system]]s.

== Storage ==
{{Refimprove section|date=February 2014}}
Historically, [[data centers]] first created "islands" of [[SCSI]] [[disk array]]s as [[direct-attached storage]] (DAS), each dedicated to an application, often visible as a number of "virtual hard drives" addressed as [[Logical Unit Number]]s (LUNs).&lt;ref&gt;{{cite web |url=http://www.novell.com/documentation/oes/stor_ovw/?page=/documentation/oes/stor_ovw/data/ami6rr0.html |title=Novel Doc: OES 1 - Direct Attached Storage Solutions}}&lt;/ref&gt; Essentially, a SAN consolidates such storage islands together using a high-speed network.

Operating systems maintain their own [[file system]]s on their own dedicated, non-shared LUNs, as though they were local to themselves. If multiple systems were simply to attempt to share a LUN, these would interfere with each other and quickly corrupt the data. Any planned sharing of data on different computers within a LUN requires software, such as [[SAN file system]]s or [[clustered computing]].

Despite such issues, SANs help to increase storage capacity utilization, since multiple servers consolidate their private storage space onto the disk arrays.
Common uses of a SAN include provision of transactionally accessed data that require high-speed [[block device|block-level access]] to the hard drives such as email servers, databases, and high usage file servers.

===SAN compared to NAS===
[[Network-attached storage]] (NAS) was designed independently of SAN systems. In both a NAS and SAN, the various computers in a network, such as individual users' desktop computers and dedicated servers running applications ("[[application server]]s"), can share a more centralized collection of storage devices via a network connection such as a [[local area network]] (LAN).

Concentrating the storage on one or more NAS servers or in a SAN instead of placing storage devices on each application server allows application server configurations to be optimized for running their applications instead of also storing all the related data and moves the storage management task to the NAS or SAN system. Both NAS and SAN have the potential to reduce the amount of excess storage that must be purchased and provisioned as spare space. In a DAS-only architecture, each computer must be provisioned with enough excess storage to ensure that the computer does not run out of space at an untimely moment. In a DAS architecture the spare storage on one computer cannot be utilized by another. With a NAS or SAN architecture, where storage is shared across the needs of multiple computers, one normally provisions a pool of shared spare storage that will serve the peak needs of the connected computers, which typically is less than the total amount of spare storage that would be needed if individual storage devices were dedicated to each computer.

In a NAS the storage devices are directly connected to a file server that makes the storage available at a file-level to the other computers. In a SAN, the storage is made available at a lower "block-level", leaving file system concerns to the "client" side. SAN protocols include [[Fibre Channel]], [[iSCSI]], [[ATA over Ethernet]] (AoE) and [[HyperSCSI]]. One way to loosely conceptualize the difference between a NAS and a SAN is that NAS appears to the client OS (operating system) as a file server (the client can map network drives to shares on that server) whereas a disk available through a SAN still appears to the client OS as a disk, visible in disk and volume management utilities (along with client's local disks), and available to be formatted with a file system and mounted.

One drawback to both the NAS and SAN architecture is that the connection between the various CPUs and the storage units are no longer dedicated high-speed busses tailored to the needs of storage access. Instead the CPUs use the LAN to communicate, potentially creating bandwidth as well as performance bottlenecks. Additional data security considerations are also required for NAS and SAN setups, as information is being transmitted via a network that potentially includes design flaws, security exploits and other vulnerabilities that may not exist in a DAS setup.

While it is possible to use the NAS or SAN approach to eliminate all storage at user or application computers, typically those computers still have some local Direct Attached Storage for the operating system, various program files and related temporary files used for a variety of purposes, including [[cache (computing)|caching]] content locally.

To understand their differences, a comparison of SAN, DAS and NAS architectures may be helpful.&lt;ref name="eval"&gt;{{cite web |title= Storage Architectures: DAS, SAN, NAS, iSCSI SAN |work= Marketing web site |url=  http://www.evaluatorgroup.com/document/storage-architectures/ |publisher= Evaluator Group |archivedate= September 17, 2016 |archiveurl= https://web.archive.org/web/20160917144751/http://www.evaluatorgroup.com/document/storage-architectures/ |accessdate= November 10, 2016 }}&lt;/ref&gt;

===SAN-NAS hybrid===
[[Image:Compingles3.png|right|thumb|260px|Hybrid using SAN, [[Direct-attached storage|DAS]] and NAS technologies.]]
Despite their differences, SAN and NAS are not mutually exclusive, and may be combined as a SAN-NAS hybrid, offering both file-level protocols (NAS) and block-level protocols (SAN) from the same system. An example of this is [[Openfiler]], a free software product running on Linux-based systems. A shared disk file system can also be run on top of a SAN to provide filesystem services.

== Benefits ==
Sharing storage usually simplifies storage administration and adds flexibility since cables and storage devices do not have to be physically moved to shift storage from one server to another.

Other benefits include the ability to allow servers to boot from the SAN itself. This allows for a quick and easy replacement of faulty servers since the SAN can be reconfigured so that a replacement server can use the [[Logical Unit Number|LUN]] of the faulty server. While this area of technology is still new, many view it as being the future of the enterprise datacenter.&lt;ref&gt;{{cite web | title=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | url=http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx | work=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | date=31 October 2008 | accessdate=2010-01-28}}&lt;/ref&gt;

SANs also tend to enable more effective [[disaster recovery]] processes. A SAN could span a distant location containing a secondary storage array. This enables [[storage replication]] either implemented by [[disk array controller]]s, by server software, or by specialized SAN devices. Since IP [[Wide area network|WAN]]s are often the least costly method of long-distance transport, the [[Fibre Channel over IP]] (FCIP) and iSCSI protocols have been developed to allow SAN extension over IP networks. The traditional physical SCSI layer could support only a few meters of distance - not nearly enough to ensure business continuance in a disaster.

The economic consolidation of disk arrays has accelerated the advancement of several features including I/O caching, [[Snapshot (computer storage)|snapshotting]], and volume cloning ([[Business Continuance Volumes]] or BCVs).

==Network types==
Most storage networks use the [[SCSI]] protocol for communication between servers and disk drive devices. A mapping layer to other protocols is used to form a network:

* [[ATA over Ethernet|ATA over Ethernet (AoE)]], mapping of [[AT Attachment|ATA]] over [[Ethernet]]
* [[Fibre Channel Protocol]] (FCP), the most prominent one, is a mapping of SCSI over [[Fibre Channel]]
* [[Fibre Channel over Ethernet]] (FCoE)
* [[ESCON]] over Fibre Channel ([[FICON]]), used by [[mainframe computer]]s
* [[HyperSCSI]], mapping of SCSI over Ethernet
* [[iFCP]]&lt;ref&gt;{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=IPstorage |title=TechEncyclopedia: IP Storage |accessdate=2007-12-09}}&lt;/ref&gt; or [[SANoIP]]&lt;ref&gt;{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=SANoIP |title=TechEncyclopedia: SANoIP |accessdate=2007-12-09}}&lt;/ref&gt; mapping of FCP over IP
* [[iSCSI]], mapping of SCSI over [[TCP/IP]]
* [[iSCSI Extensions for RDMA]] (iSER), mapping of iSCSI over [[InfiniBand]]

Storage networks may also be built using [[Serial attached SCSI|SAS]] and [[Serial ATA|SATA]] technologies. SAS evolved from SCSI direct-attached storage. SATA evolved from [[Parallel ATA|IDE]] direct-attached storage. SAS and SATA devices can be networked using [[Serial attached SCSI#SAS expanders|SAS Expanders]].

Examples of stacked protocols using SCSI:

{| class="wikitable" style="text-align:center"
| colspan="5" | Applications
|-
| colspan="5" | [[SCSI]] Layer
|-
| rowspan="4" | [[Fibre Channel Protocol|FCP]]
| rowspan="3" | [[Fibre Channel Protocol|FCP]]
| [[Fibre Channel Protocol|FCP]]
| [[Fibre Channel Protocol|FCP]]
| rowspan="2" | [[iSCSI]]
|-
| [[Fibre Channel over IP|FCIP]]
| [[Internet Fibre Channel Protocol|iFCP]]
|-
| colspan="3" | [[Internet Protocol|TCP]]
|-
| [[Fibre Channel over Ethernet|FCoE]]
| colspan="3" | [[Internet Protocol|IP]]
|-
| [[Fibre Channel|FC]]
| colspan="4" | [[Ethernet]]
|}

== SAN infrastructure ==
[[Image:ML-QLOGICNFCCONN.JPG|thumb| [[Qlogic]] SAN-[[Fibre Channel switch|switch]] with optical [[Fibre Channel]] [[Electrical connector|connectors]] installed.]]
SANs often use a [[Fibre Channel fabric]] topology, an infrastructure specially designed to handle storage communications. It provides faster and more reliable access than higher-level protocols used in [[Network-attached storage|NAS]]. A fabric is similar in concept to a [[network segment]] in a local area network. A typical Fibre Channel SAN fabric is made up of a number of [[Fibre Channel switch]]es.

Many SAN equipment vendors also offer some form of Fibre Channel routing, and these can allow data to cross between different fabrics without merging them. These offerings use proprietary protocol elements, and the top-level architectures being promoted are radically different.
For example, they might map Fibre Channel traffic over IP or over [[Synchronous optical networking|SONET/SDH]].

== Compatibility ==
One of the early problems with Fibre Channel SANs was that the switches and other hardware from different manufacturers were not compatible. Although the basic storage protocol FCP was standard, some of the higher-level functions did not interoperate well. Similarly, many host operating systems would react badly to other operating systems sharing the same fabric.{{Citation needed|date=October 2010}}.

== In media and entertainment ==
[[Video editing]] systems require very high data transfer rates and very low latency.
SANs in media and entertainment are often referred to as serverless due to the nature of the configuration which places the video workflow (ingest, editing, playout) desktop clients directly on the SAN rather than attaching to servers. Control of data flow is managed by a distributed file system such as StorNext by Quantum.&lt;ref&gt;{{cite web|url=http://www.quantum.com/products/software/stornext/index.aspx |title=StorNext Storage Manager - High-speed file sharing, Data Management and Digital Archiving Software |publisher=Quantum.com |date= |accessdate=2013-07-08}}&lt;/ref&gt;

Per-node bandwidth usage control, sometimes referred to as [[quality of service]] (QoS), is especially important in video editing as it ensures fair and prioritized bandwidth usage across the network.

==Storage virtualization==
{{main|Storage virtualization}}
[[Storage virtualization]] is the process of abstracting logical storage from physical storage. The physical storage resources are aggregated into storage pools, from which the logical storage is created. It presents to the user a logical space for data storage and transparently handles the process of mapping it to the physical location, a concept called [[location transparency]]. This is implemented in modern disk arrays, often using vendor proprietary technology. However, the goal of storage virtualization is to group multiple disk arrays from different vendors, scattered over a network, into a single storage device.  The single storage device can then be managed uniformly. {{Citation needed|date=September 2011}}

==Quality of service==
SAN Storage QoS enables the desired storage performance to be calculated and maintained for network customers accessing the device.
Some factors that affect SAN QoS are:

*[[Bandwidth (computing)|Bandwidth]] &#8211; The rate of data throughput available on the system.
*[[Latency (engineering)|Latency]] &#8211; The time delay for a read/write operation to execute.
*Queue depth &#8211; The number of outstanding operations waiting to execute to the underlying disks (traditional or [[solid-date drive]]s).

QoS can be impacted in a SAN storage system by unexpected increase in data traffic (usage spike) from one network user that can cause performance to decrease for other users on the same network. This can be known as the &#8220;noisy neighbor effect.&#8221; When QoS services are enabled in a SAN storage system, the &#8220;noisy neighbor effect&#8221; can be prevented and network storage performance can be accurately predicted.

Using SAN storage QoS is in contrast to using disk over-provisioning in a SAN environment. Over-provisioning can be used to provide additional capacity to compensate for peak network traffic loads. However, where network loads are not predictable, over-provisioning can eventually cause all bandwidth to be fully consumed and latency to increase significantly resulting in SAN performance degradation.

== See also ==
* [[ATA over Ethernet]] (AoE)
* [[Direct-attached storage]] (DAS)
* [[Disk array]]
* [[Fibre Channel]]
* [[Fibre Channel over Ethernet]]
* [[File Area Network]]
* [[Host Bus Adapter]] (HBA)
* [[iSCSI]]
* [[iSCSI Extensions for RDMA]]
* [[List of networked storage hardware platforms]]
* [[List of storage area network management systems]]
* [[Massive array of idle disks]] (MAID)
* [[Network-attached storage]] (NAS)
* [[Redundant array of independent disks]] (RAID)
* [[SCSI RDMA Protocol]] (SRP)
* [[Storage Management Initiative &#8211; Specification]] &#8212; (SMI-S)
* [[Storage hypervisor]]
* [[Storage Resource Management]] (SRM)
* [[Storage virtualization]]
* [[System area network]]

==References==
{{More footnotes|date=June 2008}}
&lt;references/&gt;

==External links==
&lt;!-- ATTENTION! Please do not add links without discussion and consensus on the talk page. Undiscussed links will be removed. --&gt;
* [https://www.redbooks.ibm.com/redbooks/pdfs/sg245470.pdf Introduction to Storage Area Networks Exhaustive Introduction into SAN, [[IBM Redbooks|IBM Redbook]]]
* [http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx SAN vs. DAS: A Cost Analysis of Storage in the Enterprise]
* [http://searchstorage.techtarget.co.uk/generic/0,295582,sid181_gci1516893,00.html SAS and SATA, solid-state storage lower data center power consumption]
* [https://www.youtube.com/playlist?list=PLivYD7W2z2HMGGRIwRoRcqLL4HMpR1dIe SAN NAS Videos]
* [http://www.storageareanetworkinfo.blogspot.com.ar/ Storage Area Network Info]

&lt;!--Interwikies--&gt;

{{Authority control}}

{{DEFAULTSORT:Storage Area Network}}
[[Category:Data management]]
[[Category:Telecommunications engineering]]
[[Category:Storage area networks| ]]</text>
      <sha1>mrvn4qp149psc8frps2dbd1v11ihsm3</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic integration</title>
    <ns>0</ns>
    <id>4381551</id>
    <revision>
      <id>757142095</id>
      <parentid>733703850</parentid>
      <timestamp>2016-12-29T01:17:14Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Applications and Methods */ cap, punct.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3407" xml:space="preserve">{{Unreferenced|date=October 2013}}
'''Semantic integration''' is the process of interrelating information from diverse sources, for example calendars and to do lists, email archives, presence information (physical, psychological, and social), documents of all sorts, contacts (including [[social graph]]s), search results, and advertising and marketing relevance derived from them. In this regard, [[semantics]] focuses on the organization of and action upon [[information]] by acting as an intermediary between heterogeneous data sources, which may conflict not only by structure but also context or value.

==Applications and methods==

In [[enterprise application integration]] (EAI), semantic integration can facilitate or even automate the communication between computer systems using [[metadata publishing]]. Metadata publishing potentially offers the ability to automatically link [[ontology (computer science)|ontologies]]. One approach to (semi-)automated ontology mapping requires the definition of a semantic distance or its inverse, [[semantic similarity]] and appropriate rules. Other approaches include so-called ''lexical methods'', as well as methodologies that rely on exploiting the structures of the ontologies.  For explicitly stating similarity/equality, there exist special properties or relationships in most ontology languages. [[Web Ontology Language|OWL]], for example has "owl:equivalentClass", "owl:equivalentProperty" and "owl:sameAs".

Eventually system designs may see the advent of composable architectures where published semantic-based interfaces are joined together to enable new and meaningful capabilities{{Citation needed|date=February 2014}}. These could predominately be described by means of design-time declarative specifications, that could ultimately be rendered and executed at run-time{{Citation needed|date=February 2014}}.

Semantic integration can also be used to facilitate design-time activities of interface design and mapping. In this model, semantics are only explicitly applied to design and the run-time systems work at the [[syntax]] level{{Citation needed|date=February 2014}}. This  "early semantic binding" approach can improve overall system performance while retaining the benefits of semantic driven design{{Citation needed|date=February 2014}}.

==Examples==

The [[Pacific Symposium on Biocomputing]] has been a venue for the popularization of the ontology mapping task in the biomedical domain, and a number of papers on the subject can be found in its proceedings.

==See also==
* [[Data integration]]
* [[Dataspaces]]
* [[Enterprise integration]]
* [[Ontology-based data integration]]
* [[Ontology matching]]
* [[Semantic heterogeneity]]
* [[Semantic translation]]
* [[Semantic unification]]

== References ==
{{Reflist|2}}

==External links==
*[https://web.archive.org/web/20070811204850/http://zapthink.com/report.html?id=ZapFlash-08082003 Semantic Integration: Loosely Coupling the Meaning of Data]
*[http://drops.dagstuhl.de/opus/volltexte/2005/40/ Ontology Mapping: The State of the Art] (2005 paper)
*[http://arxiv.org/ftp/arxiv/papers/0901/0901.4934.pdf 2010 paper by Carl Hewitt]
*[http://wwwhome.portavita.nl/~yeb/ooi.pdf OpenCyc to Oracle Interface]

{{Semantic Web}}

{{DEFAULTSORT:Semantic Integration}}
[[Category:Ontology (information science)]]
[[Category:Data management]]
[[Category:Semantics]]
[[Category:Bioinformatics]]</text>
      <sha1>adkrua5pgsihmag7zx6uj04dzj2krac</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft Query</title>
    <ns>0</ns>
    <id>684176</id>
    <revision>
      <id>750706969</id>
      <parentid>750706811</parentid>
      <timestamp>2016-11-21T10:50:33Z</timestamp>
      <contributor>
        <ip>62.252.8.106</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1541" xml:space="preserve">{{Refimprove|date=December 2009}}

{{wikibooks|Structured Query Language}}
{{wikibooks|SQL dialects reference}}

'''Microsoft Query''' is a visual method of creating [[database query|database queries]] using examples based on a text string, the name of a [[document]] or a list of documents. The QBE system converts the user input into a formal database query using [[SQL|Structured Query Language]] (SQL) on the backend, allowing the user to perform powerful searches without having to explicitly compose them in SQL, and without even needing to know SQL. It is derived from Mosh&#233; M. Zloof's original [[Query by Example]] (QBE) implemented in the mid-1970s at [[IBM]]'s Research Centre in [[Yorktown, New York]].&lt;ref&gt;Zloof, M. M., [http://dx.doi.org/10.1147/sj.164.0324 Query-by-Example: A data base language]&lt;/ref&gt;

In the context of [[Microsoft Access]], QBE is used for introducing students to database querying, and as a user-friendly [[database management system]] for small businesses.

[[Microsoft Excel]] allows results of QBE queries to be embedded in spreadsheets.&lt;ref&gt;[https://support.office.com/en-us/article/Use-Microsoft-Query-to-retrieve-external-data-42a2ea18-44d9-40b3-9c38-4c62f252da2e Use Microsoft Query to retrieve external data]&lt;/ref&gt;

==See also==
*[[Query by Example]]
*[[Microsoft Access]]
*[[Microsoft SQL Server]]

==References==
{{Reflist}}

{{DEFAULTSORT:Microsoft Query By Example}}
[[Category:Data management]]
[[Category:Microsoft database software]]


{{Microsoft-software-stub}}
{{database-software-stub}}</text>
      <sha1>hk44rthjmzd9x7mdmuvmip02wmbxias</sha1>
    </revision>
  </page>
  <page>
    <title>Commitment ordering</title>
    <ns>0</ns>
    <id>4379212</id>
    <revision>
      <id>749431938</id>
      <parentid>748688337</parentid>
      <timestamp>2016-11-14T08:23:42Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <minor />
      <comment>/* Strict CO (SCO) */change |id={{citeseerx}} to |citeseerx= using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="94670" xml:space="preserve">{{multiple issues|
{{expert subject|computer science|date=October 2012|reason=it is impossible to copy edit the article in its current state}}
{{notability|date=December 2011}}
{{more footnotes|date=November 2011}}
{{technical|date=November 2011}}
{{essay-like|date=November 2011}}
}}

'''Commitment ordering''' ('''CO''') is a class of interoperable ''[[serializability]]'' techniques in [[concurrency control]] of [[database]]s, [[transaction processing]], and related applications. It allows [[Serializability#Optimistic versus pessimistic techniques|optimistic]] (non-blocking) implementations. With the proliferation of [[multi-core processor]]s, CO has been also increasingly utilized in [[Concurrent computing|concurrent programming]], [[transactional memory]], and especially in [[software transactional memory]] (STM) for achieving serializability [[Optimistic concurrency control|optimistically]]. CO is also the name of the resulting transaction [[Schedule (computer science)|schedule]] (history) property, which was originally defined in 1988 with the name ''dynamic atomicity''.&lt;ref name=Fekete1988&gt;Alan Fekete, [[Nancy Lynch]], Michael Merritt, William Weihl (1988): [http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA200980&amp;Location=U2&amp;doc=GetTRDoc.pdf ''Commutativity-based locking for nested transactions'' (PDF)] MIT, LCS lab, Technical report MIT/LCS/TM-370, August 1988.&lt;/ref&gt; In a CO compliant schedule the chronological order of commitment events of transactions is compatible with the [[Serializability#Testing conflict serializability|precedence]] order of the respective transactions. CO is a broad special case of ''[[serializability#View and conflict serializability|conflict serializability]]'', and effective means ([[Reliability engineering|reliable]], high-performance, distributed, and [[Scalability|scalable]]) to achieve [[global serializability]] (modular serializability) across any collection of database systems that possibly use different concurrency control mechanisms (CO also makes each system serializability compliant, if not already).

Each not-CO-compliant database system is augmented with a CO component (the commitment order coordinator&#8212;COCO) which orders the commitment events for CO compliance, with neither data-access nor any other transaction operation interference. As such CO provides a low overhead, general solution for global serializability (and distributed serializability), instrumental for [[global concurrency control]] (and [[distributed concurrency control]]) of multi database systems and other [[transactional object]]s, possibly highly distributed (e.g., within [[cloud computing]], [[grid computing]], and networks of [[smartphone]]s). An [[atomic commitment protocol]] (ACP; of any type) is a fundamental part of the solution, utilized to break global cycles in the conflict (precedence, serializability) graph. CO is the most general property (a [[necessary condition]]) that guarantees global serializability, if the database systems involved do not share concurrency control information beyond atomic commitment protocol (unmodified) messages, and have no knowledge whether transactions are global or local (the database systems are ''autonomous''). Thus CO (with its variants) is the only general technique that does not require the typically costly distribution of local concurrency control information (e.g., local precedence relations, locks, timestamps, or tickets). It generalizes the popular ''[[Two-phase locking|strong strict two-phase locking]]'' (SS2PL) property, which in conjunction with the ''[[two-phase commit protocol]]'' (2PC) is the [[de facto standard]] to achieve global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join such SS2PL based solutions for global serializability.

In addition, locking based ''global deadlocks'' are resolved automatically in a CO based multi-database environment, an important side-benefit (including the special case of a completely SS2PL based environment; a previously unnoticed fact for SS2PL).

Furthermore, '''strict commitment ordering''' (SCO; [[#Raz1991c|Raz 1991c]]), the intersection of ''[[Schedule (computer science)#Strict|Strictness]]'' and CO, provides better performance (shorter average transaction completion time and resulting better transaction [[throughput]]) than SS2PL whenever read-write conflicts are present (identical blocking behavior for write-read and write-write conflicts; comparable locking overhead). The advantage of SCO is especially significant during lock contention. Strictness allows both SS2PL and SCO to use the same effective ''database recovery'' mechanisms.

Two major generalizing variants of CO exist, '''extended CO''' (ECO; [[#Raz1993a|Raz 1993a]]) and '''multi-version CO''' (MVCO; [[#Raz1993b|Raz 1993b]]). They as well provide global serializability without local concurrency control information distribution, can be combined with any relevant concurrency control, and allow optimistic (non-blocking) implementations. Both use additional information for relaxing CO constraints and achieving better concurrency and performance. '''Vote ordering''' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]) is a container schedule set (property) and technique for CO and all its variants. Local VO is a necessary condition for guaranteeing global serializability, if the atomic commitment protocol (ACP) participants do not share concurrency control information (have the ''generalized autonomy'' property). CO and its variants inter-operate transparently, guaranteeing global serializability and automatic global deadlock resolution also together in a mixed, heterogeneous environment with different variants.

==Overview==

The ''Commitment ordering'' (CO; [[#Raz1990|Raz 1990]], [[#Raz1992|1992]], [[#Raz1994|1994]], [[#Raz2009|2009]]) schedule property has been referred to also as ''Dynamic atomicity'' (since 1988&lt;ref name=Fekete1988/&gt;), ''commit ordering'', ''commit order serializability'',  and ''strong recoverability'' (since 1991). The latter is a misleading name since CO is incomparable with ''[[serializability#Correctness - recoverability|recoverability]]'', and the term "strong" implies a special case. This means that a schedule with a strong recoverability property does not necessarily have the CO property, and vice versa.

In 2009 CO has been characterized as a major concurrency control method, together with the previously known (since the 1980s) three major methods: ''Locking'', ''Time-stamp ordering'', and ''Serialization graph testing'', and as an enabler for the interoperability of systems using different concurrency control mechanisms.&lt;ref name=Bern2009&gt;[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (pages 145, 360)&lt;/ref&gt;

In a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple and possibly [[Distributed database]]s. Enforcing [[global serializability]] in such system is problematic. Even if every local schedule of a single database is serializable, still, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach conflict serializability would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. The problem of achieving global serializability effectively had been characterized as [[open problem|open]] until the public disclosure of CO in 1991 by its [[Invention|inventor]] [[Yoav Raz]] ([[#Raz1991a|Raz 1991a]]; see also [[Global serializability]]).

Enforcing CO is an effective way to enforce conflict serializability globally in a distributed system, since enforcing CO locally in each database (or other transactional object) also enforces it globally. Each database may use any, possibly different, type of concurrency control mechanism. With a local mechanism that already provides conflict serializability, enforcing CO locally does not cause any additional aborts, since enforcing CO locally does not affect the data access scheduling strategy of the mechanism (this scheduling determines the serializability related aborts; such a mechanism typically does not consider the commitment events or their order). The CO solution requires no communication overhead, since it uses (unmodified) ''[[atomic commitment]]'' protocol messages only, already needed by each distributed transaction to reach atomicity. An atomic commitment protocol plays a central role in the distributed CO algorithm, which enforces CO globally, by breaking global cycles (cycles that span two or more databases) in the global conflict graph.
CO, its special cases, and its generalizations are interoperable, and achieve global serializability while transparently being utilized together in a single heterogeneous distributed environment comprising objects with possibly different concurrency control mechanisms. As such, ''Commitment ordering'', including its special cases, and together with its generalizations (see CO variants below), provides a general, high performance, fully distributed solution (no central processing component or central data structure are needed) for guaranteeing global serializability in heterogeneous environments of multidatabase systems and other multiple transactional objects (objects with states accessed and modified only by transactions; e.g., in the framework of [[transactional processes]], and within Cloud computing and Grid computing). The CO solution scales up with network size and the number of databases without any negative impact on performance (assuming the statistics of a single distributed transaction, e.g., the average number of databases involved with a single transaction, are unchanged).

With the proliferation of [[Multi-core processor]]s, Optimistic CO (OCO) has been also increasingly utilized to achieve serializability in software transactional memory, and numerous STM articles and patents utilizing "commit order" have already been published (e.g., Zhang et al. 2006&lt;ref name=Zhang2006/&gt;).

==The commitment ordering solution for global serializability==

===General characterization of CO===

''Commitment ordering'' (CO) is a special case of conflict serializability. CO can be enforced with ''non-blocking'' mechanisms (each transaction can complete its task without having its data-access blocked, which allows [[optimistic concurrency control]]; however, commitment could be blocked). In a CO schedule the commitment events' ([[partial order|partial]]) precedence order of the transactions corresponds to the precedence (partial) order of the respective transactions in the ([[directed graph|directed]]) conflict graph (precedence graph, serializability graph), as induced by their conflicting access operations (usually read and write (insert/modify/delete) operations; CO also applies to higher level operations, where they are conflicting if [[noncommutative]], as well as to conflicts between operations upon multi-version data).

;Definition{{colon}} commitment ordering: Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be two ''committed'' transactions in a schedule, such that &lt;math&gt;T_{2}&lt;/math&gt; is ''in a conflict'' with &lt;math&gt;T_{1}&lt;/math&gt; (&lt;math&gt;T_{1}&lt;/math&gt; ''precedes'' &lt;math&gt;T_{2}&lt;/math&gt;). The schedule has the '''Commitment ordering''' (CO) property, if for every two such transactions &lt;math&gt;T_{1}&lt;/math&gt; commits before &lt;math&gt;T_{2}&lt;/math&gt; commits.

The commitment decision events are generated by either a local commitment mechanism, or an atomic commitment protocol, if different processes need to reach consensus on whether to commit or abort. The protocol may be distributed or centralized. Transactions may be committed concurrently, if the commit partial order allows (if they do not have conflicting operations). If different conflicting operations induce different partial orders of same transactions, then the conflict graph has [[cycle (graph theory)|cycles]], and the schedule will violate serializability when all the transactions on a cycle are committed. In this case no partial order for commitment events can be found. Thus, cycles in the conflict graph need to be broken by aborting transactions. However, any conflict serializable schedule can be made CO without aborting any transaction, by properly delaying commit events to comply with the transactions' precedence partial order.

CO enforcement by itself is not sufficient as a concurrency control mechanism, since CO lacks the recoverability property, which should be supported as well.

===The distributed CO algorithm===

A fully distributed ''Global commitment ordering'' enforcement algorithm exists, that uses local CO of each participating database, and needs only (unmodified) Atomic commitment protocol messages with no further communication. The distributed algorithm is the combination of local (to each database) CO algorithm processes, and an atomic commitment protocol (which can be fully distributed).
Atomic commitment protocol is essential to enforce atomicity of each distributed transaction (to decide whether to commit or abort it; this procedure is always carried out for distributed transactions, independently of concurrency control and CO). A common example of an atomic commitment protocol is the ''[[two-phase commit protocol]]'', which is resilient to many types of system failure. In a reliable environment, or when processes usually fail together (e.g., in the same [[integrated circuit]]), a simpler protocol for atomic commitment may be used (e.g., a simple handshake of distributed transaction's participating processes with some arbitrary but known special participant, the transaction's coordinator, i.e., a type of ''one-phase commit'' protocol). An atomic commitment protocol reaches consensus among participants on whether to ''commit'' or ''abort'' a distributed (global) transaction that spans these participants. An essential stage in each such protocol is the '''YES vote''' (either explicit, or implicit) by each participant, which means an obligation of the voting participant to obey the decision of the protocol, either commit or abort. Otherwise a participant can unilaterally abort the transaction by an explicit NO vote. The protocol commits the transaction only if YES votes have been received from ''all'' participants, and thus typically a missing YES vote of a participant is considered a NO vote by this participant. Otherwise the protocol aborts the transaction. The various atomic commit protocols only differ in their abilities to handle different computing environment failure situations, and the amounts of work and other computing resources needed in different situations.

The entire CO solution for global serializability is based on the fact that in case of a missing vote for a distributed transaction, the atomic commitment protocol eventually aborts this transaction.

====Enforcing global CO====

In each database system a local CO algorithm determines the needed commitment order for that database. By the characterization of CO above, this order depends on the local precedence order of transactions, which results from the local data access scheduling mechanisms. Accordingly, YES votes in the atomic commitment protocol are scheduled for each (unaborted) distributed transaction (in what follows "a vote" means a YES vote). If a precedence relation (conflict) exists between two transactions, then the second will not be voted on before the first is completed (either committed or aborted), to prevent possible commit order violation by the atomic commitment protocol. Such can happen since the commit order by the protocol is not necessarily the same as the voting order. If no precedence relation exists, both can be voted on concurrently. This ''vote ordering strategy'' ensures that also the atomic commitment protocol maintains commitment order, and it is a ''necessary condition'' for guaranteeing Global CO (and the local CO of a database; without it both Global CO and Local CO (a property meaning that each database is CO compliant) may be violated).

However, since database systems schedule their transactions independently, it is possible that the transactions' precedence orders in two databases or more are not compatible (no global partial order exists that can [[Embedding|embed]] the respective local partial orders together). With CO precedence orders are also the commitment orders. When participating databases in a same distributed transaction do not have compatible local precedence orders for that transaction (without "knowing" it; typically no coordination between database systems exists on conflicts, since the needed communication is massive and unacceptably degrades performance) it means that the transaction resides on a global cycle (involving two or more databases) in the global conflict graph. In this case the atomic commitment protocol will fail to collect all the votes needed to commit that transaction: By the ''vote ordering strategy'' above at least one database will delay its vote for that transaction indefinitely, to comply with its own commitment (precedence) order, since it will be waiting to the completion of another, preceding transaction on that global cycle, delayed indefinitely by another database with a different order. This means a '''''voting-[[deadlock]]''''' situation involving the databases on that cycle.
As a result, the protocol will eventually abort some deadlocked transaction on this global cycle, since each such transaction is missing at least one participant's vote. Selection of the specific transaction on the cycle to be aborted depends on the atomic commitment protocol's abort policies (a [[timeout (telecommunication)|timeout]] mechanism is common, but it may result in more than one needed abort per cycle; both preventing unnecessary aborts and abort time shortening can be achieved by a dedicated abort mechanism for CO). Such abort will break the global cycle involving that distributed transaction. Both deadlocked transactions and possibly other in conflict with the deadlocked (and thus blocked) will be free to be voted on. It is worthwhile noting that each database involved with the voting-deadlock continues to vote regularly on transactions that are not in conflict with its deadlocked transaction, typically almost all the outstanding transactions. Thus, in case of incompatible local (partial) commitment orders, no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause of incompatibility. This means that the above ''vote ordering strategy'' is also a ''sufficient condition'' for guaranteeing Global CO.

The following is concluded:

*'''The Vote ordering strategy for Global CO Enforcing [[Theorem]]'''

:Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be undecided (neither committed nor aborted) transactions in a database system that enforces CO for local transactions, such that &lt;math&gt;T_{2}&lt;/math&gt; is ''global'' and ''in conflict'' with &lt;math&gt;T_{1}&lt;/math&gt; (&lt;math&gt;T_{1}&lt;/math&gt; ''precedes'' &lt;math&gt;T_{2}&lt;/math&gt;). Then, having &lt;math&gt;T_{1}&lt;/math&gt; ended (either committed or aborted) before &lt;math&gt;T_{2}&lt;/math&gt; is voted on to be committed (the ''vote ordering strategy''), in each such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global CO (the condition guarantees Global CO, which may be violated without it).

:'''Comments:'''
# The ''vote ordering strategy'' that enforces global CO is referred to as &lt;math&gt;CD^3C&lt;/math&gt; in ([[#Raz1992|Raz 1992]]).
#The Local CO property of a global schedule means that each database is CO compliant. From the necessity discussion part above it directly follows that the theorem is true also when replacing "Global CO" with "Local CO" when global transactions are present. Together it means that Global CO is guaranteed [[if and only if]] Local CO is guaranteed (which is untrue for Global conflict serializability and Local conflict serializability: Global implies Local, but not the opposite).

Global CO implies Global serializability.

The '''Global CO algorithm''' comprises enforcing (local) CO in each participating database system by ordering commits of local transactions (see [[Commitment ordering#Enforcing CO locally|Enforcing CO locally]] below) and enforcing the ''vote ordering strategy'' in the theorem above (for global transactions).

====Exact characterization of voting-deadlocks by global cycles====

The above global cycle elimination process by a '''voting deadlock''' can be explained in detail by the following observation:

First it is assumed, for simplicity, that every transaction reaches the ready-to-commit state and is voted on by at least one database (this implies that no blocking by locks occurs).
Define a ''"wait for vote to commit" graph'' as a directed graph with transactions as nodes, and a directed edge from any first transaction to a second transaction if the first transaction blocks the vote to commit of the second transaction (opposite to conventional edge direction in a [[wait-for graph]]). Such blocking happens only if the second transaction is in a conflict with the first transaction (see above). Thus this "wait for vote to commit" graph is identical to the global conflict graph. A cycle in the "wait for vote to commit" graph means a deadlock in voting. Hence there is a deadlock in voting if and only if there is a cycle in the conflict graph. Local cycles (confined to a single database) are eliminated by the local serializability mechanisms. Consequently, only global cycles are left, which are then eliminated by the atomic commitment protocol when it aborts deadlocked transactions with missing (blocked) respective votes.

Secondly, also local commits are dealt with: Note that when enforcing CO also waiting for a regular local commit of a local transaction can block local commits and votes of other transactions upon conflicts, and the situation for global transactions does not change also without the simplifying assumption above: The final result is the same also with local commitment for local transactions, without voting in atomic commitment for them.

Finally, blocking by a lock (which has been excluded so far) needs to be considered: A lock blocks a conflicting operation and prevents a conflict from being materialized. If the lock is released only after transaction end, it may block indirectly either a vote or a local commit of another transaction (which now cannot get to ready state), with the same effect as of a direct blocking of a vote or a local commit. In this case a cycle is generated in the conflict graph only if such a blocking by a lock is also represented by an edge. With such added edges representing events of blocking-by-a-lock, the conflict graph is becoming an ''augmented conflict graph''.

*'''Definition: augmented conflict graph'''

:An '''augmented conflict graph''' is a [[serializability#Testing conflict serializability|conflict graph]] with added edges: In addition to the original edges a directed edge exists from transaction &lt;math&gt;T_{1}&lt;/math&gt; to transaction &lt;math&gt;T_{2}&lt;/math&gt; if two conditions are met:
# &lt;math&gt;T_{2}&lt;/math&gt; is blocked by a data-access lock applied by &lt;math&gt;T_{1}&lt;/math&gt; (the blocking prevents the conflict of &lt;math&gt;T_{2}&lt;/math&gt; with &lt;math&gt;T_{1}&lt;/math&gt; from being materialized and have an edge in the regular conflict graph), and
# This blocking will not stop before &lt;math&gt;T_{1}&lt;/math&gt; ends (commits or aborts; true for any locking-based CO)

:The graph can also be defined as the [[Union (set theory)|union]] of the (regular) ''conflict graph'' with the (reversed edge, regular) ''wait-for graph''

:'''Comments:'''
# Here, unlike the regular conflict graph, which has edges only for materialized conflicts, all conflicts, both materialized and non-materialized, are represented by edges.
# Note that all the new edges are all the (reversed to the conventional) edges of the ''wait-for graph''. The ''wait-for graph'' can be defined also as the graph of non-materialized conflicts. By the common conventions edge direction in a ''conflict graph'' defines time order between conflicting operations which is opposite to the time order defined by an edge in a ''wait-for graph''.
# Note that such global graph contains (has embedded) all the (reversed edge) regular local ''wait-for'' graphs, and also may include locking based global cycles (which cannot exist in the local graphs). For example, if all the databases on a global cycle are SS2PL based, then all the related vote blocking situations are caused by locks (this is the classical, and probably the only global deadlock situation dealt with in the database research literature). This is a global deadlock case where each related database creates a portion of the cycle, but the complete cycle does not reside in any local wait-for graph.

In the presence of CO the ''augmented conflict graph'' is in fact a (reversed edge) ''local-commit and voting wait-for graph'': An edge exists from a first transaction, either local or global, to a second, if the second is waiting for the first to end in order to be either voted on (if global), or locally committed (if local). All ''global cycles'' (across two or more databases) in this graph generate voting-deadlocks. The graph's global cycles provide complete characterization for voting deadlocks and may include any combination of materialized and non-materialized conflicts. Only cycles of (only) materialized conflicts are also cycles of the regular conflict graph and affect serializability. One or more (lock related) non-materialized conflicts on a cycle prevent it from being a cycle in the regular conflict graph, and make it a locking related deadlock. All the global cycles (voting-deadlocks) need to be broken (resolved) to both maintain global serializability and resolve global deadlocks involving data access locking, and indeed they are all broken by the atomic commitment protocol due to missing votes upon a voting deadlock.

'''Comment:''' This observation also explains the correctness of ''[[Commitment ordering#Extended CO (ECO)|Extended CO (ECO)]]'' below: Global transactions' voting order must follow the conflict graph order with vote blocking when order relation (graph path) exists between two global transactions. Local transactions are not voted on, and their (local) commits are not blocked upon conflicts. This results in same voting-deadlock situations and resulting global cycle elimination process for ECO.

The ''voting-deadlock'' situation can be summarized as follows:

*'''The CO Voting-Deadlock Theorem'''

:Let a multidatabase environment comprise CO compliant (which eliminates ''local cycles'') database systems that enforce, each, ''Global CO'' (using the condition in the theorem above). Then a ''voting-deadlock'' occurs if and only if a ''global cycle'' (spans two or more databases) exists in the ''Global augmented conflict graph'' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the ''global transactions'' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock); if a local transaction resides on the cycle, eventually it has its (local) commit blocked.

:'''Comment:''' A rare situation of a voting deadlock (by missing blocked votes) can happen, with no voting for any transaction on the related cycle by any of the database systems involved with these transactions. This can occur when local sub-transactions are [[Thread (computer science)|multi-threaded]]. The highest probability instance of such rare event involves two transactions on two simultaneous opposite cycles. Such global cycles (deadlocks) overlap with local cycles which are resolved locally, and thus typically resolved by local mechanisms without involving atomic commitment. Formally it is also a global cycle, but practically it is local (portions of local cycles generate a global one; to see this, split each global transaction (node) to local sub-transactions (its portions confined each to a single database); a directed edge exists between transactions if an edge exists between any respective local sub-transactions; a cycle is local if all its edges originate from a cycle among sub-transactions of the same database, and global if not; global and local can overlap: a same cycle among transactions can result from several different cycles among sub-transactions, and be both local and global).

Also the following locking based special case is concluded:

*'''The CO Locking-based Global-Deadlock Theorem'''

:In a CO compliant multidatabase system a locking-based global-deadlock, involving at least one data-access lock (non-materialized conflict), and two or more database systems, is a reflection of a global cycle in the ''Global augmented conflict graph'', which results in a voting-deadlock. Such cycle is not a cycle in the (regular) ''Global conflict graph'' (which reflects only materialized conflicts, and thus such cycle does not affect ''[[serializability]]'').

:'''Comments:'''
# Any blocking (edge) in the cycle that is not by a data-access lock is a direct blocking of either voting or local commit. All voting-deadlocks are resolved (almost all by ''Atomic commitment''; see comment above), including this locking-based type.
# Locking-based global-deadlocks can be generated also in a completely SS2PL-based distributed environment (special case of CO based), where all the vote blocking (and voting-deadlocks) are caused by data-access locks. Many research articles have dealt for years with resolving such global deadlocks, but none (except the CO articles) is known (as of 2009) to notice that ''atomic commitment'' automatically resolves them. Such automatic resolutions are regularly occurring unnoticed in all existing SS2PL based multidatabase systems, often bypassing dedicated resolution mechanisms.

Voting-deadlocks are the key for the operation of distributed CO.

Global cycle elimination (here voting-deadlock resolution by ''atomic commitment'') and resulting aborted transactions' re-executions are time consuming, regardless of concurrency control used. If databases schedule transactions independently, global cycles are unavoidable (in a complete analogy to cycles/deadlocks generated in local SS2PL; with distribution, any transaction or operation scheduling coordination results in autonomy violation, and typically also in substantial performance penalty). However, in many cases their likelihood can be made very low by implementing database and transaction design guidelines that reduce the number of conflicts involving a global transaction. This, primarily by properly handling hot spots (database objects with frequent access), and avoiding conflicts by using commutativity when possible (e.g., when extensively using counters, as in finances, and especially multi-transaction ''accumulation counters'', which are typically hot spots).

Atomic commitment protocols are intended and designed to achieve atomicity without considering database concurrency control. They abort upon detecting or [[Heuristic algorithm|heuristically]] finding (e.g., by timeout; sometimes mistakenly, unnecessarily) missing votes, and typically unaware of global cycles. These protocols can be specially enhanced for CO (including CO's variants below) both to prevent unnecessary aborts, and to accelerate aborts used for breaking global cycles in the global augmented conflict graph (for better performance by earlier release upon transaction-end of computing resources and typically locked data). For example, existing locking based global deadlock detection methods, other than timeout, can be generalized to consider also local commit and vote direct blocking, besides data access blocking. A possible compromise in such mechanisms is effectively detecting and breaking the most frequent and relatively simple to handle length-2 global cycles, and using timeout for undetected, much less frequent, longer cycles.

===Enforcing CO locally===

''Commitment ordering'' can be enforced locally (in a single database) by a dedicated CO algorithm, or by any algorithm/protocol that provides any special case of CO. An important such protocol, being utilized extensively in database systems, which generates a CO schedule, is the ''strong strict [[two phase locking]]'' protocol (SS2PL: "release transaction's locks only after the transaction has been either committed or aborted"; see below). SS2PL is a [[proper subset]] of the intersection of [[Two-phase locking|2PL]] and strictness.

====A generic local CO algorithm====

A '''generic local CO algorithm''' ([[#Raz1992|Raz 1992]]; Algorithm 4.1) is an algorithm independent of implementation details, that enforces exactly the CO property. It does not block data access (nonblocking), and consists of aborting a certain set of transactions (only if needed) upon committing a transaction. It aborts a (uniquely determined at any given time) minimal set of other undecided (neither committed, nor aborted) transactions that run locally and can cause serializability violation in the future (can later generate cycles of committed transactions in the conflict graph; this is the ABORT set of a committed transaction T; after committing T no transaction in ABORT at commit time can be committed, and all of them are doomed to be aborted). This set consists of all undecided transactions with directed edges in the conflict graph to the committed transaction. The size of this set cannot increase when that transaction is waiting to be committed (in ready state: processing has ended), and typically decreases in time as its transactions are being decided. Thus, unless [[Real-time computing|real-time]] constraints exist to complete that transaction, it is preferred to wait with committing that transaction and let this set decrease in size. If another serializability mechanism exists locally (which eliminates cycles in the local conflict graph), or if no cycle involving that transaction exists, the set will be empty eventually, and no abort of set member is needed. Otherwise the set will stabilize with transactions on local cycles, and aborting set members will have to occur to break the cycles. Since in the case of CO conflicts generate blocking on commit, local cycles in the ''augments conflict graph'' (see above) indicate local commit-deadlocks, and deadlock resolution techniques as in [[Serializability#Common mechanism - SS2PL|SS2PL]] can be used (e.g., like ''timeout'' and ''wait-for graph''). A local cycle in the ''augmented conflict graph'' with at least one non-materialized conflict reflects a locking-based deadlock. The local algorithm above, applied to the local augmented conflict graph rather than the regular local conflict graph, comprises the '''generic enhanced local CO algorithm''', a single local cycle elimination mechanism, for both guaranteeing local serializability and handling locking based local deadlocks. Practically an additional concurrency control mechanism is always utilized, even solely to enforce recoverability. The generic CO algorithm does not affect local data access scheduling strategy, when it runs alongside of any other local concurrency control mechanism. It affects only the commit order, and for this reason it does not need to abort more transactions than those needed to be aborted for serializability violation prevention by any combined local concurrency control mechanism. The net effect of CO may be, at most, a delay of commit events (or voting in a distributed environment), to comply with the needed commit order (but not more delay than its special cases, for example, SS2PL, and on the average significantly less).

The following theorem is concluded:

*'''The Generic Local CO Algorithm Theorem'''
:When running alone or alongside any concurrency control mechanism in a database system then
#The ''Generic local CO algorithm'' guarantees (local) CO (a CO compliant schedule).
#The ''Generic enhanced local CO algorithm'' guarantees both (local) CO and (local) locking based deadlock resolution.
: and (when not using ''timeout'', and no ''real-time'' transaction completion constraints are applied) neither algorithm aborts more transactions than the minimum needed (which is determined by the transactions' operations scheduling, out of the scope of the algorithms).

====Example: Concurrent programming and Transactional memory====
:See also ''[[The History of Commitment Ordering#Concurrent programming and Transactional memory|Concurrent programming and Transactional memory]]''

With the proliferation of Multi-core processors, variants of the Generic local CO algorithm have been also increasingly utilized in Concurrent programming, [[Transactional memory]], and especially in Software transactional memory for achieving serializability optimistically by "commit order" (e.g., Ramadan et al. 2009,&lt;ref name=Ramadan2009&gt;Hany E. Ramadan, Indrajit Roy, Maurice Herlihy, Emmett Witchel (2009): [http://portal.acm.org/citation.cfm?id=1504201 "Committing conflicting transactions in an STM"] ([http://www.cs.utexas.edu/~indrajit/pubs/ppopp121-ramadan.pdf PDF]) ''Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming'' (PPoPP '09), ISBN 978-1-60558-397-6&lt;/ref&gt; Zhang et al. 2006,&lt;ref name=Zhang2006&gt;Lingli Zhang, Vinod K.Grover, Michael M. Magruder, David Detlefs, John Joseph Duffy, Goetz Graefe (2006): [http://www.freepatentsonline.com/7711678.html  Software transaction commit order and conflict management] United States Patent 7711678, Granted 05/04/2010.&lt;/ref&gt; von Parun et al. 2007&lt;ref name=vonParun2007&gt;Christoph von Praun, Luis Ceze, Calin Cascaval (2007) [http://portal.acm.org/citation.cfm?id=1229443 "Implicit Parallelism with Ordered Transactions"] ([http://www.cs.washington.edu/homes/luisceze/publications/ipot_ppopp07.pdf PDF]), ''Proceedings of the 12th ACM SIGPLAN symposium on Principles and practice of parallel programming'' (PPoPP '07), ACM New York &#169;2007, ISBN 978-1-59593-602-8 doi 10.1145/1229428.1229443&lt;/ref&gt;). Numerous related articles and patents utilizing CO have already been published.

====Implementation considerations: The Commitment Order Coordinator (COCO)====

A database system in a multidatabase environment is assumed. From a [[software architecture]] point of view a CO component that implements the generic CO algorithm locally, the ''Commitment Order Coordinator'' (COCO), can be designed in a straightforward way as a [[mediator pattern|mediator]] between a (single) database system and an atomic commitment protocol component ([[#Raz1991b|Raz 1991b]]). However, the COCO is typically an integral part of the database system. The COCO's functions are to vote to commit on ready global transactions (processing has ended) according to the local commitment order, to vote to abort on transactions for which the database system has initiated an abort (the database system can initiate abort for any transaction, for many reasons), and to pass the atomic commitment decision to the database system. For local transactions (when can be identified) no voting is needed. For determining the commitment order the COCO maintains an updated representation of the local conflict graph (or local augmented conflict graph for capturing also locking deadlocks) of the undecided (neither committed nor aborted) transactions as a data structure (e.g., utilizing mechanisms similar to [[lock (computer science)|locking]] for capturing conflicts, but with no data-access blocking). The COCO component has an [[interface (computer science)|interface]] with its database system to receive "conflict," "ready" (processing has ended; readiness to vote on a global transaction or commit a local one), and "abort" notifications from the database system. It also interfaces with the atomic commitment protocol to vote and to receive the atomic commitment protocol's decision on each global transaction. The decisions are delivered from the COCO to the database system through their interface, as well as local transactions' commit notifications, at a proper commit order. The COCO, including its interfaces, can be enhanced, if it implements another variant of CO (see below), or plays a role in the database's concurrency control mechanism beyond voting in atomic commitment.

The COCO also guarantees CO locally in a single, isolated database system with no interface with an atomic commitment protocol.

===CO is a necessary condition for global serializability across autonomous database systems===

If the databases that participate in distributed transactions (i.e., transactions that span more than a single database) do not use any shared concurrency control information and use unmodified atomic commitment protocol messages (for reaching atomicity), then maintaining (local) ''commitment ordering'' or one of its generalizing variants (see below) is a [[necessary condition]] for guaranteeing global serializability (a proof technique can be found in ([[#Raz1992|Raz 1992]]), and a different proof method for this in ([[#Raz1993a|Raz 1993a]])); it is also a [[sufficient condition]]. This is a mathematical fact derived from the definitions of ''serializability'' and a ''[[Database transaction|transaction]]''. It means that if not complying with CO, then global serializability cannot be guaranteed under this condition (the condition of no local concurrency control information sharing between databases beyond atomic commit protocol messages). Atomic commitment is a minimal requirement for a distributed transaction since it is always needed, which is implied by the definition of transaction.

([[#Raz1992|Raz 1992]]) defines ''database autonomy'' and ''independence'' as complying with this requirement without using any additional local knowledge:
*'''Definition:''' (concurrency control based) '''autonomous database system'''
:A database system is '''Autonomous''', if it does not share with any other entity any concurrency control information beyond unmodified [[atomic commitment protocol]] messages. In addition it does not use for concurrency control any additional local information beyond conflicts (the last sentence does not appear explicitly but rather implied by further discussion in [[#Raz1992|Raz 1992]]).

Using this definition the following is concluded:

*'''The CO and Global serializability Theorem'''

#CO compliance of every ''autonomous'' database system (or transactional object) in a multidatabase environment is a ''necessary condition'' for guaranteeing Global serializability (without CO Global serializability may be violated).
#CO compliance of every database system is a ''sufficient condition'' for guaranteeing Global serializability.

However, the definition of autonomy above implies, for example, that transactions are scheduled in a way that local transactions (confined to a single database) cannot be identified as such by an autonomous database system. This is realistic for some transactional objects, but too restrictive and less realistic for general purpose database systems. If autonomy is augmented with the ability to identify local transactions, then compliance with a more general property, ''Extended commitment ordering'' (ECO, see below), makes ECO the necessary condition.

Only in ([[#Raz2009|Raz 2009]]) the notion of ''Generalized autonomy'' captures the intended notion of autonomy:
*'''Definition: generalized autonomy'''
:A database system has the ''Generalized autonomy'' property, if it does not share with any other database system any local concurrency information beyond (unmodified) atomic commit protocol messages (however any local information can be utilized).

This definition is probably the broadest such definition possible in the context of database concurrency control, and it makes CO together with any of its (useful: No concurrency control information distribution) generalizing variants (Vote ordering (VO); see CO variants below) the necessary condition for Global serializability (i.e., the union of CO and its generalizing variants is the necessary set VO, which may include also new unknown useful generalizing variants).

===Summary===

The ''Commitment ordering'' (CO) solution (technique) for global serializability can be summarized as follows:

If each ''database'' (or any other ''transactional object'') in a multidatabase environment complies with CO, i.e., arranges its local transactions' commitments and its votes on (global, distributed) transactions to the ''[[atomic commitment]]'' protocol according to the local (to the database) [[partial order]] induced by the local conflict graph (serializability graph) for the respective transactions, then ''Global CO'' and ''Global serializability'' are guaranteed. A database's CO compliance can be achieved effectively with any local [[Serializability#View serializability and conflict serializability|conflict serializability]] based concurrency control mechanism, with neither affecting any transaction's execution process or scheduling, nor aborting it. Also the database's autonomy is not violated. The only low overhead incurred is detecting conflicts (e.g., as with locking, but with no data-access blocking; if not already detected for other purposes), and ordering votes and local transactions' commits according to the conflicts.

[[Image:CO-ScheduleClasses.jpg|thumb|350px| '''Schedule classes containment:''' An arrow from class A to class B indicates that class A strictly contains B; a lack of a directed path between classes means that the classes are incomparable.

A property is '''inherently blocking''', if it can be enforced only by blocking transaction&#8217;s data access operations until certain events occur in other transactions. ([[#Raz1992|Raz 1992]])]]

In case of incompatible partial orders of two or more databases (no global partial order can [[Embedding|embed]] the respective local partial orders together), a global cycle (spans two databases or more) in the global conflict graph is generated. This, together with CO, results in a cycle of blocked votes, and a ''voting-[[deadlock]]'' occurs for the databases on that cycle (however, allowed concurrent voting in each database, typically for almost all the outstanding votes, continue to execute). In this case the atomic commitment protocol fails to collect all the votes needed for the blocked transactions on that global cycle, and consequently the protocol aborts some transaction with a missing vote. This breaks the global cycle, the voting-deadlock is resolved, and the related blocked votes are free to be executed. Breaking the global cycle in the global conflict graph ensures that both global CO and global serializability are maintained. Thus, in case of incompatible local (partial) commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause for the incompatibility. Furthermore, also global deadlocks due to locking (global cycles in the ''augmented conflict graph'' with at least one data access blocking) result in voting deadlocks and are resolved automatically by the same mechanism.

''Local CO'' is a necessary condition for guaranteeing ''Global serializability,'' if the databases involved do not share any concurrency control information beyond (unmodified) atomic commitment protocol messages, i.e., if the databases are ''autonomous'' in the context of concurrency control. This means that every global serializability solution for autonomous databases must comply with CO. Otherwise global serializability may be violated (and thus, is likely to be violated very quickly in a high-performance environment).

The CO solution [[Scalability|scales up]] with network size and the number of databases without performance penalty when it utilizes [[Two-phase commit protocol#Common architecture|common distributed atomic commitment architecture]].

==Distributed serializability and CO==

===Distributed CO===

A distinguishing characteristic of the CO solution to distributed serializability from other techniques is the fact that it requires no conflict information distributed (e.g., local precedence relations, locks, [[Timestamp-based concurrency control|timestamps]], tickets), which makes it uniquely effective. It utilizes (unmodified) atomic commitment protocol messages (which are already used) instead.

A common way to achieve distributed serializability in a [[Distributed system|(distributed) system]] is by a [[distributed lock manager]] (DLM). DLMs, which communicate lock (non-materialized conflict) information in a distributed environment, typically suffer from computer and communication [[Latency (engineering)|latency]], which reduces the performance of the system. CO allows to achieve distributed serializability under very general conditions, without a distributed lock manager, exhibiting the benefits already explored above for multidatabase environments; in particular: reliability, high performance, scalability, possibility of using ''optimistic concurrency control'' when desired, no conflict information related communications over the network (which have incurred overhead and delays), and automatic distributed deadlock resolution.

All ''distributed transactional systems'' rely on some atomic commitment protocol to coordinate atomicity (whether to commit or abort) among processes in a [[distributed transaction]]. Also, typically ''recoverable data'' (i.e., data under transactions' control, e.g., database data; not to be confused with the ''recoverability'' property of a schedule) are directly accessed by a single ''transactional data manager'' component (also referred to as a ''resource manager'') that handles local sub-transactions (the distributed transaction's portion in a single location, e.g., network node), even if these data are accessed indirectly by other entities in the distributed system during a transaction (i.e., indirect access requires a direct access through a local sub-transaction). Thus recoverable data in a distributed transactional system are typically partitioned among transactional data managers. In such system these transactional data managers typically comprise the participants in the system's atomic commitment protocol. If each participant complies with CO (e.g., by using SS2PL, or COCOs, or a combination; see above), then the entire distributed system provides CO (by the theorems above; each participant can be considered a separate transactional object), and thus (distributed) serializability. Furthermore: When CO is utilized together with an atomic commitment protocol also ''distributed deadlocks'' (i.e., deadlocks that span two or more data managers) caused by data-access locking are resolved automatically. Thus the following corollary is concluded:

*'''The CO Based Distributed Serializability Theorem'''

:Let a ''distributed transactional system'' (e.g., a [[distributed database]] system) comprise ''transactional data managers'' (also called ''resource managers'') that manage all the system's ''recoverable data''. The data managers meet three conditions:
# '''Data partition:''' Recoverable data are partitioned among the data managers, i.e., each recoverable datum (data item) is controlled by a single data manager (e.g., as common in a [[Shared nothing architecture]]; even copies of a same datum under different data managers are physically distinct, ''replicated'').
# '''Participants in atomic commitment protocol:''' These data managers are the participants in the system's atomic commitment protocol for coordinating distributed transactions' atomicity.
# '''CO compliance:''' Each such data manager is CO compliant (or some CO variant compliant; see below).
:Then
# The entire distributed system guarantees (distributed CO and) ''serializability'', and
# Data-access based ''distributed deadlocks'' (deadlocks involving two or more data managers with at least one non-materialized conflict) are resolved automatically.

:Furthermore: The data managers being CO compliant is a ''necessary condition'' for (distributed) serializability in a system meeting conditions 1, 2 above, when the data managers are ''autonomous'', i.e., do not share concurrency control information beyond unmodified messages of atomic commitment protocol.

This theorem also means that when SS2PL (or any other CO variant) is used locally in each transactional data manager, and each data manager has exclusive control of its data, no distributed lock manager (which is often utilized to enforce distributed SS2PL) is needed for distributed SS2PL and serializability. It is relevant to a wide range of distributed transactional applications, which can be easily designed to meet the theorem's conditions.

===Distributed optimistic CO (DOCO)===

For implementing Distributed Optimistic CO (DOCO) the generic local CO algorithm is utilized in all the atomic commitment protocol participants in the system with no data access blocking and thus with no local deadlocks. The previous theorem has the following corollary:

*'''The Distributed optimistic CO (DOCO) Theorem'''

:If DOCO is utilized, then:
:# No local deadlocks occur, and
:# Global (voting) deadlocks are resolved automatically (and all are serializability related (with non-blocking conflicts) rather than locking related (with blocking and possibly also non-blocking conflicts)).

:Thus, no deadlock handling is needed.

===Examples===

====Distributed SS2PL====

A distributed database system that utilizes [[Two-phase locking#Strong strict two-phase locking|SS2PL]] resides on two remote nodes, A and B. The database system has two ''transactional data managers'' (''resource managers''), one on each node, and the database data are partitioned between the two data managers in a way that each has an exclusive control of its own (local to the node) portion of data: Each handles its own data and locks without any knowledge on the other manager's. For each distributed transaction such data managers need to execute the available atomic commitment protocol.

Two distributed transactions, &lt;math&gt;T_{1}&lt;/math&gt; and &lt;math&gt;T_{2}&lt;/math&gt;, are running concurrently, and both access data x and y. x is under the exclusive control of the data manager on A (B's manager cannot access x), and y under that on B.

:&lt;math&gt;T_{1}&lt;/math&gt; reads x on A and writes y on B, i.e., &lt;math&gt;T_{1} = R_{1A}(x)&lt;/math&gt; &lt;math&gt;W_{1B}(y)&lt;/math&gt; when using notation common for concurrency control.
:&lt;math&gt;T_{2}&lt;/math&gt; reads y on B and writes x on A, i.e., &lt;math&gt;T_{2} = R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{2A}(x)&lt;/math&gt;

The respective ''local sub-transactions'' on A and B (the portions of &lt;math&gt;T_{1}&lt;/math&gt; and &lt;math&gt;T_{2}&lt;/math&gt; on each of the nodes) are the following:

:{| class="wikitable" style="text-align:center;"
|+Local sub-transactions
|-
! Transaction \ Node !! A  !! B
|-
! &lt;math&gt;T_{1}&lt;/math&gt;
|  &lt;math&gt;T_{1A}=R_{1A}(x)&lt;/math&gt; || &lt;math&gt;T_{1B}=W_{1B}(y)&lt;/math&gt;
|-
! &lt;math&gt;T_{2}&lt;/math&gt;
| &lt;math&gt;T_{2A}=W_{2A}(x)&lt;/math&gt; || &lt;math&gt;T_{2B}=R_{2B}(y)&lt;/math&gt;
|}

The database system's [[Schedule (computer science)|schedule]] at a certain point in time is the following:

:&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt;
:(also &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;R_{1A}(x)&lt;/math&gt; is possible)

&lt;math&gt;T_{1}&lt;/math&gt; holds a read-lock on x and &lt;math&gt;T_{2}&lt;/math&gt; holds read-locks on y. Thus &lt;math&gt;W_{1B}(y)&lt;/math&gt; and &lt;math&gt;W_{2A}(x)&lt;/math&gt; are blocked by the [[Two-phase locking#Data-access locks|lock compatibility]] rules of SS2PL and cannot be executed. This is a distributed deadlock situation, which is also a voting-deadlock (see below) with a distributed (global) cycle of length 2 (number of edges, conflicts; 2 is the most frequent length). The local sub-transactions are in the following states:

:&lt;math&gt;T_{1A}&lt;/math&gt; is ''ready'' (execution has ended) and ''voted'' (in atomic commitment)
:&lt;math&gt;T_{1B}&lt;/math&gt; is ''running'' and blocked (a non-materialized conflict situation; no vote on it can occur)
:&lt;math&gt;T_{2B}&lt;/math&gt; is ''ready'' and ''voted''
:&lt;math&gt;T_{2A}&lt;/math&gt; is ''running'' and blocked (a non-materialized conflict; no vote).

Since the atomic commitment protocol cannot receive votes for blocked sub-transactions (a voting-deadlock), it will eventually abort some transaction with a missing vote(s) by [[Timeout (computing)|timeout]], either &lt;math&gt;T_{1}&lt;/math&gt;, or &lt;math&gt;T_{2}&lt;/math&gt;, (or both, if the timeouts fall very close). This will resolve the global deadlock. The remaining transaction will complete running, be voted on, and committed. An aborted transaction is immediately ''restarted'' and re-executed.

'''Comments:'''
# The data partition (x on A; y on B) is important since without it, for example, x can be accessed directly from B. If a transaction &lt;math&gt;T_{3}&lt;/math&gt; is running on B concurrently with &lt;math&gt;T_{1}&lt;/math&gt; and &lt;math&gt;T_{2}&lt;/math&gt; and directly writes x, then, without a distributed lock manager the read-lock for x held by &lt;math&gt;T_{1}&lt;/math&gt; on A is not visible on B and cannot block the write of &lt;math&gt;T_{3}&lt;/math&gt; (or signal a materialized conflict for a non-blocking CO variant; see below). Thus serializability can be violated.
# Due to data partition, x cannot be accessed directly from B. However, functionality is not limited, and a transaction running on B still can issue a write or read request of x (not common). This request is communicated to the transaction's local sub-transaction on A (which is generated, if does not exist already) which issues this request to the local data manager on A.

====Variations====

In the scenario above both conflicts are ''non-materialized'', and the global voting-deadlock is reflected as a cycle in the global ''wait-for graph'' (but not in the global ''conflict graph''; see [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|Exact characterization of voting-deadlocks by global cycles]] above). However the database system can utilize any CO variant with exactly the same conflicts and voting-deadlock situation, and same resolution. Conflicts can be either ''materialized'' or ''non-materialized'', depending on CO variant used. For example, if [[Commitment ordering#Strict CO (SCO)|SCO]] (below) is used by the distributed database system instead of SS2PL, then the two conflicts in the example are ''materialized'', all local sub-transactions are in ''ready'' states, and vote blocking occurs in the two transactions, one on each node, because of the CO voting rule applied independently on both A and B: due to conflicts &lt;math&gt;T_{2A}=W_{2A}(x)&lt;/math&gt; is not voted on before &lt;math&gt;T_{1A}=R_{1A}(x)&lt;/math&gt; ends, and &lt;math&gt;T_{1B}=W_{1B}(y)&lt;/math&gt; is not voted on before &lt;math&gt;T_{2B}=R_{2B}(y)&lt;/math&gt; ends, which is a voting-deadlock. Now the ''conflict graph'' has the global cycle (all conflicts are materialized), and again it is resolved by the atomic commitment protocol, and distributed serializability is maintained. Unlikely for a distributed database system, but possible in principle (and occurs in a multi-database), A can employ SS2PL while B employs SCO. In this case the global cycle is neither in the wait-for graph nor in the serializability graph, but still in the ''augmented conflict graph'' (the union of the two). The various combinations are summarized in the following table:

{| class="wikitable" style="text-align:center;"
|+Voting-deadlock situations
|-
!Case!! Node&lt;br&gt;A  !! Node&lt;br&gt;B !!Possible schedule!!Materialized&lt;br&gt;conflicts&lt;br&gt;on cycle!!Non-&lt;br&gt;materialized&lt;br&gt;conflicts!!&lt;math&gt;T_{1A}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;R_{1A}(x)&lt;/math&gt;!!&lt;math&gt;T_{1B}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;W_{1B}(y)&lt;/math&gt;!!&lt;math&gt;T_{2A}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;W_{2A}(x)&lt;/math&gt;!!&lt;math&gt;T_{2B}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;R_{2B}(y)&lt;/math&gt;
|-
! 1
|SS2PL||SS2PL||&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt;|| 0 || 2 ||Ready&lt;br&gt;Voted||Running&lt;br&gt;(Blocked)||Running&lt;br&gt;(Blocked)||Ready&lt;br&gt;Voted
|-
! 2
|SS2PL|| SCO ||&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{1B}(y)&lt;/math&gt;|| 1 || 1 ||Ready&lt;br&gt;Voted ||Ready&lt;br&gt;Vote blocked||Running&lt;br&gt;(Blocked)||Ready&lt;br&gt;Voted
|-
! 3
|SCO||SS2PL|| &lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{2A}(x)&lt;/math&gt; || 1 || 1 ||Ready&lt;br&gt;Voted||Running&lt;br&gt;(Blocked)||Ready&lt;br&gt;Vote blocked||Ready&lt;br&gt;Voted
|-
! 4
|SCO||SCO||&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{1B}(y)&lt;/math&gt; &lt;math&gt;W_{2A}(x)&lt;/math&gt;|| 2 || 0 ||Ready&lt;br&gt;Voted||Ready&lt;br&gt;Vote blocked ||Ready&lt;br&gt;Vote blocked||Ready&lt;br&gt;Voted
|}

:'''Comments:'''
# Conflicts and thus cycles in the ''augmented conflict graph'' are determined by the transactions and their initial scheduling only, independently of the concurrency control utilized. With any variant of CO, any ''global cycle'' (i.e., spans two databases or more) causes a ''voting deadlock''. Different CO variants may differ on whether a certain conflict is ''materialized'' or ''non-materialized''.
# Some limited operation order changes in the schedules above are possible, constrained by the orders inside the transactions, but such changes do not change the rest of the table.
# As noted above, only case 4 describes a cycle in the (regular) conflict graph which affects serializability. Cases 1-3 describe cycles of locking based global deadlocks (at least one lock blocking exists). All cycle types are equally resolved by the atomic commitment protocol. Case 1 is the common Distributed SS2PL, utilized since the 1980s. However, no research article, except the CO articles, is known to notice this automatic locking global deadlock resolution as of 2009. Such global deadlocks typically have been dealt with by dedicated mechanisms.
# Case 4 above is also an example for a typical voting-deadlock when [[Commitment ordering#Distributed optimistic CO (DOCO)|Distributed optimistic CO (DOCO)]] is used (i.e., Case 4 is unchanged when Optimistic CO (OCO; see below) replaces SCO on both A and B): No data-access blocking occurs, and only materialized conflicts exist.

====Hypothetical Multi Single-Threaded Core (MuSiC) environment====

'''Comment:''' While the examples above describe real, recommended utilization of CO, this example is hypothetical, for demonstration only.

Certain experimental distributed memory-resident databases advocate multi single-threaded core (MuSiC) transactional environments. "Single-threaded" refers to transaction [[Thread (computer science)|threads]] only, and to ''serial'' execution of transactions. The purpose is possible orders of magnitude gain in performance (e.g., [[Michael Stonebraker#H-Store and VoltDB|H-Store]]&lt;ref name=Stone08&gt;Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alex Rasin, [[Stanley Zdonik]], Evan Jones, Yang Zhang, Samuel Madden, [[Michael Stonebraker]], John Hugg, Daniel Abadi (2008): [http://portal.acm.org/citation.cfm?id=1454211  "H-Store: A High-Performance, Distributed Main Memory Transaction Processing System"], ''Proceedings of the 2008 VLDB'', pages 1496 - 1499, Auckland, New-Zealand, August 2008.&lt;/ref&gt; and [[VoltDB]]) relatively to conventional transaction execution in multiple threads on a same core. In what described below MuSiC is independent of the way the cores are distributed. They may reside in one [[integrated circuit]] (chip), or in many chips, possibly distributed geographically in many computers. In such an environment, if recoverable (transactional) data are partitioned among threads (cores), and it is implemented in the conventional way for distributed CO, as described in previous sections, then DOCO and Strictness exist automatically. However, downsides exist with this straightforward implementation of such environment, and its practicality as a general-purpose solution is questionable. On the other hand, tremendous performance gain can be achieved in applications that can bypass these downsides in most situations.

'''Comment:''' The MuSiC straightforward implementation described here (which uses, for example, as usual in distributed CO, voting (and transaction thread) blocking in atomic commitment protocol when needed) is for demonstration only, and has '''no connection''' to the implementation in H-Store or any other project.

In a MuSiC environment local schedules are ''serial''. Thus both local Optimistic CO (OCO; see below) and the ''Global CO enforcement vote ordering strategy'' condition for the atomic commitment protocol are met automatically. This results in both distributed CO compliance (and thus distributed serializability) and automatic global (voting) deadlock resolution.

Furthermore, also local ''Strictness'' follows automatically in a serial schedule. By Theorem 5.2 in ([[#Raz1992|Raz 1992]]; page  307), when the CO vote ordering strategy is applied, also Global Strictness is guaranteed. Note that ''serial'' locally is the only mode that allows strictness and "optimistic" (no data access blocking) together.

The following is concluded:

* '''The MuSiC Theorem'''
:In MuSiC environments, if recoverable (transactional) data are partitioned among cores (threads), then both
:#''OCO'' (and implied ''Serializability''; i.e., DOCO and Distributed serializability)
:#''Strictness'' (allowing effective recovery; 1 and 2 implying Strict CO&#8212;see SCO below) and
:#(voting) ''deadlock resolution''
:automatically exist globally with unbounded scalability in number of cores used.

:'''Comment:''' However, two major downsides, which need special handling, may exist:
#Local sub-transactions of a global transaction are blocked until commit, which makes the respective cores idle. This reduces core utilization substantially, even if scheduling of the local sub-transactions attempts to execute all of them in time proximity, almost together. It can be overcome by detaching execution from commit (with some atomic commitment protocol) for global transactions, at the cost of possible cascading aborts.
#increasing the number of cores for a given amount of recoverable data (database size) decreases the average amount of (partitioned) data per core. This may make some cores idle, while others very busy, depending on data utilization distribution. Also a local (to a core) transaction may become global (multi-core) to reach its needed data, with additional incurred overhead. Thus, as the number of cores increases, the amount and type of data assigned to each core should be balanced according to data usage, so a core is neither overwhelmed to become a bottleneck, nor becoming idle too frequently and underutilized in a busy system. Another consideration is putting in a same core partition all the data that are usually accessed by a same transaction (if possible), to maximize the number of local transactions (and minimize the number of global, distributed transactions). This may be achieved by occasional data re-partition among cores based on load balancing (data access balancing) and patterns of data usage by transactions. Another way to considerably mitigate this downside is by proper physical data replication among some core partitions in a way that read-only global transactions are possibly (depending on usage patterns) completely avoided, and replication changes are synchronized by a dedicated commit mechanism.

==CO variants: Interesting special cases and generalizations==

Special case schedule property classes (e.g., SS2PL and SCO below) are strictly contained in the CO class. The generalizing classes (ECO and MVCO) strictly contain the CO class (i.e., include also schedules that are not CO compliant). The generalizing variants also guarantee global serializability without distributing local concurrency control information (each database has the ''generalized autonomy'' property: it uses only local information), while relaxing CO constraints and utilizing additional (local) information for better concurrency and performance: ECO uses knowledge about transactions being local (i.e., confined to a single database), and MVCO uses availability of data versions values. Like CO, both generalizing variants are ''non-blocking'', do not interfere with any transaction's operation scheduling, and can be seamlessly combined with any relevant concurrency control mechanism.

The term '''CO variant''' refers in general to CO, ECO, MVCO, or a combination of each of them with any relevant concurrency control mechanism or property (including Multi-version based ECO, MVECO). No other interesting generalizing variants (which guarantee global serializability with no local concurrency control information distribution) are known, but may be discovered.

===Strong strict two phase locking (SS2PL)===
{{main|Two-phase locking}}

'''Strong Strict Two Phase Locking''' (SS2PL;  also referred to as ''Rigorousness'' or ''Rigorous scheduling'') means that both read and write locks of a transaction are released only after the transaction has ended (either committed or aborted). The set of SS2PL schedules is a [[proper subset]] of the set of CO schedules.
This property is widely utilized in database systems, and since it implies CO, databases that use it and participate in global transactions generate together a serializable global schedule (when using any atomic commitment protocol, which is needed for atomicity in a multi-database environment). No database modification or addition is needed in this case to participate in a CO distributed solution: The set of undecided transactions to be aborted before committing in the [[Commitment ordering#The algorithm|local generic CO algorithm]] above is empty because of the locks, and hence such an algorithm is unnecessary in this case. A transaction can be voted on by a database system immediately after entering a "ready" state, i.e., completing running its task locally. Its locks are released by the database system only after it is decided by the atomic commitment protocol, and thus the condition in the ''Global CO enforcing theorem'' above is kept automatically. Interestingly, if a local timeout mechanism is used by a database system to resolve (local) SS2PL deadlocks, then aborting blocked transactions breaks not only potential local cycles in the global conflict graph (real cycles in the augmented conflict graph), but also database system's potential global cycles as a side effect, if the [[atomic commitment]] protocol's abort mechanism is relatively slow. Such independent aborts by several entities typically may result in unnecessary aborts for more than one transaction per global cycle. The situation is different for a local ''wait-for graph'' based mechanisms: Such cannot identify global cycles, and the atomic commitment protocol will break the global cycle, if the resulting voting deadlock is not resolved earlier in another database.

Local SS2PL together with atomic commitment implying global serializability can also be deduced directly: All transactions, including distributed, obey the [[Two-phase locking|2PL]] (SS2PL) rules. The atomic commitment protocol mechanism is not needed here for consensus on commit, but rather for the end of phase-two synchronization point. Probably for this reason, without considering the atomic commitment voting mechanism, automatic global deadlock resolution has not been noticed before CO.

===Strict CO (SCO)===

[[Image:SCO-VS-SS2PL.jpg|thumb|450px|'''Read-write conflict: SCO Vs. SS2PL'''. Duration of transaction T2 is longer with SS2PL than with SCO.

SS2PL delays write operation w2[x] of T2 until T1 commits, due to a lock on x by T1 following read operation r1[x]. If t time units are needed for transaction T2 after starting write operation w2[x] in order to reach ready state, than T2 commits t time units after T1 commits. However, SCO does not block w2[x], and T2 can commit immediately after T1 commits. ([[#Raz1991c|Raz 1991c]])]]

'''Strict Commitment Ordering''' (SCO; ([[#Raz1991c|Raz 1991c]])) is the intersection of [[Schedule (computer science)#Strict|strictness]] (a special case of recoverability) and CO, and provides an upper bound for a schedule's concurrency when both properties exist. It can be implemented using blocking mechanisms (locking) similar to those used for the popular SS2PL with similar overheads.

Unlike SS2PL, SCO does not block on a read-write conflict but possibly blocks on commit instead. SCO and SS2PL have identical blocking behavior for the other two conflict types: write-read, and write-write. As a result, SCO has shorter average blocking periods, and more concurrency (e.g., performance simulations of a single database for the most significant variant of ''[[locks with ordered sharing]],'' which is identical to SCO, clearly show this, with approximately 100% gain for some transaction loads; also for identical transaction loads SCO can reach higher transaction rates than SS2PL before ''lock [[Thrashing (computer science)|thrashing]]'' occurs). More concurrency means that with given computing resources more transactions are completed in time unit (higher transaction rate, [[throughput]]), and the average duration of a transaction is shorter (faster completion; see chart). The advantage of SCO is especially significant during lock contention.

*'''The SCO Vs. SS2PL Performance Theorem'''
:SCO provides shorter average transaction completion time than SS2PL, if read-write conflicts exist. SCO and SS2PL are identical otherwise (have identical blocking behavior with write-read and write-write conflicts).

SCO is as practical as SS2PL since as SS2PL it provides besides serializability also strictness, which is widely utilized as a basis for efficient recovery of databases from failure. An SS2PL mechanism can be converted to an SCO one for better performance in a straightforward way without changing recovery methods. A description of a SCO implementation can be found in (Perrizo and Tatarinov 1998).&lt;ref&gt;{{cite conference | first1 = William | last1 = Perrizo | first2 = Igor | last2 = Tatarinov | title =  A Semi-Optimistic Database Scheduler Based on Commit Ordering | citeseerx = 10.1.1.53.7318 | conference = 1998 Int'l Conference on Computer Applications in Industry and Engineering | pages = 75&#8211;79 | location = Las Vegas | date = November 11, 1998 }}&lt;/ref&gt; See also  ''[[The History of Commitment Ordering#Semi-optimistic database scheduler|Semi-optimistic database scheduler]]''.

SS2PL is a proper subset of SCO (which is another explanation why SCO is less constraining and provides more concurrency than SS2PL).

===Optimistic CO (OCO)===

For implementing '''Optimistic commitment ordering''' (OCO) the generic local CO algorithm is utilized without data access blocking, and thus without local deadlocks. OCO without transaction or operation scheduling constraints covers the entire CO class, and is not a special case of the CO class, but rather a useful CO variant and mechanism characterization.

===Extended CO (ECO)===

====General characterization of ECO====

'''Extended Commitment Ordering''' (ECO; ([[#Raz1993a|Raz 1993a]])) generalizes CO. When local transactions (transactions confined to a single database) can be distinguished from global (distributed) transactions (transactions that span two databases or more), commitment order is applied to global transactions only. Thus, for a local (to a database) schedule to have the ECO property, the chronological (partial) order of commit events of global transactions only (unimportant for local transactions) is consistent with their order on the respective local conflict graph.

*'''Definition: extended commitment ordering'''

:Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be two committed ''global'' transactions in a schedule, such that a ''directed path'' of unaborted transactions exists in the ''conflict graph'' ([[precedence graph]]) from &lt;math&gt;T_{1}&lt;/math&gt; to &lt;math&gt;T_{2}&lt;/math&gt; (&lt;math&gt;T_{1}&lt;/math&gt; precedes &lt;math&gt;T_{2}&lt;/math&gt;, possibly [[transitive relation|transitively]], indirectly). The schedule has the '''Extended commitment ordering''' (ECO) property, if for every two such transactions &lt;math&gt;T_{1}&lt;/math&gt; commits before &lt;math&gt;T_{2}&lt;/math&gt; commits.

A distributed algorithm to guarantee global ECO exists. As for CO, the algorithm needs only (unmodified) atomic commitment protocol messages. In order to guarantee global serializability, each database needs to guarantee also the conflict serializability of its own transactions by any (local) concurrency control mechanism.

* '''The ECO and Global Serializability Theorem'''

#(Local, which implies global) ECO together with local conflict serializability, is a sufficient condition to guarantee global conflict serializability.
#When no concurrency control information beyond atomic commitment messages is shared outside a database (autonomy), and local transactions can be identified, it is also a necessary condition.

:See a necessity proof in ([[#Raz1993a|Raz 1993a]]).

This condition (ECO with local serializability) is weaker than CO, and allows more concurrency at the cost of a little more complicated local algorithm (however, no practical overhead difference with CO exists).

When all the transactions are assumed to be global (e.g., if no information is available about transactions being local), ECO reduces to CO.

====The ECO algorithm====

Before a global transaction is committed, a generic local (to a database) ECO algorithm aborts a minimal set of undecided transactions (neither committed, nor aborted; either local transactions, or global that run locally), that can cause later a cycle in the conflict graph. This set of aborted transactions (not unique, contrary to CO) can be optimized, if each transaction is assigned with a weight (that can be determined by transaction's importance and by the computing resources already invested in the running transaction; optimization can be carried out, for example, by a reduction from the ''[[Max flow in networks]]'' problem ([[#Raz1993a|Raz 1993a]])). Like for CO such a set is time dependent, and becomes empty eventually. Practically, almost in all needed implementations a transaction should be committed only when the set is empty (and no set optimization is applicable). The local (to the database) concurrency control mechanism (separate from the ECO algorithm) ensures that local cycles are eliminated (unlike with CO, which implies serializability by itself; however, practically also for CO a local concurrency mechanism is utilized, at least to ensure Recoverability). Local transactions can be always committed concurrently (even if a precedence relation exists, unlike CO). When the overall transactions' local partial order (which is determined by the local conflict graph, now only with possible temporary local cycles, since cycles are eliminated by a local serializability mechanism) allows, also global transactions can be voted on to be committed concurrently (when all their transitively (indirect) preceding (via conflict) ''global'' transactions are committed, while transitively preceding local transactions can be at any state. This in analogy to the distributed CO algorithm's stronger concurrent voting condition, where all the transitively preceding transactions need to be committed).

The condition for guaranteeing ''Global ECO'' can be summarized similarly to CO:

*'''The Global ECO Enforcing Vote ordering strategy Theorem'''

:Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be undecided (neither committed nor aborted) ''global transactions'' in a database system that ensures serializability locally, such that a ''directed path'' of unaborted transactions exists in the ''local conflict graph'' (that of the database itself) from &lt;math&gt;T_{1}&lt;/math&gt; to &lt;math&gt;T_{2}&lt;/math&gt;. Then, having &lt;math&gt;T_{1}&lt;/math&gt; ended (either committed or aborted) before &lt;math&gt;T_{2}&lt;/math&gt; is voted on to be committed, in every such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global ECO (the condition guarantees Global ECO, which may be violated without it).

Global ECO (all global cycles in the global conflict graph are eliminated by atomic commitment) together with Local serializability (i.e., each database system maintains serializability locally; all local cycles are eliminated) imply Global serializability (all cycles are eliminated). This means that if each database system in a multidatabase environment provides local serializability (by ''any'' mechanism) and enforces the ''vote ordering strategy'' in the theorem above (a generalization of CO's vote ordering strategy), then ''Global serializability'' is guaranteed (no local CO is needed anymore).

Similarly to CO as well, the ECO ''voting-deadlock'' situation can be summarized as follows:

*'''The ECO Voting-Deadlock Theorem'''

:Let a multidatabase environment comprise database systems that enforce, each, both ''Global ECO'' (using the condition in the theorem above) and ''local conflict serializability'' (which eliminates local cycles in the global conflict graph). Then, a ''voting-deadlock'' occurs if and only if a ''global cycle'' (spans two or more databases) exists in the ''Global augmented conflict graph'' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the ''global transactions'' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock). If a local transaction resides on the cycle, it may be in any unaborted state (running, ready, or committed; unlike CO no local commit blocking is needed).

As with CO this means that also global deadlocks due to data-access locking (with at least one lock blocking) are voting deadlocks, and are automatically resolved by atomic commitment.

===Multi-version CO (MVCO)===

'''Multi-version Commitment Ordering''' (MVCO; ([[#Raz1993b|Raz 1993b]])) is a generalization of CO for databases with [[Multiversion concurrency control|multi-version resources]]. With such resources ''read-only transactions'' do not block or being blocked for better performance. Utilizing such resources is a common way nowadays to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object). MVCO implies ''One-copy-serializability'' (1SER or 1SR) which is the generalization of [[serializability]] for multi-version resources. Like CO, MVCO is non-blocking, and can be combined with any relevant multi-version concurrency control mechanism without interfering with it. In the introduced underlying theory for MVCO conflicts are generalized for different versions of a same resource (differently from earlier multi-version theories). For different versions conflict chronological order is replaced by version order, and possibly reversed, while keeping the usual definitions for conflicting operations. Results for the regular and augmented conflict graphs remain unchanged, and similarly to CO a distributed MVCO enforcing algorithm exists, now for a mixed environment with both single-version and multi-version resources (now single-version is a special case of multi-version). As for CO, the MVCO algorithm needs only (unmodified) [[atomic commitment]] protocol messages with no additional communication overhead. Locking-based global deadlocks translate to voting deadlocks and are resolved automatically. In analogy to CO the following holds:

*'''The MVCO and Global one-copy-serializability Theorem'''

#MVCO compliance of every ''autonomous'' database system (or transactional object) in a mixed multidatabase environment of single-version and multi-version databases is a ''necessary condition'' for guaranteeing Global one-copy-serializability (1SER).
#MVCO compliance of every database system is a ''sufficient condition'' for guaranteeing Global 1SER.
#Locking-based global deadlocks are resolved automatically.

:'''Comment''': Now a CO compliant single-version database system is automatically also MVCO compliant.

MVCO can be further generalized to employ the generalization of ECO (MVECO).

====Example: CO based snapshot isolation (COSI)====

'''CO based snapshot isolation''' (COSI) is the intersection of ''[[Snapshot isolation]]'' (SI) with MVCO. SI is a [[multiversion concurrency control]] method widely utilized due to good performance and similarity to serializability (1SER) in several aspects. The theory in (Raz 1993b) for MVCO described above is utilized later in (Fekete et al. 2005) and other articles on SI, e.g., (Cahill et al. 2008);&lt;ref name=Cahill08&gt;Michael J. Cahill, Uwe R&#246;hm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], ''Proceedings of the 2008 ACM SIGMOD international conference on Management of data'', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award&lt;/ref&gt; see also [[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]] and the references there), for analyzing conflicts in SI in order to make it serializable. The method presented in (Cahill et al. 2008), ''Serializable snapshot isolation'' (SerializableSI), a low overhead modification of SI, provides good performance results versus SI, with only small penalty for enforcing serializability. A different method, by combining SI with MVCO (COSI), makes SI serializable as well, with a relatively low overhead, similarly to combining the generic CO algorithm with single-version mechanisms. Furthermore, the resulting combination, COSI, being MVCO compliant, allows COSI compliant database systems to inter-operate and transparently participate in a CO solution for distributed/global serializability (see below). Besides overheads also protocols' behaviors need to be compared quantitatively. On one hand, all serializable SI schedules can be made MVCO by COSI (by possible commit delays when needed) without aborting transactions. On the other hand, SerializableSI is known to unnecessarily abort and restart certain percentages of transactions also in serializable SI schedules.

===CO and its variants are transparently interoperable for global serializability===

With CO and its variants (e.g., SS2PL, SCO, OCO, ECO, and MVCO above) global serializability is achieved via ''atomic commitment'' protocol based distributed algorithms. For CO and all its variants atomic commitment protocol is the instrument to eliminate global cycles (cycles that span two or more databases) in the ''global augmented'' (and thus also regular) ''conflict graph'' (implicitly; no global data structure implementation is needed). In cases of either incompatible local commitment orders in two or more databases (when no global [[partial order]] can [[Embedding|embed]] the respective local partial orders together), or a data-access locking related voting deadlock, both implying a global cycle in the global augmented conflict graph and missing votes, the atomic commitment protocol breaks such cycle by aborting an undecided transaction on it (see [[commitment ordering#The distributed CO algorithm|The distributed CO algorithm]] above). Differences between the various variants exist at the local level only (within the participating database systems). Each local CO instance of any variant has the same role, to determine the position of every global transaction (a transaction that spans two or more databases) within the local commitment order, i.e., to determine when it is the transaction's turn to be voted on locally in the atomic commitment protocol. Thus, all the CO variants exhibit the same behavior in regard to atomic commitment. This means that they are all interoperable via atomic commitment (using the same software interfaces, typically provided as [[Service (systems architecture)|service]]s, some already [[international standard|standardized]] for atomic commitment, primarily for the [[two phase commit]] protocol, e.g., [[X/Open XA]]) and transparently can be utilized together in any distributed environment (while each CO variant instance is possibly associated with any relevant local concurrency control mechanism type).

In summary, any single global transaction can participate simultaneously in databases that may employ each any, possibly different, CO variant (while concurrently running processes in each such database, and running concurrently with local and other global transactions in each such database). The atomic commitment protocol is indifferent to CO, and does not distinguish between the various CO variants. Any ''global cycle'' generated in the augmented global conflict graph may span databases of different CO variants, and generate (if not broken by any local abort) a voting deadlock that is resolved by atomic commitment exactly the same way as in a single CO variant environment. ''local cycles'' (now possibly with mixed materialized and non-materialized conflicts, both serializability and data-access-locking deadlock related, e.g., SCO) are resolved locally (each by its respective variant instance's own local mechanisms).

'''Vote ordering''' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]), the union of CO and all its above variants, is a useful concept and global serializability technique. To comply with VO, local serializability (in it most general form, commutativity based, and including multi-versioning) and the ''vote order strategy'' (voting by local precedence order) are needed.

Combining results for CO and its variants, the following is concluded:

*'''The CO Variants Interoperability Theorem'''
#In a multi-database environment, where each database system (transactional object) is compliant with some CO variant property (VO compliant), any global transaction can participate simultaneously in databases of possibly different CO variants, and Global serializability is guaranteed (''sufficient condition'' for Global serializability; and Global one-copy-serializability (1SER), for a case when a multi-version database exists).
#If only local (to a database system) concurrency control information is utilized by every database system (each has the ''generalized autonomy'' property, a generalization of ''autonomy''), then compliance of each with some (any) CO variant property (VO compliance) is a ''necessary condition'' for guaranteeing Global serializability (and Global 1SER; otherwise they may be violated).
#Furthermore, in such environment data-access-locking related global deadlocks are resolved automatically (each such deadlock is generated by a global cycle in the ''augmented conflict graph'' (i.e., a ''voting deadlock''; see above), involving at least one data-access lock (non-materialized conflict) and two database systems; thus, not a cycle in the regular conflict graph and does not affect serializability).

==References==

*{{citation|first=Yoav|last=Raz|url=http://www.vldb.org/conf/1992/P292.PDF|title=The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment|work=Proceedings of the Eighteenth International Conference on Very Large Data Bases|pages=292&#8211;312|place=Vancouver, Canada|date=August 1992}} (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990)
*{{citation|first=Yoav|last=Raz|title=Serializability by Commitment Ordering|work=Information Processing Letters|volume=51|number=5|pages=257&#8211;264|date=September 1994|doi=10.1016/0020-0190(94)90005-1}}
*{{citation|first=Yoav|last=Raz|url=http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering|title=Theory of Commitment Ordering: Summary|date=June 2009|accessdate=November 11, 2011}}
*{{citation|first=Yoav|last=Raz|url=http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf|title=On the Significance of Commitment Ordering|publisher=Digital Equipment Corporation|date=November 1990}}
*&lt;cite id=Raz1991a&gt;Yoav Raz (1991a): US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,899 (ECO)] [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=2&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,900 (CO)]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,701,480 (MVCO)] &lt;/cite&gt;
*&lt;cite id=Raz1991b&gt;Yoav Raz (1991b): "The Commitment Order Coordinator (COCO) of a Resource Manager, or Architecture for Distributed Commitment Ordering Based Concurrency Control", DEC-TR 843, Digital Equipment Corporation, December 1991. &lt;/cite&gt;
*&lt;cite id=Raz1991c&gt;Yoav Raz (1991c): "Locking Based Strict Commitment Ordering, or How to improve Concurrency in Locking Based Resource Managers", DEC-TR 844, December 1991. &lt;/cite&gt;
*&lt;cite id=Raz1993a&gt;Yoav Raz (1993a): [http://portal.acm.org/citation.cfm?id=153858 "Extended Commitment Ordering or Guaranteeing Global Serializability by Applying Commitment Order Selectivity to Global Transactions."] ''Proceedings of the Twelfth ACM Symposium on Principles of Database Systems'' (PODS), Washington, DC, pp. 83-96, May 1993. (also DEC-TR 842, November 1991) &lt;/cite&gt;
*&lt;cite id=Raz1993b&gt;Yoav Raz (1993b): [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=281924  "Commitment Ordering Based Distributed Concurrency Control for Bridging Single and Multi Version Resources."] ''Proceedings of the Third IEEE International Workshop on Research Issues on Data Engineering: Interoperability in Multidatabase Systems'' (RIDE-IMS), Vienna, Austria, pp. 189-198, April 1993. (also DEC-TR 853, July 1992)  &lt;/cite&gt;

==Footnotes==
{{reflist}}

==External links==
*[http://sites.google.com/site/yoavraz2/the_principle_of_co Yoav Raz's Commitment ordering page]

{{DEFAULTSORT:Commitment Ordering}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]
[[Category:Distributed algorithms]]</text>
      <sha1>6gdw5whuoe507ttnv5ly7xfzxoqbyp2</sha1>
    </revision>
  </page>
  <page>
    <title>Storage model</title>
    <ns>0</ns>
    <id>8288646</id>
    <revision>
      <id>415292239</id>
      <parentid>387012749</parentid>
      <timestamp>2011-02-22T09:19:21Z</timestamp>
      <contributor>
        <username>Malcolma</username>
        <id>320496</id>
      </contributor>
      <minor />
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="319" xml:space="preserve">{{Unreferenced|date=December 2006}}
A '''storage model''' is a model that captures key ''physical'' aspects of data structure in a data store. 

On the other hand, a [[data model]] is a model that captures key ''logical'' aspects of data structure in a database.



[[Category:Data management]]


{{Compu-storage-stub}}</text>
      <sha1>bpfq040ldvq0kamxpkqqlrjtg8q3xhf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data quality</title>
    <ns>14</ns>
    <id>31206312</id>
    <revision>
      <id>547255735</id>
      <parentid>532024561</parentid>
      <timestamp>2013-03-27T13:07:26Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363880]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="195" xml:space="preserve">{{Cat main|Data quality}}
:''See also:'' [[:category:Data security]] ([[data loss]] prevention is in fact an assurance of data quality)


[[Category:Data management|Quality]]
[[Category:Quality]]</text>
      <sha1>08tzbporo203mz2h4zql4uxr5zgrlfh</sha1>
    </revision>
  </page>
  <page>
    <title>Master data management</title>
    <ns>0</ns>
    <id>15103022</id>
    <revision>
      <id>761409994</id>
      <parentid>759853047</parentid>
      <timestamp>2017-01-22T21:05:13Z</timestamp>
      <contributor>
        <username>RickBeesley</username>
        <id>29355407</id>
      </contributor>
      <comment>Added a paragraph about a key problem that frequently defeats efforts to reduce overhead through MDM. May be a bit long-winded. Based on my own experience in 20 years working at a major global consulting brand.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11326" xml:space="preserve">{{refimprove|date=April 2012}}
In business, '''master data management''' ('''MDM''') comprises the processes, governance, policies, standards and tools that consistently define and manage the critical data of an [[organization]] to provide a single point of reference.&lt;ref&gt;"What is Master Data" SearchDataManagement, TechTarget, 22 November 2010, http://searchdatamanagement.techtarget.com/definition/master-data-management&lt;/ref&gt;

The data that is mastered may include:

* [[reference data]] &amp;ndash; the business objects for transactions, and the dimensions for analysis
* analytical data &amp;ndash; supports decision making&lt;ref&gt;"Introduction to Master Data Management", Mark Rittman, Director, Rittman Mead Consulting, 9 May 2008 https://s3.amazonaws.com/rmc_docs/Introduction%20to%20Oracle%20Master%20Data%20Management.pdf&lt;/ref&gt;&lt;ref&gt;"[http://www.b-eye-network.com/view/2918 "Defining Master Data"], David Loshin, BeyeNetwork, May 2006&lt;/ref&gt;

In [[computing]], a master data management tool can be used to support master data management by removing duplicates, standardizing data (mass maintaining), and incorporating rules to eliminate incorrect data from entering the system in order to create an authoritative source of master data. Master data are the products, accounts and parties for which the business transactions are completed. The root cause problem stems from business unit and product line segmentation, in which the same customer will be serviced by different product lines, with redundant data being entered about the customer (a.k.a. party in the role of customer) and account in order to process the transaction. The redundancy of party and account data is compounded in the front to back office life cycle, where the authoritative single source for the party, account and product data is needed but is often once again redundantly entered or augmented.

Master data management has the objective of providing processes for collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure consistency and control in the ongoing maintenance and application use of this information.

The term recalls the concept of a ''master file'' from an earlier computing era.

==Definition==
Master data management (MDM) is a comprehensive method of enabling an enterprise to link all of its critical data to one file, called a master file, that provides a common point of reference. When properly done, master data management streamlines data sharing among personnel and departments. In addition, master data management can facilitate computing in multiple system architectures, platforms and applications.&lt;ref&gt;{{cite web|title=Master data management|url=http://www.ibm.com/software/data/master-data-management/overview.html|publisher=[[IBM]]}}&lt;/ref&gt;

At its core Master Data Management (MDM) can be viewed as a "discipline for specialized quality improvement"&lt;ref&gt;DAMA-DMBOK Guide,2010 DAMA International&lt;/ref&gt; defined by the policies and procedures put in place by a data governance organization.  The ultimate goal being to provide the end user community with a "trusted single version of the truth" from which to base decisions.

==Issues==
At a basic level, master data management seeks to ensure that an organization does not use multiple (potentially [[Consistency (database systems)|inconsistent]]) versions of the same master data in different parts of its operations, which can occur in large organizations. A typical example of poor master data management is the scenario of a bank at which a [[customer]] has taken out a [[Mortgage loan|mortgage]] and the bank begins to send mortgage solicitations to that customer, ignoring the fact that the person already has a mortgage account relationship with the bank. This happens because the customer information used by the marketing section within the bank lacks integration with the customer information used by the customer services section of the bank.  Thus the two groups remain unaware that an existing customer is also considered a sales lead. The process of [[record linkage]] is used to associate different records that correspond to the same entity, in this case the same person.

Other problems include (for example) issues with the [[data quality|quality of data]], consistent [[classification]] and identification of data, and [[Data validation and reconciliation|data-reconciliation]] issues.  Master data management of disparate data systems requires [[data transformation]]s as the data extracted from the disparate source data system is transformed and loaded into the master data management hub.  To synchronize the disparate source master data, the managed master data extracted from the master data management hub is again transformed and loaded into the disparate source data system as the master data is updated.  As with other [[Extract, Transform, Load]]-based data movement, these processes are expensive and inefficient to develop and to maintain which greatly reduces the [[return on investment]] for the master data management product.

One of the most common reasons some large corporations experience massive issues with master data management is growth through [[merger]]s or [[Takeover|acquisitions]].  Any organizations which merge will typically create an entity with duplicate master data (since each likely had at least one master database of its own prior to the merger).  Ideally, [[database administrator]]s resolve this problem through [[Data deduplication|deduplication]] of the master data as part of the merger. In practice, however, reconciling several master data systems can present difficulties because of the dependencies that existing applications have on the master databases.  As a result, more often than not the two systems do not fully merge, but remain separate, with a special reconciliation process defined that ensures consistency between the data stored in the two systems.  Over time, however, as further mergers and acquisitions occur, the problem multiplies, more and more master databases appear, and data-reconciliation processes become extremely complex, and consequently unmanageable and unreliable. Because of this trend, one can find organizations with 10, 15, or even as many as 100 separate, poorly integrated master databases, which can cause serious operational problems in the areas of [[customer satisfaction]], operational efficiency, [[decision support]], and regulatory compliance.

Another problem concerns determining the proper degree of detail and normalization to include in the master data schema. For example, in a federated HR environment, the enterprise may focus on storing people data as a current status, adding a few fields to identify date of hire, date of last promotion, etc. However this simplification can introduce business impacting errors into dependent systems for planning and forecasting. The stakeholders of such systems may be forced to build a parallel network of new interfaces to track onboarding of new hires, planned retirements, and divestment, which works against one of the aims of master data management.  
==Solutions==
Processes commonly seen in master data management include source identification, data collection, [[data transformation]], [[database normalization|normalization]], rule administration, error detection and correction, data consolidation, [[data storage device|data storage]], data distribution, data classification, taxonomy services, item master creation, schema mapping, product codification, data enrichment and [[data governance]].

The selection of entities considered for master data management depends somewhat on the nature of an organization. In the common case of commercial enterprises, master data management may apply to such entities as customer ([[customer data integration]]), product ([[product information management]]), employee, and vendor. Master data management processes identify the sources from which to collect descriptions of these entities. In the course of transformation and normalization, administrators adapt descriptions to conform to standard formats and data domains, making it possible to remove duplicate instances of any entity. Such processes generally result in an organizational master data management repository, from which  all requests for a certain entity instance produce the same description, irrespective of the originating sources and the requesting destination.

The tools include [[data networks]], [[file systems]], a [[data warehouse]], [[data mart]]s, an [[operational data store]], [[data mining]], [[data analysis]], [[data visualization]], [[Federated database system|data federation]] and [[data virtualization]]. One of the newest tools, virtual master data management utilizes data virtualization and a persistent metadata server to implement a multi-level automated master data management hierarchy.

==Transmission of master data==
There are several ways in which master data may be collated and distributed to other systems.&lt;ref&gt;[http://dama-ny.com/images/meeting/101509/damanyc_mdmprint.pdf "Creating the Golden Record: Better Data Through Chemistry"], DAMA, slide 26, Donald J. Soulsby, 22 October 2009&lt;/ref&gt; This includes:

* Data consolidation &#8211; The process of capturing master data from multiple sources and integrating into a single hub ([[operational data store]]) for replication to other destination systems.
* [[Federated database system|Data federation]] &#8211; The process of providing a single virtual view of master data from one or more sources to one or more destination systems.
* Data propagation &#8211; The process of copying master data from one system to another, typically through point-to-point interfaces in legacy systems.

==See also==
* [[Reference data]]
* [[Master data]]
* [[Record linkage]]
* [[Data steward]]
* [[Data visualization]]
* [[Customer data integration]]
* [[Data integration]]
* [[Product information management]]
* [[Identity resolution]]
* [[Enterprise information integration]]
* [[Linked data]]
* [[Semantic Web]]
* [[Data governance]]
* [[Operational data store]]
* [[Single customer view]]

==References==
{{reflist}}

==External links==
* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 Microsoft: The What, Why, and How of Master Data Management]
* [http://msdn.microsoft.com/en-us/library/bb410798.aspx Microsoft: Master Data Management (MDM) Hub Architecture]
* [http://mike2.openmethodology.org/wiki/Master_Data_Management_Solution_Offering Open Methodology for Master Data Management]
* [http://www.semarchy.com/overview/why-do-i-need-mdm/ Semarchy: Why do I Need MDM? (Video)]
* [http://www.mdmalliancegroup.com/ MDM Community]
* [http://www.stibosystems.com/Global/explore-stibo-systems/master-data-management.aspx Multidomain Master Data Management]
* [http://blogs.gartner.com/andrew_white/2014/06/05/reprise-when-is-master-data-and-mdm-not-master-data-or-mdm/ Reprise: When is Master Data and MDM Not Master Data or MDM?]
* [http://www.orchestranetworks.com/mdm/ Master Data Management (Multidomain)]

{{Data warehouse}}
{{databases}}

{{DEFAULTSORT:Master Data Management}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]</text>
      <sha1>3w38tur3y0z1jqttj6nhejirkeluyxs</sha1>
    </revision>
  </page>
  <page>
    <title>Social information architecture</title>
    <ns>0</ns>
    <id>31377324</id>
    <revision>
      <id>532217079</id>
      <parentid>463731820</parentid>
      <timestamp>2013-01-09T18:15:27Z</timestamp>
      <contributor>
        <ip>69.106.238.83</ip>
      </contributor>
      <comment>/* See also */ Category:Internet -&gt; World Wide Web</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6018" xml:space="preserve">{{orphan|date=April 2011}}
'''Social information architecture''' is a sub-domain of [[information architecture]] which deals with the social aspects of conceptualizing, modeling and organizing information. Social Information Architecture, also known as Social iA &lt;ref&gt;[http://sweetinformationarchitecture.net/social-information-architecture/%20 Sweet Information Architecture]&lt;/ref&gt; has become more relevant because of the rise of [[Social Media]] and [[Web 2.0]] in recent times.

== Approach ==
There are different approaches to the explanation of Social iA. 

===A) The architecture model (internal space)===

Architects designing a physical community space, have to consider how the architecture will shape social interactions. A long hallway of offices creates an utterly different dynamic than desks with arranged in an open space. One might foster individuality, privacy, propriety; the other: collaboration, distraction, communalism.

Still, physical spaces can be flexibly repurposed and worked around if the inhabitants desire a social dynamic not instantly afforded by the space. Office doors can be left open to invite easier interaction. Partitions can be raised between adjacent desks to limit distraction and increase privacy.

That&#8217;s physical architecture. The information architectures of online communities are far more deterministic and far less flexible. They literally define the social architecture by pre-specifying in immutable computer code what information you have access to, who you can talk to, where you can go. In the online world, information architecture = social architecture.&lt;ref&gt;http://www.steinbock.org/&lt;/ref&gt;

===B) The social dialogue and information model (external space)===

All  major brands use information architecture to market their products online, it is then commonly wrapped under the umbrella phrase 'digital strategy'. Information architecture used for strategic purposes encompasses brand [[SEO]], strategic placement of virals, social media presence etc. 

Charities, news outlets and social dialogue forums can make a much more specific use of the same tools for positive and  important social purposes. Social Information Architecture is perceived as the socially conscious wing of  commercial information architecture &lt;ref&gt;http://www.sweetinformationarchitecture.net&lt;/ref&gt; and function to exchange information and ideas between people and groups. 

Social iA can pick up on conflicting issues that are treated with misunderstanding between  cultures and leaves individuals and societies vulnerable to exploitation and manipulation. Since the net has such a far reach it is obvious to use it for meaningful and coordinated social dialogue. 

Example of such issues are faith, environment,  politics, climate change, war, injustice and other social challenges. Information architecture can  help create frameworks in which sharing information brings people together, inspires and encourages them to participate in a forward thinking and unfragmented way. One of its core activities is to spread messages that bring people from opposite sites of social  and cultural spectrums together and to confront uncomfortable subject head on.

== How does social information architecture work? ==
Social iA utilizes a variety of [[Web2.0]] applications to filter relevant or valuable information and weave them in appropriate information repository or provide feedback to interesting channels. Social iA makes strategic use of Search Engines, Social Media, Google Algorithms, as well as websites, video &amp; news channels. It &#8216;reads&#8217; or 'listens' to social conversations and [[search engine]] queries and engages with the net actively to gather clues about the world&#8217;s pulse on the internet. It assesses data, social &amp; political trends, and respond with targeted campaigns to give people ideas, as well as help people with making sense of information.

== Principals ==
Dan Brown in his paper 8 Principals of Social Information Architecture &lt;ref&gt;[http://socialinformationarchitecture.org.uk/paper/8principal_infoarchi.pdf Eight Principles of Information Architecture], Dan Brown. Published in the Bulletin of the American Society for Information Science and Technology &#8211; August/September 2010 &#8211; Volume 36, Number 6&lt;/ref&gt; enlists the following principals:&lt;br /&gt;
1. The principle of objects: Treat content as a living, breathing thing,
with a lifecycle, behaviors and attributes.&lt;br /&gt;
2. The principle of choices: Create pages that offer meaningful choices to users, keeping the range of choices available focused on a particular task. &lt;br /&gt;
3. The principle of disclosure: Show only enough information to help
people understand what kinds of information they&#8217;ll find as they dig
deeper. &lt;br /&gt;
4. The principle of exemplars: Describe the contents of categories by
showing examples of the contents.&lt;br /&gt;
5. The principle of front doors: Assume at least half of the website&#8217;s
visitors will come through some page other than the home page.&lt;br /&gt;
6. The principle of multiple classification: Offer users several different classification schemes to browse the site&#8217;s content.&lt;br /&gt;
7. The principle of focused navigation: Don&#8217;t mix apples and oranges
in your navigation scheme.&lt;br /&gt;
8. The principle of growth: Assume the content you have today is a
small fraction of the content you will have tomorrow.

== What can social information architecture achieve? ==
Social information architecture has many potentials in terms of fostering social connections and how information is shared in social spaces on the web.

== References==
{{Reflist}}

== See also ==
Wodtke, Christina and Govella, Austin '''Information Architecture: Blueprints for the Web''' (2009) Second Edition, Published by New Riders

{{Semantic Web}}

[[Category:Information architects|*Information architecture]]
[[Category:World Wide Web]]
[[Category:Data management]]
[[Category:Information science]]
[[Category:Information technology]]
[[Category:Digital technology]]
[[Category:New media]]</text>
      <sha1>j0n390mqdmaso1cz1312utiqia6pjii</sha1>
    </revision>
  </page>
  <page>
    <title>Australian National Data Service</title>
    <ns>0</ns>
    <id>31960097</id>
    <revision>
      <id>750314629</id>
      <parentid>745574692</parentid>
      <timestamp>2016-11-19T01:07:08Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2761" xml:space="preserve">The '''Australian National Data Service''' (ANDS) was established in 2008 to help address the challenges of storing and managing Australia's research data, and making it discoverable and accessible for validation and reuse. It is a joint collaboration between [[Monash University]], [[The Australian National University]] and [[CSIRO]].

==Background==
ANDS is funded by the [[Australian Department of Education]]. The funding has been provided through Australian Government's National Collaborative Research Infrastructure Strategy (NCRIS) as part of the Platforms for Collaboration Investment Plan.&lt;ref&gt;{{cite web|url=http://ncris.innovation.gov.au/Capabilities/Pages/PfC.aspx#ANDS |title=Platforms for Collaboration |accessdate=2011-06-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20110702094824/http://ncris.innovation.gov.au:80/Capabilities/Pages/PfC.aspx |archivedate=2011-07-02 |df= }}&lt;/ref&gt; The NCRIS roadmap emphasized the vital importance of eResearch Infrastructure to Australian future research competitiveness.&lt;ref&gt;{{cite web|title=eResearch Infrastructure |url=http://www.pfc.org.au/bin/view/Main |publisher=NCRIS |accessdate=25 June 2011 }}{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; In mid-2009 ANDS was further funded by the Education Investment Fund (EIF) for the establishment of the Australian Research Data Commons under the Australian Government&#8217;s Super Science Initiative.&lt;ref&gt;{{cite web|title=Super Science Initiative |url=http://www.innovation.gov.au/SCIENCE/RESEARCHINFRASTRUCTURE/Pages/SuperScience.aspx |publisher=DIISR |accessdate=25 June 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110601193908/http://www.innovation.gov.au/Science/ResearchInfrastructure/Pages/SuperScience.aspx |archivedate=1 June 2011 |df= }}&lt;/ref&gt;

==Research Data Australia==
''Research Data Australia'' (formerly the ''ANDS Collections Registry'') is an online discovery service run by ANDS.&lt;ref&gt;[http://researchdata.ands.org.au/home/about Research Data Australia]&lt;/ref&gt;&lt;ref&gt;[http://www.ands.org.au/resource/registry.html ANDS Collections Registry] {{webarchive |url=https://web.archive.org/web/20140302113241/http://www.ands.org.au/resource/registry.html |date=March 2, 2014 }}&lt;/ref&gt; It allows researchers to publicise the existence of their research data and enable prospective users of that data to find it.

''Research Data Australia'' makes use of the [[ISO 2146]]-based [[RIF-CS]] metadata standard.&lt;ref&gt;[http://ands.org.au/guides/cpguide/cpgrifcs.html About RIF-CS]&lt;/ref&gt;

== External links ==
* {{Official website|http://www.ands.org.au}}
* [http://researchdata.ands.org.au Research Data Australia]

==References==
&lt;references /&gt;

[[Category:Data management]]


{{Australia-org-stub}}</text>
      <sha1>awj1rh15yoacuzkznnsp5hxiuj8a8mb</sha1>
    </revision>
  </page>
  <page>
    <title>Super column</title>
    <ns>0</ns>
    <id>31220085</id>
    <revision>
      <id>721058477</id>
      <parentid>577671349</parentid>
      <timestamp>2016-05-19T15:03:52Z</timestamp>
      <contributor>
        <username>Vrettos</username>
        <id>169378</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2183" xml:space="preserve">[[Image:SuperColumn (data store).png|300px|thumb|The super column consists of a (unique) super column name, and a number of columns.]]
A '''super column''' is a [[tuple]] (a pair) with a binary super column name and a value that maps it to many columns.&lt;ref&gt;{{cite web
| accessdate = 2011-03-18
| author = Arin Sarkissian
| date = 2009-09-01
| location = http://arin.me/post/40054651676/wtf-is-a-supercolumn-cassandra-data-model
| publisher = Arin Sarkissian
| title = WTF is a SuperColumn? An Intro to the Cassandra Data Model
| quote = A SuperColumn is a tuple with a binary name &amp; a value which is a map containing an unbounded number of Columns &#8211; keyed by the Column&#8216;s name.
| url = }}&lt;/ref&gt; They consist of a key-value pairs, where the values are columns. Theoretically speaking, super columns are ([[Sorting algorithm|sorted]]) [[associative array]] of columns.&lt;ref&gt;{{cite web
| accessdate = 2011-03-18
| location = http://wiki.apache.org/cassandra/DataModel
| publisher = Apache Cassandra
| title = Cassandra wiki: Data Model: Super columns
| url = http://wiki.apache.org/cassandra/DataModel}}&lt;/ref&gt; Similar to a regular [[column family]] where a row is a sorted map of column names and column values, a row in a super column family is a sorted map of super column names that maps to column names and column values. 

A super column is part of a [[keyspace (data model)]] together with other super columns and column families, and columns.

==Code example==
Written in the [[JSON]]-like syntax, a super column definition can be like this:

&lt;source lang="SQL"&gt;
 {
   "mccv": {
     "Tags": {
       "cassandra": {
         "incubator": {"url": "http://incubator.apache.org/cassandra/"},
         "jira": {"url": "http://issues.apache.org/jira/browse/CASSANDRA"}
       },
       "thrift": {
         "jira": {"url": "http://issues.apache.org/jira/browse/THRIFT"}
       }
     }
   }
 }
&lt;/source&gt;

==See also==
* [[Column (data store)]]
* [[Keyspace (NoSQL)]]

==References==
{{reflist}}

==External links==
* [http://wiki.apache.org/cassandra/DataModel The Apache Cassandra data model]

&lt;!--Interwikies--&gt;
[[Category:Data_management]]
&lt;!--Categories--&gt;


{{database-stub}}</text>
      <sha1>lh17mds6cufklvwkhdwd88sgi2a6tln</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data partitioning</title>
    <ns>14</ns>
    <id>19073984</id>
    <revision>
      <id>464076617</id>
      <parentid>419220774</parentid>
      <timestamp>2011-12-04T18:43:21Z</timestamp>
      <contributor>
        <username>Starcheerspeaksnewslostwars</username>
        <id>11554556</id>
      </contributor>
      <comment>removed [[Category:Data managementPartitioning]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28" xml:space="preserve">[[Category:Data management]]</text>
      <sha1>tpzfnyay41jj855s8j16rxxvd2yg34n</sha1>
    </revision>
  </page>
  <page>
    <title>Linked data</title>
    <ns>0</ns>
    <id>11174052</id>
    <revision>
      <id>751161017</id>
      <parentid>751036988</parentid>
      <timestamp>2016-11-23T19:37:02Z</timestamp>
      <contributor>
        <username>Denny</username>
        <id>10969</id>
      </contributor>
      <comment>Undid revision 751036988 by [[Special:Contributions/109.193.94.192|109.193.94.192]] ([[User talk:109.193.94.192|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14182" xml:space="preserve">In [[computing]], '''linked data''' (often capitalized as '''Linked Data''') is a method of publishing structured data so that it can be interlinked and become more useful through [[semantic query|semantic queries]]. It builds upon standard [[World Wide Web|Web]] technologies such as [[Hypertext Transfer Protocol|HTTP]], [[Resource Description Framework|RDF]] and [[uniform resource identifier|URIs]], but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.&lt;ref name=linkeddatastorysofar&gt;{{Cite journal |url=http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf
 |title=Linked Data&amp;mdash;The Story So Far |last=Bizer |first=Christian |last2=Heath |first2=Tom |last3=Berners-Lee
 |first3=Tim |author3-link=Tim Berners-Lee |year=2009 |accessdate=2010-12-18 |doi=10.4018/jswis.2009081901 |issn=1552-6283
 |journal=International Journal on Semantic Web and Information Systems |volume=5 |issue=3 |pages=1&#8211;22}} Solving Semantic Interoperability Conflicts in Cross&#8211;Border E&#8211;Government Services.&lt;/ref&gt;

[[Tim Berners-Lee]], director of the [[World Wide Web Consortium]] (W3C), coined the term in a 2006 design note about the [[Semantic Web]] project.&lt;ref name=DesignIssues&gt;{{cite web |url=http://www.w3.org/DesignIssues/LinkedData.html
 |title=Linked Data |work=Design Issues |author=Tim Berners-Lee |authorlink=Tim Berners-Lee |date=2006-07-27
 |publisher=[[W3C]] |accessdate=2010-12-18}}&lt;/ref&gt;

== Principles ==
Tim Berners-Lee outlined four principles of linked data in his "Linked Data" note of 2006,&lt;ref name=DesignIssues/&gt; paraphrased along the following lines:

&lt;blockquote&gt;
# Use [[uniform resource identifier|URIs]] to name (identify) things.
# Use [[Hypertext Transfer Protocol|HTTP]] URIs so that these things can be looked up (interpreted, "dereferenced").
# Provide useful information about what a name identifies when it's looked up, using open standards such as [[Resource Description Framework|RDF]], [[SPARQL]], etc.
# Refer to other things using their HTTP URI-based names when publishing data on the Web.
&lt;/blockquote&gt;

Tim Berners-Lee gave a presentation on linked data at the [[TED (conference)|TED]] 2009 conference.&lt;ref&gt;{{cite web |url=http://www.ted.com/talks/tim_berners_lee_on_the_next_web.html |title=Tim Berners-Lee on the next Web}}&lt;/ref&gt;  In it, he restated the linked data principles as three "extremely simple" rules:

&lt;blockquote&gt;
# All kinds of conceptual things, they have names now that start with HTTP.
# If I take one of these HTTP names and I look it up...I will get back some data in a standard format which is kind of useful data that somebody might like to know about that thing, about that event.
# When I get back that information it's not just got somebody's height and weight and when they were born, its got relationships. And when it has relationships, whenever it expresses a relationship then the other thing that it's related to is given one of those names that starts with HTTP.
&lt;/blockquote&gt;

== Components ==
* [[Uniform resource identifier|URI]]s
* [[HTTP]]
* [[Structured data]] using [[controlled vocabulary]] terms and dataset definitions expressed in [[Resource Description Framework]] [[serialization]] formats such as [[RDFa]], [[RDF/XML]], [[Notation 3|N3]], [[Turtle (syntax)|Turtle]], or [[JSON-LD]]
* [[Linked Data Platform]]

==Linked open data==
'''Linked open data''' is linked data that is [[open content]].&lt;ref&gt;{{cite web|url=http://linkeddata.org/faq|title=Frequently Asked Questions (FAQs) - Linked Data - Connect Distributed Data across the Web|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.coar-repositories.org/activities/repository-observatory/second-edition-linked-open-data/7-things-you-should-know-about-open-data/|title=COAR &#187;   7 things you should know about&#8230;Linked Data|publisher=}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://openorg.ecs.soton.ac.uk/wiki/Linked_Data_Basics_for_Techies#Open_Linked_Data|title=Linked Data Basics for Techies|publisher=}}&lt;/ref&gt; Tim Berners-Lee gives the clearest definition of linked open data in differentiation with linked data. {{Quote |text=Linked Open Data (LOD) is Linked Data which is released under an open licence, which does not impede its reuse for free. |author=Tim Berners-Lee |title=Linked Data&lt;ref name=DesignIssues /&gt;&lt;ref&gt;{{cite web|url=http://5stardata.info/en|title=5 Star Open Data}}&lt;/ref&gt;}} Large linked open data sets include [[DBpedia]] and [[Freebase]].

=== History ===

The term "linked open data" has been in use since at least February 2007, when the "Linking Open Data" mailing list&lt;ref&gt;{{cite web|url=http://lists.w3.org/Archives/Public/public-lod/|title=public-lod@w3.org Mail Archives|publisher=}}&lt;/ref&gt; was created.&lt;ref&gt;{{cite web|url=http://www.w3.org/wiki/SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|title=SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|publisher=}}&lt;/ref&gt; The mailing list was initially hosted by the [[SIMILE]] project&lt;ref&gt;{{cite web|url=http://simile.mit.edu/mail.html|title=SIMILE Project - Mailing Lists|publisher=}}&lt;/ref&gt; at the [[Massachusetts Institute of Technology]].

=== Linking Open Data community project ===
[[File:LOD Cloud 2014.svg|thumb|400px|The above diagram shows which Linking Open Data datasets are connected, as of August 2014.  This was produced by the 
Linked Open Data Cloud project, which was started in 2007.  Some sets may include copyrighted data which is freely available.&lt;ref&gt;Linking open data cloud diagram 2014, by Max Schmachtenberg, Christian Bizer, Anja Jentzsch and Richard Cyganiak. http://lod-cloud.net/&lt;/ref&gt;]]

The goal of the W3C Semantic Web Education and Outreach group's Linking Open Data community project is to extend the Web with a [[Knowledge commons|data commons]] by publishing various [[open knowledge|open]] [[dataset]]s as RDF on the Web and by setting [[Resource Description Framework|RDF]] links between data items from different data sources. In October 2007, datasets consisted of over two billion [[RDF triples]], which were interlinked by over two million RDF links.&lt;ref&gt;[http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data]&lt;/ref&gt;&lt;ref&gt;{{cite book |last1=Fensel |first1=Dieter |last2=Facca |first2= Federico Michele |last3=Simperl |first3=Elena |last4=Ioan |first4=Toma |title=Semantic Web Services |year=2011 |publisher=Springer|isbn=3642191924 |pages=99}}&lt;/ref&gt;  By September 2011 this had grown to 31 billion RDF triples, interlinked by around 504 million RDF links.  A detailed statistical breakdown was published in 2014.&lt;ref&gt;http://linkeddatacatalog.dws.informatik.uni-mannheim.de/state/&lt;/ref&gt;

=== European Union projects ===
There are a number of European Union projects{{Definition|date=June 2013}} involving linked data. These include the linked open data around the clock (LATC) project,&lt;ref&gt;[http://latc-project.eu/ Linked open data around the clock (LATC)]&lt;/ref&gt; the PlanetData project,&lt;ref&gt;[http://planet-data.eu/ PlanetData]&lt;/ref&gt; the DaPaaS (Data-and-Platform-as-a-Service) project,&lt;ref&gt;[http://project.dapaas.eu/ DaPaaS]&lt;/ref&gt; and  the Linked Open Data 2 (LOD2) project.&lt;ref&gt;[http://lod2.eu/ Linking Open Data 2 (LOD2)]&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&amp;ACTION=D&amp;CAT=PROJ&amp;RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects &#8211; LOD2 |date=2010-04-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://static.lod2.eu/Deliverables/LOD2_D12.5.1_Project_Fact_Sheet_Version.pdf |title=LOD2 Project Fact Sheet &#8211; Project Summary |date=2010-09-01 |accessdate=2010-12-18}}&lt;/ref&gt; Data linking is one of the main goals of the [[EU Open Data Portal]], which makes available thousands of datasets for anyone to reuse and link.

=== Datasets ===

* [[DBpedia]] &#8211; a dataset containing extracted data from Wikipedia; it contains about 3.4 million concepts described by 1 billion [[Semantic triple|triples]], including abstracts in 11 different languages
* [[FOAF (software)|FOAF]] &#8211; a dataset describing persons, their properties and relationships
* [[GeoNames]] provides RDF descriptions of more than {{formatnum:7500000}} geographical features worldwide.
* [[UMBEL]] &#8211; a lightweight reference structure of {{formatnum:20000}} subject concept classes and their relationships derived from [[OpenCyc]], which can act as binding classes to external data; also has links to 1.5 million named entities from DBpedia and [[YAGO (ontology)|YAGO]]
* [[Wikidata]] &#8211; a collaboratively-created linked dataset that acts as central storage for the structured data of its [[Wikimedia]] sister projects

=== Dataset instance and class relationships ===
Clickable diagrams that show the individual datasets and their relationships within the DBpedia-spawned LOD cloud (as shown by the figures to the right) are available.&lt;ref&gt;[http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-07-14.html Instance relationships amongst datasets]&lt;/ref&gt;&lt;ref&gt;[http://web.archive.org/web/20110828103804/http://umbel.org/sites/umbel.org/lod/lod_constellation.html Class relationships amongst datasets]&lt;/ref&gt;

==See also==
* [[Authority control]] &#8211; about ''controlled headings'' in library catalogs
* [[Citation analysis]] &#8211; for citations between scholarly articles
* [[Hyperdata]]
* [[Linked data page]]
* [[Network model]] &#8211; an older type of database management system
* [[Schema.org]]
* [[Web Ontology Language]]

== References ==
{{reflist|30em}}

== Further reading ==
{{ref begin|2}}
* Ahmet Soylu, Felix M&#246;dritscher, and Patrick De Causmaecker. 2012. [http://www.ahmetsoylu.com/wp-content/uploads/2013/10/soylu_ICAE2012.pdf &#8220;Ubiquitous Web Navigation through Harvesting Embedded Semantic Data: A Mobile Scenario.&#8221;] Integrated Computer-Aided Engineering 19 (1): 93&#8211;109.
* ''[http://linkeddatabook.com/book Linked Data: Evolving the Web into a Global Data Space]'' (2011) by Tom Heath and Christian Bizer, Synthesis Lectures on the Semantic Web: Theory and Technology, Morgan &amp; Claypool &lt;!-- note this resources supersedes the tutorial [http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/ How to publish Linked Data on the Web] by Bizer, Cyganiak, and Heath --&gt;
* [http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/LinkedDataTutorial/ How to Publish Linked Data on the Web], by Chris Bizer, Richard Cyganiak and Tom Heath, Linked Data Tutorial at Freie Universit&#228;t Berlin, Germany, 27 July 2007.
* [http://www.scientificamerican.com/article.cfm?id=berners-lee-linked-data The Web Turns 20: Linked Data Gives People Power], part 1 of 4, by Mark Fischetti, ''[[Scientific American]]'' 2010 October 23
* [http://knoesis.wright.edu/library/publications/linkedai2010_submission_13.pdf Linked Data Is Merely More Data] &#8211; Prateek Jain, Pascal Hitzler, Peter Z. Yeh, Kunal Verma, and Amit P. Sheth. In: Dan Brickley, Vinay K. Chaudhri, Harry Halpin, and Deborah McGuinness: ''Linked Data Meets Artificial Intelligence''. Technical Report SS-10-07, AAAI Press, Menlo Park, California, 2010, pp.&amp;nbsp;82&#8211;86.
* [http://knoesis.org/library/resource.php?id=1718 Moving beyond sameAs with PLATO: Partonomy detection for Linked Data] &#8211; Prateek Jain, Pascal Hitzler, Kunal Verma, Peter Z. Yeh, Amit Sheth. In:  Proceedings of the 23rd ACM Hypertext and Social Media conference (HT 2012), Milwaukee, WI, USA, June 25&#8211;28, 2012.
* Freitas, Andr&#233;, Edward Curry, Jo&#227;o Gabriel Oliveira, and Sean O&#8217;Riain. 2012. [http://www.edwardcurry.org/publications/freitas_IC_12.pdf &#8220;Querying Heterogeneous Datasets on the Linked Data Web: Challenges, Approaches, and Trends.&#8221;] IEEE Internet Computing 16 (1): 24&#8211;33.
* [http://www2008.org/papers/pdf/p1265-bizer.pdf Linked Data on the Web] &#8211; Chris Bizer, Tom Heath, [[Kingsley Uyi Idehen]], [[Tim Berners-Lee]]. In Proceedings WWW2008, Beijing, China
* [http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkingOpenData.pdf Interlinking Open Data on the Web] &#8211; Chris Bizer, Tom Heath, Danny Ayers, Yves Raimond. In Proceedings Poster Track, ESWC2007, Innsbruck, Austria
* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data] &#8211; Prateek Jain, Pascal Hitzler, Amit Sheth, Kunal Verma, Peter Z. Yeh. In proceedings of the 9th International Semantic Web Conference, ISWC 2010, Shanghai, China
* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3121711/ Linked open drug data for pharmaceutical research and development] - J Cheminform. 2011; 3: 19. Samwald, Jentzsch, Bouton, Kalles&#248;e, Willighagen, Hajagos, Marshall, Prud'hommeaux, Hassenzadeh, Pichler, and Stephens (May 2011)
* [http://www.community-of-knowledge.de/beitrag/the-hype-the-hope-and-the-lod2-soeren-auer-engaged-in-the-next-generation-lod/ Interview with S&#246;ren Auer, head of the LOD2 project about the continuation of LOD2 in 2011], June 2011
* [http://www.semantic-web.at/LOD-TheEssentials.pdf Linked Open Data: The Essentials] - Florian Bauer and Martin Kaltenb&#246;ck (January 2012)
* [http://semanticweb.com/the-flap-of-a-butterfly-wing_b26808 The Flap of a Butterfly Wing] - semanticweb.com Richard Wallis (February 2012)
{{ref end}}

== External links ==
* [http://www.w3.org/wiki/LinkedData LinkedData] at the W3C Wiki
* [http://linkeddata.org LinkedData.org]
* [http://virtuoso.openlinksw.com/white-papers/ OpenLink Software white papers]
* [http://demo.openlinksw.com/Demo/customers/CustomerID/ALFKI%23this Data from Northwind SQL schema as linked data], use case demo
* [http://nomisma.org/ Linked data for the discipline of numismatics], use case demo
* [http://en.lodlive.it Interactive LOD demo]
* [http://americanartcollaborative.org/ American Art Collaborative], consortium of US art museums committed to establishing a critical mass of linked open data on American art

{{Semantic Web}}
{{Open data navbox}}

{{Authority control}}

[[Category:Cloud standards]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
[[Category:Hypermedia]]
[[Category:Internet terminology]]
[[Category:Open data]]
[[Category:World Wide Web]]
[[Category:Semantic Web]]</text>
      <sha1>37fwszmf1rok7oe9e13vku7016ksi06</sha1>
    </revision>
  </page>
  <page>
    <title>Disaster recovery plan</title>
    <ns>0</ns>
    <id>6309764</id>
    <revision>
      <id>754651990</id>
      <parentid>754605324</parentid>
      <timestamp>2016-12-13T19:56:11Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 5 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34601" xml:space="preserve">{{merge to|Business continuity planning|date=June 2015}}
A '''disaster recovery plan''' (DRP) is a documented process or set of procedures to recover and protect a business [[Information_technology|IT]] infrastructure in the event of a [[disaster]].&lt;ref name="5 tips"&gt;{{cite web |url=http://www.smallbusinesscomputing.com/News/ITManagement/5-tips-to-build-an-effective-disaster-recovery-plan.html |title=5 Tips to Build an Effective Disaster Recovery Plan |publisher=Small Business Computing |date=14 June 2012 |accessdate=9 August 2012 |first1=Bill |last1=Abram}}&lt;/ref&gt;  Such a plan, ordinarily documented in written form, specifies  procedures an organization is to follow in the event of a disaster. It is "a comprehensive statement of consistent actions to be taken before, during and after a disaster."&lt;ref name="DR journal"&gt;{{cite web |url=http://www.drj.com/new2dr/w2_002.htm |title=Disaster Recovery Planning Process |first1=Geoffrey H. |last1=Wold |work=Disaster Recovery Journal |series=Adapted from Volume 5 #1 | publisher=Disaster Recovery World |year=1997 |accessdate=8 August 2012}}&lt;/ref&gt; The disaster could be [[Natural disaster|natural]], [[Environmental disaster|environmental]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, an act of a terrorist) or unintentional (that is, accidental, such as the breakage of a man-made dam).

Given organizations' increasing dependency on [[information technology]] to run their operations, a disaster recovery plan, sometimes erroneously called a [[Continuity of Operations]] Plan (COOP), is increasingly associated with the recovery of information technology data, assets, and facilities.

==Objectives==

Organizations cannot always avoid disasters, but with careful planning the effects of a disaster can be minimized. The objective of a disaster recovery plan is to minimize downtime and data loss.&lt;ref&gt;[http://www.comp-soln.com/DRP_whitepaper.pdf ''An Overview of the Disaster Recovery Planning Process - From Start to Finish.''] Comprehensive Consulting Solutions Inc.( "Disaster Recovey Planning, An Overview: White Paper." )March 1999. Retrieved 8 August 2012.&lt;/ref&gt; The primary objective is to protect the organization in the event that all or part of its operations and/or computer services are rendered unusable. The plan minimizes the disruption of operations and ensures that some level of organizational stability and an orderly recovery after a disaster will prevail.&lt;ref name="DR journal" /&gt;  Minimizing downtime and data loss is measured in terms of two concepts: the [[recovery time objective]] (RTO) and the [[recovery point objective]] (RPO).
 
The recovery time objective is the time within which a business process must be restored, after a [[disaster|major incident]] (MI) has occurred, in order to avoid unacceptable consequences associated with a break in [[business continuity]]. The recovery point objective (RPO) is the age of files that must be recovered from backup storage for normal operations to resume if a computer, system, or network goes down as a result of a MI. The RPO is expressed backwards in time (that is, into the past) starting from the instant at which the MI occurs, and can be specified in seconds, minutes, hours, or days.&lt;ref&gt;[http://whatis.techtarget.com/definition/recovery-point-objective-RPO ''Definition: Recovery point objective (RPO).''] Retrieved 10 August 2012.&lt;/ref&gt; The recovery point objective (RPO) is thus the maximum acceptable amount of data loss measured in time. It is the age of the files or data in backup storage required to resume normal operations after the MI.&lt;ref&gt;{{cite web |url=http://www.techopedia.com/definition/1032/recovery-point-objective-rpo |title=Recovery Point Objective (RPO): Definition - What does Recovery Point Objective (RPO) mean? |work=Techopedia |publisher=Janalta Interactive Inc. |year=2012 |accessdate=10 August 2012 }}&lt;/ref&gt;

[[File:Schematic ITSC and RTO, RPO, MI.jpg|frame|left|A DR plan illustrating the chronology of the '''{{color|#bd00e0|RPO}}''' and the '''{{color|#ff7f7c|RTO}}''' with respect to the '''{{color|#fe0000|MI}}'''.]]
{{clear}}

==Relationship to the Business Continuity Plan==

According to the SANS institute, the [[Business continuity planning|Business Continuity Plan]] (BCP) is a comprehensive organizational plan that includes the disaster recovery plan. The Institute further states that a Business Continuity Plan (BCP) consists of the five component plans:&lt;ref name="The Disaster Recovery Plan."&gt;[http://www.sans.org/reading_room/whitepapers/recovery/disaster-recovery-plan_1164 ''The Disaster Recovery Plan.''] Chad Bahan. GSEC Practical Assignment version 1.4b. SANS Institute InfoSec Reading Room. June 2003. Retrieved 24 August 2012.&lt;/ref&gt;

* Business Resumption Plan
* Occupant Emergency Plan
* Continuity of Operations Plan
* Incident Management Plan
* Disaster Recovery Plan

The Institute states that the first three plans (Business Resumption, Occupant Emergency, and Continuity of Operations Plans) do not deal with the IT infrastructure. They further state that the Incident Management Plan (IMP) does deal with the IT infrastructure, but since it establishes structure and procedures to address cyber attacks against an organization&#8217;s IT systems, it generally does not represent an agent for activating the Disaster Recovery Plan, leaving The Disaster Recovery Plan as the only BCP component of interest to IT.&lt;ref name="The Disaster Recovery Plan."/&gt;

[[Disaster Recovery Institute]] International states that disaster recovery is the area of business continuity that deals with ''technology'' recovery as opposed to the recovery of business operations.&lt;ref&gt;https://www.drii.org/glossary.php&lt;/ref&gt;

==Benefits==

Like every insurance plan, there are benefits that can be obtained from the drafting of a disaster recovery plan. Some of these benefits are:&lt;ref name="DR journal" /&gt;

* Providing a sense of security
* Minimizing risk of delays
* Guaranteeing the reliability of standby systems
* Providing a standard for testing the plan
* Minimizing decision-making during a disaster
* Reducing potential legal liabilities
* Lowering unnecessarily stressful work environment

==Types of plans==

There is no one right type of disaster recovery plan,&lt;ref name=MSU&gt;{{cite web |url=http://www.drp.msu.edu/documentation/stepbystepguide.htm |publisher=Michigan State University |title=Disaster Recovery Planning - Step by Step Guide |accessdate=9 May 2014 }}&lt;/ref&gt; nor is there a one-size-fits-all disaster recovery plan.&lt;ref name="5 tips" /&gt;&lt;ref name=MSU /&gt; However, there are three basic strategies that feature in all disaster recovery plans: (1) preventive measures, (2) detective measures, and (3) corrective measures.&lt;ref&gt;{{cite web |url=http://emailarchivingandremotebackup.com/backup-disaster-recovery.html |title=Backup Disaster Recovery |publisher=Email Archiving and Remote Backup |year=2010 |accessdate=9 May 2014}}&lt;/ref&gt; Preventive measures will try to prevent a disaster from occurring. These measures seek to identify and reduce risks. They are designed to mitigate or prevent an event from happening. These measures may include keeping data backed up and off site, using surge protectors, installing generators and conducting routine inspections. Detective measures are taken to discover the presence of any unwanted events within the IT infrastructure.  Their aim is to uncover new potential threats. They may detect or uncover unwanted events. These measures include installing fire alarms, using up-to-date antivirus software, holding employee training sessions, and installing server and [[network monitoring]] software.  Corrective measures are aimed to restore a system after a disaster or otherwise unwanted event takes place. These measures focus on fixing or restoring the systems after a disaster. Corrective measures may include keeping critical documents in the Disaster Recovery Plan or securing proper [[insurance policy|insurance policies]], after a "lessons learned" brainstorming session.&lt;ref name="5 tips" /&gt;&lt;ref&gt;{{cite web|url=http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |title=Disaster Recovery &amp; Business Continuity Plans |publisher=Stone Crossing Solutions |date=2012 |accessdate=9 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120823045007/http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |archivedate=23 August 2012 |df= }}&lt;/ref&gt;

A disaster recovery plan must answer at least three basic questions: (1) what is its objective and purpose, (2) who will be the people or teams who will be responsible in case any disruptions happen, and (3) what will these people do (the procedures to be followed) when the disaster strikes.&lt;ref&gt;{{cite web|url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |title=Disaster Recovery &#8211; Benefits of Getting Disaster Planning Software and Template and Contracting with Companies Offering Data Disaster Recovery Plans, Solutions and Services: Why Would You Need a Disaster Recovery Plan? |publisher=Continuity Compliance |date=7 June 2011 |accessdate=14 August 2012 |archivedate=9 May 2014 |archiveurl=http://www.webcitation.org/6PQaoed5G?url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |deadurl=yes |df= }}&lt;/ref&gt;

==Types of disasters==
[[Image:SH-60B helicopter flies over Sendai.jpg|thumb|right|200px|The tsunami that affected Japan in 2011, a type of natural disaster]]
[[Image:UA Flight 175 hits WTC south tower 9-11 edit.jpeg|thumb|right|200px|September 11, 2001, in New York City, a type of man-made disaster: it caused pollution, loss of lives, property damage, and considerable [[data loss]]]]

Disasters can be [[Natural disaster|natural]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, sabotage or an act of [[terrorism]]) or unintentional (that is, accidental, such as the breakage of a man-made dam).  Disasters may encompass more than weather. They may involve Internet threats or take on other man-made manifestations such as theft.&lt;ref name="5 tips" /&gt;

===Natural disaster===
{{Main article|Natural disaster}}
A natural disaster is a major adverse event resulting from the earth's natural hazards. Examples of natural disasters are [[flood]]s, [[tsunami]]s, [[tornado]]es, [[hurricane|hurricanes/cyclones]], [[volcanic eruption]]s, [[earthquake]]s, [[heat wave]]s, and [[landslide]]s.  Other types of disasters include the more [[End time|cosmic]] scenario of an [[Impact event|asteroid hitting the Earth]].

===Man-made disasters===
{{Main article|Man-made disasters}}
Man-made disasters are the consequence of technological or human hazards. Examples include [[stampede]]s, [[fire|urban fires]], [[industrial accident]]s, [[oil spill]]s, [[nuclear explosion]]s/[[nuclear radiation]] and acts of [[war]].  Other types of man-made disasters include the more cosmic scenarios of catastrophic [[global warming]], [[nuclear war]], and [[bioterrorism]].

The following table categorizes some disasters and notes first response initiatives. Note that whereas the sources of a disaster may be natural (for example, heavy rains) or man-made (for example, a broken dam), the results may be similar (flooding).&lt;ref&gt;[http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc ''Business Continuity Planning (BCP): Sample Plan For Nonprofit Organizations.''] {{wayback|url=http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc |date=20100602065521 }} Pages 11-12. Retrieved 8 August 2012.&lt;/ref&gt;

{| class="wikitable"
! rowspan="16" | Natural
! colspan="3"  | Disaster
|- bgcolor="#CCCCCC"
!Example|| Profile || First Response
|-
|[[Avalanche]]||The sudden, drastic flow of snow down a slope, occurring when either natural triggers, such as loading from new snow or rain, or artificial triggers, such as explosives or backcountry skiers, overload the snowpack||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Blizzard]]||A severe snowstorm characterized by very strong winds and low temperatures||Power off all equipment; listen to blizzard advisories; Evacuate area, if unsafe; Assess damage
|-
|[[Earthquake]]||The shaking of the earth&#8217;s crust, caused by underground volcanic forces of breaking and shifting rock beneath the earth&#8217;s surface||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Fire|Fire (wild)]]||Fires that originate in uninhabited areas and which pose the risk to spread to inhabited areas||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories
|-
|[[Flood]]||Flash flooding: Small creeks, gullies, dry streambeds, ravines, culverts or even low-lying areas flood quickly||Monitor flood advisories; Determine flood potential to facilities; Pre-stage emergency power generating equipment; Assess damage
|-
|[[Freezing Rain]]||Rain occurring when outside surface temperature is below freezing||Monitor weather advisories; Notify employees of business closure; home; Arrange for snow and ice removal
|-
|[[Heat wave]]||A prolonged period of excessively hot weather relative to the usual weather pattern of an area and relative to normal temperatures for the season||Listen to weather advisories; Power-off all servers after a graceful shutdown if there is imminent potential of power failure; Shut down main electric circuit usually located in the basement or the first floor
|-
|[[Hurricane]]||Heavy rains and high winds||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage
|-
|[[Landslide]]||Geological phenomenon which includes a range of ground movement, such as rock falls, deep failure of slopes and shallow debris flows||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Lightning strike]]||An electrical discharge caused by lightning, typically during thunderstorms||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage
|-
|[[Limnic eruption]]||The sudden eruption of carbon dioxide from deep lake water||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Tornado]]||Violent rotating columns of air which descent from severe thunderstorm cloud systems||Monitor tornado advisories; Power off equipment; Shut off utilities (power and gas); Assess damage once storm passes
|-
|[[Tsunami]]||A series of water waves caused by the displacement of a large volume of a body of water, typically an ocean or a large lake, usually caused by earthquakes, volcanic eruptions, underwater explosions, landslides, glacier calvings, meteorite impacts and other disturbances above or below water||Power off all equipment; listen to tsunami advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Assess damage
|-
|[[Volcanic eruption]]||The release of hot magma, volcanic ash and/or gases from a volcano||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
! rowspan="6" | Man-made
|[[Bioterrorism]]||The intentional release or dissemination of biological agents as a means of coercion||Get information immediately from your [[Public Health]] officials via the news media as to the right course of action; If you think you have been exposed, quickly remove your clothing and wash off your skin; Also put on a [[HEPA]] to help prevent inhalation of the agent&lt;ref&gt;[http://answers.webmd.com/answers/1176206/what-should-i-do-if-there ''What should I do if there has been a bioterrorism attack?.''] Edmond A. Hooker. WebMD. 9 October 2007. Retrieved 18 September 2012.&lt;/ref&gt; 
|-
|[[Civil unrest]]||A disturbance caused by a group of people that may include [[sit-in]]s and other forms of obstructions, riots, sabotage and other forms of crime, and which is intended to be a demonstration to the public and the government, but can escalate into general chaos||Contact local police or law enforcement&lt;ref&gt;[http://www.usfa.fema.gov/downloads/pdf/publications/fa-142.pdf ''Report of the Joint Fire/Police Task Force on Civil Unrest (FA-142): Recommendations for Organization and Operations During Civil Disturbance.''] Page 55. FEMA. Retrieved 21 October 2012.&lt;/ref&gt;&lt;ref&gt;[http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf ''Business Continuity Planning: Developing a Strategy to Minimize Risk and Maintain Operations.''] {{wayback|url=http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf |date=20140327234742 }} Adam Booher. Retrieved 19 September 2012.&lt;/ref&gt; 
|-
|[[Fire|Fire (urban)]]||Even with strict building fire codes, people still perish needlessly in fires||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories
|-
|[[Hazardous material|Hazardous material spills]]||The escape of solids, liquids, or gases that can harm people, other living organisms, property or the environment, from their intended controlled environment such as a container.||Leave the area and call the local fire department for help.&lt;ref&gt;[http://www.tnema.org/public/hazmat.html ''Hazardous Materials.''] {{wayback|url=http://www.tnema.org/public/hazmat.html |date=20121011150052 }} Tennessee Emergency Management Office. Retrieved 7 September 2012.&lt;/ref&gt; If anyone was affected by the spill, call the your local Emergency Medical Services line&lt;ref&gt;[http://www.atsdr.cdc.gov/MHMI/index.asp ''Managing Hazardous Materials Incidents (MHMIs).''] Center for Disease Control. Retrieved 7 September 2012.&lt;/ref&gt;
|-
||[[Nuclear and radiation accidents|Nuclear and Radiation Accidents]]||An event involving significant release of radioactivity to the environment or a reactor core meltdown and which leads to major undesirable consequences to people, the environment, or the facility||Recognize that a CBRN incident has or may occur. Gather, assess and disseminate all available information to first responders. Establish an overview of the affected area. Provide and obtain regular updates to and from first responders.&lt;ref&gt;[http://www.nato.int/docu/cep/cep-cbrn-response-e.pdf ''Guidelines for First Response to a CBRN Incident.''] Project on Minimum Standards and Non-Binding Guidelines for First Responders Regarding Planning, Training, Procedure and Equipment for Chemical, Biological, Radiological and Nuclear (CBRN) Incidents.] NATO. Emergency Management. Retrieved 21 October 2012.&lt;/ref&gt;
|-
|[[Power Failure]]||Caused by summer or winter storms, lightning or construction equipment digging in the wrong location||Wait 5&#8211;10 minutes; Power-off all Servers after a graceful shutdown; Do not use telephones, in the event of severe lightning; Shut down main electric circuit usually located in the basement or the first floor
|-
|}

In the realm of information technology per se, disasters may also be the result of a computer security exploit. Some of these are: [[computer virus]]es, [[cyberattack]]s, [[denial-of-service attack]]s, [[hacker (computer security)|hacking]], and [[malware]] exploits. These are ordinarily attended to by [[information security]] experts.

==Planning methodology==

According to Geoffrey H. Wold of the Disaster Recovery Journal, the entire process involved in developing a Disaster Recovery Plan consists of 10 steps:&lt;ref name="DR journal" /&gt;

===Obtaining top management commitment===
For a disaster recovery plan to be successful, the central responsibility for the plan must reside on [[Management#Top-level managers|top management]]. Management is responsible for coordinating the disaster recovery plan and ensuring its effectiveness within the organization. It is also responsible for allocating adequate time and resources required in the development of an effective plan. Resources that management must allocate include both financial considerations and the effort of all personnel involved.

===Establishing a planning committee===
A [[plan]]ning [[committee]] is appointed to oversee the development and implementation of the plan. The planning committee includes representatives from all functional areas of the organization. Key committee members customarily include the operations manager and the data processing manager. The committee also defines the scope of the plan.

===Performing a risk assessment===
The planning committee prepares a [[Probabilistic risk assessment|risk analysis]] and a [[business impact analysis]] (BIA) that includes a range of possible disasters, including natural, technical and human threats. Each functional area of the organization is analyzed to determine the potential consequence and impact associated with several disaster scenarios. The risk assessment process also evaluates the safety of critical documents and vital records. Traditionally, fire has posed the greatest threat to an organization. Intentional human destruction, however, should also be considered. A thorough plan provides for the &#8220;worst case&#8221; situation: destruction of the main building. It is important to assess the impacts and consequences resulting from loss of information and services. The planning committee also analyzes the costs related to minimizing the potential exposures.

===Establishing priorities for processing and operations===
At this point, the critical needs of each department within the organization are evaluated in order to prioritize them. Establishing [[Wiktionary:priority|priorities]] is important because no organization possesses infinite resources and criteria must be set as to where to allocate resources first. Some of the areas often reviewed during the prioritization process are functional operations, key personnel and their functions, information flow, processing systems used, services provided, existing documentation, historical records, and the department's policies and procedures.

Processing and operations are analyzed to determine the maximum amount of time that the department and organization can operate without each critical system. This will later get mapped into the [[Recovery Time Objective]]. A critical system is defined as that which is part of a system or procedure necessary to continue operations should a department, computer center, main facility or a combination of these be destroyed or become inaccessible. A method used to determine the critical needs of a department is to document all the functions performed by each department. Once the primary functions have been identified, the operations and processes are then ranked in order of priority: essential, important and non-essential.

===Determining recovery strategies===
During this phase, the most practical alternatives for processing in case of a disaster are researched and evaluated. All aspects of the organization are considered, including [[Building|physical facilities]], [[computer hardware]] and [[software]], [[communications link]]s, [[data file]]s and [[database]]s, [[customer service]]s provided, user operations, the overall [[management information system]]s (MIS) structure, [[end-user]] systems, and any other processing operations.

Alternatives, dependent upon the evaluation of the computer function, may include: [[hot site]]s, [[warm site]]s, [[cold site]]s, [[reciprocal agreement (disaster preparedness)|reciprocal agreements]], the provision of more than one data center, the installation and deployment of multiple computer system, duplication of service center, [[consortium]] arrangements, lease of equipment, and any combinations of the above.

Written [[Contract|agreements]] for the specific recovery alternatives selected are prepared, specifying contract duration, termination conditions, [[system testing]], [[cost]], any special security procedures, procedure for the notification of system changes, hours of operation, the specific hardware and other equipment required for processing, personnel requirements, definition of the circumstances constituting an [[emergency]], process to negotiate service extensions, guarantee of [[Computer compatibility|compatibility]], [[availability]], non-mainframe resource requirements, priorities, and other contractual issues.

===Collecting data===
In this phase, data collection takes place. Among the recommended data gathering materials and documentation often included are
various lists (employee backup position listing, critical telephone numbers list, master call list, master vendor list, notification checklist), inventories (communications equipment, documentation, office equipment, forms, [[insurance policy|insurance policies]], workgroup and data center computer hardware, [[microcomputer]] hardware and software, [[office supplies|office supply]], off-site storage location equipment, telephones, etc.), distribution register, software and data files backup/retention schedules, temporary location specifications, any other such other lists, materials, inventories and documentation. Pre-formatted forms are often used to facilitate the data gathering process.

===Organizing and documenting a written plan===
Next, an outline of the plan&#8217;s contents is prepared to guide the development of the detailed procedures. Top management reviews and approves the proposed plan. The outline can ultimately be used for the [[table of contents]] after final revision. Other four benefits of this approach are that (1) it helps to organize the detailed procedures, (2) identifies all major steps before the actual writing process begins, (3) identifies redundant procedures that only need to be written once, and (4) provides a [[plan|road map]] for developing the procedures.

It is often considered [[best practice]] to develop a standard format for the disaster recovery plan so as to facilitate the writing of detailed procedures and the documentation of other information to be included in the plan later. This helps ensure that the disaster plan follows a consistent format and allows for its ongoing future maintenance. [[Standardization]] is also important if more than one person is involved in writing the procedures.

It is during this phase that the actual written plan is developed in its entirety, including all detailed procedures to be used before, during, and after a disaster. The procedures include methods for maintaining and updating the plan to reflect any significant internal, external or systems changes. The procedures allow for a regular review of the plan by key personnel within the organization. The disaster recovery plan is structured using a team approach. Specific responsibilities are assigned to the appropriate team for each functional area of the organization. Teams responsible for administrative functions, [[building|facilities]], [[logistics]], user support, [[backup|computer backup]], restoration and other important areas in the organization are identified.

The structure of the contingency organization may not be the same as the existing organization chart. The contingency organization is usually structured with teams responsible for major functional areas such as administrative functions, facilities, logistics, user support, computer backup, restoration, and any other important area.

The [[management team]] is especially important because it coordinates the recovery process. The team assesses the disaster, activates the recovery plan, and contacts team managers. The management team also oversees, documents and monitors the recovery process. It is helpful when management team members are the final decision-makers in setting priorities, policies and procedures. Each team has specific responsibilities that are completed to ensure successful execution of the plan. The teams have an assigned manager and an alternate in case the team manager is not available. Other team members may also have specific assignments where possible.

===Developing testing criteria and procedures===
Best practices dictate that DR plans be thoroughly tested and evaluated on a regular basis (at least annually). Thorough DR plans include documentation with the procedures for testing the plan. The tests will provide the organization with the assurance that all necessary steps are included in the plan. Other reasons for testing include:
* Determining the feasibility and compatibility of backup facilities and procedures.
* Identifying areas in the plan that need modification.
* Providing training to the team managers and team members.
* Demonstrating the ability of the organization to recover.
* Providing motivation for maintaining and updating the disaster recovery plan.

===Testing the plan===
After testing procedures have been completed, an initial "[[Dry run (testing)|dry run]]" of the plan is performed by conducting a structured walk-through test. The test will provide additional information regarding any further steps that may need to be included, changes in procedures that are not effective, and other appropriate adjustments. These may not become evident unless an actual dry-run test is performed. The plan is subsequently updated to correct any problems identified during the test. Initially, testing of the plan is done in sections and after normal business hours to minimize disruptions to the overall operations of the organization. As the plan is further polished, future tests occur during normal business hours.

Types of tests include: checklist tests, simulation tests, parallel tests, and full interruption tests.

===Obtaining plan approval===
Once the disaster recovery plan has been written and tested, the plan is then submitted to management for approval. It is top management&#8217;s ultimate responsibility that the organization has a documented and tested plan. Management is responsible for (1) establishing the policies, procedures and responsibilities for comprehensive [[contingency plan]]ning, and (2) reviewing and approving the contingency plan annually, documenting such reviews in writing.

Organizations that receive information processing from [[service bureau]]s will, in addition, also need to (1) evaluate the adequacy of contingency plans for its service bureau, and (2)ensure that its contingency plan is compatible with its service bureau&#8217;s plan.

==Caveats/controversies==

Due to its high cost, disaster recovery plans are not without critics. [[Cormac Foster]] has identified five "common mistakes" organizations often make related to disaster recovery planning:&lt;ref&gt;[https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx ''Five Mistakes That Can Kill a Disaster Recovery Plan. In archive.org''] Cormac Foster. Dell Corporation. 25 October 2010. Retrieved 8 August 2012.&lt;/ref&gt;

===Lack of buy-in===
One factor is the perception by executive management that DR planning is "just another fake earthquake drill" or CEOs that fail to make DR planning and preparation a priority, are often significant contributors to the failure of a DR plan.

===Incomplete RTOs and RPOs===
Another critical point is failure to include each and every important business process or a block of data. "Every item in your DR plan requires a Recovery Time Objective (RTO) defining maximum process downtime or a Recovery Point Objective (RPO) noting an acceptable restore point. Anything less creates ripples that can extend the disaster's impact." As an example, "payroll, accounting and the weekly customer newsletter may not be mission-critical in the first 24 hours, but left alone for several days, they can become more important than any of your initial problems."

===Systems myopia===
A third point of failure involves focusing only on DR without considering the larger business continuity needs: "Data and systems restoration after a disaster are essential, but every business process in your organization will need IT support, and that support requires planning and resources." As an example, corporate office space lost to a disaster can result in an instant pool of teleworkers which, in turn, can overload a company's [[VPN]] overnight, overwork the IT support staff at the blink of an eye and cause serious bottlenecks and monopolies with the dial-in PBX system.

===Lax security===
When there is a disaster, an organization's data and business processes become vulnerable. As such, security can be more important than the raw speed involved in a disaster recovery plan's RTO. The most critical consideration then becomes securing the new data pipelines: from new VPNs to the connection from offsite backup services. Another security concern includes documenting every step of the recovery process&#8212;something that is especially important in highly regulated industries, government agencies, or in disasters requiring post-mortem forensics. Locking down or remotely wiping lost handheld devices is also an area that may require addressing.

===Outdated plans===
Another important aspect that is often overlooked involves the frequency with which DR Plans are updated. Yearly updates are recommended but some industries or organizations require more frequent updates because business processes evolve or because of quicker data growth. To stay relevant, disaster recovery plans should be an integral part of all [[business analysis]] processes, and should be revisited at every major corporate acquisition, at every new product launch and at every new system development milestone.

==See also==
* [[Disaster recovery]]
* [[Business continuity planning]]
* [[Federal Emergency Management Agency]]
* [[Backup rotation scheme]]
* [[Seven tiers of disaster recovery]]

==References==
{{reflist|2}}

{{DEFAULTSORT:Disaster recovery plan}}
[[Category:Disaster recovery]]
[[Category:Data management]]
[[Category:Backup]]
[[Category:IT risk management]]
[[Category:Planning]]</text>
      <sha1>skazw29sfrdej87sp7soa1jkqe4yhq6</sha1>
    </revision>
  </page>
  <page>
    <title>SQL/PSM</title>
    <ns>0</ns>
    <id>11665200</id>
    <revision>
      <id>746720088</id>
      <parentid>712337419</parentid>
      <timestamp>2016-10-29T03:32:26Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5455" xml:space="preserve">{{infobox programming language
| name                   = SQL/PSM
| logo                   =
| paradigm               = [[multi-paradigm programming language|Multi-paradigm]]
| year                   = 1996
| designer               =
| developer              =
| latest_release_version = [[SQL:2011]]
| latest_release_date    = 
| latest_preview_version = 
| latest_preview_date    = 
| turing-complete        = Yes
| typing                 = 
| implementations        = [[MySQL]] &lt;br /&gt;IBM's [[SQL PL]]
| influenced_by          = [[Ada (programming language)|Ada]]&lt;ref&gt;{{Citation | url = http://ocelot.ca/blog/blog/2015/01/15/stored-procedures-critiques-and-defences/ | title = Stored Procedures: critiques and defences | year = 2015 | first1 = Peter | last1 = Gulutzan }}&lt;/ref&gt;
| influenced             = 
| operating_system       = [[Cross-platform|Cross-platform (multi-platform)]]
| license                =
| website                = 
| file_ext               =
| dialects               =
| wikibooks              = 
}}

'''SQL/PSM''' ([[SQL]]/Persistent Stored Modules) is an [[ISO standard]] mainly defining an extension of SQL with a [[procedural language]] for use in [[stored procedure]]s. Initially published in 1996 as an extension of [[SQL-92]] (ISO/IEC 9075-4:1996, a version sometimes called PSM-96 or even SQL-92/PSM&lt;ref&gt;{{Cite journal | last1 = Eisenberg | first1 = A. | title = New standard for stored procedures in SQL | doi = 10.1145/245882.245907 | journal = ACM SIGMOD Record | volume = 25 | issue = 4 | pages = 81-88| year = 1996 | pmid =  | pmc = }}&lt;/ref&gt;), SQL/PSM was later incorporated into the multi-part [[SQL:1999]] standard, and has been part 4 of that standard since then, most recently in [[SQL:2011]].  The SQL:1999 part 4 covered less than the original PSM-96 because the SQL statements for defining, managing, and invoking routines were actually incorporated into part 2 SQL/Foundation, leaving only the procedural language itself as SQL/PSM.&lt;ref&gt;{{cite book| first1 =Jim | last1 = Melton | first2 =Alan R | last2 = Simon | title = SQL: 1999|year=2002| publisher = Morgan Kaufmann|isbn= 978-1-55860-456-8 | pages = 541&#8211;42}}&lt;/ref&gt; The SQL/PSM facilities are still optional as far as the SQL standard is concerned; most of them are grouped in Features P001-P008.

SQL/PSM standardizes syntax and semantics for [[control flow]], [[exception handling]] (called "condition handling" in SQL/PSM), local variables, assignment of expressions to variables and parameters, and (procedural) use of [[Cursor (databases)|cursors]]. It also defines an information schema ([[metadata]]) for stored procedures.  SQL/PSM is one language in which [[Method (computer programming) |methods]] for the SQL:1999 [[structured type]]s can be defined.  The other is Java, via [[SQL/JRT]].

In practice [[MySQL]]'s procedural language and IBM's [[SQL PL]] (used in DB2) are closest to the SQL/PSM standard.&lt;ref name = "HarrisonFeuerstein2008"&gt;{{cite book | first1 = Guy | last1 = Harrison| first2 = Steven | last2 = Feuerstein|title=MySQL Stored Procedure Programming|url= https://books.google.com/books?id=YpeP0ok0cO4C&amp;pg=PT75 | year=2008|publisher=O'Reilly |isbn = 978-0-596-10089-6 |page= 49}}&lt;/ref&gt; 

SQL/PSM resembles and inspired by [[PL/SQL]], as well as [[PL/pgSQL]], so they are similar languages.  With [[PostgreSQL]] v9 some SQL/PSM features, like overloading of SQL-invoked functions and procedures&lt;ref&gt;{{Citation | publisher = PostgreSQL | title = SQL standard features | edition = 9 | contribution-url = http://www.postgresql.org/docs/9.0/static/features-sql-standard.html | contribution = feature T322}}.&lt;/ref&gt; are now supported.  A [[PostgreSQL]] addon implements SQL/PSM&lt;ref&gt;{{Citation | url = https://github.com/okbob/plpsm0 | format = git | type = repository | title = plpsm0}}.&lt;/ref&gt;&lt;ref&gt;{{Citation | publisher = PostgreSQL | url = http://www.postgresql.org/message-id/1305291347.14548.13.camel@jara.office.nic.cz | date = May 2011 | title = Announce}}.&lt;/ref&gt;&lt;ref&gt;[http://www.postgresql.org/message-id/CAFj8pRDWFdcjNSnwQB_3j1-rMO6b8=TmLTNBvDCSpRrOW2Dfeg@mail.gmail.com 2012-2's Proposal PL/pgPSM announce]&lt;/ref&gt;&lt;ref&gt;{{Citation | title = SQL/PSM | format = wiki | url = http://postgres.cz/wiki/SQL/PSM_Manual | publisher = PostgreSQL | type = manual | year = 2008}}.&lt;/ref&gt; (alongside its own procedural language), although it is not part of the core product.&lt;ref&gt;{{Citation | contribution-url = http://www.postgresql.org/docs/9.2/static/features.html | publisher = PostgreSQL | title = Documentation | edition = 9.2 | contribution = SQL Conformance}}.&lt;/ref&gt;

==See also==
The following implementations adopt the standard, but they are not 100% compatible to SQL/PSM:

[[Open source]]:
* [[HSQLDB]] stored procedures and functions&lt;ref name="SQL/PSM routines"&gt;http://hsqldb.org/doc/2.0/guide/sqlroutines-chapt.html#src_psm_routines&lt;/ref&gt;
* [[MySQL]] stored procedures &lt;ref name="HarrisonFeuerstein2008"/&gt;
* [[PostgreSQL]] [[PL/pgSQL]]

Proprietary:
* Oracle [[PL/SQL]]
* Microsoft and Sybase [[Transact-SQL]]

==References==
{{reflist}}

==Further reading==
* Jim Melton, ''Understanding SQL's Stored Procedures: A Complete Guide to SQL/PSM'', Morgan Kaufmann Publishers, 1998, ISBN 1-55860-461-8

{{SQL}}

__NOTOC__

{{DEFAULTSORT:SQL PSM}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Data-centric programming languages]]
[[Category:Programming languages created in 1996]]


{{compu-lang-stub}}
{{database-stub}}</text>
      <sha1>23s3pf4l2kcfnf6rtrynjw02n3t9dlj</sha1>
    </revision>
  </page>
  <page>
    <title>Read&#8211;write conflict</title>
    <ns>0</ns>
    <id>217827</id>
    <revision>
      <id>731657231</id>
      <parentid>731653028</parentid>
      <timestamp>2016-07-26T18:30:03Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>cap, punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1516" xml:space="preserve">{{Unreferenced|date=June 2008}}
In [[computer science]], in the field of [[database]]s, '''read&#8211;write conflict''', also known as '''unrepeatable reads''', is a computational anomaly associated with interleaved execution of transactions. 

Given a schedule S

:&lt;math&gt;S = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;  \\
 &amp; R(A) \\
 &amp; W(A)\\
 &amp; Com. \\
R(A) &amp; \\
W(A) &amp; \\
Com. &amp; \end{bmatrix}&lt;/math&gt;

In this example, T1 has read the original value of A, and is waiting for T2 to finish. T2 also reads the original value of A, overwrites A, and commits.

However, when T1 reads to A, it discovers two different versions of A, and T1 would be forced to [[Abort (computing)|abort]], because T1 would not know what to do. This is an unrepeatable read. This could never occur in a serial schedule. [[Strict two-phase locking]] (Strict 2PL) prevents this conflict.

== Real-world example==
[[Alice and Bob]] are using a website to book tickets for a specific show. Only one ticket is left for the specific show. Alice signs on first to see that only one ticket is left, and finds it expensive. Alice takes time to decide. Bob signs on and also finds one ticket left, and orders it instantly. Bob purchases and logs off. Alice decides to buy a ticket, to find there are no tickets. This is a typical read-write conflict situation.

== See also ==

* [[Concurrency control]]
* [[Write&#8211;read conflict]]
* [[Write&#8211;write conflict]]

{{DEFAULTSORT:Read-write conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>lg2gxepwav2zv5ck5za7kpv3g1mccms</sha1>
    </revision>
  </page>
  <page>
    <title>Online analytical processing</title>
    <ns>0</ns>
    <id>189239</id>
    <revision>
      <id>757186245</id>
      <parentid>752282782</parentid>
      <timestamp>2016-12-29T08:23:03Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Comparison */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility/LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29193" xml:space="preserve">'''Online analytical processing''', or '''OLAP''' ({{IPAc-en|&#712;|o&#650;|l|&#230;|p}}), is an approach to answering [[multi-dimensional analytical]] (MDA) queries swiftly in [[computing]], .&lt;ref name=Codd1993&gt;{{cite web
  |url=http://www.sgpnyc.com/us/products/dataquest/whitepapers/OLAP_wp_efcodd.pdf
  |title=Providing OLAP (On-line Analytical Processing) to User-Analysts: An IT Mandate
  |publisher=Codd &amp; Date, Inc
  |author1=Codd E.F. |author2=Codd S.B. |author3=Salley C.T.  |last-author-amp=yes |year=1993
  |accessdate=2008-03-05 }}&lt;/ref&gt; OLAP is part of the broader category of [[business intelligence]], which also encompasses [[relational database]], report writing and [[data mining]].&lt;ref&gt;{{cite book
  |url=https://books.google.com/books?id=M-UOE1Cp9OEC
  |title=Business Intelligence for Telecommunications
  |publisher=CRC Press
  |author=Deepak Pareek
  |year=2007
  |pages=294 pp
  |isbn=0-8493-8792-2
  |accessdate=2008-03-18
}}&lt;/ref&gt;  Typical applications of OLAP include [[business reporting]] for sales, [[marketing]], management reporting, [[business process management]] (BPM),&lt;ref&gt;{{cite book
  |url=http://www.google.com/products?q=9783639222166
  |title=Business Process Management:A Data Cube To Analyze Business Process Simulation Data For Decision Making
  |publisher=[[VDM Verlag|VDM Verlag Dr. M&#252;ller e.K.]]
  |author=Apostolos Benisis
  |year=2010
  |pages=204 pp
  |isbn=978-3-639-22216-6
}}&lt;/ref&gt; [[budget]]ing and [[forecasting|forecast]]ing, [[financial reporting]] and similar areas, with new applications coming up, such as [[agriculture]].&lt;ref name=ahsan/&gt; The term ''OLAP'' was created as a slight modification of the traditional database term [[online transaction processing]] (OLTP).&lt;ref&gt;{{cite web
  |url=http://www.symcorp.com/downloads/OLAP_CouncilWhitePaper.pdf
  |format=PDF|title=OLAP Council White Paper
  |publisher=OLAP Council
  |year=1997

  |accessdate=2008-03-18
}}&lt;/ref&gt;

OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.&lt;ref&gt;O'Brien &amp; Marakas, 2011, p. 402-403&lt;/ref&gt; Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region's sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the [[OLAP cube]] and view (dicing) the slices from different viewpoints.  These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)

[[Database]]s configured for OLAP use a multidimensional data model, allowing for complex analytical and [[ad hoc]] queries with a rapid execution time.&lt;ref&gt;{{cite web
  |url=http://www.dwreview.com/OLAP/Introduction_OLAP.html
  |title=Introduction to OLAP &#8211; Slice, Dice and Drill!
  |publisher=Data Warehousing Review
  |author=Hari Mailvaganam
  |year=2007  |accessdate=2008-03-18
}}&lt;/ref&gt;  They borrow aspects of [[navigational database]]s, [[hierarchical database]]s and relational databases.

OLAP is typically contrasted to [[Online transaction processing|OLTP]] (online transaction processing), which is generally characterized by much less complex queries, in a larger volume, to process transactions rather than for the purpose of business intelligence or reporting. Whereas OLAP systems are mostly optimized for read, OLTP has to processes all kinds of queries (read, insert, update and delete).

== Overview of OLAP systems ==
At the core of any OLAP system is an [[OLAP cube]] (also called a 'multidimensional cube' or a [[hypercube]]). It consists of numeric facts called ''measures'' that are categorized by ''[[Dimension (data warehouse)|dimensions]]''. The measures are placed at the intersections of the hypercube, which is spanned by the dimensions as a [[vector space]]. The usual interface to manipulate an OLAP cube is a matrix interface, like [[Pivot table]]s in a spreadsheet program, which performs projection operations along the dimensions, such as aggregation or averaging.

The cube metadata is typically created from a [[star schema]] or [[snowflake schema]] or [[fact constellation]] of tables in a [[relational database]]. Measures are derived from the records in the [[fact table]] and dimensions are derived from the [[dimension table]]s.

Each ''measure'' can be thought of as having a set of ''labels'', or meta-data associated with it. A ''dimension'' is what describes these ''labels''; it provides information about the ''measure''.

A simple example would be a cube that contains a store's sales as a ''measure'', and Date/Time as a ''dimension''. Each Sale has a Date/Time ''label'' that describes more about that sale.

For example:
  Sale Fact Table
 +-------------+----------+
 | sale_amount | time_id  |
 +-------------+----------+            Time Dimension
 |      2008.10|     1234 |----+     +---------+-------------------+
 +-------------+----------+    |     | time_id | timestamp         |
                               |     +---------+-------------------+
                               +----&gt;|   1234  | 20080902 12:35:43 |
                                     +---------+-------------------+

=== Multidimensional databases ===
Multidimensional structure is defined as "a variation of the relational model that uses multidimensional structures to organize data and express the relationships between data".&lt;ref&gt;O'Brien &amp; Marakas, 2009, pg 177&lt;/ref&gt;  The structure is broken into cubes and the cubes are able to store and access data within the confines of each cube. "Each cell within a multidimensional structure contains aggregated data related to elements along each of its dimensions".&lt;ref&gt;O'Brien &amp; Marakas, 2009, pg 178&lt;/ref&gt;  Even when data is manipulated it remains easy to access and continues to constitute a compact database format.  The data still remains interrelated.
Multidimensional structure is quite popular for analytical databases that use online analytical processing (OLAP) applications.&lt;ref&gt;(O'Brien &amp; Marakas, 2009)&lt;/ref&gt;  Analytical databases use these databases because of their ability to deliver answers to complex business queries swiftly.  Data can be viewed from different angles, which gives a broader perspective of a problem unlike other models.&lt;ref&gt;Williams, C., Garza, V.R., Tucker, S, Marcus, A.M. (1994, January 24). Multidimensional models boost viewing options. InfoWorld, 16(4)&lt;/ref&gt;

=== Aggregations ===
It has been claimed that for complex queries OLAP cubes can produce an answer in around 0.1% of the time required for the same query on [[OLTP]] relational data.&lt;ref&gt;{{cite web
  | author=MicroStrategy, Incorporated
  | year=1995
  | title=The Case for Relational OLAP
  | url=http://www.cs.bgu.ac.il/~onap052/uploads/Seminar/Relational%20OLAP%20Microstrategy.pdf

  |format=PDF| accessdate=2008-03-20
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
  |author1=Surajit Chaudhuri  |author2=Umeshwar Dayal
   |lastauthoramp=yes | title = An overview of data warehousing and OLAP technology
  | journal = SIGMOD Rec.
  | publisher = [[Association for Computing Machinery|ACM]]
  | volume = 26
  | issue = 1
  | year = 1997

  | pages = 65
  | url = http://doi.acm.org/10.1145/248603.248616
  | doi = 10.1145/248603.248616

  | accessdate=2008-03-20
}}&lt;/ref&gt;  The most important mechanism in OLAP which allows it to achieve such performance is the use of ''aggregations''. Aggregations are built from the fact table by changing the granularity on specific dimensions and aggregating up data along these dimensions. The number of possible aggregations is determined by every possible combination of dimension granularities.

The combination of all possible aggregations and the base data contains the answers to every query which can be answered from the data.&lt;ref&gt;{{cite journal
  | last1 = Gray | first1 = Jim
  | author1-link = Jim Gray (computer scientist)
  | last2 = Chaudhuri | first2 = Surajit
  | last3 = Layman | first3 = Andrew
  | last4 = Reichart | first4 = Don
  | last5 = Venkatrao | first5 = Murali
  | last6 = Pellow | first6 = Frank
  | last7 = Pirahesh | first7 = Hamid
  | title = Data Cube: {A} Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals
  | journal = J. Data Mining and Knowledge Discovery
  | volume = 1
  | issue = 1
  | pages = 29&#8211;53
  | year = 1997
  | url = http://citeseer.ist.psu.edu/gray97data.html

  | accessdate=2008-03-20
}}&lt;/ref&gt;

Because usually there are many aggregations that can be calculated, often only a predetermined number are fully calculated; the remainder are solved on demand.  The problem of deciding which aggregations (views) to calculate is known as the view selection problem.  View selection can be constrained by the total size of the selected set of aggregations, the time to update them from changes in the base data, or both.  The objective of view selection is typically to minimize the average time to answer OLAP queries, although some studies also minimize the update time.  View selection is [[NP-Complete]]. Many approaches to the problem have been explored, including [[greedy algorithm]]s, randomized search, [[genetic algorithm]]s and [[A* search algorithm]].

==Types==
OLAP systems have been traditionally categorized using the following taxonomy.&lt;ref name=Pendse2006&gt;{{cite web|url=http://www.olapreport.com/Architectures.htm |title=OLAP architectures |publisher=OLAP Report |author=Nigel Pendse |date=2006-06-27 |accessdate=2008-03-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20080124155954/http://www.olapreport.com/Architectures.htm |archivedate=January 24, 2008 }}&lt;/ref&gt;

===Multidimensional OLAP (MOLAP)===
MOLAP (multi-dimensional online analytical processing) is the classic form of OLAP and is sometimes referred to as just OLAP. MOLAP stores this data in an optimized multi-dimensional array storage, rather than in a relational database.

Some MOLAP tools require the [[pre-computation]] and storage of derived data, such as consolidations &#8211; the operation known as processing. Such MOLAP tools generally utilize a pre-calculated data set referred to as a data cube. The data cube contains all the possible answers to a given range of questions. As a result, they  have a very fast response to queries. On the other hand, updating can take a long time depending on the degree of pre-computation. Pre-computation can also lead to what is known as data explosion.

Other MOLAP tools, particularly those that implement the [[Functional Database Model|functional database model]] do not pre-compute derived data but make all calculations on demand other than those that were previously requested and stored in a cache.

'''Advantages of MOLAP'''
* Fast query performance due to optimized storage, multidimensional indexing and caching.
* Smaller on-disk size of data compared to data stored in [[relational database]] due to compression techniques.
* Automated computation of higher level aggregates of the data.
* It is very compact for low dimension data sets.
* Array models provide natural indexing.
* Effective data extraction achieved through the pre-structuring of aggregated data.

'''Disadvantages of MOLAP'''
* Within some MOLAP Solutions the processing step (data load) can be quite lengthy, especially on large data volumes. This is usually remedied by doing only incremental processing, i.e., processing only the data which have changed (usually new data) instead of reprocessing the entire data set.
* Some MOLAP methodologies introduce data redundancy.

====Products====
Examples of commercial products that use MOLAP are [[Cognos]] Powerplay, [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]], [[Microsoft Analysis Services]], [[Essbase]], [[Applix|TM1]], [[Jedox]], and [[icCube]].

===Relational OLAP (ROLAP)===
'''ROLAP''' works directly with relational databases and does not require pre-computation. The base data and the dimension tables are stored as relational tables and new tables are created to hold the aggregated information. It depends on a specialized schema design. This methodology relies on manipulating the data stored in the relational database to give the appearance of traditional OLAP's slicing and dicing functionality. In essence, each action of slicing and dicing is equivalent to adding a "WHERE" clause in the SQL statement. ROLAP tools do not use pre-calculated data cubes but instead pose the query to the standard relational database and its tables in order to bring back the data required to answer the question. ROLAP tools feature the ability to ask any question because the methodology does not limit to the contents of a cube.  ROLAP also has the ability to drill down to the lowest level of detail in the database.

While ROLAP uses a relational database source, generally the database must be carefully designed for ROLAP use.  A database which was designed for [[OLTP]] will not function well as a ROLAP database.  Therefore, ROLAP still involves creating an additional copy of the data.  However, since it is a database, a variety of technologies can be used to populate the database.

==== Advantages of ROLAP ====
&lt;!--Note to editors:
Please review the discussion page before making changes to the advantages or disadvantages. Thank you.
--&gt;

* ROLAP is considered to be more scalable in handling large data volumes, especially models with [[Dimension (data warehouse)|dimensions]] with very high [[cardinality]] (i.e., millions of members).
* With a variety of data loading tools available, and the ability to fine-tune the [[Extract, transform, load|ETL]] code to the particular data model, load times are generally much shorter than with the automated [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] loads.
* The data are stored in a standard [[relational database]] and can be accessed by any [[SQL]] reporting tool (the tool does not have to be an OLAP tool).
* ROLAP tools are better at handling ''non-aggregatable facts'' (e.g., textual descriptions).  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools tend to suffer from slow performance when querying these elements.
* By [[Decoupling (electronics)|decoupling]] the data storage from the multi-dimensional model, it is possible to successfully model data that would not otherwise fit into a strict dimensional model.
* The ROLAP approach can leverage [[database]] authorization controls such as row-level security, whereby the query results are filtered depending on preset criteria applied, for example, to a given user or group of users ([[SQL]] WHERE clause).

==== Disadvantages of ROLAP ====
&lt;!--Note to editors:
Please review the discussion page before making changes to the advantages or disadvantages. Thank you.
--&gt;

* There is a consensus in the industry that ROLAP tools have slower performance than MOLAP tools. However, see the discussion below about ROLAP performance.
* The loading of ''aggregate tables'' must be managed by custom [[Extract, transform, load|ETL]] code.  The ROLAP tools do not help with this task.  This means additional development time and more code to support.
* When the step of creating aggregate tables is skipped, the query performance then suffers because the larger detailed tables must be queried. This can be partially remedied by adding additional aggregate tables, however it is still not practical to create aggregate tables for all combinations of dimensions/attributes.
* ROLAP relies on the general purpose database for querying and caching, and therefore several special techniques employed by [[MOLAP]] tools are not available (such as special hierarchical indexing).  However, modern ROLAP tools take advantage of latest improvements in [[SQL]] language such as CUBE and ROLLUP operators, DB2 Cube Views, as well as other SQL OLAP extensions.  These SQL improvements can mitigate the benefits of the [[MOLAP]] tools.
* Since ROLAP tools rely on [[SQL]] for all of the computations, they are not suitable when the model is heavy on calculations which don't translate well into [[SQL]]. Examples of such models include budgeting, allocations, financial reporting and other scenarios.

==== Performance of ROLAP ====

In the OLAP industry ROLAP is usually perceived as being able to scale for large data volumes, but suffering from slower query performance as opposed to [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]]. The [http://www.olapreport.com/survey.htm OLAP Survey], the largest independent survey across all major OLAP products, being conducted for 6 years (2001 to 2006) have consistently found that companies using ROLAP report slower performance than those using MOLAP even when data volumes were taken into consideration.

However, as with any survey there are a number of subtle issues that must be taken into account when interpreting the results.
* The survey shows that ROLAP tools have 7 times more users than [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools within each company.  Systems with more users will tend to suffer more performance problems at peak usage times.
* There is also a question about complexity of the model, measured both in number of dimensions and richness of calculations. The survey does not offer a good way to control for these variations in the data being analyzed.

==== Downside of flexibility ====

Some companies select ROLAP because they intend to re-use existing relational database tables&#8212;these tables will frequently not be optimally designed for OLAP use.  The superior flexibility of ROLAP tools allows this less than optimal design to work, but performance suffers.  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools in contrast would force the data to be re-loaded into an optimal OLAP design.

===Hybrid OLAP (HOLAP)===
The undesirable trade-off between additional [[Extract, transform, load|ETL]] cost and slow query performance has ensured that most commercial OLAP tools now use a "Hybrid OLAP" (HOLAP) approach, which allows the model designer to decide which portion of the data will be stored in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and which portion in ROLAP.

There is no clear agreement across the industry as to what constitutes "Hybrid OLAP", except that a database will divide data between relational and specialized storage.&lt;ref name="ieee_cite"&gt;{{cite journal
  | last1 = Bach Pedersen | first1 = Torben
  | last2 = S. Jensen 
  | title = Multidimensional Database Technology
  | journal = Distributed Systems Online
  | volume = 
  | issue = 
  | issn = 0018-9162
  | pages = 40&#8211;46
  | publisher = [[IEEE]]
  | location = 
  | date = December 2001
  | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00970558
  | doi = 
  | id = 
  | accessdate = | first2 = Christian }}
&lt;/ref&gt; For example, for some vendors, a HOLAP database will use relational tables to hold the larger quantities of detailed data, and use specialized storage for at least some aspects of the smaller quantities of more-aggregate or less-detailed data. HOLAP addresses the shortcomings of [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and [[#Relational_OLAP_.28ROLAP.29|ROLAP]] by combining the capabilities of both approaches. HOLAP tools can utilize both pre-calculated cubes and relational data sources.

==== Vertical partitioning ====

In this mode HOLAP stores ''aggregations'' in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and detailed data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]] to optimize time of cube ''processing''.

==== Horizontal partitioning ====

In this mode HOLAP stores some slice of data, usually the more recent one (i.e. sliced by Time dimension) in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and older data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]]. Moreover, we can store some dices in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and others in [[#Relational_OLAP_.28ROLAP.29|ROLAP]], leveraging the fact that in a large cuboid, there will be dense and sparse subregions.&lt;ref&gt;Owen Kaser and Daniel Lemire, [http://arxiv.org/abs/cs.DB/0702143 Attribute Value Reordering for Efficient Hybrid OLAP], Information Sciences, Volume 176, Issue 16, pages 2279-2438, 2006.&lt;/ref&gt;

==== Products ====
The first product to provide HOLAP storage was [[Holos]], but the technology also became available in other commercial products such as [[Microsoft Analysis Services]], [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]] and [[SAP AG]] BI Accelerator. The hybrid OLAP approach combines ROLAP and MOLAP technology, benefiting from the greater scalability of ROLAP and the faster computation of MOLAP. For example, a HOLAP server may allow large volumes of detail data to be stored in a relational database, while aggregations are kept in a separate MOLAP store. The Microsoft SQL Server 7.0 OLAP Services supports a hybrid OLAP server

===Comparison===
Each type has certain benefits, although there is disagreement about the specifics of the benefits between providers.

* Some MOLAP implementations are prone to database explosion, a phenomenon causing vast amounts of storage space to be used by MOLAP databases when certain common conditions are met: high number of dimensions, pre-calculated results and sparse multidimensional data.
* MOLAP generally delivers better performance due to specialized indexing and storage optimizations. MOLAP also needs less storage space compared to ROLAP because the specialized storage typically includes [[Data compression|compression]] techniques.&lt;ref name="ieee_cite"/&gt;
* ROLAP is generally more scalable.&lt;ref name="ieee_cite"/&gt; However, large volume pre-processing is difficult to implement efficiently so it is frequently skipped.  ROLAP query performance can therefore suffer tremendously.
* Since ROLAP relies more on the database to perform calculations, it has more limitations in the specialized functions it can use.
* HOLAP encompasses a range of solutions that attempt to mix the best of ROLAP and MOLAP.  It can generally pre-process swiftly, scale well, and offer good function support.

===Other types===
The following acronyms are also sometimes used, although they are not as widespread as the ones above:

* '''WOLAP''' &#8211; Web-based OLAP
* '''DOLAP''' &#8211; [[Desktop computer|Desktop]] OLAP
* '''[[Rtolap|RTOLAP]]''' &#8211; Real-Time OLAP

==APIs and query languages==
Unlike [[relational databases]], which had SQL as the standard query language, and widespread [[Application programming interface|API]]s such as [[ODBC]], [[JDBC]] and [[OLEDB]], there was no such unification in the OLAP world for a long time. The first real standard API was [[OLE DB for OLAP]] specification from [[Microsoft]] which appeared in 1997 and introduced the [[Multidimensional Expressions|MDX]] query language. Several OLAP vendors &#8211; both server and client &#8211; adopted it. In 2001 Microsoft and [[Hyperion Solutions Corporation|Hyperion]] announced the [[XML for Analysis]] specification, which was endorsed by most of the OLAP vendors. Since this also used MDX as a query language, MDX became the de facto standard.&lt;ref&gt;{{cite web|url=http://www.olapreport.com/Comment_APIs.htm |title=Commentary: OLAP API wars |publisher=OLAP Report |author=Nigel Pendse |date=2007-08-23 |accessdate=2008-03-18 |deadurl=yes |archiveurl=https://web.archive.org/web/20080528220113/http://www.olapreport.com/Comment_APIs.htm |archivedate=May 28, 2008 }}&lt;/ref&gt;
Since September-2011 [[LINQ]] can be used to query [[Microsoft Analysis Services|SSAS]] OLAP cubes from Microsoft .NET.&lt;ref&gt;{{cite web|url=http://www.agiledesignllc.com/Products|title=SSAS Entity Framework Provider for LINQ to SSAS OLAP}}&lt;/ref&gt;

==Products==

===History===
The first product that performed OLAP queries was ''Express,'' which was released in 1970 (and acquired by [[Oracle Corporation|Oracle]] in 1995 from Information Resources).&lt;ref&gt;{{cite web|title=The origins of today's OLAP products |url=http://olapreport.com/origins.htm |publisher=OLAP Report |date=2007-08-23 |author=Nigel Pendse |accessdate=November 27, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20071221044811/http://www.olapreport.com/origins.htm |archivedate=December 21, 2007 }}&lt;/ref&gt; However, the term did not appear until 1993 when it was coined by [[Edgar F. Codd]], who has been described as "the father of the relational database". Codd's paper&lt;ref name=Codd1993/&gt; resulted from a short consulting assignment which Codd undertook for former Arbor Software (later [[Hyperion Solutions]], and in 2007 acquired by Oracle), as a sort of marketing coup.  The company had released its own OLAP product, ''[[Essbase]]'', a year earlier. As a result, Codd's "twelve laws of online analytical processing" were explicit in their reference to Essbase. There was some ensuing controversy and when Computerworld learned that Codd was paid by Arbor, it retracted the article.
OLAP market experienced strong growth in late 90s with dozens of commercial products going into market. In 1998, Microsoft released its first OLAP Server &#8211; [[Microsoft Analysis Services]], which drove wide adoption of OLAP technology and moved it into mainstream.

===Product comparison===
{{Main|Comparison of OLAP Servers}}

===OLAP Clients===
OLAP clients include many spreadsheet programs like Excel, web application, sql,dashboard tools, etc.

===Market structure===
Below is a list of top OLAP vendors in 2006, with figures in millions of [[US Dollar]]s.&lt;ref&gt;{{cite web
  |url=http://www.olapreport.com/market.htm
  |title=OLAP Market
  |publisher=OLAP Report
  |author=Nigel Pendse
  |year=2006

  |accessdate=2008-03-17
}}&lt;/ref&gt;
{| class="wikitable sortable"
|- bgcolor="#CCCCCC" align="center"
! Vendor !! Global Revenue  !! Consolidated company
|-
| [[Microsoft Corporation]] || 1,806   || Microsoft
|-
| [[Hyperion Solutions Corporation]] || 1,077  || Oracle
|-
| [[Cognos]] || 735  || IBM
|-
| [[Business Objects (company)|Business Objects]] || 416 || SAP
|-
| [[MicroStrategy]] || 416 || MicroStrategy
|-
| [[SAP AG]] || 330 || SAP
|-
| Cartesis ([[SAP AG|SAP]]) || 210  || SAP
|-
| [[Applix]] || 205  || IBM
|-
| [[Infor]] || 199  || Infor
|-
| [[Oracle Corporation]] || 159 || Oracle
|-
| Others || 152  || Others
|-
| '''Total''' || '''5,700'''
|}

=== Open-source ===
* [[Druid (open-source data store)]] is a popular [[open-source]] distributed data store for OLAP queries that is used at scale in production by various organizations.
* [[Apache Kylin]] is a distributed data store for OLAP queries originally developed by eBay.
* [[Cubes (OLAP server)]] is another light-weight [[open-source]] toolkit implementation of OLAP functionality in the [[Python (programming language)|Python programming language]] with built-in ROLAP.
* [[Linkedin Pinot]] is used at LinkedIn to deliver scalable real time analytics with low latency.&lt;ref&gt;{{cite news |last= Yegulalp |first=Serdar |date=2015-06-11 |title= LinkedIn fills another SQL-on-Hadoop niche |url=http://www.infoworld.com/article/2934506/olap/linkedins-pinot-fills-another-sql-on-hadoop-niche.html |magazine=InfoWorld |access-date=2016-11-19}}&lt;/ref&gt; It can ingest data from offline data sources (such as Hadoop and flat files) as well as online sources (such as Kafka). Pinot is designed to scale horizontally.

== See also ==
{{portal|Computer science}}
* [[Comparison of OLAP Servers]]
* [[Data warehouse]]
* [[Online transaction processing]] (OLTP)
* [[Business analytics]]
* [[Predictive analytics]]
* [[Data Mining]]
* [[Thomsen Diagrams]]
* [[Functional Database Model]]

==Bibliography==
* {{cite web
  |url= http://www.daniel-lemire.com/OLAP/
  |title= Data Warehousing and OLAP-A Research-Oriented Bibliography
  |author= Daniel Lemire
  |date= December 2007
  }}

* {{cite book
  | title = OLAP Solutions: Building Multidimensional Information Systems, 2nd Edition
  | publisher = John Wiley &amp; Sons
  | series =
  | year = 1997
  | isbn = 978-0-471-14931-6
  | author = Erik Thomsen. }}

* Ling Liu and Tamer M. &#214;zsu (Eds.) (2009).  "[http://www.springer.com/computer/database+management+&amp;+information+retrieval/book/978-0-387-49616-0 Encyclopedia of Database Systems], 4100 p.&amp;nbsp;60 illus. ISBN 978-0-387-49616-0.
* O'Brien, J. A., &amp; Marakas, G. M. (2009). Management information systems (9th ed.). Boston, MA: McGraw-Hill/Irwin.

==References==
{{Reflist|30em|refs=
&lt;ref name=ahsan&gt;
{{cite journal
|last1=Abdullah
|first1=Ahsan
|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool
|journal=Computers and Electronics in Agriculture |date=November 2009 |volume=69 |issue=1 |pages=59&#8211;72 |doi=10.1016/j.compag.2009.07.003
}}
&lt;/ref&gt;
}}

{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Online Analytical Processing}}
[[Category:Online analytical processing| ]]
[[Category:Data management]]
[[Category:Information technology management]]</text>
      <sha1>67ic4aaez06hwtkm53r0tes3cfz4jn7</sha1>
    </revision>
  </page>
  <page>
    <title>Managed Memory Computing</title>
    <ns>0</ns>
    <id>40119072</id>
    <revision>
      <id>737441080</id>
      <parentid>737258320</parentid>
      <timestamp>2016-09-02T20:11:57Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Business intelligence]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1956" xml:space="preserve">{{multiple issues|
{{Orphan|date=November 2013}}
{{unreferenced|date=November 2013}}
}}

== This article pertains to a new technology used in Business Intelligence. Managed Memory Computing uses aggregated data for in-memory analytics   ==

Aggregated data cubes are the most effective form of storage of aggregated or summarized data for quick analysis. This technology is driven by [[Online Analytical Processing|Online Analytical Processing technology]]. Utilizing these data cubes involves intense disk I/O operations. This at times lowers the speed for users of data.

Conventional, [[In-Memory Processing|in-memory processing]] does not rely on stored and summarized or aggregated data but brings all the relevant data to the memory. This technology then utilizes intense processing and large amounts of memory to perform all calculations and aggregations while in memory.

Managed Memory Computing blends the best of both methods, allowing users to define data cubes with per-structured and aggregated data, providing a logical business layer to users, and offering in-memory computation. These features make the response time for user interactions far superior and enable the most balanced approach between disk I/O and in-memory processing.

The hybrid approach of Managed Memory Computing provides analysis, dashboards, graphical interaction, ad hoc querying, presentation, and discussion driven analytic at blazing speeds, making the [[Business intelligence|Business Intelligence Tool]] ready for everything from an interactive session in the boardroom to a [[production planning]] meeting on the factory floor.

== References ==

[http://www.cioreview.in/magazine/ElegantJ-BI-Managed-Memory-Computing--Business-Intelligence-Redefined-CZSI499492332.html Introduction of Managed Memory Computing in CIOReview]

[[Category:Business intelligence]]
[[Category:Financial data analysis]]
[[Category:Data management]]
[[Category:Computer architecture]]</text>
      <sha1>13ljlz53hpih04dcof8eqxdh6y76vq3</sha1>
    </revision>
  </page>
  <page>
    <title>Datafication</title>
    <ns>0</ns>
    <id>41731546</id>
    <revision>
      <id>759548351</id>
      <parentid>702953646</parentid>
      <timestamp>2017-01-11T21:00:17Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>added [[Category:Information society]] using [[WP:HC|HotCat]], +see also to [[Big data]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2038" xml:space="preserve">'''Datafication''' is a modern technological trend turning many aspects of our life into computerised data &lt;ref name="CukierMayer-Schoenberger2013"&gt;{{cite journal | last = Cukier | first =Kenneth | last2 = Mayer-Schoenberger | first2 = Viktor  | title =The Rise of Big Data | journal =Foreign Affairs | issue =May/June | pages = 28&#8211;40. | date =2013  | url = http://www.foreignaffairs.com/articles/139104/kenneth-neil-cukier-and-viktor-mayer-schoenberger/the-rise-of-big-data | accessdate = 24 January 2014}}&lt;/ref&gt; and transforming this information into new forms of value. 
&lt;ref name="SchuttOneil2014"&gt;
{{cite book
 | last = O'Neil | first =Cathy
 | last2 =Schutt
 | first2 = Rachel
| title =Doing Data Science
 | publisher =O&#8217;Reilly Media
 | date =2013
 | pages =406
  | isbn =978-1-4493-5865-5
 }}
&lt;/ref&gt;
Examples of datafication as applied to social and communication media are how [[Twitter]] datafies stray thoughts or datafication of [[Human resource management|HR]] by [[LinkedIn]] and others.  Alternative examples are diverse and include aspects of the built environment, and design via engineering and or other tools that tie data to formal, functional or other physical media outcomes of which [[Formsolver]]&lt;ref&gt;https://www.formsolver.com&lt;/ref&gt; is an example.

[[File:Shape optimization for buildings by formsolver.jpg|thumbnail|right|Example: Datafication of the skin and form of a building to assist engineers, designers and architects determine the performance of particular building geometries. Example provided courtesy of Formsolver.com]]
[[File:Emerging Shape Optimization Families for buildings by formsolver.jpg|thumbnail|right|Example: Shape families resulting from differing goals when data is used for the purposes of shape optimization. Example provided courtesy of Formsolver.com]]

==See also==
* [[Big data]]

==References==
{{Reflist}}

[[Category:Information science]]
[[Category:Technology forecasting]]
[[Category:Data management]]
[[Category:Big data]]
[[Category:Information society]]


{{Tech-stub}}</text>
      <sha1>l0hqjo4931c42rxj7oc1fqznwqdcrua</sha1>
    </revision>
  </page>
  <page>
    <title>Data security</title>
    <ns>0</ns>
    <id>1157832</id>
    <revision>
      <id>760636811</id>
      <parentid>757511165</parentid>
      <timestamp>2017-01-18T04:52:15Z</timestamp>
      <contributor>
        <username>Jennica</username>
        <id>623801</id>
      </contributor>
      <minor />
      <comment>removed 500px using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7847" xml:space="preserve">{{Refimprove|date=February 2012}}
'''Data security''' means protecting data, such as a database, from destructive forces and from the unwanted actions of unauthorized users.&lt;ref&gt;Summers, G. (2004). Data and databases.  In: Koehne, H Developing Databases with Access: Nelson Australia Pty Limited. p4-5.&lt;/ref&gt;

== Data security technologies ==

=== Disk encryption ===

[[Disk encryption]] refers to encryption technology that encrypts data on a [[hard disk drive]]. Disk encryption typically takes form in either software (see [[disk encryption software]]) or hardware (see [[disk encryption hardware]]). Disk encryption is often referred to as [[on-the-fly encryption]] (OTFE) or transparent encryption.

=== Software versus hardware-based mechanisms for protecting data ===

Software-based security solutions encrypt the data to protect it from theft. However, a malicious program or a hacker could corrupt the data in order to make it unrecoverable, making the system unusable. Hardware-based security solutions can prevent read and write access to data and hence offer very strong protection against tampering and unauthorized access.

Hardware based security or assisted [[computer security]] offers an alternative to software-only computer security. [[Security token]]s such as those using [[PKCS#11]] may be more secure due to the physical access required in order to be compromised. Access is enabled only when the token is connected and correct [[Personal identification number|PIN]] is entered (see [[two-factor authentication]]). However, dongles can be used by anyone who can gain physical access to it. Newer technologies in hardware-based security solves this problem offering full proof security for data.

Working of hardware-based security: A hardware device allows a user to log in, log out and set different privilege levels by doing manual actions. The device uses biometric technology to prevent malicious users from logging in, logging out, and changing privilege levels. The current state of a user of the device is read by controllers in [[peripheral devices]] such as hard disks. Illegal access by a malicious user or a malicious program is interrupted based on the current state of a user by hard disk and DVD controllers making illegal access to data impossible. Hardware-based access control is more secure than protection provided by the operating systems as operating systems are vulnerable to malicious attacks by [[Computer virus|viruses]] and hackers. The data on hard disks can be corrupted after a malicious access is obtained. With hardware-based protection, software cannot manipulate the user privilege levels. It is impossible for a [[Hacker (computer security)|hacker]] or a malicious program to gain access to secure data protected by hardware or perform unauthorized privileged operations. This assumption is broken only if the hardware itself is malicious or contains a backdoor.&lt;ref&gt;{{Citation| last1 = Waksman  | first1 = Adam | last2 = Sethumadhavan | first2 = Simha | title = Silencing Hardware Backdoors | volume = | pages =  | periodical = Proceedings of the IEEE Symposium on Security and Privacy | location = Oakland, California  | url = http://www.cs.columbia.edu/~simha/preprint_oakland11.pdf | year = 2011  | issn =  | doi =  | isbn = }}&lt;/ref&gt; The hardware protects the operating system image and file system privileges from being tampered. Therefore, a completely secure system can be created using a combination of hardware-based security and secure system administration policies.

=== Backups ===

[[Backup]]s are used to ensure data which is lost can be recovered from another source. It is considered essential to keep a backup of any data in most industries and the process is recommended for any files of importance to a user.

===Data masking===
{{main|Data masking}}
[[Data masking]] of structured data is the process of obscuring (masking) specific data within a database table or cell to ensure that data security is maintained and sensitive information is not exposed to unauthorized personnel.&lt;ref&gt;{{cite web|title=What is Data Obfuscation|url=http://www.dataobfuscation.com.au|accessdate=1 March 2016}}&lt;/ref&gt;  This may include masking the data from users (for example so banking customer representatives can only see the last 4 digits of a customers national identity number), developers (who need real production data to test new software releases but should not be able to see sensitive financial data), outsourcing vendors, etc.
&lt;ref&gt;{{Cite web
 |url = http://searchsecurity.techtarget.com/definition/data-masking
 |title = data masking
 |accessdate = 29 July 2016
}}&lt;/ref&gt;

===Data erasure===
[[Data erasure]] is a method of software-based overwriting that completely destroys all electronic data residing on a hard drive or other digital media to ensure that no sensitive data is leaked when an asset is retired or reused...

== International laws and standards ==

=== International laws ===

In the [[United Kingdom|UK]], the [[Data Protection Act 1998|Data Protection Act]] is used to ensure that personal data is accessible to those whom it concerns, and provides redress to individuals if there are inaccuracies.&lt;ref&gt;{{Cite web
 |url = https://ico.org.uk/for-organisations/guide-to-data-protection/principle-1-fair-and-lawful/
 |title = data protection act
 |accessdate = 29 July 2016
}}&lt;/ref&gt; This is particularly important to ensure individuals are treated fairly, for example for credit checking purposes. The Data Protection Act states that only individuals and companies with legitimate and lawful reasons can process personal information and cannot be shared. [[Data Privacy Day]] is an international [[holiday]] started by the [[Council of Europe]] that occurs every January 28.&lt;ref name=dataprivacyday&gt;{{cite web|url=http://googleblog.blogspot.com/2008/01/celebrating-data-privacy.html|title=Celebrating data privacy |author=[[Peter Fleischer]], [[Jane Horvath]], [[Shuman Ghosemajumder]]|publisher=[[Google Blog]] |accessdate=12 August 2011 |year=2008}}&lt;/ref&gt;

=== International standards ===

The international standards ISO/IEC 27001:2013 and ISO/IEC 27002:2013 covers data security under the topic of [[information security]], and one of its cardinal principles is that all stored information, i.e. data, should be owned so that it is clear whose responsibility it is to protect and control access to that data.

The [[Trusted Computing Group]] is an organization that helps standardize computing security technologies.

The [[PCI DSS|Payment Card Industry Data Security Standard]] is a proprietary international information security standard for organizations that handle cardholder information for the major [[Debit card|debit]], [[Credit card|credit]], prepaid, [[e-purse]], [[Cash machine|ATM]] and POS cards.&lt;ref&gt;{{cite web|title=PCI DSS Definition|url=http://www.pcmag.com/encyclopedia/term/59104/pci-dss|accessdate=1 March 2016}}&lt;/ref&gt;

== Industry and software ==
There are several data security software available to be used by consumers and one of the most used data security software with a U.S issued patent is [[Folder lock|Folder Lock]].

==See also==
* [[Copy Protection]]
* [[Data-centric security]]
* [[Data erasure]]
* [[Data masking]]
* [[Data recovery]]
* [[Digital inheritance]]
* [[Disk encryption]]
** [[Comparison of disk encryption software]]
* [[Identity Based Security]]
* [[Information security]]
* [[IT network assurance]]
* [[Pre-boot authentication]]
* [[Privacy engineering]]
* [[Secure USB drive]]
* [[Security Breach Notification Laws]]
* [[Single sign-on]]
* [[Smart card]]
* [[Trusted Computing Group]]

== Notes and references ==
{{reflist}}

==External links==
{{Commons category}}

{{Data}}
{{Privacy}}
{{Portal bar|Computer security|Information technology}}

[[Category:Data security| ]]
[[Category:Data management]]</text>
      <sha1>kx6oawomrcubukb0rskfeetss46phd7</sha1>
    </revision>
  </page>
  <page>
    <title>Query Rewriting</title>
    <ns>0</ns>
    <id>43435003</id>
    <revision>
      <id>621044622</id>
      <parentid>619196384</parentid>
      <timestamp>2014-08-13T11:10:14Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]] (10369)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1516" xml:space="preserve">{{Orphan|date=August 2014}}

'''Query Rewriting''' is a technique used in mediation based [[data integration]] systems for translating the queries formulated over the mediated schema to a query over the various sources by making use of the view definitions.&lt;ref name="refone"&gt;{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270&#8211;294}}&lt;/ref&gt; Mediation based data integration system hides from the end user the underlying heterogeneity of the various data providing sources linked to it by providing a uniform query interface in the form of a mediated schema. This schema is also referred to as the global schema whereas the schema of the various data sources is collectively referred to as the local schema. The local schema and the mediated schema are mapped to each other using view definitions. The queries formulated on the mediated schema cannot be directly used to query the sources. Therefore query rewriting translates such a query formulated over the global schema to a query over the various data sources. Examples include bucket algorithm, Minicon algorithm, inverse rules algorithm.&lt;ref name="refone"/&gt; This rewritten query is then evaluated to obtain the query response making use of the data obtained by querying the data sources.

==See also==
* [[Data integration]]
* [[Schema Matching]]
* [[Data Virtualization]]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Query Rewriting}}
[[Category:Data management]]</text>
      <sha1>me48d1rrh0z50zrn74bi0g1ks1xa8s9</sha1>
    </revision>
  </page>
  <page>
    <title>Government Performance Management</title>
    <ns>0</ns>
    <id>26105075</id>
    <revision>
      <id>749479844</id>
      <parentid>745922747</parentid>
      <timestamp>2016-11-14T15:38:26Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>5 archive templates merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8624" xml:space="preserve">{{Refimprove|date=October 2014}}
'''Government Performance Management''' (GPM) consists of a set of processes that help government organizations optimize their business performance. It provides a framework for organizing, automating, and analyzing business methodologies, metrics, processes and systems that drive business performance.&lt;ref&gt;{{cite web|url=http://www.information-management.com/bissues/20070301/2600312-1.html|title=Performance Management for Government|author=Michael Owellen|date=28 February 2007|work=BI Review Magazine|accessdate=22 October 2014}}&lt;/ref&gt; Some commentators{{who|date=October 2014}} see GPM as the next generation of [[business intelligence]] (BI) for governments. GPM helps governments to make use of their financial, human, material, and other resources.  In the past, owners have sought to drive strategy down and across their organizations; they have struggled to transform strategies into actionable metrics and they have grappled with meaningful analysis to expose the cause-and-effect relationships that, if understood, could give profitable insight to their operational decision-makers.  GPM software and methods allow a systematic, integrated approach that links government strategy to core processes and activities. "Running by the numbers" now means something: planning, budgeting, analysis, and reporting can give the measurements that empower management decisions.&lt;ref&gt;{{cite web|url=http://www.encyclopedia.com/doc/1O12-performancemanagement.html |accessdate=February 7, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20090112223214/http://www.encyclopedia.com/doc/1O12-performancemanagement.html |archivedate=January 12, 2009 }}&lt;/ref&gt;

== Performance Management (PM) Market ==
According to [[Gartner]]{{citation needed|date=October 2014}}, the Enterprise Performance Management (EPM) suite market continues to experience strong momentum, growing 19% during 2007. This is slightly in advance of their earlier market sizing and forecast analysis, which anticipated 2007 revenue to be $1.836 million, representing an 18% year-over-year growth. In the latest forecast, Gartner believe that the market for EPM will be more than $3 billion by 2011, representing a 14.4% compound annual growth rate. Several factors contributed to the continued significant growth in EPM revenue during 2007:
* Many organizations replaced difficult-to-maintain, inflexible, or outmoded spreadsheets and homegrown financial applications.
* Continued growth in large enterprises was fueled by desires to achieve greater transparency and adherence to governance and compliance legislation.
* Increased demand for applications that support strategic plans and operational activities drove new momentum in the deployment of scorecards.
* There was increased demand from mid-size enterprises, representing one of the largest untapped and dynamic areas of the business application software sector.
* Advertising and PR from increasingly large vendors and system integrators are raising the EPM profile and generating greater demand.

Gartner also expects the Business Intelligence software market to reach $3 billion in 2009. "Companies around the world have purchased more than US $40 billion worth of enterprise applications, including ERP, CRM and HR, during the past few years," said Colleen Graham, principal research analyst at Gartner. "This has generated significant volumes of data in support of the operational processes they automate. By investing in BI, companies can further leverage their enterprise application investments and turn the torrent of data into meaningful insight to better measure performance, respond more quickly to market changes and opportunities and comply with an increasingly complex regulatory environment."&lt;ref&gt;{{cite web|url=http://www.gartner.com/press_releases/asset_144782_11.html|title=Gartner News Room|publisher=Gartner.com|accessdate=22 October 2014}}&lt;/ref&gt;

== ITWorx Government Performance Management (GPM) ==

[[ITWorx]] GPM is a bilingual, [[Microsoft]]-based framework that gives governments the capability to cascade, share, track, and update strategies and plans organization-wide. It creates detailed views of multi-source [[Key Performance Indicator]]s (KPIs) using customized [[balance scorecard]]s, dashboards, strategy maps, statistical charts, and reports, as well as provides ad hoc analytical and reporting tools.

== ITWorx GPM Features ==
{{Merge to|section=yes|ITWorx|date=October 2014}}
{{advert|date=October 2014}}
ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; provides a top-down approach in recording government strategy. The strategy is cascaded and shared across government bodies to define objectives and balancing targets.

It also links strategy to execution. Operational plans are recorded, linked to strategies, assigned a time-range for implementation, broken down to initiatives and business activities, approved, and then propagated to all levels.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; defines a time-range for implementing an initiative, its owners, cost drivers, budgets, KPIs, and targets; and links initiatives to strategic objectives. Business activities, contributing in strategy execution, are defined including their KPIs and targets. KPIs can be entered and configured manually, calculated using other KPIs, or extracted from external data sources.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; enables the definition of government-specific business rules such as KPI calculation formulas; it also enables administration of system settings such as the configuration of the organization structure and definition of approval [[workflows]] for each organization unit. Furthermore, administrators can manage user roles and groups as well as archive plans and approvals.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; calculates measurement formulas, compares actual values against targets, and performs analysis. Color-coded KPIs are represented through strategic and customized scorecards, dashboards, strategy maps, and statistical charts and graphs, in addition to ad hoc analytical and reporting tools.

The solution enables communication throughout the decision-making process by allowing users to post comments and discuss topics regarding a strategy, KPI, or report. Keeping a documented record of why and when decisions are made, ITWorx GPM retains the history of contributions.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; provides a mechanism for policy-makers and strategy implementers to facilitate the strategic management process without compromising data. Government frontline officials are provided with a feedback channel to submit change requests and propositions to approved strategic plans, targets, and actual data while securing the validity and consistency of data.

Frontline officials can monitor performance though consolidated views while detailed views are provided for department and executive levels. Based on privileges, users can view rolled-up KPIs and drill-down for [[root cause analysis]] or corrective actions.

==References==
{{Reflist}}

==External links==
*[http://gpm.itworx.com Gpm.itworx.com]
*[http://www.microsoft.com/downloads/details.aspx?displaylang=en&amp;FamilyID=efdc60d3-2622-44f5-aa5d-2b79d10c93ab  Microsoft.com]
*[http://www.pcmag-mideast.com/gitex/tag/itworx/ Pcmag-mideast.com]
*[http://www.itp.net/578068-itworx-releases-new-gpm-suite Itp.net]

[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>tw0pkk9vos6zhzj1pqjjqlj1rwf04eh</sha1>
    </revision>
  </page>
  <page>
    <title>Systems of Engagement</title>
    <ns>0</ns>
    <id>43443443</id>
    <revision>
      <id>693760401</id>
      <parentid>656736565</parentid>
      <timestamp>2015-12-04T18:24:27Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor />
      <comment>Fixing [[Wikipedia:Disambiguation pages with links|links to disambiguation pages]], replaced: [[HP]] &#8594; [[Hewlett-Packard|HP]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2676" xml:space="preserve">The concept of '''Systems of Engagement''' is attributed to [[Geoffrey Moore]], a business author of such books as Crossing the Chasm.&lt;ref&gt;Crossing the Chasm: Marketing and Selling High-tech Products to Mainstream Customers (1991, revised 1999 and 2014) &#8211; ISBN 0-06-051712-3&lt;/ref&gt; In his paper for AIIM.org entitled: "Systems of Engagement and the Future of Enterprise IT" Moore states:
&#8220;Amidst the texting and Twittering and Facebooking of a generation of digital natives, the fundamentals of next-generation communication and collaboration are being worked out. For them, it is clear, there is no going back. So at minimum, if you expect these folks to be your customers, your employees, and your citizens (and, frankly, where else could you look?), then you need to apply THEIR expectations to the next generation of enterprise IT systems....Systems of Engagement &#8230; will overlay and complement our deep investments in systems of record.&#8221;&lt;ref&gt;Moore, Geoffrey (2011). "Systems of Engagement and the Future of Enterprise IT". http://www.aiim.org/futurehistory&lt;/ref&gt;{{cite news |last=Moore |first=Geoffrey |date=2011 |title=Systems of Engagement and the Future of Enterprise IT: A Sea Change in Enterprise IT |url=http://www.aiim.org/futurehistory |accessdate=October 7, 2014}}
Since then Systems of Engagement has been adopted by organizations such as [http://blogs.forrester.com/category/systems_of_engagement Forrester Research], [[Hewlett-Packard|HP]], [[IBM]], [[AIIM]], and [http://www.avoka.com/blog/2013/07/deliver-a-system-of-engagement/ Avoka]. Forrester defines Systems of Engagement as follows: "Systems of engagement are different from the traditional systems of record that log transactions and keep the financial accounting in order: They focus on people, not processes....These new systems harness a perfect storm of mobile, social, cloud, and big data innovation to deliver apps and smart products directly in the context of the daily lives and real-time workflows of customers, partners, and employees.&#8221;&lt;ref&gt;http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement&lt;/ref&gt; {{cite news |last=Schadler |first=Ted |date=February 14, 2012 |title=A Billion Smartphones Require New Systems Of Engagement |url=http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement |accessdate=October 7, 2014 }}

==See also==
* [[System of record]] &#8212; conventional enterprise systems designed to contain the authoritative data source for a given piece of information.
==References==
{{Reflist}}


[[Category:Information systems]]
[[Category:Data management]]


{{compu-stub}}</text>
      <sha1>0r73pl47bc7gdhxndqsdlsju0e4gp08</sha1>
    </revision>
  </page>
  <page>
    <title>Relational data stream management system</title>
    <ns>0</ns>
    <id>44046965</id>
    <revision>
      <id>715081210</id>
      <parentid>636904926</parentid>
      <timestamp>2016-04-13T16:04:02Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>/* RDSMS SQL Query Examples */clean up, [[WP:AWB/T|typo(s) fixed]]: one second &#8594; one-second (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3224" xml:space="preserve">A '''relational data stream management system (RDSMS)''' is a distributed, in-memory [[data stream management system]] (DSMS) that is designed to use standards-compliant [[SQL]] queries to process unstructured and structured data streams in real-time. Unlike [[SQL]] queries executed in a traditional [[RDBMS]], which return a result and exit, SQL queries executed in a RDSMS do not exit, generating results continuously as new data become available. Continuous SQL queries in a RDSMS use the [[SQL]] Window function to analyze, join and aggregate data streams over fixed or sliding windows. Windows can be specified as time-based or row-based.

== RDSMS SQL Query Examples ==

Continuous [[SQL]] queries in a RDSMS conform to the [[ANSI]] [[SQL]] standards. The most common RDSMS SQL query is performed with the declarative &lt;code&gt;SELECT&lt;/code&gt; statement. A continuous SQL &lt;code&gt;SELECT&lt;/code&gt; operates on data across one or more data streams, with optional keywords and clauses that include &lt;code&gt;FROM&lt;/code&gt; with an optional &lt;code&gt;JOIN&lt;/code&gt; subclause to specify the rules for joining multiple data streams, the &lt;code&gt;WHERE&lt;/code&gt; clause and comparison predicate to restrict the records returned by the query, &lt;code&gt;GROUP BY&lt;/code&gt; to project streams with common values into a smaller set, &lt;code&gt;HAVING&lt;/code&gt; to filter records resulting from a &lt;code&gt;GROUP BY&lt;/code&gt;, and &lt;code&gt;ORDER BY&lt;/code&gt; to sort the results.

The following is an example of a continuous data stream aggregation using a &lt;code&gt;SELECT&lt;/code&gt; query that aggregates a sensor stream from a weather monitoring station. The &lt;code&gt;SELECT&lt;/code&gt;query aggregates the minimum, maximum and average temperature values over a one-second time period, returning a continuous stream of aggregated results at one second intervals.
 
&lt;source lang="sql"&gt;
SELECT STREAM
    FLOOR(WEATHERSTREAM.ROWTIME to SECOND) AS FLOOR_SECOND,
    MIN(TEMP) AS MIN_TEMP,
    MAX(TEMP) AS MAX_TEMP,
    AVG(TEMP) AS AVG_TEMP
FROM WEATHERSTREAM
GROUP BY FLOOR(WEATHERSTREAM.ROWTIME TO SECOND);
&lt;/source&gt;

RDSMS SQL queries also operate on data streams over time or row-based windows. The following example shows a second continuous SQL query using the &lt;code&gt;WINDOW&lt;/code&gt; clause with a one-second duration. The &lt;code&gt;WINDOW&lt;/code&gt; clause changes the behavior of the query, to output a result for each new record as it arrives. Hence the output is a stream of incrementally updated results with zero result latency.

&lt;source lang="sql"&gt;
SELECT STREAM
    ROWTIME,
    MIN(TEMP) OVER W1 AS WMIN_TEMP,
    MAX(TEMP) OVER W1 AS WMAX_TEMP,
    AVG(TEMP) OVER W1 AS WAVG_TEMP
FROM WEATHERSTREAM
WINDOW W1 AS ( RANGE INTERVAL '1' SECOND PRECEDING );
&lt;/source&gt;

== See also ==
* [[SQL]]
* [[NoSQL]]
* [[NewSQL]]

== External links ==
* [http://www.sqlstream.com/stream-processing/ Stream processing with SQL]
* [http://researcher.watson.ibm.com/researcher/view_group.php?id=2531 IBM System S]
* [http://www.mcjones.org/System_R/SQL_Reunion_95/sqlr95.html ''1995 SQL Reunion: People, Projects, and Politics'', by Paul McJones (ed.)]: transcript of a reunion meeting devoted to the personal history of relational databases, SQL System R.

[[Category:Data management]]
[[Category:Relational model]]</text>
      <sha1>eszymn8xnzpfp24x0qvrd3t2n6i5lym</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Object storage</title>
    <ns>14</ns>
    <id>44628227</id>
    <revision>
      <id>637037146</id>
      <timestamp>2014-12-07T16:30:32Z</timestamp>
      <contributor>
        <username>Leoinspace</username>
        <id>14273534</id>
      </contributor>
      <comment>Making Object storage a sub-category of Data management</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28" xml:space="preserve">[[Category:Data management]]</text>
      <sha1>tpzfnyay41jj855s8j16rxxvd2yg34n</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic query</title>
    <ns>0</ns>
    <id>44626050</id>
    <revision>
      <id>759242474</id>
      <parentid>749670137</parentid>
      <timestamp>2017-01-10T01:20:33Z</timestamp>
      <contributor>
        <ip>128.189.133.101</ip>
      </contributor>
      <comment>/* Background */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8031" xml:space="preserve">'''Semantic queries''' allow for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide open questions through [[pattern matching]] and [[Reasoning system|digital reasoning]].

Semantic queries work on [[named graphs]], [[Linked Data|linked-data]] or [[Semantic triple|triples]]. This enables the query to process the actual relationships between information and ''infer'' the answers from the ''network of data''. This is in contrast to [[semantic search]], which uses [[semantics]] (the science of meaning) in [[Unstructured data|unstructured text]] to produce a better search result (see [[natural language processing]]).

From a technical point of view semantic queries are precise relational-type operations much like a [[SQL|database query]]. They work on structured data and therefore have the possibility to utilize comprehensive features like operators (e.g. &gt;, &lt; and =), namespaces, [[pattern matching]], [[Type inheritance|subclassing]], [[transitive relation]]s, [[Semantic Web Rule Language|semantic rules]] and contextual [[Full-text index|full text search]]. The [[semantic web]] technology stack of the [[W3C]] is offering [[SPARQL]]&lt;ref name="XML.com"&gt;{{cite web|url=http://www.xml.com/pub/a/2005/11/16/introducing-sparql-querying-semantic-web-tutorial.html |title=Introducing SPARQL: Querying the Semantic Web |publisher=XML.com|date=2005}}&lt;/ref&gt;&lt;ref name="W3C"&gt;{{cite web|url=http://www.w3.org/TR/rdf-sparql-query |title=SPARQL Query Language for RDF |publisher=W3C|date=2008}}&lt;/ref&gt; to formulate semantic queries in a syntax similar to [[SQL]]. Semantic queries are used in [[triplestore]]s, [[graph databases]], [[semantic wiki]]s, natural language and artificial intelligence systems.

== Background ==

[[Relational database]]s contain all relationships between data in an ''implicit'' manner only.&lt;ref name="ACM-DL"&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=1646157 |title=Semantic queries in databases: problems and challenges |publisher=ACM Digital Library|date=2009}}&lt;/ref&gt;&lt;ref name="ESWC"&gt;{{cite web|url=http://2012.eswc-conferences.org/sites/default/files/eswc2012_submission_357.pdf |title=Karma: A System for Mapping Structured Sources into the Semantic Web |publisher=eswc-conferences.org|date=2012}}&lt;/ref&gt; For example, the relationships between customers and products (stored in two content-tables and connected with an additional link-table) only come into existence in a query statement ([[SQL]] in the case of relational databases) written by a developer. Writing the query demands exact knowledge of the [[database schema]].&lt;ref name="IEEE"&gt;{{cite web|url=http://www-scf.usc.edu/~taheriya/papers/taheriyan14-icsc-paper.pdf |title=A Scalable Approach to Learn Semantic Models of Structured Sources |publisher=8th IEEE International Conference on Semantic Computing|date=2014}}&lt;/ref&gt;&lt;ref name="AAAI"&gt;{{cite web|url=http://www.isi.edu/integration/papers/knoblock13-sbd.pdf |title=Semantics for Big Data Integration and Analysis |publisher=AAAI Fall Symposium on Semantics for Big Data|date=2013}}&lt;/ref&gt;

[[Linked Data|Linked-Data]] contain all relationships between data in an ''explicit'' manner. In the above example no query code needs to be written. The correct product for each customer can be fetched automatically. Whereas this simple example is trivial, the real power of linked-data comes into play when a ''network of information'' is created (customers with their geo-spatial information like city, state and country; products with their categories within sub- and super-categories). Now the system can automatically answer more complex queries and analytics that look for the connection of a particular location with a product category. The development effort for this query is omitted. Executing a semantic query is conducted by ''walking'' the network of information and finding matches (also called ''Data Graph Traversal'').

Another important aspect of semantic queries is that the type of the relationship can be used to incorporate intelligence into the system. The relationship between a customer and a product has a fundamentally different nature than the relationship between a neighbourhood and its city. The latter enables the semantic query engine to ''infer'' that a customer ''living in Manhattan is also living in New York City'' whereas other relationships might have more complicated patterns and "contextual analytics". This process is called inference or reasoning and is the ability of the software to derive new information based on given facts.

== Articles ==

* {{Cite web
| last =  Velez
| first = Golda
| year = 2008
| url = http://www.wallstreetandtech.com/data-management/showArticle.jhtml?articleID=208700210&amp;pgno=2
| title = Semantics Help Wall Street Cope With Data Overload
| publisher = wallstreetandtech.com
}}
* {{Cite web
| last =  Zhifeng
| first = Xiao
| year = 2009
| url = http://adsabs.harvard.edu/abs/2009SPIE.7492E..60X
| title = Spatial information semantic query based on SPARQL
| publisher = International Symposium on Spatial Analysis
}}
* {{Cite web
| last = Aquin
| first = Mathieu
| year = 2010
| url = http://www.semantic-web-journal.net/sites/default/files/swj96_1.pdf
| title = Watson, more than a Semantic Web search engine
| publisher = Semantic Web Journal
}}
* {{Cite web
| last =  Prudhommeaux
| first = Eric
| year = 2010
| url = http://www.cambridgesemantics.com/semantic-university/sparql-vs-sql-intro
| title = SPARQL vs. SQL - Introduction
| publisher = Cambridge Semantics
}}
* {{Cite web
| last = Dworetzky
| first = Tom
| year = 2011
| url = http://www.ibtimes.com/how-siri-works-iphones-brain-comes-natural-language-processing-stanford-professors-teach-free-online
| title = How Siri Works: iPhone's 'Brain' Comes from Natural Language Processing
| publisher = International Business Times
}}
* {{Cite web
| last =  Horwitt
| first = Elisabeth
| year = 2011
| url = http://www.computerworld.com/s/article/9209118/The_semantic_Web_gets_down_to_businessarticleID=208700210&amp;pgno=2
| title = The semantic Web gets down to business
| publisher = computerworld.com
}}
* {{Cite web
| last = Rodriguez
| first = Marko
| year = 2011
| url = http://markorodriguez.com/2011/06/15/graph-pattern-matching-with-gremlin-1-1/
| title = Graph Pattern Matching with Gremlin
| publisher = markorodriguez.com on Graph Computing
}}
* {{Cite web
| last = Sequeda
| first = Juan
| year = 2011
| url = http://www.cambridgesemantics.com/semantic-university/sparql-nuts-and-bolts
| title = SPARQL Nuts &amp; Bolts
| publisher = Cambridge Semantics
}}
* {{Cite web
| last = Freitas
| first = Andre
| year = 2012
| url = https://www.deri.ie/sites/default/files/publications/freitas_ic_12.pdf
| title = Querying Heterogeneous Datasets on the Linked Data Web
| publisher = IEEE Internet Computing
}}
* {{Cite web
| last = Kauppinen
| first = Tomi
| year = 2012
| url = http://linkedscience.org/tools/sparql-package-for-r/tutorial-on-sparql-package-for-r/
| title = Using the SPARQL Package in R to handle Spatial Linked Data
| publisher = linkedscience.org
}}
* {{Cite web
| last = Lorentz
| first = Alissa
| year = 2013
| url = http://www.wired.com/2013/04/with-big-data-context-is-a-big-issue/
| title = With Big Data, Context is a Big Issue
| publisher = Wired
}}

== See also ==

* [[Dataspaces]]
* [[Knowledge Representation]]
* [[Linked Data]]
* [[Ontology alignment]]
* [[Semantic Integration]]
* [[Semantic publishing]]
* [[Semantics of Business Vocabulary and Business Rules]]
* [[SPARQL]]

== References ==
{{reflist}}

==External links==
* [http://www.w3.org/standards/semanticweb/query W3C Semantic Web Standards - Query]

[[Category:Data management]]
[[Category:Query languages]]
[[Category:Semantic Web]]</text>
      <sha1>t6iq6sloydr16gbc294tbgjrz3zygfu</sha1>
    </revision>
  </page>
  <page>
    <title>Data lineage</title>
    <ns>0</ns>
    <id>44783487</id>
    <revision>
      <id>756078193</id>
      <parentid>730386840</parentid>
      <timestamp>2016-12-21T21:37:33Z</timestamp>
      <contributor>
        <ip>74.118.24.163</ip>
      </contributor>
      <comment>casing on the second word in the sentence: The</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="38929" xml:space="preserve">{{peacock|date=May 2015}}

'''Data lineage''' is defined as a data life cycle that includes the data's origins and where it moves over time.&lt;ref&gt;http://www.techopedia.com/definition/28040/data-lineage&lt;/ref&gt; It describes what happens to data as it goes through diverse processes. It helps provide visibility into the analytics pipeline and simplifies tracing errors back to their sources. It also enables replaying specific portions or inputs of the dataflow for step-wise debugging or regenerating lost output. In fact, database systems have used such information, called data provenance, to address similar validation and debugging challenges already.&lt;ref name="DeSoumyarupa"&gt;De, Soumyarupa. (2012). Newt : an architecture for lineage based replay and debugging in DISC systems. UC San Diego: b7355202. Retrieved from: https://escholarship.org/uc/item/3170p7zn&lt;/ref&gt;

'''Data Lineage''' provides a visual representation to discover the data flow/movement from its source to destination via various changes and hops on its way in the enterprise environment. 
''Data lineage'' represents: how the data hops between various data points, how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. Easier representation of the ''Data Lineage'' can be shown with dots and lines, where dot represents a data container for data point(s) and lines connecting them represents the transformation(s) the data point under goes, between the data containers.

&lt;!-- Deleted image removed: [[File:DataLineage Dots lines.png]] --&gt;

Representation of ''Data Lineage'' broadly depends on scope of the ''[[Meta-data management|Metadata Management]]'' and reference point of interest. ''Data Lineage'' provides sources of the data and intermediate data flow hops from the reference point with '''Backward data lineage''', leads to the final destination's data points and its intermediate data flows with '''Forward data lineage'''.  These views can be combined with '''End to End Lineage''' for a reference point that provides complete audit trail of that data point of interest from source(s) to its final destination(s). As the data points or hops increases, the complexity of such representation becomes incomprehensible. Thus, the best feature of the data lineage view would be to be able to simplify the view by temporarily ''Masking'' unwanted peripheral data points. Tools that have the '''masking''' feature enables scalability of the view and enhances analysis with best user experience for both Technical and business users alike.

'''Scope of the data lineage''' determines the volume of metadata required to represent its data lineage. Usually, [[Data governance|Data Governance]], and [[Data management|Data Management]] determines the scope of the data lineage based on their [[regulation]]s, ''enterprise data management strategy'', ''data impact'', ''reporting attributes'', and ''critical [[data element]]s'' of the organization.

''Data Lineage'' provides the audit trail of the data points at the lowest granular level,but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to the ''analytic web maps''. ''Data Lineage'' can be visualized at various levels based on the granularity of the view. At a very high level ''data lineage'' provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior, attribute properties, and trends and ''[[Data quality|Data Quality]]'' of the data passed through that specific data point in the ''data lineage''.

''[[Data governance|Data Governance]]'' plays a key role in metadata management for guidelines, strategies, policies, implementation. ''[[Data quality|Data Quality]]'', and ''[[Master data management|Master Data Management]]'' helps in enriching the data lineage with more business value. Even though the final representation of ''Data lineage'' is provided in one interface but the way the metadata is harvested and exposed to the data lineage '''[[Graphical user interface|User Interface (UI)]]''' could be entirely different. Thus, ''Data lineage'' can be broadly divided into three categories based on the way metadata is harvested:Data lineage involving ''software packages for structured data'', ''Programming Languages'', and ''Big Data''.

''Data lineage'' expects to view at least the technical metadata involving the data points and its various transformations. Along with technical data, ''Data Lineage'' may enrich the metadata with their corresponding Data Quality results,Reference Data values, [[Data model|Data Models]], [[Glossary of business and management terms|Business Vocabulary]], [[Data steward|People]], [[Program management|Programs]], and [[Enterprise system|Systems]] linked to the data points and transformations. Masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case.  
Metadata normalization may be done in data lineage to represent disparate systems into one common view.

'''Data provenance''' documents the inputs, entities, systems, and processes that influence data of interest, in effect providing a historical record of the data and its origins. The generated evidence supports essential forensic activities such as data-dependency analysis, error/compromise detection and recovery, and auditing and compliance analysis. "'''Lineage''' is a simple type of '''why provenance'''."&lt;ref name="DeSoumyarupa"/&gt;

==Case for Data Lineage==
The world of [[big data]] is changing dramatically right before our eyes. Statistics say that Ninety percent (90%) of the world&#8217;s data has been created in the last two years alone.&lt;ref&gt;http://newstex.com/2014/07/12/thedataexplosionin2014minutebyminuteinfographic/&lt;/ref&gt; This explosion of data has resulted in the ever-growing number of systems and automation at all levels in all sizes of organizations.

Today, distributed systems like Google [[Map Reduce]],&lt;ref&gt;Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on
large clusters. Commun. ACM, 51(1):107&#8211;113, January 2008.&lt;/ref&gt; Microsoft Dryad,&lt;ref&gt;Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly.
Dryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference onComputer
Systems 2007, EuroSys &#8217;07, pages 59&#8211;72, New York, NY, USA, 2007. ACM.&lt;/ref&gt; Apache Hadoop &lt;ref&gt;Apache Hadoop. http://hadoop.apache.org.&lt;/ref&gt;(an open-source project) and Google Pregel&lt;ref&gt;Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for largescale graph processing. In Proceedings of the 2010 international conference on Managementof data, SIGMOD &#8217;10, pages 135&#8211;146, New York, NY, USA, 2010. ACM.&lt;/ref&gt; provide such platforms for businesses and users. However, even with these systems, [[big data]] analytics can take several hours, days or weeks to run, simply due to the data volumes involved. For example, a ratings prediction algorithm for the Netflix Prize challenge took nearly 20 hours to execute on 50 cores, and a large-scale image processing task to estimate geographic information took 3 days to complete using 400 cores.&lt;ref&gt;Shimin Chen and Steven W. Schlosser. Map-reduce meets wider varieties of
applications. Technical report, Intel Research, 2008.&lt;/ref&gt; "The Large Synoptic Survey Telescope is expected to generate terabytes of data every night and eventually store more than 50 petabytes, while in the bioinformatics sector, the largest genome 12 sequencing houses in the world now store petabytes of data apiece."&lt;ref&gt;The data deluge in genomics. https://www-304.ibm.com/connections/blogs/ibmhealthcare/entry/data overload in genomics3?lang=de, 2010.&lt;/ref&gt;
Due to the humongous size of the [[big data]], there could be features in the data that are not considered in the machine learning algorithm, possibly even outliers. It is very difficult for a data scientist to trace an unknown or an unanticipated result.

===Big Data Debugging===

[[Big data]] analytics is the process of examining large data sets to uncover hidden patterns, unknown correlations, market trends, customer preferences and other useful business information. They apply machine learning algorithms etc. to the data which transform the data. Due to the humongous size of the data, there could be unknown features in the data, possibly even outliers. It is pretty difficult for a data scientist to actually debug an unexpected result.

The massive scale and unstructured nature of data, the complexity of these analytics pipelines, and long runtimes pose significant manageability and debugging challenges. Even a single error in these analytics can be extremely difficult to identify and remove. While one may debug them by re-running the entire analytics through a debugger for step-wise debugging, this can be expensive due to the amount of time and resources needed. Auditing and data validation are other major problems due to the growing ease of access to relevant data sources for use in experiments, sharing of data between scientific communities and use of third-party data in business enterprises.&lt;ref&gt;Yogesh L. Simmhan, Beth Plale, and Dennis Gannon. A survey of data prove-
nance in e-science. SIGMOD Rec., 34(3):31&#8211;36, September 2005.&lt;/ref&gt;&lt;ref name="IanFosterJensVockler"&gt;Ian Foster, Jens Vockler, Michael Wilde, and Yong Zhao. Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation. In 14th International Conference on Scientific and Statistical Database Management, July 2002.&lt;/ref&gt;&lt;ref name="Benjamim&amp;Luiz"&gt;Benjamin H. Sigelman, Luiz Andr Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, and Chandan Shanbhag. Dapper, a large-scale distributed systems tracing infrastructure. Technical report, Google Inc, 2010.&lt;/ref&gt;&lt;ref name="PeterBuneman"&gt;Peter Buneman, Sanjeev Khanna, and Wang Chiew Tan. Data provenance: Some basic issues. In Proceedings of the 20th Conference on Foundations of SoftwareTechnology and Theoretical Computer Science, FST TCS 2000, pages 87&#8211;93, London, UK, UK, 2000. Springer-Verlag&lt;/ref&gt; These problems will only become larger and more acute as these systems and data continue to grow. As such, more cost-efficient ways of analyzing [[data-intensive computing|data intensive scalable computing]] (DISC) are crucial to their continued effective use.

===Challenges in [[Big Data]] Debugging===

====Massive Scale====
According to an EMC/IDC study:&lt;ref&gt;http://www.emc.com/about/news/press/2012/20121211-01.htm&lt;/ref&gt;
* 2.8ZB of data were created and replicated in 2012,
* the digital universe will double every two years between now and 2020, and
* there will be approximately 5.2TB of data for every man, woman and child on earth in 2020.
Working with this scale of data has become very challenging.

====Unstructured Data====
The phrase [[unstructured data]] usually refers to information that doesn't reside in a traditional row-column database. Unstructured data files often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered "unstructured" because the data they contain doesn't fit neatly in a database.
Experts estimate that 80 to 90 percent of the data in any organization is unstructured. And the amount of unstructured data in enterprises is growing significantly often many times faster than structured databases are growing. "[[Big data]] can include both structured and unstructured data, but IDC estimates that 90 percent of [[big data]] is unstructured data."&lt;ref&gt;Webopedia http://www.webopedia.com/TERM/U/unstructured_data.html&lt;/ref&gt;

====Long Runtime====
In today&#8217;s hyper competitive business environment, companies not only have to find and analyze the relevant data they need, they must find it quickly. The challenge is going through the sheer volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem. Both approaches allow organizations to explore huge data volumes. Even this level of sophisticated hardware and software, few of the image processing tasks in large scale take a few days to few weeks.&lt;ref&gt;SAS. http://www.sas.com/resources/asset/five-big-data-challenges-article.pdf&lt;/ref&gt; Debugging of the data processing is extremely hard due to long run times.

====Complex Platform====
[[Big Data]] platforms have a very complicated structure. Data is distributed among several machines. Typically the jobs are mapped into several machines and results are later combined by reduce operations. Debugging of a [[big data]] pipeline becomes very challenging because of the very nature of the system. It will not be an easy task for the data scientist to figure out which machine's data has the outliers and unknown features causing a particular algorithm to give unexpected results.

====Proposed Solution====
Data provenance or data lineage can be used to make the debugging of [[big data]] pipeline easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail.

==Data Provenance==
Data Provenance provides a historical record of the data and its origins. The provenance of data which is generated by complex transformations such as workflows is of considerable value to scientists. From it, one can ascertain the quality of the data based on its ancestral data and derivations, track back sources of errors, allow automated re-enactment of derivations to update a data, and provide attribution of data sources. Provenance is also essential to the business domain where it can be used to drill down to the source of data in a data warehouse, track the creation of intellectual property, and provide an audit trail for regulatory purposes.

The use of data provenance is proposed in distributed systems to trace records through a dataflow, replay the dataflow on a subset of its original inputs and debug data flows. To do so, one needs to keep track of the set of inputs to each operator, which were used to derive each of its outputs. Although there are several forms of provenance, such as copy-provenance and how-provenance,&lt;ref name="PeterBuneman" /&gt;&lt;ref&gt;Robert Ikeda and Jennifer Widom. Data lineage: A survey. Technical report, Stanford University, 2009.&lt;/ref&gt; the information we need is a simple form of '''why-provenance, or lineage''', as defined by Cui et al.&lt;ref name="YCui"&gt;Y. Cui and J. Widom. Lineage tracing for general data warehouse transformations. VLDB Journal, 12(1), 2003.&lt;/ref&gt;

==Lineage Capture==
Intuitively, for an operator T producing output o, lineage consists of triplets of form {I, T, o}, where I is the set of inputs to T used to derive o. Capturing lineage for each operator T in a dataflow enables users to ask questions such as &#8220;Which outputs were produced by an input i on operator T ?&#8221; and &#8220;Which inputs produced output o in operator T ?&#8221;&lt;ref name="DeSoumyarupa"/&gt; A query that finds the inputs deriving an output is called a backward tracing query, while one that finds the outputs produced by an input is called a forward tracing query.&lt;ref name="RobertIkedaHyunjung"&gt;Robert Ikeda, Hyunjung Park, and Jennifer Widom. Provenance for generalized map and reduce workflows. In Proc. of CIDR, January 2011.&lt;/ref&gt; Backward tracing is useful for debugging, while forward tracing is useful for tracking error propagation.&lt;ref name="RobertIkedaHyunjung" /&gt; Tracing queries also form the basis for replaying an original dataflow.&lt;ref name="IanFosterJensVockler" /&gt;&lt;ref name="YCui" /&gt;&lt;ref name="RobertIkedaHyunjung" /&gt; However, to efficiently use lineage in a DISC system, we need to be able to capture lineage at multiple levels (or granularities) of operators and data, capture accurate lineage for DISC processing constructs and be able to trace through multiple dataflow stages efficiently.

DISC system consists of several levels of operators and data, and different use cases of lineage can dictate the level at which lineage needs to be captured.  Lineage can be captured at the level of the job, using files and giving lineage tuples of form {IF i, M RJob, OF i }, lineage can also be captured at the level of each task, using records and giving, for example, lineage tuples of form {(k rr, v rr ), map, (k m, v m )}. The first form of lineage is called coarse-grain lineage, while the second form is called fine-grain lineage. Integrating lineage across different granularities enables users to ask questions such as &#8220;Which file read by a MapReduce job produced this particular output record?&#8221; and can be useful in debugging across different operator and data granularities within a dataflow.&lt;ref name="DeSoumyarupa" /&gt;
[[File:Map Reduce Job -1.png|thumb|center|500px|Map Reduce Job showing containment relationships]]

To capture end-to-end lineage in a DISC system, we use the Ibis model,&lt;ref&gt;C. Olston and A. Das Sarma. Ibis: A provenance manager for multi-layer
systems. In Proc. of CIDR, January 2011.&lt;/ref&gt; which introduces the notion of containment hierarchies for operators and data. Specifically, Ibis proposes that an operator can be contained within another and such a relationship between two operators is called '''operator containment'''. "Operator containment implies that the contained (or child) operator performs a part of the logical operation of the containing (or parent) operator."&lt;ref name="DeSoumyarupa" /&gt; For example, a MapReduce task is contained in a job. Similar containment relationships exist for data as well, called data containment. Data containment implies that the contained data is a subset of the containing data (superset).
[[File:Containment Hierarchy.png|thumb|center|500px|Containment Hierarchy]]

==Prescriptive Data Lineage==

The concept of '''Prescriptive Data Lineage''' combines both the logical model (entity) of how that data should flow with the actual lineage for that instance.&lt;ref&gt;http://info.hortonworks.com/rs/549-QAL-086/images/Hadoop-Governance-White-Paper.pdf&lt;/ref&gt;

Data lineage and provenance typically refers to the way or the steps a dataset came to its current state Data lineage, as well as all copies or derivatives. However, simply looking back at only audit or log correlations to determine lineage from a forensic point of view is flawed for certain data management cases.  For instance, it is impossible to determine with certainty if the route a data workflow took was correct or in compliance without the logic model.

Only by combining the a logical model with atomic forensic events can proper activities be validated:
#Authorized copies, joins, or CTAS operations
#Mapping of processing to the systems that those process are run on
#Ad-Hoc versus established processing sequences

Many certified compliance reports require provenance of data flow as well as the end state data for a specific instance. With these types of situations, any deviation from the prescribed path need to be accounted for and potentially remediated.&lt;ref&gt;[https://www.sec.gov/info/smallbus/secg/bd-small-entity-compliance-guide.htm SEC Small Entity Compliance Guide]&lt;/ref&gt;   This is marks a shift in thinking from purely a look back model  to a framework which is better suited to capture compliance workflows.

==Active vs Lazy Lineage==
Lazy lineage collection typically captures only coarse-grain lineage at run time. These systems incur low capture overheads due to the small amount of lineage they capture. However, to answer fine-grain tracing queries, they must replay the data flow on all (or a large part) of its input and collect fine-grain lineage during the replay. This approach is suitable for forensic systems, where a user wants to debug an observed bad output.

Active collection systems capture entire lineage of the data flow at run time. The kind of lineage they capture may be coarse-grain or fine-grain, but they do
not require any further computations on the data flow after its execution. Active fine-grain lineage collection systems incur higher capture overheads than lazy collection systems. However, they enable sophisticated replay and debugging.&lt;ref name="DeSoumyarupa" /&gt;

==Actors==
An actor is an entity that transforms data; it may be a Dryad vertex, individual map and reduce operators, a MapReduce job, or an entire dataflow pipeline. Actors act as black-boxes and the inputs and outputs of an actor are tapped to capture lineage in the form of associations, where an association is a triplet {i, T, o} that relates an input i with an output o for an actor T . The instrumentation thus captures lineage in a dataflow one actor at a time, piecing it into a set of associations for each actor. The system developer needs to capture the data an actor reads (from other actors) and the data an actor writes (to other actors). For example, a developer can treat the Hadoop Job Tracker as an actor by recording the set of files read and written by each job.
&lt;ref name="mainPaper"&gt;Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable lineage capture for debugging DISC analytics. In Proceedings of the 4th annual Symposium on Cloud Computing (SOCC '13). ACM, New York, NY, USA, , Article 17 , 15 pages.&lt;/ref&gt;

==Associations==
Association is a combination of the inputs, outputs and the operation itself. The operation is represented in terms of a black box also known as the actor. The associations describe the transformations that are applied on the data. The associations are stored in the association tables. Each unique actor is represented by its own association table. An association itself looks like {i, T, o} where i is the set of inputs to the actor T and o is set of outputs given produced by the actor. Associations are the basic units of Data Lineage. Individual associations are later clubbed together to construct the entire history of transformations that were applied to the data.&lt;ref name="DeSoumyarupa"/&gt;

==Architecture==
[[Big data]] systems scale horizontally i.e. increase capacity by adding new hardware or software entities into the distributed system. The distributed system acts as a single entity in the logical level even though it comprises multiple hardware and software entities. The system should continue to maintain this property after horizontal scaling. An important advantage of horizontal scalability is that it can provide the ability to increase capacity on the fly. The biggest plus point is that horizontal scaling can be done using commodity hardware.

The horizontal scaling feature of [[Big Data]] systems should be taken into account while creating the architecture of lineage store. This is essential because the lineage store itself should also be able to scale in parallel with the [[Big data]] system. The number of associations and amount of storage required to store lineage will increase with the increase in size and capacity of the system. The architecture of [[Big data]] systems makes the use of a single lineage store not appropriate and impossible to scale. The immediate solution to this problem is to distribute the lineage store itself.&lt;ref name="DeSoumyarupa"/&gt;

The best case scenario is to use a local lineage store for every machine in the distributed system network. This allows the lineage store also to scale horizontally. In this design, the lineage of data transformations applied to the data on a particular machine is stored on the local lineage store of that specific machine. The lineage store typically stores association tables. Each actor is represented by its own association table. The rows are the associations themselves and columns represent inputs and outputs. This design solves 2 problems. It allows horizontal scaling of the lineage store. If a single centralized lineage store was used, then this information had to be carried over the network, which would cause additional network latency. The network latency is also avoided by the use of a distributed lineage store.&lt;ref name="mainPaper"/&gt;

[[File:Selection 065.png|thumb|center|500px|Architecture of Lineage Systems]]

==Data flow Reconstruction==
The information stored in terms of associations needs to be combined by some means to get the data flow of a particular job. In a distributed system a job is broken down into multiple tasks. One or more instances run a particular task. The results produced on these individual machines are later combined together to finish the job. Tasks running on different machines perform multiple transformations on the data in the machine. All the transformations applied to the data on a machines is stored in the local lineage store of that machines. This information needs to be combined together to get the lineage of the entire job. The lineage of the entire job should help the data scientist understand the data flow of the job and he/she can use the data flow to debug the [[big data]] pipeline. The data flow is reconstructed in 3 stages.

===Association tables===
The first stage of the data flow reconstruction is the computation of the association tables. The association tables exists for each actor in each local lineage store. The entire association table for an actor can be computed by combining these individual association tables. This is generally done using a series of equality joins based on the actors themselves. In few scenarios the tables might also be joined using inputs as the key. Indexes can also be used to improve the efficiency of a join.The joined tables need to be stored on a single instance or a machine to further continue processing. There are multiple schemes that are used to pick a machine where a join would be computed. The easiest one being the one with minimum CPU load. Space constraints should also be kept in mind while picking the instance where join would happen.

===Association Graph===
The second step in data flow reconstruction is computing an association graph from the lineage information. The graph represents the steps in the data flow. The actors act as vertices and the associations act as edges. Each actor T is linked to its upstream and downstream actors in the data flow. An upstream actor of T is one that produced the input of T, while a downstream actor is one that consumes the output of T . Containment relationships are always considered while creating the links. The graph consists of three types of links or edges.

====Explicitly specified links====
The simplest link is an explicitly specified link between two actors. These links are explicitly specified in the code of a machine learning algorithm. When an actor is aware of its exact upstream or downstream actor, it can communicate this information to lineage API. This information is later used to link these actors during the tracing query. For example, in the [[MapReduce]] architecture, each map instance knows the exact record reader instance whose output it consumes.&lt;ref name="DeSoumyarupa"/&gt;

====Logically inferred links====
Developers can attach data flow [[archetypes]] to each logical actor. A data flow archetype explains how the children types of an actor type arrange themselves in a data flow. With the help of this information, one can infer a link between each actor of a source type and a destination type. For example, in the [[MapReduce]] architecture, the map actor type is the source for reduce, and vice versa. The system infers this from the data flow archetypes and duly links map instances with reduce instances. However, there may be several [[MapReduce]] jobs in the data flow, and linking all map instances with all reduce instances can create false links. To prevent this, such links are restricted to actor instances contained within a common actor instance of a containing (or parent) actor type. Thus, map and reduce instances are only linked to each other if they belong to the same job.&lt;ref name="DeSoumyarupa"/&gt;

====Implicit links through data set sharing====
In distributed systems, sometimes there are implicit links, which are not specified during execution. For example, an implicit link exists between an actor that wrote to a file and another actor that read from it. Such links connect actors which use a common data set for execution. The dataset is the output of the first actor and is the input of the actor following it.&lt;ref name="DeSoumyarupa"/&gt;

===Topological Sorting===
The final step in the data flow reconstruction is the [[Topological sorting]] of the association graph. The directed graph created in the previous step is topologically sorted to obtain the order in which the actors have modified the data. This inherit order of the actors defines the data flow of the big data pipeline or task.

==Tracing &amp; Replay==
This is the most crucial step in [[Big Data]] debugging. The  captured lineage is combined and processed to obtain the data flow of the pipeline. The data flow helps the data  scientist or a developer to look deeply into the actors and their transformations. This step allows the data scientist to figure out the part of the algorithm that is generating the unexpected output. A [[big data]] pipeline can go wrong in 2 broad ways. The first is a presence of a suspicious actor in the data-flow. The second being the existence of outliers in the data.

The first case can be debugged by tracing the data-flow. By using lineage and data-flow information together a data scientist can figure out how the inputs are converted into outputs. During the process actors that behave unexpectedly can be caught. Either these actors can be removed from the data flow or they can be augmented by new actors to change the data-flow. The improved data-flow can be replayed to test the validity of it. Debugging faulty actors include recursively performing coarse-grain replay on actors in the data-flow,&lt;ref&gt;Wenchao Zhou, Qiong Fei, Arjun Narayan, Andreas Haeberlen, Boon Thau Loo, and Micah Sherr. Secure network provenance. In Proceedings of 23rd ACM Symposium on Operating System Principles (SOSP), December 2011.&lt;/ref&gt; which can be expensive in resources for long dataflows. Another approach is to manually inspect lineage logs to find anomalies,&lt;ref name="Benjamim&amp;Luiz" /&gt;&lt;ref&gt;Rodrigo Fonseca, George Porter, Randy H. Katz, Scott Shenker, and Ion Stoica. X-trace: A pervasive network tracing framework. In In Proceedings of NSDI&#8217;07, 2007.&lt;/ref&gt; which can be tedious and time-consuming across several stages of a data-flow. Furthermore, these approaches work only when the data scientist can discover bad outputs. To debug analytics without known bad outputs, the data scientist need to analyze the data-flow for suspicious behavior in general. However, often, a user may not know the expected normal behavior and cannot specify predicates. This section describes a debugging methodology for retrospectively analyzing lineage to identify faulty actors in a multi-stage data-flow. We believe that sudden changes in an actor&#8217;s behavior, such as its average selectivity, processing rate or output size, is characteristic of an anomaly. Lineage can reflect such changes in actor behavior over time and across different actor instances. Thus, mining lineage to identify such changes can be useful in debugging faulty actors in a data-flow.
[[File:Tracing Anomalous Actors.png|thumb|center|400px|Tracing Anomalous Actors]]

The second problem i.e. the existence of outliers can also be identified by running the data-flow step wise and looking at the transformed outputs. The data scientist finds a subset of outputs that are not in accordance to the rest of outputs. The inputs which are causing these bad outputs are the outliers in the data. This problem can be solved by removing the set of outliers from the data and replaying the entire data-flow. It can also be solved by modifying the machine learning algorithm by adding, removing or moving actors in the data-flow. The changes in the data-flow are successful if the replayed data-flow does not produce bad outputs.
[[File:Tracing Outliers in the data.png|thumb|center|400px|Tracing Outliers in the data]]

==Challenges==
Even though use data lineage is a novel way of debugging of [[big data]] pipelines, the process is not simple. The challenges are scalability of lineage store, fault tolerance of the lineage store, accurate capture of lineage for black box operators and many others. These challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture.

===Scalability===
DISC systems are primarily batch processing systems designed for high throughput. They execute several jobs per analytics, with several tasks per job. The overall number of operators executing at any time in a cluster can range from hundreds to thousands depending on the cluster size. Lineage capture for
these systems must be able scale to both large volumes of data and numerous operators to avoid being a bottleneck for the DISC analytics.

===Fault tolerance===
Lineage capture systems must also be fault tolerant to avoid rerunning data flows to capture lineage. At the same time, they must also accommodate failures in the DISC system. To do so, they must be able to identify a failed DISC task and avoid storing duplicate copies of lineage between the partial lineage generated by the failed task and duplicate lineage produced by the restarted task. A lineage system should also be able to gracefully handle multiple instances of local lineage systems going down. This can achieved by storing replicas of lineage associations in multiple machines. The replica can act like a backup in the event of the real copy being lost.

===Black-box operators===
Lineage systems for DISC dataflows must be able to capture accurate lineage across black-box operators to enable fine-grain debugging. Current approaches to this include Prober, which seeks to find the minimal set of inputs that can produce a specified output for a black-box operator by replaying the data-flow several times to deduce the minimal set,&lt;ref&gt;Anish Das Sarma, Alpa Jain, and Philip Bohannon. PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines. Technical report, Yahoo, April 2010.&lt;/ref&gt; and dynamic slicing, as used by Zhang et al.&lt;ref&gt;Mingwu Zhang, Xiangyu Zhang, Xiang Zhang, and Sunil Prabhakar. Tracing lineage beyond relational operators. In Proc. Conference on Very Large Data Bases (VLDB), September 2007.&lt;/ref&gt; to capture lineage for [[NoSQL]] operators through binary rewriting to compute dynamic slices. Although producing highly accurate lineage, such techniques can incur significant time overheads for capture or tracing, and it may be preferable to instead trade some accuracy for better performance. Thus, there is a need for a lineage collection system for DISC dataflows that can capture lineage from arbitrary operators with reasonable accuracy, and without significant overheads in capture or tracing.

===Efficient tracing===
Tracing is essential for debugging, during which, a user can issue multiple tracing queries. Thus, it is important that tracing has fast turnaround times. Ikeda et al.&lt;ref name="RobertIkedaHyunjung" /&gt; can perform efficient backward tracing queries for MapReduce dataflows, but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick,&lt;ref&gt;Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, and Julia Stoyanovich. Putting lipstick on a pig: Enabling database-style workflow provenance. In Proc. of VLDB, August 2011.&lt;/ref&gt; a lineage system for Pig,&lt;ref&gt;Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: A not-so-foreign language for data processing. In Proc. of ACM SIGMOD, Vancouver, Canada, June 2008.&lt;/ref&gt; while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, there is a need for a lineage system that enables efficient forward and backward tracing for generic DISC systems and dataflows with black-box operators.

===Sophisticated replay===
Replaying only specific inputs or portions of a data-flow is crucial for efficient debugging and simulating what-if scenarios. Ikeda et al. present a methodology for lineage-based refresh, which selectively replays updated inputs to recompute affected outputs.&lt;ref&gt;Robert Ikeda, Semih Salihoglu, and Jennifer Widom. Provenance-based refresh in data-oriented workflows. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM &#8217;11, pages 1659&#8211;1668, New York, NY, USA, 2011. ACM.&lt;/ref&gt; This is useful during debugging for re-computing outputs when a bad input has been fixed. However, sometimes a user may want to remove the bad input and replay the lineage of outputs previously affected by the error to produce error-free outputs. We call this exclusive replay. Another use of replay in debugging involves replaying bad inputs for step-wise debugging (called selective replay). Current approaches to using lineage in DISC systems do not address these. Thus, there is a need for a lineage system that can perform both exclusive and selective replays to address different debugging needs.

===Anomaly detection===
One of the primary debugging concerns in DISC systems is identifying faulty operators. In long dataflows with several hundreds of operators or tasks, manual inspection can be tedious and prohibitive. Even if lineage is used to narrow the subset of operators to examine, the lineage of a single output can still span several operators. There is a need for an inexpensive automated debugging system, which can substantially narrow the set of potentially faulty operators, with reasonable accuracy, to minimize the amount of manual examination required.

==See also==
&lt;!-- please do not list specific implementations here --&gt;
* [[Provenance]]
* [[Big Data]]
* [[Topological Sorting]]
* [[Debugging]]
* [[NoSQL]]
* [[Scalability]]
* [[Directed acyclic graph]]

==References==
{{Reflist|33em}}

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Big data]]</text>
      <sha1>tb0lw1nkqukwtx7unlwg9usuakcxbnd</sha1>
    </revision>
  </page>
  <page>
    <title>Open Compute Project</title>
    <ns>0</ns>
    <id>31547791</id>
    <revision>
      <id>747591190</id>
      <parentid>747414885</parentid>
      <timestamp>2016-11-03T06:33:49Z</timestamp>
      <contributor>
        <username>Wikideas1</username>
        <id>20468248</id>
      </contributor>
      <minor />
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16176" xml:space="preserve">{{Infobox Organization
|name           = Open Compute Project
|image          = OpenCompute logo.jpg
|mcaption       = 
|formation      = 2011
|type           = Industry trade group
|purpose        = Sharing designs of [[data center]] products
|headquarters   = 
|membership     = 
|website        = {{URL|opencompute.org}}
|remarks        =
}}
[[File:Open Compute Server Front.jpg|thumb|Open Compute V2 Server]]
[[File:Open Compute 1U Drive Tray Bent.jpg|thumb|Open Compute V2 Drive Tray,&lt;br /&gt;2nd lower tray extended]]
The '''Open Compute Project''' ('''OCP''') is an organization that shares designs of [[data center]] products among companies, including [[Facebook]], [[Intel]], [[Nokia]], [[Google]], [[Apple Inc.|Apple]], [[Microsoft]], [[Seagate Technology]], [[Dell]], [[Rackspace]], [[Ericsson]],  [[Cisco]], [[Juniper Networks]], [[Goldman Sachs]], [[Fidelity Investments|Fidelity]], [[Lenovo]] and [[Bank of America]].&lt;ref&gt;{{cite web|url=http://www.wired.com/2015/03/facebook-got-even-apple-back-open-source-hardware/|title=How Facebook Changed the Basic Tech That Runs the Internet|date=11 Apr 2015}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.opencompute.org/about/ocp-incubation-committee/|title=Incubation Committee|website=Open Compute|access-date=2016-08-19}}&lt;/ref&gt;

The Open Compute Project's mission is to design and enable the delivery of the most efficient server, storage and data center hardware designs for scalable computing. "We believe that openly sharing ideas, specifications and other intellectual property is the key to maximizing innovation and reducing operational complexity in the scalable computing space."&lt;ref&gt;{{cite web|url=http://www.opencompute.org/about/mission-and-principles/|title=Mission and Principles|website = Open Compute|accessdate = 2016-05-13}}&lt;/ref&gt;&lt;br /&gt;
All Facebook Data Centers are 100% OCP: Prineville Data Center, Forest City Data Center, Altoona Data Center, Lule&#229; Data Center (Sweden).
Facebook Data Centers under construction: Fort Worth Data Center, Clonee Data Center (Ireland).&lt;ref&gt;{{cite web|url=http://uk.businessinsider.com/facebook-eu-data-center-open-compute-project-2016-1|first=Matt|last=Weinberger|title=Facebook's newest data center is going to make some big tech companies very nervous|website=Open Compute|date=January 25, 2016|accessdate = 2016-05-16}}&lt;/ref&gt;

==Details==
The initiative was announced in April 2011 by Jonathan Heiliger&lt;ref&gt;{{cite news|last1 = Heiliger|first1 = Jonathan|title = Why I Started the Open Compute Project|url = http://www.vertexventures.com/2015/06/why-i-started-the-open-compute-project/|accessdate = 18 June 2015|date = 2015-06-15}}&lt;/ref&gt; at [[Facebook]] to openly share designs of [[data center]] products.&lt;ref&gt;{{cite web |url= http://www.datacenterknowledge.com/archives/2011/04/14/will-open-compute-alter-the-data-center-market/ |title=Will Open Compute Alter the Data Center Market? |date=April 14, 2011 |first= Rich|last= Miller |work= Data Center Knowledge |accessdate= July 9, 2013 }}&lt;/ref&gt;
The effort came out of a redesign of [[Facebook]]'s data center in [[Prineville, Oregon]].&lt;ref&gt;{{Cite web |url= http://www.facebook.com/notes/facebook-engineering/building-efficient-data-centers-with-the-open-compute-project/10150144039563920 |title= Building Efficient Data Centers with the Open Compute Project |first= Jonathan|last= Heiliger |date= April 7, 2011 |work= Facebook Engineering's notes |accessdate= July 9, 2013 }}&lt;/ref&gt;
After two years, with regards to a more module server design, it was admitted that "the new design is still a long way from live data centers".&lt;ref&gt;{{Cite news |title= Facebook Shatters the Computer Server Into Tiny Pieces |date= January 16, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/01/facebook-server-pieces/ |accessdate= July 9, 2013 }}&lt;/ref&gt;
However, some aspects published were used in the Prineville center to improve the energy efficiency, as measured by the [[power usage effectiveness]] index defined by [[The Green Grid]].&lt;ref name="Stanford"&gt;{{Cite web |title= Facebook's Open Compute Project |work= Stanford EE Computer Systems Colloquium |date= February 15, 2012  |url= http://www.stanford.edu/class/ee380/Abstracts/120215.html |first= Amir|last= Michael |publisher= [[Stanford University]]}}  ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=120215-ee380-300.asx video archive])&lt;/ref&gt;

The Open Compute Project Foundation is a 501(c)(6) non-profit incorporated in the state of Delaware. Corey Bell serves as the Foundation's CEO. Currently there are 7 members who serve on board of directors which is made up of two individual members and five organizational members.  Jason Taylor ([[Facebook]]) is the Foundation's president and chairman. Frank Frankovsky (formerly of Facebook and past president and chairman) and  [[Andy Bechtolsheim]] are the two individual members.  In addition to Jason Taylor who represents [[Facebook]], other organizations on the Open Compute board of directors include [[Intel]] (Jason Waxman), [[Goldman Sachs]] (Don Duet), [[Rackspace]] (Mark Roenick), and [[Microsoft]] (Bill Laing).&lt;ref&gt;{{Cite web|title = Organization and Board|url = http://www.opencompute.org/about/organization-and-board/|website = Open Compute|accessdate = 2015-09-12}}&lt;/ref&gt;

On March 11, 2015 [[Apple Inc.|Apple]], [[Cisco]] and [[Juniper Networks]] joined the project.&lt;ref&gt;{{Cite web |title= Open Compute: Apple, Cisco Join While HP Expands |first= Charles|last= Babcock |date= March 11, 2015 |url=http://www.informationweek.com/cloud/infrastructure-as-a-service/open-compute-apple-cisco-join-while-hp-expands/d/d-id/1319421  |accessdate= March 11, 2015 }}&lt;/ref&gt;

On November 16, 2015 [[Nokia]] joined the project.&lt;ref&gt;{{Cite web |title= Nokia Networks joins Open Compute Project to advance its AirFrame Data Center Solution|date= November 16, 2015 |url=http://company.nokia.com/en/news/press-releases/2015/11/16/nokia-networks-joins-open-compute-project-to-advance-its-airframe-data-center-solution}}&lt;/ref&gt;

On February 23, 2016 [[Lenovo]] joined the project.&lt;ref&gt;{{Cite web |title= Lenovo joins Open Compute Project |date= February 23, 2016 |url=http://news.lenovo.com/blog/lenovo-joins-open-compute-projects.htm }}&lt;/ref&gt;

On March 9, 2016 [[Google]] joined the project.&lt;ref&gt;{{Cite web |title= Google joins the Open Compute Project |date= March 9, 2016 |url=http://techcrunch.com/2016/03/09/google-joins-the-open-compute-project/ }}&lt;/ref&gt;

Components of the Open Compute Project include:

* Server compute nodes included one for [[Intel]] processors and one for [[Advanced Micro Devices|AMD]] processors. In 2013, [[Calxeda]] contributed a design with [[ARM architecture]] processors.&lt;ref&gt;{{Cite web |title= ARM Server Motherboard Design for Open Vault Chassis Hardware v0.3 MB-draco-hesperides-0.3 |first= Tom|last= Schnell |date= January 16, 2013 |url=http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_ARM_Server_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;Several generations of server designs have been deployed. So far being: Freedom (Intel), Spitfire (AMD), Windmill (Intel E5-2600), Watermark (AMD), Winterfell (Intel E5-2600 v2) and Leopard (Intel E5-2600 v3)&lt;ref&gt;{{Cite web |title=Guide to Facebook&#8217;s Open Source Data Center Hardware
|author=Data Center Knowledge|date=April 28, 2016|url=http://www.datacenterknowledge.com/archives/2016/04/28/guide-to-facebooks-open-source-data-center-hardware/|accessdate=May 13, 2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=Facebook rolls out new web and database server designs|first=The|last=Register|date=January 17, 2013|url=http://www.theregister.co.uk/2013/01/17/open_compute_facebook_servers/|accessdate=May 13, 2016}}&lt;/ref&gt;

* Open Vault storage building blocks offer high disk densities, with 30 drives in a 2U Open Rack chassis designed for easy [[disk drive]] replacement. The 3.5 inch disks are stored in two drawers, five across and three deep in each drawer, with connections via [[serial attached SCSI]].&lt;ref&gt;{{Cite web |title= Open Vault Storage Hardware V0.7 OR-draco-bueana-0.7 |author= Mike Yan and Jon Ehlen |date= January 16, 2013 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Open_Vault_Storage_Specification_v0.7.pdf |accessdate= July 9, 2013 }}&lt;/ref&gt; This storage is also called Knox, there is also a cold storage variant where the disks power down if not used to save energy consumption.&lt;ref&gt;{{Cite web |title=Under the hood: Facebook&#8217;s cold storage system|date=May 4, 2015|url=https://code.facebook.com/posts/1433093613662262/-under-the-hood-facebook-s-cold-storage-system-/|accessdate=May 13, 2016}}&lt;/ref&gt; Another design concept was contributed by Hyve Solutions, a division of [[Synnex]] in 2012.&lt;ref&gt;{{Cite web |title= Hyve Solutions Contributes Storage Design Concept to OCP Community |work= News release |date= January 17, 2013 |url= http://ir.synnex.com/releasedetail.cfm?ReleaseID=733922 |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title= Torpedo Design Concept Storage Server for Open Rack Hardware v0.3 ST-draco-chimera-0.3 |first= Conor|last= Malone |date= January 15, 2012 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Storage_Server_for_Open_Rack_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;At the OCP Summit 2016 Facebook together with Taiwanese ODM Wistron's spin-off Wiwynn introduced Lightning, a flexible NVMe JBOF (just a bunch of flash), based on the existing Open Vault (Knox) design.&lt;ref&gt;{{Cite web |title=Introducing Lightning: A flexible NVMe JBOF|first=Chris|last=Petersen|date=March 9, 2016|url=https://code.facebook.com/posts/989638804458007/introducing-lightning-a-flexible-nvme-jbof/|accessdate= May 13, 2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=Wiwynn Showcases All-Flash Storage Product with Leading-edge NVMe Technology|date=March 9, 2016|url=http://www.wiwynn.com/english/company/newsinfo/23|accessdate= May 13, 2016}}&lt;/ref&gt;
* Mechanical mounting system: Open racks have the same outside width (600&amp;nbsp;mm) and depth as standard [[19-inch rack]]s, but are designed to mount wider chassis with a 537&amp;nbsp;mm width (about 21 inches). This allows more equipment to fit in the same volume and improves air flow. Compute chassis sizes are defined in multiples of an OpenU, which is 48&amp;nbsp;mm, slightly larger than the typical [[rack unit]].
* Data center designs for energy efficiency, include 277 VAC power distribution that eliminates one transformer stage in typical data centers. A single voltage (12.5 VDC) power supply designed to work with 277 VAC input and 48 VDC battery backup.&lt;ref name="Stanford" /&gt;
* On May 8, 2013, an effort to define an open [[network switch]] was announced.&lt;ref&gt;{{Cite web |title= Up next for the Open Compute Project: The Network |date= May 8, 2013 |author= Jay Hauser for Frank Frankovsky |work= Open Compute blog |url= http://www.opencompute.org/blog/up-next-for-the-open-compute-project-the-network/ |accessdate= June 20, 2014 }}&lt;/ref&gt; The plan was to allow Facebook to load its own [[operating system]] software onto the switch. Press reports predicted that more expensive and higher-performance switches would continue to be popular, while less expensive products treated more like a [[commodity]] (using the [[buzzword]] "top-of-rack") might adopt the proposal.&lt;ref&gt;{{Cite news |title= Can Open Compute change network switching? |first= David|last= Chernicoff |work= ZDNet |date= May 9, 2013 |url= http://www.zdnet.com/can-open-compute-change-network-switching-7000015141/ |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;A similar project for a custom switch for the [[Google platform]] had been rumored, and evolved to use the [[OpenFlow]] protocol.&lt;ref&gt;{{Cite news |title= Facebook Rattles Networking World With &#8216;Open Source&#8217; Gear |date= May 8, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/05/facebook_networking/ |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Going With the Flow: Google&#8217;s Secret Switch to the Next Wave of Networking |date= April 17, 2012 |first= Steven|last= Levy |work= Wired |url= http://www.wired.com/wiredenterprise/2012/04/going-with-the-flow-google/ |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;The first switch Open Sourced by Facebook was designed together with Taiwanese ODM Accton using Broadcom Trident II chip and is called Wedge, the Linux OS that it runs is called FBOSS.&lt;ref&gt;{{cite web|url=https://code.facebook.com/posts/681382905244727/introducing-wedge-and-fboss-the-next-steps-toward-a-disaggregated-network/|title=Introducing "Wedge" and "FBOSS," the next steps toward a disaggregated network|website =Meet the engineers who code Facebook|date=June 18, 2014|accessdate = 2016-05-13}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://code.facebook.com/posts/843620439027582/facebook-open-switching-system-fboss-and-wedge-in-the-open/|title=Facebook Open Switching System ("FBOSS") and Wedge in the open|website=Meet the engineers who code Facebook|date=March 10, 2015|accessdate = 2016-05-13}}&lt;/ref&gt; Later switch contributions include "6-pack" and Wedge-100, based on Broadcom Tomahawk chips.&lt;ref&gt;{{cite web|url=https://code.facebook.com/posts/203733993317833/opening-designs-for-6-pack-and-wedge-100/|title=Opening designs for 6-pack and Wedge 100|website=Meet the engineers who code Facebook|date=March 9, 2016|accessdate = 2016-05-13}}&lt;/ref&gt; Similar switch hardware designs have been contributed by: Edge-Core Networks Corporation (Accton spin-off), Mellanox Technologies, Interface Masters Technologies, Agema Systems.&lt;ref&gt;{{cite web|url=http://www.opencompute.org/wiki/Networking/SpecsAndDesigns|title=Accepted or shared hardware specifications|website=Open Compute|accessdate = 2016-05-13}}&lt;/ref&gt; Capable of running ONIE compatible Operating Systems such as Cumulus Linux, Big Switch or Pica8.&lt;ref&gt;{{cite web|url=http://www.opencompute.org/wiki/Networking/ONIE/NOS_Status|title=Current Network Operating System (NOS) List|website=Open Compute|accessdate = 2016-05-13}}&lt;/ref&gt;

== Providers ==
The promoted vendors include:&lt;ref&gt;[http://www.opencompute.org/about/open-compute-project-solution-providers/ open compute project solution providers]&lt;/ref&gt;
* [[AMAX Information Technologies]]
* Circle B
* [[Itochu Techno-Solutions]] (CTC)
* [[Hewlett Packard Enterprise]]
* Hyperscale IT
* [[Synnex|Hyve Solutions]]
* [[Penguin Computing]]
* [[Nokia]]
* [[Quanta Computer]]
* Racklive
* Stack Velocity
* Wiwynn

== See also ==
* [[Novena (computing platform)]]
* [[Open-source computing hardware]]
* [[OpenPOWER Foundation]]
* [[Telecom Infra Project]]  - [[Facebook]] sister project focusing on [[Optical networking|Optical]] [[broadband networks]] and open [[cellular network|cellular networks]]

== References ==
{{reflist|33em}}

== External links ==
{{Commons category|Data Centers}}
* {{Official website|http://opencompute.org/}}
* [https://www.facebook.com/PrinevilleDataCenter/ Prineville Data Center]
* [https://www.facebook.com/ForestCityDataCenter/ Forest City Data Center]
* [https://www.facebook.com/AltoonaDataCenter/ Altoona Data Center]
* [https://www.facebook.com/LuleaDataCenter/ Lule&#229; Data Center (Sweden)]
* [https://www.facebook.com/FortWorthDataCenter/ Fort Worth Data Center]
* [https://www.facebook.com/CloneeDataCenter/ Clonee Data Center (Ireland)]
* Videos
** {{youtube|2hTfzUmdAOw|HC23-T2: The Open Compute Project}}, Hot Chips 23, 2011 2.5 Hour Tutorial
** {{youtube|QtTF9pDQxPc|Facebook Open Compute Server}}, Facebook V1 Open Compute Server
** {{youtube|ckNzwqhDS60|Facebook V2 Windmill Server}}
** {{youtube|GbzQe3jO4hc|Hyve: Adapting Facebook's Servers for Your Data Center}}, Open Compute starts at 5:40

{{Facebook navbox|state=collapsed}}

[[Category:Open-source hardware]]
[[Category:Facebook]]
[[Category:2011 software]]
[[Category:Data centers]]
[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
[[Category:Applications of distributed computing]]
[[Category:Cloud storage]]
[[Category:Computer networking]]
[[Category:Science and technology in the San Francisco Bay Area]]</text>
      <sha1>etkvayg2tdstx0ujgt2sau4ujoi08gk</sha1>
    </revision>
  </page>
  <page>
    <title>Author Name Disambiguation</title>
    <ns>0</ns>
    <id>46474403</id>
    <revision>
      <id>718915217</id>
      <parentid>674329228</parentid>
      <timestamp>2016-05-06T12:04:02Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor />
      <comment>/* top */clean up, added [[CAT:O|orphan]], [[CAT:UL|underlinked]] tags using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1672" xml:space="preserve">{{Multiple issues|
{{Underlinked|date=May 2016}}
{{Orphan|date=May 2016}}
{{refimprove|date=April 2015}}
}}

'''Author name disambiguation''' is a type of [[Record linkage]] that is applied to scholarly documents where the goal is to find all mentions of the same author and cluster them together. Authors of scholarly documents often share names which makes it hard to distinguish each author's work. Hence, author name disambiguation aims to find all publications that belong to a given author and distinguish them from publications of other authors who share the same name.

There are multiple reasons that cause author names to be ambiguous, among which: individuals may publish under multiple names for variety of reasons including different spelling, misspelling, name change due to marriage, or the use of middle names and initials.&lt;ref&gt;{{cite journal
 | authorlink = Smalheiser, Neil R and Torvik, Vetle I
 | title = Author name disambiguation
 | journal = [[Annual Review of Information Science and Technology]]
 | url = http://onlinelibrary.wiley.com/doi/10.1002/aris.2009.1440430113/full
 | accessdate = 2015-04-20
 | doi = 10.1002/aris.2009.1440430113
}}&lt;/ref&gt;

Typical approach for author name disambiguation rely on information about the authors such as their affiliations, email addresses, year of publication, co-authors, topic information to distinguish between authors. These information can be used to learn a machine learning classifier that decides whether two mentions refer to the same author or not. Other approaches utilized heuristics to distinguish between authors.

==References==
{{Reflist}}

[[Category:Metadata]]
[[Category:Data management]]</text>
      <sha1>omckuj5zcx4m8l7hzdmcqy3kt2rbmdj</sha1>
    </revision>
  </page>
  <page>
    <title>Cleo (company)</title>
    <ns>0</ns>
    <id>42965868</id>
    <revision>
      <id>751529860</id>
      <parentid>714068884</parentid>
      <timestamp>2016-11-26T08:25:42Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10388" xml:space="preserve">{{Infobox company
| logo             = [[File:Cleo (company) logo, 2014.png]]
| name             = Cleo
| type             = [[Privately held company]]
| foundation       =  1976
| location_city    = [[Loves Park, Illinois]]
| location_country = [[United States of America]]
| key_people       =
Mahesh Rajasekharan &lt;small&gt;([[Chief executive officer|CEO]])&lt;/small&gt;&lt;br/&gt;Sumit Garg &lt;small&gt;(President)&lt;/small&gt;
| num_employees    = 200+
| industry         = [[Managed file transfer]], data integration, [[network management]] and secure file sharing
| homepage         = {{url|http://cleo.com}}
}}

'''Cleo''' is an [[enterprise software]] company that provides [[electronic data interchange]] (EDI), and application-to-application (A2A), [[business-to-business]] (B2B), and [[big data]] integration services to organizations with [[managed file transfer]] needs. The company, formerly known as Cleo Communications, was founded in 1976. Cleo was acquired by investment firm Globe Equity Partners in 2012. Mahesh Rajasekharan is Cleo's [[CEO]], and Sumit Garg serves as Cleo's president.&lt;ref&gt;{{cite web|author=Alex Gary |url=http://www.rrstar.com/x1364621329/Private-equity-firm-acquires-Loves-Park-company |title=Private equity firm acquires Loves Park company - Blogs - Rockford Register Star |publisher=Rrstar.com |date= |accessdate=2014-06-05}}&lt;/ref&gt;

== Business ==
Cleo originally began as a division of Phone 1 Inc., a voice data gathering systems manufacturer, and built data concentrators and [[terminal emulator]]s &#8212; multi-bus computers, modems, and terminals to interface with [[IBM]] mainframes via [[Binary Synchronous Communications|bisynchronous communications]]. The company then began developing [[mainframe]] middleware in the 1980s, and with the rise of the [[Personal computer|PC]], moved into B2B data communications and [[file transfer]] software.&lt;ref&gt;{{cite web|url=https://books.google.com/books?id=JNJWAAAAMAAJ&amp;q=cleo+%22phone+1%22&amp;dq=cleo+%22phone+1%22&amp;hl=en&amp;sa=X&amp;ei=6I65VLiABc6yogTf0IKABA&amp;ved=0CEcQ6AEwBTge |title=Kelly/Grimes IBM PC compatible computer directory - Brian W. Kelly, Dennis J. Grimes - Google Books |publisher=Books.google.com |date=2008-01-28 |accessdate=2015-04-02}}&lt;/ref&gt;

Cleo's portfolio features big data, extreme file transfer, [[data transformation]], person-to-person collaboration, and file sharing solutions,&lt;ref&gt;http://www.channelworld.in/interviews/high-speed-data-transfer-is-more-critical-than-ever%3A-mahesh-rajasekharan%2C-cleo&lt;/ref&gt; and its product line includes software for secure file transfer, exchange, and [[Cloud collaboration|collaboration]]; secure email, text, and voice messaging; and others.&lt;ref&gt;[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=204651549 Cleo Communications, Inc.: Private Company Information - Businessweek&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;  Cleo products use the [[AS2]] specification and other protocols for connectivity and community management.&lt;ref&gt;{{cite web|author=|url=http://www.filetransferconsulting.com/forresters-managed-file-transfer-good-bad-ugly/ |title=Forrester&#8217;s "Managed File Transfer Solutions" &#8211; Good, Bad and Ugly |publisher=Filetransferconsulting.com |date=2011-07-14 |accessdate=2014-06-05}}&lt;/ref&gt; Cleo VersaLex is the engine behind its software offerings, which include Cleo LexiCom,&lt;ref&gt;[http://www.itjungle.com/fhs/fhs030408-story10.html Four Hundred Stuff-Cleo Updates B2B Communications Software&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Cleo VLTrader,&lt;ref&gt;[http://www.itjungle.com/fhs/fhs021610-story08.html Four Hundred Stuff-Stonebranch Taps Cleo for B2B Expertise&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and Cleo Harmony, which supports the streamlining of [[data integration]].&lt;ref&gt;[http://webcache.googleusercontent.com/search?q=cache:http://lerablog.org/technology/software/ftp-tools-to-help-with-large-file-transfers/ Best FTP Tools for Large File Transfers&lt;!-- Bot generated title --&gt;]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; The company also developed the Cleo Unify and Cleo Trust secure [[file sharing]] and [[email]] messaging solutions that work independently or in conjunction with Cleo's data integration platform.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150209/News/150209528 |title=Cleo releases new software - Rockford Register Star |publisher=rrstar.com |date=2015-02-09 |accessdate=2015-02-19}}&lt;/ref&gt; In 2015, Cleo introduced the Cleo Jetsonic high-speed data transfer software solution.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150708/NEWS/150709580/-1/json |title=Cleo announces new data solution - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-09}}&lt;/ref&gt;

The City of [[Atlanta]] adopted Cleo's [[fax]] technology, Cleo Streem, in 2006 to accommodate its communication needs,&lt;ref&gt;http://citycouncil.atlantaga.gov/2013/images/proposed/13R3556.pdf&lt;/ref&gt; and the [[U.S. Department of Veterans Affairs]] did the same in 2013 when in need of [[FIPS 140-2]]-compliant technology to protect information.&lt;ref&gt;[http://www.va.gov/TRM/ToolPage.asp?tid=6568 One-VA Technical Reference Model&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Cleo also serves U.S. transportation [[logistics]] company MercuryGate International&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20140820/ENTERTAINMENTLIFE/140829886/10487/BUSINESS |title=Loves Park business selected to provide online services - Rockford Register Star |publisher=rrstar.com |date=2014-08-20 |accessdate=2015-02-19}}&lt;/ref&gt; as a customer and partners with [[Hortonworks]]&lt;ref&gt;[http://hortonworks.com/blog/secure-reliable-hadoop-data-transfer-option-cleo-mft/ Secure, reliable Hadoop data transfer with Cleo MFT - Hortonworks&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; for big data integration and [[Tech Data]] for software distribution.&lt;ref&gt;[http://logistics.cioreview.com/news/cleo-s-data-transfer-solutions-now-available-on-the-tech-data-online-store-nid-2119-cid-33.html Cleo's Data Transfer Solutions Now Available on the Tech Data Online Store&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Cleo software also powers the architecture for several major supply chain companies, such as [[JDA Software]] and [[SAP SE|SAP]].&lt;ref&gt;{{cite web|last=Grackin |first=Ann |url=http://searchmanufacturingerp.techtarget.com/tip/Smart-sensors-bring-the-supply-chain-to-life |title=Smart sensors bring the supply chain to life |publisher=Searchmanufacturingerp.techtarget.com |date= |accessdate=2015-07-08}}&lt;/ref&gt;

In 2009, Cleo was added to the [[Gartner]] [[Magic Quadrant]] for managed file transfer.&lt;ref&gt;http://www.servicecatalog.dts.ca.gov/services/sft/docs/MFT_Quad_2009_axway_3183.pdf&lt;/ref&gt;

== Expansion ==
In June 2014, Cleo opened an office in [[Chicago]] for members of its support and engineering teams.&lt;ref&gt;[http://rockrivertimes.com/2014/07/16/cleo-continues-to-grow-expands-operations-into-chicago-office/ Cleo continues to grow, expands operations into Chicago office | The Rock River Times&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; The company in 2014 hired Jorge Rodriguez as senior vice president of product development&lt;ref&gt;{{cite web|author=|url=http://www.marketwatch.com/story/jorge-rodriguez-joins-cleo-as-senior-vice-president-of-product-development-2014-02-04 |title=Jorge Rodriguez Joins Cleo as Senior Vice President of Product Development |publisher=MarketWatch |date=2014-02-04 |accessdate=2015-02-19}}&lt;/ref&gt; and John Thielens as vice president of technology.&lt;ref&gt;{{cite web|author=|url=http://www.marketwatch.com/story/john-thielens-joins-cleo-as-vice-president-of-technology-2014-01-31 |title=John Thielens Joins Cleo as Vice President of Technology |publisher=MarketWatch |date=2014-01-31 |accessdate=2015-02-19}}&lt;/ref&gt; And in 2015, Cleo hired Dave Brunswick as vice president of solutions for North America.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150705/NEWS/150709917 |title=Cleo announces new hire - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-08}}&lt;/ref&gt; Cleo also opened its Center of Innovation product development facility in [[Bengaluru, India]], in 2015.&lt;ref&gt;http://www.deccanherald.com/content/500091/cleo-bengaluru-centre-plans-co.html&lt;/ref&gt;

In 2016, Cleo acquired [[Extol International|EXTOL International]], a [[Pottsville, Pennsylvania|Pottsville, Pa.]]-based business and EDI integration and data transformation company for an undisclosed amount. The Pottsville office will operate under the Cleo name.&lt;ref&gt;http://www.lvb.com/article/20160406/LVB01/160409929/pottsville-tech-firm-acquired-by-illinois-company&lt;/ref&gt;

== Certification ==
Cleo regularly submits its products to Drummond Group's interoperability software testing for AS2,&lt;ref&gt;[http://www.supplychainbrain.com/content/technology-solutions/supplier-relationship-mgmt/single-article-page/article/cleos-versalex-wins-drummond-certification-for-as2-interoperability/ Cleo's VersaLex Wins Drummond Certification for AS2 Interoperability&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; AS3&lt;ref&gt;{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/341-as3-secure-messaging-products-are-drummond-certified-in-1q14-interoperability-test-event |title=AS3 Secure Messaging Products are Drummond Certified&#8482; in 1Q14 Interoperability Test Event |publisher=Drummond Group |date=2014-02-19 |accessdate=2014-06-05}}&lt;/ref&gt; and ebMS 2.0.&lt;ref&gt;{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/339-newest-ebms-20-secure-messaging-products-are-drummond-certified |title=Newest ebMS 2.0 Secure Messaging Products are Drummond Certified&#8482; |publisher=Drummond Group |date=2013-09-09 |accessdate=2014-06-05}}&lt;/ref&gt;

== Awards ==
Cleo has been given a [[Xerox]] partner of the year award for each of the past five years. The Cleo Streem solution integrates with Xerox multi-function products, providing customers with comprehensive solutions for network fax and interactive messaging needs.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150330/NEWS/150339927/10447/NEWS |title=Cleo Wins Xerox Partner of the Year Award - News - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-04-02}}&lt;/ref&gt;

== References ==
{{Reflist|3}}

[[Category:EDI software companies]]
[[Category:Software companies based in Illinois]]
[[Category:Network management]]
[[Category:Managed file transfer]]
[[Category:File transfer protocols]]
[[Category:Data management]]</text>
      <sha1>79mk6zmwc3zpl3zytcu7s1g3lji80l7</sha1>
    </revision>
  </page>
  <page>
    <title>Information integration</title>
    <ns>0</ns>
    <id>2714749</id>
    <revision>
      <id>760754830</id>
      <parentid>760746092</parentid>
      <timestamp>2017-01-18T22:04:19Z</timestamp>
      <contributor>
        <username>Sir mba</username>
        <id>11947451</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2896" xml:space="preserve">{{Refimprove|date=September 2014}}
'''Information integration''' ('''II''') (also called [[referential integrity]]) is the merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in [[data mining]] and consolidation of data from unstructured or semi-structured resources. Typically, ''information integration'' refers to textual representations of knowledge but is sometimes applied to [[rich-media]] content. '''Information fusion''', which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.&lt;ref name="dca"&gt;M. Haghighat, M. Abdel-Mottaleb, &amp;  W. Alhalabi (2016). [http://dx.doi.org/10.1109/TIFS.2016.2569061 Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.&lt;/ref&gt;

Examples of [[Technology|technologies]] available to integrate information include [[data deduplication|deduplication]], and [[string metrics]] which allow the detection of similar text in different data sources by [[fuzzy string searching|fuzzy matching]]. A host of methods for these research areas are available such as those presented in the International Society of Information Fusion.

==See also==
* [[Data fusion]] (is a subset of Information integration)
* [[Sensor fusion]]
* [[Data integration]]
* [[Image fusion]]

==External links==
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]&lt;ref name="dca"&gt;&lt;/ref&gt;
* [http://webcache.googleusercontent.com/search?q=cache:OrNCxOpaXAMJ:infolab.stanford.edu/pub/papers/integration-using-views.ps+information+integration&amp;cd=5&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a Information Integration Using Logical View] LNCS 1997.
* [http://www.isif.org/ International Society of Information Fusion]

==Books==
* Liggins, Martin E., David L. Hall, and James Llinas. Multisensor Data Fusion, Second Edition Theory and Practice (Multisensor Data Fusion). CRC, 2008. ISBN 978-1-4200-5308-1
* David L. Hall, Sonya A. H. McMullen, Mathematical Techniques in Multisensor Data Fusion (2004), ISBN 1-58053-335-3
* Springer, Information Fusion in Data Mining (2003), ISBN 3-540-00676-1
* H. B. Mitchell, Multi-sensor Data Fusion &#8211; An Introduction (2007) Springer-Verlag, Berlin, ISBN 978-3-540-71463-7
* S. Das, High-Level Data Fusion (2008), Artech House Publishers, Norwood, MA, ISBN 978-1-59693-281-4 and 1596932813
* Erik P. Blasch, Eloi Bosse, and Dale A. Lambert, High-Level Information Fusion Management and System Design (2012), Artech House Publishers, Norwood, MA. ISBN 1608071510 | ISBN 978-1608071517

==References==
{{Reflist}}


{{DEFAULTSORT:Information Integration}}
[[Category:Data management]]

[[ar:&#1578;&#1603;&#1575;&#1605;&#1604; &#1575;&#1604;&#1576;&#1610;&#1575;&#1606;&#1575;&#1578;]]
[[de:Informationsintegration]]</text>
      <sha1>f0twc7xm6gj48bktb0k1mbyy34rrzw4</sha1>
    </revision>
  </page>
  <page>
    <title>Big data</title>
    <ns>0</ns>
    <id>27051151</id>
    <revision>
      <id>762178998</id>
      <parentid>762178871</parentid>
      <timestamp>2017-01-27T04:16:33Z</timestamp>
      <contributor>
        <username>SeeChange</username>
        <id>30098935</id>
      </contributor>
      <minor />
      <comment>Added ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="95164" xml:space="preserve">{{About|large collections of data|the band|Big Data (band)}}
[[File:Hilbert InfoGrowth.png|thumb|right|400px|Growth of and digitization of global information-storage capacity&lt;ref&gt;{{cite web|url= http://www.martinhilbert.net/WorldInfoCapacity.html|title= The World&#8217;s Technological Capacity to Store, Communicate, and Compute Information|work= MartinHilbert.net|accessdate= 13 April 2016}}&lt;/ref&gt;]]

'''''Big data''''' is a term for [[data set]]s that are so large or complex that traditional [[data processing]] applications are inadequate to deal with them.  Challenges include [[Data analysis|analysis]], capture, [[data curation]], search, [[Data sharing|sharing]], [[Computer data storage|storage]], [[Data transmission|transfer]], [[Data visualization|visualization]], [[Query language|querying]], updating and [[information privacy]]. The term "big data" often refers simply to the use of [[predictive analytics]], [[user behavior analytics]], or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set.&lt;ref&gt;{{Cite book|url= http://link.springer.com/10.1007/978-3-319-21569-3 |title= New Horizons for a Data-Driven Economy &#8211; Springer|doi= 10.1007/978-3-319-21569-3}}&lt;/ref&gt; "There is little doubt that the quantities of data now available are indeed large, but that&#8217;s not the most relevant characteristic of this new data ecosystem."&lt;ref&gt;{{cite journal |last1=boyd |first1=dana |last2=Crawford |first2=Kate |title=Six Provocations for Big Data |journal=Social Science Research Network: A Decade in Internet Time: Symposium on the Dynamics of the Internet and Society |date=September 21, 2011 |doi=10.2139/ssrn.1926431}}&lt;/ref&gt;

Analysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on".{{r|Economist}} Scientists, business executives, practitioners of medicine, advertising and [[Government database|governments]] alike regularly meet difficulties with large data-sets in areas including [[Web search engine|Internet search]], finance, [[urban informatics]], and [[business informatics]].  Scientists encounter limitations in [[e-Science]] work, including [[meteorology]], [[genomics]],&lt;ref&gt;{{cite journal |title= Community cleverness required |journal= Nature |volume= 455 |issue= 7209 |page= 1 |date= 4 September 2008 |doi= 10.1038/455001a |url= http://www.nature.com/nature/journal/v455/n7209/full/455001a.html}}&lt;/ref&gt; [[connectomics]], complex physics simulations, biology and environmental research.&lt;ref&gt;{{cite journal |last1= Reichman |first1= O.J. |last2= Jones |first2= M.B. |last3= Schildhauer |first3= M.P. |title= Challenges and Opportunities of Open Data in Ecology |journal= Science |volume= 331 |issue= 6018 |pages= 703&#8211;5 |year= 2011 |doi= 10.1126/science.1197962 |pmid= 21311007 }}&lt;/ref&gt;

Data sets grow rapidly - in part because they are increasingly gathered by cheap and numerous information-sensing [[mobile device]]s, aerial ([[remote sensing]]), software logs, [[Digital camera|cameras]], microphones, [[radio-frequency identification]] (RFID) readers and [[wireless sensor networks]].&lt;ref&gt;{{cite web |author= Hellerstein, Joe |title= Parallel Programming in the Age of Big Data |date= 9 November 2008 |work= Gigaom Blog |url= http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/}}&lt;/ref&gt;&lt;ref&gt;{{cite book |first1= Toby |last1= Segaran |first2= Jeff |last2= Hammerbacher |title= Beautiful Data: The Stories Behind Elegant Data Solutions |url= https://books.google.com/books?id=zxNglqU1FKgC |year= 2009 |publisher= O'Reilly Media |isbn= 978-0-596-15711-1 |page= 257}}&lt;/ref&gt; The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;&lt;ref name="martinhilbert.net"&gt;{{cite journal | last1 = Hilbert | first1 = Martin | first2 = Priscila |last2=L&#243;pez | title = The World's Technological Capacity to Store, Communicate, and Compute Information | journal = Science | volume = 332 | issue = 6025 | pages = 60&#8211;65 | year = 2011 | doi = 10.1126/science.1200970 | pmid = 21310967 | url= http://martinhilbert.net/WorldInfoCapacity.html | ref= harv}}&lt;/ref&gt; {{As of|2012|lc=on}}, every day 2.5 [[exabyte]]s (2.5&#215;10&lt;sup&gt;18&lt;/sup&gt;) of data are generated.&lt;ref&gt;{{cite web|url= http://www.ibm.com/big-data/us/en/ |title= IBM What is big data? &#8211; Bringing big data to the enterprise |publisher= www.ibm.com |accessdate= 2013-08-26}}&lt;/ref&gt; One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.&lt;ref&gt;Oracle and FSN, [http://www.fsn.co.uk/channel_bi_bpm_cpm/mastering_big_data_cfo_strategies_to_transform_insight_into_opportunity#.UO2Ac-TTuys "Mastering Big Data: CFO Strategies to Transform Insight into Opportunity"], December 2012&lt;/ref&gt;

[[Relational database management system]]s and desktop statistics- and visualization-packages often have difficulty handling big data. The work may require "massively parallel software running on tens, hundreds, or even thousands of servers".&lt;ref&gt;{{cite web |author= Jacobs, A. |title= The Pathologies of Big Data |date= 6 July 2009 |work= ACMQueue |url= http://queue.acm.org/detail.cfm?id=1563874}}&lt;/ref&gt; What counts as "big data" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."&lt;ref&gt;
{{cite journal 
|last1= Magoulas |first1= Roger 
|last2= Lorica |first2= Ben 
|title= Introduction to Big Data 
|journal= Release 2.0 
|issue= 11 |date= February 2009 
|url= http://radar.oreilly.com/r2/release2-0-11.html 
|publisher= O'Reilly Media 
|location= Sebastopol CA
}}
&lt;/ref&gt;

== Definition ==
[[File:Viegas-UserActivityonWikipedia.gif|thumb|Visualization of daily Wikipedia edits created by IBM. At multiple [[terabyte]]s in size, the text and images of Wikipedia are an example of big data.]]
The term has been in use since the 1990s, with some giving credit to [[John Mashey]] for coining or at least making it popular.&lt;ref&gt;{{Cite web |title=  Big Data ... and the Next Wave of InfraStress |author= John R. Mashey |date= 25 April 1998 |publisher= Usenix |work= Slides from invited talk |url= http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf |accessdate= 28 September 2016 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The Origins of &#8216;Big Data&#8217;: An Etymological Detective Story |author=Steve Lohr |date= 1 February 2013 |url=http://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/ |publisher= [[New York Times]] |accessdate= 28 September 2016 }}&lt;/ref&gt;
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to [[data acquisition|capture]], [[data curation|curate]], manage, and process data within a tolerable elapsed time.&lt;ref name="Editorial"&gt;{{cite journal | last1 = Snijders | first1 = C. | last2 = Matzat | first2 = U. | last3 = Reips | first3 = U.-D. | year = 2012 | title = 'Big Data': Big gaps of knowledge in the field of Internet | url = http://www.ijis.net/ijis7_1/ijis7_1_editorial.html | journal = International Journal of Internet Science | volume = 7 | issue = | pages = 1&#8211;5 }}&lt;/ref&gt; Big data "size" is a constantly moving target, {{As of|2012|lc=on}} ranging from a few dozen terabytes to many [[petabyte]]s of data.
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale.&lt;ref&gt;{{cite journal | last1 = Ibrahim | first1 =  | last2 = Targio Hashem | first2 = Abaker | last3 = Yaqoob | first3 = Ibrar | last4 = Badrul Anuar | first4 = Nor | last5 = Mokhtar | first5 = Salimah | last6 = Gani | first6 = Abdullah | last7 = Ullah Khan | first7 = Samee | year = 2015 | title = big data" on cloud computing: Review and open research issues | url = | journal = Information Systems | volume = 47 | issue = | pages = 98&#8211;115 | doi = 10.1016/j.is.2014.07.006 }}&lt;/ref&gt;

In a 2001 research report&lt;ref&gt;{{cite web |first=Douglas |last=Laney |title=3D Data Management: Controlling Data Volume, Velocity and Variety |url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf |publisher=Gartner |accessdate = 6 February 2001}}&lt;/ref&gt; and related lectures, [[META Group]] (now [[Gartner]]) analyst [[Doug Laney]] defined data growth challenges and opportunities as being three-dimensional, i.e. increasing [[volume]] (amount of data), [[velocity]] (speed of data in and out), and {{linktext|variety}} (range of data types and sources). Gartner, and now much of the industry, continue to use this "3Vs" model for describing big data.&lt;ref&gt;{{cite web |last=Beyer |first=Mark |title=Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data |url=http://www.gartner.com/it/page.jsp?id=1731916 |publisher=Gartner |accessdate = 13 July 2011| archiveurl= https://web.archive.org/web/20110710043533/http://www.gartner.com/it/page.jsp?id=1731916| archivedate= 10 July 2011 | deadurl= no}}&lt;/ref&gt; In 2012, [[Gartner]] updated its definition as follows: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization." Gartner's definition of the 3Vs is still widely used, and in agreement with a consensual definition that states that "Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value".&lt;ref name="Big Data Definition"&gt;{{cite journal | last1 = De Mauro | first1 = Andrea | last2 = Greco | first2 = Marco | last3 = Grimaldi | first3 = Michele | year = 2016 | title = A Formal definition of Big Data based on its essential Features | url = http://www.emeraldinsight.com/doi/abs/10.1108/LR-06-2015-0061 | journal = Library Review | volume = 65| issue = | pages = 122&#8211;135 | doi=10.1108/LR-06-2015-0061}}&lt;/ref&gt; Additionally, a new V "Veracity" is added by some organizations to describe it,&lt;ref&gt;{{cite web|title=What is Big Data?|url=http://www.villanovau.com/university-online-programs/what-is-big-data/|publisher=[[Villanova University]]}}&lt;/ref&gt; revisionism challenged by some industry authorities.&lt;ref&gt;{{cite web|last=Grimes|first=Seth|title=Big Data: Avoid 'Wanna V' Confusion|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-avoid-wanna-v-confusion/d/d-id/1111077?|publisher=[[InformationWeek]]|accessdate = 5 January 2016}}&lt;/ref&gt; The 3Vs have been expanded to other complementary characteristics of big data:&lt;ref name="BD4D"&gt;{{cite web |last=Hilbert |first=Martin |title=Big Data for Development: A Review of Promises and Challenges. Development Policy Review. |url=http://www.martinhilbert.net/big-data-for-development |work=martinhilbert.net |accessdate=2015-10-07}}&lt;/ref&gt;&lt;ref name="WhatIsBigData" /&gt;
* Volume: big data doesn't sample; it just observes and tracks what happens
* Velocity: big data is often available in real-time
* Variety: big data draws from text, images, audio, video; plus it completes missing pieces through [[data fusion]]
* [[Machine Learning]]: big data often doesn't ask why and simply detects patterns&lt;ref&gt;Mayer-Sch&#246;nberger, V., &amp; Cukier, K. (2013). Big data: a revolution that will transform how we live, work and think. London: John Murray.&lt;/ref&gt;
* [[Digital footprint]]: big data is often a cost-free byproduct of digital interaction&lt;ref name="WhatIsBigData"&gt;{{cite av media|url=https://www.youtube.com/watch?v=XRVIh1h47sA&amp;index=51&amp;list=PLtjBSCvWCU3rNm46D3R85efM0hrzjuAIg|title=DT&amp;SC 7-3: What is Big Data?|date=12 August 2015|publisher=|via=YouTube}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://canvas.instructure.com/courses/949415|title=Digital Technology &amp; Social Change|publisher=}}&lt;/ref&gt;

The growing maturity of the concept more starkly delineates the difference between big data and [[Business Intelligence]]:&lt;ref&gt;http://www.bigdataparis.com/presentation/mercredi/PDelort.pdf?PHPSESSID=tv7k70pcr3egpi2r6fi3qbjtj6#page=4&lt;/ref&gt;
* Business Intelligence uses [[descriptive statistics]] with data with high information density to measure things, detect trends, etc..
* Big data uses [[inductive statistics]] and concepts from [[nonlinear system identification]]&lt;ref name="SAB1"&gt;Billings S.A. "Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains". Wiley, 2013&lt;/ref&gt;  to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density&lt;ref&gt;{{cite web|url=http://www.andsi.fr/tag/dsi-big-data/|title=le Blog ANDSI   &#187; DSI Big Data|publisher=}}&lt;/ref&gt; to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.&lt;ref name="SAB1" /&gt;&lt;ref&gt;{{cite web|url=http://lecercle.lesechos.fr/entrepreneur/tendances-innovation/221169222/big-data-low-density-data-faible-densite-information-com|title=Les Echos &#8211; Big Data car Low-Density Data ? La faible densit&#233; en information comme facteur discriminant &#8211; Archives|author=Les Echos|date=3 April 2013|work=lesechos.fr}}&lt;/ref&gt;

== Characteristics ==
Big data can be described by the following characteristics:&lt;ref name="BD4D" /&gt;&lt;ref name="WhatIsBigData" /&gt;

;Volume: The quantity of generated and stored data. The size of the data determines the value and potential insight- and whether it can actually be considered big data or not.

;Variety: The type and nature of the data. This helps people who analyze it to effectively use the resulting insight.

;Velocity: In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development.

;Variability: Inconsistency of the data set can hamper processes to handle and manage it.

;Veracity: The quality of captured data can vary greatly, affecting accurate analysis.

Factory work and [[Cyber-physical system]]s may have a 6C system:
* Connection (sensor and networks)
* Cloud (computing and data on demand)&lt;ref&gt;Wu, D., Liu. X., Hebert, S., Gentzsch, W., Terpenny, J. (2015). Performance Evaluation of Cloud-Based High Performance Computing for Finite Element Analysis. Proceedings of the ASME 2015 International Design Engineering Technical Conference &amp; Computers and Information in Engineering Conference (IDETC/CIE2015), Boston, Massachusetts, U.S.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Wu | first1 = D. | last2 = Rosen | first2 = D.W. | last3 = Wang | first3 = L. | last4 = Schaefer | first4 = D. | year = 2015 | title = Cloud-Based Design and Manufacturing: A New Paradigm in Digital Manufacturing and Design Innovation | url = | journal = Computer-Aided Design | volume = 59 | issue = 1| pages = 1&#8211;14 | doi = 10.1016/j.cad.2014.07.006 }}&lt;/ref&gt;
* Cyber (model and memory)
* Content/context (meaning and correlation)
* Community (sharing and collaboration)
* Customization (personalization and value)

Data must be processed with advanced tools (analytics and algorithms) to reveal meaningful information. For example, to manage a factory one must consider both visible and invisible issues with various components. Information generation algorithms must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor.&lt;ref name=INDIN2014&gt;{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics|journal=IEEE Int. Conference on Industrial Informatics (INDIN) 2014|date=2014|url=https://www.researchgate.net/profile/Behrad_Bagheri/publication/266375284_Recent_Advances_and_Trends_of_Cyber-Physical_Systems_and_Big_Data_Analytics_in_Industrial_Informatics/links/542dc0100cf27e39fa948a7d?origin=publication_detail}}&lt;/ref&gt;&lt;ref name=MfgLetters&gt;{{cite journal|last1=Lee|first1=Jay|last2=Lapira|first2=Edzel|last3=Bagheri|first3=Behrad|last4=Kao|first4=Hung-an|title=Recent advances and trends in predictive manufacturing systems in big data environment|journal=Manufacturing Letters|volume=1|issue=1|pages=38&#8211;41|doi=10.1016/j.mfglet.2013.09.005|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114}}&lt;/ref&gt;

== Architecture ==

In 2000, Seisint Inc. (now [[LexisNexis|LexisNexis Group]]) developed a C++-based distributed file-sharing framework for data storage and query. The system stores and distributes structured, semi-structured, and [[unstructured data]] across multiple servers. Users can build queries in a C++ [[Dialect (computing)|dialect]] called [[ECL programming language|ECL]]. ECL uses an "apply schema on read" method to infer the structure of stored data when it is queried, instead of when it is stored. In 2004, LexisNexis acquired Seisint Inc.&lt;ref&gt;{{cite web|url=http://www.washingtonpost.com/wp-dyn/articles/A50577-2004Jul14.html|title=LexisNexis To Buy Seisint For $775 Million|publisher=Washington Post|accessdate=15 July 2004}}&lt;/ref&gt; and in 2008 acquired [[ChoicePoint|ChoicePoint, Inc.]]&lt;ref&gt;{{cite web|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/02/21/AR2008022100809.html|title=LexisNexis Parent Set to Buy ChoicePoint|publisher=Washington Post|accessdate=22 February 2008}}&lt;/ref&gt; and their high-speed parallel processing platform. The two platforms were merged into [[HPCC]] (or High-Performance Computing Cluster) Systems and in 2011, HPCC was open-sourced under the Apache v2.0 License. [[Quantcast File System]] was available about the same time.&lt;ref&gt;{{cite web|url=http://www.datanami.com/2012/10/01/quantcast_opens_exabyte_ready_file_system/|title=Quantcast Opens Exabyte-Ready File System|publisher=www.datanami.com|accessdate=1 October 2012}}&lt;/ref&gt;

In 2004, [[Google]] published a paper on a process called [[MapReduce]] that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,&lt;ref&gt;Bertolucci, Jeff [http://www.informationweek.com/big-data/news/software-platforms/hadoop-from-experiment-to-leading-big-d/240157176 "Hadoop: From Experiment To Leading Big Data Platform"], "Information Week", 2013. Retrieved on 14 November 2013.&lt;/ref&gt; so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named [[Apache Hadoop|Hadoop]].&lt;ref&gt;Webster, John. [http://research.google.com/archive/mapreduce-osdi04.pdf "MapReduce: Simplified Data Processing on Large Clusters"], "Search Storage", 2004. Retrieved on 25 March 2013.&lt;/ref&gt;

[[MIKE2.0 Methodology|MIKE2.0]] is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled "Big Data Solution Offering".&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|title=Big Data Solution Offering|publisher=MIKE2.0|accessdate=8 December 2013}}&lt;/ref&gt; The methodology addresses handling big data in terms of useful [[permutation]]s of data sources, [[complexity]] in interrelationships, and difficulty in deleting (or modifying) individual records.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|title=Big Data Definition|publisher=MIKE2.0|accessdate=9 March 2013}}&lt;/ref&gt;

2012 studies showed that a multiple-layer architecture is one option to address the issues that big data presents. A [[List of file systems#Distributed parallel file systems|distributed parallel]] architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end user by using a front-end application server.&lt;ref&gt;{{cite journal|last=Boja|first=C|author2=Pocovnicu, A |author3=B&#259;t&#259;gan, L. |title=Distributed Parallel Architecture for Big Data|journal=Informatica Economica|year=2012|volume=16|issue=2|pages=116&#8211;127}}&lt;/ref&gt;

Big data analytics for manufacturing applications is marketed as a 5C architecture (connection, conversion, cyber, cognition, and configuration).&lt;ref&gt;{{cite web|url=http://www.imscenter.net/cyber-physical-platform|title=IMS_CPS &#8212; IMS Center|publisher=|accessdate=16 June 2016}}&lt;/ref&gt;

The [[data lake]] allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.&lt;ref&gt;http://www.hcltech.com/sites/default/files/solving_key_businesschallenges_with_big_data_lake_0.pdf&lt;/ref&gt;&lt;ref&gt;{{ cite web| url=https://secplab.ppgia.pucpr.br/files/papers/2015-0.pdf | title= Method for testing the fault tolerance of MapReduce frameworks | publisher=Computer Networks | year=2015}}&lt;/ref&gt;

== Technologies ==
{{see|Enablers of big data}}
A 2011 [[McKinsey &amp; Company|McKinsey Global Institute]] report characterizes the main components and ecosystem of big data as follows:&lt;ref name="McKinsey"&gt;{{cite journal
 | last1 = Manyika
 | first1 = James
 | first2=Michael |last2=Chui |first3=Jaques |last3=Bughin |first4=Brad |last4=Brown |first5=Richard |last5=Dobbs |first6=Charles |last6=Roxburgh |first7=Angela Hung |last7=Byers
 | title = Big Data: The next frontier for innovation, competition, and productivity
 | publisher = McKinsey Global Institute
 | date = May 2011
 | url =  http://www.mckinsey.com/Insights/MGI/Research/Technology_and_Innovation/Big_data_The_next_frontier_for_innovation
|accessdate=January 16, 2016
}}&lt;/ref&gt;
* Techniques for analyzing data, such as [[A/B testing]], [[machine learning]] and [[natural language processing]]
* Big data technologies, like [[business intelligence]], [[cloud computing]] and databases
* Visualization, such as charts, graphs and other displays of the data

Multidimensional big data can also be represented as [[tensor]]s, which can be more efficiently handled by tensor-based computation,&lt;ref&gt;{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}&lt;/ref&gt; such as [[multilinear subspace learning]].&lt;ref name="MSLsurvey"&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540&#8211;1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt; Additional technologies being applied to big data include massively parallel-processing ([[Massive parallel processing|MPP]]) databases, [[search-based application]]s, [[data mining]],&lt;ref&gt;{{cite web|last1=Pllana|first1=Sabri|last2=Janciak|first2=Ivan|last3=Brezany|first3=Peter|last4=W&#246;hrer|first4=Alexander|title=A Survey of the State of the Art in Data Mining and Integration Query Languages|url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6041580|website=2011 International Conference on Network-Based Information Systems (NBIS 2011)|publisher=IEEE Computer Society|accessdate=2 April 2016}}&lt;/ref&gt; [[distributed file system]]s, [[distributed database]]s, [[cloud computing|cloud-based]] infrastructure (applications, storage and computing resources) and the Internet.{{Citation needed|date=September 2011}}

Some but not all [[Massive parallel processing|MPP]] relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the [[RDBMS]].&lt;ref&gt;{{cite web |author=Monash, Curt |title=eBay's two enormous data warehouses |date=30 April 2009 |url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/}}&lt;br /&gt;{{cite web |author=Monash, Curt |title=eBay followup&amp;nbsp;&#8211; Greenplum out, Teradata &gt; 10 petabytes, Hadoop has some value, and more |date=6 October 2010 |url=http://www.dbms2.com/2010/10/06/ebay-followup-greenplum-out-teradata-10-petabytes-hadoop-has-some-value-and-more/}}&lt;/ref&gt;

[[DARPA]]'s [[Topological Data Analysis]] program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called [[Ayasdi]].&lt;ref&gt;{{cite web|url=http://www.ayasdi.com/resources/|title=Resources on how Topological Data Analysis is used to analyze big data|publisher=Ayasdi}}&lt;/ref&gt;

The practitioners of big data analytics processes are generally hostile to slower shared storage,&lt;ref&gt;{{cite web |title=Storage area networks need not apply |author=CNET News |date=1 April 2011 |url=http://news.cnet.com/8301-21546_3-20049693-10253464.html}}&lt;/ref&gt; preferring direct-attached storage ([[Direct-attached storage|DAS]]) in its various forms from solid state drive ([[Ssd]]) to high capacity [[Serial ATA|SATA]] disk buried inside parallel processing nodes. The perception of shared storage architectures&#8212;[[Storage area network]] (SAN) and [[Network-attached storage]] (NAS) &#8212;is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.

Real or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in memory is good&#8212;data on spinning disk at the other end of a [[Fiber connector|FC]] [[Storage area network|SAN]] connection is not. The cost of a [[Storage area network|SAN]] at the scale needed for analytics applications is very much higher than other storage techniques.

There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners {{As of|2011|lc=on}} did not favour it.&lt;ref&gt;{{cite web |title=How New Analytic Systems will Impact Storage |date=September 2011 |url=http://www.evaluatorgroup.com/document/big-data-how-new-analytic-systems-will-impact-storage-2/}}&lt;/ref&gt;

== Applications ==
[[File:2013-09-11 Bus wrapped with SAP Big Data parked outside IDF13 (9730051783).jpg|thumb|Bus wrapped with [[SAP AG|SAP]] Big data parked outside [[Intel Developer Forum|IDF13]].]]
Big data has increased the demand of information management specialists so much so that [[Software AG]], [[Oracle Corporation]], [[IBM]], [[Microsoft]], [[SAP AG|SAP]], [[EMC Corporation|EMC]], [[Hewlett-Packard|HP]] and [[Dell]] have spent more than $15&amp;nbsp;billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100&amp;nbsp;billion and was growing at almost 10&amp;nbsp;percent a year: about twice as fast as the software business as a whole.{{r|Economist}}

Developed economies increasingly use data-intensive technologies. There are 4.6&amp;nbsp;billion mobile-phone subscriptions worldwide, and between 1&amp;nbsp;billion and 2&amp;nbsp;billion people accessing the internet.{{r|Economist}} Between 1990 and 2005, more than 1&amp;nbsp;billion people worldwide entered the middle class, which means more people became more literate, which in turn lead to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 [[petabytes]] in 1986, 471 [[petabytes]] in 1993, 2.2 exabytes in 2000, 65 [[exabytes]] in 2007&lt;ref name="martinhilbert.net"/&gt; and predictions put the amount of internet traffic at 667 exabytes annually by 2014.{{r|Economist}} According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,&lt;ref name="HilbertContent"&gt;{{cite web|url=http://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748|title=An Error Occurred Setting Your User Cookie|publisher=}}&lt;/ref&gt; which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).

While many vendors offer off-the-shelf solutions for big data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.&lt;ref&gt;{{cite web |url=http://www.kdnuggets.com/2014/07/interview-amy-gershkoff-ebay-in-house-BI-tools.html |title=Interview: Amy Gershkoff, Director of Customer Analytics &amp; Insights, eBay on How to Design Custom In-House BI Tools |last1=Rajpurohit |first1=Anmol |date=11 July 2014 |website= KDnuggets|accessdate=2014-07-14|quote=Dr. Amy Gershkoff: "Generally, I find that off-the-shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data. Therefore, for medium-to-large organizations with access to strong technical talent, I usually recommend building custom, in-house solutions."}}&lt;/ref&gt;

=== Government ===
The use and adoption of big data within governmental processes is beneficial and allows efficiencies in terms of cost, productivity, and innovation,&lt;ref&gt;{{cite web|url=http://www.computerworld.com/article/2472667/government-it/the-government-and-big-data--use--problems-and-potential.html |title=The Government and big data: Use, problems and potential |date=21 March 2012 |publisher=Computerworld |access-date=12 September 2016}}&lt;/ref&gt; but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. Below are some examples of initiatives the governmental big data space.

==== United States of America ====
* In 2012, the [[Presidency of Barack Obama|Obama administration]] announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government.&lt;ref name=WH_Big_Data&gt;{{cite web|last=Kalil|first=Tom|title=Big Data is a Big Deal|url=http://www.whitehouse.gov/blog/2012/03/29/big-data-big-deal|publisher=White House|accessdate=26 September 2012}}&lt;/ref&gt; The initiative is composed of 84 different big data programs spread across six departments.&lt;ref&gt;{{cite web|last=Executive Office of the President|title=Big Data Across the Federal Government|url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_fact_sheet_final_1.pdf|publisher=White House|accessdate=26 September 2012 |date=March 2012}}&lt;/ref&gt;
* Big data analysis played a large role in [[Barack Obama]]'s successful [[Barack Obama presidential campaign, 2012|2012 re-election campaign]].&lt;ref name=infoworld_bigdata&gt;{{cite web|last=Lampitt|first=Andrew|title=The real story of how big data analytics helped Obama win|url=http://www.infoworld.com/d/big-data/the-real-story-of-how-big-data-analytics-helped-obama-win-212862|work=[[Infoworld]]|accessdate=31 May 2014}}&lt;/ref&gt;
* The [[United States Federal Government]] owns six of the ten most powerful [[supercomputer]]s in the world.&lt;ref&gt;{{cite web |last=Hoover |first=J. Nicholas |title=Government's 10 Most Powerful Supercomputers |url=http://www.informationweek.com/government/enterprise-applications/image-gallery-governments-10-most-powerf/224700271 |work=Information Week |publisher=UBM |accessdate=26 September 2012}}&lt;/ref&gt;
* The [[Utah Data Center]] has been constructed by the United States [[National Security Agency]]. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few [[exabyte]]s.&lt;ref&gt;{{cite news | last=Bamford|first=James|title=The NSA Is Building the Country's Biggest Spy Center (Watch What You Say)|url=http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1|work=Wired Magazine|accessdate=2013-03-18|date=15 March 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nsa.gov/public_info/press_room/2011/utah_groundbreaking_ceremony.shtml|title=Groundbreaking Ceremony Held for $1.2 Billion Utah Data Center|publisher=National Security Agency Central Security Service|accessdate=2013-03-18}}&lt;/ref&gt;&lt;ref&gt;{{cite news | last=Hill|first=Kashmir|title=TBlueprints of NSA's Ridiculously Expensive Data Center in Utah Suggest It Holds Less Info Than Thought|url=http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/|work=Forbes|accessdate=2013-10-31}}&lt;/ref&gt;

==== India ====
* Big data analysis was in part responsible for the [[Bharatiya Janata Party|BJP]] to win the [[Indian general election, 2014|Indian General Election 2014]].&lt;ref&gt;{{cite web|url = http://www.livemint.com/Industry/bUQo8xQ3gStSAy5II9lxoK/Are-Indian-companies-making-enough-sense-of-Big-Data.html|title = News: Live Mint|date = 23 June 2014|accessdate = 2014-11-22|website = Are Indian companies making enough sense of Big Data?|publisher = Live Mint}}&lt;/ref&gt;
* The [[Government of India|Indian government]] utilizes numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.&lt;ref&gt;{{cite web|url=http://decipherias.com/currentaffairs/big-data-whats-so-big-about-it/|title=Big Data- What&#8217;s so big about it?|date=18 March 2016|publisher=Decipher IAS|access-date=12 September 2016}}&lt;/ref&gt;

==== United Kingdom ====
Examples of uses of big data in public services:
* Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the [[National Institute for Health and Care Excellence]] guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.&lt;ref&gt;{{cite web|url=https://www.ijedr.org/papers/IJEDR1504022.pdf|title=Survey on Big Data Using Data Mining|date=2015|publisher=International Journal of Engineering Development and Research|access-date=14 September 2016}}&lt;/ref&gt;
* Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as 'meals on wheels'. The connection of data allowed the local authority to avoid any weather related delay.&lt;ref&gt;{{cite web|url=https://www.researchgate.net/publication/297762848_Recent_advances_delivered_by_mobile_cloud_computing_and_Internet_of_Things_for_Big_data_applications_A_Survey|title=Recent advances delivered by Mobile Cloud Computing and Internet of Things for Big Data applications: a survey|date=11 March 2016|publisher=International Journal of Network Management|access-date=14 September 2016}}&lt;/ref&gt;

=== International development ===
Research on the effective usage of [[information and communication technologies for development]] (also known as [[ICT4D]]) suggests that big data technology can make important contributions but also present unique challenges to [[International development]].&lt;ref&gt;{{cite web|url=http://www.unglobalpulse.org/projects/BigDataforDevelopment|title=White Paper: Big Data for Development: Opportunities &amp; Challenges (2012) &#8211; United Nations Global Pulse|publisher=|accessdate=13 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=WEF (World Economic Forum), &amp; Vital Wave Consulting. (2012). Big Data, Big Impact: New Possibilities for International Development|work= World Economic Forum|accessdate=24 August 2012|url= http://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development}}&lt;/ref&gt; Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, [[economic productivity]], crime, security, and [[natural disaster]] and resource management.&lt;ref name="HilbertBigData2013" /&gt;&lt;ref&gt;{{cite web|url=http://blogs.worldbank.org/ic4d/four-ways-to-talk-about-big-data/|title=Elena Kvochko, Four Ways To talk About Big Data (Information Communication Technologies for Development Series)|publisher=worldbank.org|accessdate=2012-05-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Daniele Medri: Big Data &amp; Business: An on-going revolution|url=http://www.statisticsviews.com/details/feature/5393251/Big-Data--Business-An-on-going-revolution.html|publisher=Statistics Views |date=21 October 2013}}&lt;/ref&gt; Additionally, user-generated data offers new opportunities to give the unheard a voice.&lt;ref&gt;{{cite web|title=Responsible use of data|author=Tobias Knobloch and Julia Manske|work= D+C, Development and Cooperation|date=11 January 2016|url= http://www.dandc.eu/en/article/opportunities-and-risks-user-generated-and-automatically-compiled-data}}&lt;/ref&gt; However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.&lt;ref name="HilbertBigData2013" /&gt;

=== Manufacturing ===
Based on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing.&lt;ref name="TCS Big Data Study &#8211; Manufacturing"&gt;{{cite web|url=http://sites.tcs.com/big-data-study/manufacturing-big-data-benefits-challenges/# |title=Manufacturing: Big Data Benefits and Challenges |work= TCS Big Data Study|publisher=[[Tata Consultancy Services Limited]] |location=Mumbai, India |accessdate=2014-06-03}}&lt;/ref&gt; Big data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.&lt;ref&gt;{{cite journal|last=Lee|first=Jay|author2=Wu, F. |author3=Zhao, W. |author4=Ghaffari, M. |author5= Liao, L |title=Prognostics and health management design for rotary machinery systems&#8212;Reviews, methodology and applications|journal=Mechanical Systems and Signal Processing|date=January 2013|volume=42|issue=1}}&lt;/ref&gt; A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. Vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as [[Prognostics]] and Health Management (PHM).&lt;ref&gt;{{cite web|url=https://www.phmsociety.org/events/conference/phm/europe/16/tutorials|title=Tutorials|publisher=PHM Society|accessdate=27 September 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.itri.org.tw/eng/Content/MSGPic01/contents.aspx?&amp;SiteID=1&amp;MmmID=620651706136357202&amp;CatID=620653256103620163&amp;MSID=654532365564567545|title=Prognostic and Health Management Technology for MOCVD Equipment|publisher=Industrial Technology Research Institute|accessdate=27 September 2016}}&lt;/ref&gt;

==== Cyber-physical models ====
Current PHM implementations mostly use data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine's lifecycle, such as system configuration, physical knowledge and working principles, are included. There is a need to systematically integrate, manage and analyze machinery or process data during different stages of machine life cycle to handle data/information more efficiently and further achieve better transparency of machine health condition for manufacturing industry.

With such motivation a cyber-physical (coupled) model scheme has been developed. The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. It can also be described as a 5S systematic approach consisting of sensing, storage, synchronization, synthesis and service. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. After that step, the simulation model can be considered a mirrored image of the real machine&#8212;able to continuously record and track machine condition during the later utilization stage. Finally, with the increased connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited.&lt;ref name="MfgLetters" /&gt;

=== Healthcare ===
Big data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries and fragmented point solutions.&lt;ref name="ref135"&gt;{{cite journal|doi=10.1016/j.ijrobp.2015.10.060|title=Impending Challenges for the Use of Big Data }}&lt;/ref&gt; Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality.&lt;ref&gt;{{cite journal|url=http://doi.acm.org/10.1145/2378016.2378021|title=Data Management Within mHealth Environments: Patient Sensors, Mobile Devices, and Databases|first1=John|last1=O'Donoghue|first2=John|last2=Herbert|date=1 October 2012|publisher=|volume=4|issue=1|pages=5:1&#8211;5:20|accessdate=16 June 2016|via=ACM Digital Library|doi=10.1145/2378016.2378021}}&lt;/ref&gt; "Big data very often means `dirty data' and the fraction of data inaccuracies increases with data volume growth." Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed.&lt;ref name="Mirkes2016"&gt;{{cite journal | last1 = Mirkes| first1 =E.M.|last2 = Coats|first2 =T.J.|last3 = Levesley|first3 =J.|last4 = Gorban|first4 = A.N.| title = Handling missing data in large healthcare dataset: A case study of unknown trauma outcomes|url =  https://www.researchgate.net/publication/300400110_Handling_missing_data_in_large_healthcare_dataset_A_case_study_of_unknown_trauma_outcomes| journal = Computers in Biology and Medicine| volume = 75| issue = | pages = 203&#8211;216| year = 2016| doi = 10.1016/j.compbiomed.2016.06.004}}&lt;/ref&gt; While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use.&lt;ref&gt;{{Cite journal|last=Murdoch|first=Travis B.|last2=Detsky|first2=Allan S.|date=2013-04-03|title=The Inevitable Application of Big Data to Health Care|url=http://jamanetwork.com/journals/jama/article-abstract/1674245|journal=JAMA|language=en|volume=309|issue=13|doi=10.1001/jama.2013.393|issn=0098-7484}}&lt;/ref&gt;

=== Education ===
A [[McKinsey &amp; Company|McKinsey Global Institute]] study found a shortage of 1.5 million highly trained data professionals and managers&lt;ref name="McKinsey"/&gt; and a number of universities&lt;ref&gt;{{cite news
| url=http://www.forbes.com/sites/jmaureenhenderson/2013/07/30/degrees-in-big-data-fad-or-fast-track-to-career-success/?
|access-date=2016-02-21
|newspaper=Forbes
|title=Degrees in Big Data: Fad or Fast Track to Career Success}}&lt;/ref&gt; including [[University of Tennessee]] and [[UC Berkeley]], have created masters programs to meet this demand.  Private bootcamps have also developed programs to meet that demand, including free programs like [[The Data Incubator]] or paid programs like [[General Assembly]].&lt;ref&gt;{{cite news
|title=NY gets new bootcamp for data scientists: It&#8217;s free, but harder to get into than Harvard
|newspaper=Venture Beat
|access-date=2016-02-21
|url=http://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/
}}&lt;/ref&gt;

=== Media ===
To understand how the media utilises big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that [[wikt:practitioner|practitioners]] in Media and Advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations.  The ultimate aim is to serve, or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various [[data-mining]] activities.&lt;ref&gt;{{cite journal|last1=Couldry|first1=Nick|last2=Turow|first2=Joseph|title=Advertising, Big Data, and the Clearance of the Public Realm: Marketers&#8217; New Approaches to the Content Subsidy|journal=International Journal of Communication|date=2014|volume=8|pages=1710&#8211;1726}}&lt;/ref&gt;
* Targeting of consumers (for advertising by marketers)
* Data-capture
* [[Data journalism]]: publishers and journalists use big data tools to provide unique and innovative insights and infographics.

==== Internet of Things (IoT) ====
{{tone|section|date=September 2016}}

{{Main article|Internet of Things}}
Big data and the IoT work in conjunction.  From a media perspective, data is the key derivative of device inter-connectivity and allows accurate targeting.  The [[Internet of Things]], with the help of big data, therefore transforms the media industry, companies and even governments, opening up a new era of economic growth and competitiveness. The intersection of people, data and intelligent algorithms have far-reaching impacts on media efficiency. The wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry.

==== Technology ====
* [[eBay.com]] uses two data warehouses at 7.5 [[petabytes]] and 40PB as well as a 40PB [[Hadoop]] cluster for search, consumer recommendations, and merchandising.&lt;ref&gt;{{cite web | last=Tay | first=Liz |url=http://www.itnews.com.au/news/inside-ebay8217s-90pb-data-warehouse-342615 | title=Inside eBay&#8217;s 90PB data warehouse | publisher=ITNews | accessdate=2016-02-12}}&lt;/ref&gt;
* [[Amazon.com]] handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.&lt;ref&gt;{{cite web|last=Layton |first=Julia |url=http://money.howstuffworks.com/amazon1.htm |title=Amazon Technology |publisher=Money.howstuffworks.com |accessdate=2013-03-05}}&lt;/ref&gt;
* [[Facebook]] handles 50&amp;nbsp;billion photos from its user base.&lt;ref&gt;{{cite web|url=https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919 |title=Scaling Facebook to 500 Million Users and Beyond |publisher=Facebook.com |accessdate=2013-07-21}}&lt;/ref&gt;
* As of August 2012, [[Google]] was handling roughly 100&amp;nbsp;billion searches per month.&lt;ref&gt;{{cite web|url=http://searchengineland.com/google-1-trillion-searches-per-year-212940|title=Google Still Doing at Least 1 Trillion Searches Per Year|date=16 January 2015|work=Search Engine Land|accessdate=15 April 2015}}&lt;/ref&gt;
* [[Oracle NoSQL Database]] has been tested to past the 1M ops/sec mark with 8 shards and proceeded to hit 1.2M ops/sec with 10 shards.&lt;ref&gt;{{cite web |last=Lamb |first=Charles |url=https://blogs.oracle.com/charlesLamb/entry/oracle_nosql_database_exceeds_1 |title=Oracle NoSQL Database Exceeds 1 Million Mixed YCSB Ops/Sec}}&lt;/ref&gt;

=== Private sector ===

=== Information Technology ===
Especially since 2015, big data has come to prominence within [[Business Operations]] as a tool to help employees work more efficiently and streamline the collection and distribution of [[Information Technology]] (IT). The use of big data to attack IT and data collection issues within an enterprise is called [[IT Operations Analytics]] (ITOA).&lt;ref name="ITOA1"&gt;{{cite web|last1=Solnik|first1=Ray|title=The Time Has Come: Analytics Delivers for IT Operations|url=http://www.datacenterjournal.com/time-analytics-delivers-operations/|website=Data Center Journal|accessdate=June 21, 2016}}&lt;/ref&gt; By applying big data principles into the concepts of [[machine intelligence]] and [[deep computing]], IT departments can predict potential issues and move to provide solutions before the problems even happen.&lt;ref name="ITOA1" /&gt; In this time, ITOA businesses were also beginning to play a major role in [[systems management]] by offering platforms that brought individual [[data silos]] together and generated insights from the whole of the system rather than from isolated pockets of data.

==== Retail ====
* [[Walmart]] handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data&#8212;the equivalent of 167 times the information contained in all the books in the US [[Library of Congress]].{{r|Economist}}

==== Retail banking ====
* FICO Card Detection System protects accounts worldwide.&lt;ref name="fico.com"&gt;{{cite web|url=http://www.fico.com/en/Products/DMApps/Pages/FICO-Falcon-Fraud-Manager.aspx |title=FICO&#174; Falcon&#174; Fraud Manager |publisher=Fico.com |accessdate=2013-07-21}}&lt;/ref&gt;
* The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.&lt;ref name="KnowWPCarey.com"&gt;{{cite web|url=http://research.wpcarey.asu.edu/managing-it/ebay-study-how-to-build-trust-and-improve-the-shopping-experience |title=eBay Study: How to Build Trust and Improve the Shopping Experience |publisher=Knowwpcarey.com |date=8 May 2012 |accessdate=2015-12-20}}&lt;/ref&gt;&lt;ref&gt;[http://www.statista.com/statistics/280444/global-leading-priorities-for-big-data-according-to-business-and-it-executives/ Leading Priorities for Big Data for Business and IT]. eMarketer. October 2013. Retrieved January 2014.&lt;/ref&gt;

==== Real estate ====
* [[Windermere Real Estate]] uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.&lt;ref&gt;{{cite news|last=Wingfield |first=Nick |url=http://bits.blogs.nytimes.com/2013/03/12/predicting-commutes-more-accurately-for-would-be-home-buyers/ |title=Predicting Commutes More Accurately for Would-Be Home Buyers &#8211; NYTimes.com |publisher=Bits.blogs.nytimes.com |date=12 March 2013 |accessdate=2013-07-21}}&lt;/ref&gt;

=== Science ===
The [[Large Hadron Collider]] experiments represent about 150 million sensors delivering data 40&amp;nbsp;million times per second. There are nearly 600&amp;nbsp;million collisions per second. After filtering and refraining from recording more than 99.99995%&lt;ref&gt;{{cite web|last1=Alexandru|first1=Dan|title=Prof|url=https://cds.cern.ch/record/1504817/files/CERN-THESIS-2013-004.pdf|website=cds.cern.ch|publisher=CERN|accessdate=24 March 2015}}&lt;/ref&gt; of these streams, there are 100 collisions of interest per second.&lt;ref&gt;{{cite web |title=LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public. |url=http://cds.cern.ch/record/1278169?ln=en |work=CERN-Brochure-2010-006-Eng. LHC Brochure, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers. |url=http://cds.cern.ch/record/1092437?ln=en |work=CERN-Brochure-2008-001-Eng. LHC Guide, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref name="nature"&gt;{{cite news |title=High-energy physics: Down the petabyte highway |work= Nature |date= 19 January 2011 |first=Geoff |last=Brumfiel |doi= 10.1038/469282a |volume= 469 |pages= 282&#8211;83 |url= http://www.nature.com/news/2011/110119/full/469282a.html }}&lt;/ref&gt;
* As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
* If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 [[exabyte]]s per day, before replication. To put the number in perspective, this is equivalent to 500 [[quintillion]] (5&#215;10&lt;sup&gt;20&lt;/sup&gt;) bytes per day, almost 200 times more than all the other sources combined in the world.

The [[Square Kilometre Array]] is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day.&lt;ref&gt;http://www.zurich.ibm.com/pdf/astron/CeBIT%202013%20Background%20DOME.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://arstechnica.com/science/2012/04/future-telescope-array-drives-development-of-exabyte-processing/|title=Future telescope array drives development of exabyte processing|work=Ars Technica|accessdate=15 April 2015}}&lt;/ref&gt; It is considered one of the most ambitious scientific projects ever undertaken.&lt;ref&gt;{{cite web|url=http://theconversation.com/australias-bid-for-the-square-kilometre-array-an-insiders-perspective-4891|title=Australia&#8217;s bid for the Square Kilometre Array &#8211; an insider&#8217;s perspective|date=1 February 2012|publisher=[[The Conversation (website)|The Conversation]]|accessdate=27 September 2016}}&lt;/ref&gt;

==== Science and research ====
{{Expand section|date=December 2016}}
* When the [[Sloan Digital Sky Survey]] (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200&amp;nbsp;GB per night, SDSS has amassed more than 140 terabytes of information.&lt;ref name="Economist"&gt;{{cite news |title=Data, data everywhere |url=http://www.economist.com/node/15557443 |newspaper=The Economist |date=25 February 2010 |accessdate=9 December 2012}}&lt;/ref&gt; When the [[Large Synoptic Survey Telescope]], successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.{{r|Economist}}
* Decoding the [[Human Genome Project|human genome]] originally took 10 years to process, now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by [[Moore's Law]].&lt;ref&gt;[http://www.oecd.org/sti/ieconomy/Session_3_Delort.pdf#page=6 Delort P., OECD ICCP Technology Foresight Forum, 2012.]&lt;/ref&gt;
* The [[NASA]] Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.&lt;ref&gt;{{cite web|url=http://www.nasa.gov/centers/goddard/news/releases/2010/10-051.html|title=NASA &#8211; NASA Goddard Introduces the NASA Center for Climate Simulation|publisher=|accessdate=13 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Webster|first=Phil|title=Supercomputing the Climate: NASA's Big Data Mission|url=http://www.csc.com/cscworld/publications/81769/81773-supercomputing_the_climate_nasa_s_big_data_mission|work=CSC World|publisher=Computer Sciences Corporation|accessdate=2013-01-18}}&lt;/ref&gt;
* Google's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any 'friction points,' or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.&lt;ref&gt;{{cite web|url=http://www.theglobeandmail.com/life/health-and-fitness/health/these-six-great-neuroscience-ideas-could-make-the-leap-from-lab-to-market/article21681731/|title=These six great neuroscience ideas could make the leap from lab to market|date=20 November 2014|publisher=[[The Globe and Mail]]|accessdate=1 October 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://cloud.google.com/customers/dnastack/|title=DNAstack tackles massive, complex DNA datasets with Google Genomics|publisher=Google Cloud Platform |accessdate=1 October 2016}}&lt;/ref&gt;
* [[23andme]]'s [[DNA database]] contains genetic information of over 1,000,000 people worldwide.&lt;ref&gt;{{cite web|title=23andMe - Ancestry|url=https://www.23andme.com/en-int/ancestry/|website=23andme.com|accessdate=29 December 2016}}&lt;/ref&gt; The company explores selling the "anonymous aggregated genetic data" to other researchers and pharmaceutical companies for research purposes if patients give their consent.&lt;ref name=verge1&gt;{{cite web|last1=Potenza|first1=Alessandra|title=23andMe wants researchers to use its kits, in a bid to expand its collection of genetic data|url=http://www.theverge.com/2016/7/13/12166960/23andme-genetic-testing-database-genotyping-research|publisher=The Verge|accessdate=29 December 2016|date=13 July 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=This Startup Will Sequence Your DNA, So You Can Contribute To Medical Research|url=https://www.fastcompany.com/3066775/innovation-agents/this-startup-will-sequence-your-dna-so-you-can-contribute-to-medical-resea|publisher=Fast Company|accessdate=29 December 2016|date=23 December 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Seife|first1=Charles|title=23andMe Is Terrifying, but Not for the Reasons the FDA Thinks|url=https://www.scientificamerican.com/article/23andme-is-terrifying-but-not-for-the-reasons-the-fda-thinks/|publisher=Scientific American|accessdate=29 December 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Zaleski|first1=Andrew|title=This biotech start-up is betting your genes will yield the next wonder drug|url=http://www.cnbc.com/2016/06/22/23andme-thinks-your-genes-are-the-key-to-blockbuster-drugs.html|publisher=CNBC|accessdate=29 December 2016|date=22 June 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Regalado|first1=Antonio|title=How 23andMe turned your DNA into a $1 billion drug discovery machine|url=https://www.technologyreview.com/s/601506/23andme-sells-data-for-drug-search/|publisher=MIT Technology Review|accessdate=29 December 2016}}&lt;/ref&gt; Ahmad Hariri, professor of psychology and neuroscience at [[Duke University]] who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists.&lt;ref name=verge1/&gt; A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.&lt;ref&gt;{{cite web|title=23andMe reports jump in requests for data in wake of Pfizer depression study {{!}} FierceBiotech|url=http://www.fiercebiotech.com/it/23andme-reports-jump-requests-for-data-wake-pfizer-depression-study|website=fiercebiotech.com|accessdate=29 December 2016}}&lt;/ref&gt;

=== Sports ===
Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.&lt;ref&gt;{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&amp;view=article&amp;id=147241|title=Data scientists predict Springbok defeat
|author=Admire Moyo|work=www.itweb.co.za|accessdate=12 December 2015}}&lt;/ref&gt;
Future performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.&lt;ref&gt;{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&amp;view=article&amp;id=147852|title= Predictive analytics, big data transform sports
 |author=Regina Pazvakavambwa|work=www.itweb.co.za|accessdate=12 December 2015}}&lt;/ref&gt;

The movie [[Moneyball (film)|''MoneyBall'']] demonstrates how big data could be used to scout players and also identify undervalued players.&lt;ref&gt;{{cite web|url=http://www.datacenterknowledge.com/archives/2011/09/23/the-lessons-of-moneyball-for-big-data-analysis/|title= The Lessons of Moneyball for Big Data Analysis|author=Rich Miller|work=www.datecenterknowledge.com|accessdate=12 December 2015}}&lt;/ref&gt;

In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency. Then, this data is transferred to team headquarters in United Kingdom through fiber optic cables that could carry data at the speed of light.&lt;ref&gt;{{cite web|url=http://www.huffingtonpost.com/dave-ryan/sports-where-big-data-fin_b_8553884.html|title= Sports: Where Big Data Finally Makes Sense |author=Dave Ryan|work=www.huffingtonpost.com|accessdate=12 December 2015}}&lt;/ref&gt;
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/frankbi/2014/11/13/how-formula-one-teams-are-using-big-data-to-get-the-inside-edge//|title= How Formula One Teams Are Using Big Data To Get The Inside Edge|author=Frank Bi|work=www.forbes.com|accessdate=12 December 2015}}&lt;/ref&gt;

== Research activities ==
Encrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at ''Tackling the challenges of Big Data'' by [[MIT Computer Science and Artificial Intelligence Laboratory]] and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.&lt;ref&gt;{{cite conference |url=http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf |title=Encrypted Search &amp; Cluster Formation in Big Data |last1=Siwach |first1=Gautam |last2=Esmailpour |first2=Amir |date=March 2014 |year= |conference=ASEE 2014 Zone I Conference |conference-url=http://ubconferences.org/ |location=[[University of Bridgeport]], [[Bridgeport, Connecticut]], US }}&lt;/ref&gt;

In March 2012, The White House announced a national "Big Data Initiative" that consisted of six Federal departments and agencies committing more than $200&amp;nbsp;million to big data research projects.&lt;ref&gt;{{cite web |title=Obama Administration Unveils "Big Data" Initiative:Announces $200 Million In New R&amp;D Investments|publisher=The White House |url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_press_release_final_2.pdf}}&lt;/ref&gt;

The initiative included a National Science Foundation "Expeditions in Computing" grant of $10 million over 5 years to the AMPLab&lt;ref&gt;{{cite web|url=http://amplab.cs.berkeley.edu |title=AMPLab at the University of California, Berkeley |publisher=Amplab.cs.berkeley.edu |accessdate=2013-03-05}}&lt;/ref&gt; at the University of California, Berkeley.&lt;ref&gt;{{cite web |title=NSF Leads Federal Efforts in Big Data|date=29 March 2012|publisher=National Science Foundation (NSF) |url=http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&amp;org=NSF&amp;from=news}}&lt;/ref&gt; The AMPLab also received funds from [[DARPA]], and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion&lt;ref&gt;{{cite conference|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|author1=Timothy Hunter|date=October 2011|author2=Teodor Moldovan|author3=Matei Zaharia|author4=Justin Ma|author5=Michael Franklin|author6=Pieter Abbeel|author7=Alexandre Bayen|title=Scaling the Mobile Millennium System in the Cloud}}&lt;/ref&gt; to fighting cancer.&lt;ref&gt;{{cite news|title=Computer Scientists May Have What It Takes to Help Cure Cancer|author=David Patterson|publisher=The New York Times|date=5 December 2011|url=http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0}}&lt;/ref&gt;

The White House Big Data Initiative also included a commitment by the  Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,&lt;ref&gt;{{cite web|title=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers |publisher="energy.gov" |url=http://energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe}}&lt;/ref&gt; led by the Energy Department&#8217;s [[Lawrence Berkeley National Laboratory]]. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department's supercomputers.

The U.S. state of [[Massachusetts]] announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.&lt;ref&gt;{{cite web |title=Governor Patrick announces new initiative to strengthen Massachusetts' position as a World leader in Big Data |publisher=Commonwealth of Massachusetts |url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html}}&lt;/ref&gt;  The [[Massachusetts Institute of Technology]] hosts the Intel Science and Technology Center for Big Data in the [[MIT Computer Science and Artificial Intelligence Laboratory]], combining government, corporate, and institutional funding and research efforts.&lt;ref&gt;{{cite web|url=http://bigdata.csail.mit.edu/ |title=Big Data @ CSAIL |publisher=Bigdata.csail.mit.edu |date=22 February 2013 |accessdate=2013-03-05}}&lt;/ref&gt;

The European Commission is funding the 2-year-long Big Data Public Private Forum through their [[Seventh Framework Program]] to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for [[Horizon 2020]], their next [[Framework Programmes for Research and Technological Development|framework program]].&lt;ref&gt;{{cite web|url=http://cordis.europa.eu/search/index.cfm?fuseaction=proj.document&amp;PJ_RCN=13267529 |title=Big Data Public Private Forum |publisher=Cordis.europa.eu |date=1 September 2012 |accessdate=2013-03-05}}&lt;/ref&gt;

The British government announced in March 2014 the founding of the [[Alan Turing Institute]], named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyse large data sets.&lt;ref&gt;{{cite news|url=http://www.bbc.co.uk/news/technology-26651179|title=Alan Turing Institute to be set up to research big data|publisher=[[BBC News]]|accessdate=2014-03-19|date=19 March 2014}}&lt;/ref&gt;

At the [[University of Waterloo Stratford Campus]] Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.&lt;ref&gt;{{cite web|url=http://www.betakit.com/event/inspiration-day-at-university-of-waterloo-stratford-campus/|title=Inspiration day at University of Waterloo, Stratford Campus |publisher=betakit.com/|accessdate=2014-02-28}}&lt;/ref&gt;

To make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research center for Intelligent Maintenance Systems (IMS) at [[university of Cincinnati]] to focus on developing advanced predictive tools and techniques to be applicable in a big data environment.&lt;ref&gt;{{cite journal|last=Lee|first=Jay|author2=Lapira, Edzel |author3=Bagheri, Behrad |author4= Kao, Hung-An |title=Recent Advances and Trends in Predictive Manufacturing Systems in Big Data Environment|journal=Manufacturing Letters|year=2013|volume=1|issue=1|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114|DOI=10.1016/j.mfglet.2013.09.005 |pages=38&#8211;41}}&lt;/ref&gt; In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in big data environment.

Computational social sciences&amp;nbsp;&#8211; Anyone can use Application Programming Interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences.&lt;ref name=pigdata&gt;{{cite journal|last=Reips|first=Ulf-Dietrich|author2=Matzat, Uwe |title=Mining "Big Data" using Big Data Services |journal=International Journal of Internet Science|year=2014|volume=1|issue=1|pages=1&#8211;8 | url=http://www.ijis.net/ijis9_1/ijis9_1_editorial_pre.html}}&lt;/ref&gt; Often these APIs are provided for free.&lt;ref name="pigdata" /&gt; [[Tobias Preis]] ''et al.'' used [[Google Trends]] data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators.&lt;ref&gt;{{cite journal |first1=Tobias |last1=Preis |first2=Helen Susannah |last2=Moat, |first3=H. Eugene |last3=Stanley |first4=Steven R. |last4=Bishop |title=Quantifying the Advantage of Looking Forward |journal=Scientific Reports |volume= 2 |page=350 |year=2012 |doi=10.1038/srep00350 |pmid=22482034 |pmc=3320057}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.newscientist.com/article/dn21678-online-searches-for-future-linked-to-economic-success.html | title=Online searches for future linked to economic success |first=Paul |last=Marks |work=New Scientist | date=5 April 2012 | accessdate=9 April 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://arstechnica.com/gadgets/news/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations.ars | title=Google Trends reveals clues about the mentality of richer nations |first=Casey |last=Johnston |work=Ars Technica | date=6 April 2012 | accessdate=9 April 2012}}&lt;/ref&gt; The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year ('2011') to the volume of searches for the previous year ('2009'), which they call the '[[future orientation index]]'.&lt;ref&gt;{{cite web | url = http://www.tobiaspreis.de/bigdata/future_orientation_index.pdf | title = Supplementary Information: The Future Orientation Index is available for download | author = Tobias Preis | date = 24 May 2012 | accessdate = 2012-05-24}}&lt;/ref&gt; They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.

[[Tobias Preis]] and his colleagues [[Helen Susannah Moat]] and [[H. Eugene Stanley]] introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.&lt;ref&gt;{{cite web | url=http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=[[Philip Ball]] | work=Nature | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt; Their analysis of [[Google]] search volume for 98 terms of varying financial relevance, published in ''[[Scientific Reports]]'',&lt;ref&gt;{{cite journal | author=Tobias Preis, Helen Susannah Moat and H. Eugene Stanley | title=Quantifying Trading Behavior in Financial Markets Using Google Trends | journal=[[Scientific Reports]] | volume= 3 | pages=1684 | year=2013 | doi=10.1038/srep01684 | pmid=23619126 | pmc=3635219}}&lt;/ref&gt; suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.&lt;ref&gt;{{cite news | url=http://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/ | title= Google Search Terms Can Predict Stock Market, Study Finds | author=Nick Bilton | work=[[New York Times]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/ | title=Trouble With Your Investment Portfolio? Google It! | author=Christopher Matthews | work=[[TIME Magazine]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url= http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=Philip Ball |work=[[Nature (journal)|Nature]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.businessweek.com/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets | title='Big Data' Researchers Turn to Google to Beat the Markets | author=Bernhard Warner | work=[[Bloomberg Businessweek]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html | title=Hamish McRae: Need a valuable handle on investor sentiment? Google it | author=Hamish McRae | work=[[The Independent]] | date=28 April 2013 | accessdate=9 August 2013 | location=London}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.ft.com/intl/cms/s/0/e5d959b8-acf2-11e2-b27f-00144feabdc0.html | title= Google search proves to be new word in stock market prediction | author=Richard Waters | work=[[Financial Times]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.forbes.com/sites/davidleinweber/2013/04/26/big-data-gets-bigger-now-google-trends-can-predict-the-market/ | title=Big Data Gets Bigger: Now Google Trends Can Predict The Market | author=David Leinweber | work=[[Forbes]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.bbc.co.uk/news/science-environment-22293693 | title=Google searches predict market moves | author=Jason Palmer | work=[[BBC]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;

Big data sets come with algorithmic challenges that previously did not exist. Hence, there is a need to fundamentally change the processing ways.&lt;ref&gt;E. Sejdi&#263;, "Adapt current tools for use with big data," ''Nature,'' vol. vol. 507, no. 7492, pp. 306, Mar. 2014.&lt;/ref&gt;

The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data.&lt;ref&gt;
Stanford.
[http://web.stanford.edu/group/mmds/ "MMDS. Workshop on Algorithms for Modern Massive Data Sets"].
&lt;/ref&gt;

=== Sampling big data ===
An important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But [[Sampling (statistics)]] enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. Is it necessary to look at all of them to determine the topics that are discussed during the day? Is it necessary to look at all the tweets to determine the sentiment on each of the topics? In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage and controller data are available at short time intervals. To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.  Big Data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data.  With large sets of data points, marketers are able to create and utilize more customized segments of consumers for more strategic targeting.

There has been some work done in Sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.&lt;ref&gt;{{cite conference |author1=Deepan Palguna |author2=Vikas Joshi |author3=Venkatesan Chakaravarthy |author4=Ravi Kothari |author5=L. V. Subramaniam |last-author-amp=yes | title=Analysis of Sampling Algorithms for Twitter | journal=[[International Joint Conference on Artificial Intelligence]] | year=2015 }}&lt;/ref&gt;

== Critique ==
Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.&lt;ref&gt;{{cite journal | doi = 10.1002/joe.21642 | title = Big Data and Business Intelligence: Debunking the Myths | journal = Global Business and Organizational Excellence| volume = 35 | issue = 1 | pages = 23&#8211;34 | year = 2015 | last1 = Kimble | first1 = C. | last2 = Milolidakis | first2 = G. }}&lt;/ref&gt; One approach to this criticism is the field of [[Critical data studies]].

=== Critiques of the big data paradigm ===
"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data".&lt;ref name="Editorial" /&gt; In their critique, Snijders, Matzat, and [[Ulf-Dietrich Reips|Reips]] point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at [[Chris Anderson (writer)|Chris Anderson]]'s assertion that big data will spell the end of theory:&lt;ref&gt;{{cite web|url=http://www.wired.com/science/discoveries/magazine/16-07/pb_theory|title=The End of Theory: The Data Deluge Makes the Scientific Method Obsolete|author=Chris Anderson|date=23 June 2008|work=WIRED}}&lt;/ref&gt; focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.&lt;ref&gt;{{cite news |author=Graham M. |title=Big data and the end of theory? |newspaper=The Guardian |url=https://www.theguardian.com/news/datablog/2012/mar/09/big-data-theory |location=London |date=9 March 2012}}&lt;/ref&gt; Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analysed, must be complemented by "big judgment," according to an article in the Harvard Business Review.&lt;ref&gt;{{cite web|title=Good Data Won't Guarantee Good Decisions. Harvard Business Review|url=http://hbr.org/2012/04/good-data-wont-guarantee-good-decisions/ar/1|work=Shah, Shvetank; Horne, Andrew; Capell&#225;, Jaime;|publisher=HBR.org|accessdate=8 September 2012}}&lt;/ref&gt;

Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably "informed by the world as it was in the past, or, at best, as it currently is".&lt;ref name="HilbertBigData2013"&gt;{{cite web|url=http://papers.ssrn.com/abstract=2205145|title=Big Data for Development: From Information- to Knowledge Societies|publisher=}}&lt;/ref&gt;  Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past.&lt;ref name="HilbertTEDx"&gt;[https://www.youtube.com/watch?v=UXef6yfJZAI Big Data requires Big Visions for Big Change.], Hilbert, M. (2014). London: TEDxUCL, x=independently organized TED talks&lt;/ref&gt; If the systems dynamics of the future change (if it is not a [[stationary process]]), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.&lt;ref name="HilbertTEDx"/&gt;  As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as [[agent-based model]]s&lt;ref name="HilbertBigData2013" /&gt; and [[Complex Systems]]. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.&lt;ref&gt;{{cite web|url=http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/|title=Seeing Around Corners|author=Jonathan Rauch|date=1 April 2002|work=The Atlantic}}&lt;/ref&gt;&lt;ref&gt;Epstein, J. M., &amp; Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.&lt;/ref&gt; In addition, use of multivariate methods that probe for the latent structure of the data, such as [[factor analysis]] and [[cluster analysis]], have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.

In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.&lt;ref&gt;[http://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pdf#page=5 Delort P., Big data in Biosciences, Big Data Paris, 2012]&lt;/ref&gt;
A new postulate is accepted now in biosciences: the information provided by the data in huge volumes ([[omics]]) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.&lt;ref&gt;{{cite web|url=http://www.cs.cmu.edu/~durand/03-711/2011/Literature/Next-Gen-Genomics-NRG-2010.pdf|title=Next-generation genomics: an integrative approach|date=July 2010|publisher=nature|accessdate=18 October 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.researchgate.net/publication/283298499_BIG_DATA_IN_BIOSCIENCES|title=BIG DATA IN BIOSCIENCES|date=October 2015|publisher=ResearchGate|accessdate=18 October 2016}}&lt;/ref&gt; In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.&lt;ref&gt;{{cite web|url=https://next.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0|title=Big data: are we making a big mistake?|date=28 March 2014|publisher=Financial Times|accessdate=20 October 2016}}&lt;/ref&gt; The search logic is reversed and the limits of induction ("Glory of Science and Philosophy scandal", [[C. D. Broad]], 1926) are to be considered.{{Citation needed|date=April 2015}}

[[Consumer privacy|Privacy]] advocates are concerned about the threat to privacy represented by increasing storage and integration of [[personally identifiable information]]; expert panels have released various policy recommendations to conform practice to expectations of privacy.&lt;ref&gt;{{cite web |first=Paul |last=Ohm |title=Don't Build a Database of Ruin |publisher=Harvard Business Review |url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html}}&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/12/03/iron-cagebook/ Iron Cagebook &#8211; The Logical End of Facebook's Patents],'' [[Counterpunch.org]], 2013.12.03&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/  Inside the Tech industry&#8217;s Startup Conference],'' [[Counterpunch.org]], 2013.09.11&lt;/ref&gt;

=== Critiques of big data execution ===
Big data has been called a "fad" in scientific research and its use was even made fun of as an absurd practice in a satirical example on "pig data".&lt;ref name="pigdata" /&gt; Researcher [[Danah Boyd]] has raised concerns about the use of big data in science neglecting principles such as choosing a [[Sampling (statistics)|representative sample]] by being too concerned about actually handling the huge amounts of data.&lt;ref name="danah"&gt;{{cite web | url=http://www.danah.org/papers/talks/2010/WWW2010.html | title=Privacy and Publicity in the Context of Big Data | author=[[danah boyd]] | work=[[World Wide Web Conference|WWW 2010 conference]] | date=29 April 2010 | accessdate = 2011-04-18}}&lt;/ref&gt; This approach may lead to results [[Bias (statistics)|bias]] in one way or another. Integration across heterogeneous data resources&#8212;some that might be considered big data and others not&#8212;presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.&lt;ref&gt;{{cite journal |last1=Jones |first1=MB |last2=Schildhauer |first2=MP |last3=Reichman |first3=OJ |last4=Bowers |first4=S |title=The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere |journal=Annual Review of Ecology, Evolution, and Systematics |volume=37 |issue=1 |pages=519&#8211;544 |year=2006 |doi=10.1146/annurev.ecolsys.37.091305.110031 |url=http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf |format=PDF}}&lt;/ref&gt;
In the provocative article "Critical Questions for Big Data",&lt;ref name="danah2"&gt;{{cite journal | doi = 10.1080/1369118X.2012.678878| title = Critical Questions for Big Data| journal = Information, Communication &amp; Society| volume = 15| issue = 5| pages = 662&#8211;679| year = 2012| last1 = Boyd | first1 = D. | last2 = Crawford | first2 = K. }}&lt;/ref&gt; the authors title big data a part of [[mythology]]: "large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy". Users of big data are often "lost in the sheer volume of numbers", and "working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth".&lt;ref name="danah2" /&gt; Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of big data, through automated [[Filter (software)|filtering]] of non-useful data and correlations.&lt;ref name="Big Decisions White Paper"&gt;[http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf Failure to Launch: From Big Data to Big Decisions], Forte Wares.&lt;/ref&gt;

Big data analysis is often shallow compared to analysis of smaller data sets.&lt;ref name="kdnuggets-berchthold"&gt;{{cite web|url=http://www.kdnuggets.com/2014/08/interview-michael-berthold-knime-research-big-data-privacy-part2.html|title=Interview: Michael Berthold, KNIME Founder, on Research, Creativity, Big Data, and Privacy, Part 2|date=12 August 2014|author=Gregory Piatetsky|authorlink=Gregory I. Piatetsky-Shapiro|publisher=KDnuggets|accessdate=2014-08-13}}&lt;/ref&gt; In many big data projects, there is no large data analysis happening, but the challenge is the [[extract, transform, load]] part of data preprocessing.&lt;ref name="kdnuggets-berchthold" /&gt;

Big data is a [[buzzword]] and a "vague term",&lt;ref&gt;{{cite web|last1=Pelt|first1=Mason|title="Big Data" is an over used buzzword and this Twitter bot proves it|url=http://siliconangle.com/blog/2015/10/26/big-data-is-an-over-used-buzzword-and-this-twitter-bot-proves-it/|website=siliconangle.com|publisher=SiliconANGLE|accessdate=4 November 2015}}&lt;/ref&gt;&lt;ref name="ft-harford"&gt;{{cite web |url=http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html |title=Big data: are we making a big mistake? |last1=Harford |first1=Tim |date=28 March 2014 |website=[[Financial Times]] |publisher=[[Financial Times]] |accessdate=2014-04-07}}&lt;/ref&gt; but at the same time an "obsession"&lt;ref name="ft-harford" /&gt; with entrepreneurs, consultants, scientists and the media. Big data showcases such as [[Google Flu Trends]] failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, [[Academy awards]] and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; and adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. [[Google Translate]]&#8212;which is based on big data statistical analysis of text&#8212;does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the [[multiple comparisons problem]]: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that "most published research findings are false"&lt;ref name="Ioannidis"&gt;{{cite journal | last1 = Ioannidis | first1 = J. P. A. | authorlink1 = John P. A. Ioannidis| title = Why Most Published Research Findings Are False | journal = PLoS Medicine | volume = 2 | issue = 8 | pages = e124 | year = 2005 | pmid = 16060722 | pmc = 1182327 | doi = 10.1371/journal.pmed.0020124}}&lt;/ref&gt; due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a "significant" result being actually false grows fast &#8211; even more so, when only positive results are published.
&lt;!-- sorry, this started overlapping with above section more and more... merging is welcome; I already dropped the intended subheadline "Hype cycle and inflated expectations". --&gt;
Furthermore, big data analytics results are only as good as the model on which they are predicated.  In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election&lt;ref&gt;{{Cite news|url=http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html|title=How Data Failed Us in Calling an Election|last=Lohr|first=Steve|date=2016-11-10|last2=Singer|first2=Natasha|newspaper=The New York Times|issn=0362-4331|access-date=2016-11-27}}&lt;/ref&gt; with varying degrees of success.  Forbes predicted "If you believe in ''Big Data'' analytics, it&#8217;s time to begin planning for a Hillary Clinton presidency and all that entails.".&lt;ref&gt;{{Cite news|url=http://www.forbes.com/sites/jonmarkman/2016/08/08/big-data-and-the-2016-election/#4802f20846d7|title=Big Data And The 2016 Election|last=Markman|first=Jon|newspaper=Forbes|access-date=2016-11-27}}&lt;/ref&gt;

== See also ==
{{portal|Information technology}}
{{Category see also|LABEL=For a list of companies, and tools, see also|Big data}}
&lt;!-- NO COMPANIES OR TOOL SPAM HERE. That would be an endless list! "See also" concepts, not linked above. --&gt;
* [[Big memory]]
* [[Datafication]]
* [[Data defined storage]]
* [[Data journalism]]
* [[Data lineage]]
* [[Data philanthropy]]
* [[Data science]]
* [[Machine learning]]
* [[Statistics]]
* [[Small data]]
* [[Urban informatics]]
* [[List of buzzwords]]

== References ==
{{Reflist|30em}}

==Further reading==
*{{cite magazine|editors=Peter Kinnaird, Inbal Talgam-Cohen|series=[[XRDS (magazine)|XRDS: Crossroads, The ACM Magazine for Students]]|title=Big Data|issue=19 (1)|date=2012|publisher=[[Association for Computing Machinery]]|issn=1528-4980 |oclc=779657714 |url=http://dl.acm.org/citation.cfm?id=2331042}}
*{{cite book|title=Mining of massive datasets|author1=[[Jure Leskovec]]|author2=[[Anand Rajaraman]]|author3=[[Jeffrey D. Ullman]]|year=2014|publisher=Cambridge University Press|url=http://mmds.org/|isbn=9781107077232 |oclc=888463433}}
*{{cite book|author1=[[Viktor Mayer-Sch&#246;nberger]]|author2=[[Kenneth Cukier]]|title=Big Data: A Revolution that Will Transform how We Live, Work, and Think|date=2013|publisher=Houghton Mifflin Harcourt|isbn=9781299903029 |oclc=828620988}}
*{{cite web |url=http://www.forbes.com/sites/gilpress/2013/05/09/a-very-short-history-of-big-data |title=A Very Short History Of Big Data |first=Gil |last=Press |work=forbes.com |date=2013-05-09 |accessdate=2016-09-17 |publisher=[[Forbes Magazine]] |location=Jersey City, NJ}}

== External links ==
*{{Commonsinline}}
* {{Wiktionary-inline|big data}}

{{Use dmy dates|date=December 2015}}
{{Authority control}}

[[Category:Big data| ]]
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]</text>
      <sha1>6tg9nbf2y1isei0drc0xvul3qkvuzko</sha1>
    </revision>
  </page>
  <page>
    <title>Data processing system</title>
    <ns>0</ns>
    <id>466099</id>
    <revision>
      <id>759707342</id>
      <parentid>759706902</parentid>
      <timestamp>2017-01-12T19:03:41Z</timestamp>
      <contributor>
        <username>Marianna251</username>
        <id>27604025</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/103.59.36.58|103.59.36.58]] ([[User talk:103.59.36.58|talk]]) ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5308" xml:space="preserve">{{other uses2|Data processing}}
{{refimprove|date=July 2013}}

A '''data processing system''' is a combination of machines, &lt;!-- "data processing" is specific to machines, there is no data processing in nature, see the OED --&gt; people, and processes that for a set of inputs produces a defined set of outputs.&lt;ref&gt;The first machines used for data processing were [[Unit record equipment|punched card machines]], now [[Computer]]s are used.&lt;/ref&gt;  The inputs and outputs are interpreted as data, facts, information, ... depending on the interpreter's relation to the system. A common synonymous term is "[[Information systems#Types of information systems|information system]]".&lt;ref name=Ralston&gt;{{cite book|title=Encyclopedia of Computer Science 4th ed.|author=Anthony Ralston et al (ed.)|year=2000|publisher=Nature Publishing Group|page=865}}&lt;/ref&gt;

A data processing system may involve some combination of:
* [[Data conversion|Conversion]] converting data to another format.
* [[Data validation|Validation]] &amp;ndash; Ensuring that supplied data is "clean, correct and useful."
* [[Sorting]] &amp;ndash; "arranging items in some sequence and/or in different sets."
* [[Summary statistic|Summarization]] &amp;ndash; reducing detail data to its main points.
* [[Aggregate data|Aggregation]] &amp;ndash; combining multiple pieces of data.
* [[Statistical analysis|Analysis]] &amp;ndash; the "collection, organization, analysis, interpretation and presentation of data.".
* Reporting &amp;ndash; list detail or summary data or computed information.

==Types of data processing systems==

===By application area===

====Scientific data processing====
Scientific data processing "usually involves a great deal of computation (arithmetic and comparison operations) upon a relatively small amount of input data, resulting in a small volume of output." &lt;ref name=Reddy&gt;{{cite book|last=Reddy|first=R.J.|title=Business Data Processing &amp; Computer Applications|year=2004|publisher=A P H Publishing Corporation|location=New Dehli|isbn=8176486493|page=17|url=https://books.google.com/books?id=FLKoXCts9ssC&amp;lpg=PA17&amp;dq=%22scientific%20data%20processing%22&amp;pg=PA17#v=onepage&amp;q=%22scientific%20data%20processing%22&amp;f=false}}&lt;/ref&gt;

====Commercial data processing====
Commercial data processing "involves a large volume of input data, relatively few computational operations, and a large volume of output."&lt;ref name=Reddy /&gt;  Accounting programs are the prototypical examples of data processing applications. [[Information systems|Information systems (IS)]] is the field that studies such organizational computer systems.

====Data analysis====
"[[Data analysis]] is a body of methods that help to describe facts, detect patterns,
develop explanations, and test hypotheses."&lt;ref&gt;{{cite web|last=Dartmouth College|title=Introduction: What Is Data Analysis?|url=http://www.dartmouth.edu/~mss/data%20analysis/Volume%20I%20pdf%20/006%20Intro%20%28What%20is%20the%20weal.pdf|accessdate=July 5, 2013}}&lt;/ref&gt;  For example, data analysis might be used to look at sales and customer data to "identify connections between products to allow for cross selling campaigns."&lt;ref&gt;{{cite book|last1=Berthold|first1=M.R.|last2=Borgelt|first2=C|last3=H&#337;ppner|first3=F.|last4=Klawonn|first4=F|title=Guide to Intelligent Data Analysis|year=2010|publisher=Springer|isbn=978-1-84882-260-3|page=15}}&lt;/ref&gt;

===By service type&lt;ref name=Ralston /&gt;=== 

* [[Transaction processing system|Transaction processing systems]]
* [[Information retrieval|Information storage and retrieval systems]]
* Command and control systems
* Computing service systems
* [[Control system|Process control systems]]
* Message switching systems

==Examples==
===Simple example===
A very simple example of a data processing system is the process of maintaining a check register.  Transactions&amp;mdash; checks and deposits&amp;mdash; are recorded as they occur and the transactions are summarized to determine a current balance.  Monthly the data recorded in the register is reconciled with a hopefully identical list of transactions processed by the bank.

A more sophisticated record keeping system might further identify the transactions&amp;mdash; for example deposits by source or checks by type, such as charitable contributions.  This information might be used to obtain information like the total of all contributions for the year.

The important thing about this example is that it is a ''system'', in which, all transactions are recorded consistently, and the same method of bank reconciliation is used each time.

===Real-world example===
This is a [[flowchart]] of a data processing system combining manual and computerized processing to handle [[accounts receivable]], billing, and [[general ledger]]

[[File:Stockbridge system flowchart example.jpg]]
&lt;ref&gt;the highest acceleration of data processing the point of software&lt;/ref&gt;

==References==
{{Reflist}}

== See also ==
* [[Data processing]]
* [[Electronic data processing]]
* [[Computational science|Scientific computing]]
* [[Information processing system]] (broader term)

== Further reading ==
* Bourque, Linda B.; Clark, Virginia A. (1992) Processing Data: The Survey Example. (Quantitative Applications in the Social Sciences, no. 07-085). Sage Publications. ISBN 0-8039-4741-0


[[Category:Data management]]
[[Category:Data processing]]</text>
      <sha1>947vdp8np5v69av8s4tqs3n7ry8kiki</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data management software</title>
    <ns>14</ns>
    <id>32164666</id>
    <revision>
      <id>701110853</id>
      <parentid>546175104</parentid>
      <timestamp>2016-01-22T15:32:53Z</timestamp>
      <contributor>
        <username>Horcrux92</username>
        <id>10845682</id>
      </contributor>
      <comment>new key for [[Category:Data management]]: "Software" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="458" xml:space="preserve">[[Software]], typically proprietary products or open-source projects, with a primary purpose of [[data management]].  [[:Category:Database management systems by license|Database management system software]] could be considered a related category, though those will typically exist for the purpose of managing a [[database]] in a particular structure (i.e. relational, object-oriented). 
[[Category:Data management|Software]]
[[Category:Application software]]</text>
      <sha1>948qwe30ruw7cmbyqb8sxgu80cydeca</sha1>
    </revision>
  </page>
  <page>
    <title>Secure Electronic Delivery</title>
    <ns>0</ns>
    <id>1282406</id>
    <revision>
      <id>726442037</id>
      <parentid>726441706</parentid>
      <timestamp>2016-06-22T05:53:04Z</timestamp>
      <contributor>
        <username>Aptiva07</username>
        <id>28290881</id>
      </contributor>
      <comment>Correcting wikilink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4212" xml:space="preserve">'''Secure Electronic Delivery'''  (SED) is a service created in 2003 and provided by the  [[British Library#Document Supply Service|British Library Document Supply Service]] (BLDSS). Its purpose is to enable faster delivery of digital materials as [[Encryption|encrypted]], copyright-compliant [[Portable Document Format| PDF Document]]s, to a personal e-mail address. These documents are supplied from the British Library via its On Demand service.&lt;ref&gt;{{cite web |url=http://www.bl.uk/sed   |title=Secure Electronic Delivery  |author=  |publisher=[[British Library]] |date=  |accessdate= }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.bl.uk/reshelp/atyourdesk/docsupply/help/receiving/deliveryoptions/electronic/sed/sedhelpsheetfinal.pdf  |title= Secure Electronic Delivery &#8211; Technical Helpsheet  |author=  |publisher=[[British Library]] |date=  |accessdate= }}&lt;/ref&gt; When the British Library supplies articles electronically, it sends them securely in order to ensure its usage is permitted (research purposes) and copyright law is observed.

==Methods==
As the [[publishing | publishing industry]], authors and creators become highly protective of their assets and [[intellectual property]], they impose strict rules on delivery methods to prevent [[copyright infringement]]. Nowadays, [[Digital rights management|DRM]]-enabled secure delivery appears to be the most widely used solution to address issues faced by libraries in supplying ebooks and digital materials to their users.&lt;ref&gt;{{cite news  | title=Secure E-mail Delivery Poised to Take Off  |url=https://books.google.com.mx/books?id=PWHbLjAQ57gC&amp;pg=PA38&amp;lpg=PA38&amp;dq=document+secure+delivery+technology&amp;source=bl&amp;ots=YgfE16c0Yy&amp;sig=qd0R1j9LtZI6_hJi6zqHCGOBzfQ&amp;hl=en&amp;sa=X&amp;redir_esc=y#v=onepage&amp;q=document%20secure%20delivery%20technology&amp;f=false   |date=23 August 1999 |author=Dominique Deckmyn  |newspaper=[[Computerworld]] }}&lt;/ref&gt;&lt;ref&gt;{{cite web  |title= Practical problems for libraries distributing ebooks &amp; secure electronic delivery |url=http://www.locklizard.com/libraries-secure-electronic-delivery/   |publisher=Locklizard Limited |date=  |accessdate= }}&lt;/ref&gt;  SED, one of these solutions, is using [[Adobe LiveCycle]] Digital Rights Management (LCDRM) as an encryption method to deliver documents.&lt;ref&gt;{{cite web |url=http://www.lancaster.ac.uk/library/using-the-library/interlending-and-document-supply/secure-electronic-delivery/ |title=British Library On Demand Electronic Delivery  |author=  |publisher=[[Lancaster University]] Library  |date=  |accessdate= }}&lt;/ref&gt;

==Advantages==
SED offers convenience, quality and speed as documents are delivered upon request at any location and on any device. Requested articles are scanned for high quality reproduction, opened anywhere on any machine, including mobile devices.&lt;ref&gt;{{cite web |url= http://www.brad.ac.uk/library/media/library/interlibraryloans/sed.pdf |title=SED &#8211; Secure Electronic Delivery  |author=  |publisher=[[University of Bradford]] |date=  |accessdate= }}&lt;/ref&gt;

== Restrictions==
The following are restrictions hold in a SED service implementation:
*  The digital material is accessible only for 14 days via a link sent to a personal message.
* Due to copyright reasons,&lt;ref&gt;{{cite journal |last=Eiblum |first= Paula   |last2= Ardito |first2= Stephanie    |date= September 1999 |title= Document Delivery &amp; Copyright: Librarians Take the Fifth |url= |journal=Online (magazine) |publisher= |volume=23 |issue=5 |pages=74&#8211;77  |doi= |access-date=7 May 2016}}&lt;/ref&gt;  the material can be opened only once, saved for 14 days and does not allow a copy-paste action.
* Upon display, the material must be printed from the same device and reprinted only once.
* The On Demand encryption technology works best on the default Safari browser although other browsers may accommodate it.

==See also==
* [[Digital rights management]]
* [[Digital asset management]]

==References==
{{Reflist}}

==External links==
[http://www.bl.uk/sed SED Web page]

{{DEFAULTSORT:Secure Electronic Delivery}}
[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Document management systems]]
[[Category:Data management]]
[[Category:Secure communication]]</text>
      <sha1>4ph6o5iya7a4l08dijsyji09hi22iop</sha1>
    </revision>
  </page>
  <page>
    <title>Big memory</title>
    <ns>0</ns>
    <id>51756257</id>
    <revision>
      <id>760003267</id>
      <parentid>741524795</parentid>
      <timestamp>2017-01-14T10:52:48Z</timestamp>
      <contributor>
        <username>DrStrauss</username>
        <id>29858946</id>
      </contributor>
      <comment>rm tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1923" xml:space="preserve">'''Big-memory''' is a term used to describe server workloads which need to run on machines with a large amount of RAM ([[Random-access memory]]) memory. Some example workloads are databases, in-memory caches, and graph analytics.&lt;ref&gt;{{cite web |url=http://research.cs.wisc.edu/multifacet/papers/isca13_direct_segment.pdf|title=Efficient Virtual Memory for Big Memory Servers|accessdate=2016-09-24 }}&lt;/ref&gt;
Or, more generally, [[Data Science]] and [[Big data]].

Some database systems are designed to run mostly in memory, rarely if ever retrieving data from disk or flash memory. See a [[List of in-memory databases]].

The performance of big memory systems depends on how the CPU's or CPU cores access the memory, via a conventional [[Memory controller]] or via NUMA ( [[Non-uniform memory access]] ). Performance also depends on the size and design of the [[CPU cache]].

Performance also depends on OS design. The "Huge pages" feature in Linux can improve the efficiency of [[Virtual Memory]].&lt;ref&gt;{{cite web |url=http://lwn.net/Articles/374424/ |title=Huge pages part 1 (Introduction)  |accessdate=2016-09-24 }}&lt;/ref&gt; The new "Transparent huge pages" feature in Linux can offer better performance for some big-memory workloads.&lt;ref&gt;{{cite web |url=http://lwn.net/Articles/423584/ |title=Transparent huge pages in 2.6.38 |accessdate=2016-09-24 }}&lt;/ref&gt; The "Large-Page Support" in Microsoft Windows enables server applications to establish large-page memory regions which are typically three orders of magnitude larger than the native page size.&lt;ref&gt;{{cite web |url=https://msdn.microsoft.com/en-us/library/windows/desktop/aa366720(v=vs.85).aspx|title=Large-Page Support |accessdate=2016-09-24 }}&lt;/ref&gt;

==References==
{{reflist}}


{{database-stub}}
[[Category:Big data| ]]
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]</text>
      <sha1>3qhz8zy29c0wfpnz9xpth1cqwagnu6v</sha1>
    </revision>
  </page>
  <page>
    <title>Metadata</title>
    <ns>0</ns>
    <id>18933632</id>
    <revision>
      <id>762719018</id>
      <parentid>762687184</parentid>
      <timestamp>2017-01-30T10:20:09Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/14.223.186.17|14.223.186.17]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="63813" xml:space="preserve">{{pp-move-indef|small=yes}}
{{use dmy dates|date=October 2016}}
[[File:Schlagwortkatalog.jpg|thumb|200px|In the 2010s, metadata typically refers to digital forms; however, even traditional card catalogues from the 1960s and 1970s are an example of metadata, as the cards contain information about the books in the library (author, title, subject, etc.).]]

'''Metadata''' is "[[data]] [information] that provides information about other data".&lt;ref&gt;http://www.merriam-webster.com/dictionary/metadata&lt;/ref&gt; Three distinct types of metadata exist: '''descriptive metadata''', '''structural metadata''', and '''administrative metadata'''.&lt;ref name="Metadata Basics Outline"&gt;{{cite web | url=http://marciazeng.slis.kent.edu/metadatabasics/types.htm | title=Metadata Types and Functions | publisher=NISO | date=2004 | accessdate=5 October 2016 | author=Zeng, Marcia}}&lt;/ref&gt;

* Descriptive metadata describes a resource for purposes such as discovery and identification. It can include elements such as title, abstract, author, and keywords.
* Structural metadata is metadata about containers of metadata and indicates how compound objects are put together, for example, how pages are ordered to form chapters.
* Administrative metadata provides information to help manage a resource, such as when and how it was created, file type and other technical information, and who can access it.&lt;ref name="Understanding Metadata (2)"&gt;{{cite book | url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf | title=Understanding Metadata | publisher=NISO Press | author=National Information Standards Organization (NISO) | year=2001 | isbn=1-880124-62-9|page=1}}&lt;/ref&gt;

== History ==
Metadata was traditionally used in the [[library catalog|card catalogs]] of [[library|libraries]] until the 1980s, when libraries converted their catalog data to digital databases. In the 2000s, as digital formats are becoming the prevalent way of storing data and information, metadata is also used to describe digital data using [[metadata standards]].

There are different metadata standards for each different discipline (e.g., [[museum]] collections, [[digital audio file]]s, [[website]]s, etc.). Describing the [[Content (media)|contents]] and [[Context (computing)|context]] of data or [[computer file|data files]] increases its usefulness. For example, a [[web page]] may include metadata specifying what software language the page is written in (e.g., HTML), what tools were used to create it, what subjects the page is about, and where to find more information about the subject. This metadata can automatically improve the reader's experience and make it easier for users to find the web page online.&lt;ref name="Practices in Using Metadata"&gt;{{cite web | url=http://www.library.illinois.edu/dcc/bestpractices/chapter_11_structuralmetadata.html | title=Best Practices for Structural Metadata | publisher=University of Illinois | date=15 December 2010 | accessdate=17 June 2016}}&lt;/ref&gt; A [[CD]] may include metadata providing information about the musicians, singers and songwriters whose work appears on the disc.

A principal purpose of metadata is to help users find relevant information and discover resources. Metadata also helps to organize electronic resources, provide digital identification, and support the archiving and preservation of resources. Metadata assists users in resource discovery by "allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information."&lt;ref name = Understanding_Metadata/&gt; Metadata of telecommunication activities including [[Internet]] traffic is very widely collected by various national governmental organizations. This data is used for the purposes of [[traffic analysis]] and can be used for mass [[surveillance]].&lt;ref&gt;https://www.schneier.com/essays/archives/2014/03/metadata_surveillanc.html&lt;/ref&gt;

In many countries, the metadata relating to emails, telephone calls, web pages, video traffic, IP connections and cell phone locations are routinely stored by government organizations.&lt;ref name="NSA_Watching"&gt;http://www.washingtonsblog.com/2014/03/nsa-recorded-every-single-call-one-country-country-america.html&lt;/ref&gt;

== Definition ==
Metadata means "data about data". Although the "meta" prefix (from the [[Greek language|Greek]] [[preposition]] and [[prefix]] &#956;&#949;&#964;&#940;-) means "after" or "beyond", it is used to mean "about" in [[epistemology]]. Metadata is defined as the data providing information about one or more aspects of the data; it is used to summarize basic information about data which can make tracking and working with specific data easier.&lt;ref&gt;{{cite web
| title = A Guardian Guide to your Metadata
| website = [[theguardian.com]]
| publisher = [[Guardian News and Media Limited]]
| date = 12 June 2013
| url = https://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=0000000
}}&lt;/ref&gt; Some examples include:
* Means of creation of the data
* Purpose of the data
* Time and date of creation
* Creator or author of the data
* Location on a [[computer network]] where the data was created
* [[Technical standard|Standards]] used
* File size

For example, a [[digital image]] may include metadata that describes how large the picture is, the color depth, the image resolution, when the image was created, the shutter speed, and other data.&lt;ref&gt;{{cite web|url=http://www.adeoimaging.com |title=ADEO Imaging: TIFF Metadata |accessdate=2013-05-20}}&lt;/ref&gt; A text document's metadata may contain information about how long the document is, who the author is, when the document was written, and a short summary of the document. Metadata within web pages can also contain descriptions of page content, as well as key words linked to the content.&lt;ref name="Rouse, M (2014)"&gt;{{cite web
| last = Rouse
| first = Margaret
| title = Metadata
| work = WhatIs
| publisher = TechTarget
| date = July 2014
| url = http://whatis.techtarget.com/definition/metadata
}}&lt;/ref&gt; These links are often called "Metatags", which were used as the primary factor in determining order for a web search until the late 1990s.&lt;ref name="Rouse, M (2014)"/&gt; The reliance of metatags in web searches was decreased in the late 1990s because of "keyword stuffing".&lt;ref name="Rouse, M (2014)"/&gt; Metatags were being largely misused to trick search engines into thinking some websites had more relevance in the search than they really did.&lt;ref name="Rouse, M (2014)"/&gt;

Metadata can be stored and managed in a [[database]], often called a [[metadata registry]] or [[metadata repository]].&lt;ref&gt;H&#252;ner, K.; Otto, B.; &#214;sterle, H.: Collaborative management of business metadata, in: ''International Journal of Information Management'', 2011&lt;/ref&gt; However, without context and a point of reference, it might be impossible to identify metadata just by looking at it.&lt;ref&gt;{{cite web|url=http://www.bls.gov/ore/pdf/st000010.pdf |title=Metadata Standards And Metadata Registries: An Overview |format=PDF |accessdate=2011-12-23}}&lt;/ref&gt; For example: by itself, a database containing several numbers, all 13 digits long could be the results of calculations or a list of numbers to plug into an equation - without any other context, the numbers themselves can be perceived as the data. But if given the context that this database is a log of a book collection, those 13-digit numbers may now be identified as [[ISBN]]s - information that refers to the book, but is not itself the information within the book. The term "metadata" was coined in 1968 by Philip Bagley, in his book "Extension of Programming Language Concepts" where it is clear that he uses the term in the ISO 11179 "traditional" sense, which is "structural metadata" i.e. "data about the containers of data"; rather than the alternate sense "content about individual instances of data content" or metacontent, the type of data usually found in library catalogues.&lt;ref name=Bagley&gt;{{Cite journal
|author=Philip Bagley
|title=Extension of programming language concepts
|date=November 1968
| url = http://www.dtic.mil/dtic/tr/fulltext/u2/680815.pdf
|publisher=University City Science Center
|location=Philadelphia
}}&lt;/ref&gt;&lt;ref&gt;"The notion of "metadata" introduced by Bagley". {{Cite journal
 | last = Solntseff
 | first = N+1
 | last2 = Yezerski
 | first2 = A
 | year = 1974
 | title = A survey of extensible programming languages
 | series = Annual Review in Automatic Programming
 | publisher = Elsevier Science Ltd
 | volume = 7
 | pages = 267&#8211;307
 | doi = 10.1016/0066-4138(74)90001-9
}}&lt;/ref&gt; Since then the fields of information management, information science, information technology, librarianship, and [[GIS]] have widely adopted the term. In these fields the word ''metadata'' is defined as "data about data".&lt;ref name=NISO &gt;{{Cite book
| last = NISO
| authorlink =NISO
| title = Understanding Metadata
| publisher = NISO Press
| url = http://www.niso.org/publications/press/UnderstandingMetadata.pdf
| isbn = 1-880124-62-9
| accessdate = 5 January 2010 }}
&lt;/ref&gt;{{page needed|date=November 2016}} While this is the generally accepted definition, various disciplines have adopted their own more specific explanation and uses of the term.

== Types ==
While the metadata application is manifold, covering a large variety of fields, there are specialized and well-accepted models to specify types of metadata. [[Francis Bretherton|Bretherton]] &amp; Singley (1994) distinguish between two distinct classes: structural/control metadata and guide metadata.&lt;ref&gt;{{Cite conference
| first1 = F. P. | last1 = Bretherton | author1-link = Francis Bretherton
|first2 = P.T. | last2 = Singley
| title = Metadata: A User's View, Proceedings of the International Conference on Very Large Data Bases (VLDB)
| pages = 1091&#8211;1094
| publisher =
| year = 1994}}
&lt;/ref&gt; ''Structural metadata'' describes the structure of database objects such as tables, columns, keys and indexes. ''Guide metadata'' helps humans find specific items and are usually expressed as a set of keywords in a natural language. According to [[Ralph Kimball]] metadata can be divided into 2 similar categories: technical metadata and business metadata. ''Technical metadata'' corresponds to internal metadata, and ''business metadata'' corresponds to external metadata. Kimball adds a third category, ''process metadata''. On the other hand, NISO distinguishes among three types of metadata: descriptive, structural, and administrative.&lt;ref name=NISO/&gt;

''Descriptive metadata'' is typically used for discovery and identification, as information to search and locate an object, such as title, author, subjects, keywords, publisher. ''Structural metadata'' describes how the components of an object are organized. An example of structural metadata would be how pages are ordered to form chapters of a book. Finally, ''administrative metadata'' gives information to help manage the source. Administrative metadata refers to the technical information, including file type, or when and how the file was created. Two sub-types of administrative metadata are rights management metadata and preservation metadata. ''Rights management metadata'' explains intellectual property rights, while ''preservation metadata'' contains information to preserve and save a resource.&lt;ref name = Understanding_Metadata&gt;{{cite book|last=National Information Standards Organization|title=Understanding Metadata|year=2004|publisher=NISO Press|location=Bethesda, MD|isbn=1-880124-62-9|url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf |author2=Rebecca Guenther |author3=Jaqueline Radebaugh|accessdate=2 April 2014}}&lt;/ref&gt;{{page needed|date=November 2016}}

== Structures ==
Metadata (metacontent) or, more correctly, the vocabularies used to assemble metadata (metacontent) statements, is typically structured according to a standardized concept using a well-defined metadata scheme, including: [[metadata standards]] and [[Metadata modeling|metadata models]]. Tools such as [[Controlled vocabulary|controlled vocabularies]], [[Taxonomy (general)|taxonomies]], [[Thesaurus (information retrieval)|thesauri]], [[Data Dictionary|data dictionaries]], and [[Metadata registry|metadata registries]] can be used to apply further standardization to the metadata. Structural metadata commonality is also of paramount importance in [[data model]] development and in [[database design]].

=== Syntax ===
Metadata (metacontent) syntax refers to the rules created to structure the fields or elements of metadata (metacontent).&lt;ref&gt;{{cite web
| last = Cathro
| first = Warwick
| authorlink =
| title = Metadata: an overview
| year = 1997
| url = http://www.nla.gov.au/nla/staffpaper/cathro3.html
| accessdate = 6 January 2010
}}&lt;/ref&gt; A single metadata scheme may be expressed in a number of different markup or programming languages, each of which requires a different syntax. For example, Dublin Core may be expressed in plain text, [[HTML]], [[XML]], and [[Resource Description Framework|RDF]].&lt;ref&gt;{{cite web
| last = DCMI
| authorlink =Dublin_Core_Metadata_Initiative
| title = Semantic Recommendations
| date =5 October 2009
| url = http://dublincore.org/specifications/
| accessdate = 6 January 2010
}}&lt;/ref&gt;

A common example of (guide) metacontent is the bibliographic classification, the subject, the [[List of Dewey Decimal classes|Dewey Decimal class number]]. There is always an implied statement in any "classification" of some object. To classify an object as, for example, Dewey class number 514 (Topology) (i.e. books having the number 514 on their spine) the implied statement is: "&lt;nowiki&gt;&lt;book&gt;&lt;subject heading&gt;&lt;514&gt;&lt;/nowiki&gt;. This is a subject-predicate-object triple, or more importantly, a class-attribute-value triple. The first two elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. "metacontent = metadata + master data". All of these elements can be thought of as "vocabulary". Both metadata and master data are vocabularies which can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by [[ISO 25964]]: "If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved."&lt;ref&gt;https://www.iso.org/obp/ui/#iso:std:iso:25964:-1:ed-1:v1:en&lt;/ref&gt; This is particularly relevant when considering search engines of the internet, such as Google. The process indexes pages then matches text strings using its complex algorithm; there is no intelligence or "inferencing" occurring, just the illusion thereof.

=== Hierarchical, linear and planar schemata ===
Metadata schemata can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent-child relationships exist between the elements.
An example of a hierarchical metadata schema is the [[Learning object metadata|IEEE LOM]] schema, in which metadata elements may belong to a parent metadata element.
Metadata schemata can also be one-dimensional, or linear, where each element is completely discrete from other elements and classified according to one dimension only.
An example of a linear metadata schema is the [[Dublin Core Metadata Initiative|Dublin Core]] schema, which is one dimensional.
Metadata schemata are often two dimensional, or planar, where each element is completely discrete from other elements but classified according to two orthogonal dimensions.&lt;ref&gt;{{cite web
| title = Types of Metadata
| publisher = [[University of Melbourne]]
| date = 15 August 2006
| url = http://www.infodiv.unimelb.edu.au/metadata/add_info.html
| accessdate = 6 January 2010
| archiveurl = https://web.archive.org/web/20091024112353/http://www.infodiv.unimelb.edu.au/metadata/add_info.html
| archivedate = 2009-10-24}}&lt;/ref&gt;

=== Hypermapping ===
In all cases where the metadata schemata exceed the planar depiction, some type of [[hypermap]]ping is required to enable display and view of metadata according to chosen aspect and to serve special views. Hypermapping frequently applies to layering of geographical and geological information overlays.&lt;ref&gt;{{cite web |url=http://www.isprs.org/proceedings/XXXII/part4/kuebler51.pdf |title=THE DESIGN AND DEVELOPMENT OF A GEOLOGIC HYPERMAP PROTOTYPE |first1=Stefanie |last1=K&#252;bler |first2=Wolfdietrich |last2=Skala |first3=Agn&#232;s |last3=Voisard}}&lt;/ref&gt;

=== Granularity ===
The degree to which the data or metadata is structured is referred to as its [[Data granularity|"granularity"]]. "Granularity" refers to how much detail is provided. Metadata with a high granularity allows for deeper, more detailed, and more structured information and enables greater levels of technical manipulation. A lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information. The major impact of granularity is not only on creation and capture, but moreover on maintenance costs. As soon as the metadata structures become outdated, so too is the access to the referred data. Hence granularity must take into account the effort to create the metadata as well as the effort to maintain it.

== Standards ==
International standards apply to metadata. Much work is being accomplished in the national and international standards communities, especially [[ANSI]] (American National Standards Institute) and [[International Organization for Standardization|ISO]] (International Organization for Standardization) to reach consensus on standardizing metadata and registries. The core metadata registry standard is [[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 11179 Metadata Registries (MDR), the framework for the standard is described in ISO/IEC 11179-1:2004.&lt;ref&gt;{{cite web
  |url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=39438
  |title=ISO/IEC 11179-1:2004 Information technology - Metadata registries (MDR) - Part 1: Framework
  |publisher=Iso.org |date=2009-03-18 |accessdate=2011-12-23
}}&lt;/ref&gt; A new edition of Part 1 is in its final stage for publication in 2015 or early 2016. It has been revised to align with the current edition of Part 3, ISO/IEC 11179-3:2013&lt;ref&gt;{{cite web
  |url=http://standards.iso.org/ittf/PubliclyAvailableStandards/c050340_ISO_IEC_11179-3_2013.zip
  |title=ISO/IEC 11179-3:2013 Information technology-Metadata registries - Part 3: Registry metamodel and basic attributes
  |publisher=iso.org|date=2014}}&lt;/ref&gt; which extends the MDR to support registration of Concept Systems.
(see [[ISO/IEC 11179]]). This standard specifies a schema for recording both the meaning and technical structure of the data for unambiguous usage by humans and computers. ISO/IEC 11179 standard refers to metadata as information objects about data, or "data about data". In ISO/IEC 11179 Part-3, the information objects are data about Data Elements, Value Domains, and other reusable semantic and representational information objects that describe the meaning and technical details of a data item. This standard also prescribes the details for a metadata registry, and for registering and administering the information objects within a Metadata Registry. ISO/IEC 11179 Part 3 also has provisions for describing compound structures that are derivations of other data elements, for example through calculations, collections of one or more data elements, or other forms of derived data. While this standard describes itself originally as a "data element" registry, its purpose is to support describing and registering metadata content independently of any particular application, lending the descriptions to being discovered and reused by humans or computers in developing new applications, databases, or for analysis of data collected in accordance with the registered metadata content. This standard has become the general basis for other kinds of metadata registries, reusing and extending the registration and administration portion of the standard.

The Geospatial community has a tradition of specialized [[geospatial metadata]] standards, particularly building on traditions of map- and image-libraries and catalogues. Formal metadata is usually essential for geospatial data, as common text-processing approaches are not applicable.

The [[Dublin Core]] metadata terms are a set of vocabulary terms which can be used to describe resources for the purposes of discovery. The original set of 15 classic&lt;ref&gt;{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=2009-12-14 |accessdate=2013-08-17}}&lt;/ref&gt; metadata terms, known as the Dublin Core Metadata Element Set&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=2013-08-17}}&lt;/ref&gt; are endorsed in the following standards documents:
* IETF RFC 5013&lt;ref&gt;{{cite web |url= http://www.ietf.org/rfc/rfc5013.txt |title=The Dublin Core Metadata Element Set |author=J. Kunze, T. Baker |work=ietf.org |year=2007 |accessdate=17 August 2013}}&lt;/ref&gt;
* ISO Standard 15836-2009&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=2009-02-18 |accessdate=2013-08-17}}&lt;/ref&gt;
* NISO Standard Z39.85.&lt;ref&gt;{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&amp;gid=None&amp;project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=2007-05-22 |accessdate=2013-08-17}}&lt;/ref&gt;

Although not a standard, [[Microformat]] (also mentioned in the section [[Metadata#Metadata on the Internet|metadata on the internet]] below) is a web-based approach to semantic markup which seeks to re-use existing HTML/XHTML tags to convey metadata. Microformat follows XHTML and HTML standards but is not a standard in itself. One advocate of microformats, [[Tantek &#199;elik]], characterized a problem with alternative approaches: {{cquote|Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry.&lt;ref name="Wharton000"&gt;{{cite web |title=What's the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&amp;ID=1247}}&lt;/ref&gt;}}

== Use ==

=== Photographs ===
Metadata may be written into a [[digital photo]] file that will identify who owns it, copyright and contact information, what brand or model of camera created the file, along with exposure information (shutter speed, f-stop, etc.) and descriptive information, such as keywords about the photo, making the file or image searchable on a computer and/or the Internet. Some metadata is created by the camera and some is input by the photographer and/or software after downloading to a computer. Most digital cameras write metadata about model number, shutter speed, etc., and some enable you to edit it;&lt;ref&gt;{{cite web|publisher=gurucamera.com|title=How To Copyright Your Photos With Metadata|url=https://gurucamera.com/copyright-photos-metadata/|work=Guru Camera}}&lt;/ref&gt; this functionality has been available on most Nikon DSLRs since the [[Nikon D3]], on most new Canon cameras since the [[Canon EOS 7D]], and on most Pentax DSLRs since the Pentax K-3. Metadata can be used to make organizing in post-production easier with the use of key-wording. Filters can be used to analyze a specific set of photographs and create selections on criteria like rating or capture time.

Photographic Metadata Standards are governed by organizations that develop the following standards. They include, but are not limited to:
* [[IPTC Information Interchange Model]] IIM (International Press Telecommunications Council),
* [[International Press Telecommunications Council|IPTC]] Core Schema for XMP
* [[Extensible Metadata Platform|XMP]] &#8211; Extensible Metadata Platform (an ISO standard)
* [[Exchangeable image file format|Exif]] &#8211; Exchangeable image file format, Maintained by CIPA (Camera &amp; Imaging Products Association) and published by JEITA (Japan Electronics and Information Technology Industries Association)
* [[Dublin Core]] (Dublin Core Metadata Initiative &#8211; DCMI)
* PLUS (Picture Licensing Universal System).
* [http://www.loc.gov/standards/vracore/schemas.html VRA Core] (Visual Resource Association)&lt;ref&gt;{{cite web|title=VRA Core Support Pages|url=http://core.vraweb.org|website=Visual Resource Association Foundation|publisher=Visual Resource Association Foundation|accessdate=27 February 2016}}&lt;/ref&gt;

=== Telecommunications ===
Information on the times, origins and destinations of phone calls, electronic messages, instant messages and other modes of telecommunication, as opposed to message content, is another form of metadata. Bulk collection of this [[call detail record]] metadata by intelligence agencies has proven controversial after disclosures by [[Edward Snowden]] Intelligence agencies such as the NSA are keeping online metadata of millions of internet user for up to a year, regardless of whether or not they are persons of interest to the agency.

=== Video ===
Metadata is particularly useful in video, where information about its contents (such as transcripts of conversations and text descriptions of its scenes) is not directly understandable by a computer, but where efficient search of the content is desirable. There are two sources in which video metadata is derived: (1) operational gathered metadata, that is information about the content produced, such as the type of equipment, software, date, and location; (2) human-authored metadata, to improve search engine visibility, discoverability, audience engagement, and providing advertising opportunities to video publishers.&lt;ref&gt;{{cite web
| last = Webcase
| first = Weblog
| authorlink =
| title = Examining video file metadata
| year = 2011
| url = http://veresoftware.com/blog/?p=364
| accessdate = 25 November 2015
}}&lt;/ref&gt; In today's society most professional video editing software has access to metadata. Avid's MetaSync and Adobe's Bridge are two prime examples of this.&lt;ref&gt;{{cite web
| last = Oak Tree Press
| authorlink =
| title = Metadata for Video
| year = 2011
| url = http://veresoftware.com/blog/?p=364
| accessdate = 25 November 2015
}}&lt;/ref&gt;

=== Web pages ===
[[Web page]]s often include metadata in the form of [[Meta element|meta tags]]. Description and keywords in meta tags are commonly used to describe the Web page's content. Meta elements also specify page description, key words, authors of the document, and when the document was last modified.&lt;ref name="Rouse, M (2014)"/&gt; Web page metadata helps search engines and users to find the types of web pages they are looking for.

== Creation ==
Metadata can be created either by automated information processing or by manual work. Elementary metadata captured by computers can include information about when an object was created, who created it, when it was last updated, file size, and file extension. In this context an ''object'' refers to any of the following:

* A physical item such as a book, CD, DVD, a paper map, chair, table, flower pot, etc.
* An electronic file such as a digital image, digital photo, electronic document, program file, database table, etc.

=== Data virtualization ===
{{main article|Data virtualization}}
Data virtualization has emerged in the 2000s as the new software technology to complete the virtualization "stack" in the enterprise. Metadata is used in data virtualization servers which are enterprise infrastructure components, alongside database and application servers. Metadata in these servers is saved as persistent repository and describe [[business object]]s in various enterprise systems and applications. Structural metadata commonality is also important to support data virtualization.

=== Statistics and census services ===
Standardization work has had a large impact on efforts to build metadata systems in the statistical community{{Citation needed|date=May 2013}}. Several metadata standards{{Which|date=May 2013}} are described, and their importance to statistical agencies is discussed. Applications of the standards{{Which|date=May 2013}} at the Census Bureau, Environmental Protection Agency, Bureau of Labor Statistics, Statistics Canada, and many others are described{{Citation needed|date=May 2013}}. Emphasis is on the impact a metadata registry can have in a statistical agency.

=== Library and information science ===

Metadata has been used in various ways as a means of cataloging items in libraries in both digital and analog format. Such data helps classify, aggregate, identify, and locate a particular book, DVD, magazine or any object a library might hold in its collection. Until the 1980s, many library catalogues used 3x5 inch cards in file drawers to display a book's title, author, subject matter, and an abbreviated [[Alphanumeric|alpha-numeric]] string ([[call number]]) which indicated the physical location of the book within the library's shelves. The [[Dewey Decimal Classification|Dewey Decimal System]] employed by libraries for the classification of library materials by subject is an early example of metadata usage. Beginning in the 1980s and 1990s, many libraries replaced these paper file cards with computer databases. These computer databases make it much easier and faster for users to do keyword searches. Another form of older metadata collection is the use by US Census Bureau of what is known as the "Long Form." The Long Form asks questions that are used to create demographic data to find patterns of distribution.&lt;ref&gt;{{cite web
| title = AGLS Metadata Element Set - Part 2: Usage Guide - A non-technical guide to using AGLS metadata for describing resources
| author = National Archives of Australia
| year = 2002
| url = http://www.naa.gov.au/records-management/publications/agls-element.aspx
| accessdate = 17 March 2010}}
&lt;/ref&gt; [[library|Libraries]] employ metadata in [[library catalog]]ues, most commonly as part of an [[Library management system|Integrated Library Management System]]. Metadata is obtained by [[Library cataloguing#Cataloging rules|cataloguing]] resources such as books, periodicals, DVDs, web pages or digital images. This data is stored in the integrated library management system, [[Library management system|ILMS]], using the [[MARC standards|MARC]] metadata standard. The purpose is to direct patrons to the physical or electronic location of items or areas they seek as well as to provide a description of the item/s in question.

More recent and specialized instances of library metadata include the establishment of [[Digital library|digital libraries]] including [[eprint|e-print]] repositories and digital image libraries. While often based on library principles, the focus on non-librarian use, especially in providing metadata, means they do not follow traditional or common cataloging approaches. Given the custom nature of included materials, metadata fields are often specially created e.g. taxonomic classification fields, location fields, keywords or copyright statement. Standard file information such as file size and format are usually automatically included.&lt;ref name=solodovnik&gt;{{cite journal | last1 = Solodovnik | first1 = Iryna | year = 2011 | title = Metadata issues in Digital Libraries: key concepts and perspectives | journal = [[JLIS.it|JLIS.it: Italian Journal of Library, Archives and Information Science]] | volume = 2 | issue = 2 | publisher = University of Florence | doi = 10.4403/jlis.it-4663 | url = http://leo.cilea.it/index.php/jlis/article/view/4663 | accessdate = 29 June 2013}}&lt;/ref&gt; Library operation has for decades been a key topic in efforts toward [[international standardization]]. Standards for metadata in digital libraries include [[Dublin Core]], [[Metadata Encoding and Transmission Standard|METS]], [[Metadata Object Description Schema|MODS]], [[Data Documentation Initiative|DDI]], [[Digital object identifier|DOI]], [[Uniform Resource Name|URN]], [[Preservation Metadata: Implementation Strategies|PREMIS]] schema, [[Ecological Metadata Language|EML]], and [[Protocol for Metadata Harvesting|OAI-PMH]]. Leading libraries in the world give hints on their metadata standards strategies.&lt;ref&gt;{{cite web |author=Library of Congress Network Development and MARC Standards Office |url=http://www.loc.gov/standards/metadata.html |title=Library of Congress Washington DC on metadata |publisher=Loc.gov |date=2005-09-08 |accessdate=2011-12-23}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.dnb.de/DE/Netzpublikationen/Ablieferung/MetadatenKernset/metadatenkernset_node.html |title=Deutsche Nationalbibliothek Frankfurt on metadata}}&lt;/ref&gt;

=== In museums ===
Metadata in a museum context is the information that trained cultural documentation specialists, such as [[archivist]]s, [[librarian]]s, museum [[registrar (museum)|registrar]]s and [[curator]]s, create to index, structure, describe, identify, or otherwise specify works of art, architecture, cultural objects and their images.&lt;ref name=":0"&gt;{{Cite journal |last=Zange|first=Charles S.|date=31 January 2015 |title=Community makers, major museums, and the Keet S'aaxw: Learning about the role of museums in interpreting cultural objects |url=http://mw2015.museumsandtheweb.com/paper/community-makers-major-museums-and-the-keet-saaxw-learning-about-the-role-of-museums-in-interpreting-cultural-objects/ |publisher=Museums and the Web }}&lt;/ref&gt;&lt;ref name=":1"&gt;{{Cite book |title=Cataloging cultural objects: a guide to describing cultural works and their images. Visual Resources Association |last=Baca |first=Murtha |publisher=Visual Resources Association |year=2006 |isbn=|location=|pages=}}&lt;/ref&gt;{{page needed|date=November 2016}}&lt;ref name=":2"&gt;{{Cite book|title=Introduction to Metadata: Second Edition. Los Angeles: Getty Information Institute|last=Baca|first=Murtha|publisher=Getty Information Institute|year=2008|isbn=|location=Los Angeles|pages=}}&lt;/ref&gt;{{page needed|date=November 2016}} Descriptive metadata is most commonly used in museum contexts for object identification and resource recovery purposes.&lt;ref name=":1" /&gt;

==== Usage ====
Metadata is developed and applied within collecting institutions and museums in order to:
* Facilitate resource discovery and execute search queries.&lt;ref name=":2" /&gt;
* Create digital archives that store information relating to various aspects of museum collections and cultural objects, and serves for archival and managerial purposes.&lt;ref name=":2" /&gt;
* Provide public audiences access to cultural objects through publishing digital content online.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt;

==== Standards ====
Many museums and cultural heritage centers recognize that given the diversity of art works and cultural objects, no single model or standard suffices to describe and catalogue cultural works.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; For example, a sculpted Indigenous artifact could be classified as an artwork, an archaeological artifact, or an Indigenous heritage item. The early stages of standardization in archiving, description and cataloging within the museum community began in the late 1990s with the development of standards such as [[Categories for the Description of Works of Art (CDWA)]], Spectrum, the [[Conceptual Reference Model (CIDOC)]], [[Cataloging Cultural Objects (CCO)]] and the [[CDWA Lite XML schema]].&lt;ref name=":1" /&gt; These standards use [[HTML]] and [[XML]] markup languages for machine processing, publication and implementation.&lt;ref name=":1" /&gt; The [[Anglo-American Cataloguing Rules (AACR)]], originally developed for characterizing books, have also been applied to cultural objects, works of art and architecture.&lt;ref name=":2" /&gt; Standards, such as the CCO, are integrated within a Museum's [[Collection Management System (CMS)]], a database through which museums are able to manage their collections, acquisitions, loans and conservation.&lt;ref name=":2" /&gt; Scholars and professionals in the field note that the "quickly evolving landscape of standards and technologies" create challenges for cultural documentarians, specifically non-technically trained professionals.&lt;ref name=":3"&gt;{{Cite book|title=Linked Data for Libraries, Archives and Museums: How to Clean, Link and Publish Your Metadata|last=Hooland|first=Seth Van|last2=Verborgh|first2=Ruben|publisher=Facet|year=2014|isbn=|location=London|pages=}}&lt;/ref&gt;{{page needed|date=November 2016}} Most collecting institutions and museums use a [[relational database]] to categorize cultural works and their images.&lt;ref name=":2" /&gt; Relational databases and metadata work to document and describe the complex relationships amongst cultural objects and multi-faceted works of art, as well as between objects and places, people and artistic movements.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; Relational database structures are also beneficial within collecting institutions and museums because they allow for archivists to make a clear distinction between cultural objects and their images; an unclear distinction could lead to confusing and inaccurate searches.&lt;ref name=":2" /&gt;

==== Cultural objects and art works ====
An object's materiality, function and purpose, as well as the size (e.g., measurements, such as height, width, weight), storage requirements (e.g., climate-controlled environment) and focus of the museum and collection, influence the descriptive depth of the data attributed to the object by cultural documentarians.&lt;ref name=":2" /&gt; The established institutional cataloging practices, goals and expertise of cultural documentarians and database structure also influence the information ascribed to cultural objects, and the ways in which cultural objects are categorized.&lt;ref name=":0" /&gt;&lt;ref name=":2" /&gt; Additionally, museums often employ standardized commercial collection management software that prescribes and limits the ways in which archivists can describe artworks and cultural objects.&lt;ref name=":3" /&gt; As well, collecting institutions and museums use [[Controlled vocabulary|Controlled Vocabularies]] to describe cultural objects and artworks in their collections.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; [[Getty Vocabularies]] and the [[Library of Congress controlled vocabularies|Library of Congress Controlled Vocabularies]] are reputable within the museum community and are recommended by CCO standards.&lt;ref name=":2" /&gt; Museums are encouraged to use controlled vocabularies that are contextual and relevant to their collections and enhance the functionality of their digital information systems.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; Controlled Vocabularies are beneficial within databases because they provide a high level of consistency, improving resource retrieval.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; Metadata structures, including controlled vocabularies, reflect the [[Ontology (information science)|ontologies]] of the systems from which they were created. Often the processes through which cultural objects are described and categorized through metadata in museums do not reflect the perspectives of the maker communities.&lt;ref name=":0" /&gt;&lt;ref&gt;{{Cite journal |last=Srinivasan |first=Ramesh |date=December 2006 |title=Indigenous, ethnic and cultural articulations of new media |url=http://ics.sagepub.com/content/9/4/497.abstract |journal=International Journal of Cultural Studies |volume=9 |issue=4 |doi=10.1177/1367877906069899}}&lt;/ref&gt;

==== Museums and the Internet ====
Metadata has been instrumental in the creation of digital information systems and archives within museums, and has made it easier for museums to publish digital content online. This has enabled audiences who might not have had access to cultural objects due to geographic or economic barriers to have access to them.&lt;ref name=":1" /&gt; In the 2000s, as more museums have adopted archival standards and created intricate databases, discussions about [[Linked data|Linked Data]] between museum databases have come up in the museum, archival and library science communities.&lt;ref name=":3" /&gt; Collection Management Systems (CMS) and [[Digital asset management|Digital Asset Management]] tools can be local or shared systems.&lt;ref name=":2" /&gt; [[Digital humanities|Digital Humanities]] scholars note many benefits of interoperability between museum databases and collections, while also acknowledging the difficulties achieving such interoperability.&lt;ref name=":3" /&gt;

=== Law ===

==== United States of America ====
{{Globalize
|date=March 2015
}}
Problems involving metadata in [[litigation]] in the [[United States]] are becoming widespread.{{when|date=February 2011}} Courts have looked at various questions involving metadata, including the [[discovery (law)|discoverability]] of metadata by parties. Although the Federal Rules of Civil Procedure have only specified rules about electronic documents, subsequent case law has elaborated on the requirement of parties to reveal metadata.&lt;ref&gt;{{Cite journal
  | last = Gelzer | first = Reed D.
  | title = Metadata, Law, and the Real World: Slowly, the Three Are Merging
  | journal = Journal of AHIMA
  | volume = 79
  | issue = 2
  | pages = 56&#8211;57, 64
  | publisher = American Health Information Management Association
  | date = February 2008
  | url = http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_036537.hcsp?dDocName=bok1_036537
  | accessdate = 8 January 2010}}&lt;/ref&gt; In October 2009, the [[Arizona Supreme Court]] has ruled that metadata records are [[public record]].&lt;ref&gt;{{Cite news
  | last = Walsh | first = Jim
  | title = Ariz. Supreme Court rules electronic data is public record
  | newspaper = The Arizona Republic
  | location = Phoenix, Arizona
  | date = 30 October 2009
  | url = http://www.azcentral.com/arizonarepublic/local/articles/2009/10/30/20091030metadata1030.html
  | accessdate = 8 January 2010
}}&lt;/ref&gt; Document metadata have proven particularly important in legal environments in which litigation has requested metadata, which can include sensitive information detrimental to a certain party in court. Using [[metadata removal tool]]s to "clean" or redact documents can mitigate the risks of unwittingly sending sensitive data. This process partially (see [[data remanence]]) protects law firms from potentially damaging leaking of sensitive data through [[electronic discovery]].

====Australia====

In Australia the need to strengthen national security has resulted in the introduction of a new metadata storage law.&lt;ref&gt;Senate passes controversial metadata laws&lt;/ref&gt; This new law means that both security and policing agencies will be allowed to access up to two years of an individual's metadata, supposedly to make it easier to stop any terrorist attacks and serious crimes from happening. In the 2000s, the law does not allow access to content of people's messages, phone calls or email and web-browsing history, but these provisions could be changed by the government.

=== In healthcare ===
Australian medical research pioneered the definition of metadata for applications in health care. That approach offers the first recognized attempt to adhere to international standards in medical sciences instead of defining a proprietary standard under the [[World Health Organization]] (WHO) umbrella. The medical community yet did not approve the need to follow metadata standards despite research that supported these standards.&lt;ref&gt;M. L&#246;be, M. Knuth, R. M&#252;cke [http://ceur-ws.org/Vol-559/Paper1.pdf TIM: A Semantic Web Application for the Specification of Metadata Items in Clinical Research], CEUR-WS.org, urn:nbn:de:0074-559-9&lt;/ref&gt;

=== Data warehousing ===
[[Data warehouse]] (DW) is a repository of an organization's electronically stored data. Data warehouses are designed to manage and store the data. Data warehouses differ from [[business intelligence]] (BI) systems, because BI systems are designed to use data to create reports and analyze the information, to provide strategic guidance to management.&lt;ref&gt;Inmon, W.H. Tech Topic: What is a Data Warehouse? Prism Solutions. Volume 1. 1995.&lt;/ref&gt; Metadata is an important tool in how data is stored in data warehouses. The purpose of a data warehouse is to house standardized, structured, consistent, integrated, correct, "cleaned" and timely data, extracted from various operational systems in an organization. The extracted data are integrated in the data warehouse environment to provide an enterprise-wide perspective. Data are structured in a way to serve the reporting and analytic requirements. The design of structural metadata commonality using a [[data modeling]] method such as [[entity relationship model]] diagramming is important in any data warehouse development effort. They detail metadata on each piece of data in the data warehouse. An essential component of a [[data warehouse]]/[[business intelligence]] system is the metadata and tools to manage and retrieve the metadata. [[Ralph Kimball]]&lt;ref&gt;{{Cite book
  |last=Kimball |first=Ralph
  |authorlink=Ralph Kimball
  |title=The Data Warehouse Lifecycle Toolkit
  |edition=Second
  |location=New York |publisher=Wiley
  |year=2008
  |isbn=978-0-470-14977-5
  |ref=harv
  |pages=10, 115&#8211;117, 131&#8211;132, 140, 154&#8211;155
}}&lt;/ref&gt;{{page needed|date=November 2016}} describes metadata as the DNA of the data warehouse as metadata defines the elements of the [[data warehouse]] and how they work together.

[[Ralph Kimball|Kimball]] et al.&lt;ref&gt;{{harvnb|Kimball|2008|pages=116&#8211;117}}&lt;/ref&gt; refers to three main categories of metadata: Technical metadata, business metadata and process metadata. Technical metadata is primarily [[definitional]], while business metadata and process metadata is primarily descriptive. The categories sometimes overlap.
* '''Technical metadata''' defines the objects and processes in a DW/BI system, as seen from a technical point of view. The technical metadata includes the system metadata, which defines the data structures such as tables, fields, data types, indexes and partitions in the relational engine, as well as databases, dimensions, measures, and data mining models. Technical metadata defines the data model and the way it is displayed for the users, with the reports, schedules, distribution lists, and user security rights.
* '''Business metadata''' is content from the data warehouse described in more user-friendly terms. The business metadata tells you what data you have, where they come from, what they mean and what their relationship is to other data in the data warehouse. Business metadata may also serve as a documentation for the DW/BI system. Users who browse the data warehouse are primarily viewing the business metadata.
* '''Process metadata''' is used to describe the results of various operations in the data warehouse. Within the [[Extract, transform, load|ETL]] process, all key data from tasks is logged on execution. This includes start time, end time, CPU seconds used, disk reads, disk writes, and rows processed. When troubleshooting the ETL or [[Information retrieval|query]] process, this sort of data becomes valuable. Process metadata is the fact measurement when building and using a DW/BI system. Some organizations make a living out of collecting and selling this sort of data to companies - in that case the process metadata becomes the business metadata for the fact and dimension tables. Collecting process metadata is in the interest of business people who can use the data to identify the users of their products, which products they are using, and what level of service they are receiving.

=== On the Internet ===
The [[HTML]] format used to define web pages allows for the inclusion of a variety of types of metadata, from basic descriptive text, dates and keywords to further advanced metadata schemes such as the [[Dublin Core]], [[e-GMS]], and AGLS&lt;ref&gt;National Archives of Australia, AGLS Metadata Standard, accessed 7 January 2010, [http://www.naa.gov.au/records-management/create-capture-describe/describe/AGLS/index.aspx]&lt;/ref&gt; standards. Pages can also be [[geotagging|geotagged]] with [[Geographic coordinate system|coordinates]]. Metadata may be included in the page's header or in a separate file. [[Microformat]]s allow metadata to be added to on-page data in a way that regular web users do not see, but computers, [[web crawler]]s and [[search engine]]s can readily access. Many search engines are cautious about using metadata in their ranking algorithms due to exploitation of metadata and the practice of search engine optimization, [[Search engine optimization|SEO]], to improve rankings. See [[Meta element]] article for further discussion. This cautious attitude may be justified as people, according to Doctorow,&lt;ref&gt;Metacrap: Putting the torch to seven straw-men of the meta-utopia http://www.well.com/~doctorow/metacrap.htm&lt;/ref&gt; are not executing care and diligence when creating their own metadata and that metadata is part of a competitive environment where the metadata is used to promote the metadata creators own purposes. Studies show that search engines respond to web pages with metadata implementations,&lt;ref&gt;The impact of webpage content characteristics on webpage visibility in search engine results http://web.simmons.edu/~braun/467/part_1.pdf&lt;/ref&gt; and Google has an announcement on its site showing the meta tags that its search engine understands.&lt;ref&gt;{{cite web|url=https://support.google.com/webmasters/answer/79812?hl=en/ |title=Meta tags that Google understands |publisher=Google.com |accessdate=2014-05-22}}&lt;/ref&gt; Enterprise search startup [[Swiftype]] recognizes metadata as a relevance signal that webmasters can implement for their website-specific search engine, even releasing their own extension, known as Meta Tags 2.&lt;ref&gt;{{Cite web|url = https://swiftype.com/documentation/meta_tags2|title = Swiftype-specific Meta Tags |work=Swiftype Documentation |publisher=Swiftype |date = 3 October 2014 }}&lt;/ref&gt;

=== In broadcast industry ===
In [[broadcast]] industry, metadata is linked to audio and video [[broadcast media]] to:
* ''identify'' the media: [[Media clip|clip]] or [[playlist]] names, duration, [[timecode]], etc.
* ''describe'' the content: notes regarding the quality of video content, rating, description (for example, during a sport event, [[Index term|keywords]] like ''goal'', ''red card'' will be associated to some clips)
* ''classify'' media: metadata allows to sort the media or to easily and quickly find a video content (a [[TV news]] could urgently need some [[archiving|archive content]] for a subject). For example, the BBC have a large subject classification system, [[Lonclass]], a customized version of the more general-purpose [[Universal Decimal Classification]].

This metadata can be linked to the video media thanks to the [[Video server#Broadcast automation|video servers]]. Most major broadcast sport events like [[FIFA World Cup]] or the [[Olympic Games]] use this metadata to distribute their video content to [[TV station]]s through [[Index term|keywords]]. It is often the host broadcaster&lt;ref&gt;{{cite web|url=http://www.hbs.tv/hostbroadcasting/ |title=HBS is the FIFA host broadcaster |publisher=Hbs.tv |date=2011-08-06 |accessdate=2011-12-23}}&lt;/ref&gt; who is in charge of organizing metadata through its ''International Broadcast Centre'' and its video servers. This metadata is recorded with the images and are entered by metadata operators (''loggers'') who associate in live metadata available in ''metadata grids'' through [[software]] (such as [[Multicam(LSM)]] or [[IPDirector]] used during the FIFA World Cup or Olympic Games).&lt;ref&gt;{{cite web|url=http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |title=Host Broadcast Media Server and Related Applications |format=PDF |accessdate=2013-08-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20111102235256/http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |archivedate=2 November 2011 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://broadcastengineering.com/worldcup/fifa-world-cup-techonlogy-0610/ |title=logs during sport events |publisher=Broadcastengineering.com |accessdate=2011-12-23}}&lt;/ref&gt;

=== Geospatial ===
Metadata that describes geographic objects in electronic storage or format (such as datasets, maps, features, or documents with a geospatial component) has a history dating back to at least 1994 (refer [http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Library page on FGDC Metadata]). This class of metadata is described more fully on the [[geospatial metadata]] article.

=== Ecological and environmental ===
Ecological and environmental metadata is intended to document the "who, what, when, where, why, and how" of data collection for a particular study. This typically means which organization or institution collected the data, what type of data, which date(s) the data was collected, the rationale for the data collection, and the methodology used for the data collection. Metadata should be generated in a format commonly used by the most relevant science community, such as [[Darwin Core]], [[Ecological Metadata Language]],&lt;ref&gt;[http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html ] {{webarchive |url=https://web.archive.org/web/20110423161141/http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html |date=23 April 2011 }}&lt;/ref&gt; or [[Dublin Core]]. Metadata editing tools exist to facilitate metadata generation (e.g. Metavist,&lt;ref&gt;{{cite web|url=http://metavist.djames.net/ |title=Metavist 2 |publisher=Metavist.djames.net |accessdate=2011-12-23}}&lt;/ref&gt; [[Mercury: Metadata Search System]], Morpho&lt;ref&gt;{{cite web|url=http://knb.ecoinformatics.org/morphoportal.jsp |title=KNB Data :: Morpho |publisher=Knb.ecoinformatics.org |date=2009-05-20 |accessdate=2011-12-23}}&lt;/ref&gt;). Metadata should describe [[data provenance|provenance]] of the data (where they originated, as well as any transformations the data underwent) and how to give credit for (cite) the data products.

=== Digital music ===
When first released in 1982, Compact Discs only contained a Table Of Contents (TOC) with the number of tracks on the disc and their length in samples.[http://s3.amazonaws.com/academia.edu.documents/32801641/Morris_2012_-_Making_Music_Behave.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1477195681&amp;Signature=2TLmhapcR0M5eYsfMQ8FgG2TZa0%3D&amp;response-content-disposition=inline%3B%20filename%3DMaking_music_behave_Metadata_and_the_dig.pdf][https://books.google.com/books?id=GkIaGZ0HWcMC&amp;pg=PA48&amp;source=gbs_toc_r&amp;cad=4#v=onepage&amp;q&amp;f=false] Fourteen years later in 1996, a revision of the [[Compact Disc Digital Audio|CD Red Book]] standard added [[CD-Text]] to carry additional metadata.[http://web.ncf.ca/aa571/cdtext.htm] But CD-Text was not widely adopted. Shortly thereafter, it became common for personal computers to retrieve metadata from external sources (e.g. [[CDDB]], [[Gracenote]]) based on the TOC.

Digital [[Sound recording and reproduction|audio]] formats such as [[digital audio file]]s superseded music formats such as [[cassette tape]]s and [[CDs]] in the 2000s. Digital audio files could be labelled with more information than could be contained in just the file name. That descriptive information is called the '''audio tag''' or audio metadata in general. Computer programs specializing in adding or modifying this information are called [[tag editor]]s. Metadata can be used to name, describe, catalogue and indicate ownership or copyright for a digital audio file, and its presence makes it much easier to locate a specific audio file within a group, typically through use of a search engine that accesses the metadata. As different digital audio formats were developed, attempts were made to standardize a specific location within the digital files where this information could be stored.

As a result, almost all digital audio formats, including [[mp3]], broadcast wav and [[AIFF]] files, have similar standardized locations that can be populated with metadata. The metadata for compressed and uncompressed digital music is often encoded in the [[ID3]] tag. Common editors such as [[TagLib]] support MP3, Ogg Vorbis, FLAC, MPC, Speex, WavPack TrueAudio, WAV, AIFF, MP4, and ASF file formats.

=== Cloud applications ===
With the availability of [[Cloud computing|Cloud]] applications, which include those to add metadata to content, metadata is increasingly available over the Internet.

== Administration and management ==

=== Storage ===
Metadata can be stored either ''internally'',&lt;ref name=id3&gt;{{cite web
| first=Dan |last=O'Neill
| url=http://id3.org
| title=ID3.org
}}&lt;/ref&gt; in the same file or structure as the data (this is also called ''embedded metadata''), or ''externally'', in a separate file or field from the described data. A data repository typically stores the metadata ''detached'' from the data, but can be designed to support embedded metadata approaches. Each option has advantages and disadvantages:
* Internal storage means metadata always travels as part of the data they describe; thus, metadata is always available with the data, and can be manipulated locally. This method creates redundancy (precluding normalization), and does not allow managing all of a system's metadata in one place. It arguably increases consistency, since the metadata is readily changed whenever the data is changed.
* External storage allows collocating metadata for all the contents, for example in a database, for more efficient searching and management. Redundancy can be avoided by normalizing the metadata's organization. In this approach, metadata can be united with the content when information is transferred, for example in [[Streaming media]]; or can be referenced (for example, as a web link) from the transferred content. On the down side, the division of the metadata from the data content, especially in standalone files that refer to their source metadata elsewhere, increases the opportunities for misalignments between the two, as changes to either may not be reflected in the other.

Metadata can be stored in either human-readable or binary form. Storing metadata in a human-readable format such as [[XML]] can be useful because users can understand and edit it without specialized tools.&lt;ref name=Sutter&gt;{{Cite book
|first1=Robbie
|last1=De Sutter
|first2=Stijn
|last2=Notebaert
|first3=Rik
|last3=Van de Walle
|chapter=Evaluation of Metadata Standards in the Context of Digital Audio-Visual Libraries
|title=Research and Advanced Technology for Digital Libraries: 10th European Conference, EDCL 2006
|editor1-last=Gonzalo
|editor1-first=Julio
|editor2-last=Thanos
|editor2-first=Constantino
|editor3-last=Verdejo
|editor3-first=M. Felisa
|editor4-last=Carrasco
|editor4-first=Rafael
|date=September 2006
|url =https://books.google.com/books?id=kU7Lqqowp54C&amp;pg=PA226&amp;cad=4#v=onepage
|isbn= 978-3540446361
|publisher=Springer
|page=226
}}&lt;/ref&gt; However, text-based formats are rarely optimized for storage capacity, communication time, or processing speed. A binary metadata format enables efficiency in all these respects, but requires special software to convert the binary information into human-readable content.

=== Database management ===
Each [[relational database]] system has its own mechanisms for storing metadata. Examples of relational-database metadata include:
* Tables of all tables in a database, their names, sizes, and number of rows in each table.
* Tables of columns in each database, what tables they are used in, and the type of data stored in each column.
In database terminology, this set of metadata is referred to as the [[database catalog|catalog]]. The [[SQL]] standard specifies a uniform means to access the catalog, called the [[information schema]], but not all databases implement it, even if they implement other aspects of the SQL standard. For an example of database-specific metadata access methods, see [[Oracle metadata]]. Programmatic access to metadata is possible using APIs such as [[JDBC]], or SchemaCrawler.&lt;ref name=schemacrawler&gt;{{cite web
| author=Sualeh Fatehi
| url=http://schemacrawler.sourceforge.net/
| title=SchemaCrawler
| work=SourceForge
}}&lt;/ref&gt;

== See also ==
{{Div col||25em}}
* [[Agris: International Information System for the Agricultural Sciences and Technology]]
* [[Classification scheme]]
* [[Crosswalk (metadata)]]
* [[DataONE]]
* [[Data Dictionary]] (aka metadata repository)
* [[Dublin Core]]
* [[Folksonomy]]
* [[GEOMS &#8211; Generic Earth Observation Metadata Standard]]
* [[Geospatial metadata]]
* [[IPDirector]]
* [[ISO/IEC 11179]]
* [[Knowledge tag]]
* [[Mercury: Metadata Search System]]
* [[Meta element]]
* [[IF-MAP|Metadata Access Point Interface]]
* [[Metadata discovery]]
* [[Metadata facility for Java]]
* [[v:4-b: Metadata|Metadata from Wikiversity]]
* [[Metadata publishing]]
* [[Metadata registry]]
* [[Metamathematics]]
* [[METAFOR]] Common Metadata for Climate Modelling Digital Repositories
* [[Microcontent]]
* [[Microformat]]
* [[Multicam (LSM)]]
* [[Observations and Measurements]]
* [[Ontology (computer science)]]
* [[Official statistics]]
* [[Paratext]]
* [[Preservation Metadata]]
* [[SDMX]]
* [[Semantic Web]]
* [[SGML]]
* [[The Metadata Company]]
* [[Universal Data Element Framework]]
* [[Vocabulary OneSource]]
* [[XSD]]
{{Div col end}}

== References ==
{{Reflist|colwidth=30em}}

== External links ==
{{Wiktionary|metadata}}
* [http://www.niso.org/apps/group_public/download.php/17446/Understanding%20Metadata ''Understanding Metadata: What is metadata, and what is it for?''] &#8212; [[NISO]], 2017
* [http://web.archive.org/web/20140522165110/http://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=1111111 "A Guardian guide to your metadata"] &#8212; ''[[The Guardian]]'', Wednesday 12 June 2013.
* [http://www.well.com/~doctorow/metacrap.htm Metacrap: Putting the torch to seven straw-men of the meta-utopia] &#8212; [[Cory Doctorow]]'s opinion on the limitations of metadata on the [[Internet]], 2001
* [http://www.dataone.org DataONE] Investigator Toolkit
* [http://www.informaworld.com/openurl?genre=journal&amp;issn=1938-6389 ''Journal of Library Metadata''], Routledge, Taylor &amp; Francis Group, ISSN 1937-5034
* [http://www.inderscience.com/ijmso ''International Journal of Metadata, Semantics and Ontologies'' (''IJMSO'')], Inderscience Publishers, ISSN 1744-263X
* {{webarchive |url=https://web.archive.org/web/20130126101115/http://www.metalounge.org/_literature_52579/Stephen_Machin_%E2%80%93_ON_METADATA_AND_METACONTENT |date=26 January 2013 |title=Metadata and metacontent }} (PDF, archived version)

{{Software engineering}}
{{Data warehouse}}

{{Authority control}}

[[Category:Data management]]
[[Category:Records management]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata| ]]
[[Category:Technical communication]]
[[Category:Business intelligence]]</text>
      <sha1>sv43xmucbh0lsc816sfa0x3zk1v6oey</sha1>
    </revision>
  </page>
  <page>
    <title>Business intelligence</title>
    <ns>0</ns>
    <id>168387</id>
    <revision>
      <id>762759162</id>
      <parentid>761909642</parentid>
      <timestamp>2017-01-30T16:00:04Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo fixing, replaced: critized &#8594; criticized using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="44183" xml:space="preserve">{{Use dmy dates|date=March 2012}}

{{Business administration}}

'''Business Intelligence''' ('''BI''') are the set of strategies, processes, [[application software|applications]], [[data]], products, technologies and technical architectures which are used to support the collection, analysis, presentation and dissemination of business information.&lt;ref&gt;Dedi&#263; N. &amp; Stanier C. (2016). Measuring the Success of Changes to Existing Business Intelligence Solutions to Improve Business Intelligence Reporting. Lecture Notes in Business Information Processing. Springer International Publishing. Volume 268, pp. 225-236.&lt;/ref&gt; BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are [[Business reporting|reporting]], [[online analytical processing]], [[analytics]], [[data mining]], [[process mining]], [[complex event processing]], [[business performance management]], [[benchmarking]], [[text mining]], [[Predictive Analysis|predictive analytics]] and [[Prescriptive Analytics|prescriptive analytics]] and are capable of handling large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal is to allow for the easy interpretation of these [[big data]]. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.&lt;ref&gt;({{cite book |title= Business Intelligence Success Factors: Tools for Aligning Your Business in the Global Economy |last= Rud|first= Olivia |year= 2009|publisher= Wiley &amp; Sons|location= Hoboken, N.J|isbn= 978-0-470-39240-9 |page= |pages= |url= |accessdate=}})&lt;/ref&gt;

Business intelligence can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an "intelligence" that cannot be derived by any singular set of data.&lt;ref&gt;{{cite book| last1= Coker| first1= Frank| title= Pulse: Understanding the Vital Signs of Your Business| publisher= Ambient Light Publishing
| publication-date= 2014| pages= 41&#8211;42| isbn= 978-0-9893086-0-1}}&lt;/ref&gt; Amongst myriad uses, business intelligence tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.&lt;ref name=":0"&gt;Chugh, R &amp; Grandhi, S 2013, &#8216;Why Business Intelligence? Significance of Business Intelligence tools and integrating BI governance with corporate governance&#8217;, International Journal of E-Entrepreneurship and Innovation, vol. 4, no.2, pp. 1-14. https://www.researchgate.net/publication/273861123_Why_Business_Intelligence_Significance_of_Business_Intelligence_Tools_and_Integrating_BI_Governance_with_Corporate_Governance&lt;/ref&gt;

==Components==
Business intelligence is made up of an increasing number of components including:
* Multidimensional aggregation and allocation
* [[Denormalization]], tagging and standardization
* Realtime reporting with analytical alert
* A method of interfacing with [[unstructured data]] sources
* Group consolidation, budgeting and [[rolling forecast]]s
* [[Statistical inference]] and probabilistic simulation
* [[Key performance indicator]]s optimization
* Version control and process management
* Open item management

==History==
The earliest known use of the term "Business Intelligence" is in Richard Millar Devens&#8217; in the &#8216;Cyclop&#230;dia of Commercial and Business Anecdotes&#8217; from 1865. Devens used the term to describe how the banker, Sir Henry Furnese, gained profit by receiving and acting upon information about his environment, prior to his competitors. &#8220;''Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the [[Siege of Namur (1695)|fall of Namur]] added to his profits, owing to his early receipt of the news''.&#8221; (Devens, (1865), p.&amp;nbsp;210).  The ability to collect and react accordingly based on the information retrieved, an ability that Furnese excelled in, is today still at the very heart of BI.&lt;ref name="Miller Devens"&gt;{{cite book|last=Miller Devens|first=Richard|title=Cyclopaedia of Commercial and Business Anecdotes; Comprising Interesting Reminiscences and Facts, Remarkable Traits and Humors of Merchants, Traders, Bankers Etc. in All Ages and Countries|url=https://books.google.dk/books?id=9MspAAAAYAAJ&amp;pg=PA210&amp;dq=%22business+intelligence%22&amp;hl=en&amp;ei=a5EPTdaRIsOWnAeVyYHQDg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;redir_esc=y#v=onepage&amp;q=%22business%20intelligence%22&amp;f=false|publisher=D. Appleton and company|accessdate=15 February 2014|page=210}}&lt;/ref&gt;

In a 1958 article, [[IBM]] researcher [[Hans Peter Luhn]] used the term business intelligence. He employed the Webster's dictionary definition of intelligence: "the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal."&lt;ref&gt;
{{cite journal|url= http://www.research.ibm.com/journal/rd/024/ibmrd0204H.pdf|doi=10.1147/rd.24.0314|title= A Business Intelligence System|author=H P Luhn |authorlink= Hans Peter Luhn |year= 1958 |journal= IBM Journal|volume= 2|issue= 4|pages= 314}}
&lt;/ref&gt;

Business intelligence as it is understood today is said to have evolved from the [[decision support system]]s (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with [[decision making]] and planning. From DSS, [[data warehouse]]s, [[Executive Information System]]s, [[Online analytical processing|OLAP]] and business intelligence came into focus beginning in the late 80s.

In 1989, Howard Dresner (later a [[Gartner]] analyst) proposed "business intelligence" as an umbrella term to describe "concepts and methods to improve business decision making by using fact-based support systems."&lt;ref name=power&gt;{{cite web |url= http://dssresources.com/history/dsshistory.html
|title= A Brief History of Decision Support Systems, version 4.0 |accessdate=10 July 2008
|author= D. J. Power |date= 10 March 2007|publisher= DSSResources.COM }}
&lt;/ref&gt; It was not until the late 1990s that this usage was widespread.&lt;ref&gt;{{cite web |url=http://dssresources.com/history/dsshistory.html |title=A Brief History of Decision Support Systems |last=Power |first=D. J. |accessdate=1 November 2010 }}&lt;/ref&gt;

Critics see BI as evolved from mere [[business reporting]] together with the advent of increasingly powerful and easy-to-use [[data analysis]] tools. In this respect it has also been criticized as a marketing buzzword in the context of the "[[big data]]" surge.&lt;ref&gt;{{cite web|title=Decoding big data buzzwords|year=2015|quote=BI refers to the approaches, tools, mechanisms that organizations can use to keep a finger on the pulse of their businesses. Also referred by unsexy versions -- &#8220;dashboarding&#8221;, &#8220;MIS&#8221; or &#8220;reporting.&#8221;|publisher=cio.com|url=http://www.cio.com/article/2919082/big-data/what-are-they-talking-about-decoding-big-data-buzzwords.html}}&lt;/ref&gt;

==Data warehousing==
Often BI applications use data gathered from a [[data warehouse]] (DW) or from a [[data mart]], and the concepts of BI and DW sometimes combine as "'''BI/DW'''"&lt;ref&gt;
{{cite book
| last1                 = Golden
| first1                = Bernard
| title                 = Amazon Web Services For Dummies
| url                   = https://books.google.com/books?id=xSVwAAAAQBAJ
| series                = For dummies
| publisher             = John Wiley &amp; Sons
| publication-date      = 2013
| page                  = 234
| isbn                  = 9781118652268
| accessdate            = 2014-07-06
| quote                 = [...] traditional business intelligence or data warehousing tools (the terms are used so interchangeably that they're often referred to as BI/DW) are extremely expensive [...]
}}
&lt;/ref&gt;
or as "'''BIDW'''". A data warehouse contains a copy of analytical data that facilitates decision support. However, not all data warehouses serve for business intelligence, nor do all business intelligence applications require a data warehouse.

To distinguish between the concepts of business intelligence and data warehouses, [[Forrester Research]] defines business intelligence in one of two ways:

# Using a broad definition: "Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making."&lt;ref&gt;{{cite web |url=http://www.forrester.com/rb/Research/topic_overview_business_intelligence/q/id/39218/t/2 |title=Topic Overview: Business Intelligence |last=Evelson |first=Boris |date=21 November 2008}}&lt;/ref&gt; Under this definition, business intelligence also includes technologies such as data integration, data quality, data warehousing, master-data management, text- and content-analytics, and many others that the market sometimes lumps into the "[[Information Management]]" segment. Therefore, Forrester refers to ''data preparation'' and ''data usage'' as two separate but closely linked segments of the business-intelligence architectural stack.
# Forrester defines the narrower business-intelligence market as, "...referring to just the top layers of the BI architectural stack such as reporting, analytics and [[Dashboards (management information systems)|dashboards]]."&lt;ref&gt;{{cite web
|url=http://blogs.forrester.com/boris_evelson/10-04-29-want_know_what_forresters_lead_data_analysts_are_thinking_about_bi_and_data_domain
|title=Want to know what Forrester's lead data analysts are thinking about BI and the data domain? |last=Evelson |first=Boris |date=29 April 2010}}&lt;/ref&gt;

==Comparison with competitive intelligence==
Though the term business intelligence is sometimes a synonym for [[competitive intelligence]] (because they both support [[decision making]]), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.&lt;ref&gt;{{cite web |url=http://blogs.forrester.com/james_kobielus/10-04-30-what%E2%80%99s_not_bi_oh_don%E2%80%99t_get_me_startedoops_too_latehere_goes |title=What&#8217;s Not BI? Oh, Don&#8217;t Get Me Started....Oops Too Late...Here Goes.... |last=Kobielus |first=James |date=30 April 2010 |quote=&#8220;Business&#8221; intelligence is a non-domain-specific catchall for all the types of analytic data that can be delivered to users in reports, dashboards, and the like. When you specify the subject domain for this intelligence, then you can refer to &#8220;competitive intelligence,&#8221; &#8220;market intelligence,&#8221; &#8220;social intelligence,&#8221; &#8220;financial intelligence,&#8221; &#8220;HR intelligence,&#8221; &#8220;supply chain intelligence,&#8221; and the like.}}&lt;/ref&gt;

==Comparison with business analytics==
Business intelligence and [[business analytics]] are sometimes used interchangeably, but there are alternate definitions.&lt;ref&gt;{{cite web|url=http://timoelliott.com/blog/2011/03/business-analytics-vs-business-intelligence.html |title=Business Analytics vs Business Intelligence? |publisher=timoelliott.com |date=2011-03-09 |accessdate=2014-06-15}}&lt;/ref&gt;  One definition contrasts the two, stating that the term business intelligence refers to collecting business data to find information primarily through asking questions, reporting, and online analytical processes. Business analytics, on the other hand, uses statistical and quantitative tools for explanatory and [[predictive modelling]].&lt;ref&gt;{{cite web|url=http://www.businessanalytics.com/difference-between-business-analytics-and-business-intelligence/ |title=Difference between Business Analytics and Business Intelligence |publisher=businessanalytics.com |date=2013-03-15 |accessdate=2014-06-15}}&lt;/ref&gt;

In an alternate definition, [[Thomas H. Davenport|Thomas Davenport]], professor of information technology and management at [[Babson College]] argues that business intelligence should be divided into [[Information retrieval|querying]], [[Business reporting|reporting]], [[Online analytical processing]] (OLAP), an "alerts" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.&lt;ref&gt;{{Cite interview |url=http://www.informationweek.com/news/software/bi/222200096 |title=Analytics at Work: Q&amp;A with Tom Davenport |last=Henschen |first=Doug |date=4 January 2010}}&lt;/ref&gt;

==Applications in an enterprise==
Business intelligence can be applied to the following business purposes, in order to drive business value.{{Citation needed|date=October 2010}}
# [[Measurement]]&amp;nbsp;&#8211; program that creates a hierarchy of [[performance metrics]] (see also [[Metrics Reference Model]]) and [[benchmarking]] that informs business leaders about progress towards business goals ([[business process management]]).
# [[Analytics]]&amp;nbsp;&#8211; program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery. Frequently involves: [[data mining]], [[process mining]], [[statistical analysis]], [[predictive analytics]], [[predictive modeling]], [[business process modeling]], [[data lineage]], [[complex event processing]] and [[Prescriptive Analytics|prescriptive analytics]].
# [[Business reporting|Reporting]]/[[enterprise reporting]]&amp;nbsp;&#8211; program that builds infrastructure for strategic reporting to serve the strategic management of a business, not operational reporting. Frequently involves [[data visualization]], [[executive information system]] and [[OLAP]].
# [[Collaboration]]/[[collaboration platform]]&amp;nbsp;&#8211; program that gets different areas (both inside and outside the business) to work together through [[data sharing]] and [[electronic data interchange]].
# [[Knowledge management]]&amp;nbsp;&#8211; program to make the company data-driven through strategies and practices to identify, create, represent, distribute, and enable adoption of insights and experiences that are true business knowledge. Knowledge management leads to [[learning management]] and [[regulatory compliance]].

In addition to the above, business intelligence can provide a pro-active approach, such as alert functionality that immediately notifies the end-user if certain conditions are met. For example, if some business metric exceeds a pre-defined threshold, the metric will be highlighted in standard reports, and the business analyst may be alerted via e-mail or another monitoring service. This end-to-end process requires data governance, which should be handled by the expert.{{Citation needed|date=January 2012}}

==Prioritization of projects==
It can be difficult to provide a positive business case for business intelligence initiatives, and often the projects must be prioritized through strategic initiatives. BI projects can attain higher prioritization within the organization if managers consider the following:
* As described by Kimball&lt;ref&gt;Kimball et al., 2008: 29&lt;/ref&gt; the BI manager must determine the tangible benefits such as eliminated cost of producing legacy reports.
* Data access for the entire organization must be enforced.&lt;ref&gt;{{cite web|url= http://content.dell.com/us/en/enterprise/d/large-business/ready-business-intelligence.aspx|title= Are You Ready for the New Business Intelligence?|publisher=Dell.com | accessdate=19 June 2012}}&lt;/ref&gt; In this way even a small benefit, such as a few minutes saved, makes a difference when multiplied by the number of employees in the entire organization.
* As described by Ross, Weil &amp; Roberson for Enterprise Architecture,&lt;ref&gt;[[Jeanne W. Ross]], [[Peter Weill]], [[David C. Robertson]] (2006) ''Enterprise Architecture As Strategy'', p. 117 ISBN 1-59139-839-8.&lt;/ref&gt; managers should also consider letting the BI project be driven by other business initiatives with excellent business cases. To support this approach, the organization must have enterprise architects who can identify suitable business projects.
* Using a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization, such as a weighted decision matrix.&lt;ref&gt;{{cite web|last=Krapohl|first=Donald|title=A Structured Methodology for Group Decision Making|url=http://www.augmentedintel.com/wordpress/index.php/a-structured-methodology-for-group-decision-making/|publisher=AugmentedIntel|accessdate=22 April 2013}}&lt;/ref&gt;

==Success factors of implementation==
According to Kimball et al., there are three critical areas that organizations should assess before getting ready to do a BI project:&lt;ref&gt;Kimball et al. 2008: p. 298&lt;/ref&gt;

# The level of commitment and sponsorship of the project from senior management.
# The level of business need for creating a BI implementation.
# The amount and quality of business data available.

===Business sponsorship===

The commitment and [[:wikt:sponsor|sponsor]]ship of senior management is according to Kimball ''et al.'', the most important criteria for assessment.&lt;ref&gt;Kimball et al., 2008: 16&lt;/ref&gt; This is because having strong management backing helps overcome shortcomings elsewhere in the project. However, as Kimball ''et al.'' state: &#8220;even the most elegantly designed DW/BI system cannot overcome a lack of business [management] sponsorship&#8221;.&lt;ref&gt;Kimball et al., 2008: 18&lt;/ref&gt;

It is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a BI system. The best business sponsor should have organizational clout and should be well connected within the organization. It is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks. The management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project. Support from multiple members of the management ensures the project does not fail if one person leaves the steering group. However, having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions, such as if different departments want to put more emphasis on their usage. This issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation. All stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground.

Another management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor. Problems of [[scope creep]] occur when the sponsor requests data sets that were not specified in the original planning phase.

===Business needs===

Because of the close relationship with senior management, another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation.&lt;ref name="Kimball et al., 2008: 17"&gt;Kimball et al., 2008: 17&lt;/ref&gt;
The needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market. Another reason for a business-driven approach to implementation of BI is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement DW or BI in order to create more oversight.

Companies that implement BI are often large, multinational organizations with diverse subsidiaries.&lt;ref&gt;{{cite web|title=How Companies Are Implementing Business Intelligence Competency Centers |url=http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |publisher=Computer World |deadurl=yes |accessdate=1 April 2014 |archiveurl=https://web.archive.org/web/20130528054421/http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |archivedate=28 May 2013 }}&lt;/ref&gt; A well-designed BI solution provides a consolidated view of key business data not available anywhere else in the organization, giving management visibility and control over measures that otherwise would not exist.

===Amount and quality of available data===

Without proper data, or with too little quality data, any BI implementation fails; it does not matter how good the management sponsorship or business-driven motivation is. Before implementation it is a good idea to do [[data profiling]]. This analysis identifies the &#8220;content, consistency and structure [..]&#8221;&lt;ref name="Kimball et al., 2008: 17"/&gt; of the data. This should be done as early as possible in the process and if the analysis shows that data is lacking, put the project on hold temporarily while the IT department figures out how to properly collect data.

When planning for business data and business intelligence requirements, it is always advisable to consider specific scenarios that apply to a particular organization, and then select the business intelligence features best suited for the scenario.

Often, scenarios revolve around distinct business processes, each built on one or more data sources. These sources are used by features that present that data as information to knowledge workers, who subsequently act on that information. The business needs of the organization for each business process adopted correspond to the essential steps of business intelligence. These essential steps of business intelligence include but are not limited to:
#Go through business data sources in order to collect needed data
#Convert business data to information and present appropriately
#Query and analyze data
#Act on the collected data
The '''quality aspect''' in business intelligence should cover all the process from the source data to the final reporting. At each step, the '''quality gates''' are different:
# Source Data:
#* Data Standardization: make data comparable (same unit, same pattern...)
#* [[Master data management|Master Data Management:]] unique referential
# [[Operational data store|Operational Data Store (ODS)]]:
#* [[Data cleansing|Data Cleansing:]] detect &amp; correct inaccurate data
#* Data Profiling: check inappropriate value, null/empty
# [[Data warehouse]]:
#* Completeness: check that all expected data are loaded
#* [[Referential integrity]]: unique and existing referential over all sources
#* Consistency between sources: check consolidated data vs sources
# Reporting:
#* Uniqueness of indicators: only one share dictionary of indicators
#* Formula accuracy: local reporting formula should be avoided or checked

==User aspect==

Some considerations must be made in order to successfully integrate the usage of business intelligence systems in a company. Ultimately the BI system must be accepted and utilized by the users in order for it to add value to the organization.&lt;ref name = kimball&gt;Kimball&lt;/ref&gt;&lt;ref name = swain&gt;Swain Scheps ''Business Intelligence for Dummies'', 2008, ISBN 978-0-470-12723-0&lt;/ref&gt; If the [[usability]] of the system is poor, the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system. If the system does not add value to the users&#180; mission, they simply don't use it.&lt;ref name = swain /&gt;

To increase user acceptance of a BI system, it can be advisable to consult business users at an early stage of the DW/BI lifecycle, for example at the requirements gathering phase.&lt;ref name = kimball /&gt; This can provide an insight into the [[business process]] and what the users need from the BI system. There are several methods for gathering this information, such as questionnaires and interview sessions.

When gathering the requirements from the business users, the local IT department should also be consulted in order to determine to which degree it is possible to fulfill the business's needs based on the available data.&lt;ref name = kimball /&gt;

Taking a user-centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the BI system.&lt;ref name = swain /&gt;

Besides focusing on the user experience offered by the BI applications, it may also possibly motivate the users to utilize the system by adding an element of competition. Kimball&lt;ref name = kimball /&gt; suggests implementing a function on the Business Intelligence portal website where reports on system usage can be found. By doing so, managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the BI system even more.

In a 2007 article, H. J. Watson gives an example of how the competitive element can act as an incentive.&lt;ref name = watson&gt;{{cite journal|title=The Current State of Business Intelligence|year=2007|doi=10.1109/MC.2007.331|last1=Watson|first1=Hugh J.|last2=Wixom|first2=Barbara H.|journal=Computer|volume=40|issue=9|pages=96}}&lt;/ref&gt; Watson describes how a large call centre implemented performance dashboards for all call agents, with monthly incentive bonuses tied to performance metrics. Also, agents could compare their performance to other team members. The implementation of this type of performance measurement and competition significantly improved agent performance.

BI chances of success can be improved by involving senior management to help make BI a part of the [[organizational culture]], and by providing the users with necessary tools, training, and support.&lt;ref name = watson /&gt; Training encourages more people to use the BI application.&lt;ref name = kimball /&gt;

Providing user support is necessary to maintain the BI system and resolve user problems.&lt;ref name = swain /&gt; User support can be incorporated in many ways, for example by creating a website. The website should contain great content and tools for finding the necessary information. Furthermore, helpdesk support can be used. The help desk can be manned by power users or the DW/BI project team.&lt;ref name = kimball /&gt;

==BI Portals==
A '''Business Intelligence portal''' (BI portal) is the primary access interface for [[Data warehouse|Data Warehouse]] (DW) and Business Intelligence (BI) applications. The BI portal is the user's first impression of the DW/BI system. It is typically a browser application, from which the user has access to all the individual services of the DW/BI system, reports and other analytical functionality.
The BI portal must be implemented in such a way that it is easy for the users of the DW/BI application to call on the functionality of the application.&lt;ref name="Ralph"&gt;''The Data Warehouse Lifecycle Toolkit (2nd ed.). Ralph Kimball (2008).''&lt;/ref&gt;

The BI portal's main functionality is to provide a navigation system of the DW/BI application. This means that the portal has to be implemented in a way that the user has access to all the functions of the DW/BI application.

The most common way to design the portal is to custom fit it to the business processes of the organization for which the DW/BI application is designed, in that way the portal can best fit the needs and requirements of its users.&lt;ref name="Wiley"&gt;''Microsoft Data Warehouse Toolkit. Wiley Publishing. (2006)''&lt;/ref&gt;

The BI portal needs to be easy to use and understand, and if possible have a look and feel similar to other applications or web content of the organization the DW/BI application is designed for ([[consistency]]).

The following is a list of desirable features for [[web portal]]s in general and BI portals in particular:

;Usable: User should easily find what they need in the BI tool.
;Content Rich: The portal is not just a report printing tool, it should contain more functionality such as advice, help, support information and documentation.
;Clean: The portal should be designed so it is easily understandable and not over-complex as to confuse the users
;Current: The portal should be updated regularly.
;Interactive: The portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal. Scalability and customization give the user the means to fit the portal to each user.
;Value Oriented: It is important that the user has the feeling that the DW/BI application is a valuable resource that is worth working on.

==Marketplace==
There are a number of business intelligence vendors, often categorized into the remaining independent "pure-play" vendors and consolidated "megavendors" that have entered the market through a recent trend&lt;ref&gt;{{cite news|url=http://www.zdnet.com/gartner-releases-2013-bi-magic-quadrant-7000011264/ |title=Gartner releases 2013 BI Magic Quadrant |publisher=ZDNet |author=Andrew Brust| date= 2013-02-14|accessdate=21 August 2013}}&lt;/ref&gt; of acquisitions in the BI industry.&lt;ref&gt;{{cite web |url=http://www.bi-verdict.com/fileadmin/FreeAnalyses/consolidations.htm |title=Consolidations in the BI industry |date=7 March 2008 |last=Pendse |first=Nigel |work=The OLAP Report}}&lt;/ref&gt; The business intelligence market is gradually growing. In 2012 business intelligence services brought in $13.1 billion in revenue.&lt;ref&gt;{{cite web|title=Why Business Intelligence Is Key For Competitive Advantage|url=https://cisonline.bu.edu/news-resources/why-business-intelligence-is-key-for-competitive-advantage/|website=Boston University|accessdate=23 October 2014}}&lt;/ref&gt;

Some companies adopting BI software decide to pick and choose from different product offerings (best-of-breed) rather than purchase one comprehensive integrated solution (full-service).&lt;ref&gt;{{cite web |url=http://www.b-eye-network.com/view/2608 |title=Three Trends in Business Intelligence Technology |last=Imhoff |first=Claudia |date=4 April 2006}}&lt;/ref&gt;

===Industry-specific===
Specific considerations for business intelligence systems have to be taken in some sectors such as [[Bank regulation|governmental banking regulations]] or healthcare.&lt;ref&gt;{{cite journal |vauthors=Mettler T, Vimarlund V |title=Understanding business intelligence in the context of healthcare |journal=Health Informatics Journal |volume=15 |issue=3 |pages=254&#8211;264 |year=2009 |doi=10.1177/1460458209337446 }}&lt;/ref&gt; The information collected by banking institutions and analyzed with BI software must be protected from some groups or individuals, while being fully available to other groups or individuals. Therefore, BI solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law.{{citation needed|date=May 2016}}

==Semi-structured or unstructured data==
Businesses create a huge amount of valuable information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material and news. According to Merrill Lynch, more than 85% of all business information exists in these forms. These information types are called either ''[[Semi-structured data|semi-structured]]'' or ''[[Unstructured data|unstructured]]'' data. However, organizations often only use these documents once.&lt;ref name = rao&gt;{{cite journal|doi=10.1109/MITP.2003.1254966 |url=http://www.ramanarao.com/papers/rao-itpro-2003-11.pdf|title=From unstructured data to actionable intelligence|year=2003|last1=Rao|first1=R.|journal=IT Professional|volume=5|issue=6|pages=29}}&lt;/ref&gt;

The managements of semi-structured data is recognized as a major unsolved problem in the information technology industry.&lt;ref name = blumberg&gt;{{cite journal|url=http://soquelgroup.com/Articles/dmreview_0203_problem.pdf|author1=Blumberg, R.  |author2=S. Atre  |lastauthoramp=yes |title=The Problem with Unstructured Data|journal=DM Review |year=2003|pages=42&#8211;46}}&lt;/ref&gt; According to projections from Gartner (2003), white collar workers spend anywhere from 30 to 40 percent of their time searching, finding and assessing unstructured data. BI uses both structured and unstructured data, but the former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making.&lt;ref name = blumberg /&gt;&lt;ref name = negash&gt;{{cite journal|url=http://site.xavier.edu/sena/info600/businessintelligence.pdf|author=Negash, S |title=Business Intelligence|journal= Communications of the Association of Information Systems|volume=13|year= 2004|pages=177&#8211;195}}&lt;/ref&gt; Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.&lt;ref name = rao /&gt;

Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.&lt;ref name = negash /&gt;

===Unstructured data vs. semi-structured data===
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered [[columns]] and [[Row (database)|rows]]. One type of unstructured data is typically stored in a [[BLOB]] (binary large object), a catch-all data type available in most [[relational database]] management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.{{citation needed|date=May 2016}}

Many of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database.
Therefore, it may be more accurate to talk about this as semi-structured documents or data,&lt;ref name = blumberg /&gt; but no specific consensus seems to have been reached.

Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.

===Problems with semi-structured or unstructured data===
There are several challenges to developing BI with semi-structured data. According to Inmon &amp; Nesavich,&lt;ref name = inmon&gt;Inmon, B. &amp; A. Nesavich, "Unstructured Textual Data in the Organization" from "Managing Unstructured data in the organization", Prentice Hall 2008, pp. 1&#8211;13&lt;/ref&gt; some of those are:

# Physically accessing unstructured textual data&amp;nbsp;&#8211; unstructured data is stored in a huge variety of formats.
# [[Terminology]]&amp;nbsp;&#8211; Among researchers and analysts, there is a need to develop a standardized terminology.
# Volume of data&amp;nbsp;&#8211; As stated earlier, up to 85% of all data exists as semi-structured data. Couple that with the need for word-to-word and semantic analysis.
# Searchability of unstructured textual data&amp;nbsp;&#8211; A simple search on some data, e.g. apple, results in links where there is a reference to that precise search term. (Inmon &amp; Nesavich, 2008)&lt;ref name = inmon /&gt; gives an example: &#8220;a search is made on the term felony. In a simple search, the term felony is used, and everywhere there is a reference to felony, a hit to an unstructured document is made. But a simple search is crude. It does not find references to crime, arson, murder, embezzlement, vehicular homicide, and such, even though these crimes are types of felonies.&#8221;

===The use of metadata===
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of [[metadata]].&lt;ref name = rao /&gt; Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content&amp;nbsp;&#8211; e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and [[information extraction]].

==2009 predictions==
A 2009 paper predicted&lt;ref&gt;[http://www.gartner.com/it/page.jsp?id=856714 Gartner Reveals Five Business Intelligence Predictions for 2009 and Beyond]. gartner.com. 15 January 2009&lt;/ref&gt; these developments in the business intelligence market:
* Because of lack of information, processes, and tools, through 2012, more than 35 percent of the top 5,000 global companies regularly fail to make insightful decisions about significant changes in their business and markets.
* By 2012, business units will control at least 40 percent of the total budget for business intelligence.
* By 2012, one-third of analytic applications applied to business processes will be delivered through [[Granularity|coarse-grained]] application [[mashup (web application hybrid)|mashups]].
* BI has a huge scope in Entrepreneurship however majority of new entrepreneurs ignore its potential.&lt;ref&gt;[http://brighterkashmir.com/role-of-business-intelligence-in-entrepreneurship/ huge scope in Entrepreneurship]&lt;/ref&gt;

A 2009 ''Information Management'' special report predicted the top BI trends: "[[green computing]], [[social networking service]]s, [[data visualization]], [[Mobile business intelligence|mobile BI]], [[predictive analytics]], [[composite application]]s, [[cloud computing]] and [[Multi-touch|multitouch]]".&lt;ref&gt;{{cite web |url=http://www.information-management.com/specialreports/2009_148/business_intelligence_data_vizualization_social_networking_analytics-10015628-1.html |title=10 Red Hot BI Trends |last=Campbell |first=Don |date=23 June 2009 |work=Information Management}}&lt;/ref&gt; Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.&lt;ref&gt;{{cite web |url=http://www.aberdeen.com/Aberdeen-Library/8906/RR-analytics-cloud-saas-bi.aspx |title=Cloud Analytics in 2014: Infusing the Workforce with Insight |last=Lock|first=Michael|date=27 March 2014 }}&lt;/ref&gt;

Other business intelligence trends include the following:

* Third party SOA-BI products increasingly address [[Extract, transform, load|ETL]] issues of volume and throughput.
* Companies embrace in-memory processing, 64-bit processing, and pre-packaged analytic BI applications.
* Operational applications have callable BI components, with improvements in response time, scaling, and concurrency.
* Near or real time BI analytics is a baseline expectation.
* Open source BI software replaces vendor offerings.

Other lines of research include the combined study of business intelligence and uncertain data.&lt;ref&gt;{{Cite journal
 | last=Rodriguez | first=Carlos
 | last2=Daniel | first2=Florian
 | last3=Casati | first3=Fabio
 | last4=Cappiello | first4=Cinzia
 | year=2010
 | title=Toward Uncertain Business Intelligence: The Case of Key Indicators |doi=10.1109/MIC.2010.59
 | journal=IEEE Internet Computing
 | volume=14
 | issue=4
 | pages=32 }}&lt;/ref&gt;&lt;ref&gt;{{citation |author=Rodriguez, C. |author2=Daniel, F. |author3=Casati, F. |author4=Cappiello, C. |last-author-amp=yes |url=http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202009/Papers/3-C.pdf |title=Computing Uncertain Key Indicators from Uncertain Data  |pages=106&#8211;120 | conference = ICIQ'09 | year = 2009}}&lt;/ref&gt; In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.

According to a study by the Aberdeen Group, there has been increasing interest in [[Software-as-a-Service]] (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago&amp;nbsp;&#8211; 15% in 2009 compared to 7% in 2008.&lt;ref&gt;{{cite web|last1=Julian|first1=Taylor|title=Business intelligence implementation according to customer's needs|url=http://apro-software.com/services/software-development/business-intelligence|publisher=APRO Software|accessdate=16 May 2016|date=10 January 2010}}&lt;/ref&gt;

An article by InfoWorld&#8217;s Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.&lt;ref&gt;[http://infoworld.com/d/cloud-computing/saas-bi-growth-will-soar-in-2010-511 SaaS BI growth will soar in 2010 | Cloud Computing]. InfoWorld (2010-02-01). Retrieved 17 January 2012.&lt;/ref&gt;

An analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables&lt;ref&gt;{{cite web|url=http://www.appsbi.com/top-100-analytics-companies-ranked-and-scored-by-mattermark|title=Top 100 analytics companies ranked and scored by Mattermark -  Business Intelligence - Dashboards - Big Data|publisher=}}&lt;/ref&gt;

==See also==
{{colbegin|3}}
* [[Accounting intelligence]]
* [[Analytic applications]]
* [[Artificial intelligence marketing]]
* [[Business Intelligence 2.0]]
* [[Business process discovery]]
* [[Business process management]]
* [[Business activity monitoring]]
* [[Business service management]]
* [[Comparison of OLAP Servers]]
* [[Customer dynamics]]
* [[Data Presentation Architecture]]
* [[Data visualization]]
* [[Decision engineering]]
* [[Enterprise planning systems]]
* [[Infonomics]]
* [[Intelligent document|Document intelligence]]
* [[Integrated business planning]]
* [[Location intelligence]]
* [[Media intelligence]]
* [[Meteorological intelligence]]
* [[Mobile business intelligence]]
* [[Multiway Data Analysis]]
* [[Operational intelligence]]
* [[Business Information Systems]]
* [[Business intelligence tools]]
* [[Process mining]]
* [[Real-time business intelligence]]
* [[Runtime intelligence]]
* [[Sales intelligence]]
* [[Test and learn]]

{{colend}}

==References==
{{Reflist|30em}}

==Bibliography==
*Ralph Kimball ''et al.'' "The Data warehouse Lifecycle Toolkit" (2nd ed.) Wiley ISBN 0-470-47957-4
*Peter Rausch, Alaa Sheta, Aladdin Ayesh : ''Business Intelligence and Performance Management: Theory, Systems, and Industrial Applications'', Springer Verlag U.K., 2013, ISBN 978-1-4471-4865-4.

==External links==
* [http://online.sju.edu/resource/engineering-technology/key-role-hadoop-plays-in-business-intelligence "The Key Role Hadoop Plays in Business Intelligence and Data Warehousing" - St. Joseph's University]
* {{cite journal
|url=http://cacm.acm.org/magazines/2011/8/114953-an-overview-of-business-intelligence-technology/fulltext
|title=An Overview Of Business Intelligence Technology
|date=August 2011 | accessdate=26 October 2011
|first1=Surajit |last1=Chaudhuri |first2=Umeshwar |last2=Dayal |first3=Vivek |last3=Narasayya
|journal=Communications of the ACM
|volume =54 |issue= 8 |pages=88&#8211;98
|doi=10.1145/1978542.1978562 }}

{{Data warehouse}}

{{DEFAULTSORT:Business Intelligence}}
[[Category:Business intelligence| ]]
[[Category:Financial data analysis]]
[[Category:Data management]]
[[Category:Financial technology]]
[[Category:Information management]]</text>
      <sha1>netsuhhdi6gjifd9gofxt3sr8csfy6s</sha1>
    </revision>
  </page>
  <page>
    <title>Cut, copy, and paste</title>
    <ns>0</ns>
    <id>157115</id>
    <revision>
      <id>762387291</id>
      <parentid>762386663</parentid>
      <timestamp>2017-01-28T14:03:33Z</timestamp>
      <contributor>
        <username>David.moreno72</username>
        <id>16075528</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/78.148.132.216|78.148.132.216]] ([[User talk:78.148.132.216|talk]]): Violation of [[WP:ELNO|external links]] policy ([[WP:HG|HG]]) (3.1.20)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18889" xml:space="preserve">{{Redirect|Copy &amp; Paste|the album|Hurricane Venus}}
{{other uses|Cut and paste (disambiguation)}}
{{Refimprove|date=August 2008}}
In [[human&#8211;computer interaction]], '''cut''', '''copy''' and '''paste''' are related [[Command (computing)|commands]] that offer a [[user interface|user-interface]] [[interprocess communication]] technique for transferring [[data (computing)|data]]. The '''cut''' command removes the [[Selection (user interface)|selected data]] from its original position, while the '''copy''' command creates a duplicate; in both cases the selected data is kept in a temporary storage tool called the [[Clipboard (software)|clipboard]]. The data in the clipboard is later inserted in the position where the '''paste''' command is issued.

The command names are an [[interface metaphor]] based on the physical procedure used in [[manuscript]] editing to create a [[page layout]].

This [[interaction technique]] has close associations with related techniques in [[graphical user interface]]s that use [[pointing device]]s such as a [[computer mouse]] (by [[drag and drop]], for example).

The capability to replicate information with ease, changing it between contexts and applications, involves [[privacy]] concerns because of the risks of disclosure when handling [[Information sensitivity|sensitive information]]. Terms like ''cloning'', ''copy forward'', ''carry forward'', or ''re-use'' refer to the dissemination of such information through documents, and may be subject to regulation by [[administrative body|administrative bodies]].&lt;ref name="Laubach"&gt;{{cite web|url=http://hcca-info.org/portals/0/pdfs/resources/conference_handouts/regional_conference/2012/seattle/laubachwakefieldprint2.pdf|title=Cloning and Other Compliance Risks in Electronic Medical Records|last1=Laubach|first1=Lori|last2=Wakefield|first2=Catherine|date=June 8, 2012|publisher=[[Moss Adams LLP]], [[MultiCare]]|accessdate=April 23, 2014}}&lt;/ref&gt;

==History==

===Origins===
The term "''cut and paste''" comes from the traditional practice in manuscript-editings whereby people would cut paragraphs from a page with [[scissors]] and [[Adhesive|paste]] them onto another page. This practice remained standard into the 1980s. Stationery stores formerly sold "editing scissors" with blades long enough to cut an 8&#189;"-wide page. The advent of [[photocopier]]s made the practice easier and more flexible.

The act of copying/transferring text from one part of a computer-based document ("[[Data buffer|buffer]]") to a different location within the same or different computer-based document was a part of the earliest on-line computer editors. As soon as computer data entry moved from punch-cards to online files (in the mid/late 1960s) there were "commands" for accomplishing this operation. This mechanism was often used to transfer frequently-used commands or text snippets from additional buffers into the document, as was the case with the [[QED (text editor)|QED]] editor.&lt;ref name="communications1967"&gt;{{citation|doi=10.1145/363848.363863|last1=Deutsch|first1=L. Peter|authorlink1=L. Peter Deutsch|last2=Lampson|first2=Butler W.|authorlink2=Butler Lampson|title=An online editor|journal=Communications of the ACM |volume=10|issue=12|year=1967|pages=793&#8211;799, 803|url=http://research.microsoft.com/en-us/um/people/blampson/04-OnlineEditor/04-OnlineEditor.htm&lt;!-- http://portal.acm.org/citation.cfm?id=363848.363863&amp;coll=ACM&amp;dl=ACM&amp;CFID=15669714&amp;CFTOKEN=68334085 --&gt;}}, p. 793.&lt;/ref&gt;

===Early methods===
The earliest editors, since they were designed for [[teleprinter]] terminals, provided [[computer keyboard|keyboard]] commands to delineate contiguous regions of text, remove such regions, or move them to some other location in the file.  Since moving a region of text required first removing it from its initial location and then inserting it into its new location various schemes had to be invented to allow for this multi-step process to be specified by the user.

Often this was done by the provision of a 'move' command, but some text editors required that the text be first put into some temporary location for later retrieval/placement. In 1983, the [[Apple Lisa]] became the first text editing system to call that temporary location "the clipboard".

Earlier control schemes such as [[NLS (computer system)|NLS]] used a [[Linguistic typology#Subject.E2.80.93verb.E2.80.93object positioning|verb-object command structure]], where the command name was provided first and the object to be copied or moved was second. The inversion from [[Subject&#8211;verb&#8211;object|verb-object]] to [[Subject&#8211;object&#8211;verb|object-verb]] on which copy and paste are based, where the user selects the object to be operated before initiating the operation, was an innovation crucial for the success of the desktop metaphor as it allowed copy and move operations based on [[direct manipulation]].&lt;ref&gt;{{cite paper|title=Metaphors create theories for users|author=Kuhn, Werner|journal=Spatial Information Theory A Theoretical Basis for GIS|pages=366&#8211;376|year=1993|publisher=Springer}}&lt;/ref&gt;

===Popularization===
Inspired by early line and character editors that broke a move or copy operation into two steps&#8212;between which the user could invoke a preparatory action such as navigation&#8212;[[Lawrence G. Tesler]] (Larry Tesler) proposed the names "cut" and "copy" for the first step and "paste" for the second step. Beginning in 1974, he and colleagues at [[Xerox PARC|Xerox Corporation Palo Alto Research Center (PARC)]] implemented several text editors that used cut/copy-and-paste commands to move/copy text.&lt;ref&gt;{{cite web|url=http://www.designinginteractions.com/ |title=Bill Moggridge, Designing Interactions, MIT Press 2007, pp. 63&#8211;68 |publisher=Designinginteractions.com |date= |accessdate=2011-11-25}}&lt;/ref&gt;

[[Apple Computer]] widely popularized the computer-based cut/copy-and-paste paradigm through the [[Apple Lisa|Lisa]] (1983) and [[Apple Macintosh|Macintosh]] (1984) operating systems and applications. Apple mapped the functionalities to key combinations consisting of the [[Command key]] (a special [[modifier key]]) held down while typing the letters X (for cut), C (for copy), and V (for paste), choosing a handful of [[keyboard shortcuts]] to control basic editing operations. The keys involved all cluster together at the left end of the bottom row of the standard [[QWERTY]] keyboard, and each key is combined with a special [[modifier key]] to perform the desired operation:
* [[control-Z|Z]] to [[undo]]
* [[control-X|X]] to cut
* [[control-C|C]] to copy
* [[control-V|V]] to paste
The [[IBM Common User Access]] (CUA) standard also uses combinations of the [[Insert key|Insert]], [[Del key|Del]], [[Shift key|Shift]] and [[Control key]]s.  Early versions of [[Microsoft Windows|Windows]]{{Dubious|date=March 2014}} used the IBM standard. [[Microsoft]] later also adopted the Apple key combinations with the introduction of [[Microsoft Windows|Windows]]{{Dubious|date=January 2016}}, using the [[control key]] as [[modifier key]]. For users migrating to Windows from [[MS-DOS]] this was a big change as MS-DOS users used the "copy" and "move" commands.

Similar patterns of key combinations, later borrowed by others, remain widely available {{As of|2007|alt= today}} in most GUI text editors, word processors, and file system browsers.

== Cut and paste ==
Computer-based editing can involve very frequent use of cut-and-paste operations. Most software-suppliers provide several methods for performing such tasks, and this can involve (for example)  key combinations, pulldown menus, pop-up menus, or [[toolbar]] buttons.
# The user selects or "highlights" the text or file for moving by some method, typically by [[dragging]] over the text or file name with the pointing-device or holding down the [[Shift key]] while using the [[arrow keys]] to move the [[Cursor (computers)|text cursor]].
# The user performs a "cut" operation via key combination [[Control key|Ctrl]]+x ([[Command key|&#8984;]]+x for [[Macintosh]] users), menu, or other means.
# Visibly, "cut" text immediately disappears from its location.  "Cut" files typically change color to indicate that they will be moved.
# Conceptually, the text has now moved to a location often called the [[Clipboard (software)|clipboard]]. The clipboard typically remains invisible. On most systems only one clipboard location exists, hence another cut or copy operation overwrites the previously stored information. Many [[Unix|UNIX]] text-editors provide multiple clipboard entries, as do some Macintosh programs such as Clipboard Master,&lt;ref&gt;{{cite web |title=Clipboard Master |work=Clipboard Master 2.0 by In Phase Consulting, July 1994|url=http://forums.info-mac.org/viewtopic.php?f=243&amp;t=14244&amp;sid=739ce1119f88340c52dc2aed3c788fff |accessdate=14 September 2009}}&lt;/ref&gt; and Windows [[clipboard manager|clipboard-manager]] programs such as the one in [[Microsoft Office]].
# The user selects a location for insertion by some method, typically by clicking at the desired insertion point.
# A ''paste'' operation takes place which visibly inserts the clipboard text at the insertion point. (The paste operation does not typically destroy the clipboard text: it remains available in the clipboard and the user can insert additional copies at other points).
Whereas cut-and-paste often takes place with a mouse-equivalent in Windows-like GUI environments, it may also occur entirely from the keyboard, especially in [[Unix|UNIX]] [[text editor]]s, such as [[Pico (text editor)|Pico]] or [[vi]]. Cutting and pasting without a mouse can involve a selection (for which Ctrl+x is pressed in most graphical systems) or the entire current line, but it may also involve text after the [[cursor (computers)|cursor]] until the end of the line and other more sophisticated operations.

When a software environment provides ''cut'' and ''paste'' functionality, a nondestructive operation called ''copy''  usually accompanies them; ''copy'' places a copy of the selected text in the clipboard without removing it from its original location.

The clipboard usually stays invisible, because the operations of cutting and pasting, while actually independent, usually take place in quick succession, and the user (usually) needs no assistance in understanding the operation or maintaining mental context. Some application programs provide a means of viewing, or sometimes even editing, the data on the clipboard.

== Copy and paste ==
The term "copy-and-paste" refers to the popular, simple method of reproducing [[Character (computing)|text]] or other [[data]] from a source to a destination. It differs from '''cut and paste''' in that the original source text or data does not get deleted or removed. The popularity of this method stems from its simplicity and the ease with which users can move data between various applications visually &#8211; without resorting to [[Disk storage|permanent storage]].

Once one has copied data into the [[clipboard]], one may '''paste''' the contents of the clipboard into a destination document.

The [[X Window System]] maintains an additional clipboard containing the most recently selected text; middle-clicking pastes the content of this "selection" clipboard into whatever the [[pointer (computing WIMP)|pointer]] is on at that time.

Most [[terminal emulator]]s and some other applications support the key combinations Ctrl-Insert to copy and Shift-Insert to paste. This is in accordance with the [[IBM Common User Access]] (CUA) standard.

== Find and go ==
The [[NeXTStep]] operating system extended the concept of having a single copy buffer by adding a second system-wide ''' Find buffer''' used for searching. The Find buffer is also available in [[OSX|Mac OS X]].

Text can be placed in the Find buffer by either using the Find panel or by selecting text and hitting {{key press|&#8984;E}}.

The text can then be searched with '''Find Next''' {{key press|&#8984;G}} and '''Find Previous''' {{key press|&#8984;D}}.

The functionality comes in handy when for example editing [[source code]]. To find the occurrence of a variable or function name elsewhere in the file, simply select the name by double clicking, hit {{key press|&#8984;E}} and then jump to the next or previous occurrence with {{key press|&#8984;G}} / {{key press|&#8984;D}}.

Note that this does ''not'' destroy your copy buffer as with other [[User interface|UIs]] like [[Windows]] or the [[X Window System]].

Together with copy and paste this can be used for quick and easy replacement of repeated text:
* select the text that you want to replace (i.e. by double clicking)
* put the text in the Find buffer with {{key press|&#8984;E}}
* overwrite the selected text with your replacement text
* select the replacement text (try {{key press| &#9095;&#8679;&#8592;}} to avoid lifting your hands from the keyboard)
* copy the replacement text {{key press|&#8984;C}}
* find the next or previous occurrence {{key press|&#8984;G}} / {{key press|&#8984;D}}
* paste the replacement text {{key press|&#8984;V}}
* repeat the last two steps as often as needed
or in short:
* select {{key press|&#8984; E}}  {{key press|replstr}}  {{key press| &#9095;&#8679;&#8592;}}  {{key press|&#8984;C}}  {{key press|&#8984;G}}{{key press|&#8984;V}} {{key press|&#8984;G}}{{key press|&#8984;V}} ...
While this might sound a bit complicated at first, it is often ''much'' faster than using the find panel, especial when only a few occurrences shall be replaced or when only some of the occurrences shall be replaced. When a text shall not be replaced, simply hit {{key press|&#8984;G}} again to skip to the next occurrence.

The find buffer is system wide. That is, if you enter a text in the find panel (or with {{key press|&#8984;E}}) in one application and then switch to another application you can immediately start searching without having to enter the search text again.

== Common keyboard shortcuts ==
{| class="wikitable"
|-
! &amp;nbsp;
! Cut
! Copy
! Paste
|-
! Apple
| Command+X
| Command-C
| Command-V
|-
! Windows/GNOME/KDE
| Control-X / Shift-Delete
| Control-C / Control-Insert
| Control-V / Shift-Insert
|-
! GNOME/KDE terminal emulators
| &lt;!-- cut --&gt;
| Shift-Control-C / Control-Insert
| Shift-Control-V / Shift-Control-Insert (Shift-Insert for pasting selected text)
|-
! BeOS
| Alt-X
| Alt-C
| Alt-V
|-
! Common User Access
| Shift+Delete
| Control+Insert
| Shift+Insert
|-
! Emacs
| Control-W (to mark)&lt;br /&gt;Control-K (to end of line)
| [[Meta key|meta]]-W (to mark)
| Control-Y
|-
! vi
| d (delete)
| y (yank)
| p (put)
|-
! X Window System
| &lt;!-- cut --&gt;
| click-and-drag to highlight
| middle mouse button
|}

== Copy and paste automation ==
Copying data one by one from one application to another, such as from [[Microsoft Excel|Excel]] to a [[Form (HTML)|web form]], might involve a lot of manual work. Copy and paste can be automated with the help of a [[Computer program|program]] that would iterate through the values list and paste them to the active [[Window (computing)|application window]]. Such programs might come in the form of [[Macro (computer science)|macros]] or dedicated programs which involve more or less scripting. Alternatively, applications supporting [[simultaneous editing]] may be used to copy or move collections of items.

== Additional differences between moving and copying ==&lt;!-- This section is linked from [[Spreadsheet]] --&gt;
In a spreadsheet, moving (cut and paste) need not equate to copying (copy and paste) and then deleting the original: when moving, references to the moved cells may move accordingly.

[[Windows Explorer]] also differentiates moving from merely copy-and-delete: a "cut" file will not actually disappear until pasted elsewhere and cannot be pasted more than once. The icon fades to show the transient "cut" state until it is pasted somewhere. Cutting a second file while the first one is cut will release the first from the "cut" state and leave it unchanged. Shift+Delete cannot be used to cut files; instead it deletes them without using the Recycle bin.

== Multiple clipboards ==
Several editors allow copying text into or pasting text from specific clipboards, typically using a special keystroke-sequence to specify a particular clipboard-number.

[[Clipboard manager]]s can be very convenient productivity-enhancers by providing many more features than system-native clipboards. Thousands of clips from the clip history are available for future pasting, and can be searched, edited, or deleted. Favorite clips that a user frequently pastes (for example, the current date, or the various fields of a user's contact info) can be kept standing ready to be pasted with a few clicks or keystrokes.

Similarly, a '''kill ring''' provides a [[LIFO (computing)|LIFO]] [[stack (data structure)|stack]] used for cut-and-paste operations as a type of clipboard capable of storing multiple pieces of data.&lt;ref&gt;{{cite web|url=http://www.ai.sri.com/~gkb/general.html#kill-ring |title=GKB (Generic Knowledge Base) Editor user's manual |work=[[Artificial Intelligence Center]] |publisher=[[SRI International]] |accessdate=2011-11-25}}&lt;/ref&gt;
For example, the [[GNU Emacs]] text editor provides a kill ring.&lt;ref&gt;{{cite web|url=https://www.gnu.org/software/emacs/manual/html_mono/emacs.html#Kill-Ring |title=GNU Emacs manual |publisher=Gnu.org |date= |accessdate=2011-11-25}}&lt;/ref&gt;
Each time a user performs a cut or copy operation, the system adds the affected text to the ring. The user can then access the contents of a specific (relatively numbered) buffer in the ring when performing a subsequent paste-operation. One can also give kill-buffers individual names, thus providing another form of multiple-clipboard functionality.

==Use in healthcare==
Concerns have been raised over the use of copy and paste functions in healthcare documentation and [[electronic health records]]. There is potential for the introduction of [[medical error|errors]], [[information overload]], and [[fraud]].&lt;ref name="Laubach" /&gt;&lt;ref&gt;{{cite web|url=http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_050621.pdf|title=Appropriate Use of the Copy and Paste Functionality in Electronic Health Records|date=March 17, 2014|publisher=[[American Health Information Management Association]]|accessdate=April 23, 2014}}&lt;/ref&gt;

==Use in software development==
[[Copy and paste programming]] is an [[antipattern]] arising from the blind pasting of pre-existing code into another [[source code]] file.

== See also ==
* [[Clipboard (software)|Clipboard]]
* [[Control key]]
* [[Cut and paste job]]
* [[Drag and drop]]
* [[Photomontage]]
* [[Publishing Interchange Language]]
* [[Simultaneous editing]]
* [[X Window selection]]

== References ==
{{Reflist}}

== External links ==
* [http://tronche.com/gui/x/icccm/sec-2.html 2. Peer-to-Peer Communication by Means of Selections] in the [[ICCCM]]

[[Category:User interface techniques]]
[[Category:Data management]]
[[Category:Clipboard (computing)]]</text>
      <sha1>s1e4b6bqomqly4u9q3m6u6qieewxtqe</sha1>
    </revision>
  </page>
  <page>
    <title>Compound document</title>
    <ns>0</ns>
    <id>299663</id>
    <revision>
      <id>751180579</id>
      <parentid>741456471</parentid>
      <timestamp>2016-11-23T21:49:47Z</timestamp>
      <contributor>
        <username>Adam Katz</username>
        <id>136718</id>
      </contributor>
      <comment>linked to [[Compound File Binary Format]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1742" xml:space="preserve">{{about|compound documents in general|the W3C standard|Compound Document Format}}
{{refimprove|date=November 2015}}
In [[computing]], a '''compound document''' is a document type typically produced using [[word processor|word processing]] software, and is a regular text document intermingled with non-text elements such as [[spreadsheet]]s, [[picture]]s, [[digital video]]s, [[digital audio]], and other [[multimedia]] features. It can also be used to collect several documents into one.

Compound document [[technology|technologies]] are commonly utilized on top of a [[software componentry]] framework, but the idea of software componentry includes several other concepts apart from compound documents, and software components alone do not enable compound documents. Well-known technologies for compound documents include:

*[[ActiveX Document]]s
*[[Bonobo (computing)|Bonobo]] by [[Ximian]] (primarily used by [[GNOME]])
*[[KPart]]s in [[KDE]]
*[[Multipurpose Internet Mail Extensions]]
*[[Object linking and embedding]] (OLE) by [[Microsoft]]; see [[Compound File Binary Format]]
*[[Open Document Architecture]] from [[ITU-T]] (not used)
*[[OpenDoc]] by [[Apple Computer]] (now defunct)
*[http://sourceforge.net/projects/verdantium Verdantium]
*[[XML]] and [[Extensible Stylesheet Language|XSL]] are encapsulation formats used for compound documents of all kinds

The first public implementation was on the [[Xerox Star]] [[workstation]], released in 1981.&lt;ref&gt;http://www.digibarn.com/collections/systems/xerox-8010/index.html&lt;/ref&gt;

==See also==
* [[COM Structured Storage]]
* [[Transclusion]]
* [[Electronic Notebook]]

==References==
{{Reflist}}

[[Category:Electronic documents]]
[[Category:Multimedia]]

{{Multimedia-software-stub}}</text>
      <sha1>boiohdw3h7bo221drpgrid5pm5j64rr</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic article</title>
    <ns>0</ns>
    <id>1005736</id>
    <revision>
      <id>737642855</id>
      <parentid>643129478</parentid>
      <timestamp>2016-09-04T04:45:41Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3215" xml:space="preserve">'''Electronic articles''' are [[Article (publishing)|article]]s in [[academic journal|scholarly journal]]s or [[magazine]]s  that can be accessed via electronic transmission. They are a specialized form of [[electronic document]], with a specialized content, purpose, format, [[metadata]], and availability&amp;ndash;they consist of individual articles from scholarly journals or  magazines (and now sometimes popular magazines), they have the purpose of providing material for academic [[research]] and study, they are formatted approximately like printed journal articles, the metadata is entered into specialized databases, such as the [[Directory of Open Access Journals]] as well as the databases for the discipline, and they are predominantly available through [[academic library|academic libraries]] and special [[library|libraries]], generally at a fixed charge. 

Electronic articles can be found in [[online and offline|online]]-only journals (par excellence), but in the 21st century they have also become common as online versions of articles that also appear in printed journals. The practice of [[Electronic publishing|publishing of an electronic version]] of an article before it later appears in print is sometimes called '''epub ahead of print''', particularly in [[PubMed]].&lt;ref&gt;{{cite web |url=http://www.nlm.nih.gov/services/ldepubahead.html |title=FAQ: Loansome Doc Article Ordering Service - Epub Ahead of Print |work= |accessdate=2010-10-23}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |title=Himmelfarb Library Blog: Epub ahead of print&#8230; What does this mean?? |format= |work= |archiveurl=https://web.archive.org/web/20100119081653/http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |archivedate=2010-01-19 |deadurl=yes }}&lt;/ref&gt;

The term can also be used for the electronic versions of less formal publications, such as online archives, working paper archives from universities, government agencies, private and public think tanks and institutes and private websites. In many academic areas, specialized [[bibliographic database]]s are available to find their online content.

Most commercial sites are [[subscription business model|subscription]]-based, or allow pay-per-view access. Many universities subscribe to electronic journals to provide access to their students and faculty, and it is generally also possible for individuals to subscribe. An increasing number of journals are now available with open access, requiring no subscription. Most working paper archives and articles on personal homepages are free, as are collections in [[institutional repository|institutional repositories]] and [[disciplinary repository|subject repositories]].

The most common formats of transmission are [[HTML]], [[Portable Document Format|PDF]] and, in specialized fields like mathematics and physics, [[TeX]] and [[PostScript]].

==See also==
* [[Academic publishing]]
* [[Eprint]]
* [[Electronic journal]]
* [[Scholarly article]]

== References ==
{{reflist}}

[[Category:Academic publishing]]
[[Category:Electronic publishing]]
[[Category:Electronic documents]]</text>
      <sha1>94d8dp10ur695gsnfblszfl3g7zmhn1</sha1>
    </revision>
  </page>
  <page>
    <title>E-bible</title>
    <ns>0</ns>
    <id>25199152</id>
    <revision>
      <id>654173743</id>
      <parentid>654173240</parentid>
      <timestamp>2015-03-30T12:44:02Z</timestamp>
      <contributor>
        <ip>46.231.189.26</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1069" xml:space="preserve">{{Orphan|date=December 2009}}
{{Refimprove|date=December 2009}}

Sometimes known as document bibles or transaction deal bibles, '''e-bibles''' are a means of storing, indexing and comprehensively searching large volumes of [[document]]s related to any corporate transaction. 

They are commonly used by [[Legal firm]]s to collate documents from a certain case in order to store or give to a client at the end of a project. e-bibles are a means of storing complex legal folders which were usually kept in hard copy.

In 2009, Proposals&lt;ref&gt;http://www.litig.org/index.php?option=com_content&amp;task=category&amp;sectionid=2&amp;id=20&amp;Itemid=33 &lt;/ref&gt; were put in place in order to standardise the creation of e-bibles throughout the [[legal industry]].

There are few suppliers of COTS solutions, however Diskbuilder&lt;ref&gt;http://www.diskbuilder.co.uk&lt;/ref&gt;  and Ideagen&lt;ref&gt;https://www.ideagenplc.com/&lt;/ref&gt; (formerly Capgen) are notable exceptions.
== References ==
{{Reflist}}

[[Category:Legal documents]]
[[Category:Electronic documents]]
[[Category:Document management systems]]</text>
      <sha1>3h70nx0rnaezrbdh1zj26bpdacm3r59</sha1>
    </revision>
  </page>
  <page>
    <title>Xena (software)</title>
    <ns>0</ns>
    <id>27437313</id>
    <revision>
      <id>739186429</id>
      <parentid>730724064</parentid>
      <timestamp>2016-09-13T08:05:45Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6396" xml:space="preserve">{{Other uses|Xena (disambiguation)}}

'''Xena''' is [[open-source software]] for use in [[digital preservation]]. Xena is short for XML Electronic Normalising for Archives.

Xena is a [[Java (programming language)|Java]] application developed by the [[National Archives of Australia]]. It is available free of charge under the [[GNU General Public License]].

Version 6.1.0 was released 31 July 2013. Source code and binaries for Linux, OS X and Windows are available from [[SourceForge]].

==Mode of operation==
Xena attempts to avoid [[digital obsolescence]] by converting files into an openly specified format, such as [[OpenDocument|ODF]] or [[Portable Network Graphics|PNG]]. If the file format is not supported or the Binary Normalisation option is selected, Xena will perform [[ASCII]] [[Base64]] encoding on binary files and wrap the output in XML metadata. The resulting .xena file is plain text, although the content of the data itself is not directly human-readable. The exact original file can be retrieved by stripping the metadata and reversing the Base64 encoding, using an internal viewer.

==Features==
Platforms supported by Xena are [[Microsoft Windows]], [[Linux]] and [[Mac OS X]].

Xena uses a series of plugins to identify file formats and convert them to an appropriate openly specified format.

Xena has an [[application programming interface]] which allows any reasonably skilled Java developer to develop a plugin to cover a new file type.

Xena can process individual files or whole directories. When processing a whole directory, it can preserve the original directory structure of the converted records.

Xena can create plain text versions of file formats such as [[Tagged Image File Format|TIFF]], [[Microsoft Word|Word]] and [[Portable Document Format|PDF]], with the use of [[Tesseract (software)]].

The Xena interface or Xena Viewer can be used to view or export a Xena file (extension .xena) in its target file format. These files contain the normalised file as well as any extra information relevant to the normalisation process.
The Xena Viewer supports bulk export of Xena files to target file formats.

Xena can be used via its [[graphical user interface]] or the [[command line]].

For Xena to be fully functional, it requires a local installation of the following external software:
*[[LibreOffice]] suite - to convert office documents to OpenDocument format
*[[Tesseract (software)|Tesseract]] - to create plain text versions of file formats
*[[ImageMagick]] - to convert a subset of image files to [[Portable Network Graphics|PNG]]
*Readpst - to convert [[Microsoft Outlook]] PST files to XML. Readpst is part of the free and open source [http://www.five-ten-sg.com/libpst/ libpst software suite].
*[[Free Lossless Audio Codec|FLAC]] - to convert audio files to FLAC format. This is also required to play back audio files using Xena.

==Supported file types==
Xena will recognize and process the file types listed below, plus a few others of minor importance. Unsupported file types will automatically undergo binary normalization.

Office file formats:
*[[Microsoft Office]] files (including [[Microsoft Office XML formats|MS Office XML]], [[SYLK]] spreadsheets and [[Rich Text Format]]) are converted to the corresponding OpenDocument files
*[[Microsoft Outlook]] [[Personal Storage Table|PST]] files are parsed for their individual messages, which are converted to XML files and a Xena index file is created
*[[Microsoft Project]] MPP files are converted to XML
*[[OpenOffice.org XML]] files (SXC, SXI, SXW) are converted to the corresponding OpenDocument formats
*[[WordPerfect]] WPD files are converted to OpenDocument ODT
*[[OpenDocument]] documents (ODT, ODS, ODB, ODP) are preserved unchanged
*Acrobat PDF files are stored as binaries
*Mailbox files (MBX) are converted to individual XML files

Graphics:
*[[BMP file format|BMP]], [[Graphics Interchange Format|GIF]], [[Adobe Photoshop|PSD]], [[PCX]], [[.ras|RAS]], and the [[X Window System]] [[X BitMap|XBM]] and [[X PixMap|XPM]] bitmap files are converted to [[Portable Network Graphics|PNG]]; [[Tagged Image File Format|TIFF]] files additionally get embedded metadata stored in Xena XML. If the [[Tesseract (software)|Tesseract]] [[Optical character recognition|OCR software]] is installed, text will be extracted from TIFF files.
*OpenDocument Drawings (ODG) and [[Scalable Vector Graphics|SVG]] files are wrapped in Xena XML
*JPG and PNG files are stored unchanged

Archive Files:
*Files are extracted from [[File archiver|archives]] ([[ZIP (file format)|ZIP]], [[gzip|GZIP]], [[tar (file format)|TAR/TAR.gz]], [[JAR (file format)|JAR]], [[WAR (Sun file format)|WAR]], Mac binary) and normalised into a separate Xena file. A Xena index file is created, which when opened in the internal Xena viewer will display the files in a table.

Audio files:
*[[MP3]], [[WAV]], [[AIFF]], and [[Vorbis|OGG]] formats are converted to [[Free Lossless Audio Codec|FLAC]] files.

Databases:
*[[SQL]] files are processed as plain text wrapped in XML

Other file types:
*HTML is converted to XHTML
*TXT text files are stored as plain text wrapped in XML; CSS files are stored as plain text wrapped in XML

==Reviews==
An April 22, 2010 review in Practical e-Records rated Xena at 82/100 points. At present Xena has no target preservation format for video files.&lt;ref&gt;{{cite web |url=http://e-records.chrisprom.com/review-of-xena-normalization-software/ |title=Review of XENA Normalization Software |date=2010-04-22 |accessdate= |archiveurl=http://archive.is/yKw1 |archivedate=2012-07-08}}&lt;/ref&gt;

==References==
&lt;references/&gt;

==External links==
*[http://xena.sourceforge.net/ Xena on SourceForge]
*[http://sourceforge.net/apps/mediawiki/xena/index.php?title=Main_Page Xena wiki on SourceForge]
*[https://web.archive.org/web/20100610095405/http://www.ask-oss.mq.edu.au/index.php?option=com_content&amp;task=view&amp;id=66&amp;Itemid=69 Xena project description at The Australian Service for Knowledge of Open Source Software]
*[http://www.naa.gov.au/records-management/secure-and-store/e-preservation/at-naa/software.aspx#section1 National Archives of Australia - software]

{{DEFAULTSORT:Xena (Software)}}
[[Category:Digital preservation]]
[[Category:Electronic documents]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Binary-to-text encoding formats]]
[[Category:Mass digitization]]</text>
      <sha1>r6i5prgqjrfcyof6glexnqao4k2h373</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of e-book formats</title>
    <ns>0</ns>
    <id>12115370</id>
    <revision>
      <id>762840547</id>
      <parentid>759567167</parentid>
      <timestamp>2017-01-31T01:00:06Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="51312" xml:space="preserve">The following is a '''comparison of e-book formats''' used to create and publish [[e-book]]s.

The [[EPUB]] format is the most widely supported vendor-independent [[XML]]-based (as opposed to [[Portable Document Format|PDF]]) e-book format; that is, it is supported by the largest number of e-Readers, including [[Kindle Fire|Amazon Kindle Fire]] (but not standard Kindle).&lt;ref name="kdp.amazon.com"&gt;{{cite web|url=https://kdp.amazon.com/help?topicId=A2GF0UFHIYG9VQ |title=Amazon Kindle Direct Publishing: Get help with self-publishing your book to Amazon's Kindle Store |publisher=Kdp.amazon.com |date= |accessdate=2015-08-31}}&lt;/ref&gt; See table below for details. 
{{TOC right}}

==Format descriptions==
Formats available include, but are not limited to:

===Broadband eBooks (BBeB) ===
{{main article |BBeB}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Sony media
|-
| style="background:#ddd;"| ''Published as'':
| .lrf; .lrx
|}
The digital book format originally used by [[Sony Corporation]].  It is a proprietary format, but some reader software for general-purpose computers, particularly under [[GNU Project|GNU]]/Linux (for example, [[Calibre (software)|Calibre]]'s internal viewer&lt;ref&gt;{{Citation | title = About | url = http://calibre-ebook.com/about | publisher = Calibre}}&lt;/ref&gt;), have the capability to read it.  The LRX file extension represents a [[Digital rights management|DRM]] encrypted eBook. More recently, Sony has converted its books from BBeB to EPUB and is now issuing new titles in EPUB.

===Comic Book Archive file ===
{{main article|Comic book archive}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| compressed images
|-
| style="background:#ddd;"| ''Published as'':
|.cbr (RAR); .cbz (ZIP); .cb7 (7z); .cbt (TAR); .cba (ACE)
|}

===Compiled HTML ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[Microsoft Compiled HTML Help]]
|-
| style="background:#ddd;"| ''Published as'':
| .chm
|}
CHM format is a proprietary format based on HTML. Multiple pages and embedded graphics are distributed along with [[metadata]] as a single compressed file. The indexing is both for keywords for full text search.

===DAISY &#8211; ANSI/NISO Z39.86  ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[DAISY Digital Talking Book|DAISY]]
|-
| style="background:#ddd;"| ''Published as'':
|
|}

The Digital Accessible Information SYstem (DAISY) is an [[XML]]-based open standard maintained by the DAISY Consortium for people with [[print disabilities]].  DAISY has wide international support with features for multimedia, navigation and synchronization. A subset of the DAISY format has been adopted by law in the United States as the National Instructional Material Accessibility Standard (NIMAS), and K-12 textbooks and instructional materials are now required to be provided to students with disabilities.

DAISY is already aligned with the EPUB technical standard, and is expected to fully converge with its forthcoming EPUB3 revision.&lt;ref&gt;{{cite web|url=http://www.daisy.org/z3986 |title=DAISY Standard &amp;#124; DAISY Consortium |publisher=Daisy.org |date= |accessdate=2015-08-31}}&lt;/ref&gt;

===DjVu ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| DjVu
|-
| style="background:#ddd;"| ''Published as'':
| .[[DjVu|djvu]]
|}
DjVu is a format specialized for storing scanned documents. It includes advanced compressors optimized for low-color images, such as text documents. Individual files may contain one or more pages. DjVu files cannot be re-flowed.

The contained page images are divided in separate layers (such as multi-color, low-resolution, background layer using [[lossy compression]], and few-colors, high-resolution, tightly compressed foreground layer), each compressed in the best available method. The format is designed to decompress very quickly, even faster than vector-based formats.

The advantage of DjVu is that it is possible to take a high-resolution scan (300&#8211;400 DPI), good enough for both on-[[screen reading]] and printing, and store it very efficiently. Several dozens of 300 DPI black-and-white scans can be stored in less than a megabyte.

===DOC===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Word
|-
| style="background:#ddd;"| ''Published as'':
| .[[Doc (computing)|DOC]]
|}

[[Doc (computing)|DOC]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format is that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.

===DOCX===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Word (XML)
|-
| style="background:#ddd;"| ''Published as'':
| .[[DOCX]]
|}

[[DOCX]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format are that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.

=== EPUB ===
{{Main article|EPUB}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IDPF/EPUB
|-
| style="background:#ddd;"| ''Published as'':
| .epub
|}
[[File:EPUB logo.svg|thumb|right|150px|The EPUB logo]]
The .epub or [[OEBPS]] format is a technical standard for e-books created by the [[International Digital Publishing Forum]] (IDPF).

The EPUB format has gained some popularity as a vendor-independent XML-based e-book format. The format can be read by the [[Kobo eReader]], [[BlackBerry]] devices, Apple's [[iBooks]] app running on [[Macintosh]] computers and [[IOS (Apple)|iOS]] devices, [[Google Play|Google Books]] app running on [[Android (operating system)|Android]] and iOS devices, Barnes &amp; Noble [[Nook]], Amazon [[Kindle Fire]],&lt;ref name="kdp.amazon.com"/&gt; [[Sony Reader]], [[BeBook]], [[Cybook Gen3|Bookeen Cybook Gen3 (with firmware v2 and up)]], COOL-ER, [[Adobe Digital Editions]], [[Lexcycle Stanza]], BookGlutton, AZARDI, [[FBReader]], [[Aldiko]], [[CoolReader]], [[Mantano Reader]], [[Moon+ Reader]], the [[Mozilla Firefox]] [[Add-on (Mozilla)|add-on]] [[EPUBReader]], [[Okular]] and other reading apps.

[[Adobe Digital Editions]] uses .epub format for its e-books, with [[digital rights management]] (DRM) protection provided through their proprietary ADEPT mechanism. The ADEPT framework and scripts have been reverse-engineered to circumvent this DRM system.&lt;ref&gt;{{cite web|author= |url=http://i-u2665-cabbages.blogspot.com/2009/02/circumventing-adobe-adept-drm-for-epub.html |title=i&#9829;cabbages: Circumventing Adobe ADEPT DRM for EPUB |publisher=I-u2665-cabbages.blogspot.com |date=2009-02-18 |accessdate=2015-08-31}}&lt;/ref&gt;

===eReader ===
;Formerly Palm Digital Media/Peanut Press
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Palm Media
|-
| style="background:#ddd;"| ''Published as'':
| .[[PDB (Palm OS)|pdb]]
|}

eReader is a [[freeware]] program for viewing Palm Digital Media electronic books which use the pdb format used by many Palm applications. Versions are available for [[Android (operating system)|Android]], [[BlackBerry]], [[IOS (Apple)|iOS]], [[Palm OS]] (not webOS), [[Symbian]], [[Windows Mobile]] Pocket PC/Smartphone, and [[OS X]]. The reader shows text one page at a time, as paper books do. eReader supports embedded hyperlinks and images. Additionally, the [[Lexcycle Stanza|Stanza]] application for the [[iPhone]] and [[iPod touch]] can read both [[encryption|encrypted]] and unencrypted eReader files.

The program supports features like bookmarks and footnotes, enabling the user to mark any page with a bookmark and any part of the text with a footnote-like commentary. Footnotes can later be exported as a Memo document.

On July 20, 2009, [[Barnes &amp; Noble]] made an announcement&lt;ref&gt;{{cite web|url=http://www.barnesandnobleinc.com/press_releases/2009_july_20_ebookstore.html |title=Barnes &amp; Noble Booksellers |publisher=Barnesandnobleinc.com |date=2009-07-20 |accessdate=2015-08-31}}&lt;/ref&gt; implying that eReader would be the company's preferred format to deliver e-books. Exactly three months later, in a press release by [[Adobe Systems|Adobe]], it was revealed Barnes &amp; Noble would be joining forces with the software company to standardize the EPUB and PDF eBook formats.&lt;ref&gt;{{cite press release
 | title = Barnes &amp; Noble adopts open EPUB eBook Format, PDF and Adobe Content Server | publisher = [[Adobe Systems]] | date = 2009-10-20 | url = https://www.adobe.com/aboutadobe/pressroom/pressreleases/200910/AdobeandBarnesNobleJoinForcestoStandardizeeBookTechnology.html | accessdate = 2013-05-06}}&lt;/ref&gt;&lt;ref&gt;{{Citation|last=Rothman |first=David |title=&#8216;Barnes &amp; Noble adopts open EPUB eBook Format, PDF and Adobe Content Server&#8217; |publisher=TeleRead |date=2009-10-20 |url=http://www.teleread.com/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130506010320/http://www.teleread.com:80/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |archivedate=2013-05-06 |deadurl=yes |df= }}&lt;/ref&gt; Barnes &amp; Noble e-books are now sold mostly in EPUB format.&lt;ref&gt;{{Citation | last = Bell | first = Ian | title = Barnes &amp; Noble Adopts ePub Standard; Aligns With Adobe | publisher = [[Digital Trends]] | date = 2009-11-18 | url = http://www.digitaltrends.com/gadgets/barnes-aligns-with-adobe/ | accessdate = 2013-05-06}}&lt;/ref&gt;&lt;ref&gt;{{Citation|last=Meadows |first=Chris |title=Barnes &amp; Noble quietly changes e-book format, neglects to tell consumers |publisher=TeleRead |date=2009-12-13 |url=http://www.teleread.com/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130130085503/http://www.teleread.com:80/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |archivedate=2013-01-30 |deadurl=yes |df= }}&lt;/ref&gt;&lt;ref&gt;{{Citation | last = James | first = Kendrick | title = Has Barnes &amp; Noble Changed Its e-Book Format to ePUB? | publisher = [[GigaOM]] | date = 2009-12-14 | url = http://gigaom.com/2009/12/14/has-barnes-noble-changed-its-e-book-format-to-epub/ | accessdate = 2013-05-06}}&lt;/ref&gt;

===FictionBook (Fb2) ===

{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| FictionBook
|-
| style="background:#ddd;"| ''Published as'':
| .[[FictionBook|fb2]]
|}

[[FictionBook]]&lt;ref&gt;[http://haali.cs.msu.ru/pocketpc/FictionBook_description.html]  {{webarchive |url=https://web.archive.org/web/20070703204958/http://haali.cs.msu.ru/pocketpc/FictionBook_description.html |date=July 3, 2007 }}&lt;/ref&gt; is a popular [[XML]]-based e-book format, supported by free readers such as [[FBReader]], [[Okular]], [[CoolReader]], [[Bebook]] and [[STDU Viewer]].

The FictionBook format does not specify the appearance of a document; instead, it describes its structure and semantics. All the ebook metadata, such as the author name, title, and publisher, is also present in the ebook file. Hence the format is convenient for automatic processing, indexing, and ebook collection management. This also is convenient to store books in it for later automatic conversion into other formats.

===Founder Electronics ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Apabi Reader
|-
| style="background:#ddd;"| ''Published as'':
| .[[XEB|xeb]]; .ceb
|}
[[APABI]] is a format devised by [[Founder Electronics]]. It is a popular format for Chinese e-books. It can be read using the [[Apabi Reader]] software, and produced using [[Apabi Publisher]]. Both .xeb and .ceb files are encoded binary files. The [[iLiad (E-book Reader)|Iliad]] e-book device includes an Apabi 'viewer'.

===Hypertext Markup Language ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Hypertext
|-
| style="background:#ddd;"| ''Published as'':
| .htm; .html and typically auxiliary images, js and css
|}
[[HTML]] is the [[markup language]] used for most [[World Wide Web|web]] pages. E-books using HTML can be read using a [[Web browser]]. The specifications for the format are  available without charge from the [[W3C]].

HTML adds specially marked meta-elements to otherwise plain text encoded using [[character set]]s like [[ASCII]] or [[UTF-8]]. As such, suitably formatted files can be, and sometimes are, generated ''by hand'' using a ''[[text editor|plain text editor]]'' or ''[[Source code editor|programmer's editor]]''. Many ''HTML generator'' applications exist to ease this process and often require less intricate knowledge of the format details involved.

HTML on its own is not a particularly efficient format to store information in, requiring more storage space for a given work than many other formats. However, several e-Book formats including the Amazon Kindle, Open eBook, Compiled HTML,  Mobipocket and EPUB store each book chapter in HTML format, then use [[ZIP (file format)|ZIP]] compression to compress the HTML data, images, metadata and style sheets into a single, significantly smaller, file.

HTML files encompass a wide range of standards&lt;ref&gt;{{cite web|url=http://www.webstandards.org/learn/faq/ |title=Frequently Asked Questions (FAQ) - The Web Standards Project |publisher=Webstandards.org |date=2002-02-27 |accessdate=2015-08-31}}&lt;/ref&gt; and displaying HTML files correctly can be complicated. Additionally many of the features supported, such as forms, are not relevant to e-books.

===iBook (Apple) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| iBook
|-
| style="background:#ddd;"| ''Published as'':
| .ibooks
|}
The .ibooks format is created with the free [[iBooks Author]] ebook layout software from [[Apple Inc.]]. This proprietary format is based on the [[EPUB]] standard, with some differences in the CSS tags used in an ibooks format file, thus making it incompatible with the EPUB specification. The End-User Licensing Agreement (EULA) that comes with iBooks Author states that "If you want to charge a fee for a work that includes files in the .ibooks format generated using iBooks Author, you may only sell or distribute such work through Apple". The "through Apple" will typically be in the Apple [[iBooks]] store. The EULA further states that "This restriction does not apply to the content of such works when distributed in a form that does not include files in the .ibooks format." Therefore, Apple has not included distribution restrictions in the iBooks Author EULA for ibooks format ebooks created in iBooks Author that are made available for free, and it does not prevent authors from re-purposing the content in other ebook formats to be sold outside the iBookstore. This software currently supports import and export functionally for three formats. ibook, Plain text and PDF. The iBooks Author 2.3 and later supports importing EPUB and export EPUB 3.0.&lt;ref&gt;{{Cite web|url=https://support.apple.com/en-us/HT204884|title=About ePubs created with iBooks Author|language=en-US|access-date=2016-09-25}}&lt;/ref&gt;

===IEC 62448===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IEC 62448
|-
| style="background:#ddd;"| ''Published as'':
|
|}

IEC 62448 is an international standard created by [[International Electrotechnical Commission]] (IEC), Technical Committee 100, Technical Area 10 (Multimedia e-publishing and e-book).

The current version of IEC 62448 is an umbrella standard that contains as appendices two concrete formats, XMDF of Sharp and BBeB of Sony. However, BBeB has been discontinued by Sony and the version of XMDF that is in the specification is out of date. The IEC TA10 group is discussing the next steps, and has invited the IDPF organization which has standardized [[EPUB]] to be a liaison. It is possible that the current version of EPUB and/or the forthcoming EPUB3 revision may be added to IEC 62448.  Meanwhile, a number of Japanese companies have proposed that IEC standardize a proposed new Japanese-centric file format that is expected to unify DotBook of Voyager Japan and XMDF of Sharp.  This new format has not been publicly disclosed as of November 2010 but it is supposed to cover basic representations for the Japanese language.  Technically speaking, this revision is supposed to provide a Japanese minimum set, a Japanese extension set, and a stylesheet language. These issues were discussed in the TC100 meeting held  in October 2010 but no decisions were taken besides offering the liaison status to IDPF.

===INF (IBM) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IBM &amp; Open Source
|-
| style="background:#ddd;"| ''Published as'':
| .inf
|}
[[IBM]] created this e-book format and used it extensively for [[OS/2]] and other of its operating systems. The INF files were often digital versions of printed books that came with some bundles of OS/2 and other products. There were many other newsletters and monthly publications (e.g.: EDM/2) available in the INF format too.

The advantage of INF is that it is very compact and very fast. It also supports images, reflowed text, tables and various list formats. INF files get generated by compiling the markup text files &#8212; in the [[Information Presentation Facility]] (IPF) format &#8212; into binary files.

Originally only IBM created an INF viewer and compiler, but later open source viewers like NewView, DocView and others appeared. There is also an open source IPF compiler named WIPFC, created by the [[Open Watcom]] project.

===KF8 (Amazon Kindle) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Kindle
|-
| style="background:#ddd;"| ''Published as'':
| .azw3; .azw; .kf8
|}
With the release of the [[Kindle Fire]] reader in late 2011, [[Amazon.com]] also released [[Kindle Format 8]], their newest file format, also known as .AZW3. The .azw3 file format supports a subset of [[HTML5]] and [[CSS3]] features, with some additional nonstandard features; the new data is stored within a container which can also be used to store a Mobi content document, allowing limited backwards compatibility.&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/feature.html?docId=1000729511|title=Kindle Format 8 Overview|publisher=Amazon.com|year=2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://musingsandmarvels.com/2012/03/06/the-new-kindle-format-8-kf8/|title=The New Kindle Format KF8|publisher=Musings and Marvels:Learning the ins and outs of the publishing industry|date=2012-03-06|accessdate=2012-03-16}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/feature.html/ref=amb_link_357613502_6?ie=UTF8&amp;docId=1000729901&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_s=right-4&amp;pf_rd_r=0GN9VRRB0NJ08VXGFKWK&amp;pf_rd_t=1401&amp;pf_rd_p=1343256942&amp;pf_rd_i=1000729511|title=HTML5 tags supported by KF8|publisher=Amazon.com|accessdate=2012-03-16}}&lt;/ref&gt;

Older [[Amazon Kindle|Kindle]] e-readers use the proprietary format, AZW. It is based on the [[Mobipocket]] standard, with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]) and its own [[Digital rights management|DRM]] formatting. Because the ebooks bought on the Kindle are delivered over its wireless system called Whispernet, the user does not see the AZW files during the download process. The Kindle format is available on a variety of platforms, such as through the Kindle app for the various mobile device platforms.

===Microsoft LIT ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Reader
|-
| style="background:#ddd;"| ''Published as'':
| .[[LIT (file format)|lit]]
|}
DRM-protected LIT files are only readable in the proprietary [[Microsoft Reader]] program, as the .LIT format, otherwise similar to Microsoft's [[Microsoft Compiled HTML Help|CHM]] format, includes [[Digital Rights Management]] features. Other third party readers, such as [[Lexcycle Stanza]], can read unprotected LIT files.

The Microsoft Reader uses patented [[ClearType]] display technology. In Reader navigation works with a keyboard, mouse, stylus, or through electronic bookmarks. The Catalog Library records reader books in a personalized "home page", and books are displayed with ClearType to improve readability.  A user can add annotations and notes to any page, create large-print e-books with a single command, or create free-form drawings on the reader pages.  A built-in dictionary allows the user to look up words.

In August 2011, Microsoft announced they were discontinuing both Microsoft Reader and the use of the .lit format for ebooks&lt;ref&gt;{{cite web|url=http://aazae.com/|title=Ebooks|work=Aazae}}&lt;/ref&gt; at the end of August 2012, and ending sales of the format on November 8, 2011.&lt;ref&gt;"Microsoft is discontinuing Microsoft Reader effective August 30, 2012, which includes download access of the Microsoft Reader application from the Microsoft Reader website."[http://www.microsoft.com/reader/ Microsoft Reader] {{webarchive |url=https://web.archive.org/web/20050822035209/http://www.microsoft.com/reader/ |date=August 22, 2005 }}&lt;/ref&gt;

===Mobipocket ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Mobipocket
|-
| style="background:#ddd;"| ''Published as'':
| [[PRC (Palm OS)|.prc]]; .mobi
|}
The [[Mobipocket]] e-book format is based on the [[Open eBook]] standard using [[XHTML]] and can include [[JavaScript]] and frames. It also supports native [[SQL]] queries to be used with embedded databases. There is a corresponding e-book reader.

The [[Mobipocket]] Reader has a home page library. Readers can add blank pages in any part of a book and add free-hand drawings. Annotations &#8211; highlights, bookmarks, corrections, notes, and drawings &#8211; can be applied, organized, and recalled from a single location. Images are converted to GIF format and have a maximum size of 64K,&lt;ref&gt;{{cite web|url=http://www.mobipocket.com/dev/article.asp?BaseFolder=creatorhome&amp;File=image.htm |title=Mobipocket Developer Center - Importing Image files |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}&lt;/ref&gt; sufficient for mobile phones with small screens, but rather restrictive for newer gadgets. [[Mobipocket]] Reader has electronic bookmarks,  and a built-in dictionary.

The reader has a full screen mode for reading and support for many [[Personal digital assistant|PDAs]], [[Personal digital assistant|Communicators]], and [[Smartphone]]s. [[Mobipocket]] products support most Windows, Symbian, BlackBerry and Palm operating systems, but not the Android platform. Using WINE, the reader works under Linux or Mac OS X. Third-party applications like [[Okular]] and [[FBReader]] can also be used under Linux or Mac OS X, but they work only with unencrypted files.

The Amazon Kindle's AZW format is basically just the Mobipocket format with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]), and .prc publications can be read directly on the Kindle.  The Kindle AZW format also lacks some Mobipocket features such as JavaScript.&lt;ref&gt;{{cite web|url=http://www.mobileread.com/forums/showpost.php?p=1299906&amp;postcount=2 |title=MobileRead Forums - View Single Post - Javascript in mobi ebooks? |publisher=Mobileread.com |date=2010-12-29 |accessdate=2015-08-31}}&lt;/ref&gt;

[[Amazon.com|Amazon]] has developed an .epub to .mobi converter called KindleGen,&lt;ref&gt;{{cite web|url=http://www.mobipocket.com/dev/ |title=Mobipocket Developer Center |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}&lt;/ref&gt; and it supports IDPF 1.0 and IDPF 2.0 EPUB format.

===Multimedia eBooks ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Eveda
|-
| style="background:#ddd;"| ''Published as'':
| .exe or .html
|}
A [[multimedia ebook]] is [[media (communication)|media]] and [[book]] [[content (media and publishing)|content]] that utilizes a combination of different book [[content format]]s. The term can be used as a noun (a medium with multiple content formats) or as an adjective describing a medium as having multiple content formats.

The "multimedia ebook" term is used in contrast to media which only utilize traditional forms of printed or text books. Multimedia ebooks include a combination of [[Written language|text]], [[Audio file format|audio]], [[image]]s, [[video]], or [[interactive]] content formats.  Much like how a traditional book can contain images to help the text tell a story, a multimedia ebook can contain other elements not formerly possible to help tell the story.

With the advent of more widespread tablet-like computers, such as the [[smartphone]], some publishing houses are planning to make multimedia ebooks, such as Penguin.&lt;ref&gt;[http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ ] {{webarchive |url=https://web.archive.org/web/20100617170741/http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ |date=June 17, 2010 }}&lt;/ref&gt;

===Newton eBook ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Newton eBook
|-
| style="background:#ddd;"| ''Published as'':
| .pkg
|}
Commonly known as an [[Apple Newton]] book; a single Newton package file can contain multiple books (for example, the three books of a trilogy might be packaged together). All systems running the Newton operating system (the most common include the Newton MessagePads, eMates, Siemens Secretary Stations, Motorola Marcos, Digital Ocean Seahorses and Tarpons) have built-in support for viewing Newton books. The Newton package format was released to the public by Newton, Inc. prior to that company's absorption into Apple Computer. The format is thus arguably open and various people have written readers for it (writing a Newton book converter has even been assigned as a university-level class project&lt;ref&gt;{{cite web|url=http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |accessdate=July 6, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20060904191234/http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |archivedate=September 4, 2006 }}&lt;/ref&gt;).

Newton books have no support for DRM or encryption. They do support internal links, potentially multiple tables of contents and indexes, embedded gray scale images, and even some scripting capability (for example, it's possible to make a book in which the reader can influence the outcome).&lt;ref&gt;{{cite web|url=http://tools.unna.org/wikiwikinewt/index.php/MakeNewtonEbooksIndex |title=WikiWikiNewt Undergoing Maintenance |publisher=Tools.unna.org |date= |accessdate=2015-08-31}}&lt;/ref&gt; Newton books utilize [[Unicode]] and are thus available in numerous languages. An individual [[Newton book]] may actually contain multiple views representing the same content in different ways (such as for different screen resolutions).

===Open Electronic Package===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Open eBook
|-
| style="background:#ddd;"| ''Published as'':
| .opf
|}

[[Open eBook|OPF]] is an [[XML]]-based e-book format created by E-Book Systems; it has been superseded by the EPUB electronic publication standard.

===Portable Document Format ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Portable Document Format
|-
| style="background:#ddd;"| ''Published as'':
| .[[Portable Document Format|pdf]]
|}

Invented by [[Adobe Systems]], and first released in 1993, [[PDF]] became ISO 32000 in 2008. The format was developed to provide a platform-independent means of exchanging fixed-layout documents. Derived from [[PostScript]], but without language features like loops, PDF adds support for features such as compression, passwords, semantic structures and DRM. Because PDF documents can easily be viewed and printed by users on a variety of computer [[Platform (computing)|platforms]], they are very common on the [[World Wide Web]] and in document management systems worldwide. The current PDF specification, ISO 32000-1:2008, is available from ISO's website, and under special arrangement, without charge from Adobe.&lt;ref&gt;{{cite web|url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=PDF Reference and Adobe Extensions to the PDF Specification &amp;#124; Adobe Developer Connection |publisher=Adobe.com |date=2007-01-29 |accessdate=2015-08-31}}&lt;/ref&gt;

Because the format is designed to reproduce fixed-layout pages, re-flowing text to fit mobile device and e-book reader screens has traditionally been problematic. This limitation was addressed in 2001 with the release of PDF Reference 1.5 and "Tagged PDF",&lt;ref&gt;{{cite web|author= |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 |title=What is Tagged PDF? |publisher=Planet PDF |date= |accessdate=2015-08-31}}&lt;/ref&gt; but 3rd party support for this feature was limited until the release of [[PDF/UA]] in 2012.

Many products support creating and reading PDF files, such as Adobe Acrobat, [[PDFCreator]] and [[OpenOffice.org]], and several programming libraries such as [[iText]] and [[Formatting Objects Processor|FOP]]. Third party viewers such as [[xpdf]] and [[Nitro PDF]] are also available. Mac OS X has built-in PDF support, both for creation as part of the printing system and for display using the built-in Preview application.

PDF files are supported by almost all modern e-book readers, tablets and smartphones. However, PDF reflow based on Tagged PDF, as opposed to re-flow based on the actual sequence of objects in the content-stream, is not yet commonly supported on mobile devices. Such Re-flow options as may exist are usually found under "view" options, and may be called "word-wrap".

===Plain text files ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| text
|-
| style="background:#ddd;"| ''Published as'':
| .txt
|}
The first e-books in history were in [[text file|plain text]] (.txt) format, supplied for free by the [[Project Gutenberg]] community, but the format itself existed before the e-book era. The plain text format doesn't support digital rights management (DRM) or formatting options (such as different fonts, graphics or colors), but it has excellent portability as it is the simplest e-book encoding possible as a plain text file contains only [[ASCII]] or [[Unicode]] text (text files with [[UTF-8]] or [[UTF-16]] encoding are also popular for languages other than English). Almost all operating systems can read ASCII text files (e.g. Unix, Macintosh, Microsoft Windows, DOS and other systems) and newer operating systems support Unicode text files as well. The only potential for portability problems of ASCII text files is that operating systems differ in their preferred line ending convention and their interpretation of values outside the ASCII range (their character encoding). Conversion of files from one to another line-ending convention is easy with free software. DOS and Windows uses CRLF, Unix and Apple's OS X use LF, Mac OS up to and including OS 9 uses CR. By convention, lines are often broken to fit into 80 characters, a legacy of older terminals and consoles. Alternately, each paragraph may be a single line.

The size in bytes of a text file is simply the number of characters, including spaces, and with a new line counting for 1 or 2. For example, the [[Bible]], which is approximately 800,000 words, is about 4 MB.&lt;ref name="bible"&gt;{{cite web|url=http://www.gutenberg.org/ebooks/10 |accessdate=January 10, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20081205071232/http://www.gutenberg.org/ebooks/10 |archivedate=December 5, 2008 }}&lt;/ref&gt;

===Plucker ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Plucker
|-
| style="background:#ddd;"| ''Published as'':
|.pdb
|}
[[Plucker]] is an Open Source [[free software|free]] mobile and desktop e-book reader application with its own associated file format and software to automatically generate Plucker files from text, PDF, HTML, or other document format files, web sites or RSS feeds.  The format is public and well-documented. Free readers are available for all kinds of desktop computers and many PDAs.

===PostScript ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| PostScript
|-
| style="background:#ddd;"| ''Published as'':
| .[[PostScript|ps]]
|}
[[PostScript]] is a [[page description language]] used in the electronic and [[desktop publishing]] areas for defining the contents and layout of a printed page, which can be used by a rendering program to assemble and create the actual output [[Raster graphics|bitmap]]. Many office printers directly support interpreting PostScript and printing the result. As a result, the format also sees wide use in the [[Unix]] world.&lt;!-- IE if you don't want to fool around with output filters, ghostscript, and whatnot, get a postscript printer. Most Unix programs with specialized ``print'' functions output ps anyway (pity the firefox print renderer sucks so much). Don't see a way to comment on that here so left in a comment. Would be nice if (the higher end) ebook readers would add a ps interpreter, though --&gt;

===RTF===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Rich Text Format
|-
| style="background:#ddd;"| ''Published as'':
| .[[Rich Text Format|rtf]]
|}

[[Rich Text Format]] is a [[document]] file format that is supported by many ebook readers. Its advantages as an ebook format is that it is widely supported, and it can be reflowed. It can be easily edited. It can be easily converted to other ebook formats, increasing its support.

===SSReader ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| SSReader
|-
| style="background:#ddd;"| ''Published as'':
| .pdg
|}
The digital book format used by a popular digital library company &#36229;&#26143;&#25968;&#23383;&#22270;&#20070;&#39302;&lt;ref&gt;[http://www.ssreader.com/downland_index.asp ]{{dead link|date=August 2015}}&lt;/ref&gt; in China.  It is a proprietary raster image compression and binding format, with reading time OCR plug-in modules.  The company scanned a huge number of Chinese books in the China National Library and this becomes the major stock of their service.  The detailed format is not published.  There are also some other commercial e-book formats used in Chinese digital libraries.

===Text Encoding Initiative ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[TEI Lite]]
|-
| style="background:#ddd;"| ''Published as'':
| .xml{{Citation needed|date=August 2009}}
|}
[[TEI Lite]] is the most{{Citation needed|date=September 2010}} popular of the [[Text Encoding Initiative|TEI]]-based (and thus [[XML]]-based or [[Standard Generalized Markup Language|SGML]]-based) electronic text formats.

===TomeRaider ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| TomeRaider
|-
| style="background:#ddd;"| ''Published as'':
| .tr2; .tr3
|}

The [[TomeRaider]] e-book format is a proprietary format. There are versions of [[TomeRaider]] for Windows, Windows Mobile (aka Pocket PC), Palm, Symbian and iPhone. Several Wikipedias are available as [[Wikipedia:TomeRaider database|TomeRaider files]] with all articles unabridged, some even with nearly all images. Capabilities of the TomeRaider3 e-book reader vary considerably per platform: the Windows and Windows Mobile editions support full [[HTML]] and [[CSS]]. The Palm edition supports limited HTML (e.g., no tables, no fonts), and CSS support is missing. For Symbian there is only the older TomeRaider2  format, which does not render images or offer category search facilities. Despite these differences any TomeRaider e-book can be browsed on all supported platforms.  The Tomeraider website&lt;ref name="tomeraider.com"&gt;{{cite web|url=http://www.tomeraider.com/ |title=tomeraider.com |publisher=tomeraider.com |date=2015-06-24 |accessdate=2015-08-31}}&lt;/ref&gt; claims to have over 4000 e-books available, including free versions of the [[Internet Movie Database]] and Wikipedia.

===Open XML Paper Specification ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| OpenXPS
|-
| style="background:#ddd;"| ''Published as'':
| [[Open XML Paper Specification|.oxps, .xps]]
|}

'''Open XML Paper Specification''' (also referred to as '''OpenXPS''') is an open [[specification]] for a [[page description language]] and a fixed-document format. [[Microsoft]] developed it as the XML Paper Specification (XPS). In June 2009, [[Ecma International]] adopted it as international standard '''ECMA-388'''.&lt;ref&gt;{{cite web|url=http://www.ecma-international.org/publications/standards/Ecma-388.htm |title=Standard ECMA-388 |publisher=Ecma-international.org |date= |accessdate=2015-08-31}}&lt;/ref&gt;

The format is intentionally restricted to sequences of:
Glyphs (a fixed run of text),
Paths (a geometry that can be filled, or stroked, by a brush), and
Brushes (a description of a shaped brush used to in rendering paths).

This reduces the possibility of inadvertent introduction of malicious content and simplifies the implementation of compatible renderers.

== Comparison tables ==

=== Features ===
{| class="wikitable sortable" style="text-align: center; width:75%;"
|-
! '''Format'''
! [[Filename extension]]
! DRM support
! Image support
! Table support
! Sound support
! Interactivity support
! [[Word wrap]] support
! [[Open standard|Open]] [[Open standard|standard]]
! Embedded annotation support
! Book- marking
! Video support
|-
| [[Comic Book Archive]]
| .cbr, .cbz, .cb7, .cbt, .cba
| ?
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
|-
| [[DjVu]]
| .djvu
| ?
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Doc (computing)|DOC]]
| .doc
| ?
| {{yes}}
| {{yes}}
| ?
| ?
| {{yes}}
| {{no}}
| ?
| ?
| ?
|-
| [[DOCX]]
| .docx
| ?
| {{yes}}
| {{yes}}
| {{yes}}
| ?
| {{yes}}
| {{no}}
| ?
| ?
| {{yes}}
|-
| [[EPUB]] (IDPF)
| .epub
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{yes}}&lt;ref group="f"&gt;With ePub 3&lt;/ref&gt;
|-
| [[FictionBook]]
| .fb2
| {{no}}
| {{yes}}
| {{yes-no}}&lt;ref group="f"&gt;Table support added in FictionBook V2.1. Not supported in V2.0&lt;/ref&gt;
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
| ?
|-
| [[HTML]]
| .html
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f" name="html5"&gt;With HTML 5&lt;/ref&gt;
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}&lt;ref group="f" name="html5" /&gt;
|-
| [[iBooks]]
| .ibook
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
|-
| [[Information Presentation Facility|INF]]
| .inf
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| ?
| {{yes}}
| {{yes}}
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{yes-no}}&lt;ref name="depreader" group="f"&gt;Depends on the eReader application&lt;/ref&gt;
| {{no}}
|-
| [[Amazon Kindle|Kindle]]
| .azw
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f"&gt;Supported in all except 1st Generation Kindle. (Support level is as it is in mobipocket)&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Joshua Tallent |url=http://kindleformatting.com/blog/2009/02/kindle-2-review-formatting-perspective.php |title=Kindle 2 Review, the Formatting Perspective |publisher=Kindle Formatting |date=2009-02-25 |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}&lt;ref group="f" name="iOS"&gt;Supported only in kindle for iPhone, iPod, iPad.&lt;/ref&gt;&lt;ref name="amazon.com"&gt;{{cite web|url=http://www.amazon.com/b?ie=UTF8&amp;node=2248263011 |title=Kindle Editions with Audio-Video: Kindle Store |publisher=Amazon.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f" name="iOS"/&gt;&lt;ref name="amazon.com"/&gt;
|-
| [[Microsoft Reader]]
| .lit
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| ?
| {{yes}}
| ?
|-
| [[Mobipocket]]
| .prc, .mobi
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Multimedia EBook]]
| .exe
| {{yes}}
| {{yes}}
| ?
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Newton Book]]
| .pkg
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
|-
| [[#eReader|eReader]]
| .pdb
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Plain text]]
| .txt
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
|-
| [[Plucker]]
| .pdb
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| ?
|-
| [[Portable Document Format]]
| .pdf
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes-no}}&lt;ref group="f"&gt;"Reflow" is implemented by some readers.&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.adobe.com/uk/epaper/tips/acr5reflow/ |title=Reflow the contents of Adobe PDF documents: Tutorial |publisher=Adobe.com |date=2001-04-02 |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group="f"&gt;With Flash Embeded&lt;/ref&gt;
|-
| [[PostScript]]
| .ps
| {{no}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| ?
| ?
| ?
|-
| [[Tome Raider]]
| .tr2, .tr3
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| ?
| ?
| ?
|-
| [[OpenXPS]]
| .oxps, .xps
| ?
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| ?
| ?
| ?
|}
&lt;references group="f"/&gt;

=== Supporting platforms ===
{| class="wikitable sortable" style="text-align: center; width:75%;"
|-
! '''Reader&amp;nbsp;'''
! Plain text
! PDF
! ePub
! HTML
! Mobi- Pocket
! Fiction- Book (Fb2)
! DjVu
! Broadband eBook (BBeB)&lt;ref group=h name=propr&gt;Proprietary format&lt;/ref&gt;
! eReader&lt;ref group=h name=propr/&gt;
! Kindle&lt;ref group=h name=propr/&gt;
! WOLF&lt;ref group=h name=propr/&gt;
! Tome Raider&lt;ref group=h name=propr/&gt;
! Open eBook&lt;ref group=h&gt;Predecessor of ePUB&lt;/ref&gt;
! Comic Book
! OpenXPS
|-
| Amazon Kindle&amp;nbsp;1
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Amazon Kindle&amp;nbsp;2,&amp;nbsp;DX
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Amazon Kindle]]&amp;nbsp;3
| {{yes}}
| {{yes}}
| {{no}}&lt;ref group=h name=3part&gt;Yes, if the Duokan alternate Kindle OS (third-party software add-on) is used.&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Kindle Fire|Amazon Kindle Fire]]
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;By adding epub capable apps, such as [[Aldiko]]&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Android Devices
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h name=firm&gt;Requires latest firmware&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://ireader.over-blog.com/ |title=iReader |publisher=Ireader.over-blog.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}&lt;ref group=h name=firm/&gt;&lt;ref&gt;{{cite web|url=https://code.google.com/p/vudroid/ |title=vudroid - Android djvu and pdf viewer - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;&lt;ref&gt;{{cite web|author= |url=http://www.barnesandnoble.com/u/nook-for-android/379002287 |title=Rise of the Android by Apps for Nook &amp;#124; 2940147132807 &amp;#124; NOOK App &amp;#124; Barnes &amp; Noble |publisher=Barnesandnoble.com |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;&lt;ref name="tomeraider.com"/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{dunno}}
| {{yes}}
|-
| Apple iOS Devices
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h&gt;With third party apps, such as CloudReader&lt;/ref&gt;
| {{dunno}}
|-
| Azbooka WISEreader
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Barnes &amp; Noble Nook]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Barnes &amp; Noble Nook Color]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Bookeen Cybook Gen3, Opus
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h name=epmb&gt;Versions support either ePUB or MobiPocket&lt;/ref&gt;
| {{yes}}
| {{yes}}&lt;ref group=h name=epmb/&gt;
| {{yes}}&lt;ref group=h&gt;Only ePUB version and with FW 2.0+&lt;/ref&gt;
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{dunno}}
| {{dunno}}
|-
| COOL-ER Classic
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Linux]] Operating System
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;KDE's [[Okular]] supports fb2&lt;/ref&gt;
| {{yes}}
| {{yes}}&lt;ref group=h&gt;[[Calibre (software)|Calibre]] supports lrf/lrx&lt;/ref&gt;
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{yes}}
| {{yes}}
|-
| [[eSlick|Foxit eSlick]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Hanlin e-Reader&amp;nbsp;V3
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Hanvon WISEreader
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| iRex iLiad
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Iriver Story
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{yes}}&lt;ref group=h name=firm/&gt;
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Kobo eReader]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{dunno}}
|-
| Nokia N900
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{dunno}}
|-
| NUUTbook&amp;nbsp;2
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| OLPC XO, Sugar
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Onyx Boox 60
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Mac OS X
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{dunno}}
| {{yes}}
| {{yes}}
| {{dunno}}
| {{dunno}}
| {{yes}}
| {{dunno}}
| {{dunno}}
|-
| TrekStor eBook Reader Pyrus&lt;ref&gt;{{cite web|url=http://www.trekstor.co.uk/detail-ebook-reader-en/product/ebook-reader-pyrus-mini.html |title=Home - SurfTabs, smart phones, MiniPCs, data storage, MP3-Player - TrekStor GmbH |publisher=Trekstor.co.uk |date= |accessdate=2015-08-31}}&lt;/ref&gt;
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
| Windows
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;[[ICE Book Reader]] for Windows supports fb2&lt;/ref&gt;
| {{yes}}
| {{dunno}}
| {{yes}}
| {{yes}}&lt;ref group=h&gt;DRM-protected publications are supported as of Kindle for PC v1.3.0&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
| {{yes}}
| {{dunno}}
| {{yes}}&lt;ref group="h"&gt;XP or later, not on Windows 2000&lt;/ref&gt;
|-
| Pocketbook 301&amp;nbsp;Plus, 302, 360&#176;
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Pocketbook Aqua
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Sony Reader
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Viewsonic VEB612
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Windows Phone 7
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|}
&lt;references group=h/&gt;

== See also ==
* [[Comparison of e-book readers]]
* [[Comparison of Android e-book reader software]] &#8211; includes software e-book readers for Android devices
* [[Comparison of iOS e-book reader software]] &#8211; includes software e-book readers for iOS devices
* [[ICUE]], a British company using mobile phone (cellphone) technology to deliver books and other publications

== References ==
;General information
{{Refbegin}}
* {{cite book|last=Cavanaugh|first=T W|title=The Digital Reader: Using E-Books in K-12 Education|year=2006|publisher=International Society for Technology in Education|location=Eugene, Oregon|isbn=1564842215}}
* {{cite book|last=Chandler|first=S|title=From Entrepreneur to Infopreneur: Make Money with Books, EBooks, and Information Products|year=2010|publisher=John Wiley &amp; Sons|location=Hoboken, New Jersey|isbn=1118044770}}
* Cope, B., &amp; Mason, D. (2002). Markets for electronic book products. C-2-C series, bk. 3.2. Altona, Vic: Common Ground Pub.
* {{cite book|last=Henke|first=H|title=Electronic Books and Epublishing: A Practical guide for Authors.|year=2001|publisher=Springer|location=London|isbn=1852334355}}
* Hanttula, D. (2001). Pocket PC handbook.
* {{cite book|last=Rich|first=J|title=Self-Publishing For Dummies|year=2006|publisher=John Wiley &amp; Sons|location=Hoboken, New Jersey|isbn=0470100370}}
{{Refend}}
;Footnotes
{{Reflist|30em}}

==External links==
*[http://wiki.mobileread.com/wiki/Main_Page ebook reader articles at Mobile Read Wiki]
*[http://digbib.ubka.uni-karlsruhe.de/volltexte/1000010574 Daisy 3: A Standard for Accessible Multimedia Books]
*[https://www.eff.org/deeplinks/2009/12/e-book-privacy An E-Book Buyer's Guide to Privacy]

{{Ebooks}}

{{DEFAULTSORT:Comparison Of E-Book Formats}}
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Computing comparisons]]</text>
      <sha1>s4r2grrfdviqmt2wkhdsbmckjtrgbvl</sha1>
    </revision>
  </page>
  <page>
    <title>TheSwizzle.com</title>
    <ns>0</ns>
    <id>37690869</id>
    <revision>
      <id>755847861</id>
      <parentid>755847529</parentid>
      <timestamp>2016-12-20T14:48:00Z</timestamp>
      <contributor>
        <username>DberkowitzNY</username>
        <id>6452429</id>
      </contributor>
      <comment>noted site/company is inactive and got acquired</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3302" xml:space="preserve">{{Multiple issues|
{{COI|date=August 2013}}
{{refimprove|date=August 2013}}
}}
{{Infobox dot-com company
| name   = TheSwizzle
| logo         = 
[[File:TheSwizzle.com Logo.jpg|150x50px|TheSwizzle.com Logo]]
| company_type   = [[Privately held company|Private]]
| location_city    = [[New York City]],&lt;ref&gt;{{cite web|url=http://www.theswizzle.com |title=TheSwizzle |accessdate=2012-11-19}}&lt;/ref&gt; [[New York (state)|New York]]
| location_country = USA
| foundation = 2010
| founder                   = [[Scott Kurnit]]
| registration              = Optional
| current_status            = Inactive
| industry       = [[advertising]], [[online advertising]], [[email]]
| homepage       = [http://www.theswizzle.com/ www.theswizzle.com]
}}
'''TheSwizzle''' was a [[webmail]] tool that worked with existing email and enabled consumers to manage email subscriptions, primarily from commercial vendors.&lt;ref&gt;{{cite web |accessdate=November 19, 2012 |url=http://mashable.com/2012/10/03/swizzle-emails |title=The Swizzle Cleans Your Inbox By Combining Promo E-mails Into a Daily Digest |publisher=Mashable |date=October 25, 2012 |author=Veena Bissram }}&lt;/ref&gt;&lt;ref&gt;{{cite web |accessdate=November 19, 2012 |url=http://revision3.com/tzdaily/swizzle-junk-email |title=Clean Junk Mail From Your Inbox! |publisher=revision3 |date=October 25, 2012 |author=Veronica Belmont }}&lt;/ref&gt;&lt;ref&gt;{{cite web |accessdate=November 19, 2012 |url=http://www.pcmag.com/article2/0,2817,2411068,00.asp |title=Swizzle |publisher=pcmag |date=October 17, 2012 |author=Samara Lynn }}&lt;/ref&gt; It was acquired by Mailstrom of 410 Labs in September 2014, and TheSwizzle.com subsequently shut down.&lt;ref&gt;{{Cite news|url=http://technical.ly/baltimore/2014/09/11/mailstrom-the-410-labs-email-helper-lands-ex-competitors-users-swizzle/|title=Mailstrom, the 410 Labs email helper, lands ex-competitor's users - Technical.ly Baltimore|date=2014-09-11|newspaper=Technical.ly Baltimore|language=en-US|access-date=2016-12-20}}&lt;/ref&gt;

==Features==
The product claims several features, including cleaning up users' inboxes by helping to unsubscribe from unwanted emails while at the same time, allowing receipt as well as searching among those commercially oriented emails an individual still wants to receive. By packaging these messages into a digest format, users can consolidate their email box.

==History==
The Swizzle is a product of Keep Holdings, a consumer and brand engagement conglomerate of business units including Keep.com, [[AdKeeper]] and TheSwizzle.com.
The company was founded in March, 2010,&lt;ref&gt;{{cite web |accessdate=January 17, 2010 |url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=114604562 |title=AdKeeper, Inc. Snapshot |publisher=Bloomberg Businessweek |author=Staff }}&lt;/ref&gt; by [[Scott Kurnit]], who serves as Chairman and CEO. Kurnit is best known as the founder of [[About.com]], which grew to a public market value of $1.7 billion, and was sold to Primedia for $724 million, in 2001. About.com is now owned by [[IAC (company)|IAC]].

==See also==
* [[Scott Kurnit]]
* [[AdKeeper]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.theswizzle.com/}}

{{DEFAULTSORT:Swizzle}}
[[Category:Electronic documents]]
[[Category:Internet properties established in 2010]]</text>
      <sha1>lmsooa3nr8r3g0bgopdoywqns2r8jj3</sha1>
    </revision>
  </page>
  <page>
    <title>E-receipt</title>
    <ns>0</ns>
    <id>40545818</id>
    <revision>
      <id>754262390</id>
      <parentid>753724960</parentid>
      <timestamp>2016-12-11T18:50:04Z</timestamp>
      <contributor>
        <username>Whoisjohngalt</username>
        <id>272397</id>
      </contributor>
      <comment>Added a reference, added categories, and cleaned up using [[WP:AutoEd|AutoEd]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="664" xml:space="preserve">An '''E-receipt''' is an electronic [[receipt]] of any goods/services that have been purchased, opposed to a paper receipt. They are usually sent via [[email]] to avoid wasting [[paper]] and for marketing purposes.&lt;ref&gt;{{cite news |last=Perring |first=Rebecca |url=http://www.express.co.uk/news/uk/639219/Ereceipts-British-shops-shopping-electronic-Internet |title=Retailers are now monitoring YOUR shopping habits and transactions with the eReceipt... |work=[[Daily Express#Sunday Express]] |date=2016-01-29 |accessdate=2016-12-11 }}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Electronic documents]]
[[Category:Accounting source documents]]

{{retailing-stub}}</text>
      <sha1>0vwyh8m2fitwdymde0y3a2qwlpovff0</sha1>
    </revision>
  </page>
  <page>
    <title>Portable Document Format</title>
    <ns>0</ns>
    <id>24077</id>
    <revision>
      <id>762882722</id>
      <parentid>762882676</parentid>
      <timestamp>2017-01-31T06:17:21Z</timestamp>
      <contributor>
        <username>Whitegum</username>
        <id>15445920</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="70129" xml:space="preserve">{{Redirect|PDF|3=}}
{{Infobox file format
| name                   = Portable Document Format
| icon                   = [[File:Adobe PDF.svg|frameless|SVG logo|150px]]
| iconcaption            = Adobe PDF icon
| extension              = .pdf
| mime                   = {{plainlist|
* &lt;code&gt;application/pdf&lt;/code&gt;,&lt;ref name="rfc3778"&gt;{{citation |url=http://tools.ietf.org/html/rfc3778 |title=The application/pdf Media Type, RFC 3778, Category: Informational |year=2004}}&lt;/ref&gt;
* &lt;code&gt;application/x-pdf&lt;/code&gt;
* &lt;code&gt;application/x-bzpdf&lt;/code&gt;
* &lt;code&gt;application/x-gzpdf&lt;/code&gt;
 }}
| _nomimecode            = true
| magic                  = &lt;code&gt;%PDF&lt;/code&gt;
| released               = {{Start date and age|1993|6|15}}
| standard               = ISO 32000-1
| free                   = Yes
| url                    = {{URL|https://www.adobe.com/devnet/pdf/pdf_reference_archive.html}}
| image                  = 
| typecode               = 'PDF '&lt;ref name="rfc3778" /&gt; (including a single space)
| uniform type           = com.adobe.pdf
| owner                  = [[Adobe Systems]]
| latest release version = 1.7
| latest release date    = &lt;!-- {{Start date and age|YYYY|mm|dd|df=yes}} --&gt;
| genre                  =
| container for          =
| contained by           =
| extended from          =
| extended to            = [[PDF/A]], [[PDF/E]], [[PDF/UA]], [[PDF/VT]], [[PDF/X]]
}}
The '''Portable Document Format''' ('''PDF''') is a [[file format]] used to present [[document]]s in a manner independent of [[application software]], [[Computer hardware|hardware]], and [[operating system]]s.&lt;ref name="pdf-ref-1.7"&gt;Adobe Systems Incorporated, [https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf PDF Reference, Sixth edition, version 1.23 (30 MB)], Nov 2006, p. 33.&lt;/ref&gt; Each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, [[font]]s, graphics, and other information needed to display it. &lt;!-- Today, three dimensional objects can be embedded in PDF documents with Acrobat 3D using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.&lt;ref name="3d#1" /&gt;&lt;ref name="3d#2" /&gt; --&gt;

&lt;blockquote&gt;A PDF file captures document text, fonts, images, and even formatting of documents from a variety of applications. You can e-mail a PDF document to your friends and it will look the same on their screens as it looks on yours, even if they have Apple computers and you have a PC.&lt;ref&gt;[http://techterms.com/definition/pdf TechTerms.com]&lt;/ref&gt;
&lt;/blockquote&gt;

== History and standardization ==

{{main article|History and standardization of Portable Document Format}}

PDF was developed in the early 1990s&lt;ref&gt;{{cite web|url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6650|title=Adobe's Bob Wulff knows Acrobat and PDF -- inside and out}}&lt;/ref&gt; as a way to share computer documents, including text formatting and inline images.&lt;ref&gt;{{cite web|url=http://www.planetpdf.com/planetpdf/pdfs/warnock_camelot.pdf|title=The Camelot Project}}&lt;/ref&gt; It was among a number of competing formats such as [[DjVu]], [[Envoy (WordPerfect)|Envoy]], Common Ground Digital Paper, Farallon Replica and even [[Adobe Systems|Adobe]]'s own [[PostScript]] format. In those early years before the rise of the [[World Wide Web]] and [[HTML]] documents, PDF was popular mainly in [[desktop publishing]] [[workflow]]s.
Adobe Systems made the PDF specification available free of charge in 1993. PDF was a [[proprietary format]] controlled by Adobe, until it was officially released as an [[open standard]] on July 1, 2008, and published by the [[International Organization for Standardization]] as ISO 32000-1:2008,&lt;ref name="iso-standard"&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51502 |title=ISO 32000-1:2008 - Document management &#8211; Portable document format &#8211; Part 1: PDF 1.7 |publisher=Iso.org |date=2008-07-01 |accessdate=2010-02-21}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Orion |first=Egan |title=PDF 1.7 is approved as ISO 32000 |work=[[The Inquirer]] |publisher=[[The Inquirer]] |date=2007-12-05 |url=http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |accessdate=2007-12-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20071213004627/http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |archivedate=December 13, 2007 }}&lt;/ref&gt; at which time control of the specification passed to an ISO Committee of volunteer industry experts. In 2008, Adobe published a Public Patent License to ISO 32000-1 granting [[royalty-free]] rights for all patents owned by Adobe that are necessary to make, use, sell, and distribute PDF compliant implementations.&lt;ref&gt;{{citation |url=https://www.adobe.com/pdf/pdfs/ISO32000-1PublicPatentLicense.pdf |title=Public Patent License, ISO 32000-1: 2008 &#8211; PDF 1.7 |author=Adobe Systems Incorporated |year=2008 |accessdate=2011-07-06}}&lt;/ref&gt;

However, there are still some proprietary technologies defined only by Adobe, such as [[XFA|Adobe XML Forms Architecture]] (XFA) and [[JavaScript]] extension for Acrobat, which are referenced by ISO 32000-1 as [[normative]] and indispensable for the application of the ISO 32000-1 specification. These proprietary technologies are not standardized and their specification is published only on Adobe&#8217;s website.&lt;ref&gt;{{cite web |url=http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=SWD:2013:0224:FIN:EN:PDF |title=Guide for the procurement of standards-based ICT - Elements of Good Practice, Against lock-in: building open ICT systems by making better use of standards in public procurement |quote=Example: ISO/IEC 29500, ISO/IEC 26300 and ISO 32000 for document formats reference information that is not accessible by all parties (references to proprietary technology and brand names, incomplete scope or dead web links). |publisher=European Commission |date=2013-06-25 |accessdate=2013-10-20}}&lt;/ref&gt;&lt;ref name="iso-meeting-n603"&gt;{{citation |url=http://pdf.editme.com/files/pdfREF-meetings/ISO-TC171-SC2-WG8_N0603_SC2WG8_MtgRept_SLC.pdf |title=ISO/TC 171/SC 2/WG 8 N 603 - Meeting Report |quote=XFA is not to be ISO standard just yet. ... The Committee urges Adobe Systems to submit the XFA Specification, XML Forms Architecture (XFA), to ISO for standardization ... The Committee is concerned about the stability of the XFA specification ... Part 2 will reference XFA 3.1 |date=2011-06-27}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0069446.s001 |title=Embedding and publishing interactive, 3-dimensional, scientificfigures in Portable Document Format (PDF) files |quote=... the implementation of the U3D standard was not complete and proprietary extensions were used. |accessdate=2013-10-20}}&lt;/ref&gt;&lt;ref name="rosenthol-adobe-2012"&gt;{{cite web |url=http://cdn.parleys.com/p/5148922a0364bc17fc56c6e5/iSUM2012_00_LRO_presentation.pdf |title=PDF and Standards |author=Leonard Rosenthol, Adobe Systems |year=2012 |accessdate=2013-10-20}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=Is_PDF_an_open_standard&amp;page=1 |title=Is PDF an open standard? - Adobe Reader is the de facto Standard, not PDF |author=Duff Johnson |date=2010-06-10 |accessdate=2014-01-19}}&lt;/ref&gt; Many of them are also not supported by popular third-party implementations of PDF.  So when organizations publish PDFs which use these proprietary technologies, they present accessibility issues for some users.

In 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").&lt;ref name="DRAFT INTERNATIONAL"&gt;{{Cite web|url=http://www.iso.org/iso/catalogue_detail.htm?csnumber=63534|title=DRAFT INTERNATIONAL STANDARD ISO/DIS 32000-2|last=|first=|date=|website=|publisher=ISO|access-date=2016-08-04|quote=Editor&#8217;s note: XFA forms have been deprecated from ISO 32000-2 in accordance with the outcome of the letter ballot following the Pretoria meetings.}}&lt;/ref&gt;

== Technical foundations ==
The PDF combines three technologies:
* A subset of the [[PostScript]] page description programming language, for generating the layout and graphics.
* A [[font embedding|font-embedding]]/replacement system to allow fonts to travel with the documents.
* A structured storage system to bundle these elements and any associated content into a single file, with [[data compression]] where appropriate.

=== PostScript ===
[[PostScript]] is a [[page description language]] run in an [[interpreter (computing)|interpreter]] to generate an image, a process requiring many resources. It can handle graphics and standard features of [[programming language]]s such as &lt;code&gt;if&lt;/code&gt; and &lt;code&gt;loop&lt;/code&gt; commands. PDF is largely based on PostScript but simplified to remove flow control features like these, while graphics commands such as &lt;code&gt;lineto&lt;/code&gt; remain.

Often, the PostScript-like PDF code is generated from a source PostScript file. The graphics commands that are output by the PostScript code are collected and [[Lexical analysis|tokenized]]. Any files, graphics, or fonts to which the document refers also are collected. Then, everything is compressed to a single file. Therefore, the entire PostScript world (fonts, layout, measurements) remains intact.

As a document format, PDF has several advantages over PostScript:
* PDF contains tokenized and interpreted results of the PostScript source code, for direct correspondence between changes to items in the PDF page description and changes to the resulting page appearance.
* PDF (from version 1.4) supports [[transparency (graphic)|graphic transparency]]; PostScript does not.
* PostScript is an [[interpreted programming language]] with an implicit global state, so instructions accompanying the description of one page can affect the appearance of any following page. Therefore, all preceding pages in a PostScript document must be processed to determine the correct appearance of a given page, whereas each page in a PDF document is unaffected by the others. As a result, PDF viewers allow the user to quickly jump to the final pages of a long document, whereas a PostScript viewer needs to process all pages sequentially before being able to display the destination page (unless the optional PostScript [[Document Structuring Conventions]] have been carefully complied with).

== Technical overview ==

=== File structure ===

A PDF file is a 7-bit ASCII file, except for certain elements that may have binary content.
A PDF file starts with a header containing the [[magic number (programming)|magic number]] and the version of the format such as &lt;code&gt;%PDF-1.7&lt;/code&gt;. The format is a subset of a COS ("Carousel" Object Structure) format.&lt;ref&gt;{{cite web|url=http://jimpravetz.com/blog/2012/12/in-defense-of-cos/|title=In Defense of COS, or Why I Love JSON and Hate XML|author=Jim Pravetz|work=jimpravetz.com}}&lt;/ref&gt; A COS tree file consists primarily of ''objects'', of which there are eight types:&lt;ref&gt;Adobe Systems, PDF Reference, p. 51.&lt;/ref&gt;
* [[Boolean data type|Boolean]] values, representing ''true'' or ''false''
* Numbers
* [[String (computer science)|Strings]], enclosed within parentheses (&lt;code&gt;(...)&lt;/code&gt;), may contain 8-bit characters.
* Names, starting with a forward slash (&lt;code&gt;/&lt;/code&gt;)
* [[Array data type|Array]]s, ordered collections of objects enclosed within square brackets (&lt;code&gt;[...]&lt;/code&gt;)
* [[Dictionary (data structure)|Dictionaries]], collections of objects indexed by Names enclosed within double pointy brackets (&lt;code&gt;&amp;lt;&amp;lt;...&amp;gt;&amp;gt;&lt;/code&gt;)
* [[Stream (computing)|Streams]], usually containing large amounts of data, which can be compressed and binary
* The [[Pointer (computer programming)|null]] object
Furthermore, there may be comments, introduced with the percent sign (&lt;code&gt;%&lt;/code&gt;). Comments may contain 8-bit characters.

Objects may be either ''direct'' (embedded in another object) or ''indirect''. Indirect objects are numbered with an ''object number'' and a ''generation number'' and defined between the &lt;code&gt;obj&lt;/code&gt; and &lt;code&gt;endobj&lt;/code&gt; keywords. An index table, also called the cross-reference table and marked with the &lt;code&gt;xref&lt;/code&gt; keyword, follows the main body and gives the byte offset of each indirect object from the start of the file.&lt;ref&gt;Adobe Systems, PDF Reference, pp. 39&#8211;40.&lt;/ref&gt; This design allows for efficient [[random access]] to the objects in the file, and also allows for small changes to be made without rewriting the entire file (''incremental update''). Beginning with PDF version 1.5, indirect objects may also be located in special streams known as ''object streams''. This technique reduces the size of files that have large numbers of small indirect objects and is especially useful for ''Tagged PDF''.

At the end of a PDF file is a trailer introduced with the &lt;code&gt;trailer&lt;/code&gt; keyword. It contains

* a dictionary
* an offset to the start of the cross-reference table (the table starting with the &lt;code&gt;xref&lt;/code&gt; keyword)
* and the &lt;code&gt;%%EOF&lt;/code&gt; [[end-of-file]] marker.

The dictionary contains

* a reference to the root object of the tree structure, also known as the ''catalog''
* the count of indirect objects in the cross-reference table
* and other optional information.

There are two layouts to the PDF files: non-linear (not "optimized") and linear ("optimized"). Non-linear PDF files consume less disk space than their linear counterparts, though they are slower to access because portions of the data required to assemble pages of the document are scattered throughout the PDF file. Linear PDF files (also called "optimized" or "web optimized" PDF files) are constructed in a manner that enables them to be read in a Web browser plugin without waiting for the entire file to download, since they are written to disk in a linear (as in page order) fashion.&lt;ref name="pdf-ref"&gt;{{cite web |url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=Adobe Developer Connection: PDF Reference and Adobe Extensions to the PDF Specification |publisher=Adobe Systems |accessdate=2010-12-13}}&lt;/ref&gt; PDF files may be optimized using [[Adobe Acrobat]] software or [[QPDF]].

=== Imaging model ===
The basic design of how [[graphics]] are represented in PDF is very similar to that of PostScript, except for the use of [[transparency (graphic)|transparency]], which was added in PDF 1.4.

PDF graphics use a [[device independence|device-independent]] [[Cartesian coordinate system]] to describe the surface of a page. A PDF page description can use a [[matrix (mathematics)|matrix]] to [[scale (ratio)|scale]], [[rotate]], or [[Shear mapping|skew]] graphical elements. A key concept in PDF is that of the ''graphics state'', which is a collection of graphical parameters that may be changed, saved, and restored by a ''page description''. PDF has (as of version 1.6) 24 graphics state properties, of which some of the most important are:
* The ''current transformation matrix'' (CTM), which determines the coordinate system
* The ''[[clipping path]]''
* The ''[[color space]]''
* The ''[[alpha compositing|alpha constant]]'', which is a key component of transparency

==== Vector graphics ====
As in PostScript, [[vector graphics]] in PDF are constructed with ''paths''. Paths are usually composed of lines and cubic [[B&#233;zier curve]]s, but can also be constructed from the outlines of text. Unlike PostScript, PDF does not allow a single path to mix text outlines with lines and curves. Paths can be stroked, filled, or used for [[clipping path|clipping]]. Strokes and fills can use any color set in the graphics state, including ''patterns''.

PDF supports several types of patterns. The simplest is the ''tiling pattern'' in which a piece of artwork is specified to be drawn repeatedly. This may be a ''colored tiling pattern'', with the colors specified in the pattern object, or an ''uncolored tiling pattern'', which defers color specification to the time the pattern is drawn. Beginning with PDF 1.3 there is also a ''shading pattern'', which draws continuously varying colors. There are seven types of shading pattern of which the simplest are the ''axial shade'' (Type 2) and ''radial shade'' (Type 3). &lt;!-- Pictures desperately needed here! --&gt;

==== Raster images ====
[[Raster graphics|Raster images]] in PDF (called ''Image XObjects'') are represented by dictionaries with an associated stream. The dictionary describes properties of the image, and the stream contains the image data. (Less commonly, a raster image may be embedded directly in a page description as an ''inline image''.) Images are typically ''filtered'' for compression purposes. Image filters supported in PDF include the general purpose filters
* '''ASCII85Decode''' a filter used to put the stream into 7-bit [[ASCII]]
* '''ASCIIHexDecode''' similar to ASCII85Decode but less compact
* '''FlateDecode''' a commonly used filter based on the [[deflate]] algorithm defined in RFC 1951 (deflate is also used in the [[gzip]], [[Portable Network Graphics|PNG]], and [[ZIP (file format)|zip]] file formats among others); introduced in PDF 1.2; it can use one of two groups of predictor functions for more compact zlib/deflate compression: ''Predictor 2'' from the [[TIFF]] 6.0 specification and predictors (filters) from the [[Portable Network Graphics|PNG]] specification (RFC 2083)
* '''LZWDecode''' a filter based on [[LZW]] Compression; it can use one of two groups of predictor functions for more compact LZW compression: ''Predictor 2'' from the TIFF 6.0 specification and predictors (filters) from the PNG specification
* '''RunLengthDecode''' a simple compression method for streams with repetitive data using the [[run-length encoding]] algorithm and the image-specific filters
* '''DCTDecode''' a [[lossy]] filter based on the [[JPEG]] standard
* '''CCITTFaxDecode''' a [[lossless]] [[bi-level image|bi-level]] (black/white) filter based on the Group 3 or [[Group 4 compression|Group 4]] [[CCITT]] (ITU-T) [[fax]] compression standard defined in ITU-T [[T.4]] and T.6
* '''JBIG2Decode''' a lossy or lossless bi-level (black/white) filter based on the [[JBIG2]] standard, introduced in PDF 1.4
* '''JPXDecode''' a lossy or lossless filter based on the [[JPEG 2000]] standard, introduced in PDF 1.5

Normally all image content in a PDF is embedded in the file. But PDF allows image data to be stored in external files by the use of ''external streams'' or ''Alternate Images''. Standardized subsets of PDF, including [[PDF/A]] and [[PDF/X]], prohibit these features.

==== Text ====
Text in PDF is represented by ''text elements'' in page content streams. A text element specifies that ''characters'' should be drawn at certain positions. The characters are specified using the ''encoding'' of a selected ''font resource''.

===== Fonts =====
A font object in PDF is a description of a digital [[typeface]]. It may either describe the characteristics of a typeface, or it may include an embedded ''font file''. The latter case is called an ''embedded font'' while the former is called an ''unembedded font''. The font files that may be embedded are based on widely used standard digital font formats: '''[[PostScript fonts|Type 1]]''' (and its compressed variant '''CFF'''), '''[[TrueType]]''', and (beginning with PDF 1.6) '''[[OpenType]]'''. Additionally PDF supports the '''Type 3''' variant in which the components of the font are described by PDF graphic operators. &lt;!--- Type 3 bit is awkward and should be cleaned up ---&gt;

===== Standard Type 1 Fonts (Standard 14 Fonts) =====
Fourteen typefaces, known as the ''standard 14 fonts'', have a special significance in PDF documents:
* [[Times Roman|Times]] (v3) (in regular, italic, bold, and bold italic)
* [[Courier (typeface)|Courier]] (in regular, oblique, bold and bold oblique)
* [[Helvetica]] (v3) (in regular, oblique, bold and bold oblique)
* [[Symbol (typeface)|Symbol]]
* [[Zapf Dingbats]]
These fonts are sometimes called the ''base fourteen fonts''.&lt;ref&gt;{{cite web|url=http://desktoppub.about.com/od/glossary/g/base14fonts.htm|title=Desktop Publishing: Base 14 Fonts - Definition|work=About.com Tech}}&lt;/ref&gt; These fonts, or suitable substitute fonts with the same metrics, should be available in most PDF readers. However, since Adobe Acrobat version 6, most of these fonts are not ''guaranteed'' to be available in the reader, and may only display correctly if the system has them installed.&lt;ref name="aquarium"&gt;[http://www.planetpdf.com/planetpdf/pdfs/pdf2k/03e/merz_fontaquarium.pdf The PDF Font Aquarium]&lt;/ref&gt; Fonts may be substituted if they are not embedded in a PDF.

===== Encodings =====
Within text strings, characters are shown using ''character codes'' (integers) that map to glyphs in the current font using an ''encoding''. There are a number of predefined encodings, including ''WinAnsi'', ''MacRoman'', and a large number of encodings for East Asian languages, and a font can have its own built-in encoding. (Although the WinAnsi and MacRoman encodings are derived from the historical properties of the [[Microsoft Windows|Windows]] and [[Macintosh]] operating systems, fonts using these encodings work equally well on any platform.) PDF can specify a predefined encoding to use, the font's built-in encoding or provide a lookup table of differences to a predefined or built-in encoding (not recommended with TrueType fonts).&lt;ref&gt;{{cite web|url=https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf |title=PDF Reference Sixth Edition, version 1.7, table 5.11}}&lt;/ref&gt; The encoding mechanisms in PDF were designed for Type 1 fonts, and the rules for applying them to TrueType fonts are complex.

For large fonts or fonts with non-standard glyphs, the special encodings ''Identity-H'' (for horizontal writing) and ''Identity-V'' (for vertical) are used. With such fonts it is necessary to provide a ''ToUnicode'' table if semantic information about the characters is to be preserved.

==== Transparency ====
The original imaging model of PDF was, like PostScript's, ''opaque'': each object drawn on the page completely replaced anything previously marked in the same location. In PDF 1.4 the imaging model was extended to allow transparency. When transparency is used, new objects interact with previously marked objects to produce blending effects. The addition of transparency to PDF was done by means of new extensions that were designed to be ignored in products written to the PDF 1.3 and earlier specifications. As a result, files that use a small amount of transparency might view acceptably in older viewers, but files making extensive use of transparency could be viewed incorrectly in an older viewer without warning.

The transparency extensions are based on the key concepts of ''transparency groups'', ''blending modes'', ''shape'', and ''alpha''. The model is closely aligned with the features of [[Adobe Illustrator]] version 9. The blend modes were based on those used by [[Adobe Photoshop]] at the time. When the PDF 1.4 specification was published, the formulas for calculating blend modes were kept secret by Adobe. They have since been published.&lt;ref&gt;[https://www.adobe.com/content/dam/Adobe/en/devnet/pdf/pdfs/pdf_reference_archives/blend_modes.pdf PDF Blend Modes Addendum]&lt;/ref&gt;

The concept of a transparency group in PDF specification is independent of existing notions of "group" or "layer" in applications such as Adobe Illustrator. Those groupings reflect logical relationships among objects that are meaningful when editing those objects,
but they are not part of the imaging model.

=== Interactive elements ===

PDF files may contain interactive elements such as annotations, form fields, video and Flash animation.

'''Rich Media PDF''' is a term that is used to describe interactive content that can be embedded or linked to inside of a PDF. This content must be produced using the Flash file format. When Adobe bought Macromedia, the jewel of the company was Flash, and the Flash player was embedded inside Adobe Acrobat and Adobe Reader, removing the need for third-party plug-ins such as Flash, QuickTime, or Windows Media. Unfortunately, this caused a rift with Apple as QuickTime video was prohibited from PDF.  [[Rich Media]] expert [[Bob Connolly (Canadian film director)#Books, eBooks and Magazine Articles|Robert Connolly]] believes this event triggered the war between Apple and Adobe over the Flash iPhone/iPad dispute. Rich Media PDF will not operate in Apple's iOS devices such as the iPad, and interactivity is limited.

'''Interactive Forms''' is a mechanism to add forms to the PDF file format.

PDF currently supports two different methods for integrating data and PDF forms. Both formats today coexist in PDF specification:&lt;ref name="iso32000"&gt;{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/PDF32000_2008.pdf |title=Document Management &#8211; Portable Document Format &#8211; Part 1: PDF 1.7, First Edition |author=Adobe Systems Incorporated |date=2008-07-01 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://gnupdf.org/Forms_Data_Format |title=Gnu PDF - PDF Knowledge - Forms Data Format |archiveurl=https://web.archive.org/web/20130101054615/http://www.gnupdf.org/Forms_Data_Format |archivedate=2013-01-01 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://livedocs.adobe.com/coldfusion/8/htmldocs/help.html?content=formsPDF_02.html |title=About PDF forms |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://forums.adobe.com/thread/301733 |title=Convert XFA Form to AcroForm? |year=2008 |accessdate=2010-02-19}}&lt;/ref&gt;
* '''AcroForms''' (also known as '''Acrobat forms'''), introduced in the PDF 1.2 format specification and included in all later PDF specifications.
* '''[[XML Forms Architecture|Adobe XML Forms Architecture]] (XFA)''' forms, introduced in the PDF 1.5 format specification. The XFA specification is not included in the PDF specification, it is only referenced as an optional feature. Adobe XFA Forms are not compatible with AcroForms.&lt;ref&gt;{{cite web |url=http://partners.adobe.com/public/developer/tips/topic_tip2.html |title=Migrating from Adobe Acrobat forms to XML forms |accessdate=2010-02-22}}&lt;/ref&gt;

==== AcroForms ====
AcroForms were introduced in the PDF 1.2 format. AcroForms permit using objects (''e.g.'' [[text box]]es, [[Radio button]]s, ''etc.'') and some code (''e.g.'' [[JavaScript]]).

Alongside the standard PDF action types, interactive forms (AcroForms) support submitting, resetting, and importing data. The "submit" action transmits the names and values of selected interactive form fields to a specified uniform resource locator (URL). Interactive form field names and values may be submitted in any of the following formats, (depending on the settings of the action&#8217;s ExportFormat, SubmitPDF, and XFDF flags):&lt;ref name="iso32000" /&gt;
* HTML Form format (HTML 4.01 Specification since PDF 1.5; HTML 2.0 since 1.2)
* Forms Data Format (FDF)
* XML Forms Data Format (XFDF) (external XML Forms Data Format Specification, Version 2.0; supported since PDF 1.5; it replaced the "XML" form submission format defined in PDF 1.4)
* PDF (the entire document can be submitted rather than individual fields and values). (defined in PDF 1.4)

AcroForms can keep form field values in external stand-alone files containing key:value pairs. The external files may use Forms Data Format (FDF) and XML Forms Data Format (XFDF) files.&lt;ref&gt;{{cite web |url=http://kb2.adobe.com/cps/325/325874.html |title=Using Acrobat forms and form data on the web |author=Adobe Systems Incorporated |date=2007-10-15 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref name="xfdf"&gt;{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfdf_2.0.pdf |format=PDF |title=XML Forms Data Format Specification, version 2 |date=September 2007 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref name="fdf-exchange"&gt;{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/fdf_data_exchange.pdf |format=PDF |title=FDF Data Exchange Specification |date=2007-02-08 |accessdate=2010-02-19}}&lt;/ref&gt; The usage rights (UR) signatures define rights for import form data files in FDF, XFDF and text ([[comma-separated values|CSV]]/[[delimiter-separated values|TSV]]) formats, and export form data files in FDF and XFDF formats.&lt;ref name="iso32000" /&gt;

===== Forms Data Format (FDF) =====
{{Infobox file format
| name                   = Forms Data Format (FDF)
| icon                   =
| logo                   =
| screenshot             =
| caption                =
| extension              = .fdf
| mime                   = application/vnd.fdf&lt;ref&gt;{{citation |url=http://www.iana.org/assignments/media-types/application/ |title=IANA Application Media Types - vnd.fdf |accessdate=2010-02-22}}&lt;/ref&gt;
| type code              = 'FDF'
| uniform type           =
| magic                  =
| owner                  = [[Adobe Systems]]
| released               = {{Start date|1996}}&lt;!-- {{Start date|YYYY|mm|dd|df=yes}} --&gt; (PDF 1.2)
| latest release version =
| latest release date    = &lt;!-- {{Start date and age|YYYY|mm|dd|df=yes}} --&gt;
| genre                  =
| container for          =
| contained by           =
| extended from          = PDF
| extended to            = XFDF
| standard               = ISO 32000-1:2008
| free                   = Yes
| url                    =
}}

The Forms Data Format (FDF) is based on PDF, it uses the same syntax and has essentially the same file structure, but is much simpler than PDF, since the body of an FDF document consists of only one required object. Forms Data Format is defined in the PDF specification (since PDF 1.2). The Forms Data Format can be used when submitting form data to a server, receiving the response, and incorporating into the interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. Beginning in PDF 1.3, FDF can be used to define a container for annotations that are separate from the PDF document they apply to. FDF typically encapsulates information such as [[X.509|X.509 certificates]], requests for certificates, directory settings, timestamp server settings, and embedded PDF files for network transmission.&lt;ref name="fdf-exchange" /&gt; The FDF uses the MIME content type application/vnd.fdf, filename extension .fdf and on Mac OS it uses file type 'FDF'.&lt;ref name="iso32000" /&gt; Support for importing and exporting FDF stand-alone files is not widely implemented in free or freeware PDF software. For example, there is no import/export support in Evince, Okular, Poppler, KPDF or Sumatra PDF, however, Evince, Okular and Poppler support filling in of PDF Acroforms and saving filled data inside the PDF file. Import support for stand-alone FDF files is implemented in Adobe Reader; export and import support (including saving of FDF data in PDF) is for example implemented in Foxit Reader and PDF-XChange Viewer Free; saving of FDF data in a PDF file is also supported in pdftk.

===== XML Forms Data Format (XFDF) =====
{{Infobox file format
| name                   = XML Forms Data Format (XFDF)
| icon                   =
| logo                   =
| screenshot             =
| caption                =
| extension              = .xfdf
| mime                   = application/vnd.adobe.xfdf&lt;ref&gt;{{citation |url=http://www.iana.org/assignments/media-types/application/vnd.adobe.xfdf |title=IANA Application Media Types - Vendor Tree - vnd.adobe.xfdf |accessdate=2010-02-22}}&lt;/ref&gt;
| type code              = 'XFDF'
| uniform type           =
| magic                  =
| owner                  = [[Adobe Systems]]
| released               = {{Start date|2003|07|df=yes}} (referenced in PDF 1.5)
| latest release version = 3.0
| latest release date    = {{Start date and age|2009|08|df=yes}}
| genre                  =
| container for          =
| contained by           =
| extended from          = PDF, FDF, [[XML]]
| extended to            =
| standard               = No (under standardization as ISO/CD 19444-1&lt;ref name="iso-xfdf"&gt;{{citation |url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?ics1=35&amp;ics2=240&amp;ics3=30&amp;csnumber=64911 |title=ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0 |accessdate=2014-11-26}}&lt;/ref&gt;)
| free                   =
| url                    = [https://partners.adobe.com/public/developer/en/xml/XFDF_Spec_3.0.pdf XFDF 3.0 specification]
}}

XML Forms Data Format (XFDF) is the XML version of Forms Data Format, but the XFDF implements only a subset of FDF containing forms and annotations. There are not XFDF equivalents for some entries in the FDF dictionary - such as the Status, Encoding, JavaScript, Pages keys, EmbeddedFDFs, Differences and Target. In addition, XFDF does not allow the spawning, or addition, of new pages based on the given data; as can be done when using an FDF file. The XFDF specification is referenced (but not included) in PDF 1.5 specification (and in later versions). It is described separately in ''XML Forms Data Format Specification''.&lt;ref name="xfdf" /&gt; The PDF 1.4 specification allowed form submissions in XML format, but this was replaced by submissions in XFDF format in the PDF 1.5 specification. XFDF conforms to the XML standard. As of November 2014, XFDF 3.0 is in the ISO/IEC standardization process under the formal name ''ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0''.&lt;ref name="iso-xfdf"/&gt;

XFDF can be used the same way as FDF; e.g., form data is submitted to a server, modifications are made, then sent back and the new form data is imported in an interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. A support for importing and exporting XFDF stand-alone files is not widely implemented in free or freeware PDF software. Import of XFDF is implemented in Adobe Reader 5 and later versions; import and export is implemented in PDF-XChange Viewer Free; embedding of XFDF data in PDF form is implemented in pdftk (pdf toolkit).

==== Adobe XML Forms Architecture (XFA) ====
{{Main article|XFA|l1=XML Forms Architecture}}
In the PDF 1.5 format, [[Adobe Systems]] introduced a new, proprietary format for forms, namely Adobe XML Forms Architecture (XFA) forms. The XFA 2.02 is referenced in the PDF 1.5 specification (and also in later versions) but is described separately in ''Adobe XML Forms Architecture (XFA) Specification'', which has several versions.&lt;ref name="xfa-adobe"&gt;{{cite web |url=http://partners.adobe.com/public/developer/xml/index_arch.html |title=Adobe XML Forms Architecture (XFA) |author=Adobe Systems Incorporated |accessdate=2010-02-19}}&lt;/ref&gt; XFA specification is not included in ISO 32000-1 PDF 1.7 and is only referenced as an external proprietary specification created by Adobe. XFA was not standardized as an ISO standard. In 2011 the ISO Committee (TC 171/SC 2/WG 8) urged Adobe Systems to submit the XFA Specification for standardization.&lt;ref name="iso-meeting-n603" /&gt;

Adobe XFA Forms are not compatible with AcroForms. Adobe Reader contains "disabled features" for use of XFA Forms, that activate only when opening a PDF document that was created using enabling technology available only from Adobe.&lt;ref&gt;{{citation |url=https://www.adobe.com/products/eulas/pdfs/Reader_Player_AIR_WWEULA-Combined-20080204_1313.pdf |format=PDF |title=Adobe Reader - Software license agreement |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.adobe.com/go/readerextensions |title=LiveCycle Reader Extensions ES features and benefits |accessdate=2010-02-19 |deadurl=yes |archiveurl=https://web.archive.org/web/20091219163323/http://www.adobe.com/go/readerextensions |archivedate=December 19, 2009 }}&lt;/ref&gt; The XFA Forms are not compatible with Adobe Reader prior to version 6.

XFA forms can be created and used as PDF files or as XDP ([[XML Data Package]]) files. The format of an XFA resource in PDF is described by the XML Data Package Specification.&lt;ref name="iso32000" /&gt; The XDP may be a standalone document or it may in turn be carried inside a PDF document. XDP provides a mechanism for packaging form components within a surrounding XML container. An XDP can also package a PDF file, along with XML form and template data.&lt;ref name="xfa-adobe" /&gt; PDF may contain XFA (in XDP format), but also XFA may contain PDF.&lt;ref name="xfa-adobe" /&gt; When the XFA (XML Forms Architecture) grammars used for an XFA form are moved from one application to another, they must be packaged as an XML Data Package.&lt;ref name="xfa25"&gt;{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_2_5.pdf |format=PDF |title=XML Forms Architecture (XFA) Specification Version 2.5 |date=2007-06-08 |accessdate=2010-02-19}}&lt;/ref&gt;

When the PDF and XFA are combined, the result is a form in which each page of the XFA form overlays a PDF background. This architecture is
sometimes referred to as XFAF (XFA Foreground). The alternative is to express all of the form, including boilerplate, directly in XFA (without using PDF, or only using "Shell PDF" which is a container for XFA with minimal skeleton of PDF markup, or using a pre-rendered depiction of a static XFA form as PDF pages). It is sometimes called ''full'' XFA.&lt;ref name="xfa25" /&gt;

Starting with PDF 1.5, the text contents of variable text form fields, as well as markup annotations may include formatting information (style information). These rich text strings are XML documents that conform to the rich text conventions specified for the XML Forms Architecture specification 2.02, which is itself a subset of the XHTML 1.0 specification, augmented with a restricted set of CSS2 style attributes.&lt;ref name="iso32000" /&gt;
In PDF 1.6, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.2.
In PDF 1.7, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.4.&lt;ref name="iso32000" /&gt;

Most PDF processors do not handle XFA content. When generating a shell PDF it is recommended to include in the PDF markup a simple one-page PDF image displaying a warning message (e.g. "To view the full contents of this document, you need a later version of the PDF viewer.", etc.). PDF processors that can render XFA content should either not display the supplied warning page image or replace it quickly with the dynamic form content.&lt;ref name="xfa33"&gt;{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_3_3.pdf |title=XML Forms Architecture (XFA) Specification Version 3.3 |date=2012-01-09 |accessdate=2014-04-09}}&lt;/ref&gt; Examples of PDF software with some support of XFA rendering include Adobe Reader for Windows, Linux, macOS (but not Adobe Reader Mobile for Android or iOS) or Nuance PDF Reader.

In 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").&lt;ref name="DRAFT INTERNATIONAL" /&gt;

=== Logical structure and accessibility ===

A "tagged" PDF (ISO 32000-1:2008 14.8) includes document structure and semantics information to enable reliable text extraction and accessibility. Technically speaking, tagged PDF is a stylized use of the format that builds on the logical structure framework introduced in PDF 1.3. Tagged PDF defines a set of standard structure types and attributes that allow page content (text, graphics, and images) to be extracted and reused for other purposes.&lt;ref&gt;[http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 What is Tagged PDF?]&lt;/ref&gt;

Tagged PDF is not required in situations where a PDF file is intended only for print. Since the feature is optional, and since the rules for Tagged PDF as specified in ISO 32000-1 are relatively vague, support for tagged PDF amongst consuming devices, including assistive technology (AT), is uneven.&lt;ref&gt;{{cite web|url=http://www.washington.edu/doit/Stem/articles?1002|title=Is PDF accessible?|work=washington.edu}}&lt;/ref&gt;

An [[AIIM]] project to develop an ISO-standardized subset of PDF specifically targeted at accessibility began in 2004, eventually becoming [[PDF/UA]].

=== Security and signatures ===

A PDF file may be encrypted for security, or digitally signed for authentication.

The standard security provided by Acrobat PDF consists of two different methods and two different passwords, ''user password'', which encrypts the file and prevents opening, and ''owner password'', which specifies operations that should be restricted even when the document is decrypted, which can include: printing, copying text and graphics out of the document, modifying the document, or adding or modifying text notes and [[Acroforms|AcroForm]] fields. The user password (controls opening) encrypts the file and requires [[password cracking]] to defeat, with difficulty depending on password strength and encryption method &#8211; it is potentially very secure (assuming good password and encryption method without known attacks). The owner password (controls operations) does not encrypt the file, and instead relies on client software to respect these restrictions, and is not secure. An "owner password" can be removed by many commonly available "PDF cracking" software, including some free online services.&lt;ref&gt;{{cite web|url=http://freemypdf.com/|title=FreeMyPDF.com - Removes passwords from viewable PDFs|work=freemypdf.com}}&lt;/ref&gt; Thus, the use restrictions that a document author places on a PDF document are not secure, and cannot be assured once the file is distributed; this warning is displayed when applying such restrictions using Adobe Acrobat software to create or edit PDF files.

Even without removing the password, most freeware or open source PDF readers ignore the permission "protections" and allow the user to print or make copy of excerpts of the text as if the document were not limited by password protection.&lt;ref&gt;{{cite web |url= http://www.macworld.com/article/1137343/pdf.html |title=Adobe admits new PDF password protection is weaker |author= Jeremy Kirk}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.cs.cmu.edu/~dst/Adobe/Gallery/PDFsecurity.pdf  |title= How secure is PDF |author=Bryan Guignard}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.planetpdf.com/planetpdf/pdfs/pdf2k/01W/merz_securitykeynote.pdf |title= PDF Security Overview: Strengths and Weaknesses }}&lt;/ref&gt;

There are a number of commercial solutions including [[Adobe LiveCycle]] [[Adobe LiveCycle#LiveCycle Rights Management ES4|Rights Management]] and Locklizard PDF DRM&lt;ref&gt;{{cite web |url=http://www.infosecurity-magazine.com/news/locklizard-develops-zero-footprint-solution-for/ |title=LockLizard Develops Zero Footprint Solution for PDF Security }}&lt;/ref&gt; that are more robust means of [[information rights management]]. Not only can they restrict document access but they also reliably enforce [[File system permissions|permissions]] in ways that the standard security handler does not.&lt;ref&gt;{{cite web |url=http://www.locklizard.com/pdf_security_drm/ |title=PDF DRM Security Software for Adobe Document Protection}}&lt;/ref&gt;

==== Usage rights ====
Beginning with PDF 1.5, Usage rights (UR) signatures are used to enable additional interactive features that are not available by default in a particular PDF viewer application. The signature is used to validate that the permissions have been granted by a bona fide granting authority. For example, it can be used to allow a user:&lt;ref name="iso32000" /&gt;
* to save the PDF document along with modified form and/or annotation data
* import form data files in FDF, XFDF and text (CSV/TSV) formats
* export form data files in FDF and XFDF formats
* submit form data
* instantiate new pages from named page templates
* apply a [[Digital data|digital]] [[signature]] to existing [[digital signature]] form field
* create, delete, modify, copy, import, export annotations

For example, Adobe Systems grants permissions to enable additional features in Adobe Reader, using public-key [[cryptography]]. Adobe Reader verifies that the signature uses a [[Public key certificate|certificate]] from an Adobe-[[authorize]]d certificate authority. The PDF 1.5 specification declares that other PDF viewer applications are free to use this same mechanism for their own purposes.&lt;ref name="iso32000" /&gt;

=== File attachments ===

PDF files can have document-level and page-level file attachments, which the reader can access and open or save to their local filesystem. PDF attachments can be added to existing PDF files for example using [[pdftk]]. Adobe Reader provides support for attachments, and [[poppler (software)|poppler]]-based readers like [[Evince]] or [[Okular]] also have some support for document-level attachments.

=== Metadata ===
PDF files can contain two types of metadata.&lt;ref&gt;[https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf Adobe PDF reference version 1.7], section 10.2&lt;/ref&gt; The first is the Document Information Dictionary, a set of key/value fields such as author, title, subject, creation and update dates. This is stored in the optional Info trailer of the file. A small set of fields is defined, and can be extended with additional text values if required.

In PDF 1.4, support was added for Metadata Streams, using the [[Extensible Metadata Platform]] (XMP) to add XML standards-based extensible metadata as used in other file formats. This allows metadata to be attached to any stream in the document, such as information about embedded illustrations, as well as the whole document (attaching to the document catalog), using an extensible schema.

== Intellectual property ==

Anyone may create applications that can read and write PDF files without having to pay royalties to [[Adobe Systems]]; Adobe holds patents to PDF, but licenses them for [[royalty-free]] use in developing software complying with its PDF specification.&lt;ref&gt;{{cite web|url=http://partners.adobe.com/public/developer/support/topic_legal_notices.html|title=Developer Resources|work=adobe.com}}&lt;/ref&gt;

== Technical issues ==

=== Accessibility ===
PDF files can be created specifically to be accessible for disabled people.&lt;ref&gt;{{cite web |url=http://www.webaim.org/techniques/acrobat/ |title=PDF Accessibility |publisher=WebAIM |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.alistapart.com/articles/pdf_accessibility |title=Facts and Opinions About PDF Accessibility |author=Joe Clark |date=2005-08-22 |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://wac.osu.edu/pdf/ |title=Accessibility and PDF documents |publisher=Web Accessibility Center |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.bbc.co.uk/guidelines/futuremedia/accessibility/accessible_pdf.shtml |title=PDF Accessibility Standards v1.2 |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=http://www.csus.edu/training/handouts/workshops/creating_accessible_pdfs.pdf |format=PDF |title=PDF Accessibility |publisher=California State University |accessdate=2010-04-24}}&lt;/ref&gt; PDF file formats in use {{As of|2014|lc=on}} can include tags ([[XML]]), text equivalents, captions, audio descriptions, etc. Tagged PDF is required in the [[PDF/A]]-1a specification.&lt;ref&gt;{{citation |url=http://www.aiim.org/documents/standards/PDF-A/19005-1_FAQ.pdf |title=Frequently Asked Questions (FAQs) &#8211; ISO 19005-1:2005 &#8211; PDF/A-1, Date: July 10, 2006 |format=PDF |date=2006-07-10 |accessdate=2011-07-06}}&lt;/ref&gt;&lt;ref name="pdfa1-tech"&gt;{{cite web |url=http://www.pdfa.org/doku.php?id=artikel:en:pdfa_a_look_at_the_technical-side |title=PDF/A &#8211; A Look at the Technical Side |accessdate=2011-07-06}}&lt;/ref&gt; Some software can automatically produce tagged PDFs, but this feature is not always enabled by default.&lt;ref&gt;{{citation |url=http://help.libreoffice.org/Common/Export_as_PDF#PDF.2FA-1a |title=LibreOffice Help - Export as PDF |accessdate=2012-09-22}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=http://www.oooninja.com/2008/01/generating-pdfa-for-long-term-archiving.html |title=Exporting PDF/A for long-term archiving |date=2008-01-11}}&lt;/ref&gt; Leading [[screen reader]]s, including [[JAWS (screen reader)|JAWS]], [[Window-Eyes]], Hal, and [[Kurzweil Educational Systems|Kurzweil 1000 and 3000]] can read tagged PDFs aloud, as can later versions of the Acrobat and Acrobat Reader programs.&lt;ref&gt;{{cite web |url=http://help.adobe.com/en_US/Reader/8.0/help.html?content=WS58a04a822e3e50102bd615109794195ff-7d15.html |title=Adobe Reader 8 - Read a PDF with Read Out Loud |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite news |url=http://gadgetwise.blogs.nytimes.com/2009/04/10/tip-of-the-week-adobe-readers-read-aloud-feature/ |title=Tip of the Week: Adobe Reader&#8217;s &#8216;Read Aloud&#8217; Feature |accessdate=2010-04-24 | work=The New York Times | date=2009-04-10 |first=J.D. |last=Biersdorfer}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=https://www.adobe.com/accessibility/pdfs/accessing-pdf-sr.pdf |format=PDF |title=Accessing PDF documents with assistive technology: A screen reader user's guide |publisher=Adobe |accessdate=2010-04-24}}&lt;/ref&gt; Moreover, tagged PDFs can be re-flowed and magnified for readers with visual impairments. Problems remain with adding tags to older PDFs and those that are generated from scanned documents. In these cases, accessibility tags and re-flowing are unavailable, and must be created either manually or with OCR techniques. These processes are inaccessible to some disabled people.

One of the significant challenges with PDF accessibility is that PDF documents have three distinct views, which, depending on the document's creation, can be inconsistent with each other. The three views are (i) the physical view, (ii) the tags view, and (iii) the content view. The physical view is displayed and printed (what most people consider a PDF document). The tags view is what screen readers and other assistive technologies use to deliver a high-quality navigation and reading experience to users with disabilities. The content view is based on the physical order of objects within the PDF's content stream and may be displayed by software that does not fully support the tags view, such as the Reflow feature in Adobe's Reader.

[[PDF/UA]], the International Standard for accessible PDF based on ISO 32000-1 was published as ISO 14289-1 in 2012, and establishes normative language for accessible PDF technology.

=== Viruses and exploits ===
{{see also|Adobe Acrobat#Security}}
PDF attachments carrying viruses were first discovered in 2001. The virus, named ''OUTLOOK.PDFWorm'' or ''Peachy'', uses [[Microsoft Outlook]] to send itself as an attachment to an Adobe PDF file. It was activated with Adobe Acrobat, but not with Acrobat Reader.&lt;ref&gt;Adobe Forums, [https://forums.adobe.com/thread/302989 Announcement: PDF Attachment Virus "Peachy"], 15 August 2001.&lt;/ref&gt;

From time to time, new vulnerabilities are discovered in various versions of Adobe Reader,&lt;ref&gt;{{cite web|url=https://www.adobe.com/support/security/#readerwin |title=Security bulletins and advisories |publisher=Adobe |date= |accessdate=2010-02-21}}&lt;/ref&gt; prompting the company to issue security fixes. Other PDF readers are also susceptible. One aggravating factor is that a PDF reader can be configured to start automatically if a web page has an embedded PDF file, providing a vector for attack. If a malicious web page contains an infected PDF file that takes advantage of a vulnerability in the PDF reader, the system may be compromised even if the browser is secure. Some of these vulnerabilities are a result of the PDF standard allowing PDF documents to be scripted with JavaScript. Disabling JavaScript execution in the PDF reader can help mitigate such future exploits, although it does not protect against exploits in other parts of the PDF viewing software. Security experts say that JavaScript is not essential for a PDF reader, and that the security benefit that comes from disabling JavaScript outweighs any compatibility issues caused.&lt;ref&gt;[http://www.grc.com/sn/sn-187.txt Steve Gibson - SecurityNow Podcast]&lt;/ref&gt; One way of avoiding PDF file exploits is to have a local or web service convert files to another format before viewing.

On March 30, 2010 security researcher Didier Stevens reported an Adobe Reader and Foxit Reader exploit that runs a malicious executable if the user allows it to launch when asked.&lt;ref&gt;{{cite web|url=http://blogs.pcmag.com/securitywatch/2010/03/malicious_pdfs_execute_code_wi.php|title=Malicious PDFs Execute Code Without a Vulnerability|work=PCMAG}}&lt;/ref&gt;

=== Usage restrictions and monitoring ===

PDFs may be [[encrypted]] so that a password is needed to view or edit the contents. The PDF Reference defines both 40-bit and 128-bit encryption, both making use of a complex system of [[RC4]] and [[MD5]]. The PDF Reference also defines ways that third parties can define their own encryption systems for PDF.

PDF files may also contain embedded [[digital rights management|DRM]] restrictions that provide further controls that limit copying, editing or printing. The restrictions on copying, editing, or printing depend on the reader software to obey them, so the security they provide is limited.

The PDF Reference has technical details for an end-user overview.&lt;ref&gt;{{cite web|url=http://createpdf.adobe.com/cgi-feeder.pl/help_security?BP=&amp;LOC=en_US |title=Create Adobe PDF Online - Security Settings Help |publisher=Createpdf.adobe.com |date= |accessdate=2010-02-21}}&lt;/ref&gt;  Like HTML files, PDF files may submit information to a web server. This could be used to track the [[IP address]] of the client PC, a process known as [[phoning home]]. After update 7.0.5 to Acrobat Reader, the user is notified "...&amp;nbsp;via a dialogue box that the author of the file is auditing usage of the file, and be offered the option of continuing."&lt;ref&gt;[https://www.adobe.com/support/techdocs/332208.html New features and issues addressed in the Acrobat 7.0.5 Update (Acrobat and Adobe Reader for Windows and Mac OS)]&lt;/ref&gt;

Through its [[Adobe LiveCycle|LiveCycle Policy Server]] product, Adobe provides a method to set security policies on specific documents. This can include requiring a user to authenticate and limiting the period during which a document can be accessed or amount of time a document can be opened while offline. Once a PDF document is tied to a policy server and a specific policy, that policy can be changed or revoked by the owner. This controls documents that are otherwise "in the wild." Each document open and close event can also be tracked by the policy server. Policy servers can be set up privately or Adobe offers a public service through Adobe Online Services. As with other forms of DRM, adherence to these policies and restrictions may or may not be enforced by the reader software being used.

=== Default display settings ===
PDF documents can contain display settings, including the page display layout and zoom level. Adobe Reader uses these settings to override the user's default settings when opening the document.&lt;ref&gt;{{cite web | title=Getting Familiar with Adobe Reader &amp;gt; Understanding Preferences | url=http://www.adobepress.com/articles/article.asp?p=412914 | accessdate=2009-04-22}}&lt;/ref&gt; The free Adobe Reader cannot remove these settings.

== Content ==
A PDF file is often a combination of [[vector graphics]], text, and [[bitmap graphics]]. The basic types of content in a PDF are:
* Text stored as content streams (i.e., not text)
* Vector graphics for illustrations and designs that consist of shapes and lines
* Raster graphics for photographs and other types of image
* Multimedia objects in the document

In later PDF revisions, a PDF document can also support links (inside document or web page), forms, JavaScript (initially available as plugin for Acrobat 3.0), or any other types of embedded contents that can be handled using plug-ins.

PDF 1.6 supports interactive 3D documents embedded in the PDF - 3D drawings can be embedded using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.&lt;ref name="3d#1"&gt;{{cite web|url=https://www.adobe.com/manufacturing/resources/3dformats/ |title=3D supported formats |publisher=Adobe |date=2009-07-14 |accessdate=2010-02-21}}&lt;/ref&gt;&lt;ref name="3d#2"&gt;{{cite web|url=https://www.adobe.com/devnet/acrobat3d/ |title=Acrobat 3D Developer Center |publisher=Adobe |date= |accessdate=2010-02-21}}&lt;/ref&gt;

Two PDF files that look similar on a computer screen may be of very different sizes. For example, a high resolution raster image takes more space than a low resolution one. Typically higher resolution is needed for printing documents than for displaying them on screen. Other things that may increase the size of a file is embedding full fonts, especially for Asiatic scripts, and storing text as graphics.

== Software ==
{{Details|List of PDF software}}
PDF viewers are generally provided free of charge, and many versions are available from a variety of sources.

There are many software options for creating PDFs, including the PDF printing capabilities built into [[macOS]] and most [[Linux]] distributions, [[LibreOffice]], [[Microsoft Office 2007]] (if updated to [[Office 2007#Service Pack 2|SP2]]) and later,&lt;ref&gt;{{cite web |url=http://support.microsoft.com/kb/953195|title=Description of 2007 Microsoft Office Suite Service Pack 2 (SP2) |publisher=[[Microsoft]] |accessdate=2009-05-09}}&lt;/ref&gt; [[WordPerfect]] 9, [[Scribus]], numerous PDF print drivers for [[Microsoft Windows]], the [[pdfTeX]] typesetting system, the [[DocBook]] PDF tools, applications developed around [[Ghostscript]] and [[Adobe Acrobat]] itself as well as [[Adobe InDesign]], [[Adobe FrameMaker]], [[Adobe Illustrator]], [[Adobe Photoshop]]. [[Google]]'s online office suite [[Google Docs]] also allows for uploading and saving to PDF.

[[Raster image processor]]s (RIPs) are used to convert PDF files into a [[raster graphics|raster format]] suitable for imaging onto paper and other media in printers, digital production presses and [[prepress]] in a process known as [[rasterisation]]. RIPs capable of processing PDF directly include the Adobe PDF Print Engine&lt;ref&gt;{{cite web|url=https://www.adobe.com/products/pdfprintengine/overview.html|title=Adobe PDF Print Engine|work=adobe.com}}&lt;/ref&gt; from [[Adobe Systems]] and Jaws&lt;ref&gt;{{cite web|url=http://www.globalgraphics.com/products/jaws_rip/|title=Jaws&#174; 3.0 PDF and PostScript RIP SDK|work=globalgraphics.com}}&lt;/ref&gt; and the [[Harlequin RIP]] from [[Global Graphics]].

=== Editing ===
{{Expand section|date=July 2010|reason=[[hybrid PDF]], a variant of [[LibreOffice]] isn't mentioned}}
There is specialized software for editing PDF files, though the choices are much more limited and often more expensive than creating and editing standard editable document formats. Version 0.46 and later of [[Inkscape]] allows PDF editing through an intermediate translation step involving [[Poppler (software)|Poppler]].

[[Serif PagePlus]] can open, edit and save existing PDF documents, as well as publishing of documents created in the package.

[[Enfocus]] PitStop Pro, a plugin for Acrobat, allows manual and automatic editing of PDF files,&lt;ref&gt;{{cite web|url=http://www.enfocus.com/product.php?id=855|title=Preflight and edit PDF files in Acrobat|work=enfocus.com}}&lt;/ref&gt; while the free Enfocus Browser makes it possible to edit the low-level structure of a PDF.&lt;ref&gt;{{cite web|url=http://www.enfocus.com/product.php?id=4530|title=Enfocus product overview - online store|work=enfocus.com}}&lt;/ref&gt;

[[Dochub]], is a free online PDF editing tool that can be used without purchasing anything.&lt;ref&gt;{{Cite web|title = DocHub|url = http://www.dochub.com|website = DocHub|accessdate = 2015-12-12}}&lt;/ref&gt;

=== Annotation ===
{{See also|Comparison of notetaking software}}
[[Adobe Acrobat]] is one example of proprietary software that allows the user to annotate, highlight, and add notes to already created PDF files. One UNIX application available as [[free software]] (under the [[GNU General Public License]]) is [[PDFedit]]. Another GPL-licensed application native to the unix environment is Xournal. Xournal allows for annotating in different fonts and colours, as well as a rule for quickly underlining and highlighting lines of text or paragraphs. Xournal also has a shape recognition tool for squares, rectangles and circles. In Xournal annotations may be moved, copied and pasted. The [[freeware]] [[Foxit Reader]], available for [[Microsoft Windows]], [[macOS]] and [[Linux]], allows annotating documents. Tracker Software's [[PDF-XChange Viewer]] allows annotations and markups without restrictions in its freeware alternative. [[Apple Inc.|Apple]]'s [[macOS]]'s integrated PDF viewer, Preview, does also enable annotations as does the freeware [[Skim (software)|Skim]], with the latter supporting interaction with [[LaTeX]], SyncTeX, and PDFSync and integration with [[BibDesk]] reference management software. Freeware [[Qiqqa]] can create an annotation report that summarizes all the annotations and notes one has made across their library of PDFs.

For mobile annotation, [[iAnnotate PDF]] (from Branchfire) and [[GoodReader]] (from Aji) allow annotation of PDFs as well as exporting summaries of the annotations.

There are also [[web annotation]] systems that support annotation in pdf and other documents formats, e.g., [[A.nnotate]], [[crocodoc]], WebNotes.

In cases where PDFs are expected to have all of the functionality of paper documents, ink annotation is required. Some programs that accept ink input from the mouse may not be responsive enough for handwriting input on a tablet. Existing solutions on the PC include [[PDF Annotator]] and [[Qiqqa]].

=== Other ===
Examples of PDF software as online services including [[Scribd]] for viewing and storing, [[Pdfvue]] for online editing, and [[Zamzar]] for conversion.

In 1993 the Jaws [[raster image processor]] from [[Global Graphics]] became the first shipping prepress RIP that interpreted PDF natively without conversion to another format. The company released an upgrade to their Harlequin RIP with the same capability in 1997.&lt;ref&gt;{{cite web |url= http://www.globalgraphics.com/products/harlequin-multi-rip |title=Harlequin MultiRIP|accessdate=2014-03-02}}&lt;/ref&gt;

[[Agfa-Gevaert]] introduced and shipped Apogee, the first prepress workflow system based on PDF, in 1997.

Many commercial offset printers have accepted the submission of press-ready PDF files as a print source, specifically the PDF/X-1a subset and variations of the same.&lt;ref&gt;[http://www.prepressx.com/ Press-Ready PDF Files] "For anyone interested in having their graphic project commercially printed directly from digital files or PDFs." (last checked on 2009-02-10).&lt;/ref&gt; The submission of press-ready PDF files are a replacement for the problematic need for receiving collected native working files.

PDF was selected as the "native" [[metafile]] format for [[macOS|Mac OS X]], replacing the [[PICT]] format of the earlier [[classic Mac OS]]. The imaging model of the [[Quartz (graphics layer)|Quartz]] graphics layer is based on the model common to [[Display PostScript]] and PDF, leading to the nickname ''Display PDF''. The Preview application can display PDF files, as can version 2.0 and later of the [[Safari (web browser)|Safari]] web browser. System-level support for PDF allows Mac OS X applications to create PDF documents automatically, provided they support the OS-standard printing architecture. The files are then exported in PDF 1.3 format according to the file header. When taking a screenshot under Mac OS X versions 10.0 through 10.3, the image was also captured as a PDF; later versions save screen captures as a [[Portable Network Graphics|PNG]] file, though this behaviour can be set back to PDF if desired.

In 2006 PDF was widely accepted as the standard print job format at the [[Open Source Development Labs]] Printing Summit. It is supported as a print job format by the [[CUPS|Common Unix Printing System]] and desktop application projects such as [[GNOME]], [[KDE]], [[Firefox]], [[Mozilla Thunderbird|Thunderbird]], [[LibreOffice]] and [[OpenOffice.org|OpenOffice]] have switched to emit print jobs in PDF.&lt;ref&gt;{{cite web|title=PDF as Standard Print Job Format|url=http://www.linuxfoundation.org/collaborate/workgroups/openprinting/pdf_as_standard_print_job_format|website=The Linux Foundation|publisher=[[Linux Foundation]]|accessdate=21 June 2016}}&lt;/ref&gt;

Some desktop printers also support direct PDF printing, which can interpret PDF data without external help. Currently, all PDF capable printers also support PostScript, but most PostScript printers do not support direct PDF printing.

The [[Free Software Foundation]] once considered one of their [[High priority free software projects|high priority projects]] to be "developing a free, high-quality and fully functional set of libraries and programs that implement the PDF file format and associated technologies to the ISO 32000 standard."&lt;ref&gt;On 2014-04-02, a note dated 2009-02-10 referred to [http://www.fsf.org/campaigns/priority.html Current FSF High Priority Free Software Projects] as a source. Content of the latter page, however, changes over time.&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://gnupdf.org/Goals_and_Motivations |title=Goals and Motivations |authors=GNUpdf contributors| publisher=''GNUpdf'' |date=2007-11-28 |website=gnupdf.org |accessdate=2014-04-02 }}&lt;/ref&gt; In 2011, however, the [[GNU PDF]] project was removed from the list of "high priority projects" due to the maturation of the [[Poppler (software)|Poppler library]],&lt;ref&gt;{{cite web|title=GNU PDF project leaves FSF High Priority Projects list; mission complete! |url=http://www.fsf.org/blogs/community/gnu-pdf-project-leaves-high-priority-projects-list-mission-complete|date=2011-10-06|first=Matt|last=Lee|publisher=Free Software Foundation|website=fsf.org|accessdate=2014-04-02}}&lt;/ref&gt; which has enjoyed wider use in applications such as [[Evince]] with the [[GNOME]] desktop environment. Poppler is based on [[Xpdf]]&lt;ref&gt;[http://poppler.freedesktop.org/ Poppler homepage] "Poppler is a PDF rendering library based on the xpdf-3.0 code base." (last checked on 2009-02-10)&lt;/ref&gt;&lt;ref&gt;[http://cgit.freedesktop.org/poppler/poppler/tree/README-XPDF Xpdf license] "Xpdf is licensed under the GNU General Public License (GPL), version 2 or 3." (last checked on 2012-09-23).&lt;/ref&gt; code base. There are also commercial development libraries available as listed in [[List of PDF software]].

The [[Apache PDFBox]] project of the [[Apache Software Foundation]] is an open source Java library for working with PDF documents. PDFBox is licensed under the [[Apache License]].&lt;ref&gt;[http://pdfbox.apache.org/ The Apache PDFBox project] . Retrieved 2009-09-19.&lt;/ref&gt;

== See also ==
{{Portal|Software}}{{columns-list|2|
* [[Open XML Paper Specification]]
* [[Comparison of OpenXPS and PDF]]
* [[DjVu]]
* [[PAdES]], &lt;small&gt;PDF Advanced Electronic Signature&lt;/small&gt;
* [[Web document]]
* [[XSL Formatting Objects]]
}}

== References ==
{{Reflist|30em}}

== Further reading ==
* {{Cite book | last1 = Hardy | first1 = M. R. B. | last2 = Brailsford | first2 = D. F. | chapter = Mapping and displaying structural transformations between XML and PDF | title = Proceedings of the 2002 ACM symposium on Document engineering  - DocEng '02 | pages = 95&#8211;102| year = 2002 | url = http://www.cs.nott.ac.uk/~dfb/Publications/Download/2002/Hardy02.pdf| doi = 10.1145/585058.585077| publisher = Proceedings of the 2002 ACM symposium on Document engineering| isbn = 1-58113-594-7}}
*Standards
** PDF 1.7 [http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf]
** PDF 1.6 (ISBN 0-321-30474-8)
** PDF 1.4 (ISBN 0-201-75839-3)
** PDF 1.3 (ISBN 0-201-61588-6)

== External links ==
{{Commons category|PDF}}
* [http://www.quora.com/PDF-file-format/How-was-the-PDF-format-created How was the PDF format created? Quora]
* [http://www.pdfa.org/ PDF Association] - The PDF Association is the industry association for software developers producing or processing PDF files.
* [http://partners.adobe.com/public/developer/tips/topic_tip31.html Adobe PDF 101: Summary of PDF]
* [https://www.adobe.com/print/features/psvspdf/ Adobe: PostScript vs. PDF] &#8211; Official introductory comparison of PS, EPS vs. PDF.
* {{webarchive |url=https://web.archive.org/web/20110424013530/http://www.aiim.org/Resources/Archive/Magazine/2007-Jul-Aug/33448 |date=April 24, 2011 |title=''PDF Standards....transitioning the PDF specification from a de facto standard to a de jure standard'' }} &#8211; Information about PDF/E and PDF/UA specification for accessible documents file format (archived by [[Wayback Machine|The Wayback Machine]])
* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=38920 ISO 19005-1:2005] the PDF/A-1 ISO standard published by the [[International Organization for Standardization]] (chargeable)
* [https://www.adobe.com/devnet/pdf/pdf_reference.html PDF Reference and Adobe Extensions to the PDF Specification]
* [http://www.mactech.com/articles/mactech/Vol.15/15.09/PDFIntro/ Portable Document Format: An Introduction for Programmers] &#8211; Introduction to PDF vs. PostScript and PDF internals (up to v1.3)
* [http://www.planetpdf.com/enterprise/article.asp?ContentID=6519 The Camelot Paper] &#8211; the paper in which John Warnock outlined the project that created PDF
* [http://river-valley.zeeba.tv/everything-you-wanted-to-know-about-pdf-but-were-afraid-to-ask/ Everything you wanted to know about PDF but were afraid to ask] - recording of talk by Leonard Rosenthol (Adobe Systems) at TUG 2007
* [http://www.data2type.de/en/xml-xslt-xslfo/xsl-fo/ How to produce PDF with XSL-FO]
* [http://pdfextractoronline.com/ PDF To Excel Converter]
{{Graphics file formats}}
{{Office document file formats}}
{{ISO standards}}
{{Ebooks}} &lt;!--navbox--&gt;

[[Category:1993 introductions]]
[[Category:Adobe Systems]]
[[Category:Digital press]]
[[Category:Electronic documents]]
[[Category:Graphics file formats]]
[[Category:ISO standards]]
[[Category:Office document file formats]]
[[Category:Open formats]]
[[Category:Page description languages]]
[[Category:Vector graphics]]</text>
      <sha1>s7di8iua6533bmdpwlkzpqps301i0rk</sha1>
    </revision>
  </page>
  <page>
    <title>Arts and Humanities Citation Index</title>
    <ns>0</ns>
    <id>2209985</id>
    <revision>
      <id>723409022</id>
      <parentid>723231144</parentid>
      <timestamp>2016-06-02T21:50:43Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/223.176.143.79|223.176.143.79]] ([[User talk:223.176.143.79|talk]]) to last revision by DavidLeighEllis. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4426" xml:space="preserve">{{ infobox bibliographic database
| image       = 
| caption     = 
| producer    =Thomson Reuters 
| country     =United States 
| history     = 
| languages   = 
| providers   =Web of Science, Dialog Bluesheets 
| cost        =Subscription 
| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio 
| depth       =Index, abstract, citation indexing, author 
| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances 
| temporal    =1975 to present 
| geospatial  =global 
| number      = 
| updates     = 
| p_title     = 
| p_dates     = 
| ISSN        = 
| web         = 
| titles      =  
}}

The '''''Arts &amp; Humanities Citation Index''''' ('''A&amp;HCI'''), also known as '''''Arts &amp; Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore, the print counterpart is Current Contents.

Subjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio.

Available citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances.

This database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.

According to Thomson Reuters, the ''Arts &amp; Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.&lt;ref name=dialog-blue&gt;
{{Cite web
  | title =Arts &amp; Humanities Search (File 255) 
  | publisher =Dialog bluesheets  
  | date = 
  | url =http://library.dialog.com/bluesheets/html/bl0439.html 
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=Iowa&gt;
Description of Arts &amp; Humanities Search. 
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/artshm.html
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=Iowa-wos&gt;
Description of Web of Science coverage.  
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/websci.html
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=TR&gt;
See the page entitled "Tech Specs" 
{{Cite web
  | title =Database description
  | publisher =Thomson Reuters  
  | year = 
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;

==History==
The index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP &amp; Science division.

==See also==
* [[Science Citation Index]]
* [[Social Sciences Citation Index]]

==References==
{{Reflist}}

== External links ==
* {{Official website|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.
* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.

{{Thomson Reuters}}

{{DEFAULTSORT:Arts And Humanities Citation Index}}
[[Category:Citation indices]]
[[Category:Thomson Reuters]]
[[Category:Arts journals| ]]
[[Category:Humanities journals| ]]</text>
      <sha1>hjsex5qbo1zc6yyfo7ukm1dftouu8tp</sha1>
    </revision>
  </page>
  <page>
    <title>Materials Science Citation Index</title>
    <ns>0</ns>
    <id>27789063</id>
    <revision>
      <id>678206130</id>
      <parentid>670536809</parentid>
      <timestamp>2015-08-28T00:13:54Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Bibliographic databases to [[:Category:Bibliographic databases and indexes]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2015 July 4]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3035" xml:space="preserve">{{Third-party|date=February 2013}}
'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science &amp; engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.&lt;ref name=msci-est&gt;Pemberton, Julia K. "''Two new databases from ISI''." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.&lt;/ref&gt;

''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.&lt;ref name=msci-jnlList&gt;[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.&lt;/ref&gt;

==Editions==
Coverage of Materials science is accomplished with the following editions:&lt;ref name=MS-indexes&gt;[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.&lt;/ref&gt;&lt;ref&gt;[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010&lt;/ref&gt;
*Materials Science, Ceramics
*Materials Science, Characterization &amp; Testing
*Materials Science, Biomaterials
*Materials Science, Coatings &amp; Films
*Materials Science, Composites
*Materials Science, Paper &amp; Wood
*Materials Science, Multidisciplinary
*Materials Science, Textiles

==See also==
* [[Science Citation Index]]
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956
* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975
* [[Impact factor]]
* [[VINITI Database RAS]]

==References==
{{Reflist}}

{{Thomson Reuters}}

[[Category:Thomson Reuters]]
[[Category:Bibliographic databases and indexes]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Materials science journals| ]]


{{science-journal-stub}}</text>
      <sha1>8dr53zeh2xbbzty7olo7igq5ii90kbx</sha1>
    </revision>
  </page>
  <page>
    <title>Book Citation Index</title>
    <ns>0</ns>
    <id>46862330</id>
    <revision>
      <id>729367224</id>
      <parentid>723222330</parentid>
      <timestamp>2016-07-11T18:41:54Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <minor />
      <comment>/* Further reading */clean up, use arxiv parameter, remove url redundant with arxiv using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5637" xml:space="preserve">The '''Book Citation Index''' ('''BCI''', '''BKCI''') is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] and is part of the [[Web of Science|Web of Science Core Collection]].&lt;ref&gt;{{cite book|last1=Campbell|first1=Robert|last2=Pentz|first2=Ed|last3=Borthwick|first3=Ian|title=Academic and Professional Publishing|date=2012|publisher=Chandos Publishing|isbn=9781780633091|pages=247&#8211;248|url=https://books.google.com/books?id=IpRlAgAAQBAJ&amp;pg=PA247&amp;dq=%22Book+Citation+Index%22&amp;hl=en&amp;sa=X&amp;ei=Ig9sVZ2QB4rLsASwyYHYBw&amp;ved=0CGoQ6AEwCg#v=onepage&amp;q=%22Book%20Citation%20Index%22&amp;f=false|accessdate=1 June 2015}}&lt;/ref&gt; It was first launched in 2011 and indexes over 60,000 editorially selected books, starting from 2005.&lt;ref name=ATL&gt;{{cite journal|title=Thomson reuters launches Book Citation Index|journal=Advanced Technology Libraries|date=11/01/2011|volume=40|issue=11|page=3|url=http://web.a.ebscohost.com.ezproxy2.library.drexel.edu/ehost/pdfviewer/pdfviewer?sid=3118f8aa-bb72-4992-b82e-be196198670d%40sessionmgr4002&amp;vid=1&amp;hid=4212|accessdate=1 June 2015}}&lt;/ref&gt; Books in the index are electronic and print scholarly texts that contain articles based on [[original research]] and/or reviews of such literature.&lt;ref name=ATL /&gt;

==Content==
The index covers series and non-series books as long as they include full footnotes and the index has two separate editions, a Science edition and a Social Sciences &amp; Humanities edition. The Science edition covers physics and chemistry, engineering, computing and technology, clinical medicine, life sciences, and agriculture and biology. Currently both series only contain books that date back to 2005.&lt;ref&gt;{{cite book|last1=Mann|first1=Thomas|title=The Oxford Guide to Library Research|date=2015|publisher=Oxford University Press|isbn=9780199394463|url=https://books.google.com/books?id=llVLBgAAQBAJ&amp;pg=PT193&amp;dq=%22Book+Citation+Index%22&amp;hl=en&amp;sa=X&amp;ei=Ig9sVZ2QB4rLsASwyYHYBw&amp;ved=0CF4Q6AEwCA#v=onepage&amp;q=%22Book%20Citation%20Index%22&amp;f=false|accessdate=1 June 2015}}&lt;/ref&gt;

==Reception==
In their 2014 book ''Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact'', [[Blaise Cronin]] and Cassidy R. Sugimoto noted that "for impact assessment of book-based fields, bibliometricians need a database with large numbers of books" and that while the Book Citation Index did meet this need, [[Google Books]] also fulfilled this purpose and was not only free, but was (at the time) more comprehensive for bibliometric analyses.&lt;ref&gt;{{cite book|last1=Cronin|first1=Blaise|last2=Sugimoto|first2=Cassidy R.|title=Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact|date=2014|publisher=MIT Press|isbn=9780262323291|pages=33, 289, 296|url=https://books.google.com/books?id=xxSaAwAAQBAJ&amp;pg=PA296&amp;dq=%22Book+Citation+Index%22&amp;hl=en&amp;sa=X&amp;ei=Ig9sVZ2QB4rLsASwyYHYBw&amp;ved=0CE8Q6AEwBQ#v=onepage&amp;q=%22Book%20Citation%20Index%22&amp;f=false|accessdate=1 June 2015}}&lt;/ref&gt; A 2013 article in the ''[[Journal of the American Society for Information Science and Technology]]'' remarked on the index's opportunities and limitations. It stated that the "most significant limitations to this potential application are the high share of publications without address information, the inflation of publication counts, the lack of cumulative citation counts from different hierarchical levels, and inconsistency in citation counts between the cited reference search and the book citation index."&lt;ref name=journal&gt;{{cite journal|last1=Gorraiz|first1=Juan|last2=Purnell|first2=Philip J.|last3=Gl&#228;nze|first3=Wolfgang|title=Opportunities for and limitations of the Book Citation Index|journal=Journal of the American Society for Information Science and Technology|date=July 2013|volume=64|issue=7|pages=1388&#8211;1398|doi=10.1002/asi.22875|url=http://onlinelibrary.wiley.com/doi/10.1002/asi.22875/full|accessdate=1 June 2015}}&lt;/ref&gt; They also stated that the Book Citation Index was "a first step toward creating a reliable and necessary citation data source for monographs &#8212; a very challenging issue, because, unlike journals and conference proceedings, books have specific requirements, and several problems emerge not only in the context of subject classification, but also in their role as cited publications and in citing publications."&lt;ref name=journal /&gt;

==Further reading==
*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Robinson-Garcia|first2=Nicolas|last3=Miguel Campanario|first3=Juan|last4=Emilio|first4=Delgado L&#243;pez-C&#243;zar|title=Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index|journal=Online Information Review|date=January 2014|volume=38|issue=1|pages=24&#8211;42|doi=10.1108/OIR-10-2012-0169|url=http://www.emeraldinsight.com.ezproxy2.library.drexel.edu/doi/full/10.1108/OIR-10-2012-0169|accessdate=1 June 2015}}
*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Rodriguez-S&#225;nchez|first2=Rosa|last3=Robinson-Garcia|first3=Nicolas|last4=Fdez-Valdivia|first4=J|last5=Garc&#237;a|first5=J.A.|title=Mapping Citation Patterns of Book Chapters in the Book Citation Index|journal=Journal of Informetrics|date=February 2013|volume=7|issue=2|pages=412&#8211;424|doi=10.1016/j.joi.2013.01.004|arxiv=1302.5544}}

==References==
{{reflist}}

==External links==
*{{official website|http://wokinfo.com/products_tools/multidisciplinary/bookcitationindex/}}

[[Category:Bibliographic databases and indexes]]
[[Category:Full text scholarly online databases]]
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Books]]</text>
      <sha1>tb27nwa6hqsbvgju8ab29s5z2ikh288</sha1>
    </revision>
  </page>
  <page>
    <title>Thomas Register</title>
    <ns>0</ns>
    <id>4755498</id>
    <revision>
      <id>747139425</id>
      <parentid>746816027</parentid>
      <timestamp>2016-10-31T17:42:04Z</timestamp>
      <contributor>
        <username>Natg 19</username>
        <id>3492060</id>
      </contributor>
      <minor />
      <comment>Disambiguating links to [[CAD]] (link changed to [[Computer-aided design]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4166" xml:space="preserve">[[Image:ThomasRegister.png|frame|1905 ''Thomas' Register of American Manufacturers'']]
The '''''Thomas Register of American Manufacturers''''', now '''ThomasNet''', is an online platform for supplier discovery and product sourcing in the USA and Canada. It was once known  as the "big green books" and "Thomas Registry", and was a multi-volume [[Yellow Pages|directory]] of [[Industry|industrial]] product information covering 650,000 [[distributors]], [[manufacturers]] and service companies within 67,000-plus [[industry|industrial]] categories that is now published on ThomasNet.

==History==
The books were first published in 1898 by Harvey Mark Thomas as ''Hardware and Kindred Trades. ''In their heyday, '''''Thomas Register of American Manufacturers ''''' was a 34-volume, 3 section buying guide offering sourcing information on industrial products and services, along with comprehensive specifications and detailed product information from thousands of manufacturers. The Thomas Regional Directory Company began as a division of Thomas Publishing in 1976. Thomas Regional Regional Industrial Buying Guides provided information in print and on CD-ROM,  on local OEMs, distributors, MRO services and other custom manufacturing services in 19 regional editions covering much of the United States. Thomas Register and Thomas Regional were available online from the mid 1990s. The company stopped publishing its print products in 2006.

Thomas has moved its database [[online]] as ThomasNet, published and maintained by Thomas Industrial Network, one of Thomas&#8217; five business units. ThomasNet has expanded to provide not only product and company information, but also [[Online shopping|online catalog]]s, [[computer-aided design]] ([[Computer-aided design|CAD]]) drawings, [[news]], [[press releases]] and [[blogs]].

==Thomas Publishing Company, LLC==

Thomas Publishing Company, LLC of [[New York City]] has been [[privately held]] since its inception. It used independent representatives to sell advertising space around its listings in print products like the Thomas Register and the Thomas Industrial Regional Directories, and these representatives continue to sell Internet related products to manufacturers, distributors, and other companies.

==ThomasNet==

ThomasNet is an information and technology company based in New York City. In April 2006 the [[New York Public Library]] named ThomasNet.com as one of its [http://www.nypl.org/branch/books/index2.cfm?ListID=300 25 Best of Reference] sources for the [[reference librarian]], and is currently listed in their [http://www.nypl.org/weblinks/1382 Best of the Web] list for Industry Information.

Since November 2010, ThomasNet has been a founding partner of GlobalTrade.net, a marketplace for international trade service providers.

==ThomasNet News==
ThomasNet News is a product of Thomas Publishing Company, LLC. ThomasNet News was introduced with &#8220;the mission of delivering timely, new industrial product information covering the whole range of products &#8230;&#8221; It manually reviews press releases submitted through the website and publishes with a small description in one of 51 different categories.

In 2000, ThomasNet News released Industry Market Trends (IMT), its first Journal. In the IMT, editors published editorials, interviews, and long form journalism on issues ranging from career skills, developments in the industry, and discussions with leading experts. Soon after, IMT Green &amp; Clean was launched in response to the growing interest in green technology and its impact on the world. In 2011, the IMT Machining Journal was launched followed by the IMT Fluid &amp; Gas Flow Journal, the IMT Career Journal, and the IMT Procurement Journal.

==Research==
Starting in 2010, ThomasNet began reaching out to its database of manufacturers to get a better understanding of where the community was, where their shortcomings were, and where they saw the landscape going in the future. This yearly survey is called the Industry Market Barometer.

== External links ==
* {{Official website}}

[[Category:Promotion and marketing communications]]
[[Category:Marketing books]]
[[Category:Directories]]</text>
      <sha1>c4ijqvmsho8dbo2llhgmqf2x87dw2zk</sha1>
    </revision>
  </page>
  <page>
    <title>The Milepost</title>
    <ns>0</ns>
    <id>240436</id>
    <revision>
      <id>741737604</id>
      <parentid>701856976</parentid>
      <timestamp>2016-09-29T10:19:33Z</timestamp>
      <contributor>
        <username>Hugo999</username>
        <id>3006008</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2777" xml:space="preserve">{{italic title}}
[[Image:49MilePost.gif|right|thumb|The original 1949 Milepost]]'''''The Milepost''''' is an extensive [[guide book]] covering [[Alaska]], the [[Yukon]], the [[Northwest Territories]], and [[British Columbia]].  It was first published in 1949 as a guide about traveling along the [[Alaska Highway]], often locally referred to as "The ALCAN".&lt;ref name="morris"&gt;[http://morris.com/divisions/mcc_magazines/the_milepost.shtml ''The MILEPOST''] from the website of  [[Morris Communications]]&lt;/ref&gt;  It has since expanded to cover all major highways in the northwest corner of [[North America]], including the [[Alaska Marine Highway]].  It is updated annually.

==History==
&lt;!-- Deleted image removed: [[Image:Milepost2008cover.jpg|right|thumb|The 2008 edition&lt;br /&gt;{{deletable image-caption|Sunday, 10 February 2013}}]] --&gt;''The Milepost'' is packaged and distributed like a [[book]]  (2008 edition: ISBN 978-189215431-6), but like the [[Yellow Pages]] it includes paid [[advertising]].&lt;ref&gt;[http://www.themilepost.com/media_kit/testimonials.shtml Testimonials from Advertisers from ''The MILEPOST'' website]&lt;/ref&gt; The original 1949 edition was a mere 72 pages, by 2014 it had expanded to 752 pages, detailing every place a traveler might eat, sleep, or just pull off the road for a moment on all of the highways of northwestern North America. In addition to the paid ads, descriptions are provided of interesting hikes or side trip drives near the highways, campgrounds and other public facilities, as well as short histories of most of the settlements on the highways. Newer additions include special sections on selected areas popular with tourists, such as the [[Kenai Peninsula]]. It is also exhaustively cross-indexed and maps and charts are provided so that travelers can determine the total driving distance between any two points covered by the guide.&lt;ref&gt;http://milepost.com/index.php?option=com_content&amp;task=view&amp;id=71&amp;Itemid=62&lt;/ref&gt;

==Publishing==
Since 1997 ''The Milepost'' has been published by [[Morris Communications]] and currently shares publishing offices with [[Alaska magazine|''Alaska'' magazine]].&lt;ref name="morris" /&gt; Beginning in 2009, The Milepost is also available in an interactive digital format or download.&lt;ref&gt;[http://milepost.com/images/media_kit/mp_mediakit_09_email_lr.pdf The Milepost media kit]&lt;/ref&gt;

==References==
&lt;references /&gt;

==External links==
* {{Official website|http://www.themilepost.com}}

{{Morris Communications}}

{{DEFAULTSORT:Milepost, The}}
[[Category:1949 establishments in Alaska]]
[[Category:1949 books]]
[[Category:Books about Alaska]]
[[Category:Directories]]
[[Category:Morris Communications]]
[[Category:Publications established in 1949]]
[[Category:Roads in Alaska]]
[[Category:Travel guide books]]</text>
      <sha1>amuxr3orh5wa353ljbymrnp1f9elawn</sha1>
    </revision>
  </page>
  <page>
    <title>Gallia Christiana</title>
    <ns>0</ns>
    <id>13968535</id>
    <revision>
      <id>541072231</id>
      <parentid>512863654</parentid>
      <timestamp>2013-02-27T23:07:47Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q748022]] ([[User talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8073" xml:space="preserve">The '''''Gallia Christiana''''', a type of work of which there have been several editions, is a documentary catalogue or list, with brief historical notices, of all the Catholic dioceses and abbeys of France from the earliest times, also of their occupants.

== First efforts ==

In 1621 [[Jean Chenu]], an ''[[avocat]]'' at the [[Parlement of Paris]], published ''Archiepiscoporum et episcoporum Galli&#230; chronologica historia''. Nearly a third of the bishops are missing, and the episcopal succession as given by Chenu was very incomplete. In 1626, Claude Robert, a priest of [[Langres]], published with the approbation of [[Baronius]], a ''Gallia Christiana''. He entered a large number of churches outside of [[Gaul]], and gave a short history of the [[metropolitan see]]s, cathedrals, and abbeys.

== The Samarthani ==

Two brothers de Sainte-Marthe, Sc&#233;vole (1571&#8211;1650) and Louis (1571&#8211;1656), appointed royal historiographers of France in 1620, had assisted Chenu and Robert. At the [[assembly of the French Clergy]] in 1626, a number of prelates commissioned these brothers to compile a more definitive work. They died before the completion of their work, and it was issued in 1656 by the sons of [[Sc&#233;vole de Sainte-Marthe]], [[Pierre de Sainte-Marthe]] (1618&#8211;90), himself historiographer of France, [[Abel de Sainte-Marthe]] (1620&#8211;71), theologian, and later general of the [[Oratory (worship)|Oratory]], and [[Nicolas-Charles de Sainte-Marthe]] (1623&#8211;62), prior of [[Claunay]]. On 13 September 1656, the Sainte-Marthe brothers were presented to the assembly of the French Clergy, who accepted the dedication of the work on condition that a passage suspected of [[Jansenism]] be suppressed. The work formed four volumes [[in folio]], the first for the [[archdiocese]]s, the second and third for the dioceses, and the fourth for the abbeys, all in alphabetical order.&lt;ref&gt;The title was ''Gallia Christiana, qua series omnia archiepiscoporum, episcoporum et abbatum Franci&#230; vicinarumque ditionum ab origine ecclesiarum ad nostra tempora per quattor tomos deducitur, et probator ex antiqu&#230; fidei manuscriptis Vaticani, regnum, principum tabulariis omnium Galli&#230; cathedralium et abbatarium''.&lt;/ref&gt; It reproduced a large number of manuscripts. Defects and omissions, however, were obvious. The Sainte-Marthe brothers themselves announced in their preface the early appearance of a second edition corrected and enlarged. 

As early as 1660 the Jesuit [[Jean Colomb]] published at Lyons the ''Noctes Blancalandan&#230;'', which contains certain additions to the work of the Samarthani, as the brothers and their successors are often called. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud." The edition promised by the Sainte-Marthe brothers did not appear.

== Revision by the Maurists ==

In 1710 the Assembly of the French Clergy offered four thousand livres to [[Denys de Sainte-Marthe]] (1650&#8211;1725), a Benedictine monk of the [[Congregation of Saint-Maur]], renowned for his polemics against the Trappist [[Abb&#233; de Ranc&#233;]] on the subject of monastic studies, on condition that he should bring the revision of the ''Gallia Christiana'' to a successful conclusion, that the first volume should appear at the end of four years, and that his Congregation should continue the undertaking after his death. Through his efforts the first volume appeared in 1715, devoted to the ecclesiastical provinces of Albi, Aix, Arles, Avignon, and Auch. In 1720 he produced the second volume dealing with the provinces of Bourges and Bordeaux; and in 1725 the third, which treated Cambrai, Cologne, and Embrun. 

After his death the Benedictines issued the fourth volume (1728) on Lyons, and the fifth volume (1731) on Mechelen and Mainz. Between 1731 and 1740, on account of the controversies over the Bull ''[[Unigenitus]]'', Dom [[F&#233;lix Hodin]] and Dom [[Etienne Brice]], who were preparing the latter volumes of the ''Gallia Christiana'', were expelled from [[Saint-Germain-des-Pr&#233;s]]. They returned to Paris in 1739 and issued the sixth volume, dealing with Narbonne, also (1744) the seventh and eighth volumes on Paris and its [[suffragan see]]s. [[P&#232;re Duplessis]] united his efforts with theirs, and the ninth and tenth volumes, both on the [[province of Reims]], appeared in 1751. The eleventh volume (1759) dealing with the [[province of Rouen]] was issued by P&#232;re [[Pierre Henri]] and Dom [[Jacques Taschereau]]. In 1770 the twelfth volume on the [[province of Sens]] and [[province of Tarentaise]] appeared, and in 1785 the thirteenth, on the provinces of Toulouse and Trier. 

At the outbreak of the revolution, four volumes were lacking: Tours, Besan&#231;on, Utrecht, and Vienne. Barth&#233;lemy Haur&#233;au published (in 1856, 1860 and 1865), for the provinces of Tours, Besan&#231;on and Vienne, respectively, and according to the Benedictine method, the fourteenth, fifteenth and sixteenth volumes of the ''Gallia Christiana''. 

The province of Utrecht alone has no place in this great collection, but this defect has been remedied in part by the ''Bullarium Trajectense'', edited by [[Gisbert Brom]], and extending from the earliest times to 1378 (The Hague, 1891&#8211;96). 

The new ''Gallia Christiana'', of which volumes I to V and XI to XIII were reprinted by Dom [[Paul Piolin]] between 1870 and 1877, and volumes VI to IX and XII by the publisher H. Welter, places after each metropolitan see its suffragan sees, and after each see the abbeys belonging to it. The original documents, instead of encumbering the body of the articles, are inserted at the end of each diocese under in a section titled ''Instrumenta''. This colossal work does great honour to the Benedictines and to the Sainte-Marthe family. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud."

== Later works ==

In 1774 the Abb&#233; [[Hugues du Temps]], vicar-general of Bordeaux, undertook in seven volumes an abridgement of the ''Gallia'' under the title "Le clerg&#233; de France" of which only four volumes appeared. About 1867 [[Honor&#233; Fisquet]] undertook the publication of an episcopal history of France ([http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF34044240.htm]''La France Pontificale''), in which, for the early period, he utilized the ''Gallia'', at the same time bringing the history of each diocese down to modern times. Twenty-two volumes appeared. 

[[Canon Alban&#232;s]] projected a complete revision of the ''Gallia Christiana'', each ecclesiastical province to form a volume. Alban&#232;s, who was one of the first scholars to search the Lateran and Vatican libraries, in his efforts to determine the initial years of some episcopal reigns, found occasionally either the acts of election or the Bulls of provision. He hoped in this way to remove certain suppositious bishops who had been introduced to fill gaps in the catalogues, but died in 1897 before the first volume appeared. Through the use of his notes and the efforts of Canon [[Ulysse Chevalier]] three addition volumes of this "Gallia Christiana (novissima)", treating Arles, Aix, and Marseilles, appeared at Montb&#233;liard.

== See also ==
* [[Jean-Barth&#233;lemy Haur&#233;au]]

== References ==

&lt;references/&gt;
* [[Dreux du Radier]], ''Biblioth&#232;que historique et critique du Poitou'' (Paris, 1754)
* ''Gallia Christiana'', Vol. IV, Pr&#233;face
* ''Gallia Christiana (novissima)'' (Montb&#233;liard, 1899), Pr&#233;face to the Aix volume
* [[de Longuemare]], ''Une famille d'auteurs aux seizi&#232;me, dix-septi&#232;me et dix-huiti&#232;me si&#232;cles; les Sainte-Marthe'' (Paris, 1902)
* Victor Fouque, ''Du "Gallia christiana" et de ses auteurs: &#233;tude bibliographique'', Paris: E. Tross, 1857. Available on the Biblioth&#232;que nationale's [http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF30453708.htm ''Gallica''] site.

== External links ==
* {{CathEncy|url=http://www.newadvent.org/cathen/06350c.htm|title=Gallia Christiana}}

{{Catholic|wstitle=Gallia Christiana}}

[[Category:Directories]]
[[Category:Religious studies books]]</text>
      <sha1>tnjjbimhvrx7697xu1g90eqtmmx3r0s</sha1>
    </revision>
  </page>
  <page>
    <title>Writer's Market</title>
    <ns>0</ns>
    <id>18980436</id>
    <revision>
      <id>754382524</id>
      <parentid>747550884</parentid>
      <timestamp>2016-12-12T10:28:30Z</timestamp>
      <contributor>
        <username>Hugo999</username>
        <id>3006008</id>
      </contributor>
      <comment>added [[Category:American literary agencies]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2478" xml:space="preserve">{{italic title}}
{{Advert|date=March 2011}}
'''''Writer's Market'' (''WM'')''' is an annual resource book for writers who wish to sell their work. The publication is released by ''[[Writer's Digest]]'' Books (an imprint of [[F+W Media]]) and usually hits bookstores around June of each year. ''Writer's Market'' was first published in 1921, and is often called "The Bible for writers" or "the freelancer's Bible."&lt;ref&gt;http://search.barnesandnoble.com/Writers-Market-2008/Robert-Lee-Brewer/e/9781582974965&lt;/ref&gt;&lt;ref&gt;http://www.epinions.com/review/Book_Writers_Market_2007/content_298510028420&lt;/ref&gt;&lt;ref&gt;http://www.thegoodwebguide.co.uk/index.php?rid=000467&lt;/ref&gt;

The most current edition is the 2016 edition; the current editor is Robert Lee Brewer.

== Listings ==
For 89 years, the book has listed thousands of markets for writers who wish to sell their work. Said markets include magazines, newspapers, theaters (for stage plays), production companies, contests of all types, greeting card companies, literary agents, and more. Each listing has detailed instructions on how to submit work, relevant contact information, as well as what work each listing seeks.

== Articles ==
The upfront section of ''WM'' has more than a dozen articles on writing topics, such as starting a freelancing business, syndication, freelancing for magazines, and a chart filled with typical payment rates concerning various writing assignments.

== "Market Books" ==
''Writer's Market'' is one of nine "[[Market (economics)|market]] books" published each year by [[Writer's Digest Books]]. Others include: ''Guide to Literary Agents'', ''Photographer's Market'', ''Children's Writer's &amp; Illustrator's Market'', ''Novel &amp; Short Story Writer's Market'', ''Artist and Graphic Designer's Market'', ''Poet's Market'', ''Screenwriter's &amp; Playwright's Market'' and ''Songwriter's Market''. Each book is designed to give writers instructions on how to submit freelance work to markets.

== See also ==
* [[Publishing]]
* ''[[Writer's Digest]]''
* [[literary agent]]
* [[Literary agent#Querying|query]]
* [[screenplay]]
* [[royalties]]
* [[Authors Guild]]
* [[poetry]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://www.writersmarket.com|The book's official website}}
*[http://www.writersdigest.com ''Writer's Digest'' magazine official site]

[[Category:Directories]]
[[Category:Literary agencies|.]]
[[Category:Literary agents|.]]
[[Category:American literary agencies]]</text>
      <sha1>gvj3jffjwwyhthbqxa3cf1yvlqcrqvq</sha1>
    </revision>
  </page>
  <page>
    <title>Annuario Pontificio</title>
    <ns>0</ns>
    <id>2940988</id>
    <revision>
      <id>757928967</id>
      <parentid>741305588</parentid>
      <timestamp>2017-01-02T14:58:13Z</timestamp>
      <contributor>
        <username>Neddyseagoon</username>
        <id>883252</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6736" xml:space="preserve">{{italic title}}
{{Refimprove|date=April 2010}}
{{Infobox book
| name = Annuario Pontificio 
| title_orig =
| translator =
| image = Annuario Pontificio 2008 (MK).jpg
| caption =
| author = [[Libreria Editrice Vaticana]], Secretary of State
| illustrator =
| cover_artist =
| country = [[Vatican City|Vatican]]
| language = Italian
| series =
| subject =
| genre = [[Annual publication]], [[Reference]]
| publisher = [[Holy See]]
| pub_date = December 2, 2014
| media_type = Printed Book
| pages = 
| isbn = 9788820997472
| isbn_note = &lt;ref&gt;[http://www.libreriaeditricevaticana.va/content/libreriaeditricevaticana/it/news-ed-eventi/annuario-pontificio-2016.html 2016]&lt;/ref&gt;
| oclc= 
| preceded_by = Annuario Pontificio 2015
| followed_by = Annuario Pontificio 2016
}}
The '''''Annuario Pontificio''''' ([[Italian language|Italian]] for ''Pontifical Yearbook'') is the annual directory of the [[Holy See]]. It [[List of popes|lists all the popes]] to date and all officials of the Holy See's [[dicastery|departments]]. It also gives complete lists, with contact information, of the [[Cardinal (Catholicism)|cardinals]] and [[Catholic Church|Catholic]] bishops throughout the world, the [[diocese]]s (with statistics about each), the departments of the [[Roman Curia]], the Holy See's [[diplomatic mission]]s abroad, the [[embassy|embassies]] accredited to the Holy See, the headquarters of [[religious institute]]s (again with statistics on each), certain academic institutions, and other similar information. The index includes, along with all the names in the body of the book, those of all priests who have been granted the title of "[[Monsignor]]".
As the title suggests, the red-covered yearbook, compiled by the Central Statistics Office of the Church and published by [[Libreria Editrice Vaticana]], is mostly in Italian.

The 2015 edition has more than 2,400 pages and costs {{&#8364;|78}}.&lt;ref&gt;{{cite web |url=http://www.vaticanum.com/en/annuario-pontificio-2015-book-2|access-date=April 5, 2016|title=Annuario Pontificio 2015|publisher=Citt&#224; del Vaticano}}&lt;/ref&gt; According to the ''Pontifical Yearbook of 2010'', the number of Catholics in the world increased from 1,147 million to 1,166 million between 2007 and 2008, a growth of 1.7 percent.&lt;ref&gt;{{cite web| url=http://www.zenit.org/article-28425?l=english |title=Number of Catholics Increases Worldwide: 2010 "Annuario" Shows Growth in Asia and Africa |publisher=Zenit |date=February 21, 2010 |accessdate=April 11, 2010}}&lt;/ref&gt; By the ''Yearbook of 2016'' it was 1,272,281,000 at the end of 2014.

==History==
A [[yearbook]] of the Catholic Church was published, with some interruptions, from 1716 to 1859 by the Cracas printing firm in Rome, under the title (in Italian) ''Information for the Year ...'' From 1851, a department of the Holy See began producing a different publication called (in Italian) ''Hierarchy of the Holy Catholic Apostolic Church Worldwide and in Every Rite, with historical notes'', which took the title ''Annuario Pontificio'' in 1860, but ceased publication in 1870. This was the first yearbook published by the Holy See itself, but its compilation was entrusted to the newspaper ''[[Giornale di Roma]]''. The publishers "Fratelli Monaldi" (Monaldi Brothers) began in 1872 to produce their own yearbook entitled (in Italian) ''The Catholic Hierarchy and the Papal Household for the Year ... with an appendix of other information concerning the Holy See''.

The [[Holy See Press Office|Vatican Press]] took this over in 1885, thus making it a semi-official publication.  It bore the indication "official publication" from 1899 to 1904, but this ceased when, giving the word "official" a more restricted sense, the ''Acta Sanctae Sedis'', forerunner of the ''[[Acta Apostolicae Sedis]]'', was declared the only "official" publication of the Holy See. In 1912, it resumed the title ''Annuario Pontificio''. From 1912 to 1924, it included not only lists of names, but also brief illustrative notes on departments of the Roman Curia and on certain posts within the [[papal court]], a practice to which it returned in 1940.

For some years, beginning in 1898, the ''Maison de [[la Bonne Presse]]'' publishing house of [[Paris]] produced a similar yearbook in [[French language|French]] called ''Annuaire Pontifical Catholique'', not compiled by the Holy See. This contained much additional information, such as detailed historical articles on the [[Swiss Guards]] and the [[Apostolic Palace|Papal Palace]] at the [[Vatican City|Vatican]].

== Statistical data ==
According to the ''Annuario Pontificio 2012'' the statistical data given in the yearbook regarding [[archdiocese]]s and [[diocese]]s are furnished by the diocesan curias concerned and reflect the diocesan situation on 31 December of the year prior to the date on the yearbook, unless there is another indication.  The data recorded are shown in the following order next to these abbreviations:
* Su &#8211; area in square kilometers of the diocesan territory
* pp &#8211; population of the diocese
* ct &#8211; number of Catholics
* pr &#8211; parishes and quasi-parishes
* ch &#8211; churches or mission stations
* sd &#8211; secular priests resident in the diocese
* dn &#8211; diocesan priests ordained during the year
* sr &#8211; religious priests resident in the diocese
* rn &#8211; religious priests ordained during the year
* dp &#8211; permanent deacons
* sm &#8211; seminarians taking courses of philosophy and theology
* rm &#8211; members of men's religious institutes
* rf &#8211; members of women's religious institutes
* ie &#8211; educational institutes
* ib &#8211; charitable institutes
* ba &#8211; baptisms

== See also ==
{{Portal|Catholicism}}
* [[Catholic Church by country]]
* [[History of the papacy]]
* [[Oldest popes]]
* [[Vatican Publishing House]]

==References==
{{reflist}}

==Bibliography==
* Secretary of State, ''Annuario Pontificio 2010.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8355-0
* Secretary of State, ''Annuario Pontificio 2009.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8191-4
* Secretary of State, ''Annuario Pontificio 2008.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8021-4
* Secretary of State, ''Annuario Pontificio 2007.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7908-9
* Secretary of State, ''Annuario Pontificio 2006.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7806-8
* Secretary of State, ''Annuario Pontificio 2005.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7678-1

==External links==
* [http://www.catholic-hierarchy.org/ CatholicHierarchy.org]
* [http://www.gcatholic.org/ GCatholic.org]

[[Category:Documents of the Catholic Church]]
[[Category:Directories]]
[[Category:Holy See]]</text>
      <sha1>9jugi6bcz39ih394jgzyhsiwuixf1r3</sha1>
    </revision>
  </page>
  <page>
    <title>Blue pages</title>
    <ns>0</ns>
    <id>5507437</id>
    <revision>
      <id>762242480</id>
      <parentid>762242454</parentid>
      <timestamp>2017-01-27T15:44:47Z</timestamp>
      <contributor>
        <username>Serols</username>
        <id>9929111</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/2405:205:3080:447D:0:0:1D04:60AD|2405:205:3080:447D:0:0:1D04:60AD]] ([[User talk:2405:205:3080:447D:0:0:1D04:60AD|talk]]) ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1710" xml:space="preserve">{{Unreferenced|date=December 2009}}
'''Blue pages''' are a [[telephone directory]] listing of American and Canadian state agencies,  [[government]] agencies, federal government and other official entities, along with specific offices, departments, or bureaus located wherein.

==Canada==
Canadian yellow-page listings currently indicate "Government Of Canada-See Government Listings In The Blue Pages"; in markets where the local telephone directory is a single volume, the blue pages and community information normally appear after the alphabetical white-page listings but before the yellow pages advertising. The blue page listings include both provincial and federal entities.{{cn|date=April 2013}}

==United States==
In the [[United States]], the blue pages included state, federal, and local offices, including [[service district]]s such as school districts, port authorities, public utility providers, parks districts, fire districts, and the like. The blue pages also provided information about government services, in addition to officials' names, addresses, telephone numbers, and other contact information. The color blue is likely derived from so-called government blue books, official publications printed by a government (such as that of a state) describing its organization, and providing a list of contact information. (The blue pages published in a printed telephone directory is usually quite abridged, compared to official blue books).

==Other==
The name "blue pages" has been used for various specialised directories by private-sector entities such as the internal IBM Staff directory. 

{{DEFAULTSORT:Blue Pages}}
[[Category:Telephone numbers]]
[[Category:Directories]]

{{telephony-stub}}</text>
      <sha1>ciadjip2h3orid6lpxi2e95c1juk2s2</sha1>
    </revision>
  </page>
  <page>
    <title>Dalilmasr</title>
    <ns>0</ns>
    <id>27707778</id>
    <revision>
      <id>601840646</id>
      <parentid>597578763</parentid>
      <timestamp>2014-03-29T17:54:01Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10067)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1576" xml:space="preserve">{{orphan|date=August 2010}}
'''Dalilmasr''' is an online [[Egyptian language|Egyptian]] directory, which provides information about any service, product, and/or tool available in the Egyptian market. The website includes information about companies, shops, showrooms, and service providers in the Egyptian Region.

In the five main languages it serves more than two billion people.
&lt;br&gt;'''Logo Identity:'''The logo is in two palms of the hand applauding (conducting the valued promised service) in black and red color along with white background (Egyptian Flag Colors), the other right and left acute blue triangles meaning the Nile River welfare.
&lt;ref&gt;[[Al-Ahram|Al-Ahram newspaper]] in large two columns, says Dalilmasr is the first Egyptian search engine focused on contents rather only information and care about all Egyptian cities &amp; suburbs. Issued on June 15, 2010 - page 22.&lt;/ref&gt;
&lt;ref&gt;[[Akhbar El Yom]] newspaper considered Dalilmasr as best Egyptian directory, appendix 3-page 6 on June 12, 2010&lt;/ref&gt;
&lt;ref&gt;[http://www.minia.edu.eg/doHtml.aspx?page=useful-sites.html Minia University's] directory proposed Dalilmasr as one of the recommended Egyptian websites.
&lt;/ref&gt;
&lt;ref&gt;Wikiwak indexed Dalilmasr since website domain was dalil-masr.com&lt;/ref&gt;
&lt;ref&gt;Google rank it on the top for best Egyptian directory keyword [http://www.google.com.eg/search?hl=en&amp;q=best+egyptian+directory&amp;aq=0&amp;aqi=g3&amp;aql=&amp;oq=best+egyptian+d&amp;gs_rfai= Search Result]&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://www.dalilmasr.com Dalimasr website]

[[Category:Directories]]</text>
      <sha1>ki2lmbzrze6vq6t4gqlfrftzifikg7w</sha1>
    </revision>
  </page>
  <page>
    <title>Navy List</title>
    <ns>0</ns>
    <id>47170</id>
    <revision>
      <id>739831965</id>
      <parentid>739658109</parentid>
      <timestamp>2016-09-17T08:43:24Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>Removed invisible unicode characters + other fixes, removed: &#8234; (4) using [[Project:AWB|AWB]] (12084)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2674" xml:space="preserve">{{for|a list of countries with navies|List of navies}}
{{multiple issues|
{{more footnotes|date = March 2013}}
{{Globalize|date=May 2009}}
}}

A '''Navy List''' or '''Naval Register''' is an official list of [[navy|naval]] officers, their ranks and seniority, the ships which they command or to which they are appointed, etc., that is published by the government or naval authorities of a country.

==Background==
The Navy List fulfills an important function in [[international law]] in that warships are required by article 29 of the [[United Nations Convention on the Law of the Sea]] to be commanded by a [[commissioned officer]] whose name appears in the appropriate service list.{{why|date=June 2016}}

Past copies of the Navy List are also important sources of information for historians and genealogists.

The Navy List for the Royal Navy is no longer published in hard-copy.

The [[Royal Navy]] (United Kingdom) publishes annual lists of active and reserve officers, and biennial lists of retired officers. The equivalent in the [[United States Navy]] is the Naval Register, which is updated online on a continuous basis.  When a ship is removed from the [[Naval Vessel Register]] in the United States, or from a Naval List of any other country, the ship is said to be "[[:wikt:stricken|stricken]]".&lt;ref&gt;Edwards, Paul.  ''[https://books.google.com/books?id=OydzBgAAQBAJ&amp;pg=PA37 Small United States and United Nations Warships in the Korean War]'', p. 37 (McFarland, 2008).&lt;/ref&gt;

== Resources ==
Good sources of historical data on UK's Navy Lists are
*The Naval Historical Branch, Portsmouth Naval Base.
*The Central Library Portsmouth, Guildhall Square.
*[[The National Archives (United Kingdom)|The National Archives]], Kew, that has an almost complete set including unpublished editions produced during the Second World War for internal use by the Admiralty.
*The Caird Library of the [[National Maritime Museum]] has in its collection bound monthly lists published by the Admiralty, and the concurrently published Steel's lists

The current editor of the Navy List is Cliona Willis

== Bibliography ==

* ''The 1766 Navy List'', Edited by E. C. Coleman, Published by Ancholme Publishing, ISBN 0-9541443-0-9

==See also==
* [[Army List]]
* [[Naval Vessel Register]]

==References==
&lt;references /&gt;

== External links ==
* [http://www.royalnavy.mod.uk/~/media/royal%20navy%20responsive/documents/useful%20resources/navy%20list.pdf Navy List 2013]
* [https://navalregister.bol.navy.mil US Naval Register] (US Navy)
* [http://www.NavyListResearch.co.uk Navy List Research] (Royal Navy)

[[Category:Royal Navy]]
[[Category:United States Navy]]
[[Category:Directories]]</text>
      <sha1>p3h5df2m8oda38jxifwvb2zuodvkjf6</sha1>
    </revision>
  </page>
  <page>
    <title>Clerical Guide or Ecclesiastical Directory</title>
    <ns>0</ns>
    <id>34261059</id>
    <revision>
      <id>748236595</id>
      <parentid>740070229</parentid>
      <timestamp>2016-11-07T04:07:40Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* The Clerical Guide after 1836 */clean up; http&amp;rarr;https for [[Google Books]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5381" xml:space="preserve">{{italic title}}
The '''''Clerical Guide or Ecclesiastical Directory''''' was the earliest ever specialist directory to cover the clergy of the [[Church of England]]. In its initial format it appeared just four times &#8211; in 1817, 1822, 1829 and 1836, under the editorial direction of [[Richard Gilbert (printer)|Richard Gilbert]].

Another edition was actually advertised for 1838,&lt;ref name="paflin"&gt;[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article ''Shop-talk and mordant wit'', by Christopher Currie &amp; Glyn Paflin, describing the background to [[Crockford's Clerical Directory]]'s first hundred editions, 6&#8211;13 December 2007&lt;/ref&gt; but no copies have in fact been found within the main academic libraries.

The title was briefly revived by Thomas Bosworth &amp; Company during the 1880s.

==Contents of the Clerical Guide==

The main alphabetical section of the directory included:

*A list of benefices together with their populations, counties, dioceses and  archdeaconries
*Their incumbents with the year of his institution
*Their values (up to the 1829 edition) in the [[Valor Ecclesiasticus]] or King's Books
*The names of their patrons.
*The 1836 edition additionally gave the income of the benefice during the year 1831, the available capacity or "church room" for the congregation, and the name of any [[impropriator]].

The preliminary pages included:

*Current lists of [[bishops]], members of [[cathedral chapter]], and other dignitaries, showing the values of their [[first fruits]]
*A section on the Doctors of Laws, the [[canon law|canonical]] specialists
*A section on the [[Chapel Royal]] together with the king's preachers and chaplains
*Sections on [[Sion College]] and [[Gresham College]]
*Sections on the two English universities ([[University of Oxford|Oxford]] and [[University of Cambridge|Cambridge]])
*Sections on the fellows and schoolmasters of [[Eton College|Eton]], [[Winchester College|Winchester]], [[Westminster School|Westminster]], [[Harrow School|Harrow]], [[Manchester Grammar School|Manchester]] and [[St Paul's School, London|St Paul's]].

The alphabetical list of benefices was also followed by an alphabetical list of the prelates, dignitaries and beneficed clergy of the Church of England (generally omitting the unbeneficed clergy).

The directories concluded with lists of ecclesiastical patronage, giving the names of those benefices within the gift of the king and also those of the lord chancellor, the chancellor of the duchy of Lancaster, the various archbishops and bishops, and the two universities.

==The publishers==

The 1817 edition stated that it was "printed for [[Rivington (publishers)|J. C. &amp; F. Rivington]], 62 St Paul's Churchyard, by R. &amp; R. Gilbert, St John's Square, [[Clerkenwell]]".  '''Richard Gilbert''' was a printer and an accountant with the [[SPCK]].  Although he appeared in the 1817 edition merely as the "printer" (alongside his brother Robert, who died the following year), he thereafter seems to have taken a more prominent role in its production.  The 1822 edition was "corrected by Richard Gilbert", as though he had been engaged in putting right someone else's mistakes.  He similarly wrote the prefaces for subsequent editions, and the 1836 edition still bore the names "Gilbert and Rivington, printers, St John's Square".

Gilbert, an industrious compiler who was additionally very active in the religious life of Clerkenwell, also produced a pocket-sized '''Clergyman's Almanack''' in 1819 &lt;ref&gt;Oxford Dictionary of National Biography: article on Richard Gilbert&lt;/ref&gt;

==The Clerical Guide after 1836==

The failure of the directory to appear after 1836 left open an opportunity for a rival publication.  This was filled after 1841 by the [[Clergy List]].

After lying dormant for fifty years, the title '''Clerical Guide and Ecclesiastical Directory''' was briefly revived in 1886 by Thomas Bosworth &amp; Company, 65 [[Great Russell Street]]. Once again the volume offered alternative listings of the clergy and the benefices, together with other "valuable information &#8230; from the office of the [[Ecclesiastical Commission (Church of England)|Ecclesiastical Commission]].&lt;ref&gt;The Times newspaper, Thursday, Mar 18, 1886; pg. 12&lt;/ref&gt;  However the relaunched title was very quickly acquired by Hamilton Adams of [[Paternoster Row]], who in 1889 merged it with their other recent acquisition, the aforementioned Clergy List.&lt;ref name="paflin" /&gt;

In the issue for 1918/19 the Clergy List was merged in its turn with [[Crockford's Clerical Directory]]. Thereafter until the 1930s the latter title still continued to advertise on its preliminary pages that it "incorporated the Clergy List", together with the "Clerical Guide and Ecclesiastical Directory".

A microfiche version of the 1829 directory was produced during the 1980s by the [[Society of Genealogists]]. In more recent years scanned copies of the early editions have also appeared on the World Wide Web.&lt;ref&gt;All four editions of the Clerical Guide from 1817-1836 may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]&lt;/&lt;/ref&gt;

==See also==
*[[Clergy of the Church of England database]]

==References==
{{reflist}}

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]</text>
      <sha1>krcngalf53enuh5glmg8joo5320gjaw</sha1>
    </revision>
  </page>
  <page>
    <title>Mobile social address book</title>
    <ns>0</ns>
    <id>20893498</id>
    <revision>
      <id>564116342</id>
      <parentid>532174633</parentid>
      <timestamp>2013-07-13T15:54:34Z</timestamp>
      <contributor>
        <ip>122.162.162.222</ip>
      </contributor>
      <comment>template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3205" xml:space="preserve">
A '''mobile social address book''' is a [[phonebook]] on a [[mobile device]] that enables [[subscribers]] to build and grow their [[social networks]]. The mobile social address book transforms the phone book on any standard mobile phone into a social networking platform that makes it easier for subscribers to exchange contact information.&lt;ref&gt;[http://www.wirelessweek.com/article.aspx?id=163626 Wireless Week, retrieved 2008-12-29]&lt;/ref&gt; The mobile social address book is the convergence of [[personal information management]] (PIM) and social networking on a mobile device. While standard mobile phonebooks force users to manually enter contacts, mobile social address books automate this process by enabling subscribers to exchange contact information following a call or SMS.&lt;ref&gt;[http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9115165 Computerworld, retrieved 2008-11-5]&lt;/ref&gt; The contact information exchange occurs instantaneously and the user&#8217;s phonebook updates automatically. Mobile social address books also provide dynamic updates of contacts if their numbers change over time.

== History ==
Mobile social address books began appearing in 2007 as a parallel social trend to the emergence of Internet-based social networking sites like [[Facebook]], [[MySpace]] and [[LinkedIn]], establishing a new paradigm for interpersonal contact and communication. Mobile social address books sought to bring the connectivity of social networking to the in-the-moment experience of the mobile phone. Users can easily exchange contact information regardless of their handset, mobile carrier, or social networking application they use.&lt;ref&gt;[http://latestgeeknews.blogspot.com/2008/02/social-address-booknext-killer-app-part.html Latest Geek News, retrieved 2008-11-5]&lt;/ref&gt;

Examples of emerging companies providing technology to support mobile social address books include: [[PicDial]] (which dynamically augments the existing address book with pictures and status from Facebook, MySpace and Twitter, integrates with the call screen so during every call you see the latest picture and status of whoever is calling.  It is a network address book so everything can be managed from Windows or Mac as well and lastly you can also set your one callerID picture and status for your friends to see when you call them) FusionOne (whose backup and synchronization solutions lets users easily transfer and update mobile content, including contact information, among different devices); [[Loopt]] (whose Loopt service provides a social compass alerting users when friends are near); OnePIN (whose CallerXchange person-to-person contact exchange service lets users share contact info with one click on the mobile phone); and VoxMobili (whose Phone Backup and Synchronized Address Book solutions let users safeguard and synchronize their contact information among different devices).

== References ==
&lt;references /&gt;

==External links==
* [http://www.loopt.com Loopt website]
* [http://www.onepin.com OnePIN website]
* [http://www.voxmobili.com VoxMobili website]
* [http://www.picdial.com PicDial website]

{{Mobile phones}}

[[Category:Social networks]]
[[Category:Directories]]</text>
      <sha1>dglk0fjixa6sf5l6cbxkf0llej2gjai</sha1>
    </revision>
  </page>
  <page>
    <title>Almanach de Bruxelles (defunct)</title>
    <ns>0</ns>
    <id>36297187</id>
    <revision>
      <id>735155121</id>
      <parentid>693696961</parentid>
      <timestamp>2016-08-18T22:42:34Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Defunct publications of France to [[:Category:Defunct periodicals of France]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 July 21]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1108" xml:space="preserve">The '''''Almanach de Bruxelles''''' is a now defunct [[France|French]] social register that listed [[royal family|royal]] and [[nobility|noble]] [[dynasties]] of [[Europe]]. It was established in 1918 during the [[Second World War]] to compete against the prominent German [[Almanach de Gotha]].&lt;ref&gt;March 17, 1918. [http://query.nytimes.com/mem/archive-free/pdf?res=9E0DEED6143AEF33A25754C1A9659C946996D6CF Almanach de Gotha has a french rival] at ''[[New York Times]]''&lt;/ref&gt;

==See also==
* ''[[Almanach de Gotha]]''

==Sources==
{{reflist}}

==External links==
*[http://www.worldcat.org/title/almanach-de-bruxelles-annuaire-genealogique-historique-heraldique-des-maisons-souverains-princieres-et-ducales/oclc/06083750 ''Almanach de Bruxelles'' (1918-] at [[WorldCat]]

[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Biographical dictionaries]]
[[Category:Defunct periodicals of France]]
[[Category:European nobility]]
[[Category:French royalty]]
[[Category:1918 establishments in France]]
[[Category:Publications established in 1918]]
 

{{royal-bio-book-stub}}
{{bio-dict-stub}}</text>
      <sha1>ageth95tli046g77qrfhlj8p6wgytom</sha1>
    </revision>
  </page>
  <page>
    <title>Slater's Directory</title>
    <ns>0</ns>
    <id>40452652</id>
    <redirect title="Isaac Slater" />
    <revision>
      <id>571631200</id>
      <timestamp>2013-09-05T11:36:49Z</timestamp>
      <contributor>
        <username>M2545</username>
        <id>9455233</id>
      </contributor>
      <comment>[[WP:AES|&#8592;]]Redirected page to [[Isaac Slater]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="51" xml:space="preserve">#REDIRECT[[Isaac Slater]]

[[Category:Directories]]</text>
      <sha1>kp59oxkmh8koqjxlfhdh0fdvxpqz879</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Domain name system</title>
    <ns>14</ns>
    <id>5737409</id>
    <revision>
      <id>604572816</id>
      <parentid>582104800</parentid>
      <timestamp>2014-04-17T09:41:24Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="300" xml:space="preserve">{{Cat main|Domain Name System}}
{{Commonscat|Domain name system}}

[[Category:Internet governance]]
[[Category:Internet Standards]]
[[Category:Internet architecture]]
[[Category:Network addressing]]
[[Category:Application layer protocols]]
[[Category:Directories]]

[[ms:Kategori:Sistem nama domain]]</text>
      <sha1>dhg3yblwc3cq2nm9pyms1samvcvxnim</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Lists</title>
    <ns>14</ns>
    <id>691070</id>
    <revision>
      <id>746120283</id>
      <parentid>712484104</parentid>
      <timestamp>2016-10-25T10:54:13Z</timestamp>
      <contributor>
        <username>Fayenatic london</username>
        <id>1639942</id>
      </contributor>
      <comment>removed [[Category:Contents]]; added [[Category:Wikipedia navigation]] using [[WP:HC|HotCat]] - see [[Wikipedia:Categories_for_discussion/Log/2016_September_23#Category:Contents]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="151" xml:space="preserve">{{Commons category|Information lists}}
{{Category see also|Timelines}}
{{Category diffuse}}

[[Category:Wikipedia navigation]]
[[Category:Directories]]</text>
      <sha1>lnc5pgjqj2xba5ikihq7opuoaqbmh77</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Metadata registry</title>
    <ns>14</ns>
    <id>3675616</id>
    <revision>
      <id>615411220</id>
      <parentid>388532863</parentid>
      <timestamp>2014-07-03T10:30:10Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="196" xml:space="preserve">All Wikipedia articles in this category are either instances of a metadata registry or related to metadata registries.
{{Cat main|Metadata registry}}
[[Category:Metadata]]
[[Category:Directories]]</text>
      <sha1>ggpzcmqqvs6ixe7zrtqsifyqxa6tbqo</sha1>
    </revision>
  </page>
  <page>
    <title>Encyclopedia of Associations</title>
    <ns>0</ns>
    <id>44017130</id>
    <revision>
      <id>722553795</id>
      <parentid>674600626</parentid>
      <timestamp>2016-05-28T22:14:53Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2194" xml:space="preserve">{{Italic title}}
The '''''Encyclopedia of Associations''''' (''EA'') is a comprehensive directory of more than 20,000 [[Voluntary associations|associations]], [[Society|societies]], and other non-profit membership organizations in the United States of America.&lt;ref&gt;[http://www.gale.cengage.com/DirectoryLibrary/GML33507EA%20GDL.pdf Encyclopedia of Associations]&lt;/ref&gt;

Originally titled the ''Encyclopedia of American Associations'', ''EA'' was created by [[Frederick Gale Ruffner, Jr.]] in 1954 while working as a market researcher in [[Detroit, Michigan]].&lt;ref&gt;[http://lj.libraryjournal.com/2014/08/publishing/gale-founder-frederick-ruffner-dies-at-88/#_ "Gale Founder Frederick Ruffner Dies at 88" ''Library Journal''. &#8211; Retrieved October 3, 2014]&lt;/ref&gt;

More than 140 scholarly articles have made use of ''EA''.&lt;ref&gt;[http://www.unc.edu/~fbaum/papers/JSTOR-EA-annotated-bibliography.pdf "An Annotated Bibliography of Articles Using the ''Encyclopedia of Associations''" - Retrieved October 3, 2014.]&lt;/ref&gt;

Past extracts from ''EA'' have included "Organized Obsessions" &lt;ref&gt;[http://lccn.loc.gov/92219621 - Library of Congress LCCN Permalink for 92219621]&lt;/ref&gt; and the "Gale Encyclopedia of Business and Professional Associations".&lt;ref&gt;[http://lccn.loc.gov/95649648 - Library of Congress LCCN Permalink for 95649648]&lt;/ref&gt;

A detailed history of ''EA'' is available in an article in ''Distinguished Classics of Reference Publishing''&lt;ref&gt;[https://archive.org/stream/DistinguishedClassicsOfReferencePublishing#page/n101/mode/2up - Tobin, Carol M. "The Book that Built Gale Research: The ''Encyclopedia of Associations''."  ''Distinguished Classics of Reference Publishing'']&lt;/ref&gt;&lt;ref&gt;[http://lccn.loc.gov/91033629 - Library of Congress LCCN Permalink for 91033629]&lt;/ref&gt;

== See also ==
* [[Gale Research]]
* [[Frederick Gale Ruffner, Jr.]]

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using&lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://gale.cengage.com/ Gale website]

&lt;!--- Categories ---&gt;
[[Category:Directories]]
[[Category:Specialized encyclopedias]]</text>
      <sha1>dxv24kp19m0nqpx5vtn1do4vicbm628</sha1>
    </revision>
  </page>
  <page>
    <title>Search link optimization</title>
    <ns>0</ns>
    <id>23265516</id>
    <revision>
      <id>644601921</id>
      <parentid>590641130</parentid>
      <timestamp>2015-01-28T20:36:07Z</timestamp>
      <contributor>
        <username>Gmodi94</username>
        <id>19457248</id>
      </contributor>
      <minor />
      <comment>/* External links and references */ 
The Link was broken
Deleting a broken link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3292" xml:space="preserve">{{orphan|date =August 2009}}

'''Search Link Optimization''' ('''SLO''') is a process by which internal and external incoming links are optimized for [[search engine]] algorithms to determine the relevance of [[web page]] content.  Relevant [[anchor text]] integration, text that contains keywords for optimizing a web page, is key to this process.

==Inbound links, outbound links, internal links==

Inbound and outbound links are those that hyperlink two independent web pages together whereas inbound links would hyperlink domain &#8220;A&#8221; to domain &#8220;B&#8221; and outbound links would hyperlink domain &#8220;B&#8221; to domain &#8220;A.&#8221;

Inbound and outbound links are essential to web page visibility often enhancing web page relevance, ranking, &amp; placement.  There are few instances where inbound links would be discouraged.  Outbound links however should be given sparingly and should only link material to other material of same or similar relevance.  Often, developers will utilize a [[nofollow]] tag used mostly to further optimize hyperlinks by &#8220;instructing&#8221; search engines not to distribute any [[PageRank]] from the hyperlink.  An example of a nofollow tag might be:

&lt;syntaxhighlight lang="html5"&gt;
&lt;a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia" rel="nofollow"&gt;Wikipedia, Online Encyclopedia&lt;/a&gt;
&lt;/syntaxhighlight&gt;

[[Internal link]]s are those that hyperlink within a single domain. Hyperlinks listed higher within the source code typically gain greater relevance. Some developers use a  practice known as PageRank Sculpting by using the nofollow tag to adjust the flow of PageRank.

==Proper coding of hyperlinks==

The mere presence of hyperlinks within a web page may not yield desired optimization results. For example, when coding a web page about "blue widgets," anchor text containing links referencing "red widgets" may alter relevance which can result in gain/loss ranking scenario where &#8220;blue widgets&#8221; gains while &#8220;red widgets&#8221; actually loses position.  Moreover, another result can be a complete loss of overall web page ranking altogether for either keyword.

A properly coded keyword contains these elements: Relevant anchor text, relevant keyword titling, and compliance-based [[Character encodings in HTML|HTML code]] structures.  Additionally, use of titles that are linked within the [[HTML element|title tag]] is also a recommended practice provided the title tag for the web page has also been properly optimized for the desired keyword(s).  Below is the proper coding of a hyperlink:

&lt;syntaxhighlight lang="html5"&gt;
&lt;a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia"&gt;Wikipedia, Online Encyclopedia&lt;/a&gt;
&lt;/syntaxhighlight&gt;

==External links and references==
The footnotes below are given in support of the statements above. Because some facts are proprietary secrets held by private companies and therefore not documented in journals, such facts are reasoned from facts that are public.
* [http://www.mattcutts.com/blog/pagerank-sculpting/ PageRank Sculpting]
* [http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html Prevent Comment Spam]
* [http://www.textlinks2u.com/search_engine_optimization.html Search Engine Optimization]

{{DEFAULTSORT:Search Link Optimization}}
[[Category:Internet search]]</text>
      <sha1>i3u6jxki6f5x616rihu4elwnlfzoj9l</sha1>
    </revision>
  </page>
  <page>
    <title>Hyper Search</title>
    <ns>0</ns>
    <id>11853249</id>
    <revision>
      <id>558370252</id>
      <parentid>555925186</parentid>
      <timestamp>2013-06-05T00:33:28Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q3787879]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="765" xml:space="preserve">'''Hyper Search''' has been the first{{cn|date=May 2013}} published technique to introduce [[link analysis]] for search engines. It was created by Italian researcher [[Massimo Marchiori]].

==Bibliography==

* [[Massimo Marchiori]], [http://www.w3.org/People/Massimo/papers/WWW6/ "The Quest for Correct Information on the Web: Hyper Search Engines"], ''Proceedings of the Sixth International World Wide Web Conference (WWW6)'', 1997.
* [[Sergey Brin]] and [[Lawrence Page]], [http://www-db.stanford.edu/~backrub/google.html "The anatomy of a large-scale hypertextual Web search engine"], ''Proceedings of the Seventh International World Wide Web Conference (WWW7)'', 1998. 

== See also ==
* [[PageRank]]
* [[Spamdexing]]

[[Category:Internet search]]

{{web-stub}}</text>
      <sha1>3fv2giwphv2iel7kfxk452j1k5g591s</sha1>
    </revision>
  </page>
  <page>
    <title>Real-time web</title>
    <ns>0</ns>
    <id>23231423</id>
    <revision>
      <id>754600281</id>
      <parentid>754406310</parentid>
      <timestamp>2016-12-13T14:46:59Z</timestamp>
      <contributor>
        <ip>2001:B07:6456:7053:4951:B96E:54D1:2317</ip>
      </contributor>
      <comment>/* True-realtime web (an "alternate" model) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6204" xml:space="preserve">{{multiple issues|
{{more footnotes|date=November 2010}}
{{refimprove|date=November 2010}}
{{essay|date=April 2015}}
{{Buzzword|date=July 2011}}xxdxxx
}}

The '''real-time web''' is a network web using technologies and practices that enable users to receive information as soon as it is published by its authors, rather than requiring that they or their software check a source periodically for updates.

==Difference from real-time computing==
The real-time web is fundamentally different from [[real-time computing]] since there is no knowing when, or if, a response will be received. The information types transmitted this way are often short messages, status updates, news alerts, or links to longer documents. The content is often "soft" in that it is based on the [[social web]]&#8212;people's opinions, attitudes, thoughts, and interests&#8212;as opposed to hard news or facts.

==(Old) True-realtime web (an "alternate" model)==
From another point of view, the real-time web consists in making the client interface (or the web side; or the web layer) of a web application, to communicate continuously with the corresponding real-time server, during every user connection. As a fast pic of the client/server model, imagine each client object (each web module of the web [[GUI]] of an application) having its object class alive as a sub process (of its user session) in the server environment. In this scenario, the web is considered as the human entrance (interface) to the real-time environment: at each connected web URL, or Internet real-time zone, corresponds a different "front-end" web application. The real-time server acts as a [[logic network operating system]] for the programmable array of applications; handles the array of connected users for each application; attends for connections from real-world appliances and second level real-time servers. Applications behaviours and the intercommunication procedures between online services or applications, online users, and connected devices or appliances, are settled in the corresponding source code of each real-time service written in the real-time-interpreted programming language of the centric server.

As opposite to previous scenario, real-time web is exactly soft [[real-time computing]]: the round trip of a data ping-pong signal from the real-time server to the client must take about 1s (max) to be considered real-time and not to be annoying for humans (or users) during their connections.{{Citation needed|date=April 2016}} About the dispute between social web and real-time web, we can say real-time web is social by default and it is not true the contrary (WEB-r comes before Web 2.0). The WEB-r model is called [[true-realtime web]] to highlight the differences with the defective (de facto) model of real-time web generally perceived. From the industry point of view, this model of (general) real-time Internet can also be defined as [[electronic web]], that comes with the intrinsic meaning of not being limited to the web side of the Net, and with the direct reference to its server/rest-of-the-world perspective as a mechanism of a single clock.

==History==
Examples of real-time web are Facebook's newsfeed, and Twitter, implemented in social networking, search, and news sites. Benefits are said to include increased user engagement ("flow") and decreased server loads. In December 2009 real-time search facilities were added to [[Google Search]].&lt;ref&gt;{{cite web|url=http://googleblog.blogspot.com/2009/12/relevance-meets-real-time-web.html|title=Relevance meets the real-time web}}&lt;/ref&gt;

The absolutely first realtime web implementation worldwide have been the WIMS true-realtime server and its web apps in 2001-2011 (WIMS = Web Interactive Management System); based on the WEB-r model of above; built in Java (serverside) and Adobe Flash (clientside). The true-realtime web model was born in 2000 at mc2labs.net by an Italian independent researcher.

==Real-time search==
A problem created by the rapid pace and huge volume of information created by real-time web technologies and practices is finding relevant information. One approach, known as '''real-time search''', is the concept of searching for and finding information online as it is produced. Advancements in web search technology coupled with growing use of [[social media]] enable online activities to be queried as they occur. A traditional [[web search]] [[Web crawler|crawls]] and [[Index (search engine)|indexes]] web pages periodically, returning results based on relevance to the search query. [[Google Real-Time Search]] was available in [[Google Search]] until July 2011.

==See also==
*[[Comet (programming)|Comet]]
*[[Collaborative real-time editor]]
*[[Firebase]]
*[[Internet of Things|Internet of Things (IoT)]]
*[[Meteor (web framework)|Meteor]]
*[[Microblogging]]
*[[Node.js]]
*[[Prospective search]]
*[[PubNub]]
*[[Push technology|Push Technology]]
*[[Scoopler]]
*[[Vert.x]]
*[https://www.syncano.io Syncano]

==References==
&lt;references /&gt;

==External links==
*{{Cite news|url=https://www.theguardian.com/business/2009/may/19/google-twitter-partnership|title=Google 'falling behind Twitter'|last=Wray|first=Richard|date=19 May 2009|work=The Guardian|accessdate=17 June 2009}}
*{{Cite news|url=http://www.nytimes.com/2009/06/14/business/14digi.html|title=Hey, Just a Minute (or Why Google Isn't Twitter)|last=Stross|first=Randall|date=13 June 2009 |work=New York Times|accessdate=17 June 2009}}
*{{Cite news|url=http://online.wsj.com/article/BT-CO-20090615-712397.html |title=Internet Giants Look For Edge In Real-Time Search |last=Morrison |first=Scott |date=15 June 2009 |work=Wall Street Journal |accessdate=17 June 2009 |deadurl=yes |archiveurl=https://web.archive.org/web/20090616204058/http://online.wsj.com/article/BT-CO-20090615-712397.html |archivedate=16 June 2009 }} 
*{{Cite news|url=http://www.readwriteweb.com/archives/explaining_the_real-time_web_in_100_words_or_less.php|title=Explaining the Real-Time Web in 100 Words or Less|last=Kirkpatrick|first=Marshall|date=22 September 2009|work=ReadWriteWeb}}

{{Use dmy dates|date=October 2010}}

{{DEFAULTSORT:Real-Time Web}}
[[Category:Internet search]]
[[Category:Real-time web| ]]</text>
      <sha1>iqbtglqgxrmmg8sqz09qz1n2ordvyiv</sha1>
    </revision>
  </page>
  <page>
    <title>Notey</title>
    <ns>0</ns>
    <id>49177882</id>
    <revision>
      <id>734493892</id>
      <parentid>724196904</parentid>
      <timestamp>2016-08-14T18:52:08Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor />
      <comment>cleanup, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2187" xml:space="preserve">{{Orphan|date=August 2016}}

{{Infobox Website
| name           = Notey
| logo           = [[Image:Notey (logo).png|220px]]
| screenshot     = 
| caption        = 
| url            = [http://www.notey.com/ notey.com]
| commercial     = 
| type           = Blog discovery platform
| language       = English
| registration   = 
| owner          = 
| launch date    = February 2015
| current status = active
| revenue        = 
| slogan         = 
| alexa          =  
}}

'''Notey''' is a [[:Category:Blog search engines|blog search]] and discovery platform founded in 2015 that helps users discover non-mainstream content and blogs. The platform ranks and features both bloggers and independent publishers on various topics including [[technology]], [[weddings]], [[sneakers]] and more than 500,000 others. Users can upvote articles they like, save them in notebooks and see what the community is reading.&lt;ref&gt;{{cite web | url=http://techcrunch.com/2015/02/17/notey | title=Notey Raises $1.6 Million For Its Topic-Focused Blog Directory  | publisher=Techcrunch | date=February 17, 2015 | accessdate=2016-01-21}}&lt;/ref&gt;

In April 2015, Business Insider named Notey &#8220;one of the fastest growing startups in the world still flying under the radar&#8221;.&lt;ref&gt;{{cite web | url= http://uk.businessinsider.com/15-of-the-fastest-growing-b2b-startups-2015-4?op=1 | title=15 of the fastest growing startups in the world still flying under the radar  | publisher=Business Insider | date=April 17, 2015 | accessdate=2016-01-21}}&lt;/ref&gt;

The company is based in Hong Kong and San Francisco.&lt;ref&gt;{{cite web | url= https://www.crunchbase.com/organization/notey | title=Notey &amp;#124; CrunchBase | year=2015 | publisher=CrunchBase | accessdate=2016-01-21}}&lt;/ref&gt; Its investors include [[Hugo Barra]], [[Ryan Holmes]], Shakil Khan and [[Steve Kirsch]].

==References==
{{reflist}}

==External links==
* [http://www.notey.com/ Notey Home Page]

{{Commons category|Internet search engines}}

[[Category:Blog search engines]]
[[Category:Websites|Search engines]]
[[Category:Internet search]]
[[Category:Aggregation websites]]
[[Category:Search engine software]]


{{searchengine-website-stub}}
{{US-company-stub}}</text>
      <sha1>o6v718j4ua2hku2k7sv13k0zxrl0yq1</sha1>
    </revision>
  </page>
  <page>
    <title>User intent</title>
    <ns>0</ns>
    <id>52689741</id>
    <revision>
      <id>762244517</id>
      <parentid>761220138</parentid>
      <timestamp>2017-01-27T16:01:02Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo/[[WP:AWB/GF|general]] fixes, replaced: the the &#8594; the, [[WP:AWB/T|typo(s) fixed]]: ie.  &#8594; i.e. using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7836" xml:space="preserve">'''User intent''' or '''query intent''' is the identification and categorization of what a user online intended or wanted when they typed their [[web search query|search terms]] into an online [[web search engine]] for the purpose of [[search engine optimization]] or [[conversion rate optimization]].&lt;ref name="Understanding Sponsored Search: Core Elements of Keyword Advertising"&gt;{{cite book|last1=Jansen|first1=Jim|title=Understanding Sponsored Search: Core Elements of Keyword Advertising|date=July 2011|publisher=Cambridge University Press|location=New York, NY, USA|isbn=9781107011977|page=44|url=https://books.google.com/books?id=L4LIGyLOwDoC&amp;pg=PA44&amp;dq=what+is+user+intent&amp;hl=en&amp;gl=us&amp;sa=X&amp;redir_esc=y#v=onepage&amp;q=what%20is%20user%20intent&amp;f=false}}&lt;/ref&gt; When a user goes online, there is always a purpose, an intent. The goal can be fact-checking, comparison shopping, filling downtime, or any other activity online.&lt;ref name="The Different Types of User Intent"&gt;{{cite web|last1=Shih|first1=Joseph|title=The Different Types of User Intent|url=https://www.twinword.com/blog/understanding-different-types-user-intent/|website=Twinword Blog|publisher=Twinword, Inc.|accessdate=26 December 2016}}&lt;/ref&gt;

==Types==
Though there are various ways of classifying or naming the categories of the different types of user intent, overall they seem to follow the same clusters. In general and up until the rise and explosion&lt;ref name="The Rise of Mobile Search: From 2012 to 2015"&gt;{{cite web|title=The Rise of Mobile Search: From 2012 to 2015|url=http://www.texodesign.com.au/the-rise-of-mobile-search/|website=Texo Design|publisher=Texo Design|accessdate=26 December 2016}}&lt;/ref&gt; of [[mobile search]], there are and were [[Web search query#Types|three very broad categories]]: informational, transactional, and navigational.&lt;ref&gt;{{cite journal|last1=Broder|first1=Andrei|title=A Taxonomy of Web Search|journal=SIGIR Forum|date=Fall 2002|volume=36|issue=2|pages=5&#8211;6|url=http://www.cis.upenn.edu/~nenkova/Courses/cis430/p3-broder.pdf|accessdate=27 December 2016}}&lt;/ref&gt; However over time and with the rise&lt;ref name="The Rise of Mobile Search: From 2012 to 2015" /&gt; of [[mobile search]], other categories have appeared or categories have segmented into more specific categorization. The following is a table showing how different organizations have categorize the different types.

{| class="wikitable" style="text-align: center;"
|+ style="text-align: left;" | The Different Types of User Intents&lt;ref name="The Different Types of User Intent" /&gt;
|-
! !! Type 1 !! colspan="2" | Type 2 (a/b) !! Type 3 !! Type 4
|-
| || "who wrote the Matrix" || "online IQ test" || "office supplies" || "google play store" || "restaurants near me"
|-
| [[Microsoft]]&lt;ref&gt;{{cite journal|last1=KhudaBukhsh|first1=Ashiqur|last2=Bennett|first2=Paul|last3=White|first3=Ryen|title=Building Effective Query Classifiers: A Case Study in Self-harm Intent Detection|journal=CIKM '15 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management|date=2015|pages=1735&#8211;1738|url=http://research.microsoft.com/en-us/um/people/pauben/papers/cikm-2015-KhudaBukhsh-et-al.pdf|accessdate=26 December 2016|format=PDF}}&lt;/ref&gt; || Informational || colspan="2" | Transactional || Navigational || --
|-
| [[Google]]&lt;ref&gt;{{cite book|title=Search Quality Evaluator Guidelines|date=28 March 2016|publisher=Google|pages=61&#8211;74|url=http://static.googleusercontent.com/media/www.google.com/en//insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf|accessdate=26 December 2016}}&lt;/ref&gt; || Know || colspan="2" | Do || Website || Visit-in-person
|-
| [[Hubspot]]&lt;ref&gt;{{cite web|title=Keyword Development: Without a computer!|url=https://cdn2.hubspot.net/hub/137828/file-331703896-pdf/docs/hubspot_keyword_development_worksheet.pdf|website=Hubspot|publisher=Hubspot|accessdate=26 December 2016}}&lt;/ref&gt; || Problem based || colspan="2" | Solution based || Brand based || --
|-
| [[SEMRush]]&lt;ref&gt;{{cite web|title=Types of keywords: commercial, informational, navigational, transactional|url=https://www.semrush.com/blog/types-of-keywords-commercial-informational-navigational-transactional/|website=SEMRush Blog|publisher=SEMRush|accessdate=26 December 2016}}&lt;/ref&gt; || Informational || Commercial || Transactional || Navigational || --
|-
| [[Web Analytics World]]&lt;ref&gt;{{cite web|last1=Levitt|first1=Dean|title=Using Intent, Demographics and Micro-Moments to Better Understand your Web Traffic|url=http://www.webanalyticsworld.net/2016/09/intent-demographics-and-micro-moments-in-analytics.html|website=Web Analytics World|publisher=Jump Digital|accessdate=26 December 2016}}&lt;/ref&gt; || Know || Do || Buy || -- || Go
|-
| Summary || Know || Do || Buy || Web || Local
|}

* Know - An informational search query looking for facts or other information (e.g. "who wrote the Matrix")
* Do - A transactional search query wanting to fulfill a task online (e.g. "online IQ test")
* Buy - A transactional search query wanting to buy something (e.g. "office supplies")
* Web - A navigational search query wanting to visit to a specific web site or page (e.g. "google play store")
* Local - A search query wanting to visit-in-person a physical location (e.g. "restaurants near me")

Please note that many search queries may be ambiguous and thus may be classified into multiple intents. For example, a user who typed a query "matrix" into a search bar may want to purchase the [[The Matrix|1999 American-Australian philosophical sci-fi film]] or may want to learn more about the [[Matrix (mathematics)|matrices in mathematics]].

==Importance==
With the prevalence of search engines being the first starting point of many online sessions,&lt;ref&gt;{{cite web|last1=Purcell|first1=Kristen|title=Search and email still top the list of most popular online activities|url=https://searchenginewatch.com/sew/study/2101282/search-engines-92-adult-internet-users-study|website=Pew Research Center Internet, Science &amp; Tech|publisher=Pew Research Center|accessdate=26 December 2016}}&lt;/ref&gt; search engines are tasked with surfacing the [[Search engine results page|best results]] or best [[Online advertising|ads]] that will satisfy the various user intents. Because [[Web search engine#Search engine bias|search engines do not actually read and understand]] web pages and ad copy completely, [[Digital marketing|digital marketers]] have to align their [[Keyword research|target keywords]] to the correct user intent that they are trying to satisfy&lt;ref name="The Different Types of User Intent" /&gt; if they want to rank high on [[Search engine result page|SERPs]] and improve their [[Conversion rate optimization|conversion rate]].

Take for example, a company selling colored contact lenses who wants their ad to show up for relevant searches may target the keyword "blue eyes". However, this may not be the most effective strategy as users who search "blue eyes" may want to learn biological facts about blue eyes. Instead, the company can target keywords that clearly indicates that the user is looking to buy colored contact lenses (i.e. "blue contact lenses" most likely implies "buy blue contact lenses"). With the correct keyword intent targeting, studies have shown that conversion rates increase significantly.&lt;ref&gt;{{cite web|last1=daSilva|first1=Tiffany|title=Why Ignoring User Intent is Costing You Money in AdWords|url=http://unbounce.com/ppc/ignoring-user-intent-costs-you-money-in-adwords/|website=unbounce Pay Per Click|publisher=unbounce|accessdate=26 December 2016}}&lt;/ref&gt;

==See also==
* [[Web search query]]
* [[Keyword research]]
* [[Intent marketing]]
* [[Search engine optimization]]
* [[Conversion rate optimization]]
* [[Search engine result page]]
* [[Principle of least astonishment]]

==References==
{{reflist}}

[[Category:Internet search]]</text>
      <sha1>jxyxb0dcq6349vb3ng74epevjcyzupe</sha1>
    </revision>
  </page>
  <page>
    <title>ZyLAB Technologies</title>
    <ns>0</ns>
    <id>2744940</id>
    <revision>
      <id>738080704</id>
      <parentid>730924932</parentid>
      <timestamp>2016-09-06T20:07:44Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12984" xml:space="preserve">{{advert|date=April 2012}}
{{Infobox company |
  name   = ZyLAB |
  logo   = &lt;!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --&gt; |
  slogan = "eDiscovery &amp; Information Risk Management" |
  type   = Private |
  foundation     = 1983 |
  location       = [[McLean, Virginia]]&lt;br&gt;[[Amsterdam]] |
  key_people     = [[Pieter Varkevisser]], president &amp; CEO&lt;br&gt;[[Dr. Johannes C. Scholtes]], chairman &amp; chief strategy officer | Mary Mack, Enterprise Technology Counsel
  num_employees  = 140 |
  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |
  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email &amp; SharePoint archiving, text-analytics, visualization, contract management, and workflow. |

  homepage       = [http://www.zylab.com/ www.zylab.com]
}}

'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB&#8217;s most important products are ZyLAB eDiscovery &amp; Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.

== History ==
In 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.

In 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]&#8217;s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.

In 1998, the company developed support to full-text search email, including attachments.

In 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.

In 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].

2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.

Platforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.

2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.

==Customers==
Initial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).

Other well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milo&#353;evi&#263;]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB&#8217;s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].

Public websites also use the ZyLAB Webserver.

[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.

ZyLAB&#8217;s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.

==System overview and compatibility==
According to the company&#8217;s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:

Systems:
*ZyLAB eDiscovery and Production
*ZyLAB Compliance and Litigation readiness
*ZyLAB Law Enforcement and Investigations
*ZyLAB Communications Intelligence
*ZyLAB Digital Print and Media Archiving
*ZyLAB Enterprise Information Management

Bundles:
*E-Mail Archiving Bundle
*Microsoft SharePoint Bundle
*Analytics Bundle
*eDiscovery EDRM Processing bundle
*DoD and Sox Compliant RMA Bundle
*TIFF Archiving and Production Bundle
*WebPublishing Bundle
*Commercial Publishing Bundle
*Business Process Automation Bundle
*Development and Integrators Bundle
*Scanning Bundle
*Digital Copier Bundle
*Professional Text Mining
*Machine translation

===Supported configurations===
*'''Server OS''': Windows 2003, Windows 2008
*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL
*'''Web Servers''': IIS
*'''Client OS''': Windows XP, Windows Vista, Windows 7
*'''Clustering''': Support for Active/Passive Failover.
*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.
*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.

===Languages supported===
*'''Unicode'''. Support for documents in all languages.
*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.

==Zy-IMAGE-nation Annual Conference==
The annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.

==See also==
* [[Electronic discovery|e-Discovery]]
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Document Imaging]]
* [[E-mail archiving|E-mail Archiving]]
* [[Knowledge Management]]
* [[Document management system|Document Management (System)]]
* [[Enterprise content management|Enterprise Content Management]]
* [[Records management|Records Management]]
* [[Contract management|Contract Management]]
* [[Workflow]]
* [[Text mining|Text Mining]]
* [[Text analytics|Text Analytics]]
* [[Machine translation|Automatic Machine Translation]]
* [[Data visualization|Data Visualization]]

==References==
{{Reflist}}
*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&amp;i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&amp;i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[https://web.archive.org/web/20090108050805/http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''
*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]
*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''
*[https://web.archive.org/web/20090224230803/http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)
*[https://web.archive.org/web/20110717125042/http://www.computerwoche.de/index.cfm?pid=2123&amp;pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)
*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&amp;s_site=miami&amp;p_multi=MH&amp;p_theme=realcities&amp;p_action=search&amp;p_maxdocs=200&amp;p_topdoc=1&amp;p_text_direct-0=0EB367D56736E685&amp;p_field_direct-0=document_id&amp;p_perpage=10&amp;p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''
*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]
*[http://www.fcw.com/print/6_31/news/70014-1.html Review]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of ZyIMAGE on ''Federal Computer Week (FCW.com)''
*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.
*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&amp;nbsp;20.
*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&amp;nbsp;22.
*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&amp;nbsp;88&#8211;89.
*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&amp;nbsp;73, 76, and 77.
*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&amp;nbsp;100.
*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&amp;nbsp;127, 129, 133 and 137.
*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&amp;nbsp;56.
*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&amp;nbsp;46.
*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&amp;nbsp;22.
*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&amp;nbsp;62.

===Gartner reports===
*Introduction to Investigative Case Management Products (18 April 2007)
*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)
*MarketScope for Contract Management, 2007 (16 July 2007)
*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)
*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)
*Magic Quadrant for Information Access Technology, 2008
*Magic Quadrant for Information Access Technology, 2009
*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)
*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)
*MarketScope for E-Discovery Product Vendors, 2008
*MarketScope for E-Discovery Product Vendors, 2009
*MarketScope for Records Management (20 May 2008)
*Hype Cycle for Content Management, 2008 (8 July 2008)
*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)
*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)
*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)

==External links==
*[http://www.zylab.com/ ZyLAB official website]
*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]
*[http://www.aiim.org/ AIIM]

[[Category:Companies established in 1983]]
[[Category:Software companies of the United States]]
[[Category:Information retrieval organizations]]</text>
      <sha1>i3tm9qkkrzbida5kexd39lhnhywfo8s</sha1>
    </revision>
  </page>
  <page>
    <title>European Summer School in Information Retrieval</title>
    <ns>0</ns>
    <id>22254915</id>
    <revision>
      <id>748286815</id>
      <parentid>735906504</parentid>
      <timestamp>2016-11-07T12:27:04Z</timestamp>
      <contributor>
        <username>Gianmaria.silvello</username>
        <id>9355814</id>
      </contributor>
      <comment>/* ESSIR Editions */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4970" xml:space="preserve">The '''European Summer School in Information Retrieval''' ('''ESSIR''') is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.
The aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: "''The term IR identifies the activities that a person &#8211; the user &#8211; has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''"&lt;ref&gt;Agosti, M.: "Information Access using the Guide of User Requirements". In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).&lt;/ref&gt;

IR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[computer science]] to [[information science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].

ESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the Internet.

Two books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval'',&lt;ref&gt;Agosti, M., Crestani, F. and Pasi, G. (Eds): "Lectures on Information Retrieval". Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11&#8211;15, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.&lt;/ref&gt; the second one is ''Advanced Topics in Information Retrieval''.&lt;ref&gt;Melucci, M., and Baeza-Yates, R. (Eds): "Advanced Topics in Information Retrieval". The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.&lt;/ref&gt;

== ESSIR Editions ==
ESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by [http://www.dei.unipd.it/~agosti/ Maristella Agosti], [[University of Padua]], Italy and [[Nick Belkin]], [[Rutgers University]], U.S.A., for an Italian audience in 1989.

{| class="wikitable" border="1"
|-
! Edition
! Web Site
! Location
! Organiser(s)
|-
|  10th
|  [http://mklab.iti.gr/essir2015/ ESSIR 2015]
|  Thessaloniki, Greece
|  Ioannis (Yiannis) Kompatsiaris, Symeon Papadopoulos, Theodora Tsikrika, and Stefanos Vrochidis
|-
|  9th
|  [http://www.ugr.es/~essir2013/ ESSIR 2013]
|  Granada, Spain
|  Juan M. Fernadez-Luna and Juan F. Huete
|-
|  8th
|  [http://essir.uni-koblenz.de/ ESSIR 2011]
|  Koblenz, Germany
|  Sergej Sizov and Steffen Staab
|-
|  7th
|  [http://essir2009.dei.unipd.it/ ESSIR 2009]
|  Padua, Italy
|  Massimo Melucci and Ricardo Baeza-Yates
|-
|  6th
|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]
|  Glasgow, Scotland, United Kingdom
|  Iadh Ounis and Keith van Rijsbergen
|-
|  5th
|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]
|  Dublin, Ireland
|  Alan Smeaton
|-
|  4th
|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]
|  Aussois (Savoie), France
|  Catherine Berrut and Yves Chiaramella
|-
|  3rd
|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]
|  Varenna, Italy
|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi
|-
|  2nd
|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]
|  Glasgow, United Kingdom
|  Keith van Rijsbergen
|-
|  1st
|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]
|  Brixen, Italy
|  Maristella Agosti
|}

==Notes==
{{reflist}}

==External links==
* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]
* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering &#8211; University of Padua, Italy]
* [http://www.dei.unipd.it/ Department of Information Engineering &#8211; University of Padua, Italy]
* [http://www.unipd.it/en/index.htm University of Padua, Italy]

[[Category:Information retrieval organizations]]
[[Category:Summer schools]]</text>
      <sha1>5ct5fbviqxkqbuw9icnck9giksv98sk</sha1>
    </revision>
  </page>
  <page>
    <title>Dandelon.com</title>
    <ns>0</ns>
    <id>41725036</id>
    <revision>
      <id>748144486</id>
      <parentid>718751029</parentid>
      <timestamp>2016-11-06T16:08:46Z</timestamp>
      <contributor>
        <username>The American Farmer</username>
        <id>26806929</id>
      </contributor>
      <comment>copy edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6420" xml:space="preserve">
{{more footnotes|date=January 2014}}
'''Dandelon.com''' is a collaborative community of libraries in multiple countries as well as a [[Search engine (computing)|search engine]], a search or discovery service, a library information system for the academic community. It is additionally a platform allowing registered libraries to exchange library catalogue enrichment data: tables of content of monographs, deep indexing data, cover pages and bibliographic descriptions of articles published in periodicals, with abstracts and / or full texts provided for part of the items. The domain name was created in 2004. It is derived from the plant [[Taraxacum|dandelion]]. The name is an allusion to the flower's worldwide occurrence: It is thought to spread around the world as easily as human words and thoughts. Dandelon's aim is to uncover knowledge assets for students from around the world. It is free of charge for private use and without user tracking or advertising.&lt;ref&gt;[http://www.dandelon.com Dandelon.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

Traditionally, the number of searchable relevant subject words comes up to about five semantically different subjects words, located in titles and generated by human indexing. A book registered at dandelon.com typically is assigned between 20 and 500 subject words depending on the size of the book and the knowledge domain. Based on this extended set of terms representing each library item, queries can be more specific, and relevance ranking can be more efficient. Dandelon.com also expands user queries by adding closely related words (default: synonyms and translations, optionally: narrower terms) from multilingual [[Thesaurus|thesauri]] from various knowledge domains.

Search results can be restricted to a specific library. Automatic backlinks to the related library management system allows online access or requesting a book. Dandelon.com does not replace library management systems, it is an additional option for searching and first of all a platform for data exchange between libraries associated with its community. Its user interface supports a number of languages, and it provides content in about 130 languages.

The core of dandelon.com is the content production software &#8220;&#8220;intelligentCAPTURE mobile&#8221;&#8221; employed by all member libraries. It reads from and sends data to each library management system, receiving text content via digitization and [[optical character recognition]] (OCR) for close to 200 languages or via native digital content import. Additionally, it automatically extracts major subject words, which are translated into 60 languages by machine translation. Computers and scanners can be placed in a special mobile furniture to be used between shelves and narrow compactus.

The provider of production software and search and distribution services is the German-based company AGI-Information Management Consultants &lt;ref&gt;[http://www.agi-imc.de&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; as well as the hosting center of  GBV - Gemeinsamer Bibliotheksverbund - , a state-owned German library service center for more than 800 libraries.&lt;ref&gt;[http://www.gbv.de&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; The solution was invented in 2001 by Manfred Hauer of AGI and Karl Raedler from Vorarlberger Landesbibliothek, Austria.&lt;ref&gt;[http://vlb.vorarlberg.at&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Dandelon.com shares part of its data with GBV. GBV, in turn, exchanges some of its catalogue enrichment data with OCLC, [[WorldCat]] and other service centers. HEBIS,&lt;ref&gt;[http://www.hebis.de&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; another state-owned service center shares with the German National Library. German National Library charges fees for enrichment content.&lt;ref&gt;[http://www.dnb.de/kataloganreicherung&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; AGI and a number of the producing libraries have been pioneering catalogue enrichment in Europe since 2001 and form one of the largest communities of producers of digitalized tables of content of monographs in Europe. In 2013 close to 2&amp;nbsp;million tables of contents were digitalized, not all of which are available on dandelon.com for the general public. The large collection produced for the [[German National Library]] is not yet shared and was announced for public use in 2014. Dandelon.com and intelligentCAPTURE are [[IBM Lotus Domino|IBM Domino and Notes]] applications. Dandelon.com runs [[Apache Lucene]] as retrieval engine.

== References ==
{{Reflist}}
*Manfred Hauer: 2012 "[http://www.agi-imc.de/internet.nsf/dda9df579aa6429dc12567f5004ad7ed/659e168d74f0bc38c12579bb004ea5b8/$FILE/HAUER_SLA_Bahrain_2012_gb.pdf Web 2.0: Which features are wanted by academic library clients? A HEBIS Survey Report]" (PDF; 172&amp;nbsp;kB) ''Gulf special library association (SLA)'', Conference Proceeding - on CD
*Nienerza,Heike / Sunckel, Bettina / Meier, Berthold: 2011 "[http://www.degruyter.com/view/j/abitech.2011.31.issue-3/ABI.2011.020/ABI.2011.020.xml?format=INT Unser Katalog soll besser werden! Kataloge und Portale im Web-2.0-Zeitalter. Ergebnisse einer Online-Umfrage im HeBIS-Verbund]"  ''ABI-Technik'', De Gruyter, Berlin, Issue 31, pp. 130-149, DOI: 10.1515/ABI.2011.020
*Manfred Hauer: 2013 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3d26118ce2a8ebccc1257b1800356e8b?OpenDocument Zur Bedeutung normierter Terminologien in Zeiten moderner Sprach- und Information-Retrieval-Technologien]" (PDF; 205&amp;nbsp;kB) ''[http://www.degruyter.com/view/j/abitech] ABI-Technik'', De Gruyter, Berlin, issue 1, pp. 2-6
*Manfred Hauer, Rainer Diedrichs: 2010 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3f191bb231f0d57ec1257749004a9e7d/$FILE/Kataloganreicherung_in_Europa_2010_c.pdf  Kataloganreicherung in Europa]" (PDF; 525&amp;nbsp;kB) ''Buch und Bibliothek [http://www.b-u-b.de/]'', issue 5, pp.&amp;nbsp;394&#8211;397
*Manfred Hauer: 2005 ''[http://www.agi-imc.de/internet.nsf/94280a18b17ee318c12567d2003c3bb2/3267dae6428c5f02c125711600527ffd?OpenDocument/Vergleich der Retrievalleistungen von Bibliothekskatalogen gegen erweiterte und neue Konzept. Benchmarking: Google Scholar, dandelon.com, Vorarlberger Landesbibliothek, weitere OPACs.] In: [http://www.degruyter.com/view/j/abitech] ABI-Technik, De Gruyter, Berlin, December, pp.&amp;nbsp;295&#8211;301.

[[Category:Information retrieval organizations]]
[[Category:Information retrieval systems]]
[[Category:Digital library projects]]</text>
      <sha1>miq5t7jmwcfaescors083w2tnguwdk3</sha1>
    </revision>
  </page>
  <page>
    <title>Information Retrieval Specialist Group</title>
    <ns>0</ns>
    <id>10218640</id>
    <revision>
      <id>667009502</id>
      <parentid>666857473</parentid>
      <timestamp>2015-06-15T06:23:15Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* External links */Removed invisible unicode characters + other fixes, removed: &#8206; using [[Project:AWB|AWB]] (11140)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1466" xml:space="preserve">{{Unreferenced|date=January 2010}}

The '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}

==European Conference on Information Retrieval==
Organising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.

== External links ==
* [http://irsg.bcs.org/ IRSG website]

[[Category:Information retrieval organizations]]
[[Category:BCS Specialist Groups]]</text>
      <sha1>089oclkbv3h674f2hsenmea3841d5ef</sha1>
    </revision>
  </page>
  <page>
    <title>Special Interest Group on Information Retrieval</title>
    <ns>0</ns>
    <id>14109784</id>
    <revision>
      <id>747323389</id>
      <parentid>733168913</parentid>
      <timestamp>2016-11-01T18:57:33Z</timestamp>
      <contributor>
        <ip>50.53.1.33</ip>
      </contributor>
      <comment>/* Conferences */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3418" xml:space="preserve">{{Infobox organization
|name           = ACM Special Interest Group on Information Retrieval
|image          = sig-information-retrieval-logo.png
|size           = 140px
|alt            = ACM SIGIR
|parent_organization = [[Association for Computing Machinery]]
|website        = {{URL|sigir.org}}
}}

'''SIGIR''' is the [[Association for Computing Machinery]]'s '''Special Interest Group on Information Retrieval'''. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, [[Information retrieval|retrieval]] and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.

== Conferences ==
The annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval. SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[ACM SIGWEB|SIGWEB]], the [[Conference on Information and Knowledge Management]] (CIKM), and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[ACM SIGWEB|SIGWEB]].

=== SIGIR conference locations ===
{| class="wikitable" border="1"
|-
! Number
! Year
! Location
|-
| 22
| 1999
| [[Berkeley, California]]
|-
| 23
| 2000
| [[Athens]]
|-
| 24
| 2001
| [[New Orleans]]
|-
| 25
| 2002
| [[Tampere]]
|-
| 26
| 2003
| [[Toronto]]
|-
| 27
| 2004
| [[Sheffield]]
|-
| 28
| 2005
| [[Salvador, Bahia]]
|-
| 29
| 2006
| [[Seattle]]
|-
| 30
| 2007
| [[Amsterdam]]
|-
| 31
| 2008
| [[Singapore]]
|-
| 32
| 2009
| [[Boston]]
|-
| 33
| 2010
| [[Geneva]]
|-
| 34
| 2011
| [[Beijing]]
|-
| 35
| 2012
| [[Portland, Oregon]]
|-
| 36
| 2013
| [[Dublin]]
|-
| 37
| 2014
| [[Gold Coast, Queensland]]
|-
| 38
| 2015
| [[Santiago]]
|-
| 39
| 2016
| [[Pisa]]
|-
| 40
| 2017
| [[Tokyo]]
|-
| 41
| 2018
| [[Ann Arbor]]
|}

== Awards ==
The group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award &lt;ref&gt;{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}&lt;/ref&gt; to recognize the highest quality paper at each conference. "Test of time" Award &lt;ref&gt;{{cite web | url=http://sigir.org/awards/test-of-time-awards/ | title=SIGIR Conference Test of Time Awards | accessdate=2015-12-29 }}&lt;/ref&gt; is a recent award that is given to a paper that  has had "long-lasting influence, including impact on a subarea of information retrieval research, across subareas of information retrieval research, and outside of the information retrieval research community". This award is selected from a set of full papers presented at the main SIGIR conference 10-12 years before.

==See also==
* [[Conference on Information and Knowledge Management]]

==References==

{{Reflist}}
==External links==
* {{official website|http://www.sigir.org/}}

{{Authority control}}

[[Category:Association for Computing Machinery Special Interest Groups]]
[[Category:Information retrieval organizations]]</text>
      <sha1>q0swjfk64i6o4j0sxgtfl53cfzj75wj</sha1>
    </revision>
  </page>
  <page>
    <title>TeLQAS</title>
    <ns>0</ns>
    <id>21727808</id>
    <revision>
      <id>666703845</id>
      <parentid>554506388</parentid>
      <timestamp>2015-06-13T01:42:09Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1746" xml:space="preserve">'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.&lt;ref&gt;Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.&lt;/ref&gt;

==Architecture==
TeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.&lt;ref&gt;Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).&lt;/ref&gt; The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.

==References==
&lt;references/&gt;

[[Category:Computational linguistics]]
[[Category:Information retrieval systems]]
[[Category:Natural language processing software]]</text>
      <sha1>if93i1ftck0zqzd8dr0swf94hg141gh</sha1>
    </revision>
  </page>
  <page>
    <title>EXCLAIM</title>
    <ns>0</ns>
    <id>8239120</id>
    <revision>
      <id>762772034</id>
      <parentid>762771888</parentid>
      <timestamp>2017-01-30T17:29:42Z</timestamp>
      <contributor>
        <ip>2A01:E35:8B97:C020:25F9:6719:65F1:4026</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5192" xml:space="preserve">{{For|the Canadian magazine|Exclaim!}}
The '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' was an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006, with some support for more than a dozen languages. The lead developers were Justin Nuger and Jesse Saba Kirchner.

Early work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.&lt;ref&gt;
{{cite web
|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web
|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf
|format=PDF|publisher=ACM-SIGIR 1999
|accessdate=2006-12-02
}}
&lt;/ref&gt;

EXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).

One of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.

==Current status==

In 2009, EXCLAIM was in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, was available for the following twenty-three languages:

{| class="wikitable"
| [[Albanian language|Albanian]]
|-
| [[Amharic]]
|-
| [[Bengali language|Bengali]]
|-
| [[Gothic language|Gothic]]
|-
| [[Greek language|Greek]]
|-
| [[Icelandic language|Icelandic]]
|-
| [[Indonesian language|Indonesian]]
|-
| [[Irish language|Irish]]
|-
| [[Javanese language|Javanese]]
|-
| [[Latvian language|Latvian]]
|-
| [[Malagasy language|Malagasy]]
|-
| [[Mandarin Chinese]]
|-
| [[Nahuatl]]
|-
| [[Navajo language|Navajo]]
|-
| [[Quechua languages|Quechua]]
|-
| [[Sardinian language|Sardinian]]
|-
| [[Swahili language|Swahili]]
|-
| [[Tagalog language|Tagalog]]
|-
| [[Standard Tibetan|Tibetan]]
|-
| [[Turkish language|Turkish]]
|-
| [[Welsh language|Welsh]]
|-
| [[Wolof language|Wolof]]
|-
| [[Yiddish]]
|}

Support using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:

{| class="wikitable"
|-
| [[Dutch language|Dutch]]
|-
| [[Spanish language|Spanish]]
|}

Significant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.

Future versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.

The EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.

==Further applications==

EXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[Front and back ends|backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].&lt;ref&gt;{{cite web
|title=A crosslinguistic readability framework
|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf
|format=PDF|publisher=ACL-IJNLP 2009
|accessdate=2009-09-04
}}
&lt;/ref&gt;

==Notes and references==

{{reflist}}

==External links==
*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website] (dead link)
*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]
*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]
*[http://ju-st.in/ Justin Nuger's professional webpage]

{{DEFAULTSORT:Exclaim}}
[[Category:Information retrieval systems]]</text>
      <sha1>d3mhwtko1u07rqs0u0io16n5w6tmaan</sha1>
    </revision>
  </page>
  <page>
    <title>IBM Omnifind</title>
    <ns>0</ns>
    <id>13762814</id>
    <revision>
      <id>738672037</id>
      <parentid>706436098</parentid>
      <timestamp>2016-09-10T11:33:28Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2380" xml:space="preserve">'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].
It did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.&lt;ref&gt;[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]&lt;/ref&gt; IBM OmniFind as a standalone product was withdrawn in April 2011&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&amp;infotype=an&amp;appname=iSource&amp;supplier=897&amp;letternum=ENUS911-075 IBM US Announcement Letter]&lt;/ref&gt; and is now part of [[IBM Watson Content Analytics with Enterprise Search]].&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&amp;subtype=CA&amp;htmlfid=897/ENUS211-133 IBM US Announcement Letter]&lt;/ref&gt;

'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&amp;infotype=an&amp;appname=iSource&amp;supplier=897&amp;letternum=ENUS910-115 IBM US Announcement Letter]&lt;/ref&gt;

'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.

== See also ==
* [[Languageware]]
* [[UIMA]]
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]

==External links==
* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]
* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}
* [https://web.archive.org/web/20071030125647/http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] 
* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]

==Notes==
{{reflist}}

[[Category:IBM software|OmniFind]]
[[Category:Information retrieval systems]]</text>
      <sha1>bd4rbvriv0xecc5unj62vxil1nowjbm</sha1>
    </revision>
  </page>
  <page>
    <title>Locate (Unix)</title>
    <ns>0</ns>
    <id>3522125</id>
    <revision>
      <id>731966698</id>
      <parentid>731966661</parentid>
      <timestamp>2016-07-28T18:03:48Z</timestamp>
      <contributor>
        <ip>128.40.9.123</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3283" xml:space="preserve">{{lowercase}}
'''&lt;code&gt;locate&lt;/code&gt;''' is a [[Unix]] utility which serves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by the &lt;code&gt;updatedb&lt;/code&gt; command or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than &lt;code&gt;[[find]]&lt;/code&gt;, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements, particularly on very large filesystems.

&lt;code&gt;locate&lt;/code&gt; was first created in 1982.&lt;ref&gt;{{cite magazine|last=Woods|first=James A.|date=1983-01-15|title=Finding Files Fast|url=https://archive.org/stream/login-feb83/login_feb83_issue#page/n9/mode/2up|magazine=[[;login:]]|volume=8|issue=1|pages=8&#8211;10|publisher=[[Usenix]]|access-date=2016-03-27}}&lt;/ref&gt;  The BSD and [[GNU Findutils]] versions derive from the original implementation.&lt;ref&gt;{{cite web|url=https://www.gnu.org/software/findutils/manual/html_node/find_html/Introduction.html#Introduction|title=Finding Files|date=2012-11-17|website=[[GNU]]|publisher=[[Free Software Foundation]]|access-date=2016-03-27|quote=GNU locate and its associated utilities were originally written by James Woods, with enhancements by David MacKenzie.}}&lt;/ref&gt;  Their primary database is world-readable, so the index is built as an unprivileged user.

&lt;code&gt;mlocate&lt;/code&gt; (Merging Locate) and the earlier &lt;code&gt;slocate&lt;/code&gt; (Secure Locate) use a restricted-access database, only showing filenames accessible to the user.&lt;ref&gt;{{cite web|url=http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-url=https://web.archive.org/web/20060411074142/http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-date=2006-04-11|title=mlocate|date=2005|author=Miloslav Trma&#269;|access-date=2016-03-27|quote=...faster and does not trash the system caches as much...attempts to be compatible to GNU locate, when it does not conflict with slocate compatibility.|dead-url=yes}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.geekreview.org/slocate/|archive-url=https://web.archive.org/web/20050507092723/http://www.geekreview.org/slocate/|archive-date=2005-05-07|title=Secure Locate|date=1999|author=Kevin Lindsay|access-date=2016-03-27|quote=...will also check file permissions and ownership so that users will not see files they do not have access to.|dead-url=yes}}&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]
* [https://fedorahosted.org/mlocate/ mlocate]
* {{man|1|locate|FreeBSD}}
* {{man|1|locate|OpenBSD}}

Variants:
* [http://rlocate.sourceforge.net/ rlocate] - Variant using kernel module and daemon for continuous updates.
* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate
* [http://www.locate32.net/ Locate32 for Windows] - GPL'ed graphical Windows variant

{{unix commands}}

[[Category:GNU Project software]]
[[Category:Unix file system-related software]]
[[Category:Information retrieval systems]]


{{Unix-stub}}</text>
      <sha1>74kxxbl2zv8k5e9k3buttpilfouie4o</sha1>
    </revision>
  </page>
  <page>
    <title>Statistically improbable phrase</title>
    <ns>0</ns>
    <id>2724706</id>
    <revision>
      <id>757758056</id>
      <parentid>757664538</parentid>
      <timestamp>2017-01-01T15:43:47Z</timestamp>
      <contributor>
        <username>Derek R Bullamore</username>
        <id>698799</id>
      </contributor>
      <comment>Filling in 1 references using [[WP:REFLINKS|Reflinks]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3643" xml:space="preserve">A '''statistically improbable phrase''' ('''SIP''') is a phrase or set of words that occurs more frequently in a document (or collection of documents) than in some larger [[Text corpus|corpus]].&lt;ref&gt;{{cite web|url=http://courses.cms.caltech.edu/cs145/2011/wikipedia.pdf |title=SIPping Wikipedia |website=Courses.cms.caltech.edu |accessdate=2017-01-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.plagiarismtoday.com/2012/07/03/how-long-should-a-statistically-improbably-phrase-be/|title=How Long Should a Statistically Improbably Phrase Be?|author=Jonathan Bailey|date=3 July 2012|work=Plagiarism Today}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|url=http://bioinformatics.oxfordjournals.org/content/26/11/1453|title=Identifying duplicate content using statistically improbable phrases|first1=Mounir|last1=Errami|first2=Zhaohui|last2=Sun|first3=Angela C.|last3=George|first4=Tara C.|last4=Long|first5=Michael A.|last5=Skinner|first6=Jonathan D.|last6=Wren|first7=Harold R.|last7=Garner|date=1 June 2010|publisher=|journal=Bioinformatics|volume=26|issue=11|pages=1453&#8211;1457|accessdate=1 January 2017|via=bioinformatics.oxfordjournals.org|doi=10.1093/bioinformatics/btq146|pmid=20472545|pmc=2872002}}&lt;/ref&gt; [[Amazon.com]] uses this concept in determining keywords for a given book or chapter, since keywords of a book or chapter are likely to appear disproportionately within that section.&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}&lt;/ref&gt;&lt;ref&gt;{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2005/08/29/AR2005082901873.html|title=Amazon's Vital Statistics Show How Books Stack Up|last=Weeks|first=Linton|work=[[The Washington Post]]|date=August 30, 2005|accessdate=September 8, 2015}}&lt;/ref&gt; [[Christian Rudder]] has also used this concept with data from [[Online dating service|online dating profiles]] and [[Twitter]] posts to determine the phrases most characteristic of a given race or gender in his book ''Dataclysm''.&lt;ref&gt;{{cite book |last=Rudder |first=Christian |date=2014 |title=Dataclysm: Who We Are When We Think No One's Looking |location=New York |publisher=Crown Publishers |page= |isbn=978-0-385-34737-2}}&lt;/ref&gt;

==Example== 
In a document about [[computer]]s, the most common word is likely to be the word "the", but since "the" is the most commonly used word in the English language, it is likely that any given document will have the word "the" used very frequently.  However, a word like "program" might occur in the document at a much higher rate than its average rate in the English language.  Hence, it is a word unlikely to occur in any given document, but ''did'' occur in the document given.  "Program" would be a statistically improbable phrase.

The statistically improbable phrases of Darwin's ''[[On the Origin of Species]]'' are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.&lt;ref&gt;[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005&lt;/ref&gt;

==See also==
* [[Googlewhack]] &#8211; A pair of words occurring on a single webpage, as indexed by Google
* [[tf-idf]] &#8211; A statistic used in information retrieval and text mining

==References==
{{Reflist}}

{{Amazon}}

[[Category:Amazon.com]]
[[Category:Bookselling]]
[[Category:Information retrieval systems]]</text>
      <sha1>5vrsp5klmle3t1tfbclc0y4sunbj508</sha1>
    </revision>
  </page>
  <page>
    <title>Indexing Service</title>
    <ns>0</ns>
    <id>4047242</id>
    <revision>
      <id>740025932</id>
      <parentid>735633616</parentid>
      <timestamp>2016-09-18T16:29:50Z</timestamp>
      <contributor>
        <username>Entalpia2</username>
        <id>11842605</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6973" xml:space="preserve">{{distinguish|Indexing and abstracting service}}
{{Use dmy dates|date=February 2011}}
{{Infobox Windows component
| name                = Indexing Service
| screenshot          = Indexing Service Query Form.PNG
| screenshot_size     = 300px
| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].
| type                = [[Desktop search]]
| service_name        = Indexing Service
| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.
| replaced_by         = [[Windows Search]]
| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]&lt;ref name="MIS-Intro" /&gt;&lt;br/&gt;[[Windows 2000]]&lt;ref name="MIS-v3" /&gt;&lt;br/&gt;[[Windows XP]]&lt;ref name="TnC-144" /&gt;&lt;br/&gt;[[Windows Server 2003]]&lt;ref name="TnC-144" /&gt;&lt;br/&gt;[[Windows Server 2008]]&lt;ref name="WIS-Install2008" /&gt;
}}

'''Indexing Service''' (originally called '''Index Server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by a newer [[Windows Search]] indexer. The [[IFilter]] plugins to extend the indexing capabilities to more file formats and protocols are compatible between the legacy Indexing Service and the newer Windows Search indexer.

== History ==
Indexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]&lt;ref name="MIS-Intro" /&gt; as well as [[Windows 2000]] and later.&lt;ref name="MIS-v3" /&gt;&lt;ref name="TnC-144" /&gt;&lt;ref name="WIS-What" /&gt; The first incarnation of the indexing service was shipped in August 1996&lt;ref name="MIS-Intro" /&gt; as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}

In [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.&lt;ref name="WIS-Install2008" /&gt;

Indexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.&lt;ref&gt;{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}&lt;/ref&gt; It has been removed from [[Windows 8]].

== Search interfaces ==

Comprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.

Once the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.

Microsoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.&lt;ref&gt;{{cite web
 | url = http://support.microsoft.com/kb/319506
 | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)
 | work = Microsoft Support
 | publisher = 10 May 2002
 | accessdate = 1 February 2011
}}&lt;/ref&gt;

== References ==
{{Reflist|refs=
&lt;ref name = "MIS-Intro"&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx
  |title = Introduction to Microsoft Index Server
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date = 15 October 1997
  |accessdate = 1 February 2011
  |first1 = Krishna
  |last1 = Nareddy
  }}&lt;/ref&gt;
&lt;ref name = "MIS-v3"&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx
  |title = Indexing Service Version 3.0
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}&lt;/ref&gt;
&lt;ref name = "WIS-What"&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx
  |title = What is Indexing Service?
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}&lt;/ref&gt;
&lt;ref name="WIS-Install2008"&gt;{{Cite web
  |url = http://support.microsoft.com/kb/954822
  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)
  |work = Microsoft Support
  |publisher = Microsoft Corporation
  |date = 3 May 2010
  |accessdate = 1 February 2011
  }}&lt;/ref&gt;
&lt;ref name="TnC-144"&gt;{{Cite book
  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&amp;displaylang=en
  |format = Microsoft Word
  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP
  |edition = 2.0
  |publisher = Microsoft Corporation
  |page = 144
  |date=December 2005
  |first1 = Mike
  |last1  = Danseglio
  |first2 = Kurt
  |last2  = Dillard
  |first3 = Jos&#233;
  |last3  = Maldonado
  |first4 = Paul
  |last4  = Robichaux
  |editor1-first = Reid
  |editor1-last  = Bannecker
  |editor2-first = John
  |editor2-last  = Cobb
  |editor3-first = Jon
  |editor3-last  = Tobey
  |editor4-first = Steve
  |display-editors = 3 |editor4-last  = Wacker
  }}&lt;/ref&gt;
}}

{{Microsoft Windows components}}
{{DEFAULTSORT:Indexing Service}}
[[Category:Windows communication and services]]
[[Category:Desktop search engines|Desktop search engines]]
[[Category:Information retrieval systems]]
[[Category:Windows components]]</text>
      <sha1>akrag5u0pwinq19lntocw8v51f832z7</sha1>
    </revision>
  </page>
  <page>
    <title>Outline of search engines</title>
    <ns>0</ns>
    <id>34320324</id>
    <revision>
      <id>754383548</id>
      <parentid>754373476</parentid>
      <timestamp>2016-12-12T10:37:43Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>add section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6494" xml:space="preserve">&lt;!--... Attention:  THIS IS AN OUTLINE

        part of the set of 700+ outlines listed at
             [[Portal:Contents/Outlines]].

                 Wikipedia outlines are
              a special type of list article.
              They make up one of Wikipedia's
                content navigation systems

                See [[Wikipedia:Outlines]]
                      for more details.
                   Further improvements
              to this outline are on the way
...--&gt;
The following [[Outline (list)|outline]] is provided as an overview of and topical guide to search engines. 

'''[[Search engine (computing)|Search engine]]''' &amp;ndash; [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented as a list, and are commonly called ''hits''.

{{TOC limit|limit=2}}

== What ''type'' of thing is a search engine? ==

A search engine can be described as all of the following:

* [[Software]] &amp;ndash;
** [[Computer program]] &amp;ndash;
*** [[Application software]] &amp;ndash; computer software designed to help the user to perform specific tasks. Also known as an application or an "app".

== Types of search engines ==

* [[Database search engine]] &amp;ndash;
* [[Desktop search engine]] &amp;ndash;
* [[Distributed search engine]] &amp;ndash; search engine where there is no central server. Unlike traditional centralized search engines, work such as crawling, data mining, indexing, and query processing is distributed among several peers in decentralized manner where there is no single point of control.
* [[Enterprise search engine]] &amp;ndash; search engine employed on and for access to the information on an organization's computer network.
* [[Human search engine]] &amp;ndash; uses human participation to filter the search results and assist users in clarifying their search request. The goal is to provide users with a limited number of relevant results, as opposed to traditional search engines that often return a large number of results that may or may not be relevant.
* [[Hybrid search engine]] &amp;ndash; uses different types of data with or without ontologies to produce the algorithmically generated results based on web crawling. Previous types of search engines only use text to generate their results.
* [[Intelligent medical search engine]]
* [[Metasearch engine]] &amp;ndash; search tool[1] that sends user requests to several other search engines and/or databases and aggregates the results into a single list or displays them according to their source. Metasearch engines enable users to enter search criteria once and access several search engines simultaneously.
** [[Search aggregator]]
* [[Organic search engine]] &amp;ndash; manually operated search service which uses a combination of computer algorithms and human researchers to look up a search query. A search query submitted to an organic search engine is analysed by a human operator who researches the query then formats the response to the user.
* [[Web search engine]] &amp;ndash; designed to search for information on the World Wide Web and FTP servers. The search results are generally presented in a list of results often referred to as SERPS, or "search engine results pages".
** [[Audio search engine]] &amp;ndash; web-based search engine which crawls the web for audio content.
** [[Collaborative search engine]] &amp;ndash; emerging trend for Web search and Enterprise search within company intranets. CSEs let users concert their efforts in information retrieval (IR) activities, share information resources collaboratively using knowledge tags, and allow experts to guide less experienced people through their searches.
** [[Social search engine]] &amp;ndash; type of web search that takes into account the Social Graph of the person initiating the search query.
** [[Video search engine]] &amp;ndash; web-based search engine which crawls the web for video content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers.
* [[Visual search engine]] &amp;ndash; designed to search for information on the World Wide Web through the input of an image or a search engine with a visual display of the search results. Information may consist of web pages, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query).

== Specific search engines ==
{{Main|List of search engines}}

== Search engine software ==

* [[List of search engine software]]

== Search-based applications ==

[[Search-based application]] &amp;ndash;
* [[Bibliographic database]]
* [[Online database]]
** [[List of online databases]]
*** [[List of academic databases and search engines]]
* [[Digital library]]
** [[List of digital library projects]]
*** [[List of online magazines]]
*** [[Wikipedia:List of online newspaper archives]]
* [[Electronic journal]]
** [[Lists of academic journals]]
*** [[List of open-access journals]]
* Digital encyclopedia
** [[Internet encyclopedia]]*
*** [[List of online encyclopedias]]
* [[Wiki]]
** [[List of wikis]]
* Digital dictionary
** [[Online dictionary]]
*** [[List of online dictionaries]]

== Search engine technology ==

[[Search engine technology]]
* [[Search algorithm]]
* [[Search engine image protection]]
* [[Search engine indexing]]
* [[Search engine optimization]]
* [[Search engine results page]]
* [[List of search engine software|Search engine software]]
* [[Search engine submission]]
** [[Search engine optimization copywriting]]
* [[Web crawler]]

== Search engine marketing ==
[[Search engine marketing]]
* [[Pay per click]]
* [[Cost per impression]]
* [[Search analytics]]
* [[Web analytics]]

== Persons influential in search engines ==
* [[Sergey Brin]]
* [[Larry Page]]
* [[Eric Schmidt]]

== See also ==
* [[Outline of the Internet]]
** [[Outline of Google]]
* [[Human flesh search engine]]
{{Clear}}

== References ==
{{Reflist}}

== External links ==
{{Sisterlinks|Search engine}}

* [http://wikimindmap.com/viewmap.php?wiki=en.wikipedia.org&amp;topic=Outline+of+search+engines&amp;Submit=Search This outline displayed as a mindmap], at ''wikimindmap.com''
* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}

{{Outline footer}}

[[Category:Information retrieval systems]]
[[Category:Wikipedia outlines|Search engines]]
[[Category:Articles created via the Article Wizard|Search engines]]</text>
      <sha1>06r1edv9lqr7ni39lukk8l21ov48ztd</sha1>
    </revision>
  </page>
  <page>
    <title>TREX search engine</title>
    <ns>0</ns>
    <id>13179109</id>
    <revision>
      <id>733031596</id>
      <parentid>730352803</parentid>
      <timestamp>2016-08-04T22:21:15Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* top */ template ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3263" xml:space="preserve">'''TREX''' is a search engine in the [[NetWeaver|SAP NetWeaver]] integrated technology platform produced by [[SAP AG]] using [[columnar storage]].&lt;ref&gt;{{cite journal|url=http://db.csail.mit.edu/pubs/abadi-column-stores.pdf|doi=10.1561/1900000024|title=The Design and Implementation of Modern Column-Oriented Database Systems|author1=Daniel Abadi|author2=Peter Boncz|author3=Stavros Harizopoulos|author4=Stratos Idreos|author5=Samuel Madden|journal=Foundations and Trends in Databases|volume=5|issue=3|year=2012|pages=197&#8211;280}}&lt;/ref&gt; The TREX engine is a standalone component that can be used in a range of system environments but is used primarily as an integral part of such SAP products as Enterprise Portal, Knowledge Warehouse, and '''Business Intelligence (BI, formerly [[SAP Business Information Warehouse]]).''' In SAP NetWeaver BI, the TREX engine powers the BI Accelerator, which is a plug-in appliance for enhancing the performance of [[online analytical processing]]. The name "TREX" stands for '''Text Retrieval and information EXtraction''', but it is not a registered trade mark of SAP and is not used in marketing collateral.

==Search functions==

TREX supports various kinds of text search, including exact search, boolean search, wildcard search, linguistic search (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an index term are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency ([[tf-idf]]) weighting, and results can include snippets with the search terms highlighted.

TREX supports text mining and classification using a [[vector space model]]. Groups of documents can be classified using query based classification, example based classification, or a combination of these plus keyword management.

TREX supports structured data search not only for document metadata but also for mass business data and data in SAP [[Business Objects]]. Indexes for structured data are implemented compactly using [[data compression]] and the data can be aggregated in linear time, to enable large volumes of data to be processed entirely in memory.

Recent developments include:
* A join engine to join structured data from different fields in business objects
* A fast update capability to write a delta index beside a main index and to merge them offline while a second delta index takes updates
* A [[data mining]] feature pack for advanced mathematical analysis

==History==

The first code for the engine was written in 1998 and TREX became an SAP component in 2000. The SAP NetWeaver BI Accelerator was first rolled out in 2005. As of Q1 2013, the current release of TREX is SAP NW 7.1.

==References==
{{Reflist}}

==External links==
* [http://www.sap.com/platform/netweaver/index.epx SAP NetWeaver]
* [http://www.sap.com/platform/netweaver/components/bi/index.epx SAP NetWeaver Business Intelligence]
* [http://www.sap.com/platform/netweaver/businessinformation.epx SAP NetWeaver Business Information Management]
* [http://scn.sap.com/docs/DOC-8489 Search and Classification (TREX) on SAP Community Network]

[[Category:SAP NetWeaver]]
[[Category:Information retrieval systems]]
[[Category:Business intelligence]]</text>
      <sha1>5pmkjw02zj6upmut7ig1zdm678dyz0k</sha1>
    </revision>
  </page>
  <page>
    <title>Poliqarp</title>
    <ns>0</ns>
    <id>2398780</id>
    <revision>
      <id>666861518</id>
      <parentid>607634522</parentid>
      <timestamp>2015-06-14T05:53:44Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="943" xml:space="preserve">'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].

==Features==
* Custom [[query language]]
* Two-level [[regular expressions]]:
** operating at the level of characters in words
** operating at the level of words in statements/paragraphs
* Good performance
* Compact corpus representation (compared to similar projects)
* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]
* Lack of portability across [[endianness]] (current release works only on little endian devices)

==External links==
* [http://www.korpus.pl/index.php?lang=en&amp;page=welcome Polish corpus website (in English)]
* [http://poliqarp.sourceforge.net/ Project website on SourceForge]
* [http://poliqarp.suxx.pl/ Search plugin for Firefox]

[[Category:Information retrieval systems]]</text>
      <sha1>j87cyjiu7n2wkh0vayn6d2n7vnyzlji</sha1>
    </revision>
  </page>
  <page>
    <title>Relevance (information retrieval)</title>
    <ns>0</ns>
    <id>442684</id>
    <revision>
      <id>758504944</id>
      <parentid>758504623</parentid>
      <timestamp>2017-01-05T21:09:20Z</timestamp>
      <contributor>
        <username>Nihiltres</username>
        <id>236191</id>
      </contributor>
      <minor />
      <comment>Added "(disambiguation)" suffix in hatnote per [[WP:INTDAB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9308" xml:space="preserve">{{other uses|Relevance (disambiguation)}}

In [[information science]] and [[information retrieval]], '''relevance''' denote how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.

== History ==

The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.{{citation needed|date=June 2015}}

The formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.&lt;ref&gt;Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810&#8208;832.&lt;/ref&gt;

Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".{{citation needed|date=June 2015}}

== Evaluation ==
{{main article|Information retrieval#Performance and correctness measures}}

The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.{{citation needed|date=June 2015}}

In order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.

In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance.{{citation needed|date=June 2015}} These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).

== Clustering and relevance ==

The [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.&lt;ref name=diazthesis&gt;F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.&lt;/ref&gt;    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:
* cluster-based information retrieval&lt;ref name=croftcbir&gt;W. B. Croft, &#8220;A model of cluster searching based on classification,&#8221; Information Systems, vol. 5, pp. 189&#8211;195, 1980.&lt;/ref&gt;&lt;ref name=griffithscbir&gt;A. Griffiths, H. C. Luckhurst, and P. Willett, &#8220;Using interdocument similarity information in document retrieval systems,&#8221; Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3&#8211;11, 1986.&lt;/ref&gt;
* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.&lt;ref name=lmcbir&gt;X. Liu and W. B. Croft, &#8220;Cluster-based retrieval using language models,&#8221; in SIGIR &#8217;04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186&#8211;193, ACM Press, 2004.&lt;/ref&gt;    It is important to ensure that clusters &#8211; either in isolation or combination &#8211; successfully model the set of possible relevant documents.

A second interpretation, most notably advanced by Ellen Voorhees,&lt;ref name=voorheescbir&gt;E. M. Voorhees, &#8220;The cluster hypothesis revisited,&#8221; in SIGIR &#8217;85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188&#8211;196, ACM Press, 1985.&lt;/ref&gt;    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,
* multiple cluster retrieval&lt;ref name=griffithscbir/&gt;&lt;ref name=voorheescbir/&gt;
* spreading activation&lt;ref name=preece&gt;S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.&lt;/ref&gt; and relevance propagation&lt;ref name=relprop&gt;T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, &#8220;A study of relevance propagation for web search,&#8221; in SIGIR &#8217;05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408&#8211;415, ACM Press, 2005.&lt;/ref&gt; methods
* local document expansion&lt;ref name=docexpansion&gt;A. Singhal and F. Pereira, &#8220;Document expansion for speech retrieval,&#8221; in SIGIR &#8217;99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34&#8211;41, ACM Press, 1999.&lt;/ref&gt;
* score regularization&lt;ref name=diazreg&gt;F. Diaz, &#8220;Regularizing query-based retrieval scores,&#8221; Information Retrieval, vol. 10, pp. 531&#8211;562, December 2007.&lt;/ref&gt;
Local methods require an accurate and appropriate document similarity measure.

==Problems and alternatives==

The documents which are most relevant are not necessarily those which are most useful to display in the first page of search results.  For example, two duplicate documents might be individually considered quite relevant, but it is only useful to display one of them.  A measure called "maximal marginal relevance" (MMR) has been proposed to overcome this shortcoming. It considers the relevance of each document only in terms of how much new information it brings given the previous results.&lt;ref&gt;{{cite journal|last1=Carbonell|first1=Jaime|last2=Goldstein|first2=Jade|title=The use of MMR, diversity-based reranking for reordering documents and producing summaries|journal=Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval|date=1998|doi=10.1145/290941.291025|url=http://dl.acm.org/citation.cfm?id=291025}}&lt;/ref&gt;

In some cases, a query may have an ambiguous interpretation, or a variety of potential responses.  Providing a diversity of results can be a consideration when evaluating the utility of a result set.&lt;ref&gt;http://www.dcs.gla.ac.uk/workshops/ddr2012/&lt;/ref&gt;

==References==
 {{reflist}}

==Additional reading==
*Hj&#248;rland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.
*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9
*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])
*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])
*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])
*Introduction to Information Retrieval: Evaluation. Stanford. ([http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf presentation in PDF])

[[Category:Information retrieval evaluation]]</text>
      <sha1>eaqe0iui82ul4m0yk21s6hphf010uuf</sha1>
    </revision>
  </page>
  <page>
    <title>Champion list</title>
    <ns>0</ns>
    <id>26304039</id>
    <revision>
      <id>729017937</id>
      <parentid>729017905</parentid>
      <timestamp>2016-07-09T07:52:28Z</timestamp>
      <contributor>
        <username>David.moreno72</username>
        <id>16075528</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/113.193.171.202|113.193.171.202]] ([[User talk:113.193.171.202|talk]]) ([[WP:HG|HG]]) (3.1.19)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="579" xml:space="preserve">{{orphan|date=January 2011}}

A '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].

[[Category:Information retrieval evaluation]]


{{computing-stub}}</text>
      <sha1>i9u628mnj62etw7iiqtpwor0y7f1eno</sha1>
    </revision>
  </page>
  <page>
    <title>Mean reciprocal rank</title>
    <ns>0</ns>
    <id>11184711</id>
    <revision>
      <id>740090554</id>
      <parentid>723517997</parentid>
      <timestamp>2016-09-19T00:53:39Z</timestamp>
      <contributor>
        <ip>1.186.132.102</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2340" xml:space="preserve">{{Refimprove|date=June 2007}}
The '''mean reciprocal rank''' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:&lt;ref&gt;{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&amp;ndash;82}}&lt;/ref&gt;

:&lt;math&gt; \text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}. \!&lt;/math&gt;

where &lt;math&gt; \text{rank}_i&lt;/math&gt; refers to the rank position of the ''first'' relevant document for the ''i''-th query.

The reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.

== Example ==
For example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:

{| class="wikitable"
|-
! Query
! Results
! Correct response
! Rank
! Reciprocal rank
|-
| cat
| catten, cati, '''cats'''
| cats
| 3
| 1/3
|-
|tori
| torii, '''tori''', toruses
| tori
| 2
| 1/2
|-
| virus
| '''viruses''', virii, viri
| viruses
| 1
| 1
|}

Given those three samples, we could calculate the mean reciprocal rank as (1/3&amp;nbsp;+&amp;nbsp;1/2&amp;nbsp;+&amp;nbsp;1)/3 = 11/18 or about 0.61.

This basic definition does not specify what to do if none of the proposed results are correct, though reciprocal rank 0 could be used in this situation.  It also does not specify what do to if there are multiple correct answers in the list. In this case, [[Information retrieval#Mean average precision|mean average precision]] is a potential alternative metric.

==See also==
* [[Information retrieval]]
* [[Question answering]]

==References==
{{Reflist}}

==External links==
* {{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC |author1=D. R. Radev |author2=H. Qi |author3=H. Wu |author4=W. Fan |year=2002 }}

[[Category:Summary statistics]]
[[Category:Information retrieval evaluation]]</text>
      <sha1>r9qtxgr7ak2xbozt6i2cfaobhhtkn5b</sha1>
    </revision>
  </page>
  <page>
    <title>Cranfield experiments</title>
    <ns>0</ns>
    <id>20289869</id>
    <revision>
      <id>667052035</id>
      <parentid>667051957</parentid>
      <timestamp>2015-06-15T14:30:26Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>removed [[Category:Information retrieval]]; added [[Category:Information retrieval evaluation]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1161" xml:space="preserve">The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.&lt;ref&gt;Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.&lt;/ref&gt;&lt;ref&gt;Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.&lt;/ref&gt;&lt;ref&gt;Cleverdon, C. W., &amp; Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. 
&lt;/ref&gt;

They represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).

==See also==
*[[ASLIB]]
*[[Information history]]

==References==
{{Reflist}}

==External links==
* [http://ir.dcs.gla.ac.uk/resources/test_collections/cran/ Cranfield 1400 corpus]

[[Category:Experiments]]
[[Category:Information retrieval evaluation]]


{{database-stub}}</text>
      <sha1>ipyf44584c6axuscve827yocbtdwsxf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Ranking functions</title>
    <ns>14</ns>
    <id>19988453</id>
    <revision>
      <id>666714674</id>
      <parentid>608644061</parentid>
      <timestamp>2015-06-13T03:51:25Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="67" xml:space="preserve">[[Category:Information retrieval techniques]]
[[Category:Rankings]]</text>
      <sha1>3xe4cn7gysqdyvegjdpo0ba1bqx758r</sha1>
    </revision>
  </page>
  <page>
    <title>Vocabulary mismatch</title>
    <ns>0</ns>
    <id>36749242</id>
    <revision>
      <id>666715749</id>
      <parentid>665814962</parentid>
      <timestamp>2015-06-13T04:06:49Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>clean up, move to Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2638" xml:space="preserve">{{refimprove|date=June 2015}}
'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.

Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.&lt;ref&gt;Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.&lt;/ref&gt;  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].

The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)&lt;ref&gt;Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.&lt;/ref&gt; were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.&lt;ref name="cnf"&gt;Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.&lt;/ref&gt;

== Techniques that solve mismatch ==

* [[Stemming]]
* [[Full-text indexing]] instead of only indexing keywords or abstracts
* Indexing text on inbound links from other documents (or other social tagging
* [[Query expansion]].  A 2012 study by Zhao and Callan&lt;ref name="cnf"/&gt; using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].
* Translation-based models

== References ==

{{Reflist}}

[[Category:Linguistic research]]
[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]</text>
      <sha1>2lvjw6kza9z27499yrq0fv4wvmakz4u</sha1>
    </revision>
  </page>
  <page>
    <title>Thesaurus (information retrieval)</title>
    <ns>0</ns>
    <id>39000674</id>
    <revision>
      <id>708228933</id>
      <parentid>708217770</parentid>
      <timestamp>2016-03-04T11:31:49Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Refimprove section}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9190" xml:space="preserve">{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}

In the context of [[information retrieval]], a '''thesaurus''' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".&lt;ref&gt;ANSI &amp; NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11&lt;/ref&gt; The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.&lt;ref&gt;ANSI &amp; NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12&lt;/ref&gt;

A thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a &#8220;controlled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.&#8221;

A thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.

== History ==
Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a &#8220;thesaurus&#8221; (by analogy with the well known thesaurus developed by [[Peter Roget]]).&lt;ref&gt;Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.&lt;/ref&gt; The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.&lt;ref&gt;Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging &amp; Classification Quarterly'', 37 (3/4), 2004, p.5-21.&lt;/ref&gt;&lt;ref&gt;Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.&lt;/ref&gt;

The first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.
Hundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:
(a)	Extension from monolingual to multilingual capability; and 
(b)	Addition of a conceptually organized display to the basic alphabetical presentation.

Here we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:

* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)
* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)
* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)
* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)
* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985
* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)
* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.

The most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.&lt;ref&gt;Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.&lt;/ref&gt; Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.&lt;ref&gt;''[http://www.niso.org/schemas/iso25964/ ISO 25964 &#8211; the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.&lt;/ref&gt;

== Purpose ==
{{refimprove section|small=z|date=March 2016}}
In information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.

The [[Art and Architecture Thesaurus|Art &amp; Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN&#8217;s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.

== Structure ==
{{refimprove section|small=z|date=March 2016}}
Information retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, &#8220;citrus fruits&#8221; might be linked to the broader concept of &#8220;fruits&#8221;, and the narrower ones of &#8220;oranges&#8221;, &#8220;lemons&#8221;, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as &#8220;mad cow disease&#8221;, &#8220;bovine spongiform encephalopathy&#8221;, &#8220;BSE&#8221;, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.

== See also ==
* [[Controlled vocabulary]]
* [[ISO 25964]]
* [[Thesaurus]]

== References ==
{{Reflist}}

== External links ==
* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] 
* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]

[[Category:Information retrieval techniques]]
[[Category:Thesauri]]</text>
      <sha1>9xhiigcku4d2fjctrwju5z0oyf6eues</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Substring indices</title>
    <ns>14</ns>
    <id>33958933</id>
    <revision>
      <id>666717294</id>
      <parentid>548118218</parentid>
      <timestamp>2015-06-13T04:30:37Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="263" xml:space="preserve">{{cat main|Substring index}}

[[Category:String (computer science)]]
[[Category:Algorithms on strings]]
[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Information retrieval techniques]]
[[Category:Bioinformatics algorithms]]</text>
      <sha1>qokg2koub79fsbjqukwkrt1h46576xh</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Music information retrieval</title>
    <ns>14</ns>
    <id>46973988</id>
    <revision>
      <id>712094053</id>
      <parentid>666858897</parentid>
      <timestamp>2016-03-26T22:34:42Z</timestamp>
      <contributor>
        <username>Clusternote</username>
        <id>11739815</id>
      </contributor>
      <comment>+{{Commons category|Music information retrieval}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="136" xml:space="preserve">{{Commons category|Music information retrieval}}
[[Category:Information retrieval genres]]
[[Category:Information retrieval techniques]]</text>
      <sha1>oqd3qqqncv3osv5bs9g64xg9ce5kk5g</sha1>
    </revision>
  </page>
  <page>
    <title>Compound term processing</title>
    <ns>0</ns>
    <id>18046649</id>
    <revision>
      <id>679091134</id>
      <parentid>666859577</parentid>
      <timestamp>2015-09-02T11:39:51Z</timestamp>
      <contributor>
        <username>Dexbot</username>
        <id>16752040</id>
      </contributor>
      <minor />
      <comment>Bot: Deprecating [[Template:Cite doi]] and some minor fixes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5396" xml:space="preserve">'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications to perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.

Compound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.

== Techniques ==

In August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing.&lt;ref&gt;{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} The British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&amp;ETOC=RN]&lt;/ref&gt;

CLAMOUR is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information and statistics. CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.&lt;ref&gt;[http://webarchive.nationalarchives.gov.uk/20040117000117/statistics.gov.uk/methods_quality/clamour/default.asp] National Statistics CLAMOUR project&lt;/ref&gt;

== History ==

Techniques for probabilistic weighting of single word terms date back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Sp&#228;rck Jones]].&lt;ref&gt;{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Sp&#228;rck Jones | first2 = K. | authorlink2 = Karen Sp&#228;rck Jones}}&lt;/ref&gt; Robertson stated that the assumption of word independence is not justified and exists as a matter of mathematical convenience. His objection to the term independence is not a new idea, dating back to at least 1964 when H. H. Williams stated that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".&lt;ref&gt;{{cite journal|last=WILLIAMS |first=J.H. |title=Results of classifying documents with multiple discriminant functions |url=http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0612272 |journal= Statistical Association Methods for Mechanized Documentation, National Bureau of Standards |location=Washington |pp=217-224 |year=1965}}&lt;/ref&gt;

In 2004, Anna Lynn Patterson filed patents on "phrase-based searching in an information retrieval system"&lt;ref&gt;{{patent|US|20060031195}}&lt;/ref&gt; to which [[Google]] subsequently acquired the rights.&lt;ref&gt;[http://www.seobythesea.com/2012/02/google-acquires-cuil-patent-applications/ Google Acquires Cuil Patent Applications]&lt;/ref&gt;

== Adaptability ==

Statistical compound term processing is more adaptable than the process described by Patterson. Her process is targeted at searching the [[World Wide Web]] where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.

Statistical compound term processing is also more adaptable than the linguistic approach taken by the CLAMOUR project, which must consider the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.

== Applications ==
Compound Term Processing allows information retrieval applications, such as [[search engines]], to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.

Early search engines looked for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. A [[phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.

==See also==
* [[Concept Searching Limited]]
* [[Enterprise search]]
* [[Information retrieval]]

== References ==
{{Reflist|30em}}

==  External links ==

{{Natural Language Processing}}

{{DEFAULTSORT:Compound Term Processing}}
[[Category:Information retrieval techniques]]</text>
      <sha1>jkmp0jintzy646bfl7omxn6eyj3d3sf</sha1>
    </revision>
  </page>
  <page>
    <title>Anchor text</title>
    <ns>0</ns>
    <id>1225632</id>
    <revision>
      <id>754471045</id>
      <parentid>751323820</parentid>
      <timestamp>2016-12-12T21:07:08Z</timestamp>
      <contributor>
        <username>Andy028</username>
        <id>27064433</id>
      </contributor>
      <comment>Added a wiki link to Google Penguin update and a case study by Moz.com</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7577" xml:space="preserve">{{Use dmy dates|date=February 2013}}
The '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, web search engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],&lt;ref&gt;{{cite web|author1=Bader Aljaber |author2=Nicola Stokes |author3=James Bailey |author4=Jian Pei |url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}&lt;/ref&gt; and anchor text from documents linked in [[mind maps]] may be used too.&lt;ref&gt;Needs new reference link&lt;/ref&gt; [[File:Anchor text.png|thumb|Visual implementation of anchor text]]

==Overview==
Anchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:

:&lt;code&gt;&lt;nowiki&gt;&lt;a href="http://en.wikipedia.org/wiki/Main_Page"&gt;Wikipedia&lt;/a&gt;&lt;/nowiki&gt;&lt;/code&gt;

The anchor text in this example is "Wikipedia"; the longer, but vital, URL &lt;code&gt;&lt;nowiki&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/nowiki&gt;&lt;/code&gt; needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.

==Common misunderstanding of the concept==

This proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds significant [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}

==Search engine algorithms==
Anchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.&lt;ref name="Search Engine Watch 1"&gt;{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=
How the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}&lt;/ref&gt;

[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.&lt;ref&gt;{{cite web
|last=Fox
|first=Vanessa
|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html
|title=Get a more complete picture about how other sites link to you
|date=15 March 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= https://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
In the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.&lt;ref&gt;{{cite web
|last=Cutts
|first=Matt
|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html
|title=A quick word about Googlebombs
|date=25 January 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= https://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

In April 2012, Google announced in its March "[[Google Penguin|Penguin]]" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.&lt;ref&gt;{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}&lt;/ref&gt; Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.
.&lt;ref name="Search Engine Watch 2"&gt;{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=
Google Penguin Update: Impact of Anchor Text Diversity &amp; Link Relevancy|accessdate=6 July 2012}}&lt;/ref&gt;

However a 2016 study of anchor text influence across 16,000 keywords found that presence of exact and partial match anchor links continues to have a strong correlation with Google rankings.&lt;ref&gt;{{cite web|publisher=Ahrefs|url=https://ahrefs.com/blog/anchor-text|title=
Everything You Ever Wanted To Know About Anchor Text|accessdate=27 July 2016}}&lt;/ref&gt;

August 2016 study conducted by Moz, found that Exact and partial match domains can be affected by over optimization penalty since Google considers domain Brand and naked URL links as Exact match.&lt;ref&gt;{{Cite news|url=https://moz.com/ugc/case-study-the-interconnectedness-of-local-seo-and-exact-match-domains|title=Case Study: The Interconnectedness of Local SEO and Exact Match Domains|newspaper=Moz|access-date=2016-12-12}}&lt;/ref&gt;

==Terminology==
There are different classifications of anchor text that are used within the search engine optimization community such as the following:

;Exact Match: an anchor that is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it's linking to a page about "search engine optimization.
;Branded: a brand that is used as the anchor. "[[Wikipedia]]" is a branded anchor text.
;Naked Link: a URL that is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.
;Generic: a generic word or phrase that is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.
;Images: whenever an image is linked, Google will use the "ALT" tag as the anchor text.

==References==

{{reflist|colwidth=30em}}

[[Category:Information retrieval techniques]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]
[[Category:Search engine optimization]]
[[Category:Hypertext]]</text>
      <sha1>iwzmwcuze067dqbedqj64brbydkmzf7</sha1>
    </revision>
  </page>
  <page>
    <title>Probabilistic relevance model</title>
    <ns>0</ns>
    <id>25959000</id>
    <revision>
      <id>714750470</id>
      <parentid>711593428</parentid>
      <timestamp>2016-04-11T16:51:45Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval techniques since this is apparently helpful to deriving the algorithm, not evaluating the performance of an algorithm?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2366" xml:space="preserve">
The '''probabilistic relevance model'''&lt;ref&gt;{{citation | author=S. E. Robertson and  K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129&#8211;146 | date=May&#8211;June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}&lt;/ref&gt;&lt;ref name="robertson2009"&gt;{{Cite journal | author=Stephen Robertson and Hugo Zaragoza | title=The Probabilistic Relevance Framework: BM25 and Beyond | date=2009 | url=http://dl.acm.org/citation.cfm?id=1704810 | publisher=Found. Trends Inf. Retr. | volume=3 | issue=4 | pages=333-389 | doi=10.1561/1500000019 }}&lt;/ref&gt; was devised by Robertson and Jones as a framework for [[Statistical model | probabilistic models]] to come. It is a formalism of [[information retrieval]] useful to derive [[ranking function]]s used by [[search engine]]s and  [[web search engine]]s in order to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query.
 
It makes an estimation of the probability of finding if a document ''d&lt;sub&gt;j&lt;/sub&gt;'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.

&lt;math&gt;sim(d_{j},q) = \frac{P(R|\vec{d}_j)}{P(\bar{R}|\vec{d}_j)}&lt;/math&gt;

==Related models==
There are some limitations to this framework that need to be addressed by further development:
* There is no accurate estimate for the first run probabilities
* Index terms are not weighted
* Terms are assumed mutually independent

To address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and its BM25F brother.

==References==
{{reflist}}

[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]</text>
      <sha1>0y38brzfanbc2ha3sikzt2f15ee8gsu</sha1>
    </revision>
  </page>
  <page>
    <title>XML retrieval</title>
    <ns>0</ns>
    <id>21106742</id>
    <revision>
      <id>747606906</id>
      <parentid>738982032</parentid>
      <timestamp>2016-11-03T09:26:36Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <minor />
      <comment>change |id={{citeseerx}} to |citeseerx=</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6546" xml:space="preserve">{{Multiple issues|
{{expert-subject|Computer science|date=January 2015}}
{{COI|date=February 2009}}
}}

'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.&lt;ref&gt;{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}&lt;/ref&gt;

==Queries==
Most XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.

==Exploiting XML structure==
Taking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.

==Ranking==
Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.&lt;ref name="INEX2006"&gt;{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |title=Overview of INEX 2006 |last=Malik |first=Saadia |author2=Trotman, Andrew |author3=Lalmas, Mounia |author4=Fuhr, Norbert |year=2007 |work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081016101202/http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |archivedate=October 16, 2008 }}&lt;/ref&gt;

==Existing XML search engines==
An overview of two potential approaches is available.&lt;ref&gt;{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}&lt;/ref&gt;&lt;ref&gt;{{Cite paper|citeseerx = 10.1.1.109.5986|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR }}&lt;/ref&gt; The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.&lt;ref name="INEX2006" /&gt; Three different areas influence XML-Retrieval:&lt;ref name="INEX2002"&gt;{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |title=INEX: Initiative for the Evaluation of XML Retrieval |last=Fuhr |first=Norbert |author2=G&#246;vert, N. |author3=Kazai, Gabriella |author4=Lalmas, Mounia |year=2003 |work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002 |publisher=ERCIM Workshop Proceedings, France |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081121135758/http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |archivedate=November 21, 2008 }}&lt;/ref&gt;

===Traditional XML query languages===
[[Query language]]s such as the [[W3C]] standard [[XQuery]]&lt;ref&gt;{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fern&#225;ndez, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Sim&#233;on, J&#233;r&#244;me |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}&lt;/ref&gt; supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].&lt;ref name="Schlieder2002"&gt;{{Cite journal|url=http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |title=Querying and Ranking XML Documents |last=Schlieder |first=Torsten |author2=Meuss, Holger |year=2002 |work=Journal of the American Society for Information Science and Technology, Vol. 53, No. 6 |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |archivedate=June 10, 2007 }}&lt;/ref&gt;

===Databases===
Classic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]&lt;ref name="INEX2002" /&gt; and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.

===Information retrieval===
Classic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.&lt;ref name="Schlieder2002"/&gt; They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.&lt;ref&gt;{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}&lt;/ref&gt;

==See also==
*[[Document retrieval]]
*[[Information retrieval applications]]

==References==
{{Reflist}}

{{DEFAULTSORT:Xml-Retrieval}}
[[Category:XML]]
[[Category:Information retrieval genres]]</text>
      <sha1>1nr3r6p6oad13cuska25jwgg8h2so63</sha1>
    </revision>
  </page>
  <page>
    <title>Legal information retrieval</title>
    <ns>0</ns>
    <id>24997830</id>
    <revision>
      <id>726948118</id>
      <parentid>723242992</parentid>
      <timestamp>2016-06-25T14:36:00Z</timestamp>
      <contributor>
        <username>Krauss</username>
        <id>1222358</id>
      </contributor>
      <comment>/* Notes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14823" xml:space="preserve">'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 1&lt;/ref&gt; Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.&lt;ref name=Jackson&gt;Jackson et al., p. 60&lt;/ref&gt; Legal information retrieval is a part of the growing field of [[legal informatics]].

== Overview ==

In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,&lt;ref name="Blair, D.C. 1985, p.293"&gt;Blair, D.C., and Maron, M.E., 1985, p.293&lt;/ref&gt; meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.&lt;ref name="Blair, D.C. 1985, p.293"/&gt; This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.&lt;ref&gt;American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html&lt;/ref&gt;

Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],&lt;ref&gt;Peters, W. et al. 2007, p. 118&lt;/ref&gt; [[polysemes]]&lt;ref&gt;Peters, W. et al. 2007, p. 130&lt;/ref&gt; (words that have different meanings when used in a legal context), and constant change.

Techniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.

== Problems ==

Application of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].&lt;ref name=LOIS1&gt;Peters, W. et al. 2007, p. 120&lt;/ref&gt; Instead, the law is generally filled with open-ended terms, which may change over time.&lt;ref name=LOIS1 /&gt; This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.&lt;ref&gt;Saravanan, M. et al.  2009, p. 101&lt;/ref&gt;

Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:&lt;ref name="Peters, W. et al. 2007, p. 131"&gt;Peters, W. et al. 2007, p. 131&lt;/ref&gt;

#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.
#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;
#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;
#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;

In addition, it also has the common meaning: 
&lt;ol start="5"&gt;
&lt;li&gt;A person who works at a specific occupation.&lt;ref name="Peters, W. et al. 2007, p. 131"/&gt; &lt;/li&gt;
&lt;/ol&gt;

Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results.

Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.&lt;ref name=MaxwellA &gt;Maxwell, K.T., and Schafer, B. 2008, p. 8&lt;/ref&gt; Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.&lt;ref name=MaxwellA  /&gt; The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).&lt;ref name=MaxwellA  /&gt; A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.

Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.&lt;ref name=MaxwellA  /&gt; He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.&lt;ref name=MaxwellA /&gt;

Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day&lt;ref name=Jackson /&gt;), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.&lt;ref name=Jackson /&gt;&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2007, p.1&lt;/ref&gt;

== Techniques ==

===Boolean searches===

[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above.

The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.&lt;ref name="Blair, D.C. 1985, p.293"/&gt; Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.&lt;ref&gt;Saravanan M., et al. 2009, p. 116&lt;/ref&gt;

===Manual classification===

In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.&lt;ref name="Maxwell, K.T. 2008, p. 2"&gt;Maxwell, K.T., and Schafer, B. 2008, p. 2&lt;/ref&gt; These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s &#8220;Natural Language&#8221;&lt;ref name=WL&gt;Westlaw Research, http://www.westlaw.com&lt;/ref&gt; or [[LexisNexis]]' Headnote&lt;ref name=LN&gt;Lexis Research, http://www.lexisnexis.com&lt;/ref&gt; searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers&lt;ref name=WL /&gt; or Lexis' Headnotes.&lt;ref name=LN /&gt; Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).&lt;ref name="Maxwell, K.T. 2008, p. 2"/&gt;

These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.&lt;ref name="Maxwell, K.T. 2008, p. 3"&gt;Maxwell, K.T., and Schafer, B. 2008, p. 3&lt;/ref&gt; In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.&lt;ref&gt;Saravanan, M. et al.  2009, p. 116&lt;/ref&gt; The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.&lt;ref&gt;Saravanan, M. et al. 2009, p. 103&lt;/ref&gt;

The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.&lt;ref name="Maxwell, K.T. 2008, p. 3"/&gt;&lt;ref&gt;Schweighofer, E. and Liebwald, D. 2008, p. 108&lt;/ref&gt; As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2008, p. 4&lt;/ref&gt;

===Natural language processing===

In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.&lt;ref name=Jackson /&gt;&lt;ref name=AshleyA&gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 125&lt;/ref&gt;&lt;ref name=Gelbart&gt;Gelbart, D. and Smith, J.C. 1993, p. 142&lt;/ref&gt; Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,&lt;ref name=Jackson /&gt;&lt;ref name=AshleyA /&gt;&lt;ref name=Gelbart /&gt; few have reported results. One system, &#8220;SMILE,&#8221; which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).&lt;ref name=AshleyB &gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 159&lt;/ref&gt; This is probably much lower than an acceptable rate for general usage.&lt;ref name=AshleyB /&gt;&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 3&lt;/ref&gt;

Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 9&lt;/ref&gt;&lt;ref&gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 126&lt;/ref&gt;

== List of retrieval systems ==
Free-to-use law-texts and associated oficial metadata:

* [[LexML Brazil]]
* [http://www.legislation.gov.uk/ legislation.gov.uk]
* [[EUR-Lex#N-Lex|N-Lex]]
* ...

== Notes ==
{{Reflist|2}}

==References==
{{Refbegin}}
*{{cite journal
|author1=Maxwell, K.T. |author2=Schafer, B.
|year       = 2008
|title      = Concept and Context in Legal Information Retrieval
|url        = http://portal.acm.org/citation.cfm?id=1564016
|journal    = Frontiers in Artificial Intelligence and Applications
|volume     = 189
|pages      = 63&#8211;72
|publisher  = IOS Press
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Jackson, P.|year       = 1998
|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation
|url        = http://portal.acm.org/citation.cfm?id=288627.288642
|journal    = Conference on Information and Knowledge Management
|pages      = 60&#8211;67
|publisher  = ACM
|accessdate = 2009-11-07
|display-authors=etal}}
*{{cite journal
|author1=Blair, D.C. |author2=Maron, M.E.
|year       = 1985
|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval
|url        = http://portal.acm.org/citation.cfm?id=3166.3197&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=61732097&amp;CFTOKEN=95519997
|journal    = Communications of the ACM
|volume     = 28
|issue      = 3 
|pages      = 289&#8211;299
|publisher  = ACM
|accessdate = 2009-11-07
|doi=10.1145/3166.3197
}}
*{{cite journal
|author     = Peters, W.|year       = 2007
|title      = The structuring of legal knowledge in LOIS
|url        = http://www.springerlink.com/content/d04l7h2507700g45/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 117&#8211;135
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9034-4
|display-authors=etal}}
*{{cite journal
|author     = Saravanan, M.|year       = 2007
|title      = Improving legal information retrieval using an ontological framework 
|url        = http://www.springerlink.com/content/h66412k08h855626/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 101&#8211;124
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9075-y
|display-authors=etal}}
*{{cite journal
|author1=Schweighofer, E.  |author2=Liebwald, D.
|year       = 2007
|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary
|url        = http://www.springerlink.com/content/v62v7131x10413v0/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 103&#8211;115
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9029-1
}}
*{{cite journal
|author1=Gelbart, D.  |author2=Smith, J.C.
|year       = 1993
|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management
|url        = http://portal.acm.org/citation.cfm?id=158994
|journal    = International Conference on Artificial Intelligence and Law
|pages      = 142&#8211;151
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author1=Ashley, K.D.  |author2=Bruninghaus, S.
|year       = 2009
|title      = Automatically classifying case texts and predicting outcomes
|url        = http://www.springerlink.com/content/lhg8837331hgu024/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 125&#8211;165
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9077-9
}}
{{Refend}}

{{DEFAULTSORT:Legal Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Natural language processing]]
[[Category:Legal research]]</text>
      <sha1>euemo8s2euykm0uhtam4h7l4h8hxaw2</sha1>
    </revision>
  </page>
  <page>
    <title>Human&#8211;computer information retrieval</title>
    <ns>0</ns>
    <id>14473878</id>
    <revision>
      <id>751208472</id>
      <parentid>721731904</parentid>
      <timestamp>2016-11-24T02:02:06Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* What is HCIR? */ punct., simplify heading</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11019" xml:space="preserve">'''Human-computer information retrieval''' ('''HCIR''') is the study and engineering of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. It combines the fields of [[human-computer interaction]] (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback.

== History ==

This term ''human&#8211;computer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.&lt;ref name=march2006&gt;[http://www.asis.org/Bulletin/Jun-06/marchionini.html Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science]&lt;/ref&gt; Marchionini&#8217;s main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."

In 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human&#8211;computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] &#8211; changes that were only embryonic in the late 1990s.

A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].

== Description ==

HCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."&lt;ref name=ingwer1992&gt;[http://vip.db.dk/pi/iri/index.htm Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham.]&lt;/ref&gt;

A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.&lt;ref&gt;{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}&lt;/ref&gt;

Most modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document's [[relevance]] to the query.&lt;ref&gt;Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. &lt;/ref&gt; In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).

Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models &#8211; one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]'s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information retrieval#Precision|precision]] and [[Information retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.&lt;ref name=koene1996&gt;[http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13&#8211;18, 1996). M. J. Tauber, Ed. CHI '96. ACM Press, New York, NY, 205-212]&lt;/ref&gt; Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.&lt;ref name=borlund2003&gt;[http://informationr.net/ir/8-3/paper152.html Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152]&lt;/ref&gt;

== Goals ==
HCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results.&lt;ref name=march2006/&gt;&lt;ref name=ipm2013&gt;[https://dl.acm.org/citation.cfm?id=2504017 White, R., Capra, R., Golovchinsky, G., Kules, B., Smith, C., and Tunkelang, D. (2013). Introduction to Special Issue on Human-computer Information Retrieval. Journal of Information Processing and Management 49(5), 1053-1057]&lt;/ref&gt;

Systems should
*no longer only deliver the relevant documents, but must also provide semantic information along with those documents
*increase user responsibility as well as control; that is, information systems require human intellectual effort
*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases
*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services
*support the entire information life cycle (from creation to preservation) rather than only the dissemination or use phase
*support tuning by end users and especially by information professionals who add value to information resources
*be engaging and fun to use

In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process &#8212; the [[information professional]] &#8212; to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).

== Techniques ==

The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.

Many [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user&#8217;s hands.

[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.&lt;ref&gt;Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.&lt;/ref&gt;

[[Combinatorial search#Lookahead|Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.

[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.&lt;ref&gt;Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.&lt;/ref&gt;

Summarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].

[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].

== Related Areas ==
* [[Exploratory Video Search]]

== References ==

&lt;References/&gt;

==External links==
*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}
*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}

{{DEFAULTSORT:Human-computer information retrieval}}
[[Category:Information retrieval genres]]
[[Category:Human&#8211;computer interaction]]</text>
      <sha1>38ge7ic9rsip6ee4nj1zzxb1e9q2jo0</sha1>
    </revision>
  </page>
  <page>
    <title>Full-text search</title>
    <ns>0</ns>
    <id>1315248</id>
    <revision>
      <id>760868003</id>
      <parentid>760866952</parentid>
      <timestamp>2017-01-19T15:11:03Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <comment>remove red links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12773" xml:space="preserve">{{Multiple issues|
{{refimprove|date=August 2012}}
{{cleanup|date=September 2009}}
}}

In [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).

In a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.&lt;ref&gt;In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.&lt;/ref&gt;

==Indexing==
When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "[[Serial memory processing|serial scanning]]". This is what some tools, such as [[grep]], do when searching.

However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.&lt;ref name="Capabilities of Full Text Search System "&gt;[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{webarchive |url=https://web.archive.org/web/20101223192214/http://www.lucidimagination.com/full-text-search |date=December 23, 2010 }}&lt;/ref&gt;

The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive".

==The precision vs. recall tradeoff==
[[Image:Full-text-search-results.png|150px|thumb|right|Diagram of a low-precision, low-recall search]]
Recall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned.

The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only 1 relevant result of 3 possible relevant results was returned, so the recall is a very low ratio of 1/3, or 33%. The precision for the example is a very low 1/4, or 25%, since only 1 of the 4 results returned was relevant.&lt;ref name="isbn1430215941"&gt;{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}&lt;/ref&gt;

Due to the ambiguities of [[natural language]], full-text-search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision.&lt;ref name="YuwonoLee"&gt;{{Cite conference | first = Yuwono | last = B. |author2=Lee, D. L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}&lt;/ref&gt;

{{See also|Precision and recall}}

==False-positive problem==

Free text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).

Clustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "bank", clustering can be used to categorize the document/data universe into "financial institution", "place to sit", "place to store" etc. Depending on the occurrences of words relevant to the categories, search terms or a search result can be placed in one or more of the categories. This technique is being extensively deployed in the [[Electronic discovery|e-discovery]] domain.{{clarify|date=January 2012}}

==Performance improvements==

The deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.

===Improved querying tools===

*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.
* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."
* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, &lt;tt&gt;"encyclopedia" [[Logical conjunction|AND]] "online" [[Negation|NOT]] "Encarta"&lt;tt&gt;) can dramatically increase the precision of a free text search. The &lt;tt&gt;AND&lt;/tt&gt; operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The &lt;tt&gt;NOT&lt;/tt&gt; operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the &lt;tt&gt;OR&lt;/tt&gt; operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, &lt;tt&gt;"encyclopedia" AND "online" [[Logical disjunction|OR]] "Internet" NOT "Encarta"&lt;/tt&gt;. This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.&lt;ref&gt;Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]&lt;/ref&gt;
* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as &lt;tt&gt;"Wikipedia, the free encyclopedia."&lt;/tt&gt;
* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.
* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.
* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for &lt;tt&gt;"Wikipedia" WITHIN2 "free"&lt;tt&gt; would retrieve only those documents in which the words &lt;tt&gt;"Wikipedia" and "free"&lt;/tt&gt; occur within two words of each other.
* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.
* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)
* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example, using the asterisk in a search query &lt;tt&gt;"s*n"&lt;/tt&gt; will find "sin", "son", "sun", etc. in a text.

===Improved search algorithms===
The [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.&lt;ref&gt;{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt; See [[Search engine]] for additional examples.

==Software==

The following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.

{{col-float}}

=== Free and open source software ===
&lt;!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
* [[BaseX]]
* [[Clusterpoint|Clusterpoint Database]]
* [[Elasticsearch]]
* [[Ht-//Dig|ht://Dig]]
* [[KinoSearch]]
* [[Lemur Project|Lemur/Indri]]
* [[Lucene]]
* [[mnoGoSearch]]
* [[Searchdaimon]]
* [[Sphinx (search engine)|Sphinx]]
* [[Swish-e]]
* [[Xapian]]
* [[Apache Solr]]

{{col-float-break}}

=== Proprietary software ===
&lt;!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
* [[Algolia]]
* [[Autonomy Corporation]]
* [[Azure Search]]
* [[Bar Ilan Responsa Project]]
* [[Brainware]]
* [[BRS/Search]] 
* [[Concept Searching Limited]]
* [[Dieselpoint]]
* [[dtSearch]]
* [[Endeca]]
* [[Exalead]]
* [[Funnelback]]
* [[Fast Search &amp; Transfer]]
* [[Inktomi (company)|Inktomi]]
* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)
* [[Lucid Imagination]]
* [[MarkLogic]]
* [[SAP HANA]]&lt;ref&gt;http://www.martechadvisor.com/news/databases-big-data/sap-adds-hanabased-software-packages-to-iot-portfolio/&lt;/ref&gt;
* [[Swiftype]]
* [[Thunderstone Software LLC.]]
* [[Viv&#237;simo]]
{{col-float-end}}

==Notes==
{{Reflist}}

==See also==
*[[Pattern matching]] and [[string matching]]
*[[Compound term processing]]
*[[Enterprise search]]
*[[Information extraction]]
*[[Information retrieval]]
*[[Faceted search]]
*[[List of enterprise search vendors]]
*[[WebCrawler]], first FTS engine
*[[Search engine indexing]] - how search engines generate indices to support full text searching

{{DEFAULTSORT:Full Text Search}}
[[Category:Text editor features]]
[[Category:Information retrieval genres]]</text>
      <sha1>5g8k8uk87xs5exxy8fhi4oaeeyjin4d</sha1>
    </revision>
  </page>
  <page>
    <title>Maarten de Rijke</title>
    <ns>0</ns>
    <id>31200516</id>
    <revision>
      <id>750205266</id>
      <parentid>719731370</parentid>
      <timestamp>2016-11-18T09:17:51Z</timestamp>
      <contributor>
        <username>1Veertje</username>
        <id>12358798</id>
      </contributor>
      <comment>+img</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3080" xml:space="preserve">[[File:Maarten de Rijke - CLEF 2011 (cropped).jpg|thumb|Maarten de Rijke, 2011]]'''Maarten de Rijke''' (born 1 August 1961) is a [[Netherlands|Dutch]] computer scientist. His work initially focused on [[modal logic]] and [[knowledge representation]], but since the early years of the 21st century he has worked mainly in [[information retrieval]]. His work is supported by grants from the [[Nederlandse Organisatie voor Wetenschappelijk Onderzoek]] (NWO), public-private partnerships, and the European Commission (under the Sixth and Seventh Framework programmes).


==Biography==
Maarten de Rijke was born in [[Vlissingen]].  He studied philosophy (MSc 1989) and mathematics (MSc 1990) and wrote a PhD thesis, defended in 1993, on extended modal logics, under the supervision of [[Johan van Benthem (logician)|Johan van Benthem]].

De Rijke worked as a postdoc at the [[Centrum Wiskunde &amp; Informatica]], before becoming a Warwick Research Fellow at the [[University of Warwick]]. He joined the [[University of Amsterdam]] in 1998, and was appointed professor of Information Processing and Internet at the [[Informatics Institute]] of the University of Amsterdam in 2004.&lt;ref name="MdR11Bio"&gt;[http://staff.science.uva.nl/~mdr/Bio/ Bio of Maarten de Rijke] at the University of Amsterdam. Retrieved 16 March 2011.&lt;/ref&gt;

He leads the Information and Language Processing group&lt;ref&gt;[http://ilps.science.uva.nl Information and Language Processing group]&lt;/ref&gt; at the University of Amsterdam, the Intelligent Systems Lab Amsterdam&lt;ref&gt;[http://isla.science.uva.nl Intelligent Systems Lab Amsterdam] within the Informatics Institute of the University of Amsterdam.&lt;/ref&gt; and the Center for Creation, Content and Technology.&lt;ref&gt;[http://www.ccct.uva.nl Center for Creation, Content and Technology] at the University of Amsterdam.&lt;/ref&gt;

==Work==
During the first ten years of his scientific career Maarten de Rijke worked on formal and applied aspects of modal logic. At the start of the 21st century, De Rijke switched to information retrieval. He has since worked on [[XML retrieval]], [[question answering]], [[expert finding]] and [[social media analysis]].

==Publications==
Maarten de Rijke has published more than 600 papers and books.&lt;ref name="MdR11Pubs"&gt;[http://staff.science.uva.nl/~mdr/Publications/ List of publications of Maarten de Rijke] at the University of Amsterdam". Retrieved 16 March 2011.&lt;/ref&gt;

==References==
{{reflist}}
*[http://albumacademicum.uva.nl/cgi/b/bib/bib-idx?type=simple;lang=en;c=ap;rgn1=entirerecord;q1=rijke;x=0;y=0;cc=ap;view=reslist;sort=achternaam;fmt=long;page=reslist;size=1;start=14 Prof. dr. M. de Rijke, 1961 -] at the [[University of Amsterdam]] ''Album Academicum'' website

==External links==
* [http://staff.science.uva.nl/~mdr Home page]

{{DEFAULTSORT:Rijke, Maarten De}}
[[Category:1961 births]]
[[Category:Living people]]
[[Category:Dutch computer scientists]]
[[Category:University of Amsterdam alumni]]
[[Category:University of Amsterdam faculty]]
[[Category:People from Vlissingen]]
[[Category:Information retrieval researchers]]</text>
      <sha1>98ri2nkzofz3isx1jtts5fy4tp59v1e</sha1>
    </revision>
  </page>
  <page>
    <title>W. Bruce Croft</title>
    <ns>0</ns>
    <id>24963451</id>
    <revision>
      <id>737978961</id>
      <parentid>733554283</parentid>
      <timestamp>2016-09-06T05:43:58Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* top */Removed invisible unicode characters + other fixes, replaced: &#8594;   (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3683" xml:space="preserve">'''W. Bruce Croft''' is a [[distinguished professor]] of [[computer science]] at the [[University of Massachusetts Amherst]] whose work focuses on [[information retrieval]].&lt;ref&gt;
{{Cite web
  | last = Croft
  | first = W. Bruce
  | title = Biography
  | url=http://ciir.cs.umass.edu/personnel/croftbio.pdf
  | accessdate = November 4, 2009}}&lt;/ref&gt;
He is the founder of the [[Center for Intelligent Information Retrieval]] and served as the editor-in-chief of [[ACM Transactions on Information Systems]] from 1995 to 2002.  He was also a member of the [[United States National Research Council|National Research Council]] [http://sites.nationalacademies.org/CSTB/index.htm Computer Science and Telecommunications Board] from 2000 to 2003. Since 2015, he is the Dean of the College of Information and Computer Sciences at the University of Massachusetts Amherst. He was Chair of the UMass Amherst Computer Science Department from 2001 to 2007.

Bruce Croft formed the [[Center for Intelligent Information Retrieval]] (CIIR) in 1991, since when he and his students have worked with more than 90 industry and government partners on research and technology projects and have produced more than 900 papers. Bruce Croft has made major contributions to most areas of information retrieval, including pioneering work in clustering, passage retrieval, sentence retrieval, and distributed search. One of the most important areas of work for Croft  relates to ranking functions and retrieval models, where he has led the development of one of the major approaches to modeling search: language modelling. In later years, Croft also led the way in the development of feature-based ranking functions. Croft and his research group have also developed a series of search engines: InQuery, the Lemur toolkit, Indri, and Galago. These search engines are open source and offer unique capabilities that are not replicated in other research retrieval platforms source &#8211; consequently they are downloaded by hundreds of researchers world wide. As a consequence of his work, Croft is one of the most cited researchers in information retrieval.

==Education==
Croft earned a bachelor's degree with honors in 1973 and a master's degree in computer science in 1974 from [[Monash University]] in [[Melbourne|Melbourne, Australia]].  He earned his Ph.D in computer science from the [[University of Cambridge]] in 1979 and joined the [[University of Massachusetts Amherst|University of Massachusetts, Amherst]] faculty later that year.

==Honors and awards==
Croft has received several prestigious awards, including:
* [[ACM Fellow]] in 1997
* [[American Society for Information Science and Technology]] Research Award in 2000
* [[Gerard Salton Award]] (a lifetime achievement award) from ACM SIGIR in 2003
* [[Tony Kent Strix award|Tony Kent Strix Award]] in 2013
* IEEE Computer Society Technical Achievement Award in 2014
* [http://sigir.org/awards/best-student-paper-awards/ Best Student Paper Award] from SIGIR in 1997 and 2005
* [http://sigir.org/awards/test-of-time-awards/ Test of Time Award] from SIGIR for his papers published in 1990, 1995, 1996, 1998, 2001
* Many other publications are short-listed as the Best Paper Award in SIGIR and CIKM

==References==
&lt;references/&gt;

==External links==
* [http://ciir.cs.umass.edu/personnel/croft.html Faculty homepage]

{{DEFAULTSORT:Croft, W. Bruce}}
[[Category:American computer scientists]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:University of Massachusetts Amherst faculty]]
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:Information retrieval researchers]]


{{Compu-bio-stub}}</text>
      <sha1>bch9e61g1i4m4aeq9fk1sbndjhb8ipv</sha1>
    </revision>
  </page>
  <page>
    <title>Cutter Expansive Classification</title>
    <ns>0</ns>
    <id>7515</id>
    <revision>
      <id>762354927</id>
      <parentid>762265323</parentid>
      <timestamp>2017-01-28T07:44:26Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14317" xml:space="preserve">The '''Cutter Expansive Classification''' system is a [[library classification]] system devised by [[Charles Ammi Cutter]]. The system was the basis for the top categories of the [[Library of Congress Classification]].&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 226.&lt;/ref&gt;

==History of the Expansive Classification==
[[Charles Ammi Cutter]] (1837&amp;ndash;1903), inspired by the decimal classification of his contemporary [[Melvil Dewey]], and with Dewey's initial encouragement, developed his own classification scheme for the Winchester Town Library and then the [[Boston Athenaeum]],&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 208.&lt;/ref&gt; at which he served as librarian for twenty-four years. He began work on it around the year 1880, publishing an overview of the new system in 1882. The same classification would later be used, but with a different notation, also devised by Cutter, at the [[Cary Memorial Library|Cary Library]] in [[Lexington, Massachusetts]].&lt;ref&gt;Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891&#8211;93, p. 1.&lt;/ref&gt;

Many libraries found this system too detailed and complex for their needs, and Cutter received many requests from librarians at small libraries who wanted the classification adapted for their collections. He devised the Expansive Classification in response, to meet the needs of growing libraries, and to address some of the complaints of his critics
.&lt;ref&gt;For the Expansive Classification as a response to Cutter's critics, see: Miksa, Francis L., ed. ''Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.
* For the Expansive Classification as a response to the growing needs of libraries, see Miksa, above, and also: LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 209.
* The above issues are also discussed by Cutter in his [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891&#8211;93.
&lt;/ref&gt; Cutter completed and published an introduction and schedules for the first six classifications of his new system ([https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']), but his work on the seventh was interrupted by his death in 1903.&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 210.&lt;/ref&gt;

The Cutter Expansive Classification, although adopted by comparatively few libraries, mostly in [[New England]]{{Citation needed|date=August 2011}}, has been called one of the most logical and scholarly of American classifications{{Citation needed|date=August 2011}}. Library historian Leo E. LaMontagne writes:

&lt;blockquote&gt;Cutter produced the best classification of the nineteenth century. While his system was less "scientific" than that of [[J. P. Lesley]], its other key features &#8211; notation, specificity, and versatility &#8211; make it deserving of the praise it has received.&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 215&lt;/ref&gt;&lt;/blockquote&gt;

Its top level divisions served as a basis for the Library of Congress classification, which also took over some of its features.&lt;ref&gt;LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, Connecticut, Shoe String Press. 1961, p. 226.&lt;/ref&gt; It did not catch on as did Dewey's system because Cutter died before it was completely finished, making no provision for the kind of development necessary as the bounds of knowledge expanded and scholarly emphases changed throughout the twentieth century.&lt;ref&gt;https://journals.ala.org/index.php/lrts/article/view/5419/6654&lt;/ref&gt;

==Structure of the Expansive Classification==
The Expansive Classification uses seven separate schedules, each designed to be used by libraries of different sizes. After the first, each schedule was an expansion of the previous one,&lt;ref&gt;Miksa, Francis L., ed. ''Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.&lt;/ref&gt; and Cutter provided instructions for how a library might change from one expansion to another as it grows.&lt;ref&gt;Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891&#8211;93, p. 21&#8211;23.&lt;/ref&gt;

==Summary of the Expansive Classification Schedules==

===First Classification===
The first classification is meant for only the very smallest libraries. The first classification has only seven top level classes, and only eight classes in all.

* '''A''' Works of reference and general works which include several of the following sections, and so could not go in any one.
* '''B''' [[Outline of philosophy|Philosophy]] and [[Outline of religion|Religion]]
* '''E''' Biography
* '''F''' [[Outline of history|History]] and [[Outline of geography|Geography]] and Travels
* '''H''' [[Outline of social science|Social sciences]]
* '''L''' [[Outline of natural science|Natural sciences]] and [[The arts|Arts]]
* '''Y''' [[Outline of linguistics|Language]] and [[Outline of literature|Literature]]
* '''YF''' [[Outline of fiction|Fiction]]

===Further Classifications===
Further expansions add more top level classes and subdivisions. Many subclasses arranged systematically, with common divisions, such as those by geography and language, following a consistent system throughout.&lt;ref&gt;https://archive.org/details/cu31924092476229&lt;/ref&gt;

By the fifth classification all the letters of the alphabet are in use for top level classes. These are:

* '''A''' General Works
* '''B''' [[Outline of philosophy|Philosophy]]
* '''C''' [[Outline of Christianity|Christianity]] and [[Outline of Judaism|Judaism]]
* '''D''' Ecclesiastical History
* '''E''' Biography
* '''F''' [[Outline of history|History]], Universal History
* '''G''' [[Outline of geography|Geography]] and Travels
* '''H''' [[Outline of social science|Social Sciences]]
* '''I''' Demotics, [[Outline of sociology|Sociology]]
* '''J''' Civics, Government, [[Outline of political science|Political Science]]
* '''K''' Legislation
* '''L''' [[Outline of science|Science]] and [[The arts|Arts]] together
* '''M''' Natural History
* '''N''' [[Outline of botany|Botany]]
* '''O''' [[Outline of zoology|Zo&#246;logy]]
* '''P''' [[Outline of anthropology|Anthropology]] and Ethnology
* '''Q''' [[Outline of medicine|Medicine]]
* '''R''' Useful arts, [[Outline of technology|Technology]]
* '''S''' Constructive arts ([[Outline of engineering|Engineering]] and [[Outline of construction|Building]])
* '''T''' [[Outline of manufacturing|Manufactures]] and Handicrafts
* '''U''' [[Outline of military science and technology|Art of War]]
* '''V''' Recreative arts, [[Outline of sports|Sports]], [[Outline of games|Games]], [[Outline of festivals|Festivals]]
* '''W''' [[Outline of the visual arts|Art]]
* '''X''' English Language
* '''Y''' English and American literature
* '''Z''' Book arts

These schedules were not meant to be fixed, but were to be adapted to meet the needs of each library. For example, books on the English language may be put in X, and books on language in general in a subclass of X, or this can be reversed. The first option is less logical, but results in shorter marks for most English language libraries.&lt;ref&gt;Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&amp;pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891&#8211;93, p. 27.&lt;/ref&gt;

==How Expansive Classification call numbers are constructed==
{{Expand section|citations and corrections|date=August 2011}}
Most call numbers in the Expansive Classification follow conventions offering clues to the book's subject. The first line represents the subject, the second the author (and perhaps title), the third and fourth dates of editions, indications of translations, and critical works on particular books or authors. All numbers in the Expansive Classification are (or should be) shelved as if in decimal order.

Size of volumes is indicated by points (.), pluses (+), or slashes (/ or //).

For some subjects a numerical geographical subdivision follows the classification letters on the first line. The number 83 stands for the United States&amp;mdash;hence, F83 is U.S. history, G83 U.S. travel, JU83 U.S. politics, WP83 U.S. painting. Geographical numbers are often further expanded decimally to represent more specific areas, sometimes followed by a capital letter indicating a particular city.

The second line usually represents the author's name by a capital letter plus one or more numbers arranged decimally. This may be followed by the first letter or letters of the title in lower-case, and/or sometimes the letters a,b,c indicating other printings of the same title. When appropriate, the second line may begin with a 'form' number&amp;mdash;e.g., 1 stands for history and criticism of a subject, 2 for a bibliography, 5 for a dictionary, 6 for an atlas or maps, 7 for a periodical, 8 for a society or university publication, 9 for a collection of works by different authors.

On the third line a capital Y indicates a work about the author or book represented by the first two lines, and a capital E (for English&amp;mdash;other letters are used for other languages) indicates a translation into English. If both criticism and translation apply to a single title, the number expands into four lines.

=== Cutter Numbers (Cutter Codes) ===
{{Expand section|examples and additional citations|date=August 2011}}
One of the features adopted by other systems, including Library of Congress, is the Cutter number. It is an alphanumeric device to code text so that it can be arranged in alphabetical order using the fewest characters. It contains one or two initial letters and Arabic numbers, treated as a decimal. To construct a Cutter number, a cataloguer consults a Cutter table as required by the classification rules.  Although Cutter numbers are mostly used for coding the names of authors, the system can be used for titles, subjects, geographic areas, and more.

{| class=wikitable
|+Cutter table
|-
! || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9
|-
| S || a || ch || e || h-i || m-p || t || u || w-z
|-
| Qu || || a || e || i || o || r || t || y
|-
| other consonants || || a || e || i || o || r || u || y
|-
| vowels || b || d || l-m || n || p || r || s-t || u-y
|-
| additional letters || || a-d || e-h || i-l || m-o || p-s || t-v || w-z
|}

Initial letters Qa-Qt are assigned Q2-Q29, while entries beginning with numerals have a Cutter number A12-A19, therefore sorting before the first A entry.&lt;ref&gt;{{cite web|title=LC Cutter Tables |url=http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |website=Queen Elizabeth II Libraries |publisher=Memorial University of Newfoundland |accessdate=14 August 2014 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20140814173419/http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |archivedate=14 August 2014 |df= }}&lt;/ref&gt;

So to make the three digit Cutter number for "Cutter", you would start with "C", then looking under ''other consonants'', find that "u" gives the number 8, and under ''additional letters'', "t" is 8, giving a Cutter number of "C88".

==Notes==
{{reflist}}

==References==
* Bliss, Henry Evelyn. ''The Organization of Knowledge in Libraries: and the Subject-Approach to Books'', 2nd ed. New York: H. W. Wilson, 1939.
* Cutter, Charles A. ''Rules for a Dictionary Catalog''. W. P. Cutter, ed. 4th ed. Washington, D.C.: Government Printing Office, 1904. London: The Library Association, 1962.
* Cutter, William Parker. ''Charles Ammi Cutter''. Chicago: American Library Association, 1931. Ann Arbor, MI: University Microfilms, 1969.
* Foster, William E. "Charles Ammi Cutter: A Memorial Sketch". ''Library Journal'' 28 (1903): 697-704.
* Hufford, Jon R. "The Pragmatic Basis of Catalog Codes: Has the User Been Ignored?". ''Cataloging and Classification Quarterly'' 14 (1991): 27-38.
* Immroth, John Philip. "Cutter, Charles Ammi". ''Encyclopedia of Library and Information Science''. [[Allen Kent]] and Harold Lancour, ed. 47 vols. New York, M. Dekker [1968- ]
* LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961.
*Slavis, Dobrica. "CUTT-x: An Expert System for Automatic Assignment of Cutter Numbers". ''Cataloging and Classification Quarterly''. Vol 22, no. 2, 1996.
* Tauber, Maurice F., and Edith Wise. "Classification Systems". [[Ralph R. Shaw (Librarian)|Ralph R. Shaw]], ed.. ''The State of the Library Art''. New Brunswick, NJ: Rutgers U. Graduate School of Library Service, 1961. 1-528.

==External links==
* [http://catalog.bostonathenaeum.org/cutterguide.html The Boston Athenaeum's Guide to the classification system developed by Cutter for their collection]
* [http://www.forbeslibrary.org/research/index.php?n=Main.CutterClassification Forbes Library's Outline of Cutter's Expansive Classification system]
* [http://www.forbeslibrary.org/pathfinders/Shelvingrules.pdf A brief guide to the Expansive Classification from Forbes Library]
* [http://digital.library.unt.edu/permalink/meta-dc-1048:1 ''Rules for a dictionary catalog, by Charles A. Cutter, fourth edition''], hosted by the [http://digital.library.unt.edu/ UNT Libraries Digital Collections]
* [http://www.loc.gov/aba/pcc/053/table.html Library of Congress Guidelines for using the LC Online Shelflist and formulating a literary author number: Cutter Table]
* [http://www.oclc.org/dewey/support/program/default.htm Dewey Cutter Program]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>5a3a5s0nwdmxthbqyoho2rjko6qdx18</sha1>
    </revision>
  </page>
  <page>
    <title>Medical algorithm</title>
    <ns>0</ns>
    <id>1551981</id>
    <revision>
      <id>745167466</id>
      <parentid>685986851</parentid>
      <timestamp>2016-10-19T17:12:10Z</timestamp>
      <contributor>
        <username>Biogeographist</username>
        <id>18201938</id>
      </contributor>
      <comment>/* See also */ removed link that is already in article body, per [[WP:NOTSEEALSO]]; removed red link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5698" xml:space="preserve">{{Expand Russian|&#1052;&#1077;&#1076;&#1080;&#1094;&#1080;&#1085;&#1089;&#1082;&#1080;&#1081; &#1072;&#1083;&#1075;&#1086;&#1088;&#1080;&#1090;&#1084;|date=September 2015}}
{{Original research|date=October 2007}}
[[File:Assessment and treatment algorithm for overweight and obesity.png|thumb|450px|A medical algorithm for assessment and treatment of [[overweight]] and [[obesity]].]]
A '''medical algorithm''' is any [[computation]], [[formula]], [[statistical survey]], [[nomogram]], or [[look-up table]], useful in [[healthcare]].  [[Medical]] [[algorithm]]s include [[decision tree]] approaches to healthcare treatment (e.g., if [[symptom]]s A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.

==Scope==
Medical algorithms are part of a broader field which is usually fit under the aims of [[medical informatics]] and medical [[decision-making]]. Medical decisions occur in several areas of medical activity including medical test selection, [[diagnosis]], therapy and [[prognosis]], and [[automatic control]] of [[medical equipment]].

In relation to [[logic]]-based and [[artificial neural network]]-based [[clinical decision support system]], which are also computer applications to the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.

==Examples==
A wealth of medical information exists in the form of published medical algorithms.  These algorithms range from simple [[calculation]]s to complex outcome [[prediction]]s.  Most [[clinician]]s use only a small subset routinely.

Examples of medical algorithms are:
* '''[[Calculators]],'''. e.g., an on-line or stand-alone calculator for [[body mass index]] (BMI) when stature and body weight are given;
* '''[[Flowcharts]],''' e.g., a [[Wiktionary:binary|binary]] [[decision tree]] for deciding what is the [[etiology]] of [[chest pain]]
* '''[[Look-up table]]s,''' e.g., for looking up [[food energy]] and nutritional contents of foodstuffs
* '''[[Nomogram]]s,''' e.g., a moving circular slide to calculate body surface area or drug dosages.

A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the [[Texas Medication Algorithm Project]] or otherwise working on their production.

A grammar&#8212;the [[Arden syntax]]&#8212;exists for describing algorithms in terms of [[medical logic module]]s. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.

==Purpose==
The intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of [[medical care]]. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm [[automation]] intended to reduce potential introduction of errors.  Some attempt to predict the outcome, for example [[ICU scoring systems|critical care scoring systems]].

Computerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to [[evidence-based medicine|evidence-based]] [[guideline (medical)|guidelines]], and be a resource for education and research. 

Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as [[Clinical trial protocol|protocol]]s and it is a key task in training to ensure people step outside the protocol when necessary.  In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.

==Cautions==
In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.  

[[Computation]]s obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and [[physician]] judgment.

==See also==
* [[Consensus (medical)]]
* [[Evidence-based medicine]]
* [[Journal club]]
* [[Medical guideline]]
* [[Medical informatics]]
* [[Odds algorithm]]
* ''[[Treatment Guidelines from The Medical Letter]]''

==Further reading==
* {{cite journal| title=Automated Medical Algorithms:  Issues for Medical Errors| first1=Kathy A.| last1=Johnson| first2=John R.| last2=Svirbely| first3=M.G.| last3=Sriram| first4=Jack W.| last4=Smith| first5=Gareth |last5=Kantor| first6=Jorge Raul |last6=Rodriguez| journal=[[Journal of the American Medical Informatics Association]]| pmc=419420| doi=10.1197/jamia.M1228| volume=9| issue=6 Suppl 1| pages=s56-s57| date=November 2002}}

==External links==

* [http://www.alternativementalhealth.com/articles/fieldmanual.htm AlternativeMentalHealth.com] - 'Alternative Health Medical Evaluation Field Manual', Lorrin M. Koran, MD, [[Stanford University]] Medical Center (1991)

[[Category:Health informatics]]
[[Category:Algorithms]]
[[Category:Knowledge representation]]</text>
      <sha1>ehmdshujxlbvuff3xtx6axtsh64lzfq</sha1>
    </revision>
  </page>
  <page>
    <title>Guideline execution engine</title>
    <ns>0</ns>
    <id>2804505</id>
    <revision>
      <id>758777023</id>
      <parentid>722973190</parentid>
      <timestamp>2017-01-07T14:53:42Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor />
      <comment>/* Use of third party workflow engine as a guideline execution engine */Journal cites, set missing volume/pages parameter,  using [[Project:AWB|AWB]] (12142)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3959" xml:space="preserve">A '''Guideline Execution Engine''' is a [[computer program]] which can interpret a [[guideline (medical)|clinical guideline]] represented in a computerized format and perform actions towards the user of an [[electronic medical record]].

A Guideline Execution Engine needs to communicate with a host [[Clinical information system]]. [[virtual Medical Record|vMR]] is one possible interface which can be used.

The engine's main function is to manage instances of executed guidelines of individual patients.

Delivering the inferred engine recommendations or impacts to the host Clinical information system  has to carefully respect current workflow of the clinicians (physicians, nurses, clerks, etc.)

== Architecture of Guideline Execution Engine ==
The following modules are generally needed for any engine

* interface to Clinical Information System
* new guidelines loading module
* guideline interpreter module
* clinical events parser
* alert/recommendations dispatch

== Guideline Interchange Format ==

The ''Guideline Interchange Format (GLIF)'' is computer representation format for [[clinical guideline]]s.&lt;ref&gt;{{cite web |url=http://mis.hevra.haifa.ac.il/~morpeleg/Intermed/ |title=Guideline Representation Page: GLIF 2.0, 3.4, 3.5 Specifications |work=Stanford University, School of Medicine, InterMed Collaboratory }}&lt;/ref&gt; Represented guidelines can be executed using a guideline execution engine.

The format has several versions as it has been improved. In 2003 GLIF3 was introduced.

== Use of third party workflow engine as a guideline execution engine ==
Some commercial Electronic Health Record systems use a [[workflow engine]] to execute clinical guidelines. RetroGuide&lt;ref name=eval&gt;{{Cite journal 
| last1 = Huser | first1 = V. 
| last2 = Narus | first2 = S. P. 
| last3 = Rocha | first3 = R. A. 
| doi = 10.1016/j.jbi.2009.06.001 
| title = Evaluation of a flowchart-based EHR query system: A case study of RetroGuide&#9734; 
| journal = Journal of Biomedical Informatics 
| volume = 43 
| issue = 1 
| pages = 41&#8211;50 
| year = 2010 
| pmid = 19560553 
| pmc =2840619 
}}&lt;/ref&gt; and HealthFlow&lt;ref name=hf2010&gt;{{citation|pmc=3079703|title=Implementation of workflow engine technology to deliver basic clinical decision support functionality|journal=BMC Med Res Methodol.|year=2011|volume= 11|page= 43|doi=10.1186/1471-2288-11-43|pmid=21477364|vauthors=Huser V, Rasmussen LV, Oberg R, Starren JB}}&lt;/ref&gt;  are examples of such an approach.

== See also ==

*[[Electronic medical record]]
*[[Clinical practice guideline]]
*[[Medical algorithm]]
*[[Arden syntax]]
*[[Healthcare workflow]]
*[[Glif]]
*[[RetroGuide]]

== References ==
&lt;references/&gt;

== External links ==
*{{cite journal  |vauthors=Wang D, Peleg M, Tu SW, etal |title=Design and implementation of the GLIF3 guideline execution engine |journal=J Biomed Inform |volume=37 |issue=5 |pages=305&#8211;18 |date=October 2004 |pmid=15488745 |doi=10.1016/j.jbi.2004.06.002 |url=http://linkinghub.elsevier.com/retrieve/pii/S1532046404000668}} [http://bmir.stanford.edu/file_asset/index.php/940/BMIR-2004-1008.pdf (PDF)]
*{{cite journal  |vauthors=Ram P, Berg D, Tu S, etal |title=Executing clinical practice guidelines using the SAGE execution engine |journal=Stud Health Technol Inform |volume=107 |issue=Pt 1 |pages=251&#8211;5 |year=2004 |pmid=15360813 }}
*{{cite journal |vauthors=Tu SW, Campbell J, Musen MA |title=The structure of guideline recommendations: a synthesis |journal=AMIA Annu Symp Proc |volume= |issue= |pages=679&#8211;83 |year=2003 |pmid=14728259 |pmc=1480008 }} [http://bmir.stanford.edu/file_asset/index.php/1511/BMIR-2003-0966.pdf (PDF)]
*{{cite journal |vauthors=Tu SW, Musen MA |title=A flexible approach to guideline modeling |journal=Proc AMIA Symp |volume= |issue= |pages=420&#8211;4 |year=1999 |pmid=10566393 |pmc=2232509 }} [http://bmir.stanford.edu/file_asset/index.php/211/BMIR-1999-0789.pdf (PDF)]

[[Category:Health informatics]]
[[Category:Knowledge representation]]</text>
      <sha1>013tbzl6f2j90c1vi978tdudqiaa1hk</sha1>
    </revision>
  </page>
  <page>
    <title>Attribute-value system</title>
    <ns>0</ns>
    <id>7512482</id>
    <revision>
      <id>752569793</id>
      <parentid>752524580</parentid>
      <timestamp>2016-12-02T00:42:48Z</timestamp>
      <contributor>
        <username>Ibadibam</username>
        <id>1138432</id>
      </contributor>
      <minor />
      <comment>/* References */ [[MOS:LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5530" xml:space="preserve">An '''attribute-value system''' is a basic [[knowledge representation]] framework comprising a table with columns designating "attributes" (also known as "properties", "predicates," "features," "dimensions," "characteristics", "[[Field (computer science)|fields]]", "headers" or "independent variables" depending on the context) and "[[Row (database)|rows]]" designating "objects" (also known as "entities," "instances," "exemplars," "elements", "[[Record (computer science)|records]]" or "dependent variables"). Each table cell therefore designates the value (also known as "state") of a particular attribute of a particular object.

== Example of attribute-value system==
Below is a sample attribute-value system. It represents 10 objects (rows) and five features (columns). In this example, the table contains only integer values. In general, an attribute-value system may contain any kind of data, numeric or otherwise. An attribute-value system is distinguished from a simple "feature list" representation in that each feature in an attribute-value system may possess a range of values (e.g., feature &lt;math&gt;P_{1}&lt;/math&gt; below, which has domain of {0,1,2}), rather than simply being ''present'' or ''absent'' {{Harv|Barsalou|Hale|1993}}.

:{| class="wikitable" style="text-align:center; width:30%" border="1"
|+ Sample Attribute-Value System
! Object !! &lt;math&gt;P_{1}&lt;/math&gt; !! &lt;math&gt;P_{2}&lt;/math&gt; !! &lt;math&gt;P_{3}&lt;/math&gt; !! &lt;math&gt;P_{4}&lt;/math&gt; !! &lt;math&gt;P_{5}&lt;/math&gt;
|-
! &lt;math&gt;O_{1}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{2}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{3}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{4}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 1
|-
! &lt;math&gt;O_{5}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 1
|-
! &lt;math&gt;O_{6}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 2
|-
! &lt;math&gt;O_{7}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{8}&lt;/math&gt;
| 0 || 1 || 2 || 2 || 1
|-
! &lt;math&gt;O_{9}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 2
|-
! &lt;math&gt;O_{10}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|}

== Other terms used for "attribute-value system"==
Attribute-value systems are pervasive throughout many different literatures, and have been discussed under many different names:
*''Flat data''
*''[[Spreadsheet]]''
*''Attribute-value system'' (Ziarko &amp; Shan 1996)
*''Information system'' ([[Zdzislaw Pawlak|Pawlak]] 1981)
*''Classification system'' (Ziarko 1998)
*''Knowledge representation system'' (Wong &amp; Ziarko 1986)
*''Information table'' (Yao &amp; Yao 2002)
*''Object-predicate table'' (Watanabe 1985)
*''Aristotelian table'' (Watanabe 1985)
*''Simple frames'' {{Harv|Barsalou|Hale|1993}}
*''[[First normal form]]'' database

==See also==
*[[Bayes networks]]
*[[Entity&#8211;attribute&#8211;value model]]
*[[Joint distribution]]
*[[Knowledge representation]]
*[[wikibooks:Optimal Classification|Optimal classification]] (in Wikibooks)
*[[Rough set]]
*[[Triplestore]]

== References ==
* {{Cite book
 | last1=Barsalou
 | given1=Lawrence W.
 | surname2=Hale
 | given2=Christopher R.
 | year= 1993
 | chapter=Components of conceptual representation: From feature lists to recursive frames
 | editor=Iven Van Mechelen |editor2=James Hampton |editor3=Ryszard S. Michalski |editor4=Peter Theuns
 | title=Categories and Concepts: Theoretical Views and Inductive Data Analysis
 | pages=97&#8211;144
 | edition=
 | publisher=Academic Press
 | place=London
 | url=
 | accessdate=
 | ref=harv
 | postscript=&lt;!--None--&gt;}}
*{{cite book
  | last = Pawlak
  | first = Zdzis&#322;aw
  | authorlink = Zdzislaw Pawlak
  | title = Rough sets: Theoretical Aspects of Reasoning about Data
  | publisher = Kluwer
  | year = 1991
  | location = Dordrecht}}
*{{cite journal
  | last = Ziarko
  | first = Wojciech 
  | last2 = Shan
  | first2 = Ning
  | title = A method for computing all maximally general rules in attribute-value systems
  | journal = Computational Intelligence
  | volume = 12
  | issue = 2
  | pages = 223&#8211;234
  | year = 1996
  | doi = 10.1111/j.1467-8640.1996.tb00260.x
  | ref = harv}}
*{{cite journal
  | last = Pawlak
  | first = Zdzis&#322;aw
  | last2 = Shan
  | first2 = Ning
  | title = Information systems: Theoretical foundations
  | journal = Information Systems
  | volume = 6
  | issue = 3
  | pages = 205&#8211;218
  | year = 1981
  | doi = 10.1016/0306-4379(81)90023-5
  | ref = harv}}
*{{cite journal
  | last = Wong
  | first = S. K. M.
  | last2 = Ziarko
  | first2 = Wojciech
  | last3 = Ye
  | first3 = R. Li
  | title = Comparison of rough-set and statistical methods in inductive learning
  | journal = International Journal of Man-Machine Studies
  | volume = 24
  | pages = 53&#8211;72
  | year = 1986
  | ref = harv}}
*{{cite conference
  | first = Yao
  | last = J. T.
  |author2=Yao, Y. Y.
  | title = Induction of classification rules by granular computing
  | booktitle = Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC'02)
  | pages = 331&#8211;338
  | publisher = Springer-Verlag
  | year = 2002
  | location = London, UK}}
*{{cite book
  | last = Watanabe
  | first = Satosi
  | title = Pattern Recognition: Human and Mechanical
  | publisher = John Wiley &amp; Sons
  | year = 1985
  | location = New York}}
*{{cite conference
  | first = Wojciech
  | last = Ziarko
  | title = Rough sets as a methodology for data mining
  | booktitle = Rough Sets in Knowledge Discovery 1: Methodology and Applications
  | pages = 554&#8211;576
  | editor    = Polkowski, Lech |editor2=Skowron, Andrzej
  | publisher = Physica-Verlag
  | year = 1998
  | location = Heidelberg}}

[[Category:Knowledge representation]]
[[Category:Specific models]]</text>
      <sha1>oot0tuusozpfbsjrziue34qgs94ajhf</sha1>
    </revision>
  </page>
  <page>
    <title>MultiNet</title>
    <ns>0</ns>
    <id>2473220</id>
    <revision>
      <id>729278174</id>
      <parentid>710620820</parentid>
      <timestamp>2016-07-11T03:59:39Z</timestamp>
      <contributor>
        <username>Yaron K.</username>
        <id>2276977</id>
      </contributor>
      <comment>Removed extra newlines</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2803" xml:space="preserve">'''Multilayered extended semantic networks''' ('''MultiNets''') are both a [[knowledge representation]] paradigm and a language for meaning representation of [[natural language]] expressions that has been developed by Prof. Dr. Hermann Helbig on the basis of earlier [[Semantic network|Semantic Networks]].

MultiNet is claimed to be one of the most comprehensive and thoroughly described knowledge representation systems. It specifies conceptual structures by means of about 140 predefined relations and functions, which are systematically characterized and underpinned by a formal [[axiomatic]] apparatus. Apart from their relational connections, the concepts are embedded in a multidimensional space of layered attributes and their values. Another characteristic of MultiNet discerning it from simple semantic networks is the possibility to encapsulate whole partial networks and represent the resulting conceptual capsule as a node of higher order, which itself can be an argument of relations and functions. MultiNet has been used in practical [[Natural language processing|NLP]] applications such as natural language interfaces to the Internet or [[question answering]] systems over large semantically annotated [[Corpus linguistics|corpora]] with millions of sentences. MultiNet is also a cornerstone of the commercially available search engine SEMPRIA Search, where it is used for the description of the computational lexicon and the background knowledge, for the syntactic-semantic analysis, for logical answer finding, as well as for the generation of natural language answers.

MultiNet is supported by a set of [[software tools]] and has been used to build large semantically based computational lexicons. The tools include a semantic interpreter WOCADI which translates natural language expressions (phrases, sentences, texts) into formal MultiNet expressions, a workbench MWR+ for the knowledge engineer (comprising modules for automatic knowledge acquisition and reasoning), and a workbench LIA+ for the computer [[lexicographer]] supporting the creation of large semantically based computational lexica.

== References ==
* Hermann Helbig, ''Die semantische Struktur nat&#252;rlicher Sprache - Wissensrepr&#228;sentation mit MultiNet''. Springer, Heidelberg, 2001.
* Hermann Helbig. ''Knowledge Representation and the Semantics of Natural Language'', (2006) Springer, Berlin
* Sven Hartrumpf, Hermann Helbig, Johannes Leveling, Rainer Osswald. ''An Architecture for Controlling Simple Language in Web Pages'', eMinds: International Journal on Human-Computer Interaction, 1(2), 2006.

== External links ==
* [http://pi7.fernuni-hagen.de/forschung/multinet/multinet_en.html MultiNet] and its software environment

[[Category:Semantic Web]]
[[Category:Knowledge representation]]


{{software-stub}}</text>
      <sha1>tvpsp1mjd6b22clyogfc0bbihittvaw</sha1>
    </revision>
  </page>
  <page>
    <title>Ramification problem</title>
    <ns>0</ns>
    <id>496055</id>
    <revision>
      <id>657214475</id>
      <parentid>522771267</parentid>
      <timestamp>2015-04-19T18:13:25Z</timestamp>
      <contributor>
        <username>Valoem</username>
        <id>1024002</id>
      </contributor>
      <comment>corrected link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1606" xml:space="preserve">{{Cleanup|date=April 2011}}

In [[philosophy]] and [[artificial intelligence]] (especially, knowledge based systems), the '''ramification problem''' is concerned with the indirect consequences of an action. It might also be posed as ''how to represent what happens implicitly due to an action'' or how to control the secondary and tertiary effects of an action. It is strongly connected to, and is opposite the [[qualification problem|qualification side]] of, the [[frame problem]].

Limit theory helps in [[operational]] usage. For instance, in [[Knowledge-based engineering|KBE]] derivation of a populated design (geometrical objects, etc., similar concerns apply in shape theory), equivalence assumptions allow convergence where potentially large, and perhaps even computationally indeterminate, solution sets are handled deftly. Yet, in a chain of computation, downstream events may very well find some types of results from earlier resolutions of '''ramification''' as problematic for their own algorithms.

==See also==
*[[Non-monotonic logic]]
*[[Ramification (mathematics)]]

==External links==
*Nikos Papadakis [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/&amp;toc=comp/proceedings/ictai/2002/1849/00/1849toc.xml&amp;DOI=10.1109/TAI.2002.1180791 "Actions with Duration and Constraints: the Ramification Problem in Temporal Databases"] IEEE ICTAI'02
*Deepak Kumar "[http://blackcat.brynmawr.edu/~dkumar/UGAI/planning.html AI Planning]" Bryn Mawr College

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Epistemology]]


{{epistemology-stub}}</text>
      <sha1>5nci4nmwsrbnvkw7vke0adosv2fi29o</sha1>
    </revision>
  </page>
  <page>
    <title>Microformat</title>
    <ns>0</ns>
    <id>2346998</id>
    <revision>
      <id>760445523</id>
      <parentid>755821224</parentid>
      <timestamp>2017-01-17T01:08:19Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>simplify headings, rm items linked already/integrate items</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22410" xml:space="preserve">{{About||the photographic miniaturization of documents|Microform|details of microformats used on Wikipedia|:Wikipedia:Microformats}}
A '''microformat''' (sometimes abbreviated '''&#956;F''') is a [[World Wide Web]]-based approach to semantic markup which uses [[HTML]]/[[XHTML]] tags supported for other purposes to convey additional [[metadata]]&lt;ref&gt;{{cite web |url=http://microformats.org/wiki/existing-classes |work=Microformats.org |title=Class Names Across All Microformats |date=2007-09-23 |accessdate=2008-09-06}}&lt;/ref&gt; and other attributes in web pages and other contexts that support (X)HTML, such as [[RSS]]. This approach allows [[software agent|software]] to process information intended for end-users (such as [[Address book|contact information]], [[Geographic coordinate system|geographic coordinates]], calendar events, and similar information) automatically.

Although the content of web pages has been capable of some "automated processing" since the inception of the web, such processing is difficult because the [[markup language|markup tags]] used to display information on the web do not describe what the information means.&lt;ref name="Wharton000"&gt;{{cite web |title=What&#8217;s the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&amp;ID=1247}}&lt;/ref&gt; Microformats can bridge this gap by attaching [[semantics]], and thereby obviate other, more complicated, methods of automated processing, such as [[natural language processing]] or [[screen scraping]]. The use, adoption and processing of microformats enables data items to be indexed, searched for, saved or cross-referenced, so that information can be reused or combined.&lt;ref name="Wharton000"/&gt;

{{As of | 2013}} microformats allow the encoding and extraction of event details, contact information, social relationships and similar information. Established microformats such as [[hCard]] are published on the web more than alternatives like schema ([[Microdata (HTML)|microdata]]) and [[RDFa]].&lt;ref&gt;{{cite web|url=http://webdatacommons.org/structureddata/index.html#toc2
 |date=2013 |work= section 3.1, "Extraction Results from the November 2013 Common Crawl Corpus" |accessdate=2015-02-21 |title=Web Data Commons &#8211; RDFa, Microdata, and Microformat Data Sets}}&lt;/ref&gt;{{failed verification|date=November 2016}}

== Background ==
Microformats emerged around 2005&lt;ref&gt;The ''microformats'' is a community-standard maintained by its Wiki, and [http://microformats.org/wiki/index.php?title=Main_Page&amp;dir=prev&amp;action=history the Wiki arrived ~2005].&lt;/ref&gt; as part of a grassroots movement to make recognizable data items (such as events, contact details or geographical locations) capable of automated processing by software, as well as directly readable by end-users.&lt;ref name="Wharton000"/&gt;&lt;ref&gt;In this context, the definition of "end-user" includes a person reading a web page on a computer screen or mobile device, or an [[assistive technology]] such as a [[screen reader]].&lt;/ref&gt; Link-based microformats emerged first. These include vote links that express opinions of the linked page, which search engines can tally into instant polls.&lt;ref name="Khare000"&gt;{{cite journal |title=Microformats: The Next (Small) Thing on the Semantic Web? |first=Rohit |last=Khare |journal=[[IEEE Internet Computing]] |volume=10 |issue=1 |pages=68&#8211;75 |date=January&#8211;February 2006 |publisher=[[IEEE Computer Society]] |url=http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&amp;toc=comp/mags/ic/2006/01/w1toc.xml&amp;DOI=10.1109/MIC.2006.13 |doi=10.1109/MIC.2006.13 |accessdate=2008-09-06}}
&lt;/ref&gt;

[[CommerceNet]], a nonprofit organization that promotes [[e-commerce]] on the Internet, has helped sponsor and promote the technology and support the microformats community in various ways.&lt;ref name="Khare000"/&gt; CommerceNet also helped co-found the Microformats.org community site.&lt;ref name="Khare000"/&gt;

Neither CommerceNet nor Microformats.org operates as a [[standards body]]. The microformats community functions through an open [[wiki]], a mailing list, and an Internet relay chat ([[Internet Relay Chat|IRC]]) channel.&lt;ref name="Khare000"/&gt; Most of the existing microformats originated at the Microformats.org wiki and the associated mailing list{{citation needed|date=October 2012}} by a process of gathering examples of web-publishing behaviour, then codifying it. Some other microformats (such as [[nofollow|rel=nofollow]] and [[unAPI]]) have been proposed, or developed, elsewhere.

== Technical overview ==

XHTML and HTML standards allow for the embedding and encoding of semantics within the [[HTML element|attributes of markup tags]]. Microformats take advantage of these standards by indicating the presence of metadata using the following attributes:

; &lt;code&gt;class&lt;/code&gt;
: [[Class (computer programming)|Classname]]

; &lt;code&gt;rel&lt;/code&gt;
: relationship, description of the target address in an anchor-element (&lt;code&gt;&lt;a href=... rel=...&gt;...&lt;/a&gt;&lt;/code&gt;)

; &lt;code&gt;rev&lt;/code&gt;
: reverse relationship, description of the referenced document (in one case, otherwise deprecated in microformats&lt;ref name="uF-rel-faq"&gt;{{cite web |url=http://microformats.org/wiki/rel-faq |title="rel" attribute frequently asked questions |work=Microformats.org |date=2008-08-06 |accessdate=2008-09-06}}&lt;/ref&gt;)

For example, in the text "The birds roosted at &lt;span class="geo"&gt;&lt;span class="latitude"&gt;52.48&lt;/span&gt;, &lt;span class="longitude"&gt;-1.89&lt;/span&gt;&lt;/span&gt;" is a pair of numbers which may be understood, from their context, to be a set of [[geographic coordinate system|geographic coordinates]]. With wrapping in [[Span and div|spans]] (or other HTML elements) with specific class names (in this case &lt;code&gt;geo&lt;/code&gt;, &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;longitude&lt;/code&gt;, all part of the [[Geo (microformat)|geo microformat]] specification):

 &lt;syntaxhighlight lang="xml"&gt;The birds roosted at
   &lt;span class="geo"&gt;
     &lt;span class="latitude"&gt;52.48&lt;/span&gt;,
     &lt;span class="longitude"&gt;-1.89&lt;/span&gt;
   &lt;/span&gt;
 &lt;/syntaxhighlight&gt;

software agents can recognize exactly what each value represents and can then perform a variety of tasks such as indexing, locating it on a map and exporting it to a [[GPS]] device.

=== Examples ===
In this example, the contact information is presented as follows:

 &lt;syntaxhighlight lang="xml"&gt;
 &lt;ul&gt;
   &lt;li&gt;Joe Doe&lt;/li&gt;
   &lt;li&gt;The Example Company&lt;/li&gt;
   &lt;li&gt;604-555-1234&lt;/li&gt;
   &lt;li&gt;&lt;a href="http://example.com/"&gt;http://example.com/&lt;/a&gt;&lt;/li&gt;
 &lt;/ul&gt;
 &lt;/syntaxhighlight&gt;

With hCard microformat markup, that becomes:

 &lt;syntaxhighlight lang="xml"&gt;
 &lt;ul class="vcard"&gt;
   &lt;li class="fn"&gt;Joe Doe&lt;/li&gt;
   &lt;li class="org"&gt;The Example Company&lt;/li&gt;
   &lt;li class="tel"&gt;604-555-1234&lt;/li&gt;
   &lt;li&gt;&lt;a class="url" href="http://example.com/"&gt;http://example.com/&lt;/a&gt;&lt;/li&gt;
 &lt;/ul&gt;
 &lt;/syntaxhighlight&gt;

Here, the formatted name (&lt;code&gt;fn&lt;/code&gt;), organisation (&lt;code&gt;org&lt;/code&gt;), telephone number (&lt;code&gt;tel&lt;/code&gt;) and [[Uniform Resource Locator|web address]] (&lt;code&gt;url&lt;/code&gt;) have been identified using specific class names and the whole thing is wrapped in &lt;code&gt;class="vcard"&lt;/code&gt;, which indicates that the other classes form an hCard (short for "HTML [[vCard]]") and are not merely coincidentally named. Other, optional, hCard classes also exist. Software, such as browser plug-ins, can now extract the information, and transfer it to other applications, such as an address book.

&lt;div class="noprint"&gt; &lt;!-- ensures that the following "Live" example dies not carry over to printed mirrors --&gt;

&lt;!-- Note "noprint" div started in previous section
--&gt;=== In-context examples ===
For annotated examples of microformats on live pages, see [[HCard#Live example]] and [[Geo (microformat)#Usage]].
&lt;/div&gt;

== Specific microformats ==
Several microformats have been developed to enable semantic markup of particular types of information. However, only hCard and hCalendar have been ratified, the others remaining as drafts:

* [[hAtom]] (superseded by [[h-entry]] and [[h-feed]]) &#8211; for marking up [[Atom (standard)|Atom]] feeds from within standard HTML
* [[hCalendar]] &#8211; for events
* [[hCard]] &#8211; for contact information; includes:
** adr &#8211; for postal addresses
** [[geo (microformat)|geo]] &#8211; for geographical coordinates ([[latitude]], [[longitude]])
* hMedia - for audio/video content&lt;ref&gt;[http://microformats.org/wiki/hmedia hMedia &#183; Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;&lt;ref&gt;[http://sixrevisions.com/web-development/ultimate-guide-to-microformats-reference-and-examples/ Ultimate Guide to Microformats: Reference and Examples&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* hAudio &#8211; for audio content
* [[hNews]] -  for news content
* [[hProduct]] &#8211; for products
* [[hRecipe]] - for recipes and foodstuffs.
* [[hResume]] &#8211; for resumes or [[curriculum vitae|CVs]]
* [[hReview]] &#8211; for reviews
* rel-[[directory (file systems)|directory]] &#8211; for distributed directory creation and inclusion&lt;ref&gt;[http://microformats.org/wiki/rel-directory rel-directory &#183; Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* rel-enclosure &#8211; for multimedia attachments to web pages&lt;ref&gt;[http://microformats.org/wiki/rel-enclosure rel="enclosure" &#183; Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* rel-license &#8211; specification of copyright license&lt;ref&gt;[http://microformats.org/wiki/rel-license rel="license" &#183; Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* rel-[[nofollow]], an attempt to discourage third-party content spam (e.g. [[spam in blogs]])
* rel-[[tag (metadata)|tag]] &#8211; for decentralized tagging ([[Folksonomy]])&lt;ref&gt;[http://microformats.org/wiki/rel-tag rel="tag" &#183; Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* [[xFolk]] &#8211; for tagged links
* [[XHTML Friends Network]] (XFN) &#8211; for social relationships
* [[XOXO (microformat)|XOXO]] &#8211; for lists and outlines

== Uses ==
Using microformats within HTML code provides additional formatting and semantic data that applications can use. For example, applications such as [[web crawler]]s can collect data about on-line resources, or desktop applications such as e-mail clients or scheduling software can compile details. The use of microformats can also facilitate "mash ups" such as exporting all of the geographical locations on a web page into (for example) [[Google Maps]] to visualize them spatially.

Several browser extensions, such as [[Operator (extension)|Operator]] for [[Firefox]] and Oomph for [[Internet Explorer]], provide the ability to detect microformats within an HTML document. When hCard or hCalendar are involved, such browser extensions allow microformats to be exported into formats compatible with contact management and calendar utilities, such as [[Microsoft Outlook]]. When dealing with geographical coordinates, they allow the location to be sent to applications such as [[Google Maps]]. [[Yahoo! query language|Yahoo! Query Language]] can be used to extract microformats from web pages.&lt;ref&gt;{{cite web|url=http://developer.yahoo.net/blog/archives/2009/01/wikipedia_w_yql.html|title=Retrieving and displaying data from Wikipedia with YQL|last=Heilman|first=Chris|date=2009-01-19|work=Yahoo Developer Network|publisher=Yahoo|accessdate=2009-01-19}}&lt;/ref&gt; On 12 May 2009 [[Google search|Google]] announced that they would be parsing the hCard, hReview and hProduct microformats, and using them to populate search result pages.&lt;ref name="Rich-Snippets"&gt;{{cite web|url=http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-snippets.html|title=Introducing Rich Snippets|last=Goel|first=Kavi|author2=Ramanathan V. Guha |author3=Othar Hansson |date=2009-05-12|work=Google Webmaster Central Blog|publisher=Google|accessdate=2009-05-25}}&lt;/ref&gt; They have since extended this to use hCalendar for events&lt;ref name="Google-recipes"&gt;{{cite web|url=http://googlewebmastercentral.blogspot.com/2010/04/better-recipes-on-web-introducing.html|title=Better recipes on the web: Introducing recipe rich snippets|last=Gong|first=Jun|author2=Kosuke Suzuki |author3=Yu Watanabe |date=2010-04-13|publisher=Google|accessdate=17 March 2011}}&lt;/ref&gt; and hRecipe for cookery recipes.&lt;ref name="Google-recipes" /&gt; Similarly, microformats are also processed by [[Bing (search engine)|Bing]]&lt;ref name="Bing"&gt;{{cite web|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/06/02/bing-google-and-yahoo-unite-to-build-the-web-of-objects.aspx|title=Bing Introducing Schema.org: Bing, Google and Yahoo Unite to Build the Web of Objects - Search Blog - Site Blogs - Bing Community|date=2011-06-02|work=[[Bing (search engine)|Bing]]|accessdate=2 June 2011}}&lt;/ref&gt; and [[Yahoo!]].&lt;ref name="YSearch"&gt;{{cite web|url=http://www.ysearchblog.com/2011/06/02/introducing-schema-org-a-collaboration-on-structured-data|title=Introducing schema.org: A Collaboration on Structured Data|date=2011-06-02|accessdate=2 June 2011}}&lt;/ref&gt; Together, these are the world's top three search engines.&lt;ref&gt;{{cite web |url=http://gs.statcounter.com/#search_engine-ww-monthly-201010-201012 |title=Top 5 Search Engines from Oct to Dec 10 &amp;#124; StatCounter Global Stats |author= |work= |publisher=StatCounter |accessdate=17 January 2011}}&lt;/ref&gt;

[[Microsoft]] said they needed to incorporate Microformats into upcoming projects,&lt;ref&gt;{{cite web|url=http://microformats.org/blog/2006/03/20/bill-gates-at-mix06-we-need-microformats |title=Bill Gates at Mix06 &#8211; "We need microformats" |date=2006-03-20 |quote=We need microformats and to get people to agree on them. It is going to bootstrap exchanging data on the Web&#8230; &#8230;we need them for things like contact cards, events, directions&#8230; |accessdate=2008-09-06}}&lt;/ref&gt; as did other software companies.

Alex Faaborg summarizes the arguments for putting the responsibility for microformat user interfaces in the web browser rather than making more complicated HTML:&lt;ref&gt;[http://blog.mozilla.com/faaborg/2007/02/04/microformats-part-4-the-user-interface-of-microformat-detection/ Microformats &#8211; Part 4: The User Interface of Microformat Detection &#171; Alex Faaborg&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* Only the web browser knows what applications are accessible to the user and what the user's preferences are
* It lowers the barrier to entry for web site developers if they only need to do the markup and not handle "appearance" or "action" issues
* Retains backwards compatibility with web browsers that don't support microformats
* The web browser presents a single point of entry from the web to the user's computer, which simplifies security issues

== Evaluation ==
Various commentators have offered review and discussion on the design principles and practical aspects of microformats. Microformats have been compared to other approaches that seek to serve the same or similar purpose.&lt;ref name="criticism000"&gt;{{cite web |title=Criticism |work=Microformats.org |url=http://microformats.org/wiki?title=criticism&amp;oldid=18478 |date=2007-03-24 |accessdate=2007-08-15}}&lt;/ref&gt; From time to time, there is criticism of one, or all, microformats.&lt;ref name="criticism000"/&gt; The spread and use of microformats has been advocated.&lt;ref name="advocacy000"&gt;{{cite web |title=Advocacy |work=Microformats.org |url=http://microformats.org/wiki/advocacy |date=2008-08-27 |accessdate=2007-08-15}}&lt;/ref&gt;&lt;ref name="spread000"&gt;{{cite web |title=Spread Microformats |work=Microformats.org |url=http://microformats.org/wiki/spread-microformats |date=2008-08-29 |accessdate= 2007-08-15}} This includes community resources for marketing microformats such as buttons, banners, wallpaper / desktop screens, logo graphics, etc.&lt;/ref&gt; [[Opera Software]] CTO and [[Cascading Style Sheets|CSS]] creator [[H&#229;kon Wium Lie]] said in 2005 "We will also see a bunch of microformats being developed, and that&#8217;s how the [[Semantic Web|semantic web]] will be built, I believe."&lt;ref name="advocacy001"&gt;{{cite web |title=Interview with H&#229;kon Wium Lie |url=http://www.molly.com/2005/03/31/interview-with-hkon-wium-lie/ |first=Molly E. |last=Holzschlag |authorlink=Molly Holzschlag |date=2005-03-31 |work=Molly.com |accessdate=2007-11-18}}&lt;/ref&gt; However, in August 2008 Toby Inkster, author of the "Swignition" (formerly "Cognition") microformat parsing service, pointed out that no new microformat specifications had been published since 2005.&lt;ref name="threeyears"&gt;{{cite web |title=More than three years |url=http://microformats.org/discuss/mail/microformats-discuss/2008-August/012402.html |work=Microformats.org |first=Toby A. |last=Inkster |date=2008-04-22 |accessdate=2008-08-24}}&lt;/ref&gt;

=== Design principles ===
Computer scientist and entrepreneur, [[Rohit Khare]] stated that ''reduce, reuse, and recycle'' is "shorthand for several design principles" that motivated the development and practices behind microformats.&lt;ref name="Khare000"/&gt;{{rp|71&#8211;72}} These aspects can be summarized as follows:

*Reduce: favor the simplest solutions and focus attention on specific problems;
*Reuse: work from experience and favor examples of current practice;
*Recycle: encourage modularity and the ability to embed, valid XHTML can be reused in blog posts, RSS feeds, and anywhere else you can access the web.&lt;ref name="Khare000"/&gt;

=== Accessibility ===
Because some microformats make use of title attribute of HTML's {{tag|abbr|open}} element to conceal [[machine-readable data]] (particularly date-times and geographical coordinates) in the "[http://microformats.org/wiki/abbr-design-pattern abbr design pattern]", the plain text content of the element is inaccessible to [[screen reader]]s that expand abbreviations.&lt;ref name="ATF"&gt;{{cite web |url=http://www.webstandards.org/2007/04/27/haccessibility/ | title=hAccessibility | first=James |last=Craig |publisher= [[Web Standards Project]] |date=2007-04-27 |accessdate=2007-08-16}}&lt;/ref&gt; In June 2008 the [[BBC]] announced that it would be dropping use of microformats using the &lt;code&gt;abbr&lt;/code&gt; design pattern because of accessibility concerns.&lt;ref name="BBCabbr"&gt;{{cite web |url=http://www.bbc.co.uk/blogs/radiolabs/2008/06/removing_microformats_from_bbc.shtml |title=Removing Microformats from bbc.co.uk/programmes |first=Michael |last=Smethurst |publisher= [[BBC]] |date=2008-06-23 |accessdate=2008-08-24}}&lt;/ref&gt;

=== Comparison with alternative approaches ===
Microformats are not the only solution for providing "more intelligent data" on the web; alternative approaches are used and are under development. For example, the use of [[XML]] markup and standards of the Semantic Web are cited as alternative approaches.&lt;ref name="Khare000"/&gt; Some contrast these with microformats in that they do not necessarily coincide with the design principles of "reduce, reuse, and recycle", at least not to the same extent.&lt;ref name="Khare000"/&gt;

One advocate of microformats, [[Tantek &#199;elik]], characterized a problem with alternative approaches: {{cquote|Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry.&lt;ref name="Wharton000"/&gt;}}

For some applications the use of other approaches may be valid. If the type of data to be described does not map to an existing microformat, [[RDFa]] can embed arbitrary vocabularies into HTML, such as for example domain-specific scientific data such as zoological or chemical data for which there is no microformat. Standards such as W3C's [[GRDDL]] allow microformats to be converted into data compatible with the Semantic Web.&lt;ref name="King007"&gt;{{cite web |title=W3C GRDDL Recommendation Bridges HTML/Microformats and the Semantic Web |work=XML Coverpages |date=2007-09-13 |url=http://xml.coverpages.org/ni2007-09-13-a.html |publisher=[[OASIS (organization)|OASIS]] |accessdate=2007-11-23}}&lt;/ref&gt;

Another advocate of microformats, Ryan King, put the compatibility of microformats with other approaches this way: {{cquote|Microformats provide an easy way for many people to contribute semantic data to the web. With GRDDL all of that data is made available for RDF Semantic Web tools. Microformats and GRDDL can work together to build a better web.&lt;ref name="King007"/&gt;}}

== See also ==
*[[COinS]]
*[[Embedded RDF]]
*[[Intelligent agent]]s
*[[RDFa Lite]]
*[[JSON-LD]]
*[[S5 (file format)]]
*[[Schema.org]]
*[[Simple HTML Ontology Extensions]]
*[[XMDP]]

== Notes ==
{{Reflist|2}}

== References ==
{{Refbegin|2}}
*{{cite book |last=Allsopp |first=John | title=Microformats: Empowering Your Markup for Web 2.0 |date=March 2007 |publisher=[[Apress|Friends of ED]] |isbn=978-1-59059-814-6 |page=368}}
*{{cite book |last=Orchard |first=Leslie M |title=Hacking RSS and Atom |date=September 2005 |publisher=[[John Wiley &amp; Sons]] |isbn=978-0-7645-9758-9 |page=602}}
*{{cite book |last=Robbins |first=Jennifer Niederst |authorlink=Jennifer Niederst Robbins |first2=Tantek|last2=&#199;elik|authorlink2=Tantek &#199;elik|first3=Derek|last3=Featherstone|first4=Aaron|last4=Gustafson|title=Web Design In A Nutshell |edition=Third |date=February 2006 |publisher=[[O'Reilly Media]] |isbn=978-0-596-00987-8 |page=826}}

{{Refend}}

== Further reading ==
* {{cite book |last= Suda |first= Brian |title= Using Microformats |date=September 2006 |publisher= [[O'Reilly Media]] |isbn=978-0-596-52821-8 |page=45}}
* Ahmet Soylu, Patrick De Causmaecker, Fridolin Wild [http://www.rintonpress.com/journals/jmmonline.html#v6n1  Ubiquitous Web for Ubiquitous Environments: The Role of Embedded Semantics], article in Journal of Mobile Multimedia, Vol. 6, No.1, pp.&amp;nbsp;26&#8211;48, (2010). [https://lirias.kuleuven.be/bitstream/123456789/243944/2/JMM_soylu_et_al_2010.pdf PDF]

== External links ==
{{Commons category|Microformat screenshots}}
* [http://microformats.org/ microformats.org]
* [http://www.digital-web.com/articles/microformats_primer/ Microformats Primer]
* [http://microformatique.com/optimus/ Optimus] microformats parser and validator
* [http://blog.mozilla.com/faaborg/2006/12/11/microformats-part-0-introduction A four-part discussion of Microformats, UI issues, and possible presentation in Firefox 3 by Alex Faaborg of Mozilla]

{{Semantic Web}}
{{Use dmy dates|date=January 2011}}

[[Category:Microformats| ]]
[[Category:Knowledge representation]]
[[Category:Semantic HTML]]
[[Category:Semantic Web]]
[[Category:Web design]]
[[Category:Web development]]</text>
      <sha1>hekueqczn94w75imdifo87pbnb6tc1n</sha1>
    </revision>
  </page>
  <page>
    <title>Frame problem</title>
    <ns>0</ns>
    <id>11306</id>
    <revision>
      <id>744479298</id>
      <parentid>723788348</parentid>
      <timestamp>2016-10-15T13:55:40Z</timestamp>
      <contributor>
        <username>The Anome</username>
        <id>76</id>
      </contributor>
      <comment>"[[block world]]", [[axiom]]s</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="25168" xml:space="preserve">In [[artificial intelligence]], the '''frame problem''' describes an issue with using [[first-order logic]] (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a "[[block world]]" with rules about stacking blocks together. In a FOL system, additional [[axiom]]s are required to make inferences about the environment (for example, that a block cannot change position unless it's physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment.&lt;ref&gt;{{cite journal|last=Hayes|first=Patrick|title=The Frame Problem and Related Problems in Artificial Intelligence|journal=University of Edinburgh|url=http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Hayes-FrameProblem.pdf}}&lt;/ref&gt;

[[John McCarthy (computer scientist)|John McCarthy]] and [[Patrick J. Hayes]] defined this problem in their 1969 article, ''Some Philosophical Problems from the Standpoint of Artificial Intelligence''.  In this paper and many that came after the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment.&lt;ref&gt;{{cite journal|last=McCarthy|first=J|author2=P.J. Hayes|title=Some philosophical problems from the standpoint of artificial intelligence|journal=Machine Intelligence|year=1969|volume=4|pages=463&#8211;502}}&lt;/ref&gt;  Later, the term acquired a broader meaning in [[philosophy]], where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged.

==Description==
The frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two [[proposition]]s &lt;math&gt;\textit{open}&lt;/math&gt; and &lt;math&gt;\textit{on}&lt;/math&gt;. If these conditions can change, they are better represented by two [[Predicate (computer programming)|predicate]]s &lt;math&gt;\textit{open}(t)&lt;/math&gt; and &lt;math&gt;\textit{on}(t)&lt;/math&gt; that depend on time; such predicates are called [[fluent (artificial intelligence)|fluent]]s. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic{{clarify|reason=What kind of logic? If ordinary predicate logic is meant, what is the purpose of the 'true &#8594;' in the 3rd formula? If some other logic (situation calculus?) is meant, it should be stated explicitly here, together with the purpose of the 'true &#8594;' (e.g. some empty action?) in that logic.|date=August 2013}} by the following formulae:

:&lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
:&lt;math&gt;\textit{true} \rightarrow \textit{open}(1)&lt;/math&gt;

The first two formulae represent the initial situation; the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by &lt;math&gt;\neg \textit{locked}(0) \rightarrow \textit{open}(1)&lt;/math&gt;. In practice, one would have a predicate &lt;math&gt;\textit{executeopen}(t)&lt;/math&gt; for specifying when an action is executed and a rule &lt;math&gt;\forall t . \textit{executeopen}(t) \wedge \textit{true} \rightarrow \textit{open}(t+1)&lt;/math&gt; for specifying the effects of actions.  The article on the [[situation calculus]] gives more details.

While the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions (representing the expected situation) are consistent with the three formulae above, they are not the only ones.

:{|
| &lt;math&gt;\neg \textit{open}(0)&lt;/math&gt; &amp;nbsp; &amp;nbsp;  || &lt;math&gt;\textit{open}(1)&lt;/math&gt;
|-
| &lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;   || &lt;math&gt;\neg \textit{on}(1)&lt;/math&gt;
|}

Indeed, another set of conditions that is consistent with the three formulae above is:

:{|
| &lt;math&gt;\neg \textit{open}(0)&lt;/math&gt; &amp;nbsp; &amp;nbsp;  || &lt;math&gt;\textit{open}(1)&lt;/math&gt;
|-
| &lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;   || &lt;math&gt;\textit{on}(1)&lt;/math&gt;
|}

The frame problem is that specifying only which conditions are changed by the actions do not allow, in logic, to conclude that all other conditions are not changed. This problem can be solved by adding the so-called &#8220;frame axioms&#8221;, which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1:

:&lt;math&gt;\textit{on}(0) \leftrightarrow \textit{on}(1)&lt;/math&gt;

The frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition.{{clarify|reason=Shouldn't then the frame axiom be the following modification of the above rule: '&#8704;t.executeopen(t)&#8594;open(t+1)&#8743;(on(t+1)&#8596;on(t))' ? In contrast, the formula 'on(0)&#8596;on(1)' seems to be too particular taylored to the 'executeopen(0)' situation.|date=August 2013}} In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms.

The solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred; this solution is formalized using the framework of [[Circumscription (logic)|circumscription]]. The [[Yale shooting problem]], however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, [[successor state axiom]]s, etc.; they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved{{clarify|reason=Mention the (combination of) approach(es) by which the frame problem was solved.|date=August 2013}}. Even after that, however, the term &#8220;frame problem&#8221; was still used, in part to refer to the same problem but under different settings (e.g., concurrent actions), and in part to refer to the general problem of representing and reasoning with dynamical domains.

== Solutions ==
The following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full: what is presented are simplified versions that are sufficient to explain the full solution.

===Fluent occlusion solution===
This solution was proposed by [[Erik Sandewall]], who also defined a [[formal language]] for the specification of dynamical domains; therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names.

The rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be ''occluded'' in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as &#8220;permission to change&#8221;: if a condition is occluded, it is relieved from obeying the constraint of inertia.

In the simplified example of the door and the light, occlusion can be formalized by two predicates &lt;math&gt;\textit{occludeopen}(t)&lt;/math&gt; and &lt;math&gt;\textit{occludeon}(t)&lt;/math&gt;. The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed.

:&lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
:&lt;math&gt;\textit{true} \rightarrow \textit{open}(1) \wedge \textit{occludeopen}(1)&lt;/math&gt;
:&lt;math&gt;\forall t . \neg \textit{occludeopen}(t) \rightarrow (\textit{open}(t-1) \leftrightarrow \textit{open}(t))&lt;/math&gt;
:&lt;math&gt;\forall t . \neg \textit{occludeon}(t) \rightarrow (\textit{on}(t-1) \leftrightarrow \textit{on}(t))&lt;/math&gt;

In general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case, &lt;math&gt;\textit{occludeopen}(1)&lt;/math&gt; is true, making the antecedent of the fourth formula above false for &lt;math&gt;t=1&lt;/math&gt;; therefore, the constraint that &lt;math&gt;\textit{open}(t-1) \leftrightarrow \textit{open}(t)&lt;/math&gt; does not hold for &lt;math&gt;t=1&lt;/math&gt;. Therefore, &lt;math&gt;\textit{open}&lt;/math&gt; can change value, which is also what is enforced by the third formula.

In order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by [[Circumscription (logic)|circumscription]] or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change: for example, executing the action of opening the door when it was already open (in the formalization above) makes the predicate &lt;math&gt;\textit{occludeopen}&lt;/math&gt; true and makes &lt;math&gt;\textit{open}&lt;/math&gt; true; however, &lt;math&gt;\textit{open}&lt;/math&gt; has not changed value, as it was true already.

===Predicate completion solution===
This encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example, &lt;math&gt;\textit{changeopen}(t)&lt;/math&gt; represents the fact that the predicate &lt;math&gt;\textit{open}&lt;/math&gt; will change from time &lt;math&gt;t&lt;/math&gt; to &lt;math&gt;t+1&lt;/math&gt;. As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa.

:&lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
:&lt;math&gt;\neg \textit{open}(0) \wedge \textit{true} \rightarrow \textit{changeopen}(0)&lt;/math&gt;
:&lt;math&gt;\forall t. \textit{changeopen}(t) \leftrightarrow (\neg \textit{open}(t) \leftrightarrow \textit{open}(t+1))&lt;/math&gt;
:&lt;math&gt;\forall t. \textit{changeon}(t) \leftrightarrow (\neg \textit{on}(t) \leftrightarrow \textit{on}(t+1))&lt;/math&gt;

The third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time &lt;math&gt;t&lt;/math&gt; if and only if the corresponding change predicate is true at time &lt;math&gt;t&lt;/math&gt;. To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions.

===Successor state axioms solution===
The value of a condition after the execution of an action can be determined by
the fact that the condition is true if and only if:

# the action makes the condition true; or
# the condition was previously true and the action does not make it false.

A [[successor state axiom]] is a formalization in logic of these two facts. For
example, if &lt;math&gt;\textit{opendoor}(t)&lt;/math&gt; and &lt;math&gt;\textit{closedoor}(t)&lt;/math&gt; are two
conditions used to denote that the action executed at time &lt;math&gt;t&lt;/math&gt; was
to open or close the door, respectively, the running example is encoded as
follows.

: &lt;math&gt;\neg \textit{open}(0)&lt;/math&gt;
: &lt;math&gt;\neg \textit{on}(0)&lt;/math&gt;
: &lt;math&gt;\textit{opendoor}(0)&lt;/math&gt;
: &lt;math&gt;\forall t . \textit{open}(t+1) \leftrightarrow \textit{opendoor}(t) \vee (\textit{open}(t) \wedge \neg \textit{closedoor}(t))&lt;/math&gt;

This solution is centered around the value of conditions, rather than the
effects of actions. In other words, there is an axiom for every condition,
rather than a formula for every action. Preconditions to actions (which are not
present in this example) are formalized by other formulae. The successor state
axioms are used in the variant to the [[situation calculus]] proposed by
[[Ray Reiter]].

===Fluent calculus solution===
The [[fluent calculus]] is a variant of the situation calculus. It solves the frame problem by using first-order logic
[[First-order logic#Formation rules|terms]], rather than predicates, to represent the states. Converting
predicates into terms in first order logic is called [[Reification (knowledge representation)|reification]]; the
fluent calculus can be seen as a logic in which predicates representing the
state of conditions are reified.

The difference between a predicate and a term in first order logic is that a term is a representation of an object (possibly a complex object composed of other objects), while a predicate represents a condition that can be true or false when evaluated over a given set of terms.

In the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term &lt;math&gt;\textit{open} \circ \textit{on}&lt;/math&gt;. It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term &lt;math&gt;\textit{open} \circ \textit{on}&lt;/math&gt; represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g., &lt;math&gt;\textit{state}(\textit{open} \circ \textit{on}, 10)&lt;/math&gt; means that this is the state at time &lt;math&gt;10&lt;/math&gt;.

The solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula:

: &lt;math&gt;\textit{state}(s \circ \textit{open}, 1) \leftrightarrow \textit{state}(s,0)&lt;/math&gt;

The action of closing the door, which makes a condition false instead of true, is represented in a slightly different way:

: &lt;math&gt;\textit{state}(s, 1) \leftrightarrow \textit{state}(s \circ \textit{open}, 0)&lt;/math&gt;

This formula works provided that suitable axioms are given about &lt;math&gt;\textit{state}&lt;/math&gt; and &lt;math&gt;\circ&lt;/math&gt;, e.g., a term containing two times the same condition is not a valid state (for example, &lt;math&gt;\textit{state}(\textit{open} \circ s \circ \textit{open}, t)&lt;/math&gt; is always false for every &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt;).

===Event calculus solution===
The [[event calculus]] uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated.

===Default logic solution===
The frame problem can be thought of as the problem of formalizing the principle that, by default, "everything is presumed to remain in the state in which it is" ([[Gottfried Wilhelm Leibniz|Leibniz]], "An Introduction to a Secret Encyclop&#230;dia", ''c''. 1679).  This default, sometimes called the ''commonsense law of inertia'', was expressed by [[Raymond Reiter]] in [[default logic]]:

: &lt;math&gt;\frac{R(x,s)\; :\ R(x,\textit{do}(a,s))}{R(x,\textit{do}(a,s))}&lt;/math&gt;

(if &lt;math&gt;R(x)&lt;/math&gt; is true in situation &lt;math&gt;s&lt;/math&gt;, and it can be assumed&lt;ref&gt;i.e., no contradicting information is known&lt;/ref&gt; that &lt;math&gt;R(x)&lt;/math&gt; remains true after executing action &lt;math&gt;a&lt;/math&gt;, then we can conclude that &lt;math&gt;R(x)&lt;/math&gt; remains true).

Steve Hanks and [[Drew McDermott]] argued, on the basis of their [[Yale shooting problem|Yale shooting]] example, that this solution to the frame problem is unsatisfactory.  Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates.

===Answer set programming solution===
The counterpart of the default logic solution in the language of [[answer set programming]] is a rule with [[stable model semantics#Strong negation|strong negation]]:

:&lt;math&gt;r(X,T+1) \leftarrow r(X,T),\ \hbox{not }\sim r(X,T+1)&lt;/math&gt;

(if &lt;math&gt;r(X)&lt;/math&gt; is true at time &lt;math&gt;T&lt;/math&gt;, and it can be assumed that &lt;math&gt;r(X)&lt;/math&gt; remains true at time &lt;math&gt;T+1&lt;/math&gt;, then we can conclude that &lt;math&gt;r(X)&lt;/math&gt; remains true).

===Action description languages===
[[Action description language]]s elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action &lt;math&gt;\textit{opendoor}&lt;/math&gt; makes the door open if not locked is expressed by:

: &lt;math&gt;\textit{opendoor}&lt;/math&gt; causes &lt;math&gt;\textit{open}&lt;/math&gt; if &lt;math&gt;\neg \textit{locked}&lt;/math&gt;

The semantics of an action description language depends on what the language can express (concurrent actions, delayed effects, etc.) and is usually based on [[transition system]]s.

Since domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to [[answer set programming]] rather than first-order logic.

==See also==
* [[Yale shooting problem]]
* [[Binding problem]]
* [[Ramification problem]]
* [[Qualification problem]]
* [[Common sense]]
* [[Commonsense reasoning]]
* [[Defeasible reasoning]]
* [[Non-monotonic logic]]
* [[Symbol grounding]]
* [[Linear logic]]

==Notes==
{{reflist}}

==References==
* {{cite journal | last1 = Doherty | first1 = P. | last2 = Gustafsson | first2 = J. | last3 = Karlsson | first3 = L. | last4 = Kvarnstr&#246;m | first4 = J. | year = 1998 | title = TAL: Temporal action logics language specification and tutorial | url = http://www.ep.liu.se/ej/etai/1998/009 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 273&#8211;306 }}
* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1993 | title = Representing action and change by logic programs | url = | journal = Journal of Logic Programming | volume = 17 | issue = | pages = 301&#8211;322 | doi=10.1016/0743-1066(93)90035-f}}
* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1998 | title = Action languages | url = http://www.ep.liu.se/ej/etai/1998/007 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 193&#8211;210 }}
* {{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic logic and temporal projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379&#8211;412 | doi=10.1016/0004-3702(87)90043-9}}
* {{cite journal | last1 = Levesque | first1 = H. | authorlink3 = Raymond Reiter | last2 = Pirri | first2 = F. | last3 = Reiter | first3 = R. | year = 1998 | title = Foundations for the situation calculus | url = http://www.ep.liu.se/ej/etai/1998/005 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 159&#8211;178 }}
* {{cite journal | last1 = Liberatore | first1 = P. | year = 1997 | title = The complexity of the language A | url = http://www.ep.liu.se/ej/etai/1997/002 | journal = [[Electronic Transactions on Artificial Intelligence]] | volume = 1 | issue = 1-3| pages = 13&#8211;37 }}
* {{cite journal |first=V. |last=Lifschitz |year=2012 |url=http://www.cs.utexas.edu/~vl/papers/jmc.pdf |title=The frame problem, then and now |publisher=[[University of Texas at Austin]]}} Presented at ''Celebration of John McCarthy's Accomplishments'', [[Stanford University]], March 25, 2012.
* {{cite journal | last1 = McCarthy | first1 = J. | last2 = Hayes | first2 = P. J. | year = 1969 | title = Some philosophical problems from the standpoint of artificial intelligence | url = http://www-formal.stanford.edu/jmc/mcchay69.html | journal = Machine Intelligence | volume = 4 | issue = | pages = 463&#8211;502 }}
* {{cite journal | last1 = McCarthy | first1 = J. | year = 1986 | title = Applications of circumscription to formalizing common-sense knowledge | url = http://www-formal.stanford.edu/jmc/applications.html | journal = Artificial Intelligence | volume = 28 | issue = | pages = 89&#8211;116 | doi=10.1016/0004-3702(86)90032-9}}
* {{cite journal | last1 = Miller | first1 = R. | last2 = Shanahan | first2 = M. | year = 1999 | title = The event-calculus in classical logic - alternative axiomatizations | url = http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html | journal = Electronic Transactions on Artificial Intelligence | volume = 3 | issue = 1| pages = 77&#8211;105 }}
* {{cite journal | last1 = Pirri | first1 = F. | last2 = Reiter | first2 = R. | year = 1999 | title = Some contributions to the metatheory of the Situation Calculus | url = | journal = [[Journal of the ACM]] | volume = 46 | issue = 3| pages = 325&#8211;361 | doi = 10.1145/316542.316545 }}
* {{cite journal | last1 = Reiter | first1 = R. | authorlink = Raymond Reiter | year = 1980 | title = A logic for default reasoning | url = | journal = Artificial Intelligence | volume = 13 | issue = | pages = 81&#8211;132 | doi=10.1016/0004-3702(80)90014-4}}
* {{cite book |authorlink=Raymond Reiter |first=Raymond |last=R. |year=1991 |chapter=The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression |editor=Lifschitz, Vladimir |title=Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy |pages=359&#8211;380 |publisher=Academic Press |location=New York}}
* {{cite journal | last1 = Sandewall | first1 = E. | year = 1972 | title = An approach to the Frame Problem and its Implementation | url = | journal = Machine Intelligence | volume = 7 | issue = | pages = 195&#8211;204 }}
* {{cite book |first=E. |last=Sandewall |year=1994 |title=Features and Fluents |volume=(vol. 1) |publisher=Oxford University Press |location=New York |isbn=0-19-853845-6}}
* {{cite book |first1=E. |last1=Sandewall |first2=Y. |last2=Shoham |year=1995 |chapter=Non-monotonic Temporal Reasoning |editor1=Gabbay, D. M. |editor2=Hogger, C. J. |editor3=Robinson, J. A. |title=Handbook of Logic in Artificial Intelligence and Logic Programming |volume=(vol. 4) |pages=439&#8211;498 |publisher=Oxford University Press |isbn=0-19-853791-3}}
* {{cite journal | last1 = Sandewall | first1 = E. | year = 1998 | title = Cognitive robotics logic and its metatheory: Features and fluents revisited | url = http://www.ep.liu.se/ej/etai/1998/010 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 307&#8211;329 }}
* {{cite book |first=M. |last=Shanahan |year=1997 |title=Solving the frame problem: A mathematical investigation of the common sense law of inertia |publisher=MIT Press}}
* {{cite journal | last1 = Thielscher | first1 = M. | year = 1998 | title = Introduction to the fluent calculus | url = http://www.ep.liu.se/ej/etai/1998/006 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 179&#8211;192 }}
* {{cite journal | last1 = Toth | first1 = J.A. | year = 1995 | title = Book review. Kenneth M. and Patrick J. Hayes, eds | url = | journal = Reasoning agents in a dynamic world: The frame problem. Artificial Intelligence | volume = 73 | issue = | pages = 323&#8211;369 | doi=10.1016/0004-3702(95)90043-8}}
* {{cite journal | last1 = Turner | first1 = H. | year = 1997 | title = Representing actions in logic programs and default theories: a situation calculus approach | url = http://www.d.umn.edu/~hudson/papers/ralpdt6.pdf | format = PDF | journal = Journal of Logic Programming | volume = 31 | issue = | pages = 245&#8211;298 | doi=10.1016/s0743-1066(96)00125-2}}

==External links==
* {{cite SEP |url-id=frame-problem |title=The Frame Problem}}
* [http://www-formal.stanford.edu/jmc/mcchay69/mcchay69.html Some Philosophical Problems from the Standpoint of Artificial Intelligence]; the original article of McCarthy and Hayes that proposed the problem.

{{John McCarthy navbox}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Epistemology]]
[[Category:Logic programming]]
[[Category:Philosophical problems]]
[[Category:1969 introductions]]</text>
      <sha1>3ggdzenmo6r4hr9cr5oeu1t5trdn1g3</sha1>
    </revision>
  </page>
  <page>
    <title>Event calculus</title>
    <ns>0</ns>
    <id>2897680</id>
    <revision>
      <id>725178201</id>
      <parentid>725178163</parentid>
      <timestamp>2016-06-14T01:53:14Z</timestamp>
      <contributor>
        <ip>86.30.201.123</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10676" xml:space="preserve">The '''event calculus''' is a [[logic]]al language for representing and reasoning about events and their effects first presented by [[Robert Kowalski]] and [[Marek Sergot]] in 1986.
It was extended by [[Murray Shanahan]] and [[Rob Miller (Computer Scientist)|Rob Miller]] in the 1990s. Similar to other languages for reasoning about change, the event calculus represents the effects of [[Action (artificial intelligence)|action]]s on [[fluent (artificial intelligence)|fluent]]s. However, [[Event (computing)|event]]s can also be external to the system. In the event calculus, one can specify the value of fluents at some given time points, the events that take place at given time points, and  their effects.

==Fluents and events==

In the event calculus, fluents are [[Reification (knowledge representation)|reified]]. This means that they are not formalized by means of [[Predicate (mathematics)|predicate]]s but by means of [[function (mathematics)|function]]s. A separate predicate &lt;math&gt;HoldsAt&lt;/math&gt; is used to tell which fluents hold at a given time point. For example, &lt;math&gt;HoldsAt(on(box,table),t)&lt;/math&gt; means that the box is on the table at time &lt;math&gt;t&lt;/math&gt;; in this formula, &lt;math&gt;HoldsAt&lt;/math&gt; is a predicate while &lt;math&gt;on&lt;/math&gt; is a function.

Events are also represented as terms. The effects of events are given using the predicates &lt;math&gt;Initiates&lt;/math&gt; and &lt;math&gt;Terminates&lt;/math&gt;. In particular, &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; means that,
if the event represented by the term &lt;math&gt;e&lt;/math&gt; is executed at time &lt;math&gt;t&lt;/math&gt;,
then the fluent &lt;math&gt;f&lt;/math&gt; will be true after &lt;math&gt;t&lt;/math&gt;.
The &lt;math&gt;Terminates&lt;/math&gt; predicate has a similar meaning, with the only difference 
being that &lt;math&gt;f&lt;/math&gt; will be false and not true after &lt;math&gt;t&lt;/math&gt;.

==Domain-independent axioms==

Like other languages for representing actions, the event calculus formalizes the correct evolution of the fluent via formulae telling the value of each fluent after an arbitrary action has been performed. The event calculus solves the [[frame problem]] in a way that is similar to the [[successor state axiom]]s of the [[situation calculus]]: a fluent is true at time &lt;math&gt;t&lt;/math&gt; if and only if it has been made true in the past and has not been made false in the meantime.
 
:&lt;math&gt;HoldsAt(f,t) \leftarrow
[Happens(e,t_1) \wedge Initiates(e,f,t_1) 
\wedge (t_1&lt;t) \wedge \neg Clipped(t_1,f,t)]&lt;/math&gt;

This formula means that the fluent represented by the term &lt;math&gt;f&lt;/math&gt; is true at time &lt;math&gt;t&lt;/math&gt; if:

# an event &lt;math&gt;e&lt;/math&gt; has taken place: &lt;math&gt;Happens(e,t_1)&lt;/math&gt;;
# this took place in the past: &lt;math&gt;t_1&lt;t&lt;/math&gt;;
# this event has the fluent &lt;math&gt;f&lt;/math&gt; as an effect: &lt;math&gt;Initiates(e,f,t_1)&lt;/math&gt;; 
# the fluent has not been made false in the meantime: &lt;math&gt;Clipped(t_1,f,t)&lt;/math&gt;

A similar formula is used to formalize the opposite case in which a fluent is false at a given time. Other formulae are also needed for correctly formalizing fluents before they have been effects of an event. These formulae are similar to the above, but &lt;math&gt;Happens(e,t_1) \wedge Initiates(e,f,t_1)&lt;/math&gt; is replaced by &lt;math&gt;HoldsAt(f,t_1)&lt;/math&gt;.

The &lt;math&gt;Clipped&lt;/math&gt; predicate, stating that a fluent has been made false during an interval, can be axiomatized, or simply taken as a shorthand, as follows:

:&lt;math&gt;Clipped(t_1,f,t_2) \equiv
\exists e,t 
[Happens(e,t) \wedge (t_1 \leq t &lt; t_2) \wedge Terminates(e,f,t)]&lt;/math&gt;

==Domain-dependent axioms==

The axioms above relate the value of the predicates &lt;math&gt;HoldsAt&lt;/math&gt;, &lt;math&gt;Initiates&lt;/math&gt; and &lt;math&gt;Terminates&lt;/math&gt;, but do not specify which fluents are known to be true and which events actually make fluents true or false. This is done by using a set of domain-dependent axioms. The known values of fluents are stated as simple literals &lt;math&gt;HoldsAt(f,t)&lt;/math&gt;. The effects of events are stated by formulae relating the effects of events with their preconditions. For example, if the event &lt;math&gt;open&lt;/math&gt; makes the fluent &lt;math&gt;isopen&lt;/math&gt; true, but only if &lt;math&gt;haskey&lt;/math&gt; is currently true, the corresponding formula in the event calculus is:

:&lt;math&gt;Initiates(e,f,t) \equiv
[ e=open \wedge f=isopen \wedge HoldsAt(haskey, t)] \vee \cdots
&lt;/math&gt;

The right-hand expression of this equivalence is composed of a disjunction: for each event and fluent that can be made true by the event, there is a disjunct saying that &lt;math&gt;e&lt;/math&gt; is actually that event, that &lt;math&gt;f&lt;/math&gt; is actually that fluent, and that the precondition of the event is met.

The formula above specifies the [[truth value]] of &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; for every possible event and fluent. As a result, all effects of all events have to be combined in a single formulae. This is a problem, because the addition of a new event requires modifying an existing formula rather than adding new ones. This problem can be solved by the application of [[Circumscription (logic)|circumscription]] to a set of formulae each specifying one effect of one event:

: &lt;math&gt;Initiates(open, isopen, t) \leftarrow HoldsAt(haskey, t)&lt;/math&gt;
: &lt;math&gt;Initiates(break, isopen, t) \leftarrow HoldsAt(hashammer, t)&lt;/math&gt;
: &lt;math&gt;Initiates(break, broken, t) \leftarrow HoldsAt(hashammer, t)&lt;/math&gt;

These formulae are simpler than the formula above, because each effect of each event can be specified separately. The single formula telling which events &lt;math&gt;e&lt;/math&gt; and fluents &lt;math&gt;f&lt;/math&gt; make &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; true has been replaced by a set of smaller formulae, each one telling the effect of an event on a fluent.
 
However, these formulae are not equivalent to the formula above. Indeed, they only specify sufficient conditions for &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; to be true, which should be completed by the fact that &lt;math&gt;Initiates&lt;/math&gt; is false in all other cases. This fact can be formalized by simply circumscribing the predicate &lt;math&gt;Initiates&lt;/math&gt; in the formula above. It is important to note that this circumscription is done only on the formulae specifying &lt;math&gt;Initiates&lt;/math&gt; and not on the domain-independent axioms. The predicate &lt;math&gt;Terminates&lt;/math&gt; can be specified in the same way &lt;math&gt;Initiates&lt;/math&gt; is.

A similar approach can be taken for the &lt;math&gt;Happens&lt;/math&gt; predicate. The evaluation of this predicate can be enforced by formulae specifying not only when it is true and when it is false:

:&lt;math&gt;Happens(e,t) \equiv
(e=open \wedge t=0) \vee (e=exit \wedge t=1) \vee \cdots&lt;/math&gt;

Circumscription can simplify this specification, as only necessary conditions can be specified:

:&lt;math&gt;Happens(open, 0)&lt;/math&gt;
:&lt;math&gt;Happens(exit, 1)&lt;/math&gt;

Circumscribing the predicate &lt;math&gt;Happens&lt;/math&gt;, this predicate will be false at all points in which it is not explicitly specified to be true. This circumscription has to be done separately from the circumscription of the other formulae. In other words, if &lt;math&gt;F&lt;/math&gt; is the set of formulae of the kind &lt;math&gt;Initiates(e,f,t) \leftarrow \cdots&lt;/math&gt;, &lt;math&gt;G&lt;/math&gt; is the set of formulae &lt;math&gt;Happens(e, t)&lt;/math&gt;, and &lt;math&gt;H&lt;/math&gt; are the domain independent axioms, the correct formulation of the domain is:

:&lt;math&gt;Circ(F; Initiates, Terminates) \wedge
Circ(G; Happens) \wedge H&lt;/math&gt;

==The event calculus as a logic program==

The event calculus was originally formulated as a set of [[Horn clauses]] augmented with [[negation as failure]] and could be run as a [[Prolog]] program. 
In fact, circumscription is one of the several semantics that can be given to negation as failure, and is closely related to the completion semantics (in which "if" is interpreted as "if and only if" &amp;mdash; see [[logic programming]]).

==Extensions and applications==

The original event calculus paper of Kowalski and Sergot focused on applications to database updates and narratives. Extensions of the event 
calculus can also formalize non-deterministic actions, concurrent actions, actions with delayed effects, gradual changes, actions with duration, continuous change, and non-inertial fluents.

Kave Eshghi showed how the event calculus can be used for planning, using [[Abduction (logic)|abduction]] to generate hypothetical events in [[Abductive Logic Programming|abductive logic programming]]. Van Lambalgen and Hamm showed how the event calculus can also be used to give an algorithmic semantics to tense and aspect in natural language using constraint logic programming.

==Reasoning tools==

In addition to Prolog and its variants, several other tools for reasoning using the event calculus are also available:
* [http://www.doc.ic.ac.uk/~mpsha/planners.html Abductive Event Calculus Planners]
* [http://decreasoner.sourceforge.net/ Discrete Event Calculus Reasoner]
* [http://reasoning.eas.asu.edu/ecasp/ Event Calculus Answer Set Programming]
* [https://www.inf.unibz.it/~montali/tools.html Reactive Event Calculus]

==See also==

* [[First-order logic]]
* [[Frame problem]]
* [[Situation calculus]]

==References==
* Brandano, S. (2001) "[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnumber=20130&amp;arnumber=930691&amp;count=35&amp;index=2 The Event Calculus Assessed,]" ''IEEE TIME Symposium'': 7-12.
* Eshghi, K. (1988) "Abductive Planning with Event Calculus," ''ICLP/SLP'': 562-79.
* Kowalski, R. (1992) "Database updates in the event calculus," ''Journal of Logic Programming 12 (162)'': 121-46.
* -------- and M. Sergot (1986) "[http://www.doc.ic.ac.uk/~rak/papers/event%20calculus.pdf A Logic-Based Calculus of Events,]" ''New Generation Computing 4'': 67&#8211;95.
* -------- and F. Sadri (1995) "Variants of the Event Calculus," ''ICLP'': 67-81.
* Miller, R., and M. Shanahan (1999) "[http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html The event-calculus in classical logic &#8212; alternative axiomatizations,]" ''[[Electronic Transactions on Artificial Intelligence]]'' 3(1): 77-105.
* Mueller, Erik T. (2015). ''Commonsense Reasoning: An Event Calculus Based Approach (2nd Ed.)''. Waltham, MA: Morgan Kaufmann/Elsevier. ISBN 978-0128014165. (Guide to using the event calculus)
* Shanahan, M. (1997) ''Solving the frame problem: A mathematical investigation of the common sense law of inertia''. MIT Press.
* -------- (1999) "[http://www.springerlink.com/content/1bxk8gd0n6pajxbq/?p=8f3428a89bad4589a949d74b6f0ec98d&amp;pi=0 The Event Calculus Explained,]" Springer Verlag, LNAI (1600): 409-30.
* Van Lambalgen, M., and F. Hamm (2005) ''The proper treatment of events''. Oxford and Boston: Blackwell Publishing.

[[Category:1986 introductions]]
[[Category:Logic in computer science]]
[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Logical calculi]]</text>
      <sha1>1glfkhixe5v7en3g1zq7ohiwlthnvhh</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic analysis (knowledge representation)</title>
    <ns>0</ns>
    <id>13645056</id>
    <revision>
      <id>743107116</id>
      <parentid>730580376</parentid>
      <timestamp>2016-10-07T21:27:33Z</timestamp>
      <contributor>
        <username>Klbrain</username>
        <id>11677590</id>
      </contributor>
      <comment>Removing stale merge proposal from June 2013; no support since case was made more than 3 years ago (see [[Talk:Semantic analysis (knowledge representation)#Semantic analysis (linguistics)]]); different topics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1310" xml:space="preserve">{{Cleanup|date=November 2008}}
{{context|date=February 2014}}
{{Vague|date=February 2009}}
'''Semantic analysis''' is a method for eliciting and representing [[knowledge]] about [[organisation]]s.{{Vague|date=February 2009}}&lt;ref&gt;[[Liu Kecheng]], (2000) Semiotics in [[information systems engineering]], Cambridge University Press.&lt;/ref&gt; 

Initially the problem must be defined by domain experts and passed to the project analyst(s). The next step is the generation of candidate affordances. This step will generate a list of semantic units that may be included in the schema. The candidate grouping follows where some of the semantic units that will appear in the schema are placed in simple groups. Finally the groups will be integrated together into an [[Ontology_(information_science)|ontology]] chart. 

Semantic analysis always starts from the problem definition which if not clear, require the analyst to employ relevant [[literature]], [[interview]]s with the [[Stakeholder (corporate)|stakeholders]] and other techniques towards collecting supplementary [[information]]. All assumptions made must be genuine and not limiting the system. 

== See also ==
* [[Semantic analysis (machine learning)]]
* [[Ontology chart]]

==References==
{{reflist}}

[[Category:Knowledge representation]]

{{Library-stub}}</text>
      <sha1>g4mc8lhahve376d9f63csabsw11cn7c</sha1>
    </revision>
  </page>
  <page>
    <title>Unique name assumption</title>
    <ns>0</ns>
    <id>15056340</id>
    <revision>
      <id>621641792</id>
      <parentid>526557033</parentid>
      <timestamp>2014-08-17T16:06:56Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>copyedit, expand, add reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1351" xml:space="preserve">The '''unique name assumption''' is a simplifying assumption made in some [[ontology (computer science)|ontology]] languages and [[description logic]]s. In logics with the unique name assumption, different names always refer to different entities in the world.&lt;ref&gt;{{Cite AIMA |edition=2 |pages=333}}&lt;/ref&gt;

The standard ontology language [[Web Ontology Language|OWL]] does not make this assumption, but provides explicit constructs to express whether two names denote the same or distinct entities.&lt;ref&gt;{{cite conference |first1=Jiao |last1=Tao |first2=Evren |last2=Sirin |first3=Jie |last3=Bao |first4=Deborah L. |last4=McGuinness |title=Integrity constraints in OWL |conference=Proc. AAAI |year=2010 |url=http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931}}&lt;/ref&gt;&lt;ref&gt;[http://www.w3.org/TR/owl-ref/ OWL Web Ontology Language Reference]&lt;/ref&gt;
* &lt;code&gt;owl:sameAs&lt;/code&gt; is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to the same individual or entity.
* &lt;code&gt;owl:differentFrom&lt;/code&gt; is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to different individuals or entities.

==See also==
* [[Closed-world assumption]]
* [[Coreference]]

==References==
{{reflist}}

[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]

{{logic-stub}}</text>
      <sha1>1yndkh76w1d0pnf7ez2v5d4gevwvbt1</sha1>
    </revision>
  </page>
  <page>
    <title>Digital curation</title>
    <ns>0</ns>
    <id>16702334</id>
    <revision>
      <id>754518343</id>
      <parentid>752422419</parentid>
      <timestamp>2016-12-13T02:41:45Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11750" xml:space="preserve">'''Digital curation''' is the selection,&lt;ref name="ALA, Scime" &gt;{{Cite web
  |title=The Content Strategist as Digital Curator
  |author=Erin Scime
  |date=8 December 2009
  |publisher=[[A List Apart]]
  |url=http://www.alistapart.com/articles/content-strategist-as-digital-curator/
}}&lt;/ref&gt; [[Preservation (library and archival science)|preservation]], maintenance,  collection and [[archiving]] of [[Digital data|digital]] assets.&lt;ref name="paper"&gt;{{Cite book | last1 = Rusbridge | first1 = C. | last2 = Buneman | first2 = P. | authorlink2 = Peter Buneman| last3 = Burnhill | first3 = P. | last4 = Giaretta | first4 = D. | last5 = Ross | first5 = S. | last6 = Lyon | first6 = L. | last7 = Atkinson | first7 = M. | authorlink7 = Malcolm Atkinson| chapter = The Digital Curation Centre: A Vision for Digital Curation | doi = 10.1109/LGDI.2005.1612461 | title = 2005 IEEE International Symposium on Mass Storage Systems and Technology | pages = 31 | year = 2005 | url = http://eprints.erpanet.org/82/01/DCC_Vision.pdf| isbn = 0-7803-9228-0 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="dccdefn"&gt;{{cite web |title=What is Digital Curation? |publisher=[[Digital Curation Centre]] |url=http://www.dcc.ac.uk/about/what |accessdate=2008-04-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=Digital curation |author=Elizabeth Yakel |publisher=Emerald Group Publishing |year=2007 |url=http://www.ingentaconnect.com/content/mcb/164/2007/00000023/00000004/art00003 |accessdate=2008-04-01}}&lt;/ref&gt;
Digital curation establishes, maintains and adds value to repositories of digital data for present and future use.&lt;ref name="dccdefn"/&gt; This is often accomplished by [[archivist]]s, librarians, scientists, historians, and scholars. Enterprises are starting to use digital curation to improve the quality of information and data within their operational and strategic processes.&lt;ref&gt;E. Curry, A. Freitas, and S. O'Ri&#225;in, [http://3roundstones.com/led_book/led-curry-et-al.html "The Role of Community-Driven Data Curation for Enterprises,"] in Linking Enterprise Data, D. Wood, Ed. Boston, MA: Springer US, 2010, pp. 25-47.&lt;/ref&gt; Successful digital curation will mitigate digital obsolescence, keeping the information accessible to users indefinitely. 

The term ''[[curator|curation]]'' in the past commonly referred to museum and library professionals. It has since been applied to interaction with [[social media]] including compiling digital images, web links and movie files.

==Approaches==
===Create new representation===
For some topics, knowledge is embodied in forms that have not been conducive to print, such as how choreography of dance or of the motion of skilled workers or artisans is difficult to encode. New digital approaches such as 3d holograms and other computer-programmed expressions are developing. 

For mathematics, it seems possible for a new common language to be developed that would express mathematical ideas in ways that can be digitally stored, linked, and made accessible. The [[Global Digital Mathematics Library]] is a project to define and develop such a language.

===Convert print resources===
The process of converting printed resources into digital collections has been epitomized to some degree by librarians and related specialists. For example,
The [[Digital Curation Centre]] is claimed to be a "world leading centre of expertise in digital information curation"&lt;ref name="Digital Curation Centre"&gt;{{cite web|last=Digital Curation Centre|title=About the DCC|url=http://www.dcc.ac.uk/about-us|work=Website|publisher=Digital Curation Centre|accessdate=6 March 2013}}&lt;/ref&gt; that assists higher education research institutions in such conversions. The DCC, based in the UK, began operations in early 2004 and suggests the following as a general outline of their approach to digital curation:

* Conceptualize: Consider what digital material you will be creating and develop storage options. Take into account websites, publications, email, among other types of digital output.
* Create: Produce digital material and attach all relevant metadata, typically the more metadata the more accessible the information.
* Access and use: Determine the level of accessibility for the range of digital material created. Some material may be accessible only by password and other material may be freely accessible to the public.
* Appraise and select: Consult the mission statement of the institution or private collection and determine what digital data is relevant. There may also be legal guidelines in place that will guide the decision process for a particular collection.
* Dispose: Discard any digital material that is not deemed necessary to the institution.
* Ingest: Send digital material to the predetermined storage solution. This may be an archive, repository or other facility.
* Preservation action: Employ measures to maintain the integrity of the digital material.
* Reappraise: Reevaluate material to ensure that is it still relevant and is true to its original form.
* Store: Secure data within the predetermined storage facility.
* Access and reuse: Routinely check that material is still accessible for the intended audience and that the material has not been compromised through multiple uses.
* Transform: If desirable or necessary the material may be transferred into a different digital format.

==="Sheer curation"===
''Sheer curation'' is an approach to digital curation where curation activities are quietly integrated into the normal work flow of those creating and managing data and other digital assets. The word sheer is used to emphasize the lightweight and virtually transparent nature of these curation activities. The term ''sheer curation'' was coined by Alistair Miles in the ImageStore project,&lt;ref&gt;[http://imageweb.zoo.ox.ac.uk/wiki/index.php/The_ImageStore_Project The ImageStore Project - ImageWeb&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and the UK Digital Curation Centre's SCARP project.&lt;ref&gt;[http://www.dcc.ac.uk/scarp/ Digital Curation Centre: DCC SCARP Project&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; The approach depends on curators having close contact or 'immersion' in data creators' working practices. An example is the case study of a neuroimaging research group by Whyte et al., which explored ways of building its digital curation capacity around the apprenticeship style of learning of neuroimaging researchers, through which they share access to datasets and re-use experimental procedures.&lt;ref&gt;Whyte, A., Job, D., Giles, S. and Lawrie, S. (2008) '[http://www.ijdc.net/index.php/ijdc/article/view/74/53 Meeting Curation Challenges in a Neuroimaging Group]', The International Journal of Digital Curation Issue 1, Volume 3, 2008&lt;/ref&gt;      

Sheer curation depends on the hypothesis that good data and digital asset management at the point of creation and primary use is also good practice in preparation for sharing, publication and/or [[long-term preservation]] of these assets. Therefore, sheer curation attempts to identify and promote tools and good practices in local data and digital asset management in specific domains, where those tools and practices add immediate value to the creators and primary users of those assets. Curation can best be supported by identifying existing practices of sharing, stewardship and re-use that add value, and augmenting them in ways that both have short-term benefits, and in the longer term reduce risks to digital assets or provide new opportunities to sustain their long-term accessibility and re-use value.    

The aim of sheer curation is to establish a solid foundation for other curation activities which may not directly benefit the creators and primary users of digital assets, especially those required to ensure long-term preservation. By providing this foundation, further curation activities may be carried out by specialists at appropriate institutional and organisation levels, whilst causing the minimum of interference to others.

A similar idea is ''curation at source'' used in the context of Laboratory Information Management Systems [[Laboratory information management system|LIMS]]. This refers more specifically to automatic recording of [[metadata]] or information about data at the point of capture, and has been developed to apply semantic web techniques to integrate laboratory instrumentation and documentation systems.&lt;ref&gt;Frey, J. [http://www.allhands.org.uk/2008/programme/jeremyfrey.cfm 'Sharing and Collaboration' keynote presentation at UK e-Science All Hands Meeting], 8&#8211;11 September 2008, Edinburgh&lt;/ref&gt; Sheer curation and curation-at-source can be contrasted with post hoc [[digital preservation]], where a project is initiated to preserve a collection of digital assets that have already been created and are beyond the period of their primary use.

===Channelisation===
''Channelisation'' is curation of digital assets on the web, often by brands and media companies, into continuous flows of content, turning the user experience from a lean-forward interactive medium, to a lean-back passive medium. The curation of content can be done by an independent third party, that selects media from any number of on-demand outlets from across the globe and adds them to a playlist to offer a digital "channel" dedicated to certain subjects, themes, or interests so that the end user would see and/or hear a continuous stream of content.

==Challenges==
* Storage format evolution and obsolescence&lt;ref name=ijdcpw200711&gt;{{cite web|title=Digital Preservation Theory and Application: Transcontinental Persistent Archives Testbed Activity |publisher=The International Journal of Digital Curation |url=http://www.ijdc.net/./ijdc/article/view/43/50 |author=Paul Watry |date=November 2007 |accessdate=2008-04-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20080315030030/http://www.ijdc.net:80/ijdc/article/view/43/50 |archivedate=2008-03-15 |df= }}&lt;/ref&gt; 
* Rate of creation of new data and data sets
* Maintaining accessibility to data through links and search results
* Comparability of [[semantic]] and [[ontology|ontological]] definitions of data sets&lt;ref name="ijdcpw200711"/&gt;

===Responses===
* Specialized research institutions&lt;ref&gt;[http://www.dcc.ac.uk/ Digital Curation Centre]&lt;/ref&gt;&lt;ref&gt;[http://www.dpconline.org/ Digital Preservation Coalition]&lt;/ref&gt;
* Academic courses
* Dedicated symposia&lt;ref&gt;[http://www.ils.unc.edu/digccurr2007/ DigCCurr 2007 - an international symposium on Digital Curation, April 18-20, 2007]&lt;/ref&gt;&lt;ref&gt;[http://stardata.nrf.ac.za/nadicc 1st African Digital Management and Curation Conference and Workshop - Date: 12-13 February 2008]&lt;/ref&gt;
* Peer reviewed technical and industry journals&lt;ref&gt;[http://www.ijdc.net/ International Journal of Digital Curation]&lt;/ref&gt;

==See also==
* [[Biocurator|Biocuration]]
* [[Curator]]
* [[Data curation]]
* [[Data format management]]
* [[Digital artifactual value]]
* [[Digital asset management]]
* [[Digital obsolescence]]
* [[International Society for Biocuration]]

==References==
{{reflist|33em}}

==External links==
*[https://www.youtube.com/watch?v=pbBa6Oam7-w Animations introducing digital preservation and curation]
*[http://www.alistapart.com/articles/content-strategist-as-digital-curator/ Content Strategist as Digital Curator], A List Apart Journal, December 2009
*[http://www.dcc.ac.uk/ Digital Curation Centre]
*[http://journals.tdl.org/jodi/article/view/229/183 Digital Curation and Trusted Repositories: Steps Toward Success]*[http://www.digcur-education.org DigCurV] A project funded by the European Commission to establish a curriculum framework for vocational training in digital curation.

[[Category:Archival science]]
[[Category:Databases]]
[[Category:Knowledge representation]]
[[Category:Digital libraries]]
[[Category:Digital preservation]]</text>
      <sha1>iht2tsgyvr5lzqmabyzwncvh8gy27j3</sha1>
    </revision>
  </page>
  <page>
    <title>Ronald J. Brachman</title>
    <ns>0</ns>
    <id>853832</id>
    <revision>
      <id>760061969</id>
      <parentid>744809531</parentid>
      <timestamp>2017-01-14T19:15:17Z</timestamp>
      <contributor>
        <ip>96.248.115.152</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4640" xml:space="preserve">{{Infobox scientist
| name              = Ronald Jay Brachman
| image             = &lt;!--(filename only)--&gt;
| image_size        = 
| alt               = 
| caption           = 
| birth_date        = {{Birth year and age|1949}} 
| birth_place       = 
| death_date        = &lt;!-- {{Death date and age|YYYY|MM|DD|YYYY|MM|DD}} (death date then birth date) --&gt;
| death_place       = 
| resting_place             = 
| resting_place_coordinates = &lt;!-- {{Coord|LAT|LONG|type:landmark|display=inline,title}} --&gt;
| residence         = 
| citizenship       = 
| nationality       = 
| fields            = 
| workplaces        = [[Harvard University]]&lt;br&gt;[[Yahoo! Research]]&lt;br&gt;[[AT&amp;T Corporation]]&lt;br&gt;[[DARPA]]
| alma_mater        = [[Harvard University]]&lt;br&gt;[[Princeton University]]
| thesis_title      = A structural paradigm for representing knowledge
| thesis_url        = https://books.google.com/books?id=ThS-HAAACAAJ
| thesis_year       = 1977
| doctoral_advisor  = William A. Woods
| academic_advisors = 
| doctoral_students = 
| notable_students  = 
| known_for         = 
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = 
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           = {{URL|www.brachman.org}}&lt;br&gt;{{URL|research.yahoo.com/Ron_Brachman}}
| footnotes         = 
| spouse            = 
}}'''Ronald Jay "Ron" Brachman''' (born 1949) is the director of the Jacobs Technion-Cornell Institute at [[Cornell Tech]].&lt;ref&gt;{{Cite web|url=http://tech.cornell.edu/news/ron-brachman-joins-the-jacobs-technion-cornell-institute-at-cornell-tech-as|title=Ron Brachman Joins the Jacobs Technion-Cornell Institute at Cornell Tech as the New Director|website=Cornell Tech|access-date=2016-05-25}}&lt;/ref&gt; Previously, he was the Chief Scientist of Yahoo! and head of [[Yahoo! Labs]].  Prior to that, he was the Associate Head of Yahoo! Labs and Head of Worldwide Labs and Research Operations.

==Education==
Brachman earned his [[Bachelor of Engineering|B.S.E.E.]] degree from [[Princeton University]], and his [[Master of Science|S.M.]] and [[Doctor of Philosophy|Ph.D.]] degrees from [[Harvard University]].

==Career==
Prior to working at Yahoo!, Brachman worked at [[DARPA]] as the Director of the [[Information Processing Techniques Office]] (IPTO), one of DARPA's eight offices at the time.  While at IPTO, he helped develop [[DARPA]]'s Cognitive Systems research efforts. Before that, he worked at [[AT&amp;T Corporation|AT&amp;T]] [[Bell Labs|Bell Laboratories]] ([[Murray Hill, New Jersey]]) as the Head of the [[Artificial Intelligence]] Principles Research Department (2004) and Director of the Software and Systems Research Laboratory.  When AT&amp;T split with Lucent in 1996, he became Communications Services Research Vice President and was one of the founders of [[AT&amp;T Labs]].

He is considered by some to be the godfather{{citation needed|date=August 2012}} of [[Description Logic]], the logic-based [[knowledge representation]] [[Semantics (computer science)|formalism]] underlying the [[Web Ontology Language]] OWL.]

==Publications==
He is the co-author with [[Hector Levesque]] of a popular book on [[knowledge representation and reasoning]]&lt;ref&gt;{{cite book |author1=Reiter, Ray |author2=Brachman, Ronald J. |author3=Levesque, Hector J. |title=Knowledge representation |publisher=MIT Press |location=Cambridge, Mass |year=1992 |pages= |isbn=0-262-52168-7 |oclc= |doi= |accessdate=}}&lt;/ref&gt;&lt;ref&gt;{{cite book |author1=Levesque, Hector J. |author2=Brachman, Ronald J. |title=Knowledge representation and reasoning |publisher=Elsevier/Morgan Kaufmann |location=Amsterdam |year=2004 |pages= |isbn=1-55860-932-6 |oclc= |doi= |accessdate=}}&lt;/ref&gt; and many scientific papers.&lt;ref name="microsoft"&gt;{{AcademicSearch|9029466}}&lt;/ref&gt;&lt;ref name="dblp"&gt;{{DBLP|name=Ronald J. Brachman}}&lt;/ref&gt;&lt;ref&gt;Ronald J. Brachman (1983) "What IS-A is and isn't. An Analysis of Taxonomic Links in [[Semantic network|Semantic Networks]]"; ''IEEE Computer'', 16 (10); October.&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
* [http://www.cc.gatech.edu/events/dr-ronald-brachman-yahoo-research-distinguished-guest-lecture External biography]

{{DEFAULTSORT:Brachman, Ronald J.}}
[[Category:Living people]]
[[Category:Artificial intelligence researchers]]
[[Category:Knowledge representation]]
[[Category:Harvard University alumni]]
[[Category:Princeton University alumni]]
[[Category:Fellow Members of the IEEE]]
[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]
[[Category:Yahoo! employees]]
[[Category:1959 births]]


{{compu-bio-stub}}</text>
      <sha1>e6ik8u1v2uwu55qxpbwdfw6ehqmkwop</sha1>
    </revision>
  </page>
  <page>
    <title>ITools Resourceome</title>
    <ns>0</ns>
    <id>17727869</id>
    <revision>
      <id>723083957</id>
      <parentid>521403614</parentid>
      <timestamp>2016-05-31T22:10:34Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* top */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1292" xml:space="preserve">{{lowercase title}}
[[Image:Biositemap iTools NCBC.png|thumb|right|300px| NCBC iTools]]
'''iTools'''&lt;ref&gt;{{cite journal|vauthors=Dinov ID, Rubin D, Lorensen W, Dugan J, Ma J, Murphy S, Kirschner B, Bug W, Sherman M, Floratos A, Kennedy D, Jagadish HV, Schmidt J, Athey B, Califano A, Musen M, Altman R, Kikinis R, Kohane I, Delp S, Parker DS, Toga AW | title=iTools: A Framework for Classification, Categorization and Integration of Computational Biology Resources | journal=PLoS ONE |volume=3|issue=5|pages= e2265| doi=10.1371/journal.pone.0002265 | year=2008 |url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0002265 | pmid=18509477 | pmc=2386255}}&lt;/ref&gt; is a distributed infrastructure for managing, discovery, comparison and integration of computational biology resources. iTools employs [[Biositemap]] technology to retrieve and service meta-data about diverse bioinformatics data services, tools, and web-services. iTools is developed by the [[National Centers for Biomedical Computing]] as part of the [http://nihroadmap.nih.gov/ NIH Road Map Initiative].

==See also==
* [[Biositemaps]]

==References==
&lt;references/&gt;

== External links ==
* [http://iTools.ccb.ucla.edu Interactive iTools Server]

[[Category:Knowledge representation]]
[[Category:Bioinformatics]]</text>
      <sha1>360xzohfzms2si46t7wj19jsjkbmdg6</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge bases</title>
    <ns>14</ns>
    <id>20750664</id>
    <revision>
      <id>732976277</id>
      <parentid>718313668</parentid>
      <timestamp>2016-08-04T14:41:23Z</timestamp>
      <contributor>
        <username>Narky Blert</username>
        <id>22041646</id>
      </contributor>
      <comment>DN tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="425" xml:space="preserve">A '''[[Knowledge base]]''' is a special kind of [[database]] for [[knowledge management]]. It provides the means for the computerized collection, organization, and [[retrieval]]{{dn|date=August 2016}} of [[knowledge]].  It is also used for specified information and as a [[personal knowledge base]]

[[Category:Online databases]]
[[Category:Semantic Web]]
[[Category:Knowledge representation]]
[[Category:Types of databases]]</text>
      <sha1>gic1r84zdmdzxvl302jwldsoh6nc0mv</sha1>
    </revision>
  </page>
  <page>
    <title>User modeling</title>
    <ns>0</ns>
    <id>12781902</id>
    <revision>
      <id>753810701</id>
      <parentid>723169565</parentid>
      <timestamp>2016-12-09T09:20:08Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[adaptive system]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12978" xml:space="preserve">'''User modeling''' is the subdivision of [[human&#8211;computer interaction]] which describes the
process of building up and modifying a conceptual understanding of the user. The main goal of user modeling is customization and [[Adaptation (computer science)|adaptation of systems]] to the user's specific needs. The system needs to "say the 'right' thing at the 'right' time in the 'right' way".&lt;ref name=Fischer&gt;{{Citation
  | last1 = Fischer | first1 = Gerhard
  | title = User Modeling in Human-Computer Interaction
  | journal = User Modeling and User-Adapted Interaction 11
  | pages = 65&#8211;68
  | year = 2001 }}&lt;/ref&gt; To do so it needs an internal representation of the user. Another common purpose is modeling specific kinds of users, including modeling of their skills and declarative knowledge, for use in automatic software-tests.&lt;ref name=JohnsonTaatgen&gt; {{Citation
  | last1 = Johnson | first1 = Addie
  | last2=Taatgen | first2 = Niels
  | chapter = User Modeling
  | title = Handbook of human factors in Web design
  | pages = 424&#8211;439
  | publisher = Lawrence Erlbaum Associates
  | year = 2005 }}&lt;/ref&gt; User-models can thus serve as a cheaper alternative to [[user testing]].

== Background ==

A user model is the collection and categorization of personal data associated with a specific user. Therefore, it is the basis for any adaptive changes to the system's behavior. Which data is included in the model depends on the purpose of the application. It can include personal information such as users' names and ages, their interests, their skills and knowledge, their goals and plans, their preferences and their dislikes or data about their behavior and their interactions with the system.

There are different design patterns for user models, though often a mixture of them is used.&lt;ref name=JohnsonTaatgen /&gt;&lt;ref&gt;{{Citation
  | last1 = Hothi | first1 = Jatinder
  | last2=Hall | first2 = Wendy
  | title = An Evaluation of Adapted Hypermedia Techniques Using Static User Modelling
  | journal = Proceedings of the 2nd Workshop on Adaptive Hypertext and Hypermedia
  | place = Southampton University, Electronics and Computer Science University Road, Southampton, Hampshire, UK
  | year = June 1998
  | url = http://wwwis.win.tue.nl/ah98/Hothi/Hothi.html }}&lt;/ref&gt;
* '''Static user models'''
:Static user models are the most basic kinds of user models. Once the main data is gathered they are normally not changed again, they are static. Shifts in users' preferences are not registered and no learning algorithms are used to alter the model.
* '''Dynamic user models'''
:Dynamic user models allow a more up to date representation of users. Changes in their interests, their learning progress or interactions with the system are noticed and influence the user models. The models can thus be updated and take the current needs and goals of the users into account.
* '''Stereotype based user models '''
:Stereotype based user models are based on [[Demographics|demographic statistics]]. Based on the gathered information users are [[Classification_in_machine_learning|classified]] into common stereotypes. The system then adapts to this stereotype. The application therefore can make assumptions about a user even though there might be no data about that specific area, because demographic studies have shown that other users in this stereotype have the same characteristics. Thus, stereotype based user models mainly rely on statistics and do not take into account that personal attributes might not match the stereotype. However, they allow predictions about a user even if there is rather little information about him or her.
* '''Highly adaptive user models'''
:Highly adaptive user models try to represent one particular user and therefore allow a very high adaptivity of the system. In contrast to stereotype based user models they do not rely on demographic statistics but aim to find a specific solution for each user. Although users can take great benefit from this high adaptivity, this kind of model needs to gather a lot of information first.

== Data gathering ==

Information about users can be gathered in several ways. There are three main methods:

* '''Asking for specific facts while (first) interacting with the system'''&lt;ref name=JohnsonTaatgen /&gt;
:Mostly this kind of data gathering is linked with the registration process. While registering users are asked for specific facts, their likes and dislikes and their needs. Often the given answers can be altered afterwards.
* '''Learning users' preferences by observing and interpreting their interactions with the system'''&lt;ref name=JohnsonTaatgen /&gt;
:In this case users are not asked directly for their personal data and preferences, but this information is derived from their behavior while interacting with the system. The ways they choose to accomplish a tasks, the combination of things they takes interest in, these observations allow inferences about a specific user. The application dynamically learns from observing these interactions. Different [[machine learning]] algorithms may be used to accomplish this task.
* '''A hybrid approach which asks for explicit feedback and alters the user model by adaptive learning'''&lt;ref name=Montaner&gt;{{Citation
  | last = Montaner | first = Miguel
  | last2 = L&#243;pez | first2 = Beatriz
  | last3 = De La Rosa | first3 = Josep Llu&#237;s
  | title = A Taxonomy of Recommender Agents on the Internet,
  | journal = Artif. Intell. Rev.
  | volume = 19
  | pages = 285&#8211;330
  | year = 2003 }}&lt;/ref&gt;
:This approach is a mixture of the ones above. Users have to answer specific questions and give explicit feedback. Furthermore, their interactions with the system are observed and the derived information are used to automatically adjust the user models.

Though the first method is a good way to quickly collect main data it lacks the ability to automatically adapt to shifts in users' interests. It depends on the users' readiness to give information and it is unlikely that they are going to edit their answers once the registration process is finished. Therefore, there is a high likelihood that the user models are not up to date. However, this first method allows the users to have full control over the collected data about them. It is in their decision which information they are willing to provide. This possibility is missing in the second method. Adaptive changes in a system that learns users' preferences and needs only by interpreting their behavior might appear a bit opaque to the users, because they cannot fully understand and reconstruct why the system behaves the way it does.&lt;ref name=Montaner /&gt; Moreover, the system is forced to collect a certain amount of data before it is able to predict the users' needs with the required accuracy. Therefore, it takes a certain learning time before a user can benefit from adaptive changes. However, afterwards these automatically adjusted user models allow a quite accurate adaptivity of the system. The hybrid approach tries to combine the advantages of both methods. Through collecting data by directly asking its users it gathers a first stock of information which can be used for adaptive changes. By learning from the users' interactions it can adjust the user models and reach more accuracy. Yet, the designer of the system has to decide, which of these information should have which amount of influence and what to do with learned data that contradicts some of the information given by a user.

== System adaptation ==

Once a system has gathered information about a user it can evaluate that data by preset analytical algorithm and then start to adapt to the user's needs. These adaptations may concern every aspect of the system's behavior and depend on the system's purpose. Information and functions can be presented according to the user's interests, knowledge or goals by displaying only relevant features, hiding information the user does not need, making proposals what to do next and so on. One has to distinguish between [[Adaptation (computer science)#Adaptivity_and_adaptability|adaptive and adaptable systems]].&lt;ref name=Fischer /&gt; In an adaptable system the user can manually change the system's appearance, behavior or functionality by actively selecting the corresponding options. Afterwards the system will stick to these choices. In an [[adaptive system]] a dynamic adaption to the user is automatically performed by the system itself, based on the built user model. Thus, an adaptive system needs ways to interpret information about the user in order to make these adaptations. One way to accomplish this task is implementing rule-based filtering. In this case a set of IF... THEN... rules is established that covers the [[knowledge base]] of the system.&lt;ref name=JohnsonTaatgen /&gt; The IF-conditions can check for specific user-information and if they match the THEN-branch is performed which is responsible for the adaptive changes. Another approach is based on [[collaborative filtering]].&lt;ref name=JohnsonTaatgen /&gt;&lt;ref name=Montaner /&gt; In this case information about a user is compared to that of other users of the same systems. Thus, if characteristics of the current user match those of another, the system can make assumptions about the current user by presuming that he or she is likely to have similar characteristics in areas where the model of the current user is lacking data. Based on these assumption the system then can perform adaptive changes.

== Usages ==

* [[Adaptive hypermedia]]: In an adaptive hypermedia system the displayed content and the offered hyperlinks are chosen on basis of users' specific characteristics, taking their goals, interests, knowledge and abilities into account. Thus, an adaptive hypermedia system aims to reduce the "lost in hyperspace" syndrome by presenting only relevant information.
* [[Adaptive educational hypermedia]]: Being a subdivision of adaptive hypermedia the main focus of adaptive educational hypermedia lies on education, displaying content and hyperlinks corresponding to the user's knowledge on the field of study.
* [[Intelligent tutoring system]]: Unlike adaptive educational hypermedia systems intelligent tutoring systems are stand-alone systems. Their aim is to help students in a specific field of study. To do so, they build up a user model where they store information about abilities, knowledge and needs of the user. The system can now adapt to this user by presenting appropriate exercises and examples and offering hints and help where the user is most likely to need them. 
* [[Expert systems]]: Expert systems are computer systems that emulate the decision-making ability of a human expert in order to help the user solving a problem in a specific area. Step by step they ask questions to identify the current problem and to find a solution. User models can be used to adapt to the current user's knowledge, differentiating between experts and novices. The system can assume, that experienced users are able to understand and answer more complex questions than someone who is new to the topic. Therefore, it can adjust the used vocabulary and the type of question which are presented to the user, thus reducing the steps needed to find a solution.
* [[Recommender system]]: The basic idea of recommender systems is to present a selection of items to the user which best fit his or her needs. This selection can be based on items the user has bookmarked, rated, bought, recently viewed, etc. Recommender systems are often used in [[e-commerce]] but may also cover areas like social networks, websites, news, etc.
* [[Usability testing|User-Simulation]]: Since user modeling allows the system to hold an internal representation of a specific user, different types of users can be simulated by artificially modeling them. Common types are "experts" or "novices" on the scope of the system or the usage of the system. Based on these characteristics user tests can be simulated.

== Standards==
A certain number of representation formats and standards are available for representing the users in computer systems,&lt;ref&gt;Nabeth Thierry (2005), [http://www.fidis.net/resources/fidis-deliverables/identity-of-identity/#c1753: Models], FIDIS Deliverable, October 2005. &lt;/ref&gt; such as:
* [[IMS-LIP]] (IMS &amp;ndash; Learner Information Packaging, used in [[e-learning]]) 
* [[HR-XML Standards|HR-XML]] (used in [[human resource management]])
* [[JXDM]] (Justice with the Global Justice Extensible Markup)
* [[Europass]] (the Europass online CV)

== See also ==

* [[Personalization]]
* [[Cognitive model]]
* [[User profile]]
* [[Identity management]]

== References ==
&lt;references/&gt;

== External references ==
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] The Journal of Personalization Research 
* [http://www.cs.cmu.edu/~bej/cogtool/ CogTool Project at CMU]
* [http://www.iit.demokritos.gr/um2007/ UserModeling conference 2007]

[[Category:Knowledge representation]]</text>
      <sha1>7a6scjhwzvfjrjsit4m3bxz6fl8blww</sha1>
    </revision>
  </page>
  <page>
    <title>Sears Subject Headings</title>
    <ns>0</ns>
    <id>22257394</id>
    <redirect title="Minnie Earl Sears" />
    <revision>
      <id>281347285</id>
      <parentid>281346842</parentid>
      <timestamp>2009-04-02T19:24:51Z</timestamp>
      <contributor>
        <username>EmphasisMine</username>
        <id>13429</id>
      </contributor>
      <minor />
      <comment>categories that are less appropriate for the target page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="146" xml:space="preserve">#REDIRECT [[Minnie Earl Sears]] {{R with possibilities}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>bh14b3cww5grwex3813t6odjce0aion</sha1>
    </revision>
  </page>
  <page>
    <title>AGRIS</title>
    <ns>0</ns>
    <id>23241698</id>
    <revision>
      <id>742012625</id>
      <parentid>731646017</parentid>
      <timestamp>2016-10-01T03:36:21Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 2 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10009" xml:space="preserve">'''AGRIS''' (International System for Agricultural Science and Technology) is a global public domain database with more than 8 million structured bibliographical records on agricultural science and technology. The database is maintained by [[CIARD]], and its content is provided by more than 150 participating institutions from 65 countries. The AGRIS Search system,&lt;ref&gt;{{cite web|url=http://agris.fao.org |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2016-03-14}}&lt;/ref&gt; allows scientists, researchers and students to perform sophisticated searches using keywords from the [[AGROVOC]] thesaurus, specific journal titles or names of countries, institutions, and authors.

== Early AGRIS years ==
As [[information management]] flourished in the 1970s, the AGRIS [[metadata]] [[Text corpus|corpus]] was developed to allow its users to have free access to knowledge available in agricultural science and technology. AGRIS was developed to be an international cooperative system to serve both developed and [[developing countries]].

With the advent of the Internet, along with the promises offered by [[open access (publishing)|open access]] publishing, there was growing awareness that the management of agricultural science and technology information, would have various facets: [[Technical standard|standards]] and methodologies for [[interoperability]] and facilitation of knowledge exchange; tools to enable information management specialists to process data; information and knowledge exchange across countries. Common [[interoperability]] criteria were thus adopted in its implementation, and the AGRIS AP [[metadata]] was accordingly created in order to allow exchange and retrieval of Agricultural information Resources.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description |publisher=Fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt;

== AGRIS 2.0 ==
AGRIS covers the wide range of subjects related to agriculture science and technology, including forestry, animal husbandry, aquatic sciences and fisheries, human nutrition, and extension. Its content includes unique grey literature such as unpublished scientific and technical reports, theses, conference papers, government publications, and more. A growing number (around 20%) of bibliographical records have a corresponding full text document on the web which can easily be retrieved by Google.

On 5th December 2013 AGRIS 2.0 was released. AGRIS 2.0 is at the same time:

# A collaborative network of more than 150 institutions from 65 countries, maintained by FAO of the UN, promoting free access to agricultural information.
# A multilingual bibliographic database for agricultural science, fuelled by the AGRIS network, containing more than 8 million records largely enhanced with AGROVOC, FAO&#8217;s multilingual thesaurus covering all areas of interest to FAO, including food, nutrition, agriculture, fisheries, forestry, environment etc.
# A mash-up web application that links the bibliographic AGRIS knowledge to related resources on the web using the [[Linked Open Data]] methodology. An AGRIS mashup page (e.g. http://agris.fao.org/agris-search/search.do?recordID=QM2008000025 ) is a web page where an AGRIS resource is displayed together with relevant knowledge extracted from external data sources (as the World Bank, DBPedia, and Nature). The availability of external data sources is not under AGRIS control. Thus, if an external data source is temporary unreachable, it won&#8217;t be displayed in AGRIS mashup pages.

Access to the AGRIS Repository is provided through the AGRIS Search Engine.&lt;ref&gt;{{cite web|url=http://agris.fao.org/ |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt;  As such, it:
# enables retrieval of bibliographic records contained in the AGRIS Repository,
# allows users to perform either full-text or fielded, parametric and assisted queries.

AGRIS data was converted to RDF and the resulting linked dataset created some 200 million triples.
AGRIS is also registered in the Data Hub at http://thedatahub.org/dataset/agris

The AGRIS partners contributing to the AGRIS Database use several formats for exchanging data, including simple DC, from [[OAI-PMH]] systems.
The AGRIS AP format is anyway adopted directly by:
# Open Archive Initiative (OAI) partners: Scielo, Viikki Science Library
# BIBSYS, Norway, National Library of Portugal, Wageningen UR Library.
# ''National networks'': NARIMS&lt;ref&gt;[http://www.arc.sci.eg/ arc.sci.eg]&lt;/ref&gt; in [[Egypt]], [[PhilAgriNet]] in [[Philippines]], [[KAINet]] in [[Kenya]], NAC in [[Thailand]], GAINS in [[Ghana]].
# ''National institutional repositories'': Russia, Belarus, Uruguay, Spain, Iran.
# ''Information service providers'': [[Wolters Kluwer]], [[NISC]], CGIR, [[CGIAR]], [[Agriculture Network Information Center|AgNIC]], GFIS.
# ''Database systems/tools'': AgriOceanDspace,&lt;ref&gt;{{cite web|url=http://aims.fao.org/agriocean-dspace |title=AgriOcean DSpace &amp;#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt; NewGenlib, WebAGRIS, NERAKIN, AgriDrupal.&lt;ref&gt;{{cite web|url=http://aims.fao.org/tools/agridrupal |title=AgriDrupal &amp;#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt;

=== AGRIS under the CIARD umbrella ===
Falling under the umbrella of CIARD,&lt;ref&gt;{{cite web|url=http://www.ciard.net |title=What is CIARD? &amp;#124; Coherence in Information for Agricultural Research for Development |publisher=Ciard.net |date= |accessdate=2013-07-09}}&lt;/ref&gt; a joint initiative co-led by the CGIAR,&lt;ref&gt;{{cite web|last=Rijsberman |first=Frank |url=http://www.cgiar.org |title=CGIAR Home |publisher=Cgiar.org |date=2013-07-04 |accessdate=2013-07-09}}&lt;/ref&gt; GFAR&lt;ref&gt;{{cite web|url=http://www.egfar.org |title=EGFAR web Site |publisher=Egfar.org |date= |accessdate=2013-07-09}}&lt;/ref&gt; and [[FAO]], the new AGRIS aims to promote the sharing and management of agricultural science and technology information through the use of common [[Technical standard|standards]] and methodologies. These will incorporate [[Web 2.0]] features, in order to make the search experience as comprehensive, intuitive and far-reaching as possible for users of the new AGRIS.

Furthermore, the new AGRIS will also leverage the data and infrastructure of one of CIARD's projects: the CIARD RING. An acronym standing for Routemap to Information Nodes and Gateways (RING), the CIARD RING project is led by [[Global Forum on Agricultural Research|GFAR]] and it aims to:

* give an overview of the current offer of information services in ARD; as well as
* support those who want to implement new services.

A directory of ARD (Agricultural Research for Development) information services will allow the monitoring, describing and classifying of existing services, whilst benchmarking them against [[interoperability]] criteria, to ensure for maximum outreach and global availability.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AgMES]]
* [[Agricultural Ontology Service]]
* [[AGROVOC]]
* [[Information management]]
* [[IMARK]]
* [[Disciplinary repository]]

== References ==
{{Reflist|2}}

== Other publications ==
* [http://f1000research.com/articles/4-432/v2 Discovering, Indexing and Interlinking Information Resources (F1000research 2015)]
* [http://f1000research.com/articles/4-110/v1 AGRIS: providing access to agricultural research data exploiting open data on the web (F1000research 2015)]
* [http://iospress.metapress.com/content/l15562xk70234n79/fulltext.pdf Migrating bibliographic datasets to the Semantic Web: The AGRIS case (Semantic Web journal 2013)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
* [https://web.archive.org/web/20140908155657/http://eprints.rclis.org/21112/1/2013_EFITA%20Pushing_Pulling.pdf Pushing, Pulling, Harvesting, Linking - Rethinking Bibliographic Workflows for the Semantic Web (EFITA-2013)] 
* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&amp;SERIES=339 Agricultural Information and Knowledge Management Papers]
* [http://www.fao.org/docrep/008/ae909e/ae909e00.htm The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description]
* [http://www.fao.org/kce/consultations/coaim/coaim-2000/en/ Consultations on Agricultural Management (COAIM)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
* [http://www.fao.org/docrep/x7936e/x7936e00.htm First Consultation on Agricultural Information Management (2000 COAIM Report)]
* [http://www.fao.org/docrep/meeting/005/y7963e/Y7963e00.htm#Top Report on the Second Consultation on Agricultural Information Management (2002 COAIM Report)]
* [http://departments.agri.huji.ac.il/economics/gelb-agris-10.pdf#AGRIS 1968-1994: Insights and Lessons]

== External links ==
* [http://agris.fao.org '''AGRIS''']
* [http://agris.fao.org/agris-search/agrisMap.do AGRIS network map]
* [http://agrovoc.fao.org/axis/services/SKOSWS?wsdl AGROVOC Web services]
* [http://aims.fao.org/ Agricultural Information Management Standards (AIMS)]
* [http://www.fao.org Food and Agriculture Organization of the United Nations (FAO) Web site]
* [http://www.ciard.net Coherence in Information for Agricultural Research for Development (CIARD) Wed site]
* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&amp;status=10&amp;dateto=31/12/2006&amp;lang=en&amp;sites=1 RSS feed of news and events]

{{DEFAULTSORT:Agris}}
[[Category:Agricultural organizations]]
[[Category:Interoperability]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Food and Agriculture Organization]]
[[Category:Public domain databases]]
[[Category:Agricultural databases]]</text>
      <sha1>eo2zrivj83obp8krdrnpktaawg07kd5</sha1>
    </revision>
  </page>
  <page>
    <title>Basic Formal Ontology</title>
    <ns>0</ns>
    <id>18025074</id>
    <revision>
      <id>746536505</id>
      <parentid>724945879</parentid>
      <timestamp>2016-10-28T01:04:27Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v1.2.6)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4244" xml:space="preserve">The '''Basic Formal Ontology''' ([http://www.ifomis.org/bfo BFO]) is a formal ontological framework developed by [[Barry Smith (ontologist)|Barry Smith]] and his associates that consists in a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: '''continuant''' (or snapshot) ontologies, comprehending continuant entities such as three-dimensional enduring objects, and '''occurrent''' ontologies, comprehending processes conceived as extended through (or as spanning) time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. Each continuant ontology is an inventory of all entities existing at a time. Each occurrent ontology is an inventory (processory) of all the processes unfolding through a given interval of time. Both types of ontology serve as basis for a series of sub-ontologies, each of which can be conceived as a window on a certain portion of reality at a given level of granularity.

==Applications of BFO==

BFO has been adopted as a foundational ontology by many [http://www.ifomis.org/bfo/users projects], principally in the areas of biomedical ontology and security and defense (intelligence) ontology. An example application of BFO can be seen in the [[Ontology for Biomedical Investigations]] (OBI).

==References==

*Arp, R., Smith, B., and Spear, A. D. ''[http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology]'', Cambridge, MA: MIT Press, August 2015, xxiv + 220pp.
* Grenon, P. and Smith, B. (2004) [http://ontology.buffalo.edu/smith/articles/SNAP_SPAN.pdf &#8220;SNAP and SPAN: Towards Dynamic Spatial Ontology&#8221;], Spatial Cognition and Computation, 4:1, 69-103.
* Smith, B. and Grenon, P. (2004) [http://ontology.buffalo.edu/smith/articles/cornucopia.pdf &#8220;The Cornucopia of Formal-Ontological Relations&#8221;], Dialectica, 58:3, 279-296.
*http://www.ifomis.uni-saarland.de/bfo/
*https://github.com/bfo-ontology/BFO/wiki
*http://ncorwiki.buffalo.edu/index.php/Basic_Formal_Ontology_2.0
==See also==

* [[Formal Ontology]]
* [[Upper ontology]]

==External links==

* [http://www.ifomis.org/bfo Basic Formal Ontology at IFOMIS]
*Katherine Munn, Barry Smith (Eds.): [http://www.ontosverlag.com/index.php?page=shop.product_details&amp;flypage=flypage.tpl&amp;product_id=108&amp;category_id=13&amp;option=com_virtuemart&amp;Itemid=1&amp;lang=en ''Applied Ontology: An Introduction''], Ontos Verlag.
*Ludger Jansen: "[http://ontology.buffalo.edu/bfo/Tendencies.pdf Tendencies and other Realizables in Medical Information Sciences]"
*Fabian Neuhaus, Pierre Grenon, Barry Smith: "[http://ontology.buffalo.edu/bfo/SQU.pdf A Formal Theory of Substances, Qualities, and Universals]"
*Luc Schneider: "[http://www.ifomis.org/bfo/documents/schneider-fois2010.pdf Revisiting the Ontological Square]"
*Lars Vogt: "[http://www.biomedcentral.com/1471-2105/11/289 Spatio-structural granularity of biological material entities]"
*Barry Smith, Werner Ceusters, Bert Klagges, Jacob K&#246;hler, Anand Kumar, Jane Lomax, Chris Mungall, Fabian Neuhaus, Alan Rector and Cornelius Rosse: "[http://genomebiology.com/2005/6/5/R46 Relations in Biomedical Ontologies]", Genome Biology (2005), 6 (5), R46
*Thomas Bittner, Maureen Donnelly and Barry Smith: "[http://www.acsu.buffalo.edu/~bittner3/Publications_files/Bittner-NA-2006-28.pdf A Spatio-Temporal Ontology for Geographic Information Integration]", International Journal for Geographical Information Science, 23 (6), 2009, 765-798
* [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0012258 Realism in Biology]
* Smith, B. and Ceusters, W. (2010) &#8220;[http://iospress.metapress.com/content/1551884412214u67/fulltext.pdf Ontological Realism as a Methodology for Coordinated Evolution of Scientific Ontologies]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&#8221;, Applied Ontology, 5 (2010), 139&#8211;188.


[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Ontology]]
[[Category:Ontology (information science)]]</text>
      <sha1>fkdd8y7dnihwyzyy4qvp816xxuyy936</sha1>
    </revision>
  </page>
  <page>
    <title>Reason maintenance</title>
    <ns>0</ns>
    <id>849986</id>
    <revision>
      <id>732073166</id>
      <parentid>712416772</parentid>
      <timestamp>2016-07-29T12:22:03Z</timestamp>
      <contributor>
        <ip>2001:4898:8010:0:0:0:0:4FE</ip>
      </contributor>
      <comment>/* Other references */ Corrected author's name.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6769" xml:space="preserve">{{more footnotes|date=September 2009}}

'''Reason maintenance'''&lt;ref name="insNouts"&gt;Doyle, J.: The ins and outs of reason maintenance.&lt;/ref&gt;&lt;ref name="originalTR"&gt;Doyle, J.: Truth maintenance systems for problem solving. Tech. Rep. AI-TR-419,
Dep. of Electrical Engineering and Computer Science of MIT (1978)&lt;/ref&gt; is a [[knowledge representation]] approach to efficient handling of inferred information that is explicitly stored. Reason maintenance distinguishes between base facts, which can be [[Defeasible reasoning|defeated]], and derived facts. As such it differs from [[belief revision]] which, in its basic form, assumes that all facts are equally important. Reason maintenance was originally developed as a technique for implementing problem solvers.&lt;ref name="originalTR"/&gt; It encompasses a variety of techniques that share a common architecture:&lt;ref name="mcAllesterInterface"&gt;McAllester, D.A.: Truth maintenance. AAAI90 (1990)&lt;/ref&gt; two components - a reasoner and a reason maintenance system - communicate with each other via an interface. The reasoner uses the reason maintenance system to record its inferences and justifications of ("reasons" for) the inferences. The reasoner also informs the reason maintenance system which are the currently valid base facts (assumptions). The reason maintenance system uses the information to compute the truth value of the stored derived facts and to restore consistency if an inconsistency is derived.

A '''truth maintenance system''', or '''TMS''', is a [[knowledge representation]] method for representing both beliefs and their dependencies and an algorithm called the "truth maintenance algorithm" that manipulates and maintains the dependencies. The name ''truth maintenance'' is due to the ability of these systems to restore consistency.   

It is also termed as a belief revision system, a truth maintenance system maintains consistency between old believed knowledge and current believed knowledge in the knowledge base (KB) through revision. If the current believed statements contradict the knowledge in the KB, then the KB is updated with the new knowledge. It may happen that the same data will again come into existence, and the previous knowledge will be required in the KB. If the previous data is not present, it is required for new inference. But if the previous knowledge was in the KB, then no retracing of the same knowledge was needed. Hence the use of TMS to avoid such retracing; it keeps track of the contradictory data with the help of a dependency record. This record reflects the retractions and additions which makes the inference engine (IE) aware of its current belief set.

Each statement having at least one valid justification is made a part of the current belief set. When a contradiction is found, the statement(s) responsible for the contradiction are identified and an appropriate is retraced. This process is called dependency-directed backtracking.

The TMS algorithm maintains the records in the form of a dependency network. The nodes in the network are one of the entries in the KB (a premise, antecedent, or inference rule etc.) Each arc of the network represent the inference steps from which the node was derived.

A premise is a fundamental belief which is assumed to be always true. They do not need justifications. Considering premises are base from which justifications for all other nodes will be stated.

There are two types of justification for each node. They are:

# Support List [SL]
# Conditional Proof (CP)

Many kinds of truth maintenance systems exist.   Two major types are single-context and multi-context truth maintenance.   
In single context systems, consistency is maintained among all facts in memory (database) and relates to the notion of consistency found in [[classical logic]]. Multi-context systems support [[paraconsistency]] by allowing consistency to be  relevant to a subset of facts in memory (a context) according to the history of logical inference.  This is achieved by tagging each fact or deduction with its logical history. Multi-agent truth maintenance systems perform truth maintenance across multiple memories, often located on different machines. de Kleer's assumption-based truth maintenance system (ATMS, 1986) was utilized in systems based upon [[AI winter#The fall of expert systems|KEE]] on the [[Lisp Machine]]. The first multi-agent TMS was created by Mason and Johnson. It was a multi-context system.  Bridgeland and Huhns created the first single-context multi-agent system.

==See also==
* [[Knowledge representation]]
* [[Artificial intelligence]]
* [[Belief revision]]
* [[Knowledge acquisition]]

==References==
&lt;references /&gt;

==Other references==
* Bridgeland, D. M. &amp; Huhns, M. N.,  Distributed Truth Maintenance. Proceedings of. AAAI&#8211;90: Eighth National Conference on Artificial Intelligence, 1990.
* J. de Kleer (1986). An assumption-based TMS. ''Artificial Intelligence'', 28:127&#8211;162.
* J. Doyle. A Truth Maintenance System. AI. Vol. 12. No 3, pp.&amp;nbsp;251&#8211;272. 1979.
* U. Junker and K. Konolige (1990). Computing the extensions of autoepistemic and default logics with a truth maintenance system. In ''Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI'90)'', pages 278&#8211;283. [[MIT Press]].
* Mason, C. and Johnson, R. DATMS: A Framework for Assumption Based Reasoning, in Distributed Artificial Intelligence, Vol. 2, [[Morgan Kaufmann Publishers]], Inc., 1989.
* D. A. McAllester. A three valued maintenance system. [[Massachusetts Institute of Technology]], Artificial Intelligence Laboratory. AI Memo 473. 1978.
* G. M. Provan (1988). A complexity analysis of assumption-based truth maintenance systems. In B. Smith and G. Kelleher, editors, ''Reason Maintenance Systems and their Applications'', pages 98&#8211;113. Ellis Horwood, New York.
* G. M. Provan (1990). The computational complexity of multiple-context truth maintenance systems. In ''Proceedings of the Ninth European Conference on Artificial Intelligence (ECAI'90)'', pages 522&#8211;527.
* R. Reiter and J. de Kleer (1987). Foundations of assumption-based truth maintenance systems: Preliminary report. In ''Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI'87)'', pages 183&#8211;188. [http://www2.parc.com/spl/members/dekleer/Publications/Foundations%20of%20Assumption-Based%20Truth%20Maintenance%20Systems.pdf  PDF]

==External links==
* [http://scholar.google.com/scholar?q=Truth+maintenance+system&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en&amp;btnG=Search Google Scholar on TMSs]
* [http://plato.stanford.edu/entries/logic-ai/#3.2.1 Belief Revision and TMSs] at [[Stanford Encyclopedia of Philosophy]]

[[Category:Belief revision]]
[[Category:Knowledge representation]]
[[Category:Information systems]]</text>
      <sha1>ofbk0oqkmzseyotmtmkkzzamjw2kz8d</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise interoperability</title>
    <ns>0</ns>
    <id>24007403</id>
    <revision>
      <id>692606278</id>
      <parentid>692605929</parentid>
      <timestamp>2015-11-26T23:30:08Z</timestamp>
      <contributor>
        <ip>108.221.18.208</ip>
      </contributor>
      <comment>Removed a little gibberish (translation errors?)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7361" xml:space="preserve">'''Enterprise interoperability''' is the ability of an enterprise&#8212;a company or other large organization&#8212;to functionally link activities, such as [[product design]], [[supply chains]], manufacturing, in an efficient and competitive way. 

The research in interoperability of enterprise practised in is various domains itself ([[Enterprise Modelling]], [[Ontologies]], [[Information systems]], Architectures and Platforms) which it is a question of positioning.&lt;ref name="Doumei2008"/&gt;

== Enterprise interoperability topics ==
===Interoperability in enterprise architecture===
Enterprise architecture (EA) presents a high level design of enterprise capabilities that defines successful IT projects in coherence with enterprise principals and business related requirements. EA covers mainly (i) the business capabilities analysis and validation; (ii) the development of business, application, data and technical architectures and solutions, and finally (iii) the control of programme and project implementation and governance. The application of EA methodology feeds the enterprise repository reference frame with sets of building blocks used to compose the targeted system.

The interoperability can be considered either as a principal, requirement or constraint that impact the definition of patterns to compose building blocks in the definition of targeted architectural roadmap. In this scope, EA within the TOGAF perspective,&lt;ref name="TOGAF2011"/&gt; aims to reconcile interoperability requirements with potential solutions that make developed systems interoperable.
So as to maintain the interoperability challenge quite present in the next steps of system&#8217;s lifecycle, several models and Frameworks are developed under the topic enterprise interoperability.

===Enterprise interoperability frameworks===
To preserve interoperability, several [[Enterprise Interoperability Framework|enterprise interoperability frameworks]] can be identified in the literature:
* 2003: IDEAS:&lt;ref name="IDEAS"/&gt; Interoperability Developments for Enterprise Application and Software.
* 2004: EIF:&lt;ref name="EIF"/&gt; The European Interoperability Framework
* 2004: e-GIF:&lt;ref name="eGIF"/&gt; e-Government Interoperability Framework
* 2006: FEI:&lt;ref name="FEI"/&gt; The Framework for Enterprise Interoperability
* 2006: C4IF:&lt;ref name="Peristeras2006"/&gt; Connection, Communication, Consolidation, Collaboration Interoperability Framework
* 2007: AIF:&lt;ref name="ATHENA"/&gt; Athena Interoperability Framework
* 2007:&lt;ref name="EntSysArch2007"/&gt;  Enterprise Architecture Framework for Agile and Interoperable Virtual Enterprises

The majority of these frameworks considers enterprise at several aspects, viewpoints or abstraction levels: business, process, knowledge, application, technology, data, technic, etc. and proposes guidelines to support modeling and connection capabilities between these levels. The semantic challenge is considered as transversal to all these abstraction levels.
Setting up and applying guidelines and methodologies developed within these frameworks requires modeling efforts that identify and connect artifacts.

===Interoperability in software engineering===
The evolution of IT technologies aims to outsource IT capabilities to vendors to manage for use on demand. The evolution pathway starts form packaged solutions and goes through Infrastructure as a service (Iaas), Platform as a service (Paas), Software as a service (Saas) and recently the Cloud. Interoperability efforts are still mainly expected among these levels:
* strategy to business 
* business to processes
* processes to application

Dealing with business process definition, alignment, collaboration and interoperability, several international standards propose methodologies and guidelines in these perspectives:
* ISO 15704&#8212;Requirements for enterprise-reference architectures and methodologies
* CEN-ISO DIS 19439&#8212;Framework for Enterprise Modeling
* CEN-ISO WD 19440&#8212;Constructs for Enterprise Modeling
* ISO 18629&#8212;Process specification language
* ISO/IEC 15414&#8212;ODP Reference Model&#8212;Enterprise Language 
In addition, recent standards (BPMN, BPEL, etc.) and their implementation technologies propose relevant integration capabilities. Furthermore, model driven-engineering &lt;ref name="MDAguide"/&gt; provides capabilities that connect, transform and refine models to support interoperability.

===Metrics for interoperability maturity assessment===
The following approaches propose some metrics to assess the interoperability maturity,&lt;ref name="Ford2008"/&gt;&lt;ref name="GUEDRIA"/&gt; 
* LISI: Levels of Information Systems Interoperability
* OIM: Organizational Interoperability Model
* NMI: NC3TA reference Model for Interoperability
* LCIM: Levels of Conceptual Interoperability Model
* EIMM: Enterprise Interoperability Maturity Model
* Smart Grid Interoperability Maturity Model Rating System

For the several interoperability aspects identified previously, the listed maturity approaches define interoperability categories (or dimensions) and propose qualitative as well as qualitative cross cutting issues to assess them. While interoperability aspects are not covered by a single maturity approach, some propositions go deeply in the definition of metric dimensions at one interoperability aspect such as the business interoperability measurement proposed by Aneesh.&lt;ref name="Zutshi"/&gt;

== See also ==
* [[INTEROP-VLab]]

== References ==
{{reflist|
refs=
&lt;ref name=Doumei2008&gt;Chen, D., [[Guy Doumeingts|Doumeingts]], G., and [[Fran&#231;ois Vernadat|Vernadat, F.]] 2008. Architectures for enterprise integration and interoperability: Past, present and future. ''Comput. Ind.'' 59, 7 (Sep. 2008), 647&#8211;659. {{en icon}} [http://dx.doi.org/10.1016/j.compind.2007.12.016 : DOI]&lt;/ref&gt;
&lt;ref name=TOGAF2011&gt;TOGAF&#174; 9 Certified, 2nd edition. The Open Group, 2011.&lt;/ref&gt;
&lt;ref name=IDEAS&gt;&#8220;A Contribution to Enterprise Interoperability Maturity Assessment&#8221;&lt;/ref&gt;
&lt;ref name=EIF&gt;EIF 2.0 http://ec.europa.eu/idabc/servlets/Docb0db.pdf&lt;/ref&gt;
&lt;ref name=eGIF&gt;http://edina.ac.uk/projects/interoperability/e-gif-v6-0_.pdf&lt;/ref&gt;
&lt;ref name=FEI&gt;http://chen33.free.fr/M2/Elearning/CIGI2009.Chen.final.pdf&lt;/ref&gt;
&lt;ref name=Peristeras2006&gt;Peristeras, V., and K. Tarabanis (2006): The Connection, Communication, Consolidation, Collaboration Interoperability Framework (C4IF) for Information Systems Interoperability, International Journal of Interoperability in Business Information Systems (IBIS), Vol. 1, No. 1, pp. 61-72.&lt;/ref&gt;
&lt;ref name=ATHENA&gt;http://www.asd-ssg.org/html/ATHENA/Deliverables/Deliverables%20provided%20to%20EC%206th%206%20Months/070306_ATHENA_DA82_V10.pdf&lt;/ref&gt;
&lt;ref name=EntSysArch2007&gt;Handbook of Enterprise Systems Architecture in Practice, 2007&lt;/ref&gt;
&lt;ref name=MDAguide&gt;http://www.omg.org/cgi-bin/doc?omg/03-06-01.pdf&lt;/ref&gt;
&lt;ref name=Ford2008&gt;Ford T., et al. Measuring System Interoperability: An i-Score Improvement. Proceedings of the 6th Annual Conference on Systems Engineering Research. Los Angeles, CA, April 4&#8211;5, 2008&lt;/ref&gt;
&lt;ref name=GUEDRIA&gt;GUEDRIAhttp://ori-oai.u-bordeaux1.fr/pdf/2012/GUEDRIA_WIDED_2012.pdf&lt;/ref&gt;
&lt;ref name=Zutshi&gt;http://run.unl.pt/bitstream/10362/2646/1/Zutshi_2010.pdf&lt;/ref&gt;
}}

== External links ==
* [http://www.interop-vlab.eu INTEROP-VLab]

[[Category:Interoperability]]
[[Category:Enterprise modelling]]
[[Category:Knowledge representation]]</text>
      <sha1>8qu979rhik3qxnf2niug0tly9wzrssa</sha1>
    </revision>
  </page>
  <page>
    <title>Linguistic value</title>
    <ns>0</ns>
    <id>15299080</id>
    <revision>
      <id>690594418</id>
      <parentid>690557883</parentid>
      <timestamp>2015-11-14T11:53:38Z</timestamp>
      <contributor>
        <ip>108.183.102.223</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1619" xml:space="preserve">:''For the similar but unrelated term in linguistics see '' [[Linguistic variable]]

In [[artificial intelligence]], [[operations research]], and related fields, a '''Linguistic value''', (for some authors '''Linguistic variable''') is a [[natural language]] term which is derived using quantitative or qualitative reasoning such as with probability and statistics or [[fuzzy sets and systems]].&lt;ref&gt;''Fuzzy Logic for Business and Industry'' Earl Cox, Charles River Media, pp188,214,302,306,352  1995 ISBN 1-886801-01-0&lt;/ref&gt;&lt;ref&gt;''The Fuzzy Systems Handbook, Second Edition'' Earl Cox, Academic Press, 1999 ISBN 0-12-194455-7 Ch 6 Fuzzy Reasoning, &amp;sect; 1 The Role of Linguistic Variables&lt;/ref&gt;
&lt;ref&gt;''On the Modeling of Linguistic Information using Random Sets'' Hung T. Nguyen p. 242 in Readings in Fuzzy Sets for Intelligent Systems. Morgan Kaufmann 1993. Dubois, Prade, and Yager eds. &lt;/ref&gt;
&lt;ref&gt;''Fuzzy Sets And The Social Nature of Truth'' J. Goguen. CS UCLA p. 49-67 in Advances in Fuzzy Sets and Systems, North Holland, 1979. &amp;sect; 2.3 ''Linguistic Truth Values''. ISBN 0-444-85372-3&lt;/ref&gt;

==Example of Linguistic Value==

For example, if a shuttle heat shield is deemed of having a linguistic value of a "very low" percentage of damage in re-entry, based upon knowledge from experts in the field, that probability would be given a value of say, 5%.  From there on out, if it were to be used in an equation, the variable of percentage of damage will be at 5% if it deemed very low percentage.

==References==
{{Reflist}}

{{DEFAULTSORT:Linguistic Value}}
[[Category:Knowledge representation]]


{{AI-stub}}</text>
      <sha1>sc5pqvdi127rfd6amm5f6kv7fn0wbnw</sha1>
    </revision>
  </page>
  <page>
    <title>Completeness (knowledge bases)</title>
    <ns>0</ns>
    <id>25154746</id>
    <revision>
      <id>418198636</id>
      <parentid>327195102</parentid>
      <timestamp>2011-03-10T22:07:22Z</timestamp>
      <contributor>
        <ip>129.93.162.155</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="909" xml:space="preserve">A [[knowledge base]] KB is '''complete''' ''if'' there is no formular &#945; such that KB &#8877; &#945; and KB &#8877; &#172;&#945;.

Example of knowledge base with incomplete knowledge:

KB := { A &#8744; B }

Then we have KB &#8877; A and KB &#8877; &#172;A.

In some cases, you can make a [[Consistency (knowledge bases)|consistent knowledge base]] complete with the [[closed world assumption]] - that is, adding all not-entailed literals as negations to the knowledge base. In the above example though, this would not work because it would make the knowledge base inconsistent:

KB' = { A &#8744; B, &#172;A, &#172;B }

In the case you have KB := { P(a), Q(a), Q(b) }, you have KB &#8877; P(b) and KB &#8877; &#172;P(b), so with the closed world assumption you would get KB' = { P(a), &#172;P(b), Q(a), Q(b) } where you have KB' &#8872; &#172;P(b).

See also:

* [[Vivid knowledge]]

{{computable knowledge}}

[[Category:Knowledge representation]]
{{logic-stub}}
{{database-stub}}</text>
      <sha1>772grdrxxl9j3uze7d0fptigpp61zn1</sha1>
    </revision>
  </page>
  <page>
    <title>OntoCAPE</title>
    <ns>0</ns>
    <id>26200279</id>
    <revision>
      <id>713113942</id>
      <parentid>561738158</parentid>
      <timestamp>2016-04-02T00:57:09Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* top */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1302" xml:space="preserve">'''OntoCAPE''' is a large-scale [[ontology (computer science)|ontology]] for the [[Domain knowledge|domain]] of [[Computer-Aided Process Engineering]] (CAPE). It can be downloaded free of charge from the [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&amp;L=1 OntoCAPE Homepage].

OntoCAPE is partitioned into 62 sub-ontologies, which can be used individually or as an integrated suite. 
The sub-ontologies are organized across different [[abstraction layer]]s, which separate general knowledge from knowledge about particular domains and applications.

* The upper layers have the character of an [[Upper ontology (information science)|upper ontology]], covering general topics such  as mereotopology, systems theory, quantities and units.
* The lower layers conceptualize the domain of chemical process engineering, covering domain-specific topics such as materials, chemical reactions, or unit operations.

==Further reading==
* Marquardt et al. (2010). [http://www.springer.com/chemistry/book/978-3-642-04654-4 ''OntoCAPE: A Re-Usable Ontology for Chemical Process Engineering'']. Springer-Verlag, Berlin Heidelberg.

== External links ==
* [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&amp;L=1 OntoCAPE Homepage]

[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]</text>
      <sha1>82phacfuq6i7uiq8rog7py0efu7qlk0</sha1>
    </revision>
  </page>
  <page>
    <title>Polythematic Structured Subject Heading System</title>
    <ns>0</ns>
    <id>27847641</id>
    <revision>
      <id>724690730</id>
      <parentid>622097579</parentid>
      <timestamp>2016-06-10T21:14:28Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category Indexing to [[:Category:Index (publishing)]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8755" xml:space="preserve">[[Image:PSH logo kratke.gif|thumb|right|PSH logo]]
'''Polythematic Structured Subject Heading System''' (abbreviated as '''PSH''' from the [[Czech language|Czech]] ''Polytematick&#253; Strukturovan&#253; Hesl&#225;&#345;'') is a bilingual Czech&#8211;English [[controlled vocabulary]] of [[Index term|subject headings]] developed and maintained by the National Technical Library (the former State Technical Library) in [[Prague]]. It was designed for describing and searching information resources according to their subject. PSH contains more than 13,900 terms, which cover the main fields of human knowledge.

[[Image:Lod-datasets 2010-09-22 colored.png|thumb|The Linking Open Data cloud diagram]]
Thanks to its release in [[Simple Knowledge Organization System|SKOS]], PSH can be used not only for describing [[document]]s in a [[library]], but also for [[Web indexing|indexing web pages]]. Everyone can use PSH for free. PSH is a part of the Linking Open Data cloud diagram (LOD cloud diagaram). The image of the LOD cloud diagram shows datasets that have been published in [[Linked Data]] format, by contributors to the [http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data] community project and other individuals and organisations.

== History and development ==
The PSH preparation project started in 1993, supported by several grants from the Czech Ministry of Culture and Czech Ministry of Education, Youth and Sport. Since 1995, PSH has been used for indexing the State Technical Library&#8217;s documents. Starting 1997,&lt;ref&gt;KLOU&#268;KOV&#193;, Zdenka. Polytematick&#253; strukturovan&#253; hesl&#225;&#345; St&#225;tn&#237; technick&#233; knihovny. &#268;ten&#225;&#345;. 1997, vol. 49, no. 4, p. 128-129. ISSN 0011-2321.&lt;/ref&gt; PSH has been distributed to other libraries and companies, originally as a commercial, paid product; since 2009&lt;ref&gt;MYNARZ, Jind&#345;ich; KAMR&#193;DKOV&#193;, Kate&#345;ina; KO&#381;UCHOV&#193;, Krist&#253;na. [http://www.techlib.cz/files/download/id/649/psh-cc.pdf Polythematic Structured Subject Heading System &amp; Creative Commons]. In Semin&#225;&#345; ke zp&#345;&#237;stup&#328;ov&#225;n&#237; &#353;ed&#233; literatury [online]. 2008&#8211; [retrieved 2010-05-28]. Praha : St&#225;tn&#237; technick&#225; knihovna, 2008.&lt;/ref&gt; for free. In 2000, the State Technical Library received a grant from the Ministry of Culture to translate PSH into English. The next milestone in its development was its releasing in the [[Simple Knowledge Organization System|SKOS]] format, in 2009.&lt;ref name="mynarz"&gt;MYNARZ, Jind&#345;ich; KO&#381;UCHOV&#193;, Krist&#253;na; KAMR&#193;DKOV&#193;, Kate&#345;ina. [http://www.ikaros.cz/node/5591 Novinky z oblasti Polytematick&#233;ho strukturovan&#233;ho hesl&#225;&#345;e]. Ikaros [online]. 2009, vol. 13, no. 7 [retrieved 2010-05-28]. URN-NBN:cz-ik5591. ISSN 1212-5075.&lt;/ref&gt;

The vast majority of new subject headings is suggested and approved by the indexing experts from the National Technical Library. However, the users and public can also make suggestions, using an online form, which are then assessed by the experts. The main decisions about the development and the future of PSH are done by the Committee for Coordination of Polythematic Structured Subject Heading System. The Committee consists of specialists from the National Technical Library and cooperating institutions, and representatives from the libraries and companies which use PSH. The Committee meets once a year in the National Technical Library; in the meantime, the members communicate using an [[electronic mailing list]].&lt;ref name="mynarz" /&gt;

== Browsing PSH ==
[http://psh.ntkcz.cz/skos/ PSH Browser] was released in June 2009. It serves for browsing the PSH system and its distribution in SKOS format. This tool navigates users through PSH from general to specific terms. Users can also use the Search field. [http://pshmanager.techlib.cz/ PSH manager] tool was released in 2012. It serves as an indexing tool especially to catalogers. Catalogers can easy orient in its clear structure. All the terms in PSH manager contain link to the catalogue of NTK. There can be also viewed the record in MARC21 format.

== Autoindexing ==
In 2012 was released beta version of autoindexing application. It is accessible on [http://invenio.ntkcz.cz/indexer/ Autoindexing]. Users enter chosen text into indexing field and activate indexing. In few seconds the terms describing content are displayed.

== PSH structure ==
PSH is a [[tree structure]] with 44 thematic sections. Subject headings are included in a hierarchy of six (or seven) levels according to their [[Semantics|semantic]] content and specificity. There are hierarchical, associative ("see also") and [[equivalence relation|equivalence]] ("see") relations in PSH. Hierarchical relations are represented by broader and narrower terms (e.g. ''physical diagnostic methods'' is broader term to ''electrocardiography'', and on the other hand, ''electrocardiography'' is narrower term to ''physical diagnostic methods''). Equivalence relations link subject headings with their nonpreferred versions (e.g. ''electrocardiography'' and ''ECG''). Moreover, associative relations are used to link related subject headings from different parts of PSH, regardless their affiliation to a section, (e.g. ''electrocardiography'': see also ''cardiology''). Every subject heading belongs to just one section, which has its own two-character abbreviation, assigned to every subject heading of the section. This enables users to recognize affiliation of subject headings from lower levels to the thematic sections. The 44 thematic sections have following [[Tree (data structure)|root nodes]]: 
{{Col-begin}}
{{Col-break}}
* agriculture
* anthropology
* architecture and town planning
* art
* astronomy
* biology
* chemistry
* civil engineering
* communications
* computer technology
* consumer industry
* economic sciences
* electronics
* electrotechnics
* food industry
{{Col-break}}
* generalities
* geography
* geology
* geophysics
* health services
* history
* informatics
* information science
* law
* linguistics
* literature
* mathematics
* mechanical engineering
* metallurgy
* military affairs
{{Col-break}}
* mining engineering
* pedagogy
* philosophy
* physics
* politology
* power engineering
* psychology
* religion
* science and technology
* sociology
* sport
* theory of systems
* transport
* water management
{{Col-end}}

== PSH formats ==
The main format for storage, maintenance and sharing PSH is the [[MARC standards|MARC 21 Format for Authority Data]], which is implemented in [[integrated library system|library automated systems]]. PSH is also available in [[Simple Knowledge Organization System|SKOS]], using [[RDF/XML]] syntax, which is a version suitable for web distribution. Single headings can be accessed on the PSH website through [[Uniform Resource Identifier|URI]] links. Alternatively, the whole vocabulary can be downloaded in one file. It is possible to display tags from PSH ([[metadata]] snippets &#8211; [[Dublin Core]] and CommonTag), which can be embedded in an HTML document to provide its semantic description in a machine-readable way.

== New subject headings ==
New subject headings are primarily obtained through the log analysis in the National Technical Library's on-line catalogue of documents, which are the terms used by end-users when searching various documents. Google Analytics service is now used for gaining search queries used by users. Within the data analysis, users queries are divided into seven categories that contain the title of the document, person, subject, action, institution, geographical terms and others. Then the candidates for new preferred terms and non-preferred terms are identified in the subject category.

Users can suggest preferred or non-preferred terms through the [https://www.techlib.cz/en/82958-tech-subject-headings#tab_heading web form] or via e-mail psh(@)techlib.cz.

== PSH &amp; Creative Commons ==
PSH/SKOS has been available under the Creative Commons License CC BY 3.0 CZ (Attribution-ShareAlike 3.0 Czech Republic)since 2011. Users are free to copy, distribute, display and perform the work and make derivative works, but they must give the original author credit and if they alter, transform, or build upon this work, they have to distribute the resulting work only under a licence identical to this one. Users can download all data in one [https://www.techlib.cz/en/82958-tech-subject-headings#tab_documentation zip file], which is continuously updated.

== See also ==
*[[Thesaurus]]
*[[Library of Congress Subject Headings]]
*[[Information retrieval]]
*[[Semantic Web]]

== References ==
{{Reflist}}

== External links ==
* [https://www.techlib.cz/en/82958-tech-subject-headings/ PSH official web page]

[[Category:Index (publishing)]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>exph44ywhdttofe4pl7d2t3fsy29kzn</sha1>
    </revision>
  </page>
  <page>
    <title>E-services</title>
    <ns>0</ns>
    <id>202311</id>
    <revision>
      <id>762028843</id>
      <parentid>762028830</parentid>
      <timestamp>2017-01-26T06:10:01Z</timestamp>
      <contributor>
        <username>Alex Cohn</username>
        <id>60778</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/119.148.44.254|119.148.44.254]] ([[User talk:119.148.44.254|talk]]) to last version by MolBio7</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37034" xml:space="preserve">The concept of '''e-service''' (short for electronic service) represents one prominent application of utilizing the use of [[Information and communication technology|information and communication technologies]] (ICTs) in different areas. However, providing an exact definition of e-service is hard to come by as researchers have been using different definitions to describe e-service. Despite these different definitions, it can be argued that they all agree about the role of technology in facilitating the delivery of services which make them more of electronic services.

It seems compelling to adopt Rowley (2006)&lt;ref name=autogenerated1&gt;Rowley, J. (2006) An analysis of the e-service literature: towards a research agenda. Internet Research, 16 (3), 339-359&lt;/ref&gt; approach who defines e-services as: &#8220;&#8230;deeds, efforts or performances whose delivery is mediated by information technology. Such e-service includes the service element of e-tailing, customer support, and service delivery&#8221;. This definition reflect three main components- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.

==Definitions and origin of the term e-service==
Since its conceptual inception in the late 1980s in Europe{{Citation needed|date=October 2012}} and formal introduction in 1993 by the US Government,&lt;ref&gt;Alasem, A. (2009). An Overview of e-Government Metadata Standards and Initiatives based on Dublin Core. Electronic Journal of e-Government, 7(1), 1 &#8211; 10&lt;/ref&gt; the term &#8216;[[E-Government]]&#8217; has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization.&lt;ref&gt;Wimmer, M., Codagnone, C. and Janssen, M. (2008) &#8220;Future of e-Government Research: 13 research themes identified in the eGovRTD2020 project&#8217;. Proceeding of the 41st Hawaii International Conference on System Sciences, USA&lt;/ref&gt; E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.&lt;ref&gt;L&#1255;fstedt, U. (2005) &#8216;Assessment of current research and some proposals for future direction&#8217;, International Journal of Public IS&lt;/ref&gt;

E-service (or eservice) is a highly generic term, usually referring to &#8216;The provision of services via the Internet (the prefix 'e' standing for &#8216;electronic&#8217;, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.&#8217; (Irma Buntantan &amp; G. David Garson, 2004: 169-170; Muhammad Rais &amp; Nazariah, 2003: 59, 70-71).

'E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.' (Jeong, 2007).&lt;ref&gt;Jeong Chun Hai @Ibrahim. (2007). ''Fundamental of Development Administration.'' Selangor: Scholar Press. ISBN 978-967-5-04508-0&lt;/ref&gt;

==Importance of E-service ==
Lu (2001)&lt;ref&gt;Lu, J. (2001). Measuring cost/benefits of e-business applications and customer satisfaction&#8221;, Proceedings of the 2nd International Web Conference, 29&#8211;30 November, Perth, Australia, 139-47&lt;/ref&gt; identifies a number of benefits for e-services, some of these are:

* Accessing a greater customer base
* Broadening market reach
* Lowering of entry barrier to new markets and cost of acquiring new customers
* Alternative communication channel to customers
* Increasing services to customers
* Enhancing perceived company image
* Gaining competitive advantages
* Potential for increasing [[Customer knowledge]]

==Importance and advantages of E-shopping==
*E-shops are open 24 hours a day.
*There is no need to travel to the malls or wait at the checkout counters.
*There is usually a wide selection of goods and services.
*It is easy to compare prices and quality by using the E-shopping tool.
*Price reduction and discounts are electronically conveyed.

==E-service domain==
The term &#8216;e-service&#8217; has many applications and can be found in many disciplines. The two dominant application areas of e-services are

E-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).

E-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix &#8220;public&#8221;: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of &#8220;e-Service&#8221; in both domains of e-government and [[e-business]].

==Architecture==
Depending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to &#8211; Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer ([[Enterprise application integration|Enterprise Application Integration]]&#8211; EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).

==E-service quality==
Measuring [[service quality]] and service excellence are important in a competitive organizational environment. The [[SERVQUAL]]- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:

{| class="sortable wikitable"
|-
!'''SERVQUAL&lt;ref&gt;Jiang, J.J.; Klein, G. and Crampton, S.M. (2000). A note on SERVQUAL reliability and validity in information system service quality measurement. Decision Sciences. Atlanta: Summer 2000. Vol. 31, Iss. 3; p. 725&lt;/ref&gt;'''
!'''Kaynama &amp; Black (2000)'''&lt;ref&gt;Kaynama, S. A., and Black, C. I.  (2000). A Proposal to assess the Service Quality of Online Travel Agencies: An Exploratory Study. Journal of Professional Services Marketing (21:1), 63-88&lt;/ref&gt;
!'''Zeithaml (2002)'''&lt;ref&gt;Zeithaml, V. A. (2002). Service Excellence in Electronic Channels. Managing Service Quality (12:3), 2002, 135-138&lt;/ref&gt;
!'''Janda et al. (2002)'''&lt;ref&gt;Janda, S., Trocchia, P. J., and Gwinner, K. (2002). Consumer perceptions of Internet Retail Service Quality. International Journal of Service Industry Management (13:5),  412-431&lt;/ref&gt;
!'''Alawattegama &amp; Wattegama (2008)'''&lt;ref&gt;Alawattegama, L. and Wattegama, C. (2008). Benchmarking Asia Pacific National Telecom Regulatory Authority Websites. LIRNEasia&lt;/ref&gt;
|-
|| Reliability      ||Content      ||Access      ||Access      || Factual information
|-
||Responsiveness      ||Access      ||Ease of navigation      ||Security     || Business information
|-
||Assurance      ||Navigation      ||Efficiency     ||Sensation        || General information
|-
||Tangibles      ||Design      ||Flexibility      ||Information/content      || Consumer&#8208; related information
|-
||Empathy      ||Response      ||Reliability ||     ||
|-
||     ||Background      ||Personalization      ||     ||
|-
||     ||Personalization     ||Security/privacy      ||     ||
|-
||     ||     ||Responsiveness      ||     ||
|-
||     ||     ||Assurance/trust     ||     ||
|-
||     ||     ||Site aesthetics      ||     ||
|-
||     ||    ||Price knowledge      ||     ||
|}

The [[LIRNEasia]] study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of [[information society]] reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.

==E-service cost factor==
Some major cost factors are (Lu, 2001):&lt;ref&gt;Lu, J. (2001). Measuring cost/benefits of e-business applications and customer
satisfaction&#8221;, Proceedings of the 2nd International Web Conference, 29&#8211;30 November, Perth, Australia, 139-47&lt;/ref&gt;

* Expense of setting up applications
* Maintaining applications
* Internet connection
* Hardware/software
* Security concerns
* legal issues
* Training; and
* Rapid technology changes

==Practical examples of e-services in the Developing World==
Information technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.&lt;ref name=autogenerated4&gt;Ndou,V.(2004)E-Government for developing countries: Opportunities and Challenges, EJISDC 18, 1, 1-24&lt;/ref&gt;

Many government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).&lt;ref&gt;Graham, S. and Aurigi, A. (1997) Virtual Cities, Social Polarisation, and the Crisis in Urban Public Space, Journal of Urban Technology, 4, 1, 19-52&lt;/ref&gt;

But the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,
issues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an &#8220;enabler&#8221;, but on the other hand it should also be regarded as a challenge and a peril in itself. The organizations, public or private,which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc.&lt;ref name=autogenerated4 /&gt; What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions,needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).&lt;ref&gt;Allen, A.B., Juillet, L., Paquet, G. and Roy, J. (2001) E-Governance and Government Online in Canada: Partnerships, People and Prospects, Government Information Quarterly,18, 93-104.)&lt;/ref&gt;

Following are a few examples regarding e-services in some developing countries:

===E-services in Rwanda===
Only a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,
has become one of the continent&#8217;s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country
where legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is
puzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the
same region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.&lt;ref&gt;Mawangi, W.(2006) The social relations of e-government diffusion in developing countries: the case of Rwanda, Proceedings of the 2006 international conference on Digital government research, May 21&#8211;24, 2006, San Diego, California&lt;/ref&gt;

===E-services in South Africa===
In South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance.&lt;ref name=autogenerated3&gt;van Brakel, P.A.(2009) Proceedings of the 11th Annual Conference on World Wide Web Applications, Port Elizabeth, 2&#8211;4 September&lt;/ref&gt; The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans &amp; Yen, 2006:208).&lt;ref&gt;Evans, D. &amp; Yen, D. C. 2006. e-Government: evolving relationship of citizens and government, domestic, and international development. Government Information Quarterly, 23(2): 207-235.)&lt;/ref&gt; In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the [http://www.gov.za/  Batho Pele portal], [[Sars efiling|SARS e-filing]], the [http://www.enatis.com/  e-Natis system], electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors
which collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.&lt;ref name=autogenerated3 /&gt;

===E-services in Malaysia===
E-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor ([http://www.mscmalaysia.my/  MSC]) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad ([http://www.tnb.com.my  TNB]) and Telekom Malaysia Berhad ([http://www.tm.net.my  TM]) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one&#8217;s own convenience. Also, Electronic Labour Exchange ([http://www.elx.gov.my  ELX])is one stop-centre for labor market information, as supervised by the Ministry of Human Resource ([http://www.mohr.gov.my  MOHR]), to enable employers and job seekers to communicate on the same platform.

[http://www.esyariah.gov.my/  e-Syariah] is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.

==Challenges to e-services in the Developing World==
The future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth &amp; Sharma (2007)&lt;ref&gt;Sheth., J.N. , Sharma, A., (2007). E-Services: A framework for growth.  Journal of Value Chain Management, 1(1/2)&lt;/ref&gt; identify, are:

* Low penetration of ICT especially in the developing countries;
* [[Internet fraud|Fraud]] on the internet space which is estimated around 2.8billion USD
* [[Internet privacy|Privacy]] due the emergence of various types of spyware and security holes, and
* intrusive characteristics of the service (e.g. mobile phones based) as customers may not like to be contacted with the service providers at any time and at any place.

The first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations.  For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles.  Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)&lt;ref&gt;Heiner and lyer (2007) E-Service opportunities and Threats, Journal of value chain management, 1, 11.&lt;/ref&gt;

==Major e-service keywords==
A considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley&#8217;s study (2006)&lt;ref name=autogenerated1 /&gt; who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality &#8220;In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.&#8221;

Some of the major keywords of e-service as found in the e-government research are as follows:

===Acceptance===
User acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p.&amp;nbsp;1)&lt;ref&gt;Wu, Philip F. (2009). User Acceptance of Emergency Alert Technology: A Case Study. Proceedings of the 6th International ISCRAM Conference &#8211; Gothenburg, Sweden&lt;/ref&gt; as &#8220;the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support&#8221;. This definition can be brought into the context of e-service where acceptance can be defined as the users&#8217; willingness to use e-service or the willingness to decide when and how to use the e-service.

===Accessibility===
Users&#8217; ability to access to the e-service is important theme in the previous literature. For example, Huang (2003)&lt;ref&gt;Huang, C.J. (2003). Usability of E-Government Web Sites for People with Disabilities, In Proceedings of the 36th Hawaii International Conference on System Sciences (HICSS&#8217;03), IEEE Computer Society, 2003&lt;/ref&gt; finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006)&lt;ref&gt;Jaeger, P.T. Assessing Section 508 compliance on federal e-government Web sites: A multi-method, user-centered evaluation of accessibility for persons with disabilities. Government Information Quarterly 23 (2006) 169&#8211;190&lt;/ref&gt; who suggests the following to improve e-services&#8217; accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site &#8230;Focus on the benefits of an accessible Web site to all users.

===Administrative literacy===
According to Gr&#246;nlund et al. (2007),&lt;ref&gt;Gr&#246;nlund, &#197;., Hatakka, M. and Ask, A. (2007) &#8216; Inclusion in the E-Service Society &#8211; Investigating Administrative Literacy Requirements for Using E-Services&#8217;. 6th International Conference (EGOV 2007, Regensburg, Germany), 4656&lt;/ref&gt; for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.

===Benchmarking===
This theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007)&lt;ref&gt;Bannister F. (2007). The curse of the benchmark: an assessment of the validity and value of e-government comparisons, International Review of Administrative Sciences, 73 (2), 171-188&lt;/ref&gt; &#8220;&#8230; benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs&#8221;

===Digital divide===
[[Digital divide]] is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009),&lt;ref&gt;Helbig, N; Gil-Garc&#237;a, J ; Ferro, E (2009). Understanding the complexity of electronic government: Implications from the digital divide literature. Government Information Quarterly, 26(2009), 89&#8211;97&lt;/ref&gt; &#8220;we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites."

===E-readiness===
Most of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or [[e-readiness]]. According to by Shalini (2009),&lt;ref&gt;Shalini, R. (2009). Are Mauritians ready for e-Government services?. Government Information Quarterly 26 (2009) 536&#8211;539&lt;/ref&gt; &#8220;the results of the research project reveal that a high [http://www.eiu.com/site_info.asp?info_name=eiu_2007_e_readiness_rankings&amp;rf=0|e-readiness index] may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not&#8221;

``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country&#8217;s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative,for instance depending on a country in question's priorities and perspective.&lt;ref&gt;GeoSINC International (2002). E-Readiness Guide. Available at http://www.apdip.net/documents/evaluation/e-readiness/geosinc01042002.pdf&lt;/ref&gt;

===Efficiency===
As opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness &#8220;There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies&#8221;&lt;ref&gt;Codagnone, C.  Undheim T.A (2008).  Benchmarking eGovernment: tools, theory, and practice. European Journal of ePractice. N&#186; 4 &#8226; August 2008&lt;/ref&gt;

===Security===
Security is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services.  According to the GAO report&lt;ref&gt;GAO.(2002). E-Government: Proposal addresses Critical Challenges. U.S General Accounting Office, Govt of the USA&lt;/ref&gt; of 2002 &#8220;security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials&#8212;as well as the general public&#8212;reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.&#8221; By and Large,  Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential

===Stakeholders===
Axelsson et al. (2009)&lt;ref&gt;Axelsson, K, Melin, f, Lindgren, I, (2009) DEVELOPING PUBLIC E-SERVICES FOR SEVERAL STAKEHOLDERS &#8211; A MULTIFACETED VIEW OF THE NEEDS FOR AN E-SERVICE. 17th European Conference on Information Systems&lt;/ref&gt; argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the [[stakeholder theory]] in public settings.&lt;ref&gt;Scholl, H. J. (2001). Applying stakeholder theory to e-government: Benefits and Limits. Kluwer Academic Publishers, Massachusetts&lt;/ref&gt; The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.

===Usability===
Compared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive&lt;ref&gt;Kaylor, C.,  Deshazo, R. &amp; Eck, D. V. (2001) "Gauging e-government: A report on implementing services among American cities". Government Information Quarterly (GIQ), 18(4), 293 - 307&lt;/ref&gt;

``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.&lt;ref&gt;[http://openlearn.open.ac.uk/mod/resource/view.php?id=211245 Open Learning - OpenLearn - Open University]&lt;/ref&gt;&#180;&#180;

==Social, cultural and ethical implications of e-services==
The perceived effectiveness of e-service can be influenced by public&#8217;s view of the social and cultural implications of e-technologies and e-service.

Impacts on individuals&#8217; rights and privacy &#8211; as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees' computer usage patterns in order to assess individual or workgroup performance.&lt;ref&gt;Asgarkhani, M. (2002). Strategic Management of Information systems and Technology in an e-World&#8221;, Proceedings of the 21st IT Conference, Sri Lanka, pp103-111.&lt;/ref&gt; Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern&lt;ref name=autogenerated2&gt;Asgarkhani, M. (2002b) &#8220;e-Governance in Asia Pacific&#8221;, Proceedings of the International Conference on Governance in Asia, Hong Kong.&lt;/ref&gt; that access to a wide range of information can be dangerous within politically corrupt government agencies.

Impact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk &#8211; such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.

Potential Impacts on Society &#8211; despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.

Impact on Social Interaction &#8211; advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.

Information Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information &#8211; more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.&lt;ref name=autogenerated2 /&gt;

==E-service awards==
The benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards

===Best online e-service in Europe===
[http://www.epractice.eu/en/awards|The European eGovernment Awards program] started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the [http://ec.europa.eu/idabc/en/document/7842|4th European eGovernment Awards] were announced in the award ceremony that took place at the [http://www.egov2009.se/ 5th Ministerial eGovernment Conference] on 19 November 2009 (Sweden); the winners in their respective categories are:

* Category 1. eGovernment supporting the Single Market: EU-OPA, the European Order for Payment Application ({{flag|Austria}} and {{flag|Germany}})
* Category 2a. eGovernment empowering citizens: Genvej ({{flag|Denmark}})
* Category 2b. eGovernment empowering businesses: MEPA, the Public Administration eMarketplace ({{flag|Italy}})
* Category 3. eGovernment enabling administrative efficiency and effectiveness: Licensing of Hunters via the &#8220;Multibanco&#8221; ATM Network ({{flag|Portugal}})
* Public prize: SMS Information System ({{flag|Turkey}})

===Other awards===

[http://www.ita.gov.om/HMAward/ Sultan Qaboos Award for excellence in eGovernance] {{flag|Oman}}(Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.

[http://www.egovawards.bh/AboutEn.aspx?Id=1|Bahrain eGovernment Excellence Awards] {{flag|Bahrain}}(Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy,eEducation Citizen Awards: Best eContent, eCitizen.

[http://www.e-servicesphils.com/esp2010/ Philippines e-Service Awards] {{flag|Philippines}}(Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.

==Major journals focusing on e-services==
There are some journals particularly interested for &#8220;e-Service &#8220;. Some of these are:
* [http://www.igi-global.com/journal/international-journal-services-mobile-applications/1114  International Journal of E-services and Mobile Applications]
* [http://eservicejournal.org/ eService Journal]
* [http://www.palgrave-journals.com/ejis/index.html  European Journal of Information Systems]
* [http://www.misq.org/  MIS Quarterly]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/505553/description#description  Information &amp; Management]
* [http://www.wiley.com/bw/journal.asp?ref=1350-1917&amp;site=1  Information Systems Journal]
* [http://www.igi-global.com/Bookstore/TitleDetails.aspx?TitleId=1091  International Journal of Electronic Government]
* [http://www.ejeg.com/  Electronic Journal of e-Government]
* [http://www.gvsu.edu/business/ijec/  International Journal of Electronic Commerce]
* [http://www.emeraldinsight.com/Insight/viewContainer.do?containerType=Journal&amp;containerId=11229  Internet Research]
* [http://www.palgrave-journals.com/jit/index.html  Journal Information Technology]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/525447/description#description  Journal of Strategic Information Systems]
* [http://aisel.aisnet.org/jais/  Journal of the Association for Information Systems]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/620202/description#description  Government Information Quarterly]
* [http://www.wiley.com/bw/journal.asp?ref=0033-3352  Public Administration Review]

==Major conferences focusing on e-services==

Major conferences considering e-service as one of the themes are:
* [http://ec.europa.eu/information_society/newsroom/cf/itemdetail.cfm?item_id=5649&amp;utm_campaign=isp&amp;utm_medium=rss&amp;utm_source=newsroom&amp;utm_content=tpa-8  eServices in European Civil Registration conference]
* [http://www.iist.unu.edu/I3E/IFIP  Conference on e-Business, e-Services, and e-Society]
* [http://www.eafricaconference.org/  International ICST Conference on e-service]
* [http://www.e-servicesphils.com/esp2010/  E-service Global Sourcing Conference &amp; Exhibition]
* [http://www.hicss.hawaii.edu/hicss_43/minitracks/eg-sin.htm  Annual Hawaii International Conference on Systems Sciences]
* [http://www.egov-conference.org/egov-2011-preview  Electronic Government Conference (EGOV)]
* [http://www.dexa.org/  International Conference on Electronic Government and the Information Systems Perspective (EGOVIS)]
* [http://www.icegov.org/  International Conference on Theory and Practice of Electronic Governance ( ICEGOV)]

==See also==
* [[Electronic services delivery]]
* [[Customer knowledge]]

==References==
{{Reflist}}

==External links==
* [http://www.e-govwatch.org.nz/criteria/e-services_delivery.html  E-services delivery]
* [http://www.computerworld.com/s/article/9005371/Report_Card_The_Best_E_Government_Sites  The Best E-Government Sites]
* [http://egov.infodev.org/en/Section.78.html#citizen-or-business-centric-portals  The World Bank (InfoDev) e-Government toolkit]

[[Category:Digital divide]]
[[Category:E-commerce]]
[[Category:Information technology]]
[[Category:Knowledge representation]]
[[Category:Open government]]
[[Category:Technology in society]]</text>
      <sha1>1uvymloeaak019ayqdda6ucfg918dlk</sha1>
    </revision>
  </page>
  <page>
    <title>AgMES</title>
    <ns>0</ns>
    <id>5465589</id>
    <revision>
      <id>742711864</id>
      <parentid>727374110</parentid>
      <timestamp>2016-10-05T09:19:19Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7951" xml:space="preserve">{{Refimprove|date=January 2015}}
The '''AgMES''' (Agricultural Metadata Element set) initiative was developed by the [[Food and Agriculture Organization]] (FAO) of the [[United Nations]] and aims to encompass issues of semantic standards in the domain of agriculture with respect to description, resource discovery, interoperability and data exchange for different types of information resources.

There are numerous other metadata schemas for different types of information resources. The following list contains a list of a few examples:

* Document-like Information Objects (DLIOs): [[Dublin Core]], Agricultural Metadata Element Set (AgMES)
* Events: [[VCalendar]]
* Geographic and Regional Information: Geographic information&#8212;Metadata ISO/IEC 11179 Standards&lt;ref&gt;{{cite web|url=http://isotc.iso.org/livelink/livelink/fetch/2000/2489/Ittf_Home/PubliclyAvailableStandards.htm |title=Freely Available Standards |publisher=Isotc.iso.org |date= |accessdate=2013-07-10}}&lt;/ref&gt;
* Persons: [[FOAF (software)|Friend-of-a-friend]] (FOAF), [[vCard]]
* Plant Production and Protection: Darwin Core (1.0 and 2.0) (DwC)

AgMES as a namespace is designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]], AGLS&lt;ref&gt;http://www.naa.gov.au/recordkeeping/gov_online/agls/summary.html&lt;/ref&gt; etc. Thus to be used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc., it will have to be used in conjunction with the standard namespaces mentioned before.  The AgMES initiative strives to achieve improved interoperability between information resources in agricultural domain by enabling means for exchange of information.

Describing a DLIO with AgMES means exposing its major characteristics and contents in a standard way that can be reused easily in any information system. The more institutions and organizations in the agricultural domain that use AgMES to describe their DLIOs, the easier it will be to interchange data in between information systems like digital libraries and other repositories of agricultural information.

== Use of AgMES ==
Metadata on agricultural Document-like Information Objects (DLIOs) can be created and stored in various formats:
* embedded in a web site (in the manor as with the HTML meta tag)
* in a separate metadata database
* in an XML file
* in an RDF file

AgMES defines elements that can be used to describe a DLIO that can be used together with other metadata standards such as the Dublin Core, the Australian Government Locator Service. A complete list of all elements, refinements and schemes endorsed by AgMES is available from the AgMES website.&lt;ref&gt;{{cite web|url=http://aims.fao.org/standards/agmes/namespace-specification |title=AgMES 1.1 Namespace Specification &amp;#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt;

=== Creating application profiles ===
[[Application profile]]s are defined as schemas which consist of data elements drawn from one or more namespaces, combined by implementers, and optimized for a particular local application. Application profiles share the following four characteristics:
* They draw upon existing pool of metadata definition standards to extract suitable application- or requirement oriented elements.
* An application profile cannot create new elements.
* Application profiles specify the application specific details such as the schemes or controlled vocabularies. An application profile also contains information such as the format for the element value, cardinality or [[data type]].
* Lastly, an application profile can refine standardized definitions as long as it is "semantically narrower or more specific". This capability of application profiles caters to situations where a domain specific terminology is needed to replace a more general one.

=== Sample application profiles using AgMES ===
* The AGRIS Application Profile&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt; is a standard created specifically to enhance the description, exchange and subsequent retrieval of agricultural Document-like Information Objects (DLIOs). It is a format that allows sharing of information across dispersed bibliographic systems and is based on well-known and accepted metadata standards.
* The Event Application Profile&lt;ref&gt;{{cite web|url=http://www.fao.org/aims/ap_applied.jsp |title=Agricultural Information Management Standards (AIMS) &amp;#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt; is a standard created to allow members of the Agricultural community to 'know' about an upcoming event and guide them to the event Web site where they can find further information. The information communicated is thus minimum yet interoperable across domains and organizations.

== AgMES and the semantic web ==

One of the advantages of the AgMES metadata schema is the ability to link between the [[Data element|metadata element]] and [[Controlled vocabulary|controlled vocabularies]]. The use of controlled vocabulary provides a "known" set of options to the indexer (and the search programmer) as to how the field can be filled out. Often the values may come from a specific thesaurus (e.g. [[AGROVOC]]) or classification schemes (e.g. the AGRIS/CARIS classification scheme) etc.&lt;ref&gt;{{cite web|url=http://www.fao.org/aims/index_en.jsp?callingPage=ag_classifschemes.jsp |title=Agricultural Information Management Standards (AIMS) &amp;#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt;

Thanks to the possibility to use controlled vocabularies for metadata elements, the user is provided with the most precise information. In this context, work is also being carried out on exploiting the power of controlled vocabularies expressed as using URIs and machine-understandable semantics.  In this context, FAO is promoting the [[Agricultural Ontology Service]] (AOS) initiative with the objective of expressing more semantics within the traditional thesaurus AGROVOC and build a Concept Server&lt;ref&gt;{{cite web|url=http://www.fao.org/aims/cs.htm |title=Agricultural Information Management Standards (AIMS) &amp;#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}&lt;/ref&gt; as a repository from which it will be always possible to extract traditional KOS.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AGRIS]]
* [[AGROVOC]]

==References==
{{Reflist}}

== External links ==
* {{Official website|https://web.archive.org/web/20060519100855/http://www.fao.org:80/aims/agmes_intro.jsp }}
* [http://dublincore.org/ Dublin Core Metadata Initiative]
* [http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]
* [http://xml.coverpages.org/ni2003-05-12-a.html FAO's AgMES Project Releases a New Application Profile for Encoding Metadata.] (''Cover Pages'', May 2003)
* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&amp;SERIES=339 Agricultural Information and Knowledge Management Papers]
* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&amp;status=10&amp;dateto=31/12/2006&amp;lang=en&amp;sites=1 RSS feed of news and events]
* [https://web.archive.org/web/20060617132639/http://www.dgroups.org:80/groups/fao/agstandards/index.cfm?op=dsp_join Agstandards Discussion List]: This is a forum established for discussing metadata standards and the development of multilingual thesauri and ontologies.

{{DEFAULTSORT:Agmes}}
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Semantic Web]]
[[Category:Food and Agriculture Organization]]</text>
      <sha1>ro6m4aty4xfhnleky7v4668ff9hqb0t</sha1>
    </revision>
  </page>
  <page>
    <title>VoID</title>
    <ns>0</ns>
    <id>20451641</id>
    <revision>
      <id>662546561</id>
      <parentid>662545623</parentid>
      <timestamp>2015-05-16T05:33:53Z</timestamp>
      <contributor>
        <username>Voidxor</username>
        <id>329764</id>
      </contributor>
      <minor />
      <comment>Voidxor moved page [[VoiD]] to [[VoID]]: Capitalization per linked sources</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1026" xml:space="preserve">{{Other uses|Void (disambiguation)}}
{{Multiple issues|
{{Expert-subject|Internet|date=November 2008}}
{{Technical|date=November 2008}}
{{Unreferenced|date=December 2008}}
{{Orphan|date=February 2009}}
{{Underlinked|date=November 2013}}
}}

The '''Vocabulary of Interlinked Datasets''' ('''VoID''') is an [[Resource Description Framework|RDF]] vocabulary, and a set of instructions, that enables the discovery and usage of [[Linked Data|linked data]] sets. A linked dataset is a collection of data, published and maintained by a single provider, available as RDF on the Web, where at least some of the resources in the dataset are identified by dereferencable URIs.

==References==
{{Reflist}}

==External links==
* [http://www.w3.org/TR/void/ Describing Linked Datasets with the VoID Vocabulary, W3C TR]
* [http://semanticweb.org/wiki/VoID VoID at Semantic Web Wiki]

{{Semantic Web}}

{{DEFAULTSORT:Void}}
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Knowledge representation]]
[[Category:XML-based standards]]</text>
      <sha1>ba59t7lk5ix2uuzz6rekn6zlrcmicb5</sha1>
    </revision>
  </page>
  <page>
    <title>NeOn Toolkit</title>
    <ns>0</ns>
    <id>30724333</id>
    <revision>
      <id>639347773</id>
      <parentid>509051156</parentid>
      <timestamp>2014-12-23T15:33:38Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Free software programmed in Java to [[:Category:Free software programmed in Java (programming language)]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2014 December 11]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1450" xml:space="preserve">'''The NeOn Toolkit''' is an open source, multi-platform [[ontology editor]], which supports the development of ontologies in [[Web Ontology Language|OWL]]/[[Resource Description Framework|RDF]]. The editor is based on the [[Eclipse (software)|Eclipse platform]] and provides a set of plug-ins (currently 20 plug-ins are available for the latest version, v2.4) covering a number of ontology engineering activities, including Annotation and Documentation, Modularization and Customization, Reuse, Ontology Evolution, translation&lt;ref name="Espinoza2008"&gt;M. Espinoza, A. Gomez-Perez, and E. Mena. [http://sid.cps.unizar.es/PUBLICATIONS/POSTSCRIPTS/eswc08-localization.pdf Enriching an ontology with multilingual information]. In Proc. of 5th European Semantic Web Conference (ESWC'08), Tenerife, (Spain), June 2008.&lt;/ref&gt; and others. 

The NeOn Toolkit has been developed in the course of the EU-funded NeOn project and is currently maintained and distributed by the NeOn Technologies Foundation.

==References==
&lt;references/&gt;

== External links ==
* [http://www.neon-foundation.org/ NeOn Technologies Foundation]
* [http://neon-toolkit.org/ NeOn Toolkit Website]
* [http://www.neon-project.org/ NeOn Project Website]

[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Free software programmed in Java (programming language)]]</text>
      <sha1>pnjx3ufvvdpigwnje4f27vmffi7pg20</sha1>
    </revision>
  </page>
  <page>
    <title>Linear belief function</title>
    <ns>0</ns>
    <id>24134105</id>
    <revision>
      <id>758833049</id>
      <parentid>758832978</parentid>
      <timestamp>2017-01-07T21:00:15Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* Knowledge representation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23222" xml:space="preserve">{{Notability|date=January 2010}}

'''Linear belief function''' is an extension of the [[Dempster&#8211;Shafer theory]] of [[belief functions]] to the case when variables of interest are [[Continuous function|continuous]]. Examples of such variables include financial asset prices, portfolio performance, and other antecedent and consequent variables. The theory was originally proposed by [[Arthur P. Dempster]]&lt;ref&gt;A. P. Dempster, "Normal belief functions and the [[Kalman filter]]," in ''Data Analysis from Statistical Foundations'', A. K. M. E. Saleh, Ed.: Nova Science Publishers, 2001, pp. 65&#8211;84.&lt;/ref&gt; in the context of Kalman Filters and later was reelaborated, refined, and applied to knowledge representation in artificial intelligence and decision making in finance and accounting by Liping Liu.&lt;ref&gt;Liu, Liping, Catherine Shenoy, and Prakash P. Shenoy, &#8220;Knowledge Representation and Integration for Portfolio Evaluation Using Linear Belief Functions,&#8221; IEEE Transactions on Systems, Man, and Cybernetics, Series A, vol. 36 (4), 2006, pp. 774&#8211;785.&lt;/ref&gt;

== Concept ==

A linear belief function intends to represent our belief regarding the location of the true value as follows: We are certain that the truth is on a so-called certainty [[hyperplane]] but we do not know its exact location; along some dimensions of the certainty hyperplane, we believe the true value could be anywhere from &#8211;&#8734; to +&#8734; and the probability of being at a particular location is described by a [[normal distribution]]; along other dimensions, our knowledge is [[vacuous]], i.e., the true value is somewhere from &#8211;&#8734; to +&#8734; but the associated probability is unknown. A [[belief function]] in general is defined by a [[mass function]] over a class of [[focal elements]], which may have nonempty intersections. A linear belief function is a special type of [[belief function]] in the sense that its [[focal elements]] are exclusive, parallel sub-hyperplanes over the certainty hyperplane and its [[mass function]] is a [[normal distribution]] across the sub-hyperplanes.

Based on the above geometrical description, Shafer&lt;ref&gt;G. Shafer, "A note on Dempster's Gaussian belief functions," School of Business, University of Kansas, Lawrence, KS, Technical Report 1992.&lt;/ref&gt; and Liu&lt;ref&gt;L. Liu, "A theory of Gaussian belief functions," ''International Journal of Approximate Reasoning'', vol. 14, pp. 95&#8211;126, 1996&lt;/ref&gt;  propose two mathematical representations of a LBF: a wide-sense inner product and a linear functional in the variable space, and as their duals over a hyperplane in the sample space. Monney &lt;ref&gt;P. A. Monney, ''A Mathematical Theory of Arguments for Statistical Evidence''. New York, NY: Springer, 2003.&lt;/ref&gt; proposes still another structure called Gaussian hints. Although these representations are mathematically neat, they tend to be unsuitable for knowledge representation in expert systems.

== Knowledge representation ==

A linear belief function can represent both logical and probabilistic knowledge for three types of variables: deterministic such as an observable or controllable, random whose distribution is normal, and vacuous on which no knowledge bears. Logical knowledge is represented by linear equations, or geometrically, a certainty hyperplane. Probabilistic knowledge is represented by a normal distribution across all parallel focal elements.

In general, assume X is a vector of multiple normal variables with mean &#956; and covariance &#931;. Then, the multivariate normal distribution can be equivalently represented as a moment matrix:

: &lt;math&gt;
M(X) = \left( \begin{array}{*{20}c}
   \mu \\
   \Sigma
\end{array} \right).
&lt;/math&gt;

If the distribution is non-degenerate, i.e., &#931; has a full rank and its inverse exists, the moment matrix can be fully swept:

: &lt;math&gt; 
M(\vec X) = \left( \begin{array}{*{20}c}
   \mu \Sigma^{-1} \\
   -\Sigma^{-1}
\end{array} \right)
&lt;/math&gt;

Except for normalization constant, the above equation completely determines the normal density function for ''X''. Therefore,  &lt;math&gt;M(\vec X)&lt;/math&gt; represents the probability distribution of ''X'' in the potential form.

These two simple matrices allow us to represent three special cases of linear belief functions. First, for an ordinary normal probability distribution M(X) represents it. Second, suppose one makes a direct observation on X and obtains a value &#956;. In this case, since there is no uncertainty, both variance and covariance vanish, i.e., &#931; = 0. Thus, a direct observation can be represented as:

: &lt;math&gt;M(X) = \left( \begin{array}{*{20}c}
   \mu \\
   0
\end{array} \right)
&lt;/math&gt;

Third, suppose one is completely ignorant about X. This is a very thorny case in Bayesian statistics since the density function does not exist. By using the fully swept moment matrix, we represent the vacuous linear belief functions as a zero matrix in the swept form follows:

: &lt;math&gt;M(\vec X) = \left[ \begin{array}{*{20}c}
   0 \\
   0
\end{array} \right]
&lt;/math&gt;

One way to understand the representation is to imagine complete ignorance as the limiting case when the variance of X approaches to &#8734;, where one can show that &#931;&lt;sup&gt;&#8722;1&lt;/sup&gt; = 0 and hence &lt;math&gt;M(\vec X)&lt;/math&gt; vanishes. However, the above equation is not the same as an improper prior or normal distribution with infinite variance. In fact, it does not correspond to any unique probability distribution. For this reason, a better way is to understand the vacuous linear belief functions as the neutral element for combination (see later).

To represent the remaining three special cases, we need the concept of partial sweeping. Unlike a full sweeping, a partial sweeping is a transformation on a subset of variables. Suppose X and Y are two vectors of normal variables with the joint moment matrix:

: &lt;math&gt;M(X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   \mu_1 \\
   \Sigma _{11} \\
   \Sigma_{21}
\end{array} &amp; \begin{array}{*{20}c}
   \mu_2 \\
   \Sigma _{12} \\
   \Sigma_{22}
\end{array}
\end{array} \right]&lt;/math&gt;

Then M(X, Y) may be partially swept. For example, we can define the partial sweeping on X as follows:

: &lt;math&gt; M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   \mu _1 (\Sigma_{11})^{-1} \\
   -(\Sigma_{11})^{-1} \\
   \Sigma_{21} (\Sigma_{11})^{-1}
\end{array} &amp; \begin{array}{*{20}c}
   \mu_2  - \mu_1 (\Sigma_{11} )^{-1} \Sigma_{12} \\
   (\Sigma_{11} )^{-1} \Sigma_{12} \\
   \Sigma_{22} - \Sigma_{21} (\Sigma_{11})^{-1} \Sigma_{12}
\end{array}
\end{array} \right]
&lt;/math&gt;

If ''X'' is one-dimensional, a partial sweeping replaces the variance of ''X'' by its negative inverse and multiplies the inverse with other elements. If ''X'' is multidimensional, the operation involves the inverse of the covariance matrix of ''X'' and other multiplications. A swept matrix obtained from a partial sweeping on a subset of variables can be equivalently obtained by a sequence of partial sweepings on each individual variable in the subset and the order of the sequence does not matter. Similarly, a fully swept matrix is the result of partial sweepings on all variables.

We can make two observations. First, after the partial sweeping on&amp;nbsp;''X'', the mean vector and covariance matrix of ''X'' are respectively &lt;math&gt; \mu_1 (\Sigma _{11} )^{-1} &lt;/math&gt; and &lt;math&gt; -(\Sigma_{11} )^{-1} &lt;/math&gt;, which are the same as that of a full sweeping of the marginal moment matrix of&amp;nbsp;''X''. Thus, the elements corresponding to X in the above partial sweeping equation represent the marginal distribution of X in potential form. Second, according to statistics, &lt;math&gt; \mu_2  - \mu_1 (\Sigma_{11} )^{-1} \Sigma_{12} &lt;/math&gt;is the conditional mean of ''Y'' given ''X''&amp;nbsp;=&amp;nbsp;0; &lt;math&gt; \Sigma_{22} - \Sigma_{21} (\Sigma_{11} )^{-1} \Sigma_{12} &lt;/math&gt;  is the conditional covariance matrix of ''Y'' given ''X''nbsp;=&amp;nbsp;0; and  &lt;math&gt;(\Sigma_{11} )^{-1} \Sigma_{12} &lt;/math&gt; is the slope of the regression model of ''Y'' on&amp;nbsp;''X''. Therefore, the elements corresponding to Y indices and the intersection of ''X'' and ''Y'' in &lt;math&gt; M(\vec X,Y)&lt;/math&gt;represents the conditional distribution of ''Y'' given&amp;nbsp;''X''&amp;nbsp;=&amp;nbsp;0.

These semantics render the partial sweeping operation a useful method for manipulating multivariate normal distributions. They also form the basis of the moment matrix representations for the three remaining important cases of linear belief functions, including proper belief functions, linear equations, and linear regression models.

=== Proper linear belief functions ===
For variables ''X'' and ''Y'', assume there exists a piece of evidence justifying a normal distribution for variables ''Y'' while bearing no opinions for variables&amp;nbsp;''X''. Also, assume that ''X'' and ''Y'' are not perfectly linearly related, i.e., their correlation is less than&amp;nbsp;1. This case involves a mix of an ordinary normal distribution for Y and a vacuous belief function for&amp;nbsp;''X''. Thus, we represent it using a partially swept matrix  as follows:

: &lt;math&gt;M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   0
\end{array} &amp; \begin{array}{*{20}c}
   \mu_2 \\
   0  \\
   \Sigma_{22} \\
\end{array}
\end{array} \right]
&lt;/math&gt;

This is how we could understand the representation. Since we are ignorant on&amp;nbsp;''X'', we use its swept form and set  &lt;math&gt; \mu_1 (\Sigma_{11})^{-1} = 0&lt;/math&gt; and &lt;math&gt; - (\Sigma_{11})^{-1} = 0&lt;/math&gt;. Since the correlation between ''X'' and ''Y'' is less than&amp;nbsp;1, the regression coefficient of ''X'' on ''Y'' approaches to 0 when the variance of ''X'' approaches to&amp;nbsp;&#8734;. Therefore,  &lt;math&gt;(\Sigma_{11})^{-1} \Sigma_{12} = 0&lt;/math&gt;. Similarly, one can prove that &lt;math&gt;\mu_1 (\Sigma_{11})^{-1} \Sigma_{12}  = 0&lt;/math&gt; and  &lt;math&gt; \Sigma_{21} (\Sigma_{11})^{-1} \Sigma_{12} = 0&lt;/math&gt;.

=== Linear equations ===
Suppose X and Y are two row vectors, and Y = XA + b, where A and b are the coefficient matrices. We represent the equation using a partially swept matrix as follows:

: &lt;math&gt;M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   A^T
\end{array} &amp; \begin{array}{*{20}c}
   b  \\
   A  \\
   0
\end{array}
\end{array} \right]
&lt;/math&gt;

We can understand the representation based on the fact that a linear equation contains two pieces of knowledge: (1) complete ignorance about all variables; and (2) a degenerate conditional distribution of dependent variables given independent variables. Since X is an independent vector in the equation, we are completely ignorant about it. Thus, &lt;math&gt; \mu_1 (\Sigma _{11})^{-1} = 0&lt;/math&gt; and &lt;math&gt; -(\Sigma_{11})^{-1} = 0&lt;/math&gt;. Given ''X'' = 0, ''Y'' is completely determined to be b. Thus, the conditional mean of Y is b and the conditional variance is 0. Also, the regression coefficient matrix is A.

Note that the knowledge to be represented in linear equations is very close to that in a proper linear belief functions, except that the former assumes a perfect correlation between X and Y while the latter does not. This observation is interesting; it characterizes the difference between partial ignorance and linear equations in one parameter &#8212; correlation.

=== Linear regression models ===
A linear regression model is a more general and interesting case than previous ones. Suppose X and Y are two vectors and Y = XA + b + E, where A and b are the appropriate coefficient matrices and E is an independent white noise satisfying E ~ N(0, &#931;). We represent the model as the following partially swept matrix:

: &lt;math&gt;
M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   A^T
\end{array} &amp; \begin{array}{*{20}c}
   b  \\
   A \\
   \Sigma
\end{array}
\end{array} \right]
&lt;/math&gt;

This linear regression model may be considered as the combination of two pieces of knowledge (see later), one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, &#931;). Alternatively, one may consider it similar to a linear equation, except that, given X = 0, Y is not completely determined to be b. Instead, the conditional mean of Y is b while the conditional variance is &#931;. Note that, in this alternative interpretation, a linear regression model forms a basic building block for knowledge representation and is encoded as one moment matrix. Besides, the noise term E does not appear in the representation. Therefore, it makes the representation more efficient.

From representing the six special cases, we see a clear advantage of the moment matrix representation, i.e., it allows a unified representation for seemingly diverse types of knowledge, including linear equations, joint and conditional distributions, and ignorance. The unification is significant not only for knowledge representation in artificial intelligence but also for statistical analysis and engineering computation. For example, the representation treats the typical logical and probabilistic components in statistics &#8212; observations, distributions, improper priors (for Bayesian statistics), and linear equation models &#8212; not as separate concepts, but as manifestations of a single concept. It allows one to see the inner connections between these concepts or manifestations and to interplay them for computational purposes.

== Knowledge operations ==

There are two basic operations for making inferences in [[expert system]]s using linear belief functions: combination and marginalization. Combination corresponds to the integration of knowledge whereas marginalization corresponds to the coarsening of knowledge. Making an inference involves combining relevant knowledge into a full body of knowledge and then projecting the full body of knowledge to a partial domain, in which an inference question is to be answered.

=== Marginalization ===
Marginalization projects a linear belief function into one with fewer variables. Expressed as a moment matrix, it is simply the restriction of a nonswept moment matrix to a submatrix corresponding to the remaining variables. For example, for the joint distribution M(X, Y), its marginal to Y is:

: &lt;math&gt;
M^{\downarrow Y} (X,Y) = \left[ {\begin{array}{*{20}c}
   \mu_2  \\
   \Sigma_{22}
\end{array}} \right]
&lt;/math&gt;

When removing a variable, it is important that the variable has not been swept on in the corresponding moment matrix, i.e., it does not have an arrow sign above the variable. For example, projecting the matrix &lt;math&gt;M(\vec X,Y)&lt;/math&gt; to Y produces:

: &lt;math&gt; M^{ \downarrow Y} (\vec X,Y) = \left[ {\begin{array}{*{20}c}
   \mu _2  - \mu _1 (\Sigma _{11} )^{-1} \Sigma _{12} \\
   \Sigma_{22} - \Sigma_{21} (\Sigma _{11})^{-1} \Sigma_{12}
\end{array}} \right]
&lt;/math&gt;

which is not the same linear belief function of Y. However, it is easy to see that removing any or all variables in Y from the partially swept matrix will still produce the correct result &#8212; a matrix representing the same function for the remaining variables.

To remove a variable that has been already swept on, we have to reverse the sweeping using partial or full reverse sweepings. Assume  &lt;math&gt;M(\vec X)&lt;/math&gt; is a fully swept moment matrix,

: &lt;math&gt;
M(\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu }  \\
   {\bar \Sigma }  \\
\end{array}} \right)
&lt;/math&gt;

Then a full reverse sweeping of &lt;math&gt;M(\vec X)&lt;/math&gt; will recover the moment matrix M(X) as follows:

: &lt;math&gt;
M(X) = \left( {\begin{array}{*{20}c}
   { - \bar \mu \bar \Sigma ^{ - 1} }  \\
   { - \bar \Sigma ^{ - 1} }  \\
\end{array}} \right)
&lt;/math&gt;

If a moment matrix is in a partially swept form, say

: &lt;math&gt;
M(\vec X,Y) = \left[ {\begin{array}{*{20}c}
   {\begin{array}{*{20}c}
   {\bar \mu _1 }  \\
   {\bar \Sigma _{11} }  \\
   {\bar \Sigma _{21} }  \\
\end{array}} &amp; {\begin{array}{*{20}c}
   {\bar \mu _2 }  \\
   {\bar \Sigma _{12} }  \\
   {\bar \Sigma _{22} }  \\
\end{array}}  \\
\end{array}} \right]
&lt;/math&gt;

its partially reverse sweeping on X is defined as follows:

: &lt;math&gt; M(X,Y) = \left[ {\begin{array}{*{20}c}
   {\begin{array}{*{20}c}
   { - \bar \mu _1 (\bar \Sigma _{11} )^{ - 1} }  \\
   { - (\bar \Sigma _{11} )^{ - 1} }  \\
   { - \bar \Sigma _{21} (\bar \Sigma _{11} )^{ - 1} }  \\
\end{array}} &amp; {\begin{array}{*{20}c}
   {\bar \mu _2  - \bar \mu _1 (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
   { - (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
   {\bar \Sigma _{22}  - \bar \Sigma _{21} (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
\end{array}}  \\
\end{array}} \right]
&lt;/math&gt;

Reverse sweepings are similar to those of forward ones, except for a sign difference for some multiplications. However, forward and reverse sweepings are opposite operations. It can be easily shown that applying the fully reverse sweeping to &lt;math&gt;M(\vec X)&lt;/math&gt;   will recover the initial moment matrix  M(X). It can also be proved that applying a partial reverse sweeping on X to the matrix  &lt;math&gt; M(\vec X,Y)&lt;/math&gt; will recover the moment matrix M(X,Y). As a matter of fact, Liu&lt;ref&gt;L. Liu, "Local Computation of Gaussian Belief Functions," ''International Journal of Approximate Reasoning'', vol. 22, pp. 217&#8211;248, 1999&lt;/ref&gt; proves that a moment matrix will be recovered through a reverse sweeping after a forward sweeping on the same set of variables. It can be also recovered through a forward sweeping after a reverse sweeping. Intuitively, a partial forward sweeping factorizes a joint into a marginal and a conditional, whereas a partial reverse sweeping multiplies them into a joint.

=== Combination ===
According to [[Dempster&#8217;s rule]], the combination of belief functions may be expressed as the intersection of focal elements and the multiplication of probability density functions. [[Liping Liu]] applies the rule to linear belief functions in particular and obtains a formula of combination in terms of density functions. Later he proves a claim by [[Arthur P. Dempster]] and reexpresses the formula as the sum of two fully swept matrices. Mathematically, assume &lt;math&gt;M_1 (\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _1 }  \\
   {\bar \Sigma _1 }  \\
\end{array}} \right)
&lt;/math&gt;  and &lt;math&gt; M_2 (\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _2 }  \\
   {\bar \Sigma _2 }  \\
\end{array}} \right)
&lt;/math&gt;  are two LBFs for the same vector of variables X. Then their combination is a fully swept matrix:

: &lt;math&gt; M(\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _1  + \bar \mu _2 }  \\
   {\bar \Sigma _1  + \bar \Sigma _2 }  \\
\end{array}} \right)
&lt;/math&gt;

This above equation is often used for multiplying two normal distributions. Here we use it to define the combination of two linear belief functions, which include normal distributions as a special case. Also, note that a vacuous linear belief function (0 swept matrix) is the neutral element for combination. When applying the equation, we need to consider two special cases. First, if two matrices to be combined have different dimensions, then one or both matrices must be vacuously extended, i.e., assuming ignorance on the variables that are no present in each matrix. For example, if M&lt;sub&gt;1&lt;/sub&gt;(X,Y)  and M&lt;sub&gt;2&lt;/sub&gt;(X,Z)  are to be combined, we will first extend them into &lt;math&gt; M_1 (X,Y,\vec Z)&lt;/math&gt;  and &lt;math&gt; M_2 (X,\vec Y,Z)&lt;/math&gt;  respectively such that  &lt;math&gt; M_1 (X,Y,\vec Z)&lt;/math&gt; is ignorant about Z and  &lt;math&gt; M_2 (X,\vec Y,Z)&lt;/math&gt; is ignorant about Y. The vacuous extension was initially proposed by Kong &lt;ref&gt;A. Kong, "Multivariate belief functions and graphical models," in Department of Statistics. Cambridge, MA: Harvard University, 1986&lt;/ref&gt; for discrete belief functions. Second, if a variable has zero variance, it will not permit a sweeping operation. In this case, we can pretend the variance to be an extremely small number, say &#949;, and perform the desired sweeping and combination. We can then apply a reverse sweeping to the combined matrix on the same variable and let &#949; approaches 0. Since zero variance means complete certainty about a variable, this &#949;-procedure will vanish &#949; terms in the final result.

In general, to combine two linear belief functions, their moment matrices must be fully swept. However, one may combine a fully swept matrix with a partially swept one directly if the variables of the former matrix have been all swept on in the later. We can use the linear regression model &#8212; Y = XA + b + E &#8212; to illustrate the property. As we mentioned, the regression model may be considered as the combination of two pieces of knowledge: one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, &#931;). Let  &lt;math&gt;M_1 (\vec X,\vec {\rm E},Y) = \left[ {\begin{array}{*{20}c}
   0 &amp; 0 &amp; b  \\
   0 &amp; 0 &amp; A  \\
   0 &amp; 0 &amp; I  \\
   {A^T } &amp; I &amp; 0  \\
\end{array}} \right]
&lt;/math&gt; and &lt;math&gt; M_2 (\vec {\rm E}) = \left[ {\begin{array}{*{20}c}
   0  \\
   { - \Sigma ^{ - 1} }  \\
\end{array}} \right]
&lt;/math&gt;  be their moment matrices respectively. Then the two matrices can be combined directly without sweeping &lt;math&gt; M_1 (\vec X,\vec {\rm E},Y)
&lt;/math&gt;  on Y first. The result of the combination is a partially swept matrix as follows:

: &lt;math&gt; M(\vec X,\vec {\rm E},Y) = \left[ {\begin{array}{*{20}c}
   0 &amp; 0 &amp; b  \\
   0 &amp; 0 &amp; A  \\
   0 &amp; { - \Sigma ^{ - 1} } &amp; I  \\
   {A^T } &amp; I &amp; 0  \\
\end{array}} \right]
&lt;/math&gt;

If we apply a reverse sweeping on E and then remove E from the matrix, we will obtain the same representation of the regression model.

== Applications ==

We may use an audit problem to illustrate the three types of variables as follows. Suppose we want to audit the ending balance of accounts receivable (''E''). As we saw earlier, ''E'' is equal to the beginning balance (''B'') plus the sales (''S'') for the period minus the cash receipts (''C'') on the sales plus a residual (''R'') that represents insignificant sales returns and cash discounts. Thus, we can represent the logical relation as a linear equation:

: &lt;math&gt;E=B+S-C+R&lt;/math&gt;
	
Furthermore, if the auditor believes ''E'' and ''B'' are 100 thousand dollars on the average with a standard deviation 5 and the covariance 15, we can represent the belief as a multivariate normal distribution. If historical data indicate that the residual R is zero on the average with a standard deviation of 0.5 thousand dollars, we can summarize the historical data by normal distribution ''R''&amp;nbsp;~&amp;nbsp;N(0,&amp;nbsp;0.5&lt;sup&gt;2&lt;/sup&gt;).  If there is a direct observation on cash receipts, we can represent the evidence as an equation say, C = 50 (thousand dollars). If the auditor knows nothing about the beginning balance of accounts receivable, we can represent his or her ignorance by a vacuous LBF. Finally, if historical data suggests that, given cash receipts&amp;nbsp;''C'', the sales ''S'' is on the average 8''C''&amp;nbsp;+&amp;nbsp;4 and has a standard deviation 4 thousand dollars, we can represent the knowledge as a linear regression model ''S''&amp;nbsp;~&amp;nbsp;N(4&amp;nbsp;+&amp;nbsp;8''C'',&amp;nbsp;16).

==References==
&lt;references/&gt;

{{DEFAULTSORT:Linear Belief Function}}
[[Category:Knowledge representation]]</text>
      <sha1>sd11h71trh5fpbmkus9tmmldbc74dd9</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Lexical databases</title>
    <ns>14</ns>
    <id>31484297</id>
    <revision>
      <id>423805300</id>
      <timestamp>2011-04-13T04:47:41Z</timestamp>
      <contributor>
        <username>4th-otaku</username>
        <id>7579047</id>
      </contributor>
      <comment>created</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="141" xml:space="preserve">{{catmain|Lexical database}}

[[Category:Translation databases]]
[[Category:Computational linguistics]]
[[Category:Knowledge representation]]</text>
      <sha1>lozpxzugc6h3nqf9kvdonkx766oa85c</sha1>
    </revision>
  </page>
  <page>
    <title>Pinakes</title>
    <ns>0</ns>
    <id>19391789</id>
    <revision>
      <id>762495998</id>
      <parentid>744589821</parentid>
      <timestamp>2017-01-29T05:06:37Z</timestamp>
      <contributor>
        <username>Jg2904</username>
        <id>11940553</id>
      </contributor>
      <minor />
      <comment>/* Description */ Replaced "thusly" with "thus."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7418" xml:space="preserve">{{italic title}}
{{hatnote|"Pinakes" may be plural of [[pinax]], a votive tablet that served as a votive object deposited in a sanctuary or burial chamber.}}
[[Image:Ancientlibraryalex.jpg|thumb|Imaginary depiction of the [[Library of Alexandria]]]]
The '''''Pinakes''''' ({{lang-grc|&#928;&#943;&#957;&#945;&#954;&#949;&#962;}} "tables", plural of {{lang|grc|[[wikt:&#960;&#943;&#957;&#945;&#958;|&#960;&#943;&#957;&#945;&#958;]]}}) was a [[bibliography|bibliographic]] work composed by [[Callimachus]] (310/305&#8211;240 BCE) that is popularly considered to be the first [[library catalog]]; its contents were based upon the holdings of the [[Library of Alexandria]] during Callimachus' tenure there during the third century BCE.&lt;ref&gt;N. Krevans 2002: 173&lt;/ref&gt;

==History==

The Library of Alexandria had been founded by [[Ptolemy I Soter]] about 306 BCE. The first recorded librarian was [[Zenodotus]] of Ephesus. During Zenodotus' tenure, Callimachus, who was never the head librarian, compiled the ''Pinakes'', thus becoming the first bibliographer and the scholar who organized the library by authors and subjects about 245 BCE.&lt;ref&gt;Neil Hopkinson, ''A Hellenistic Anthology'' (CUP, 1988) 83.&lt;/ref&gt;&lt;ref name ="alexandria3"&gt;{{cite web|url= http://www.greekplanet.com.au/forum/lofiversion/index.php/t486.html|title=
Greek Inventions|accessdate= 2008-09-19}}&lt;/ref&gt; His work was 120 volumes long.&lt;ref&gt;Hopkinson&lt;/ref&gt;

[[Apollonius of Rhodes]] was the successor to Zenodotus. [[Eratosthenes]] of Cyrene succeeded Apollonius in 235 BCE and compiled his ''tetagmenos epi teis megaleis bibliothekeis'', the "scheme of the great bookshelves." In 195 BCE [[Aristophanes of Byzantium]] was the librarian and updated the ''Pinakes'',&lt;ref&gt;Pfeiffer, R. ''History of Classical Scholarship from the Beginnings to the End of the Hellenistic Age'' (OUP, 1968) 133.&lt;/ref&gt; although it is also possible that his work was not a supplement of Callimachus' ''Pinakes'' themselves, but an independent polemic against, or commentary upon, their contents.&lt;ref&gt;Slater, W.J. "Grammarians on Handwashing", ''Phoenix'' 43 (1989) 100&amp;ndash;11, at 102.&lt;/ref&gt;

==Description==

The collection at the Library of Alexandria contained nearly 500,000 [[papyrus]] scrolls, which were grouped together by subject matter and stored in bins.&lt;ref&gt;P.J. Parson, "Libraries", in the ''Oxford Classical Dictionary'', 3rd ed. (OUP, 1996) describes the evidence for the size of the library's holdings thus: "The first Ptolemies (see Ptolemy (1) ) collected ambitiously and systematically; the Alexandrian Library (see ALEXANDRIA (1) ) became legend, and *Callimachus (3)'s ''Pinakes'' made its content accessible. There were rivals at *Pella, *Antioch (1) (where *Euphorion (2) was librarian), and especially *Pergamum. Holdings were substantial: if the figures can be trusted, Pergamum held at least 200,000 rolls (Plut. ''Ant.'' 58. 9), the main library at Alexandria nearly 500,000 (*Tzetzes, ''Prolegomena de comoedia'' 11a. 2. 10&#8211;11 Koster)&amp;mdash;the equivalent, perhaps, of 100,000 modern books."&lt;/ref&gt; Each bin carried a label with painted tablets hung above the stored papyri. ''Pinakes'' was named after these tablets and are a set of index lists. The bins gave bibliographical information for every roll.&lt;ref&gt;Phillips, Heather A., [http://unllib.unl.edu/LPP/phillips.htm "The Great Library of Alexandria?". Library Philosophy and Practice, August 2010]&lt;/ref&gt; A typical entry started with a title and also provided the author's name, birthplace, father's name, any teachers trained under, and educational background. It contained a brief biography of the author and a list of the author's publications. The entry had the first line of the work, a summary of its contents, the name of the author, and information about the origin of the roll.&lt;ref name ="alexandria4"&gt;{{cite web|url= http://www.greece.org/hec01/www/arts-culture/alexandria/library/library11.htm|title= The Pinakes|accessdate= 2010-05-29}}&lt;/ref&gt;

Callimachus' system divided works into six [[genres]] and five sections of prose: rhetoric, law, epic, tragedy, comedy, lyric poetry, history, medicine, mathematics, natural science and miscellanies. Each category was alphabetized by author.

Callimachus composed two other works that were referred as ''pinakes'' and were probably somewhat similar in format to the ''Pinakes'' (of which they "may or may not be subsections"&lt;ref&gt;Nita Krevans, "Callimachus and the Pedestrian Muse," in M.A. harder et al., eds., ''Callimachus II'' (Hellenistica Groningana 7), 2002, p. [https://books.google.com/books?id=CL4A5I3K-KsC&amp;lpg=PA173&amp;dq=callimachus%20democritus%20catalog&amp;pg=PA173#v=onepage&amp;q&amp;f=false 173] n. 1.&lt;/ref&gt;), but were concerned with individual topics. These are listed by the ''[[Suda]]'' as: ''A Chronological Pinax and Description of [[Theatre director#The director in theatre history|Didaskaloi]] from the Beginning'' and ''Pinax of the Vocabulary and Treatises of [[Democritus]]''.&lt;ref&gt;[http://www.stoa.org/sol-entries/kappa/227 ''Suda'' On Line]&lt;/ref&gt;

==Later bibliographic ''pinakes''==
The term ''pinax'' was used for bibliographic catalogs beyond Callimachus. For example, [[Ptolemy-el-Garib]]'s catalog of [[Aristotle]]'s writings comes to us with the title ''Pinax (catalog) of Aristotle's writings''.&lt;ref&gt;[[Ingemar D&#252;ring]], ''Aristotle in the Ancient Biographical Tradition'' (G&#246;teborg 1957), p. 221.&lt;/ref&gt;

==Legacy==
The ''Pinakes'' proved indispensable to librarians for centuries. They became a model to use all over the [[Mediterranean]]. Their later influence can be traced to medieval times, even to the Arabic counterpart of the tenth century: [[Ibn al-Nadim]]'s ''Al-Fihrist'' ("Index"). Variations on this system were used in libraries until the late 1800s when [[Melvil Dewey]] developed the [[Dewey Decimal Classification]] in 1876, which is still in use today.&lt;ref name ="alexandria4"/&gt;

==Notes==
{{reflist|35em}}

==Bibliography==

===Texts and translations===
* The evidence concerning the Pinakes is collected by [[Rudolf Pfeiffer]] (ed.), ''Callimachus, vol. I: Fragmenta'', Oxford: Clarendon Press 1949, frr. 429-456 (with reference to the most important literature).
* Witty, F. J. "The Pinakes of Callimachus", ''Library Quarterly'' 28:1/4 (1958), 132&amp;ndash;36.
* Witty, F. J. "The Other Pinakes and Reference Works of Callimachus", ''Library Quarterly'' 43:3 (1973), 237&amp;ndash;44.

===Studies===
* [[Roger S. Bagnall|Bagnall, R. S.]] [http://archive.nyu.edu/bitstream/2451/28263/2/D172-Alexandria%20Library%20of%20Dreams.pdf "Alexandria: Library of Dreams"], ''Proceedings of the American Philosophical Society'' 46 (2002) 348&amp;ndash;62.
* Blum, R. ''Kallimachos. The Alexandrian Library and the Origins of Bibliography'', trans. H.H. Wellisch (U. Wisconsin, 1991). ISBN 978-0-299-13170-8.
* Krevans, N. [https://books.google.com/books?id=CL4A5I3K-KsC&amp;lpg=PA173&amp;dq=callimachus%20democritus%20catalog&amp;pg=PA173#v=onepage&amp;q&amp;f=false "Callimachus and the Pedestrian Muse"], in: A. Harder et al. (eds.) ''Callimachus II'', Hellenistic Groningana 6 (Groningen, 2002) 173&amp;ndash;84.
* West, M. L. "The Sayings of Democritus", ''Classical Review'' (1969) 142.

{{coord missing|Egypt}}

{{Callimachus}}

[[Category:Defunct libraries]]
[[Category:Libraries in Egypt]]
[[Category:3rd-century BC books]]
[[Category:History of museums]]
[[Category:Ptolemaic Alexandria]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Bibliographies]]</text>
      <sha1>0uobw8vptn7r0f8mkoffjd41a974uvc</sha1>
    </revision>
  </page>
  <page>
    <title>POSC Caesar</title>
    <ns>0</ns>
    <id>23872172</id>
    <revision>
      <id>723650021</id>
      <parentid>708161192</parentid>
      <timestamp>2016-06-04T09:07:29Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <comment>[[User:Green Cardamom/WaybackMedic|WaybackMedic]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16664" xml:space="preserve">{{advert|date=September 2011}}

{{Infobox Organization
|name         = POSC Caesar Association
|image        = POSC Caesar logo.gif
|size         = 200
|alt          = Logo for POSC Caesar Association.
|caption      = Logo for POSC Caesar Association.
|abbreviation = PCA
|formation    = 1997-11-14
|status       = Association
|purpose      = Promote the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters
|location     = B&#230;rum, Norway
|region_served = Worldwide
|membership   = 36
|language     = English
|leader_title = General Manager
|leader_name  = Nils Sandsmark
|main_organ   = Board of Directors
|affiliations = &lt;!-- if any --&gt;
|num_staff    =
|num_volunteers =
|budget       =
|website      = http://www.posccaesar.org
}}
'''POSC Caesar Association''' (PCA) is an international, open, [[Non-profit organization|not-for-profit]], member organization that promotes the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters.

PCA is the initiator of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities" and is committed to its maintenance and enhancement.

Nils Sandsmark has been the General Manager of POSC Caesar Association since 1999&lt;ref name="BioSandsmarkDaraTech"&gt;
{{cite web
|url=http://www.daratech.com/plant2007/bios/sandsmark_nils.html
|title=Biography Dr. Ing. Nils Sandsmark
|author=Daratech
|accessdate=2009-08-11
}}&lt;/ref&gt; and Thore Langeland, [[Norwegian Oil Industry Association]] ({{lang-no|Oljeindustriens Landsforening}}, OLF), is the Chairman of the Board.

== History ==

=== Caesar Offshore ===
The first predecessor of POSC Caesar Association, the '''Caesar Offshore program''', started in 1993.&lt;ref name="PCAHis"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/History
|title=History of POSC Caesar
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-05
}}&lt;/ref&gt;&lt;ref name=POSCStatus98&gt;{{cite web
  | title = POSC/CAESAR project - Information and Status - January 1998
  | publisher = Petrotechnical Open Software Corporation (POSC)
  | date = 1998-03-17
  | url = http://posc.org/caesar/caesar_info.html
  | accessdate = 2009-08-06}}&lt;/ref&gt;&lt;ref name=Pein&gt;{{cite book
  | last = Pein
  | first = Martin
  | title = On data models, their transformations and consistency preserving programming interfaces
  | publisher = Books on Demand GmbH
  | year = 2002
  | location = Norderstedt, Germany
  | pages = 228
  | url = http://www.amazon.com/transformations-consistency-preserving-programming-interfaces/dp/3831139288/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1249538783&amp;sr=1-1
  | isbn = 3-8311-3928-8}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
  | title = Data warehouse manages offshore project information
  | journal = Oil &amp; Gas Journal
  | volume = 96
  | issue = 18
  | pages = 94
  | url = http://www.ogj.com/index/current-issue/s-oil-gas-journal/s-volume-96/s-issue-18.html
  | publisher = Pennwell Corporation
  | location = Tulsa, OK
  | date = 1998-05-04
  | issn = 0030-1388
}}&lt;/ref&gt;&lt;ref name=Stanley&gt;{{cite web
  | last = Port
  | first = Stanley
  | title = Plant Information Management at Statoil Norway
  | date = 1998-04-13
  | url = http://www.hydrocarbononline.com/article.mvc/Plant-Information-Management-at-Statoil-Norwa-0001
  | accessdate = }}&lt;/ref&gt;
The original focus was on standardizing technical data definitions for capital intensive projects at the handover from the [[EPC (contract)|EPC]] contractor to the owner/operators of onshore and offshore oil and gas production facilities. The program was sponsored by [[The Research Council of Norway]], two [[EPC (contract)|EPC]] contractors ([[Aker Maritime]] and [[Kv&#230;rner]]), three owners/operators ([[Norsk Hydro]], [[Saga Petroleum]] and [[Statoil]]) and [[DNV]] as service provider and project owner.

=== POSC Caesar project ===
During the period 1994-96, Caesar Offshore Program was defined as a project of [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]), and changed its name to the '''POSC Caesar Project'''.&lt;ref name="PCAHis"/&gt;&lt;ref name=POSCStatus98/&gt;&lt;ref name="Pein"/&gt;&lt;ref name="Stanley"/&gt;

In 1995 the project was joined by [[BP]], [[Brown and Root]] and [[Elf Aquitaine]] and in 1997 by [[Intergraph]], [[IBM]], [[Oracle Corporation|Oracle]], [[Lloyd's]], [[Royal Dutch Shell|Shell]], [[ABB Group|ABB]] and [http://www.umoe.no/ UMOE Technologies].&lt;ref name=POSCStatus98/&gt;

During that time, POSC Caesar also became a member of European Process Industries STEP Technical Liaison Executive (EPISTLE) where it collaborates with PISTEP (UK), and USPI-NL (The Netherlands) on the development of [[ISO 10303]], also known as "Standard for the Exchange of Product model data (STEP)."

=== POSC Caesar Association ===
In 1997, POSC Caesar Association was founded as an independent, global, non-profit, member organization. POSC Caesar Association serves an international membership and collaborates with other international organizations. It has its main office in Norway.

Albeit the name of POSC Caesar Association still hints to its past as a project within the [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]&lt;ref name="POSCEnergistics"&gt;
{{cite web
|url=http://www.energistics.org/posc/NewsBot.asp?MODE=VIEW&amp;ID=606&amp;SnID=2
|title=POSC Rebrands as Energistics - Press release
|author=Energistics
|accessdate=2009-09-01
|date=2006-11-09
}}&lt;/ref&gt;), from 1997 onwards, the organization has been independent. Energistics and POSC Caesar Association do collaborate, and are formally member in each other's organization.&lt;ref name="EnergisticsMembers"&gt;
{{cite web
|url=http://www.energistics.org//assnfe/companydirectory.asp?MODE=FINDRESULTS&amp;SEARCH_TYPE=1&amp;COMPANY_TYPE=2,3,6&amp;SnID=1941691677
|title=Energistics Member Directory
|author=Energistics
|accessdate=2009-08-11
}}&lt;/ref&gt;&lt;ref name="PCAMembers"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Membership
|title=POSC Caesar Association Membership
|author=POSC Caesar Association
|accessdate=2009-08-11
}}&lt;/ref&gt;

== Membership ==
POSC Caesar Association has with its current 36 members&lt;ref name="PCAMem"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Membership
|title=POSC Caesar Association - Membership
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-13
}}&lt;/ref&gt; from around the world established an international footprint (with a strong membership in Norway) that includes a wide range from academia, solution providers to engineering contractors and owners/operators. The members are (subdivided by organization type):
* Associations: Energistics (USA) and [[Norwegian Oil Industry Association|The Norwegian Oil Industry Association (OLF, Norway)]],
* [[Universities]] and [[Research institute|Research Institutes]]: International Research Institute of Stavanger (IRIS, Norway), [[Norwegian University of Science and Technology|Norwegian University of Science and Technology (NTNU, Norway)]], [[KAIST|Korea Advanced Institute of Science and Technology (KAIST, Korea)]], [[SINTEF|SINTEF (Norway)]], [[University of Bergen|University of Bergen (Norway)]], [[University of Oslo|University of Oslo (Norway)]], [[University of Stavanger|University of Stavanger (Norway)]], [[University of Troms&#248;|University of Troms&#248; (Norway)]] and Western Norway Research Institute (Norway),
* [[Petroleum Industry|Oil and Gas Companies]]: [[BP|BP (UK)]], [[Petronas|Petronas (Malaysia)]] and [[Statoil|Statoil (Norway)]],
* Engineering contractors and consultants:  Akvaplan-niva (Norway), [[Aker Solutions|Aker Solutions (Norway)]], Asset Life Cycle Information Management (ALCIM, Malaysia), CAESAR systems (USA), [[Bechtel|Bechtel (USA)]], [[Det Norske Veritas|Det Norske Veritas (DNV, Norway)]], Information Logic (USA) and iXIT Engineering Technology (Germany), Phusion IM Ltd (UK).&lt;ref&gt;https://www.posccaesar.org/wiki/PCA/Membership&lt;/ref&gt;
* Solution providers: [[Aveva|Aveva (UK)]], [[Bentley Systems|Bentley Systems (USA)]], Jotne EPM Technology (Norway), Epsis (Norway), Eurostep (Sweden), [[IBM|International Business Machines Corporation (IBM, USA)]], Siemens - Comos Industry Solutions (before Innotec) (Germany), [[Intergraph|Intergraph (USA)]], Invenia (Norway), Keel Solution (Denmark), Noumenon (UK), NRX (Canada), Octaga (Norway) and Tektonisk (Norway).

In general, the organization holds three membership meetings a year;&lt;ref name="PCAAgenda"&gt;{{cite web|url=http://www.posccaesar.org/wiki/PCA/Agenda |title=POSC Caesar Association - Agenda |author=POSC Caesar Association (PCA) |accessdate=2009-08-13 }}{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}&lt;/ref&gt; one in January / February in North-America (typically USA), one in April / May in Europe (typically Norway) and one in October in Asia (typically Malaysia).

== Activities and services ==

=== Initiator and custodian of ISO 15926 ===

In consultation with the other EPISTLE members and the [[International Organization for Standardization|International Organization for Standardization (ISO)]], it was decided in 2003 (some say already in 1997{{Citation needed|date=August 2009}}) that for modeling-technical reasons it was better to discontinue the development of [[ISO 10303]]&lt;ref name="SC4Legacy"&gt;
{{cite web
|url=http://www.tc184-sc4.org/SC4_Open/SC4%20Legacy%20Products%20(2001-08)/STEP_(10303)/
|title=ISO - SC4- Legacy products - STEP (ISO 10303)
|author=ISO
|accessdate=2009-08-05
}}&lt;/ref&gt; and to initiate the development of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities."

Over the years, the scope of the standard has increased from the initial capital-intensive projects in the [[Upstream (oil industry)|upstream oil and gas industry]], to include also relevant terminology for [[Downstream (oil industry)|downstream oil and gas industry]] applications and to deal with real-time data related to the actual oil and gas production.

[[ISO 15926]] has also over the years evolved from a dictionary (a list of terms with definitions), over a [[Taxonomy (general)|taxonomy]] (added hierarchy) to an [[Ontology (information science)|ontology]] (a formal representation of a set of concepts within a domain and the relationships between those concepts). [[ISO 15926]] is therefore sometimes nicknamed the "Oil and Gas Ontology.",&lt;ref name="OLFIOOntology"&gt;
{{cite web
|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf
|title=Integrated Operations and the Oil &amp; Gas Ontology
|author=[[Norwegian Oil Industry Association]] (OLF)
|accessdate=2009-08-05
}}&lt;/ref&gt; for some considered to be an essential prerequisite together with [[Semantic Web]] technologies&lt;ref name="W3C"&gt;
{{cite web
|url=http://www.w3.org/2008/12/ogws-report
|title=W3C workshop on Semantic Web in Oil and Gas Industry - Report
|author=[[W3C]]
|date=2009-01-13
|accessdate=2009-08-05
}}&lt;/ref&gt;
to get to better interoperability, an optimal use of all available data across boundaries and an increase in efficiency. This is what some call the next generation of [[Integrated Operations]].&lt;ref name="OLFIOOntology"/&gt;

=== Reference data services ===
Placeholders:
* Flow scheme of WIP - RDS - ISO and role of SIGs
* RDS
* Standards in database pilot (ISO)

=== Special interest groups ===
Placeholders:
* Overview of SIGs
* Drilling and Completion
* Reservoir and Production
* Operations and Maintenance

== Projects ==
There are a number of projects (co-)organized by POSC Caesar Association working on the extension of the [[ISO 15926]] standard in different application areas.

=== Capital intensive projects application domain ===
The following projects are running at the moment (August 2009):

* The ADI Project of FIATECH, to build the tools (which will then be made available in the public domain)
* The IDS Project of POSC Caesar Association, to define product models required for data sheets
* A joint collaboration project between FIATECH POSC Caesar Association is the ADI-IDS project is the [[ISO 15926 WIP]]

=== Upstream oil and gas industry application domain ===
The following projects are currently running (August 2009):

* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].&lt;ref&gt;
{{cite web
|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html
|title=Norway takes a leading role in next generation Integrated Operations
|author=The [[Norwegian Oil Industry Association]] (OLF)
|accessdate=2009-08-05
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="Rigzone"&gt;
{{cite web
|url=http://www.rigzone.com/news/article.asp?a_id=65883
|title=Norway Takes Reign to Provide Next Generation Integrated Operations
|author=Rigzone E&amp;P News
|accessdate=2009-08-05
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="DEJ"&gt;
{{cite web
|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&amp;PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7
|title=Norway developing next generation Integrated Operations
|author=Digital Energy Journal
|accessdate=2009-08-05
|date=2008-08-27
}}&lt;/ref&gt;&lt;ref name="EPMag"&gt;
{{cite web
|url=http://www.epmag.com/Magazine/2008/12/item24047.php
|title=Offshore R&amp;D pushes the limits
|author=E&amp;P Magazine
|accessdate=2009-08-05
|date=2008-12-02
}}&lt;/ref&gt;
* The Environment Web project to include environmental reporting terms and definitions as used in EPIM's EnvironmentWeb in ISO 15926.

Finalised projects include:

* The Integrated Information Platform (IIP) project working on establishing a real-time information pipeline based on open standards. It worked among others on:
** Daily Drilling Report (DDR) to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008&lt;ref&gt;{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-08-05 }}&lt;/ref&gt; for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and Safety Authority Norway (PSA). NPD says that the quality of the reports has improved considerably since.
** Daily Production Report (DPR) to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and &#197;sgard ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]&#8217; [[PRODML]] standard.

== Conferences and events ==

=== Semantic Days ===
{{Empty section|date=January 2011}}

=== Sogndal academic network meeting ===
{{Empty section|date=January 2011}}

== Collaborations ==

POSC Caesar is collaborating with a number of standardization bodies,&lt;ref name="PCAColl"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Collaboration
|title=POSC Caesar - Collaboration
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-05
}}&lt;/ref&gt; including:
* Mimosa: collaboration on open information standards for Operations and Maintenance mainly for the [[Downstream (oil industry)|downstream oil and gas industry]],
* FIATECH: collaboration on open information standards for life cycle data of capital projects&lt;ref name="IDSADI"&gt;
{{cite web
|url=http://www.posccaesar.org/wiki/IdsAdiProject
|title=POSC Caesar / FIATECH IDS-ADI Projects
|author=FIATECH &amp; POSC Caesar Association
|accessdate=2009-08-05
}}&lt;/ref&gt;&lt;ref name="iRing"&gt;
{{cite press release
|url=http://fiatech.org/press-releases/364-iring-version100.html
|title=iRING Version 1.0.0 Available Now
|publisher=FIATECH &amp; POSC Caesar Association
|accessdate=2009-08-05
|date=2009-06-05
}}&lt;/ref&gt;
* Energistics: collaboration on information standards for the [[Upstream (oil industry)|upstream oil and gas industry]], including [[WITSML]] and [[PRODML]]
* OASIS: collaboration on e-business standards,
* [[ISO TC 184/SC 4|ISO TC184/SC4]]: the host of the ISO 15926 standard.

== See also ==
* [[ISO 15926]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://www.posccaesar.org/ POSC Caesar Association] website

[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Standards organizations]]</text>
      <sha1>eqmoynir86it45s5c9oxvq1xwvkuh7r</sha1>
    </revision>
  </page>
  <page>
    <title>Composite Capability/Preference Profiles</title>
    <ns>0</ns>
    <id>4476270</id>
    <revision>
      <id>544325318</id>
      <parentid>535501541</parentid>
      <timestamp>2013-03-15T09:25:47Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2990630]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1836" xml:space="preserve">'''Composite Capability/Preference Profiles''' ('''CC/PP''') is a specification for defining capabilities and preferences (also known as 'delivery context') of [[user agents]]. CC/PP is a [[vocabulary extension]] of the [[Resource Description Framework]] (RDF). Delivery context can be used to guide the process of tailoring content for a [[user agent]].

The CC/PP specification is maintained by the [[World Wide Web Consortium|W3C]]'s [[UWAWG|Ubiquitous Web Applications Working Group (UWAWG)]] Working Group.

== History ==
* Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0 became a W3C recommendation on 15 January 2004.
* A "Last-Call Working-Draft" of CC/PP 2.0 was issued in April 2007

== See also ==
* [[Resource Description Framework|Resource Description Framework (RDF)]]
* [[UAProf|User Agent Profile (UAProf)]]
* [[WURFL|Wireless Universal Resource File (WURFL)]]

== External links ==
* [http://www.w3.org/Mobile/CCPP/ W3C CC/PP Information Page]
* [http://www.w3.org/TR/CCPP-struct-vocab2/ Newest version of CC/PP: Structure and Vocabularies]
* [http://www.w3.org/TR/2004/REC-CCPP-struct-vocab-20040115/ Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0]
* [http://www.w3.org/2001/di/ W3C Device Independence working group]
* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite/ CC/PP: Structure and Vocabularies Test Suite]
* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite-20030827/implementation-report.html CC/PP: Structure and Vocabularies Implementation Report]
* [http://www.w3.org/Consortium/Offices/Presentations/CCPP/ CC/PP presentation]
* [http://java.sun.com/j2ee/ccpp/ Sun J2EE CC/PP Processing Tools]

{{DEFAULTSORT:Composite Capability Preference Profiles}}
[[Category:Knowledge representation]]
[[Category:World Wide Web Consortium standards]]</text>
      <sha1>4gedybyq2vokvq5wvhiel7cp1qfgqah</sha1>
    </revision>
  </page>
  <page>
    <title>Chow&#8211;Liu tree</title>
    <ns>0</ns>
    <id>12680566</id>
    <revision>
      <id>724727874</id>
      <parentid>689740496</parentid>
      <timestamp>2016-06-11T02:58:24Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs, [[WP:AWB/T|typo(s) fixed]]: so called &#8594; so-called using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8558" xml:space="preserve">[[File:Chow-liu.png|thumb|400 px|A first-order dependency tree representing the product on the left.]]

In probability theory and statistics '''Chow&#8211;Liu tree''' is an efficient method for constructing a second-[[Orders of approximation|order]] product approximation of a [[joint probability distribution]], first described in a paper by {{Harvtxt|Chow|Liu|1968}}.  The goals of such a decomposition, as with such [[Bayesian networks]] in general, may be either [[data compression]] or [[inference]].

==The Chow&#8211;Liu representation==
The Chow&#8211;Liu method describes a [[joint probability distribution]] &lt;math&gt;P(X_{1},X_{2},\ldots,X_{n})&lt;/math&gt; as a product of second-order conditional and marginal distributions.  For example, the six-dimensional distribution &lt;math&gt;P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})&lt;/math&gt; might be approximated as

:&lt;math&gt;
P^{\prime
}(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})=P(X_{6}|X_{5})P(X_{5}|X_{2})P(X_{4}|X_{2})P(X_{3}|X_{2})P(X_{2}|X_{1})P(X_{1})
&lt;/math&gt;

where each new term in the product introduces just one new variable, and the product can be represented as a first-order dependency tree, as shown in the figure.  The Chow&#8211;Liu algorithm (below) determines which conditional probabilities are to be used in the product approximation.   In general, unless there are no third-order or higher-order interactions, the Chow&#8211;Liu approximation is indeed an ''approximation'', and cannot capture the complete structure of the original distribution.  {{Harvtxt|Pearl|1988}} provides a modern analysis of the Chow&#8211;Liu tree as a [[Bayesian network]].

==The Chow&#8211;Liu algorithm==
Chow and Liu show how to select second-order terms for the product approximation so that, among all such second-order approximations (first-order dependency trees), the constructed approximation &lt;math&gt;P^{\prime}&lt;/math&gt; has the minimum [[Kullback&#8211;Leibler distance]] to the actual distribution &lt;math&gt;P&lt;/math&gt;, and is thus the ''closest'' approximation in the classical [[information theory|information-theoretic]] sense. The Kullback&#8211;Leibler distance between a second-order product approximation and the actual distribution is shown to be

:&lt;math&gt;
D(P\parallel P^{\prime })=-\sum I(X_{i};X_{j(i)})+\sum
H(X_{i})-H(X_{1},X_{2},\ldots ,X_{n})
&lt;/math&gt;

where &lt;math&gt;I(X_{i};X_{j(i)})&lt;/math&gt; is the [[mutual information]] between variable &lt;math&gt;X_{i}&lt;/math&gt; and its parent &lt;math&gt;X_{j(i)}&lt;/math&gt; and &lt;math&gt;H(X_{1},X_{2},\ldots ,X_{n})&lt;/math&gt; is the [[joint entropy]] of variable set &lt;math&gt;\{X_{1},X_{2},\ldots ,X_{n}\}&lt;/math&gt;.   Since the terms &lt;math&gt;\sum H(X_{i})&lt;/math&gt; and  &lt;math&gt;H(X_{1},X_{2},\ldots ,X_{n})&lt;/math&gt; are independent of the dependency ordering in the tree, only the sum of the pairwise [[mutual information]]s, &lt;math&gt;\sum I(X_{i};X_{j(i)})&lt;/math&gt;, determines the quality of the approximation. Thus, if every branch (edge) on the tree is given a weight corresponding to the mutual information between the variables at its vertices, then the tree which provides the optimal second-order approximation to the target distribution is just the ''maximum-weight tree''. The equation above also highlights the role of the dependencies in the approximation: When no dependencies exist, and the first term in the equation is absent, we have only an approximation based on first-order marginals, and the distance between the approximation and the true distribution is due to the redundancies that are not accounted for when the variables are treated as independent. As we specify second-order dependencies, we begin to capture some of that structure and reduce the distance between the two distributions.

Chow and Liu provide a simple algorithm for constructing the optimal tree; at each stage of the procedure the algorithm simply adds the maximum [[mutual information]] pair to the tree.  See the original paper, {{Harvtxt|Chow|Liu|1968}}, for full details. A more efficient tree construction algorithm for the common case of sparse data was outlined in {{Harvtxt|Meil&#259;|1999}}.

Chow and Wagner proved in a later paper {{Harvtxt|Chow|Wagner|1973}} that the learning of the Chow&#8211;Liu tree is consistent given samples (or observations) drawn i.i.d. from a tree-structured distribution. In other words, the probability of learning an incorrect tree decays to zero as the number of samples tends to infinity. The main idea in the proof is the continuity of the mutual information in the pairwise marginal distribution. Recently, the exponential rate of convergence of the error probability was provided.&lt;ref name="Tan"&gt;A Large-Deviation Analysis for the Maximum-Likelihood Learning of Tree Structures. V. Y. F. Tan, A. Anandkumar, L. Tong and A. Willsky. In the International symposium on information theory (ISIT), July 2009.&lt;/ref&gt;

==Variations on Chow&#8211;Liu trees==
The obvious problem which occurs when the actual distribution is not in fact a second-order dependency tree can still in some cases be addressed by fusing or aggregating together densely connected subsets of variables to obtain a "large-node" Chow&#8211;Liu tree {{Harv|Huang|King|2002}}, or by extending the idea of greedy maximum branch weight selection to non-tree (multiple parent) structures {{Harv|Williamson|2000}}. (Similar techniques of variable substitution and construction are common in the [[Bayes network]] literature, e.g., for dealing with loops.  See {{Harvtxt|Pearl|1988}}.)

Generalizations of the Chow&#8211;Liu tree are the so-called [[t-cherry junction trees]]. It is proved that the t-cherry junction trees provide a better or at least as good approximation for a  discrete multivariate probability distribution as the Chow&#8211;Liu tree gives.
For the third order t-cherry junction tree see {{Harv|Kov&#225;cs|Sz&#225;ntai|2010}}, for the ''k''th-order t-cherry junction tree see {{Harv|Sz&#225;ntai|Kov&#225;cs|2010}}. The second order t-cherry junction tree is in fact the Chow&#8211;Liu tree.

==See also==
*[[Bayesian network]]
*[[Knowledge representation]]

==Notes==
{{reflist}}

==References==
{{refbegin|2}}
*{{Citation
 | last=Chow | first=C. K. | last2=Liu | first2=C.N.
 | title=Approximating discrete probability distributions with dependence trees
 | journal=IEEE Transactions on Information Theory
 | volume=IT-14  | issue=3 | year=1968 | pages=462&#8211;467 | url= | doi=10.1109/tit.1968.1054142}}.
*{{Citation
 | last=Huang | first=Kaizhu  | last2=King | first2=Irwin
 | last3=Lyu | first3=Michael R.  | year= 2002
 | chapter=Constructing a large node Chow&#8211;Liu tree based on frequent itemsets
 |editor1=Wang, Lipo |editor2=Rajapakse, Jagath C. |editor3=Fukushima, Kunihiko |editor4=Lee, Soo-Young |editor5=Yao, Xin | title=Proceedings of the 9th International Conference on Neural Information Processing ({ICONIP}'02)
 | place=[[Singapore]] | url= | accessdate= | pages=498&#8211;502}}.
*{{Citation
 | last=Pearl | first=Judea
 | title=Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference
 | publisher=[[Morgan Kaufmann]] | place=[[San Mateo, CA]] | year=1988}}
*{{Citation
 | last=Williamson | first=Jon | year= 2000
 | chapter=Approximating discrete probability distributions with Bayesian networks
 | title=Proceedings of the International Conference on Artificial Intelligence in Science and Technology
 | place=[[Tasmania]] | accessdate= | pages=16&#8211;20}}.
*{{Citation
 | last=Meil&#259; | first=Marina | year= 1999
 | chapter=An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data
 | title=Proceedings of the Sixteenth International Conference on Machine Learning
 | publisher=Morgan Kaufmann | accessdate= | pages=249&#8211;257}}.
*{{Citation
 | last=Chow | first=C. K. | last2=Wagner | first2=T.
 | title=Consistency of an estimate of tree-dependent probability distribution
 | journal=IEEE Transactions on Information Theory
 | volume=IT-19  | issue=3 | year=1973 | pages=369&#8211;371 | doi=10.1109/tit.1973.1055013}}.
*{{Citation
 | last=Kov&#225;cs | first=E. | last2=Sz&#225;ntai | first2=T.
 | title=On the approximation of a discrete multivariate probability distribution using the new concept of t-cherry junction tree
 | journal=Lecture Notes in Economics and Mathematical Systems
 | volume=633, Part 1 | year=2010 | pages= 39&#8211;56 | doi=10.1007/978-3-642-03735-1_3}}.
*{{Citation
 | last=Sz&#225;ntai | first=T. | last2=Kov&#225;cs | first2=E.
 | title=Hypergraphs as a mean of discovering the dependence structure of a discrete multivariate probability distribution
 | journal=Annals of Operations Research | year=2010 | pages= }}.
{{refend}}

{{DEFAULTSORT:Chow-Liu tree}}
[[Category:Knowledge representation]]</text>
      <sha1>9yc8ocambiudu6d36omwvzf340xh86w</sha1>
    </revision>
  </page>
  <page>
    <title>Composite portrait</title>
    <ns>0</ns>
    <id>39089943</id>
    <revision>
      <id>654281071</id>
      <parentid>644274549</parentid>
      <timestamp>2015-03-31T03:26:40Z</timestamp>
      <contributor>
        <ip>69.86.191.173</ip>
      </contributor>
      <comment>deleted links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3240" xml:space="preserve">[[File:Composite portraiture Galton.jpg|thumb|Composite portraiture, [[Francis Galton]], 1883.]]
'''Composite portraiture''' (also known as [[composite photograph]]s) is a technique invented by Sir [[Francis Galton]] in the 1880s after a suggestion by [[Herbert Spencer]] for registering photographs of human faces on the two eyes to create an "average" photograph of all those in the photographed group.&lt;ref&gt;Benson, P., &amp; Perrett, D. (1991). [http://www.abebooks.com/9781854890368/Photovideo-1854890360/plp Computer averaging and manipulations of faces.] In P. Wombell (ed.), ''Photovideo: Photography in the age of the computer'' (pp. 32&#8211;38). London: Rivers Oram Press.&lt;/ref&gt;&lt;ref&gt;Galton, F. (1878). [http://www.galton.org/essays/1870-1879/galton-1879-jaigi-composite-portraits.pdf Composite portraits.] ''Journal of the Anthropological Institute of Great Britain and Ireland, 8'', 132&#8211;142.&lt;/ref&gt;

Spencer had suggested using onion paper and line drawings, but Galton devised a technique for multiple exposures on the same photographic plate.  He noticed that these composite portraits were more attractive than any individual member, and this has generated a large body of research on human [[attractiveness]] and [[averageness]] one hundred years later.  He also suggested in a [[Royal Society]] presentation in 1883 that the composites provided an interesting concrete representation of human [[ideal type]]s and [[concept]]s.  He discussed using the technique to investigate characteristics of common types of humanity, such as criminals.  In his mind, it was an extension of the statistical techniques of [[average]]s and [[correlation]].  In this sense, it represents one of the first implementations of [[convolution]] [[factor analysis]] and [[neural network]]s in the understanding of [[knowledge representation]] in the human mind. Galton also suggested that the technique could be used for creating natural types of common objects.

During the late 19th century, English psychometrician [[Sir Francis Galton]] attempted to define [[Physiognomy|physiognomic]] characteristics of health, disease, beauty, and criminality, via a method of composite photography. Galton's process involved  the photographic superimposition of two or more faces by multiple exposures. After averaging together photographs of violent criminals, he found that the composite appeared "more respectable" than any of the faces comprising it; this was likely due to the irregularities of the skin across the constituent images being averaged out in the final blend. With the advent of computer technology during the early 1990s, Galton's composite technique has been adopted and greatly improved using computer graphics software.&lt;ref&gt;Yamaguchi, M. K., Hirukawa, T., &amp; Kanazawa, S. (1995). [http://www.perceptionweb.com/abstract.cgi?id=p240563 Judgment of gender through facial parts.] ''Perception, 24'', 563&#8211;575.&lt;/ref&gt;

== References ==
&lt;references /&gt;

==External links==
* [http://www.medienkunstnetz.de/works/composite-fotografie/ Samples of Galton's composites]
* [http://www.compositeportraits.com/ A Visual History of Composite Portraiture in Photography. Edited by Jake Rowland]

[[Category:Portrait art]]
[[Category:Knowledge representation]]</text>
      <sha1>7snvmrfx8otdyp53pws8xsnqocj0kw2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Dewey Decimal Classification</title>
    <ns>14</ns>
    <id>39327232</id>
    <revision>
      <id>590555073</id>
      <parentid>553986460</parentid>
      <timestamp>2014-01-13T19:47:16Z</timestamp>
      <contributor>
        <ip>63.251.123.2</ip>
      </contributor>
      <comment>I don't see how this is an instance of [[:Category:Science studies]], although Science Studies certainly concerns itself with "Knowledge representation"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="119" xml:space="preserve">{{catmore}}

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:OCLC]]</text>
      <sha1>cqml8oaxohfyvwocpbvzjwe0jmc8iqk</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic reasoner</title>
    <ns>0</ns>
    <id>13536810</id>
    <revision>
      <id>753805031</id>
      <parentid>753804987</parentid>
      <timestamp>2016-12-09T08:09:02Z</timestamp>
      <contributor>
        <username>Shvahabi</username>
        <id>8307062</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6630" xml:space="preserve">{{Redirect|Reasoner}}
A '''semantic reasoner''', '''reasoning engine''', '''rules engine''', or simply a '''reasoner''', is a piece of software able to infer [[logical consequence]]s from a set of asserted facts or [[axioms]]. The notion of a semantic reasoner generalizes that of an [[inference engine]], by providing a richer set of mechanisms to work with. The [[inference rules]] are commonly specified by means of an [[ontology language]], and often a [[description logic]] language.  Many reasoners use [[first-order predicate logic]] to perform reasoning; [[inference]] commonly proceeds by [[forward chaining]] and [[backward chaining]]. There are also examples of probabilistic reasoners, including Pei Wang's [[non-axiomatic reasoning system]],&lt;ref name=Wang&gt;{{cite web|last1=Wang|first1=Pei|title=Grounded on Experience Semantics for intelligence, Tech report 96|url=http://www.cogsci.indiana.edu/pub/wang.semantics.ps|website=http://www.cogsci.indiana.edu/|publisher=CRCC|accessdate=13 April 2015}}&lt;/ref&gt; and [[probabilistic logic network]]s.&lt;ref name=Goertzel2008&gt;{{cite book|last1=Goertzel|first1=Ben|last2=Ikl&#233;|first2=Matthew|last3=Goertzel|first3=Izabela Freire|last4=Heljakka|first4=Ari|title=Probabilistic Logic Networks: A Comprehensive Framework for Uncertain Inference|date=2008|publisher=Springer Science &amp; Business Media|isbn=9780387768724|page=42}}&lt;/ref&gt;

==List of semantic reasoners==

Existing semantic reasoners and related software:

===Commercial software===
* Bossam (software), an RETE-based rule engine with native supports for reasoning over [[Web Ontology Language|OWL]] ontologies, SWRL rules, and RuleML rules.
* RacerPro

===Free to use (Closed Source)===
* [[Cyc]] inference engine, a forward and backward chaining inference engine with numerous specialized modules for high-order logic. ([http://research.cyc.com/] ResearchCyc) ([http://opencyc.org/] OpenCyc)
* [[KAON2]] is an infrastructure for managing [[OWL-DL]], [[Semantic Web Rule Language|SWRL]], and [[F-Logic]] ontologies.
* [[ Internet Business Logic (software)]]&#8212;A reasoner designed for end-user app authors. Automatically generates and runs complex networked SQL queries. Explains the results in English at the end-user level.

===Free software (open source)===
* [[Cwm (software)|Cwm]], a forward-chaining reasoner used for querying, checking, transforming and filtering information. Its core language is RDF, extended to include rules, and it uses RDF/XML or N3 serializations as required. ([http://www.w3.org/2000/10/swap/doc/cwm.html CWM],  W3C software license)
* [[Drools]], a forward-chaining inference-based rules engine which uses an enhanced implementation of the [[Rete algorithm]]. ([http://www.jboss.org/drools/ Drools], Apache license 2.0)
* [http://owl.cs.manchester.ac.uk/tools/fact/ FaCT++ Reasoner], a tableaux-based reasoner for expressive Description Logics (DL), covering OWL and OWL 2 but lacking support for key constraints and some datatypes. Written in C++. (LGPL)
* [[Flora-2]], an object-oriented, rule-based knowledge-representation and reasoning system. ([http://flora.sourceforge.net Flora-2], Apache 2.0)
* [https://gndf.io/ Gandalf], open-source decision rules engine on PHP (GPL).
* [[Prova]], a semantic-web rule engine which supports data integration via SPARQL queries and type systems (RDFS, OWL ontologies as type system). ([http://prova.ws Prova], GNU GPL v2, commercial option available)
* [https://github.com/stardog-union/pellet Pellet], OWL 2 DL reasoner (AGPL, commercial option available)
* [http://www.hermit-reasoner.com/ HermiT], OWL 2 DL reasoner (LGPL)
* [https://github.com/liveontologies/elk-reasoner ELK], OWL 2 EL reasoner (Apache 2)
* [https://lat.inf.tu-dresden.de/systems/cel CEL], OWL 2 EL reasoner (Apache 2)
* [https://github.com/julianmendez/jcel jcel], OWL 2 EL reasoner (LGPL / Apache 2)
* [https://github.com/ha-mo-we/Racer RACER], OWL 2 DL reasoner (BSD-3)
* [[Jena (framework)]], an open-source semantic-web framework for Java which includes a number of different semantic-reasoning modules. ([http://jena.apache.org/ Apache Jena], Apache License 2.0)
* [[RDFSharp]], an open source semantic web framework for .NET which includes a semantic extension implementing RDFS/OWL-DL/custom rule-based reasoning. ([http://rdfsharp.codeplex.com/ RDFSharp], Apache License 2.0)

=== Applications that contain reasoners ===
* [[Apache Marmotta]] includes a rule-based reasoner in its KiWi [[triple store]].
* [http://techinvestlab.ru/dot15926Editor dot15926 Editor]&#8212;Ontology management framework initially designed for engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] rule scripting and pattern-based data analysis. Supports extensions.

==See also==
{{portal|Software}}
* [[Business rules engine]]
* [[Expert systems]]
* [[Doxastic logic]]
* [[Method of analytic tableaux]]
*[[Logic Programming]]

==References==
{{reflist}}

==External links==
* [https://www.w3.org/2001/sw/wiki/OWL/Implementations OWL 2 Reasoners listed on W3C SW Working Group homepage]
* [http://www.w3.org/TR/rdf-sparql-query/ SPARQL Query Language for RDF]
* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics DL course] by Enrico Franconi, Faculty of Computer Science, [[Free University of Bolzano]], Italy
* [http://trimc-nlp.blogspot.com/2013/04/owl-properties.html ''Inference using OWL 2.0 Semantics''] by Craig Trim (IBM).
* Marko Luther, Thorsten Liebig, Sebastian B&#246;hm, Olaf Noppens: [http://dx.doi.org/10.1007/978-3-642-02121-3_9 Who the Heck Is the Father of Bob?]. ESWC 2009: 66-80
* Jurgen Bock, Peter Haase, Qiu Ji, Raphael Volz. [http://www.aifb.uni-karlsruhe.de/WBS/pha/publications/owlbenchmark_07_2007.pdf Benchmarking OWL Reasoners]. In ARea2008 - Workshop on Advancing Reasoning on the Web: Scalability and Commonsense (June 2008)
* Tom Gardiner, Ian Horrocks, Dmitry Tsarkov. [http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-189/submission_23.pdf Automated Benchmarking of Description Logic Reasoners]. Description Logics Workshop 2006
* [http://www2009.org/proceedings/pdf/p601.pdf OpenRuleBench] Senlin Liang, Paul Fodor, Hui Wan, Michael Kifer. OpenRuleBench: An Analysis of the Performance of Rule Engines. 2009.  Latest benchmarks at [http://rulebench.projects.semwebcentral.org/ OpenRuleBench website].

{{Semantic Web}}
{{Computable knowledge}}

{{DEFAULTSORT:Semantic Reasoner}}
[[Category:Rule engines| ]]
[[Category:Knowledge representation]]
[[Category:Knowledge engineering]]
[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Reasoning]]</text>
      <sha1>hzqzetc3v8arkw5emm361qxkitjc1ew</sha1>
    </revision>
  </page>
  <page>
    <title>DOAP</title>
    <ns>0</ns>
    <id>4842020</id>
    <revision>
      <id>704970903</id>
      <parentid>704970730</parentid>
      <timestamp>2016-02-14T19:23:00Z</timestamp>
      <contributor>
        <ip>108.24.111.89</ip>
      </contributor>
      <comment>/* External links */ better formatting</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2428" xml:space="preserve">{{one source|date=October 2012}}

'''DOAP''' ('''Description of a Project''') is an [[RDF Schema]] and [[XML]] vocabulary to describe  software projects, in particular [[free and open source software]].

It was created and initially developed by [[Edd Dumbill]] to convey semantic information associated with open source software projects.

== Adoption ==

There are currently generators, [[validator]]s, viewers, and converters to enable more projects to be able to be included in the [[semantic web]]. [[Freshmeat]]'s 43 000 projects are now available published with DOAP.&lt;ref&gt;{{ cite web | url = http://fgiasson.com/blog/index.php/2007/08/04/freshmeatnet-now-available-in-doap-43-000-new-doap-projects/ | title = Freshmeat.net now available in DOAP: 43 000 new DOAP projects | first = Frederick | last = Giasson | accessdate = 2010-04-08 }}&lt;/ref&gt; It is currently used in the [[Mozilla Foundation]]'s project page and in several other software repositories, notably the [[Python Package Index]].

Major properties include: doap:homepage, doap:developer, doap:programming-language, doap:os

== Examples ==

The following is an example in RDF/XML:

&lt;source lang="xml"&gt;
 &lt;rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:doap="http://usefulinc.com/ns/doap#"&gt;
  &lt;doap:Project&gt;
   &lt;doap:name&gt;Example project&lt;/doap:name&gt;
   &lt;doap:homepage rdf:resource="http://example.com" /&gt;
   &lt;doap:programming-language&gt;javascript&lt;/doap:programming-language&gt;
   &lt;doap:license rdf:resource="http://example.com/doap/licenses/gpl"/&gt;
  &lt;/doap:Project&gt;
 &lt;/rdf:RDF&gt;
&lt;/source&gt;

Other properties include &lt;code&gt;Implements specification, anonymous root, platform, browse, mailing list, category, description, helper, tester, short description, audience, screenshots, translator, module, documenter, wiki, repository, name, repository location, language, service endpoint, created, download mirror, vendor, old homepage, revision, download page, license, bug database, maintainer, blog, file-release&lt;/code&gt; and &lt;code&gt;release.&lt;/code&gt;{{citation needed|date=January 2013}}

==References==

{{reflist}}

==External links==
* {{github|edumbill/doap/|Doap Project}}
* [http://www.oss-watch.ac.uk/resources/doap.xml OSS Watch DOAP Briefing Note]
* [http://crschmidt.net/semweb/doapamatic/ doapamatic]: DOAP generator

{{Semantic Web}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]</text>
      <sha1>b7nh0l2clc2mzre08odcdjmyuez9ll0</sha1>
    </revision>
  </page>
  <page>
    <title>HiLog</title>
    <ns>0</ns>
    <id>34697893</id>
    <revision>
      <id>594331762</id>
      <parentid>591711354</parentid>
      <timestamp>2014-02-07T06:50:20Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (9916)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3842" xml:space="preserve">'''HiLog''' is a programming [[logic]] with higher-order syntax, which allows arbitrary terms to appear in predicate and function positions. However, the [[model theory]] of HiLog is first-order. Although syntactically HiLog strictly extends [[first order logic]], HiLog can be embedded into this logic.

HiLog is described in detail in
&lt;ref name="hilog-jlp"&gt;
W. Chen, M. Kifer and D.S. Warren (1993), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.7860 ''HiLog: A Foundation for Higher-Order Logic Programming'']. Journal of Logic Programming, 1993.
&lt;/ref&gt;
.&lt;ref&gt;
W. Chen, M. Kifer and D.S. Warren (1989), [http://citeseerx.ist.psu.edu/showciting?cid=2016805 ''HiLog: A first order semantics for higher-order logic programming constructs'']. Proc. North American Logic Programming Conference, 1989.
&lt;/ref&gt; 
It was later extended in the direction of [[many-sorted logic]] in.&lt;ref&gt;
W. Chen and M. Kifer (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.4332 ''Sorted HiLog: Sorts in Higher-Order Logic Data Languages'']. Int&#8217;l Conference on Database Theory, Springer Lecture Notes in Computer Science #893.
&lt;/ref&gt;
Other contributions to the theory of HiLog include
&lt;ref&gt;
K.A. Ross (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2148 ''On Negation in HiLog'']. Journal of Logic Programming, 1994.
&lt;/ref&gt;
.&lt;ref&gt;
J. de Bruijn and S. Heymans (2008), [http://www.kr.tuwien.ac.at/staff/bruijn/priv/publications/frames-predicates-fi.pdf ''On the Relationship between Description Logic-based and F-Logic-based Ontologies'']. Fundamenta Informaticae 82:3, 2008, pp. 213-236.
&lt;/ref&gt;

The [[XSB|XSB System]] parses HiLog syntax, but the integration of HiLog into XSB is only partial. In particular, HiLog is not integrated with the XSB module system. A full implementation of HiLog is available in the [[Flora-2|Flora-2 system]].

In,&lt;ref name="hilog-jlp"/&gt; it has been shown that HiLog can be embedded into [[first-order logic]] through a fairly simple transformation. For instance, &lt;tt&gt;p(X)(Y,Z(V)(W))&lt;/tt&gt; gets embedded as the following first-order term:

  apply(p(X),Y,apply(apply(Z,V),W))

Details can be found in.&lt;ref name="hilog-jlp"/&gt;

The [[Rule Interchange Format#FLD|Framework for Logic-Based Dialects]] (RIF-FLD) of the [[Rule Interchange Format]] (RIF) is largely based on the ideas underlying HiLog and [[F-logic]].

== Examples ==

In all the examples, below, capitalized symbols denote variables and the comma denotes [[logical conjunction]], as in most [[logic programming]] languages. The first and the second examples show that variables can appear in predicate positions. Predicates can even be complex terms, such as &lt;tt&gt;closure(P)&lt;/tt&gt; or &lt;tt&gt;maplist(F)&lt;/tt&gt; below. The third example shows that variables can also appear in place of atomic formulas, while the fourth example illustrates the use of variables in place of function symbols. The first example defines a generic transitive closure operator, which can be applied to an arbitrary binary predicate. The second example is similar. It defines a [[LISP]]-like mapping operator, which applies to an arbitrary binary predicate. The third example shows that the [[Prolog]] meta-predicate &lt;tt&gt;call/1&lt;/tt&gt; can be expressed in HiLog in a natural way and without the use of extra-logical features. The last example defines a predicate that traverses arbitrary binary trees represented as [[Term (logic)|first-order term]]s.
&lt;source lang="prolog"&gt;
  closure(P)(X,Y) &lt;- P(X,Y).
  closure(P)(X,Y) &lt;- P(X,Z), closure(P)(Z,Y).

  maplist(F)([],[]).
  maplist(F)([X|R],[Y|Z]) &lt;- F(X,Y), maplist(F)(R,Z).

  call(X) &lt;- X.

  traverse(X(L,R)) &lt;- traverse(L), traverse(R).
&lt;/source&gt;

==References==
{{reflist}}

[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]</text>
      <sha1>o1v2lx074q6c4wasjgdnpf7yxtcm1rf</sha1>
    </revision>
  </page>
  <page>
    <title>Enactivism</title>
    <ns>0</ns>
    <id>7082881</id>
    <revision>
      <id>757941878</id>
      <parentid>757806128</parentid>
      <timestamp>2017-01-02T16:29:38Z</timestamp>
      <contributor>
        <username>Mrmatiko</username>
        <id>12110782</id>
      </contributor>
      <comment>/* References */ Fixed refs &amp; wrapped</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="54252" xml:space="preserve">'''Enactivism''' argues that [[cognition]] arises through a dynamic interaction between an acting [[organism]] and its environment.&lt;ref name="Evan Thompson"/&gt; It claims that our environment is one which we selectively create through our capacities to interact with the world.&lt;ref name=Rowlands/&gt; "Organisms do not passively receive information from their environments, which they then translate into internal representations. Natural cognitive systems...participate in the generation of meaning ...engaging in transformational and not merely informational interactions: ''they enact a world''."&lt;ref name=Jaegher1/&gt; These authors suggest that the increasing emphasis upon enactive terminology presages a new era in thinking about cognitive science.&lt;ref name=Jaegher1/&gt; How the actions involved in enactivism relate to age-old questions about [[free will]] remains a topic of active debate.&lt;ref name=Manetti/&gt;

The term 'enactivism' is close in meaning to 'enaction', defined as "the manner in which a subject of perception creatively matches its actions to the requirements of its situation".&lt;ref name=Tascano0/&gt; The introduction of the term ''enaction'' in this context is attributed to  [[Francisco Varela]], [[Evan Thompson]], and [[Eleanor Rosch]],&lt;ref name=Tascano0/&gt;&lt;ref name=RWilson/&gt; who proposed the name to "emphasize the growing conviction that cognition is not the representation of a pre-given world by a pre-given mind but is rather the enactment of a world and a mind on the basis of a history of the variety of actions that a being in the world performs".&lt;ref name=Varela/&gt; This was further developed by Thompson and others,&lt;ref name="Evan Thompson"/&gt; to place emphasis upon the idea that experience of the world is a result of mutual interaction between the sensorimotor capacities of the organism and its environment.&lt;ref name=RWilson/&gt;

The initial emphasis of enactivism upon sensorimotor skills has been criticized as "cognitively marginal",&lt;ref name=ClarkA/&gt; but it has been extended to apply to higher level cognitive activities, such as social interactions.&lt;ref name="Jaegher1"/&gt; "In the enactive view,... knowledge is constructed: it is constructed by an agent through its sensorimotor interactions with its environment, co-constructed between and within living species through their meaningful interaction with each other. In its most abstract form, knowledge is co-constructed between human individuals in socio-linguistic interactions...Science is a particular form of social knowledge construction...[that] allows us to perceive and predict events beyond our immediate cognitive grasp...and also to construct further, even more powerful scientific knowledge."&lt;ref name=Rohde/&gt;

Enactivism is closely related to [[situated cognition]] and [[embodied cognition]], and is presented as an alternative to [[cognitivism (psychology)|cognitivism]], [[computationalism]], and [[Cartesian dualism]].

==Philosophical aspects&lt;!--'Enaction (philosophy)' redirects here--&gt;==

Enactivism is one of  a cluster of related theories sometimes known as the ''4Es'', As described by [[Mark Rowlands]], mental processes are:&lt;ref name=Rowlands/&gt;
* '''Embodied''' involving more than the brain, including a more general involvement of bodily structures and processes.
* '''Embedded''' functioning only in a related external environment.
* '''Enacted''' involving not only neural processes, but also things an organism ''does''.
* '''Extended''' into the organism's environment.

Enactivism proposes an alternative to [[Dualism (philosophy of mind)|dualism]] as a philosophy of mind, in that it emphasises the interactions between mind, body and the environment, seeing them all as inseparably intertwined in mental processes.&lt;ref name=EThompson/&gt; The self arises as part of the process of an embodied entity interacting with the environment in precise ways determined by its physiology.   In this sense, individuals can be seen to "grow into" or arise from their interactive role with the world.&lt;ref name=Burman/&gt;
:"Enaction is the idea that organisms create their own experience through their actions. Organisms are not passive receivers of input from the environment, but are actors in the environment such that what they experience is shaped by how they act."&lt;ref name=Hutchins/&gt;

In ''The Tree of Knowledge'' Maturana &amp; Varela proposed the term ''enactive''&lt;ref name=Maturana/&gt; "to evoke the view of knowledge that what is known is brought forth, in contraposition to the more classical views of either cognitivism&lt;ref group=Note name=Cognitivism/&gt; or connectionism.&lt;ref group=Note name=Connectionism/&gt; They see enactivism as providing a middle ground between the two extremes of [[representationalism]] and [[solipsism]]. They seek to "confront the problem of understanding how our existence-the [[Praxis (process)|praxis]] of our living- is coupled to a surrounding world which appears filled with regularities that are at every instant the result of our biological and social histories.... to find a ''via media'': to understand the regularity of the world we are experiencing at every moment, but without any point of reference independent of ourselves that would give certainty to our descriptions and cognitive assertions.  Indeed the whole mechanism of generating ourselves, as describers and observers tells us that our world, as the world which we bring forth in our coexistence with others, will always have precisely that mixture of regularity and mutability, that combination of solidity and shifting sand, so typical of human experience when we look at it up close."[''Tree of Knowledge'', p.&amp;nbsp;241]

Enactivism also addresses the [[hard problem of consciousness]], referred to by Thompson as part of the ''[[explanatory gap]]'' in explaining how consciousness and subjective experience are related to brain and body.&lt;ref name=EThompson2/&gt;  "The problem with the dualistic concepts of consciousness and life in standard formulations of the hard problem is that they exclude each other by construction".&lt;ref name=EThompson3/&gt; Instead, according to Thompson's view of enactivism, the study of consciousness or [[Phenomenology (philosophy)|phenomenology]] as exemplified by [[Husserl]] and [[Merleau-Ponty]] is to complement science and its objectification of the world. "The whole universe of science is built upon the world as directly experienced, and if we want to subject science itself to rigorous scrutiny and arrive at a precise assessment of its meaning and scope, we must begin by reawakening the basic experience of the world of which science is the second-order expression" (Merleau-Ponty, ''The phenomenology of perception'' as quoted by Thompson, p.&amp;nbsp;165). In this interpretation, enactivism asserts that science is formed or enacted as part of humankind's interactivity with its world, and by embracing phenomenology "science itself is properly situated in relation to the rest of human life and is thereby secured on a sounder footing."&lt;ref name=EThompson4/&gt;&lt;ref name=Baldwin/&gt;

Enaction has been seen as a move to conjoin [[representationalism]] with [[phenomenalism]], that is, as adopting a [[constructivist epistemology]], an epistemology centered upon the active participation of the subject in constructing reality.&lt;ref name=Mutelesi/&gt;&lt;ref name=Chiari/&gt; However, 'constructivism' focuses upon more than a simple 'interactivity' that could be described as a minor adjustment to 'assimilate' reality or 'accommodate' to it.&lt;ref name=Glaserfeld/&gt; Constructivism looks upon interactivity as a radical, creative, revisionist process in which the knower ''constructs'' a personal 'knowledge system' based upon their experience and tested by its viability in practical encounters with their environment. Learning is a result of perceived anomalies that produce dissatisfaction with existing conceptions.&lt;ref name=Glasersfeld2/&gt;

How does constructivism relate to enactivism? From the above remarks it can be seen that [[Ernst von Glasersfeld|Glasersfeld]] expresses an interactivity between the knower and the known quite acceptable to an enactivist, but does not emphasize  the structured probing of the environment by the knower that leads to the "perturbation relative to some expected result" that then leads to a new understanding.&lt;ref name=Glasersfeld2/&gt; It is this probing activity, especially where it is not accidental but deliberate, that characterizes enaction, and invokes ''affect'',&lt;ref name=Ward2/&gt; that is, the motivation and planning that lead to doing and to fashioning the probing, both observing and modifying the environment, so that "perceptions and nature condition one another through generating one another."&lt;ref name=Diettrich/&gt; The questioning nature of this probing activity is not an emphasis of [[Jean Piaget|Piaget]] and Glasersfeld.

Sharing enactivism's stress upon both action and embodiment in the incorporation of knowledge, but giving Glasersfeld's mechanism of viability an [[Introduction to evolution|evolutionary]] emphasis,&lt;ref name=Diettrich2/&gt; is [[evolutionary epistemology]]. Inasmuch as an organism must reflect its environment well enough for the organism to be able to survive in it, and to be competitive enough to be able to reproduce at sustainable rate, the structure and reflexes of the organism itself embody knowledge of its environment. This biology-inspired theory of the growth of knowledge is closely tied to [[universal Darwinism]], and is associated with evolutionary epistemologists such as [[Karl Popper]], [[Donald T. Campbell]], [[Peter Munz]], and [[Gary Cziko]].&lt;ref name=Gontier/&gt; According to Munz, "an organism is an ''embodied theory'' about its environment... Embodied theories are also no longer expressed in language, but in anatomical structures or reflex responses, etc."&lt;ref name=Gontier/&gt;&lt;ref name=Munz/&gt;

==Psychological aspects==
McGann &amp; others&lt;ref name=McGann&gt;{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203&#8211;209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935}}
&lt;/ref&gt; argue that enactivism attempts to mediate between the explanatory role of the coupling between cognitive agent and environment and the traditional emphasis on brain mechanisms found in neuroscience and psychology.  In the interactive approach to social cognition developed by De Jaegher &amp;  others,&lt;ref name=Gallagher0&gt;{{cite journal |author=Shaun Gallagher |year=2001 |title=The practice of mind |journal=Journal of Consciousness Studies |volume=8 |issue=5&#8211;7 |pages=83&#8211;107 |url=http://www.ummoss.org/Gallagher01.pdf}}
&lt;/ref&gt;&lt;ref name=Gallager1&gt;
{{cite book |author=Shaun Gallagher |isbn=978-0199204168 |edition=Paperback |year=2006 |title=How the Body Shapes the Mind |publisher=Oxford University Press |url=https://books.google.com/books/about/How_the_Body_Shapes_the_Mind.html?id=zhv5F-GYm98C}}
&lt;/ref&gt;&lt;ref name=Ratcliffe&gt;
{{cite book |author=Matthew Ratcliffe |year=2008 |title=Rethinking Commonsense Psychology: A Critique of Folk Psychology, Theory of Mind and Simulation |publisher=Palgrave Macmillan |isbn=978-0230221208 |url=https://books.google.com/books/about/Rethinking_Commonsense_Psychology.html?id=-JNyQgAACAAJ}}
&lt;/ref&gt; the dynamics of interactive processes are seen to play significant roles in coordinating interpersonal understanding, processes that in part include what they call [[#Participatory sense-making|''participatory sense-making'']].&lt;ref name=DeJaeger0&gt;
{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |url=http://www.enactionschool.com/resources/papers/DeJaegherDiPaolo2007.pdf |year=2007 |title=Participatory Sense-Making: An enactive approach to social cognition |journal=Phenomenology and the Cognitive Sciences |volume=6 |issue=4 |pages=485&#8211;507 |doi=10.1007/s11097-007-9076-9}}
&lt;/ref&gt;&lt;ref name=DeJaegher1&gt;
{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |year=2010 |title=Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |volume=14 |issue=10 |pages=441&#8211;447 |url=http://ezequieldipaolo.files.wordpress.com/2011/10/dejaegher_dipaolo_gallagher_tics_2010.pdf |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}
&lt;/ref&gt; Recent developments of enactivism in the area of social neuroscience involve the proposal of ''The Interactive Brain Hypothesis''&lt;ref name=DiPaolo3&gt;
{{cite journal |url=http://journal.frontiersin.org/Journal/10.3389/fnhum.2012.00163/full |author1=Ezequiel Di Paolo |author2=Hanne De Jaegher |date=June 2012 |title= The Interactive Brain Hypothesis |journal=Frontiers in Human Neuroscience |volume=7 |issue=6 |doi=10.3389/fnhum.2012.00163}}&lt;/ref&gt; where social cognition brain mechanisms, even those used in non-interactive situations, are proposed to have interactive origins.

===Enactive views of perception===
In the enactive view, perception "is not conceived as the transmission of information but more as an exploration of the world by various means. Cognition is not tied into the workings of an 'inner mind', some cognitive core, but occurs in directed interaction between the body and the world it inhabits."&lt;ref name=McGann2&gt;
{{cite book |title=Consciousness &amp; Emotion: Agency, conscious choice, and selective perception |page=184 |author1=Marek McGann |author2=Steve Torrance |chapter=Doing It and Meaning It: And the relation between the two  |isbn=9789027294616 |publisher=John Benjamins Publishing |year=2005 |url=https://books.google.com/books?id=LZk6AAAAQBAJ&amp;pg=PA184 |editor1=Ralph D. Ellis |editor2=Natika Newton }}
&lt;/ref&gt;

[[Alva No&#235;]] in advocating an enactive view of perception&lt;ref name=Noe&gt;
{{cite book |author=Alva No&#235; |title= Action in Perception |url=https://books.google.com/books?id=kFKvU2hPhxEC&amp;pg=PA1 |pages=1 ''ff'' |chapter=Chapter 1: The enactive approach to perception: An introduction |isbn=9780262140881 |year=2004 |publisher=MIT Press}}
&lt;/ref&gt; sought to resolve how we perceive three-dimensional objects, on the basis of two-dimensional input.  He argues that we perceive this solidity (or 'volumetricity') by appealing to patterns of sensorimotor expectations. These arise from our agent-active  'movements and interaction' with objects, or 'object-active' changes in the object itself. The solidity is perceived through our expectations and skills in knowing how the object's appearance would change with changes in how we relate to it. He saw all perception as an active exploration of the world, rather than being a passive process, something which happens to us.

No&#235;'s idea of the role of 'expectations' in three-dimensional perception has been opposed by several philosophers, notably by [[Andy Clark]].&lt;ref name=ClarkA1/&gt; Clark points to difficulties of the enactive approach.  He points to internal processing of visual signals, for example, in the ventral and dorsal pathways, [[Two-streams hypothesis|the two-streams hypothesis]]. This results in an integrated perception of objects (their recognition and location, respectively) yet this processing cannot be described as an action or actions. In a more general criticism, Clark suggests that perception is not a matter of expectations about sensorimotor mechanisms guiding perception. Rather, although the limitations of sensorimotor mechanisms constrain perception, this sensorimotor activity is drastically filtered to fit current needs and purposes of the organism, and it is these imposed 'expectations' that govern perception, filtering for the 'relevant' details of sensorimotor input (called "sensorimotor summarizing").&lt;ref name=ClarkA1/&gt;

Another application of enaction to perception is analysis of the human hand. The many remarkably demanding uses of the hand are not learned by instruction, but through a history of engagements that lead to the acquisition of skills. According to one interpretation, it is suggested that "the hand [is]...an organ of cognition", not a faithful subordinate working under top-down instruction, but a partner in a "bi-directional interplay between manual and brain activity."&lt;ref name=Hutto&gt;
{{cite book |title= Radicalizing Enactivism: Minds without content |author=[[Daniel D Hutto]], Erik Myin |pages=46 ''ff'' |chapter=A helping hand |url=https://books.google.com/books?id=pAj-96LlBuMC&amp;pg=PA46 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}
&lt;/ref&gt; According to [[Daniel Hutto]]: "Enactivists are concerned to defend the view that our most elementary ways of engaging with the world and others - including our basic forms of perception and perceptual experience - are mindful in the sense of being phenomenally charged and intentionally directed, despite being non-representational and content-free."&lt;ref name=Hutto2&gt;
{{cite book |title= Radicalizing Enactivism: Minds without content |author1=Daniel D Hutto |author2=Erik Myin |pages=12&#8211;13  |chapter=Chapter 1: Enactivism: The radical line |url=https://books.google.com/books?id=pAj-96LlBuMC&amp;pg=PA12 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}
&lt;/ref&gt; Hutto calls this position 'REC' (&lt;u&gt;R&lt;/u&gt;adical &lt;u&gt;E&lt;/u&gt;nactive &lt;u&gt;C&lt;/u&gt;ognition): "According to REC, there is no way to distinguish neural activity that is imagined to be genuinely content involving (and thus truly mental, truly cognitive) from other non-neural activity that merely plays a supporting or enabling role in making mind and cognition possible."&lt;ref name=Hutto2/&gt;

===Participatory sense-making===

[[Hanne De Jaegher]] and [[Ezequiel Di Paolo]] (2007)&lt;ref name="DeJaeger0"/&gt; have extended the enactive concept of sense-making&lt;ref name="EThompson3"/&gt; into the social domain. The idea takes as its departure point the process of interaction between individuals in a social encounter.&lt;ref name=DeJaegher_etal&gt;{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |title= Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |year=2010  |volume=14 |issue=10 |pages=441&#8211;447 |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}
&lt;/ref&gt; De Jaegher and Di Paolo argue that the interaction process itself can take on a form of autonomy (operationally defined). This allows them to define social cognition as the generation of meaning and its transformation through interacting individuals.

The notion of participatory sense-making has led to the proposal that interaction processes can sometimes play constitutive roles in social cognition (De Jaegher, Di Paolo, Gallagher, 2010).&lt;ref name="DeJaegher1"/&gt; It has been applied to research in [[social neuroscience]]''&lt;ref name="DiPaolo3"/&gt;&lt;ref name=SchilbachTimmermans&gt;
{{cite journal |author1=Leonhard Schilbach |author2=Bert Timmermans |author3=Vasudevi Reddy |author4=Alan Costall |author5=Gary Bente |author6=Tobias Schlicht |author7=Kai Vogeley |title= Toward a second-person neuroscience |journal=Behavioral and Brain Sciences |year=2013  |volume=36 |issue=4 |pages=393&#8211;414 |doi=10.1017/S0140525X12000660}}&lt;/ref&gt;'' and [[autism]].''&lt;ref name="DeJaegher_autism"&gt;{{cite journal |author= Hanne De Jaegher |title= Embodiment and sense-making in autism|journal=Frontiers in Integrative Neuroscience |year=2012  |volume=7 |pages=15 |doi=10.3389/fnint.2013.00015}}
&lt;/ref&gt;''

In a similar vein, "an inter-enactive approach to agency holds that the behavior of agents in a social situation unfolds not only according to their individual abilities and goals, but also according to the conditions and constraints imposed by the autonomous dynamics of the interaction process itself".&lt;ref name=STorrance&gt;
{{cite journal |title=An Inter-Enactive Approach to Agency: Participatory Sense-Making, Dynamics, and Sociality |author1=Steve Torrance |author2=Tom Froese |url=http://sacral.c.u-tokyo.ac.jp/pdf/froese_humana_2011.pdf |journal=Human Mente |volume=15 |pages=21&#8211;53 |year=2011 }} 
&lt;/ref&gt; According to Torrance, enactivism involves five interlocking themes related to the question "What is it to be a (cognizing, conscious) agent?" It is:&lt;ref name=STorrance/&gt;
:1. to be a biologically autonomous ([[Autopoiesis|autopoietic]]) organism
:2. to generate ''significance'' or ''meaning'', rather than to act via...updated internal representations of the external world
:3. to engage in sense-making via dynamic coupling with the environment
:4. to 'enact' or 'bring forth' a world of significances by mutual co-determination of the organism with its enacted world
:5. to arrive at an experiential awareness via lived embodiment in the world.

Torrance adds that "many kinds of agency, in particular the agency of human beings, cannot be understood separately from understanding the nature of the interaction that occurs between agents." That view introduces the social applications of enactivism. "Social cognition is regarded as the result of a special form of action, namely ''social interaction''...the enactive approach looks at the circular dynamic within a dyad of embodied agents."&lt;ref name=FuchsT&gt;
{{cite book |url=https://books.google.com/books?id=Olm10GVwV74C&amp;pg=PA206 |page=206 |chapter=Non-representational intersubjectivity |author1=Thomas Fuchs |author2=Hanne De Jaegher |isbn=9783794527915 |year=2010 |publisher=Schattauer Verlag |title=The Embodied Self: Dimensions, Coherence and Disorders |editor1=Thomas Fuchs |editor2=Heribert C. Sattel |editor3=Peter Henningsen }}
&lt;/ref&gt;
 
In [[cultural psychology]], enactivism is seen as a way to uncover cultural influences upon feeling, thinking and acting.&lt;ref name=Verheggen&gt;{{cite book |chapter=Chapter 8: Enactivism |author1=Cor Baerveldt |author2=Theo Verheggen |title=The Oxford Handbook of Culture and Psychology  |url=https://books.google.com/books?id=WljI1r2e-SUC&amp;pg=PA165 |pages=165''ff'' |doi=10.1093/oxfordhb/9780195396430.013.0009 |isbn=9780195396430 |date=May 2012 |quote= Whereas the enactive approach in general has focused on sense-making as an embodied and situated activity, enactive cultural psychology emphasizes the expressive and dynamically enacted nature of cultural meaning.}}&lt;/ref&gt;  Baerveldt and Verheggen argue that "It appears that seemingly natural experience is thoroughly intertwined with sociocultural realities." They suggest that the social patterning of experience is to be understood through enactivism, "the idea that the reality we have in common, and in which we find ourselves, is neither a world that exists independently from us, nor a socially shared way of representing such a pregiven world, but a world itself brought forth by our ways of communicating and our joint action....The world we inhabit is manufactured of 'meaning' rather than 'information'.&lt;ref name=Baerveldt&gt;
{{cite journal |title=Enactivism and the experiential reality of culture: Rethinking the epistemological basis of cultural psychology |author1=Cor Baerveldt |author2=Theo Verheggen |url=https://docs.google.com/file/d/0Bz8cVS8LoO7OTk9ZUkVqazFiU1U/edit |journal=Culture &amp; Psychology |volume=5 |issue=2 |pages=183&#8211;206 |year=1999 |doi=10.1177/1354067x9952006}}
&lt;/ref&gt;

[[Niklas Luhmann|Luhmann]] attempted to apply Maturana and Varela's notion of autopoiesis to social systems.&lt;ref name=Luhmann&gt;
{{cite book |title=Social systems |url=https://books.google.com/books?id=zVZQW4gxXk4C&amp;pg=PA34&amp;lpg=PA34 |isbn= 9780804726252 |year=1995 |publisher=Stanford University Press |author=Niklas Luhmann}}
&lt;/ref&gt; "A core concept of social systems theory is derived from biological systems theory: the concept of ''autopoiesis''. Chilean biologist Humberto Maturana come up with the concept to explain how biological systems such as cells are a product of their own production." "Systems exist by way of operational closure and this means that they each construct themselves and their own realities."&lt;ref name=Moeller&gt;
{{cite book  |chapter=Part 1: A new way of thinking about society |pages= 12 ''ff'' |author=Hans-Georg Moeller |year=2011 |isbn= 978-0812695984 |publisher=Open Court |title=Luhmann Explained: From Souls to Systems |url=https://books.google.com/books?id=tuKsEvpcj9MC&amp;pg=PA12}}
&lt;/ref&gt;

==Educational aspects==
The first definition of enaction was introduced by psychologist [[Jerome Bruner]],&lt;ref name=Pugliese&gt;
{{cite book |title=Intelligent Virtual Agents: |chapter=A framework for motion based bodily enaction with virtual characters; &#167;2.1 Enaction |author1=Roberto Pugliese |author2=Klaus Lehtonen |url=https://books.google.com/books?id=QU9b_IjVMF4C&amp;pg=PA163 |page=163 |isbn=9783642239731 |publisher=Springer |year=2011}}
&lt;/ref&gt;&lt;ref name=Beck&gt;
{{cite book |url=https://books.google.com/books?id=8V9BAAAAQBAJ&amp;pg=PA104 |page=104 |title=From Diagnostics to Learning Success: Proceedings in Vocational Education and Training |isbn=978-9462091894 |edition=Paperback |year=2013 |publisher=Springer Science &amp; Business |author=Stephanie A Hillen |chapter=Chapter III: What can research on technology for learning in vocational educational training teach media didactics? |editor1=Klaus Beck |editor2=Olga Zlatkin-Troitschanskaia }}
&lt;/ref&gt; who introduced enaction as 'learning by doing' in his discussion of how children learn, and how they can best be helped to learn.&lt;ref name=Bruner&gt;{{cite book |author=[[Jerome Bruner]]|year=1966 |title=Toward a theory of instruction |publisher=Belknap Press of Harvard University Press |isbn=978-0674897007}}&lt;/ref&gt;&lt;ref name=Bruner2&gt;{{cite book |author=Jerome Bruner |year=1968 |title=Processes of cognitive growth: Infancy |publisher= Crown Pub |isbn= 978-0517517482}}{{oclc|84376}}&lt;/ref&gt; He associated enaction with two other ways of knowledge organization: [[Cultural icon|Iconic]] and [[Symbol]]ic.&lt;ref name=Bruner3&gt;Quote from
{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}} as quoted from {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&amp;pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor &amp; Francis |isbn= 0415326982 |edition=Paperback}}&lt;/ref&gt;

:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully  (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"
The term 'enactive framework' was elaborated upon by [[Francisco Varela]] and [[Humberto Maturana]].&lt;ref name=Bopry&gt;
{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |author=Jeanette Bopry |chapter=Providing a warrant for constructivist practice: the contribution of Francisco Varela |quote=Varela's enactive framework beginning with his collaboration on [[autopoiesis]] theory with his mentor Humberto Maturana [and the development of] enaction as a framework within which these theories work as a matter of course. |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |year=2007 |publisher=Greenwood Publishing Group |isbn=9780313331237 |url=https://books.google.com/books?id=O1ugEIEid6YC&amp;pg=PA474 |pages=474 ''ff''}}
&lt;/ref&gt;

Sriramen argues that enactivism provides "a rich and powerful explanatory theory for learning and being."&lt;ref name= Sriraman&gt;
{{cite book |title=Theories of Mathematics Education: Seeking New Frontiers |author1=Bharath Sriraman |author2=Lyn English |isbn=3642007422 |year=2009 |publisher=Springer |url=https://books.google.com/books?id=Kd_LgW2AXIoC&amp;pg=PA42 |pages=42 ''ff'' |chapter=Enactivism}}&lt;/ref&gt; and that it is closely related to both the [[Piaget's theory of cognitive development|ideas of cognitive development]] of [[Jean Piaget|Piaget]], and also the [[social constructivism]] of [[Vygotsky]].&lt;ref name=Sriraman/&gt; Piaget focused on the child's immediate environment, and suggested cognitive structures like spatial perception emerge as a result of the child's interaction with the world.&lt;ref name=Roth&gt;
{{cite book |title=Geometry as Objective Science in Elementary School Classrooms: Mathematics in the Flesh |author=Wolff-Michael Roth |isbn=1136732209 |year=2012 |publisher=Routledge |pages=41 ''ff'' |url=https://books.google.com/books?id=cXSsAgAAQBAJ&amp;pg=PT41 |chapter=Epistemology and psychology: Jean Piaget and modern constructivism}}
&lt;/ref&gt; According to Piaget, children ''construct'' knowledge, using what they know in new ways and testing it, and the environment provides feedback concerning the adequacy of their construction.&lt;ref name= Cziko&gt;
{{cite book |title=Without Miracles: Universal Selection Theory and the Second Darwinian Revolution |author=Gary Cziko |chapter=Chapter 12: Education; The provision and transmission of truth, or the selectionist growth of fallible knowledge? |page=222 |url=https://books.google.com/books?id=v1JEypylerUC&amp;pg=PA222&amp;lpg=PA222 |year=1997 |isbn=9780262531474 |publisher=MIT Press}}
&lt;/ref&gt; In a cultural context, Vygotsky suggested that the kind of cognition that can take place is not dictated by the engagement of the isolated child, but is also a function of social interaction and dialogue that is contingent upon a sociohistorical context.&lt;ref name=Kincheloe&gt;
{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |chapter=Interpretivists drawing on the power of enactivism |url=https://books.google.com/books?id=O1ugEIEid6YC&amp;pg=PA24 |pages=24 ''ff'' |publisher=Greenwood Publishing Group |year=2007 |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |author=Joe L Kincheloe |isbn=0313331235}}
&lt;/ref&gt;  Enactivism in educational theory "looks at each learning situation as a complex system consisting of teacher, learner, and context, all of which frame and co-create the learning situation."&lt;ref name=Vithal&gt;
{{cite book |editor1=Renuka Vithal |editor2=Jill Adler |editor3=Christine Keitel |title=Researching Mathematics Education in South Africa: Perspectives, Practices and Possibilities |chapter=Chapter 9: Dilemmas of change: seeing the complex rather than the complicated?  |page=240 |author=Chris Breen |isbn=0796920478 |publisher=HSRC Press |year=2005 |url=https://books.google.com/books?id=byWHt_NVUEgC&amp;pg=RA6-PA240}}
&lt;/ref&gt; Enactivism in education is very closely related to [[situated cognition]],&lt;ref name=VanDeGevel&gt;
{{cite book |title=The nexus between artificial intelligence and economics |chapter=&#167;3.2.2 Enactive artificial intelligence |quote=''Enactivism'' may be considered as the most developed model of embodied situated cognition...Knowing is inseparable from doing. |url=https://books.google.com/books?id=uek_AAAAQBAJ&amp;pg=PA21 |page=21 |author=Ad J. W. van de Gevel, Charles N. Noussair |isbn=3642336477 |publisher=Springer |year=2013}}
&lt;/ref&gt; which holds that "knowledge is situated, being in part a product of the activity, context, and culture in which it is developed and used."&lt;ref name=Collins&gt;
{{cite journal |title=Situated cognition and the culture of learning |author1=John Seely Brown |author2=Allan Collins |author3=Paul Duguid |url=http://www.exploratorium.edu/ifi/resources/museumeducation/situated.html |journal=Educational Researcher |volume=18 |number=1 |pages=32&#8211;42 |date=Jan&#8211;Feb 1989 |doi=10.3102/0013189x018001032}}
&lt;/ref&gt; This approach challenges the "separating of what is learned from how it is learned and used."&lt;ref name=Collins/&gt;

==Artificial intelligence aspects==
{{importance section|date=May 2014}}
{{main|Enactive interfaces}}
The ideas of enactivism regarding how organisms engage with their environment have interested those involved in [[Cognitive robotics|robotics]] and [[Human&#8211;computer interaction|man-machine interfaces]]. The analogy is drawn that a robot can be designed to interact and learn from its environment in a manner similar to the way an organism does that,&lt;ref name=Sandini&gt;
{{cite book |chapter=The ''iCub'' cognitive humanoid robot: An open-system research platform for enactive cognition |author1=Giulio Sandini |author2=Giorgio Metta |author3=David Vernon |title=50 Years of Artificial Intelligence: Essays Dedicated to the 50th Anniversary of Artificial Intelligence |editor1=Max Lungarella |editor2=Fumiya Iida |editor3=Josh Bongard |editor4=Rolf Pfeifer |publisher=Springer |year=2007 |isbn= 9783540772958}}
&lt;/ref&gt; and a human can interact with a computer-aided design tool or data base using an interface that creates an enactive environment for the user, that is, all the user's tactile, auditory, and visual capabilities are enlisted in a mutually explorative engagement, capitalizing upon all the user's abilities, and not at all limited to cerebral engagement.&lt;ref name=Bordegoni&gt;
{{cite book |title=Emotional Engineering: Service Development |chapter=&#167;4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&amp;pg=PA78 |pages=78 ''ff'' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}
&lt;/ref&gt; In these areas it is common to refer to [[affordance]]s as a design concept, the idea that an environment or an interface affords opportunities for enaction, and good design involves optimizing the role of such affordances.&lt;ref name=Norman&gt;
{{cite book |title=The Design of Everyday Things |edition=Revised and expanded |quote=An affordance is a relationship between the properties of an object and the capabilities of the agent that determine just how the object could possibly be used. |url=https://books.google.com/books?id=nVQPAAAAQBAJ&amp;pg=PT17 |year=2013 |page=11 |isbn=978-0465050659 |publisher=Basic Books |author=Don Norman |chapter=Affordances }}
&lt;/ref&gt;&lt;ref name=Kim&gt;
{{cite book |title=Encyclopedia of human computer interaction |chapter=The use and evolution of affordance in HCI  |url=https://books.google.com/books?id=h9iZh_I1YREC&amp;pg=PA668 |pages=668 ''ff'' |isbn=9781591407980 |year=2006 |publisher=Idea Group Inc |author=Georgios S Christou |editor=Claude Ghaoui}}
&lt;/ref&gt;&lt;ref name=Kaipainen&gt;
{{cite journal |title=Enactive Systems and Enactive Media: Embodied Human-Machine Coupling beyond Interfaces |url=http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_00244#.U3_JKygT0cs  |journal=Leonardo |volume=44 |pages=433&#8211;438 |date=October 2011 |issue=5 |doi=10.1162/LEON_a_00244 |author1=Mauri Kaipainen |author2=Niklas Ravaja |author3=Pia Tikka |display-authors=etal}} 
&lt;/ref&gt;&lt;ref name=Boy&gt;

{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&amp;pg=PA118&amp;lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118 |quote=The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela's sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself.}}

&lt;/ref&gt;&lt;ref name=Thannhuber&gt;
{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-J&#246;rg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&amp;rct=j&amp;q=&amp;esrc=s&amp;sa=X&amp;ei=N-h_U6HtHIiEogSy_oHAAw&amp;ved=0CCcQgAMoADAA&amp;usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&amp;cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 ''ff'' |doi=10.1016/s0007-8506(07)62129-5}}&lt;/ref&gt;

The activity in the AI community also has influenced enactivism as whole. Referring extensively to modeling techniques for [[evolutionary robotics]] by Beer,&lt;ref name=Beer&gt;
{{cite journal |author=Randall D Beer |year=1995 |title=A dynamical systems perspective on agent-environment interaction.
 |journal= Artificial Intelligence |volume=72 |pages=173&#8211;215 |url=http://dx.doi.org/10.1016/0004-3702%2894%2900005-L |doi=10.1016/0004-3702(94)00005-l}}
&lt;/ref&gt; the modeling of learning behavior by Kelso,&lt;ref name=Kelso&gt;
{{cite book |author=James AS Kelso |year=2009 |chapter=Coordination dynamics |editor=R. A. Meyers |title= Encyclopedia of complexity and system science |pages= 1537&#8211;1564 |isbn=978-0-387-75888-6 |url=http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30440-3_101}}
&lt;/ref&gt; and to modeling of sensorimotor activity by Saltzman,&lt;ref name=Saltzman&gt;
{{cite book |author=Eliot L. Saltzman |year=1995 |chapter=Dynamics and coordinate systems in skilled sensorimotor activity |editor1=T. van Gelder |editor2=R. F. Port |title= Mind as motion: Explorations in the dynamics of cognition  |publisher= MIT Press |isbn=9780262161503 |url=https://books.google.com/books?id=e6HUM6V8QbQC&amp;pg=PA151 |page=151 ''ff''}}
&lt;/ref&gt; McGann, De Jaegher, and Di Paolo discuss how this work makes the dynamics of coupling between an agent and its environment, the foundation of enactivism, "an operational, empirically observable phenomenon."&lt;ref name=McGann3&gt;{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203&#8211;209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935 |quote=Such modeling techniques allow us to explore the parameter space of coupling between agent and environment...to the point that their basic principles (the universals, if such there are, of enactive psychology) can be brought clearly into view.}}
&lt;/ref&gt; That is, the AI environment invents examples of enactivism using concrete examples that, although not as complex as living organisms, isolate and illuminate basic principles.

==See also==
{{colbegin}}
*[[Action-specific perception]]
*[[Autopoesis]]
*[[Biosemiotics]]
*[[Cognitive science]]
*[[Cognitive psychology]]
*[[Computational theory of mind]]
*[[Connectivism]]
*[[Cultural psychology]]
*[[Distributed cognition]]
*[[Embodied cognition]]
*[[Embodied embedded cognition]]
*[[Enactive interfaces]]
*[[Extended cognition]]
*[[Extended mind]]
*[[Externalism#Enactivism and embodied cognition]]
*[[Mind&#8211;body problem]]
*[[Phenomenology (philosophy)]]
*[[Representationalism]]
*[[Situated cognition]]
*[[Social cognition]]
{{colend}}

==References==
{{reflist|30em|refs=

&lt;ref name=Baldwin&gt;
{{cite book |author=Thomas Baldwin |title=[[Maurice Merleau-Ponty]]: Basic Writings |chapter-url=https://books.google.com/books?id=OS8FM-AFvvsC&amp;pg=PA65 |chapter=Part One: Merleau-Ponty's prospectus of his work |page=65  |quote=Science has not and never will have, by its nature, the same significance ''qua'' form of being as the world which we perceive, for the simple reason that it is a rationale or explanation of that world. |isbn= 978-0415315869 |year=2003 |publisher=Routledge}}
&lt;/ref&gt;

&lt;ref name=Burman&gt;
{{cite journal |author=Jeremy Trevelyan Burman  |year=2006 |journal=Journal of Consciousness Studies |title=Book reviews: ''Consciousness &amp; Emotion'' |url=http://www.imprint.co.uk/pdf/13_12_br.pdf  |volume=13 |issue=12 |pages=115&#8211;124}}  From a review of {{cite book |title=Consciousness &amp; Emotion: Agency, conscious choice, and selective perception |editor1=Ralph D. Ellis |editor2=Natika Newton |url=https://books.google.com/books?id=LZk6AAAAQBAJ&amp;printsec=frontcover |isbn=9789027294616 |year=2005 |publisher=John Benjamins Publishing}}
&lt;/ref&gt;

&lt;ref name=Chiari&gt;
{{cite web |title=Constructivism |author1=Gabriele Chiari |author2=M. Laura Nuzzo |work=The Internet Encyclopaedia of Personal Construct Psychology |url=http://www.pcp-net.org/encyclopaedia/constructivism.html}}
&lt;/ref&gt;

&lt;ref name=ClarkA&gt;
{{cite journal |author1=Andy Clark |author2=Josefa Toribio |title=Doing without representing |journal =Synthese |volume=101 |pages=401&#8211;434 |year=1994 |url=http://www.philosophy.ed.ac.uk/people/clark/pubs/DoingW-O-rep.pdf |doi=10.1007/bf01063896}}
&lt;/ref&gt;

&lt;ref name=ClarkA1&gt;
{{cite journal  |author=Andy Clark   |title=Vision as Dance? Three Challenges for Sensorimotor Contingency Theory |journal= Psyche |volume=12 |issue=1 |date=March 2006 |url= https://www.era.lib.ed.ac.uk/bitstream/1842/1444/1/Psyche%20Clark.pdf}}
&lt;/ref&gt;

&lt;ref name=Diettrich&gt;
{{cite book |author=Olaf Diettrich |chapter=The biological boundary conditions for our classical physical world view |title=Evolutionary Epistemology, Language and Culture |page=88 |year=2006 |publisher=Springer |editor1=Nathalie Gontier |editor2=Jean Paul van Bendegem |editor3=Diederik Aerts |isbn=9781402033957 |url=https://books.google.com/books?id=hp2JiTDBbWkC&amp;pg=PA88}}
&lt;/ref&gt;

&lt;ref name=Diettrich2&gt;
"The notion of 'truth' is replaced with 'viability' within the subjects' experiential world." From {{cite book |title= The handbook of evolution: The evolution of human societies and culture |author=Olaf Diettrich |chapter=Cognitive evolution; footnote 2 |page=61 |url=https://books.google.com/books?id=Ex5c_pyOsTwC&amp;pg=PA61&amp;lpg=PA61#v=onepage&amp;q&amp;f=false |editor1=Franz M. Wuketits |editor2=Christoph Antweiler |year=2008 |publisher=Wiley-Blackwell}} and in ''Evolutionary Epistemology, Language and Culture'' cited above, p. 90.
&lt;/ref&gt;

&lt;ref name=Glaserfeld&gt;
{{cite book |author= Ernst von Glasersfeld |title=Epistemology and education |chapter=Report no. 14: Piaget and the Radical Constructivist Epistemology |url=http://www.vonglasersfeld.com/034 |editor1=CD Smock |editor2=E von Glaserfeld |publisher=Follow Through Publications |pages=1&#8211;24 |year=1974}} 
&lt;/ref&gt;

&lt;ref name=Glasersfeld2&gt;
{{cite journal |author= Ernst von Glasersfeld |url=http://www.univie.ac.at/constructivism/EvG/papers/118.pdf |title=Cognition, construction of knowledge and teaching |journal=Synthese |volume=80 |issue=1 |pages=121&#8211;140 |year=1989 |doi=10.1007/bf00869951}}
&lt;/ref&gt;

&lt;ref name=Gontier&gt;
{{cite web |author=Nathalie Gontier |title=Evolutionary Epistemology |url=http://www.iep.utm.edu/evo-epis/ |work=Internet Encyclopedia of Philosophy |year=2006}}
&lt;/ref&gt;

&lt;ref name=Hutchins&gt;	
{{cite book |title=Cognition in the Wild |author=Edwin Hutchins |url= |isbn=9780262581462 |year=1996 |page=428 |publisher=MIT Press }} Quoted by {{cite journal |title=Cognitive, embodied or enacted? :Contemporary perspectives for HCI and interaction  |url=http://trans-techresearch.net/wp-content/uploads/2010/11/Rocha-01.pdf |author=Marcio Rocha |year=2011 |publisher=Transtechnology Research Reader |isbn=978-0-9538332-2-1}}	
&lt;/ref&gt;

&lt;ref name=Jaegher1&gt;
{{cite book |author1=Ezequiel A Di Paolo |author2=Marieke Rhohde |author3=Hanne De Jaegher |chapter=Horizons for the enactive mind: Values, social interaction, and play |title=Enaction: Toward a New Paradigm for Cognitive Science |editor1=John Stewart |editor2=Oliver Gapenne |editor3=Ezequiel A Di Paolo |url=https://books.google.com/books?id=UtFDJx-gysQC&amp;pg=PA39 |pages=33 ''ff'' |isbn=  978-0262526012 |publisher=MIT Press |year=2014}}
&lt;/ref&gt;

&lt;ref name=Manetti&gt;
A collection of papers on this topic is introduced by {{cite journal |title=Agency: From embodied cognition to free will |author1=Duccio Manetti |author2=Silvano Zipoli Caiani |journal=Humana Mente |volume=15 |date=January 2011 |pages=''V''-''XIII'' |url=http://www.humanamente.eu/PDF/Issue15_CompletePDF.pdf}}
&lt;/ref&gt;

&lt;ref name=Maturana&gt;
{{cite book |author1=Humberto R Maturana |author2=Francisco J Varela |year=1992 |title= The tree of knowledge: the biological roots of human understanding |edition=Revised |publisher=Shambhala Publications Inc |chapter=Afterword |page=255 |isbn=978-0877736424}}
&lt;/ref&gt;

&lt;ref name=Munz&gt;
{{cite book |url=https://books.google.com/books?id=tMuIAgAAQBAJ&amp;pg=PA154&amp;lpg=PA154 |page=154 |author=Peter Munz |title=Philosophical Darwinism: On the Origin of Knowledge by Means of Natural Selection |year=2002 |isbn=9781134884841 |publisher=Routledge}}
&lt;/ref&gt;

&lt;ref name=Mutelesi&gt;
{{cite journal |title=Radical constructivism seen with Edmund Husserl as starting point |author=Edmond Mutelesi |url=http://www.univie.ac.at/constructivism/journal/2/1/006.mutelesi |journal=Constructivist foundations |volume=2 |issue=1 |pages=6&#8211;16 |date=November 15, 2006}}
&lt;/ref&gt;

&lt;ref name=Rohde&gt;
{{cite book |title=Enaction, Embodiment, Evolutionary Robotics: Simulation Models for a Post-Cognitivist Science of Mind  |chapter= &#167;3.1 The scientist as observing subject |pages=30 ''ff'' |author=Marieke Rohde |isbn=978-9078677239 |publisher=Atlantis Press |year=2010 |url=https://books.google.com/books?id=LlpZjLMPiHYC&amp;pg=PA30}}
&lt;/ref&gt;

&lt;ref name=Rowlands&gt;
{{cite book |author=Mark Rowlands |chapter=Chapter 3: The mind embedded &#167;5 The mind enacted |pages=70 ''ff'' |year=2010 |isbn=0262014556 |publisher=MIT Press |url=https://books.google.com/books?id=AiwjpL-0hDgC&amp;pg=PA70 |title=The new science of the mind: From extended mind to embodied phenomenology}} Rowlands attributes this idea to {{cite book |author=D M MacKay |year=1967 |chapter=Ways of looking at perception |title=Models for the perception of speech and visual form (Proceedings of a symposium) |editor=W Watthen-Dunn |publisher=MIT Press |pages=25 ''ff'' |url=https://books.google.com/books?id=Ts9JAAAAMAAJ&amp;focus=searchwithinvolume&amp;q=MacKay+Ways+of+looking+at+perception}}
&lt;/ref&gt;

&lt;ref name=EThompson&gt;
{{cite book |title= Mind in life |chapter=The enactive approach |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=13 ''ff'' |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA13 |publisher=Harvard University Press |year=2007 }}  ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here]
&lt;/ref&gt;

&lt;ref name=EThompson2&gt;
{{cite book |title= Mind in life |chapter=Autonomy and emergence |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=37 ''ff'' |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA13 |publisher=Harvard University Press |year=2007}} See also the Introduction, p. ''x''.
&lt;/ref&gt;

&lt;ref name=EThompson3&gt;
{{cite book |title= Mind in life |chapter=Chapter 8: Life beyond the gap |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=225  |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA225 |publisher=Harvard University Press |year=2007}}
&lt;/ref&gt;

&lt;ref name=EThompson4&gt;
{{cite book |title= Mind in life |chapter=Life can be known only by life |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=165  |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA165 |publisher=Harvard University Press |year=2007}}
&lt;/ref&gt;

&lt;ref name="Evan Thompson"&gt;
{{cite book |title=Mind in life:Biology, phenomenology, and the sciences of mind |author=Evan Thompson |url=http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf |publisher=Harvard University Press |isbn= 978-0674057517 |chapter=Chapter 1: The enactive approach |year=2010}} ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here].
&lt;/ref&gt;

&lt;ref name=Tascano0&gt;
{{cite book |title=A Dictionary of Continental Philosophy |editor=John Protevi |url=https://books.google.com/books?id=kRUZ61uISUMC&amp;pg=PA169 |pages=169&#8211;170 |chapter = Enaction |isbn=9780300116052 |publisher=Yale University Press |year=2006}}
&lt;/ref&gt;

&lt;ref name=Varela&gt;
{{cite book |title=The embodied mind: Cognitive science and human experience |author1=Francisco J Varela |author2=Evan Thompson |author3=Eleanor Rosch |url=https://books.google.com/books?id=QY4RoH2z5DoC&amp;printsec=frontcover#v=snippet&amp;q=We%20propose%20as%20a%20%20name%20the%20term%20%20enactive&amp;f=false |year=1992 |publisher=MIT Press |page=9 |quote= |isbn=978-0262261234}}&lt;/ref&gt;

&lt;ref name=Ward2&gt;
"The underpinnings of cognition are inextricable from those of affect, that the phenomenon of cognition itself is essentially bound up with affect.." See p. 104: {{cite book |author1=Dave Ward |author2=Mog Stapleton |year=2012 |url=https://books.google.com/books?id=Y1E7FogqvJ0C&amp;pg=PA89 |chapter=Es are good. Cognition as enacted, embodied, embedded, affective and extended |editor= Fabio Paglieri |title=Consciousness in Interaction: The role of the natural and social context in shaping consciousness |publisher=John Benjamins Publishing |pages=89 ''ff'' |isbn=978-9027213525}} [http://philpapers.org/archive/WAREAG.pdf On-line version here].
&lt;/ref&gt;

&lt;ref name=RWilson&gt;
{{cite web |author1=Robert A Wilson |author2=Lucia Foglia |title=Embodied Cognition: &#167;2.2 Enactive cognition |work=The Stanford Encyclopedia of Philosophy (Fall 2011 Edition) |editor=Edward N. Zalta |url = http://plato.stanford.edu/archives/fall2011/entries/embodied-cognition/#EnaCog |date=July 25, 2011}}
&lt;/ref&gt;

}}

==Further reading==
* {{cite journal |author1=De Jaegher H. |author2=Di Paolo E. A. | year = 2007 | title = Participatory sense-making: An enactive approach to social cognition | url = | journal = Phenomenology and the Cognitive Sciences | volume = 6 | issue = 4| pages = 485&#8211;507 | doi=10.1007/s11097-007-9076-9}}
* Di Paolo, E. A., Rohde, M. and De Jaegher, H., (2010). ''Horizons for the Enactive Mind: Values, Social Interaction, and Play.'' In J. Stewart, O. Gapenne and E. A. Di Paolo (eds), Enaction: Towards a New Paradigm for Cognitive Science, Cambridge, MA: MIT Press, pp.&amp;nbsp;33 &#8211; 87. ISBN 9780262014601
* [[Daniel Hutto|Hutto, D. D.]] (Ed.)  (2006).  ''Radical Enactivism: Intentionality, phenomenology, and narrative.''  In R. D. Ellis &amp; N. Newton (Series Eds.), ''Consciousness &amp; Emotion, vol. 2.'' ISBN 90-272-4151-1
* McGann, M. &amp; Torrance, S. (2005).  Doing it and meaning it (and the relationship between the two).  In R. D. Ellis &amp; N. Newton, ''Consciousness &amp; Emotion, vol. 1: Agency, conscious choice, and selective perception''. Amsterdam: John Benjamins. ISBN 1-58811-596-8
* {{cite journal |title=The enactive approach: Theoretical sketches from cell to society |author1=Tom Froese |author2=Ezequiel A DiPaolo |citeseerx = 10.1.1.224.5504 |journal=Pragmatics and Cognition |volume=19 |issue=1 |year=2011 |pages=1&#8211;36 |doi=10.1075/pc.19.1.01fro}}
* {{cite journal |author1=Steve Torrance |author2=Tom Froese |title=An inter-enactive approach to agency: participatory sense-making, dynamics, and sociality. |journal=Humana. Mente |volume=15 |year=2011 |pages=21&#8211;53 |citeseerx = 10.1.1.187.1151 }}

==Notes==
{{reflist |group=Note |refs=
&lt;ref group=Note name=Cognitivism&gt;
Cognition as information processing like that of a digital computer. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} ''Cognitivism'', p. 4; See also {{cite web |title=The computational theory of mind |author=Steven Horst |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/spr2011/entries/computational-mind/ |date=December 10, 2009}}
&lt;/ref&gt;

&lt;ref group=Note name=Connectionism&gt;
Cognition as emergent patterns of activity in a neural network. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} ''Connectionism'', p. 8; See also {{cite web |title=Connectionism |author=James Garson |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/win2012/entries/connectionism/ |date=July 27, 2010}}
&lt;/ref&gt;

}}

==External links==
*{{cite web |title=Consciousness as the emergent property of the interaction between brain, body, &amp; environment: the crucial role of haptic perception  |author=Pietro Morasso |url=http://www.consciousness.it/iwac2005/Material/Morasso.pdf |year=2005 }} Slides related to a chapter on [[haptic perception]] (recognition through touch): {{cite book |editor1=Antonio Chella |editor2=Riccardo Manzotti |author=Pietro Morasso |chapter=Chapter 14: The crucial role of haptic perception |page=234 ''ff'' |title= Artificial Consciousness |publisher= Academic  |year=2007 |isbn=978-1845400705 |url=https://www.google.com/search?tbo=p&amp;tbm=bks&amp;q=isbn:1845400704&amp;num=10}}
*{{cite web |title=Questioning Life and Cognition: Some Foundational Issues in the Paradigm of Enaction |url=http://www.enactionseries.com/library/bookjs/co/Original_book_JS.html#Pk1qsEYBVxgUwAM6tVeiff |author=John Stewart |work=Enaction Series: Online Collaborative Publishing |editor1=Olivier Gapenne |editor2=Bruno Bachimont |publisher=Enaction Series |accessdate=April 27, 2014}} 
*{{cite web |title=Educational Multimedia Task Force &#8211; MM 1045, REPRESENTATION |url=http://halshs.archives-ouvertes.fr/docs/00/00/18/64/PDF/REPRDel1.pdf |publisher= |author1=George-Louis Baron |author2=Eric Bruillard |author3=Christophe Dansac |date=January 1999}} An overview of the rationale and means and methods for the study of representations that the learner constructs in his/her attempt to understand knowledge in a given field. See in particular &#167;1.2.1.4 ''Toward social representations'' (p.&amp;nbsp;24)
*{{cite web |author=Randall Whittaker |year=2001 |title=Autopoiesis and enaction |url=http://www.enolagaia.com/AT.html |publisher=Observer Web}} An extensive but uncritical introduction to the work of [[Francisco Varela]] and [[Humberto Maturana]]
*{{cite journal|title=Enactivism: Arguments &amp; Applications.|journal=Avant|date=Autumn 2014|volume= V| issue =  2/2014|doi=10.12849/50202014.0109.0002|url=http://avant.edu.pl/en/22014-2|accessdate=27 November 2014}} Entire journal issue on enactivism's status and current debates.

[[Category:Behavioral neuroscience]]
[[Category:Cognitive science]]
[[Category:Consciousness]]
[[Category:Educational psychology]]
[[Category:Enactive cognition]]
[[Category:Epistemology of science]]
[[Category:Knowledge representation]]
[[Category:Metaphysics of mind]]
[[Category:Motor cognition]]
[[Category:Neuropsychology]]
[[Category:Perception]]
[[Category:Philosophical theories]]
[[Category:Philosophy of psychology]]
[[Category:Psychological concepts]]
[[Category:Psychological theories]]
[[Category:Sociology of knowledge]]
[[Category:Action (philosophy)]]</text>
      <sha1>2vpk0cqio8mf69fzay45ertsnmucmqk</sha1>
    </revision>
  </page>
  <page>
    <title>KL-ONE</title>
    <ns>0</ns>
    <id>17188</id>
    <revision>
      <id>738216007</id>
      <parentid>722298196</parentid>
      <timestamp>2016-09-07T16:34:11Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>add pronunciation, by request on talk page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3044" xml:space="preserve">{{distinguish | KL1 }}

'''KL-ONE''' (pronounced "kay ell won") is a well known [[knowledge representation]] system in the tradition of [[semantic networks]] and [[Frame (Artificial intelligence) | frames]]; that is, it is a [[frame language]]. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.&lt;ref&gt;{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers &amp; Mathematics with Applications | volume = 23 | issue = 2&#8211;5 | pages = 133 | year = 1992 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171 | year = 1985 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{cite book | last = D.A. Duce | first = G.A. Ringland  | title = Approaches to Knowledge Representation, An Introduction | year = 1988 | publisher = Research Studies Press, Ltd. | isbn = 0-86380-064-5 }}&lt;/ref&gt;

There is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a [[deductive classifier]], an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. 

Frames in KL-ONE are called [[concepts]]. These form hierarchies using subsume-relations; in the KL-ONE terminology a [[superclass (computer science)|super class]] is said to subsume its [[Subclass (computer science) | subclasses]]. 
[[Multiple inheritance]] is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. 

In KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are [[necessary and sufficient]] conditions to classify the concept.

The slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler.

==See also==
* [[Ontology language]]

==References==
{{reflist}}


{{FOLDOC}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Ontology languages]]</text>
      <sha1>kbm0e0qbero6e8lxz4ewdymipzb83j2</sha1>
    </revision>
  </page>
  <page>
    <title>Closed-world assumption</title>
    <ns>0</ns>
    <id>2526582</id>
    <revision>
      <id>751601040</id>
      <parentid>651094943</parentid>
      <timestamp>2016-11-26T18:49:13Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9923" xml:space="preserve">{{more footnotes|date=August 2011}}
The '''closed-world assumption''' (CWA), in a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], is the presumption that a statement that is true is also known to be true. Therefore, conversely, what is not currently known to be true, is false. The same name also refers to a [[formal logic|logical]] formalization of this assumption by [[Raymond Reiter]]. The opposite of the closed-world assumption is the [[open-world assumption]] (OWA), stating that lack of knowledge does not imply falsity. Decisions on CWA vs. OWA determine the understanding of the actual semantics of a conceptual expression with the same notations of concepts. A successful formalization of natural language semantics usually cannot avoid an explicit revelation of whether the implicit logical backgrounds are based on CWA or OWA.

[[Negation as failure]] is related to the closed-world assumption, as it amounts to believing false every predicate that cannot be proved to be true.

== Example ==

In the context of [[knowledge management]], the closed-world assumption is used in at least two situations: (1) when the knowledge base is known to be complete (e.g., a corporate database containing records for every employee), and (2) when the knowledge base is known to be incomplete but a "best" definite answer must be derived from incomplete information. For example, if a [[database]] contains the following table reporting editors who have worked on a given article, a query on the people not having edited the article on Formal Logic is usually expected to return "Sarah Johnson".

{| border="1" cellspacing="0" cellpadding="2" align="center"
! colspan="2" style="background:#ffdead;" | Edit
|-
! style="background:#efefef;" | Editor
! style="background:#efefef;" | Article
|-
| John Doe || Formal Logic
|-
| Joshua A. Norton || Formal Logic
|-
| Sarah Johnson || Introduction to Spatial Databases
|-
| Charles Ponzi || Formal Logic
|-
| Emma Lee-Choon || Formal Logic
|}
&lt;br /&gt; &lt;!-- The preceding tag creates a whitespace line separating the table above from the text paragraph below --&gt;
In the closed-world assumption, the table is assumed to be [[Complete knowledge base|complete]] (it lists all editor-article relationships), and Sarah Johnson is the only editor who has not edited the article on Formal Logic. In contrast, with the open-world assumption the table is not assumed to contain all editor-article tuples, and the answer to who has not edited the Formal Logic article is unknown. There is an unknown number of editors not listed in the table, and an unknown number of articles edited by Sarah Johnson that are also not listed in the table.

==Formalization in logic==

The first formalization of the closed-world assumption in [[logic|formal logic]] consists in adding to the knowledge base the negation of the literals that are not currently [[logical consequence|entailed]] by it. The result of this addition is always [[consistent]] if the knowledge base is in [[Horn clause|Horn form]], but is not guaranteed to be consistent otherwise. For example, the knowledge base
:&lt;math&gt;\{English(Fred) \vee Irish(Fred)\}&lt;/math&gt;
entails neither &lt;math&gt;English(Fred)&lt;/math&gt; nor &lt;math&gt;Irish(Fred)&lt;/math&gt;.

Adding the negation of these two literals to the knowledge base leads to
:&lt;math&gt;\{English(Fred) \vee Irish(Fred), \neg English(Fred), \neg Irish(Fred)\}&lt;/math&gt;
which is inconsistent. In other words, this formalization of the closed-world assumption sometimes turns a consistent knowledge base into an inconsistent one. The closed-world assumption does not introduce an inconsistency on a knowledge base &lt;math&gt;K&lt;/math&gt; exactly when the intersection of all [[Herbrand model]]s of &lt;math&gt;K&lt;/math&gt; is also a model of &lt;math&gt;K&lt;/math&gt;; in the propositional case, this condition is equivalent to &lt;math&gt;K&lt;/math&gt; having a single minimal model, where a model is minimal if no other model has a subset of variables assigned to true.

Alternative formalizations not suffering from this problem have been proposed. In the following description, the considered knowledge base &lt;math&gt;K&lt;/math&gt; is assumed to be propositional. In all cases, the formalization of the closed-world assumption is based on adding to &lt;math&gt;K&lt;/math&gt; the negation of the formulae that are &#8220;free for negation&#8221; for &lt;math&gt;K&lt;/math&gt;, i.e., the formulae that can be assumed to be false. In other words, the closed-world assumption applied to a [[propositional formula]] &lt;math&gt;K&lt;/math&gt; generates the formula:
:&lt;math&gt;K \wedge \{\neg f ~|~ f \in F\}&lt;/math&gt;.
The set &lt;math&gt;F&lt;/math&gt; of formulae that are free for negation in &lt;math&gt;K&lt;/math&gt; can be defined in different ways, leading to different formalizations of the closed-world assumption. The following are the definitions of &lt;math&gt;f&lt;/math&gt; being free for negation in the various formalizations.

; CWA (closed-world assumption) : &lt;math&gt;f&lt;/math&gt; is a positive literal not entailed by &lt;math&gt;K&lt;/math&gt;;

; GCWA (generalized CWA) : &lt;math&gt;f&lt;/math&gt; is a positive literal such that, for every positive clause &lt;math&gt;c&lt;/math&gt; such that &lt;math&gt;K \not\vdash c&lt;/math&gt;, it holds &lt;math&gt;K \not\vdash c \vee f&lt;/math&gt;;&lt;ref&gt;{{Citation
 | first = Jack | last = Minker
 | author-link = Jack Minker
 | title = On indefinite databases and the closed world assumption
 | publisher = [[Springer Berlin Heidelberg]]
 | series = Lecture Notes in Computer Science
 | volume = 138
 | year = 1982
 | pages = 292&#8211;308
 | url = http://link.springer.com/chapter/10.1007/BFb0000066
 | doi = 10.1007/BFb0000066
 | isbn = 978-3-540-11558-8 }}&lt;/ref&gt;

; EGCWA (extended GCWA): same as above, but &lt;math&gt;f&lt;/math&gt; is a conjunction of positive literals;

; CCWA (careful CWA): same as GCWA, but a positive clause is only considered if it is composed of positive literals of a given set and (both positive and negative) literals from another set;

; ECWA (extended CWA): similar to CCWA, but &lt;math&gt;f&lt;/math&gt; is an arbitrary formula not containing literals from a given set.

The ECWA and the formalism of [[Circumscription (logic)|circumscription]] coincide on propositional theories. The complexity of query answering (checking whether a formula is entailed by another one under the closed-world assumption) is typically in the second level of the [[polynomial hierarchy]] for general formulae, and ranges from [[P (complexity)|P]] to [[coNP]] for [[Horn clause|Horn formulae]]. Checking whether the original closed-world assumption introduces an inconsistency requires at most a logarithmic number of calls to an [[Oracle machine|NP oracle]]; however, the exact complexity of this problem is not currently known.

==See also==

* [[Open-world assumption]]
* [[Non-monotonic logic]]
* [[Circumscription (logic)]]
* [[Negation as failure]]
* [[Default logic]]
* [[Stable model semantics]]
* [[Unique name assumption]]

==References==
{{Reflist}}
*{{cite journal |last1=Cadoli |first1=Marco |last2=Lenzerini |first2=Maurizio |title=The complexity of propositional closed world reasoning and circumscription |journal=Journal of Computer and System Sciences |date=April 1994 |volume=48 |issue=2 |pages=255&#8211;310 |doi=10.1016/S0022-0000(05)80004-2 |url=http://www.sciencedirect.com/science/article/pii/S0022000005800042 |accessdate=20 February 2013 |issn=0022-0000}}
*{{cite journal |last1=Eiter |first1=Thomas |last2=Gottlob |authorlink2=Georg Gottlob |first2=Georg |title=Propositional circumscription and extended closed-world reasoning are &lt;math&gt;\Pi^p_2&lt;/math&gt;-complete |journal=Theoretical Computer Science |date=June 1993 |volume=114 |issue=2 |pages=231&#8211;245 |doi=10.1016/0304-3975(93)90073-3 |url=http://www.sciencedirect.com/science/article/pii/0304397593900733 |accessdate=20 February 2013 |issn=0304-3975}}
*{{cite journal |last1=Rajasekar |first1=Arcot |last2=Lobo |first2=Jorge |last3=Minker |first3=Jack |authorlink3=Jack Minker |title=Weak Generalized Closed World Assumption |journal=Journal of Automated Reasoning |date=September 1989 |volume=5 |issue=3 |pages=293&#8211;307 |doi=10.1007/BF00248321 |url=http://link.springer.com/article/10.1007/BF00248321 |accessdate=20 February 2013 |publisher=Kluwer Academic Publishers |issn=0168-7433}}
*{{cite journal |last=Lifschitz |first=Vladimir |authorlink=Vladimir Lifschitz |title=Closed-world databases and circumscription |journal=Artificial Intelligence |date=November 1985 |volume=27 |issue=2 |pages=229&#8211;235 |doi=10.1016/0004-3702(85)90055-4 |url=http://www.sciencedirect.com/science/article/pii/0004370285900554 |accessdate=20 February 2013 |issn=0004-3702}}
*{{cite book |last=Reiter |first=Raymond |authorlink=Raymond Reiter |editor1-last=Gallaire |editor1-first=Herv&#233; |editor2-last=Minker |editor2-first=Jack |editor2-link=Jack Minker |title=Logic and Data Bases |year=1978 |publisher=Plenum Press |isbn=9780306400605 |url=http://www.springer.com/computer/security+and+cryptology/book/978-0-306-40060-5 |accessdate=21 February 2013 |chapter=On Closed World Data Bases |pages=119&#8211;140}}
*{{cite journal |last1=Duan |first1=Yucong |last2=Cruz |first2=Christophe |title=Formalizing Semantic of Natural Language through Conceptualization from Existence |journal=International Journal of Innovation, Management and Technology |date=February 2011 |volume=2 |issue=1 |pages=37&#8211;42 |doi=10.7763/IJIMT.2011.V2.100 |url=http://ijimt.org/abstract/100-E00187.htm |accessdate=21 February 2013 |issn=2010-0248}}

==External links==
* https://web.archive.org/web/20090624113015/http://www.betaversion.org:80/~stefano/linotype/news/91/
* [http://owl1-1.googlecode.com/svn-history/r374/trunk/www.webont.org/owled/2005/sub12.pdf Closed World Reasoning in the Semantic Web through Epistemic Operators]
* [http://books.hammerpig.com/the-closed-world-assumption-of-databases.html Excerpt from Reiter's 1978 talk on the closed world assumption]

{{DEFAULTSORT:Closed-world assumption}}
[[Category:Logic programming]]
[[Category:Knowledge representation]]</text>
      <sha1>t2ic0nllkgwvhup9gvcmbcpcnsswq6n</sha1>
    </revision>
  </page>
  <page>
    <title>Deductive classifier</title>
    <ns>0</ns>
    <id>43342432</id>
    <revision>
      <id>730453007</id>
      <parentid>714726433</parentid>
      <timestamp>2016-07-19T03:08:05Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8117" xml:space="preserve">A '''deductive classifier''' is a type of [[artificial intelligence]] [[inference engine]]. It takes as input a set of declarations in a [[frame language]] about a domain such as medical research or molecular biology. For example, the names of [[Class hierarchy|classes, sub-classes]], properties, and restrictions on allowable values.  The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to [[Automated theorem proving|theorem provers]] in that they take as input and produce output via [[First Order Logic]]. Classifiers originated with [[KL-ONE]] [[Frame language]]s. They are increasingly significant now that they form a part in the enabling technology of the [[Semantic Web]]. Modern classifiers leverage the [[Web Ontology Language]]. The models they analyze and generate are called [[Ontologies (computer science)|ontologies]].&lt;ref&gt;{{cite journal|last=Berners-Lee|first=Tim|first2=James|last2=Hendler|first3=Ora|last3=Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34&#8211;43}}&lt;/ref&gt;

== History ==
A classic problem in [[knowledge representation]] for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.&lt;ref&gt;{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}&lt;/ref&gt;

As a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on [[modus ponens]], i.e. IF-THEN rules. [[Rule-based systems]] were the predominate knowledge representation mechanism for virtually all early [[expert systems]]. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.&lt;ref&gt;{{cite book|last=Hayes-Roth|first=Frederick|pages=6&#8211;7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|first2=Donald|last2=Waterman|first3=Douglas|last3=Lenat}}&lt;/ref&gt;

However, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an [[Ontology (computer science)|ontology]]) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The [[Loom (ontology)|LOOM language]] from ISI was heavily influenced by KL-ONE.  LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.&lt;ref&gt;{{cite journal|last1=MacGregor|first1=Robert|title=A Descriptive Classifier for the Predicate Calculus|journal=AAAI - 94 Proceedings|date=1994|url=http://www.aaai.org/Papers/AAAI/1994/AAAI94-033.pdf|accessdate=17 July 2014}}&lt;/ref&gt;

== Implementations ==
[[File:Prot&#233;g&#233; 3.4.3.png|500px|thumbnail|right|Protege Ontology Editor]]
The earliest versions of classifiers were [[Automated theorem proving|logic theorem provers]]. The first classifier to work with a [[Frame language]] was the [[KL-ONE]] classifier.&lt;ref&gt;{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers &amp; Mathematics with Applications | volume = 23 | issue = 2&#8211;5 | pages = 133&#8211;177 | year = 1992 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171&#8211;216 | year = 1985 | pmid =  | pmc = }}&lt;/ref&gt; A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language.&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41&#8211;46}}&lt;/ref&gt; In the Semantic Web the [[Prot&#233;g&#233; (software)|Protege]] tool from [[Stanford University|Stanford]] provides classifiers (also known as reasonsers) as part of the default environment.&lt;ref&gt;{{cite web|title=Protege Wiki: Reasoners that integrate with Protege|url=http://protegewiki.stanford.edu/wiki/Reasoning|publisher=Stanford University|accessdate=19 July 2014}}&lt;/ref&gt;

== External links ==
* [http://owl.man.ac.uk/factplusplus/ Fact++ Reasoner]
* [http://hermit-reasoner.com/ HermiT Reasoner]
* [http://protege.stanford.edu/ Protege Ontology Editor]

==References==
{{Reflist}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Ontology languages]]
[[Category:Classification algorithms]]</text>
      <sha1>37030torfyuzkmliwbbha1ye4nkpkse</sha1>
    </revision>
  </page>
  <page>
    <title>User profile</title>
    <ns>0</ns>
    <id>35773358</id>
    <revision>
      <id>760668334</id>
      <parentid>684921790</parentid>
      <timestamp>2017-01-18T10:42:49Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1479" xml:space="preserve">{{pp-move-indef|small=yes}}
{{pp-semi-indef|small=yes}}
{{for|Wikipedia's guideline on its own user pages|WP:USERPAGE}}

{{unreferenced|date=September 2014}}
[[File:User Profile Info Model.png|thumb|User Profile Info Model]]

A '''user profile''' is a visual display of [[personal data]] associated with a specific [[User (computing)|user]], or a [[customized]] [[desktop environment]]. A profile refers therefore to the explicit digital representation of a person's [[Online identity|identity]]. A user profile can also be considered as the computer representation of a [[user modeling|user model]].

A profile can be used to store the description of the characteristics of person. This information can be exploited by systems taking into account the persons' characteristics and preferences.

[[Profiling (information science)|Profiling]] is the process that refers to construction of a profile via the extraction from a set of data.

User profiles can be found on [[operating system]]s, [[computer program]]s, [[recommender system]]s, or [[Website|dynamic websites]] (such as [[Social network service|online social networking]] sites or [[bulletin board]]s).

==See also==
*[[Online identity]]
*[[Online identity management]]
*[[Personally identifiable information]]
*[[Web mining]]
*[[Internet privacy]]

{{Online social networking}}
{{Social networking}}

[[Category:Identity management]]
[[Category:Knowledge representation]]
[[Category:Software features]]


{{compu-stub}}</text>
      <sha1>9nzmlzbtrl2tuapy9i8l27d62343qku</sha1>
    </revision>
  </page>
  <page>
    <title>Argument map</title>
    <ns>0</ns>
    <id>6190251</id>
    <revision>
      <id>762093638</id>
      <parentid>758094953</parentid>
      <timestamp>2017-01-26T16:27:12Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>/* Key features of an argument map */Typo fixing, replaced: The the &#8594; The using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="42966" xml:space="preserve">[[File:Argument Map.png|thumb|A schematic argument map showing a contention (or conclusion), supporting arguments and objections, and an inference objection.]]
In [[informal logic]] and [[philosophy]], an '''argument map''' or '''argument diagram''' is a visual representation of the structure of an [[argument]]. An argument map typically includes the key components of the argument, traditionally called the ''[[Logical consequence|conclusion]]'' and the ''[[premise]]s'', also called ''[[Main contention|contention]]'' and ''[[Reason (argument)|reason]]s''.&lt;ref&gt;{{harvnb|Freeman|1991|pp=49&#8211;90}}&lt;/ref&gt; Argument maps can also show [[co-premise]]s, [[Objection (argument)|objection]]s, [[counterargument]]s, [[rebuttal]]s, and [[Lemma (logic)|lemma]]s. There are different styles of argument map but they are often functionally equivalent and represent an argument's individual claims and the relationships between them.

Argument maps are commonly used in the context of teaching and applying [[critical thinking]].&lt;ref&gt;For example: {{harvnb|Davies|2012}}; {{harvnb|Facione|2013|p=86}}; {{harvnb|Fisher|2004}}; {{harvnb|Kelley|2014|p=73}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}; {{harvnb|Walton|2013|p=10}}&lt;/ref&gt; The purpose of mapping is to uncover the logical structure of arguments, identify unstated assumptions, evaluate the support an argument offers for a conclusion, and aid understanding of debates. Argument maps are often designed to support deliberation of issues, ideas and arguments in [[wicked problem]]s.&lt;ref&gt;For example: {{harvnb|Culmsee|Awati|2013}}; {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Metcalfe|Sastrowardoyo|2013}}; Ricky Ohl, [//books.google.com/books?id=loa4BAAAQBAJ&amp;pg=PA360 "Computer supported argument visualisation: modelling in consultative democracy around wicked problems"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=361&#8211;380}}&lt;/ref&gt;

An argument map is not to be confused with a [[concept map]] or a [[mind map]], which are less strict in relating claims.

==Key features of an argument map==
A number of different kinds of argument map have been proposed but the most common, which Chris Reed and Glenn Rowe called the ''standard diagram'',&lt;ref name="ReedRowe64"&gt;{{harvnb|Reed|Rowe|2007|p=64}}&lt;/ref&gt; consists of a [[tree structure]] with each of the reasons leading to the conclusion. There is no consensus as to whether the conclusion should be at the top of the tree with the reasons leading up to it or whether it should be at the bottom with the reasons leading down to it.&lt;ref name="ReedRowe64"/&gt; Another variation diagrams an argument from left to right.&lt;ref&gt;For example: {{harvnb|Walton|2013|pp=18&#8211;20}}&lt;/ref&gt;

According to [[Doug Walton]] and colleagues, an argument map has two basic components: "One component is a set of circled numbers arrayed as points. Each number represents a proposition (premise or conclusion) in the argument being diagrammed. The other component is a set of lines or arrows joining the points. Each line (arrow) represents an inference. The whole network of points and lines represents a kind of overview of the reasoning in the given argument..."&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|p=2}}&lt;/ref&gt; With the introduction of software for producing argument maps, it has become common for argument maps to consist of boxes containing the actual propositions rather than numbers referencing those propositions.

There is disagreement on the terminology to be used when describing argument maps,&lt;ref&gt;{{harvnb|Freeman|1991|pp=49&#8211;90}}; {{harvnb|Reed|Rowe|2007}}&lt;/ref&gt; but the ''standard diagram'' contains the following structures:

'''Dependent premises''' or '''co-premises''', where at least one of the joined premises requires another premise before it can give support to the conclusion: An argument with this structure has been called a ''linked'' argument.&lt;ref&gt;{{harvnb|Harrell|2010|p=19}}&lt;/ref&gt;

[[File:Dependent premises.jpg|thumb|centre|100px|Statements 1 and 2 are dependent premises or co-premises]]

'''Independent premises''', where the premise can support the conclusion on its own: Although independent premises may jointly make the conclusion more convincing, this is to be distinguished from situations where a premise gives no support unless it is joined to another premise. Where several premises or groups of premises lead to a final conclusion the argument might be described as ''convergent''. This is distinguished from a ''divergent'' argument where a single premise might be used to support two separate conclusions.&lt;ref&gt;{{harvnb|Freeman|1991|pp=91&#8211;110}}; {{harvnb|Harrell|2010|p=20}}&lt;/ref&gt;

[[File:Independent premises.jpg|thumb|centre|150px|Statements 2, 3, 4 are independent premises]]

'''Intermediate conclusions''' or '''sub-conclusions''', where a claim is supported by another claim that is used in turn to support some further claim, i.e. the final conclusion or another intermediate conclusion: In the following diagram, statement '''4''' is an intermediate conclusion in that it is a conclusion in relation to statement '''5''' but is a premise in relation to the final conclusion, i.e. statement '''1'''. An argument with this structure is sometimes called a ''complex'' argument. If there is a single chain of claims containing at least one intermediate conclusion, the argument is sometimes described as a ''serial'' argument or a ''chain'' argument.&lt;ref&gt;{{harvnb|Beardsley|1950|pp=18&#8211;19}}; {{harvnb|Reed|Walton|Macagno|2007|pp=3&#8211;8}}; {{harvnb|Harrell|2010|pp=19&#8211;21}}&lt;/ref&gt;

[[File:Intermediate conclusion.jpg|thumb|centre|150px|Statement 4 is an intermediate conclusion or sub-conclusion]]

Each of these structures can be represented by the equivalent "box and line" approach to argument maps. In the following diagram, the ''contention'' is shown at the top, and the boxes linked to it represent supporting ''reasons'', which comprise one or more ''premises''. The green arrow indicates that the two ''reasons'' support the ''contention'':

[[File:A box and line diagram.png|thumb|center|A box and line diagram]]

Argument maps can also represent counterarguments. In the following diagram, the two ''objections'' weaken the ''contention'', while the ''reasons'' support the ''premise'' of the objection:

[[File:A sample argument using objections.png|thumb|center|A sample argument using objections]]

==Representing an argument as an argument map==
A written text can be transformed into an argument map by following a sequence of steps. [[Monroe Beardsley]]'s 1950 book ''Practical Logic'' recommended the following procedure:&lt;ref name="Beardsley"&gt;{{harvnb|Beardsley|1950}}&lt;/ref&gt;
#Separate statements by brackets and number them.
#Put circles around the logical indicators.
#Supply, in parenthesis, any logical indicators that are left out.
#Set out the statements in a diagram in which arrows show the relationships between statements.

[[File:Diagram using Beardsley's procedure.jpg|thumb|right|100px|A diagram of the example from Beardsley's ''Practical Logic'']]

Beardsley gave the first example of a text being analysed in this way:

:Though &lt;span style="color:red;"&gt;&#9312; [&lt;/span&gt;people who talk about the "social significance" of the arts don&#8217;t like to admit it&lt;span style="color:red;"&gt;]&lt;/span&gt;, &lt;span style="color:red;"&gt;&#9313; [&lt;/span&gt;music and painting are bound to suffer when they are turned into mere vehicles for propaganda&lt;span style="color:red;"&gt;]&lt;/span&gt;. &lt;span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;"&gt;For&lt;/span&gt; &lt;span style="color:red;"&gt;&#9314; [&lt;/span&gt;propaganda appeals to the crudest and most vulgar feelings&lt;span style="color:red;"&gt;]&lt;/span&gt;: &lt;span style="color:red;"&gt;(for)&lt;/span&gt; &lt;span style="color:red;"&gt;&#9315; [&lt;/span&gt;look at the academic monstrosities produced by the official Nazi painters&lt;span style="color:red;"&gt;]&lt;/span&gt;. What is more important, &lt;span style="color:red;"&gt;&#9316; [&lt;/span&gt;art must be an end in itself for the artist&lt;span style="color:red;"&gt;]&lt;/span&gt;, &lt;span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;"&gt;because&lt;/span&gt; &lt;span style="color:red;"&gt;&#9317; [&lt;/span&gt;the artist can do the best work only in an atmosphere of complete freedom&lt;span style="color:red;"&gt;]&lt;/span&gt;.

Beardsley said that the conclusion in this example is statement &#9313;. Statement &#9315; needs to be rewritten as a declarative sentence, e.g. "Academic monstrosities [were] produced by the official Nazi painters." Statement &#9312; points out that the conclusion isn't accepted by everyone, but statement &#9312; is omitted from the diagram because it doesn't support the conclusion. Beardsley said that the logical relation between statement &#9314; and statement &#9315; is unclear, but he proposed to diagram statement &#9315; as supporting statement &#9314;.

[[File:Using Harrell's procedure.jpg|thumb|right|200px|A box and line diagram of Beardsley's example, produced using Harrell's procedure]]

More recently, philosophy professor Maralee Harrell recommended the following procedure:&lt;ref&gt;{{harvnb|Harrell|2010|p=28}}&lt;/ref&gt;
#Identify all the claims being made by the author.
#Rewrite them as independent statements, eliminating non-essential words.
#Identify which statements are premises, sub-conclusions, and the main conclusion.
#Provide missing, implied conclusions and implied premises. (This is optional depending on the purpose of the argument map.)
#Put the statements into boxes and draw a line between any boxes that are linked.
#Indicate support from premise(s) to (sub)conclusion with arrows.

Argument maps are useful not only for representing and analyzing existing writings, but also for thinking through issues as part of a [[Problem structuring methods|problem-structuring process]] or [[writing process]]. The use of such argument analysis for thinking through issues has been called "reflective argumentation".&lt;ref&gt;For example: {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Hoffmann|2015}}&lt;/ref&gt;

==History==

===The philosophical origins and tradition of argument mapping===
[[File:Whatley.png|thumb|From Whately's Elements of Logic p467, 1852 edition]]
In the ''Elements of Logic'', which was published in 1826 and issued in many subsequent editions,&lt;ref&gt;{{harvnb|Whately|1834}} (first published 1826)&lt;/ref&gt; Archbishop [[Richard Whately]] gave probably the first form of an argument map, introducing it with the suggestion that "many students probably will find it a very clear and convenient mode of exhibiting the logical analysis of the course of argument, to draw it out in the form of a Tree, or Logical Division".

However, the technique did not become widely used, possibly because for complex arguments, it involved much writing and rewriting of the premises.

[[File:Wigmore chart.png|thumb|Wigmore evidence chart, from 1905]]
Legal philosopher and theorist [[John Henry Wigmore]] produced maps of legal arguments using numbered premises in the early 20th century,&lt;ref&gt;{{harvnb|Wigmore|1913}}&lt;/ref&gt; based in part on the ideas of 19th century philosopher [[Henry Sidgwick]] who used lines to indicate relations between terms.&lt;ref&gt;{{harvnb|Goodwin|2000}}&lt;/ref&gt;

===Anglophone argument diagramming in the 20th century===
Dealing with the failure of [[Formal system|formal]] reduction of informal argumentation, English speaking [[argumentation theory]] developed diagrammatic approaches to informal reasoning over a period of fifty years.

[[Monroe Beardsley]] proposed a form of argument diagram in 1950.&lt;ref name="Beardsley"/&gt; His method of marking up an argument and representing its components with linked numbers became a standard and is still widely used. He also introduced terminology that is still current describing ''convergent'', ''divergent'' and ''serial'' arguments.

[[File:Toulmindiag.png|thumb|A Toulmin argument diagram, redrawn from his 1959 ''Uses of Argument'']]
[[File:Toulmingeneral.png|thumb|A generalised Toulmin diagram]]
[[Stephen Toulmin]], in his groundbreaking and influential 1958 book ''The Uses of Argument'',&lt;ref&gt;{{harvnb|Toulmin|2003}} (first published 1958)&lt;/ref&gt; identified several elements to an argument which have been generalized. The Toulmin diagram is widely used in educational critical teaching.&lt;ref name="Simon06"&gt;{{harvnb|Simon|Erduran|Osborne|2006}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|B&#246;ttcher|Meisert|2011}}; {{harvnb|Macagno|Konstantinidou|2013}}&lt;/ref&gt; Whilst Toulmin eventually had a significant impact on the development of [[informal logic]] he had little initial impact and the Beardsley approach to diagramming arguments along with its later developments became the standard approach in this field. Toulmin introduced something that was missing from Beardsley's approach. In Beardsley, "arrows link reasons and conclusions (but) no support is given to the implication itself between them. There is no theory, in other words, of inference distinguished from logical deduction, the passage is always deemed not controversial and not subject to support and evaluation".&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|p=8}}&lt;/ref&gt; Toulmin introduced the concept of ''warrant'' which "can be considered as representing the reasons behind the inference, the backing that authorizes the link".&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|p=9}}&lt;/ref&gt;

Beardsley's approach was refined by Stephen N. Thomas, whose 1973 book ''Practical Reasoning In Natural Language''&lt;ref&gt;{{harvnb|Thomas|1997}} (first published 1973)&lt;/ref&gt; introduced the term ''linked'' to describe arguments where the premises necessarily worked together to support the conclusion.&lt;ref name="SnoeckHenkemans"&gt;{{harvnb|Snoeck Henkemans|2000|p=453}}&lt;/ref&gt; However, the actual distinction between dependent and independent premises had been made prior to this.&lt;ref name="SnoeckHenkemans"/&gt; The introduction of the linked structure made it possible for argument maps to represent missing or "hidden" premises. In addition, Thomas suggested showing reasons both ''for'' and ''against'' a conclusion with the reasons ''against'' being represented by dotted arrows. Thomas introduced the term ''argument diagram'' and defined ''basic reasons'' as those that were not supported by any others in the argument and the ''final conclusion'' as that which was not used to support any further conclusion.

[[File:Scrivendiag.png|thumb|Scriven's argument diagram. The explicit premise 1 is conjoined with additional unstated premises a and b to imply 2.]]
[[Michael Scriven]] further developed the Beardsley-Thomas approach in his 1976 book ''Reasoning''.&lt;ref&gt;{{harvnb|Scriven|1976}}&lt;/ref&gt; Whereas Beardsley had said "At first, write out the statements...after a little practice, refer to the statements by number alone"&lt;ref&gt;{{harvnb|Beardsley|1950|p=21}}&lt;/ref&gt; Scriven advocated clarifying the meaning of the statements, listing them and then using a tree diagram with numbers to display the structure. Missing premises (unstated assumptions) were to be included and indicated with an alphabetical letter instead of a number to mark them off from the explicit statements. Scriven introduced counterarguments in his diagrams, which Toulmin had defined as rebuttal.&lt;ref&gt;{{harvnb|Reed|Walton|Macagno|2007|pp=10&#8211;11}}&lt;/ref&gt; This also enabled the diagramming of "balance of consideration" arguments.&lt;ref&gt;{{harvnb|van Eemeren|Grootendorst|Snoeck Henkemans|Blair|1996|p=175}}&lt;/ref&gt;

In the 1990s, [[Tim van Gelder]] and colleagues developed a series of computer software applications that permitted the premises to be fully stated and edited in the diagram, rather than in a legend.&lt;ref&gt;{{harvnb|van Gelder|2007}}&lt;/ref&gt; Van Gelder's first program, Reason!Able, was superseded by two subsequent programs, bCisive and Rationale.&lt;ref&gt;{{harvnb|Berg|van Gelder|Patterson|Teppema|2009}}&lt;/ref&gt;

Throughout the 1990s and 2000s, many other software applications were developed for argument visualization. By 2013, more than 60 such software systems existed.&lt;ref&gt;{{harvnb|Walton|2013|p=11}}&lt;/ref&gt; One of the differences between these software systems is whether collaboration is supported.&lt;ref name="Scheuer10"&gt;{{harvnb|Scheuer|Loll|Pinkwart|McLaren|2010}}&lt;/ref&gt; Single-user argumentation systems include Convince Me, iLogos, LARGO, Athena, [[Araucaria (software)|Araucaria]], and Carneades; small group argumentation systems include Digalo, QuestMap, [[Compendium (software)|Compendium]], Belvedere, and AcademicTalk; community argumentation systems include [[Debategraph]] and [[Collaboratorium]].&lt;ref name="Scheuer10"/&gt; For more software examples, see: {{section link||External links}}.

In 1998 a series of large-scale argument maps released by [[Robert E. Horn]] stimulated widespread interest in argument mapping.&lt;ref&gt;{{harvnb|Holmes|1999}}; {{harvnb|Horn|1998}} and Robert E. Horn, [http://www.stanford.edu/~rhorn/a/topic/arg/artclCmptrSpArgmttn.pdf "Infrastructure for navigating interdisciplinary debates: critical decisions for representing argumentation"], in {{harvnb|Kirschner|Buckingham Shum|Carr|2003|pp=165&#8211;184}}&lt;/ref&gt;

==Applications==
Argument maps have been applied in many areas, but foremost in educational, academic and business settings, including [[design rationale]].&lt;ref name="Applications"&gt;{{harvnb|Kirschner|Buckingham Shum|Carr|2003}}; {{harvnb|Okada|Buckingham Shum|Sherborne|2014}}&lt;/ref&gt; Argument maps are also used in [[forensic science]],&lt;ref&gt;For example: {{harvnb|Bex|2011}}&lt;/ref&gt; [[law]], and [[artificial intelligence]].&lt;ref&gt;For example: {{harvnb|Verheij|2005}}; {{harvnb|Reed|Walton|Macagno|2007}}; {{harvnb|Walton|2013}}&lt;/ref&gt; It has also been proposed that argument mapping has a great potential to improve how we understand and execute democracy, in reference to the ongoing evolution of [[e-democracy]].&lt;ref&gt;{{harvnb|Hilbert|2009}}&lt;/ref&gt;

===Difficulties with the philosophical tradition===
It has traditionally been hard to separate teaching critical thinking from the philosophical tradition of teaching [[logic]] and method, and most critical thinking textbooks have been written by philosophers. [[Informal logic]] textbooks are replete with philosophical examples, but it is unclear whether the approach in such textbooks transfers to non-philosophy students.&lt;ref name="Simon06" /&gt; There appears to be little statistical effect after such classes. Argument mapping, however, has a measurable effect according to many studies.&lt;ref&gt;{{harvnb|Twardy|2004}}; {{harvnb|&#193;lvarez Ortiz|2007}}; {{harvnb|Harrell|2008}}; Yanna Rider and Neil Thomason, [//books.google.com/books?id=loa4BAAAQBAJ&amp;pg=PA112 "Cognitive and pedagogical benefits of argument mapping: LAMP guides the way to better thinking"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=113&#8211;134}}; {{harvnb|Dwyer|2011}}; {{harvnb|Davies|2012}}&lt;/ref&gt; For example, instruction in argument mapping has been shown to improve the critical thinking skills of business students.&lt;ref&gt;{{harvnb|Carrington|Chen|Davies|Kaur|2011}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}&lt;/ref&gt;

===Evidence that argument mapping improves critical thinking ability===
There is empirical evidence that the skills developed in argument-mapping-based critical thinking courses substantially transfer to critical thinking done without argument maps. Alvarez's meta-analysis found that such critical thinking courses produced gains of around 0.70 SD, about twice as much as standard critical-thinking courses.&lt;ref&gt;{{harvnb|&#193;lvarez Ortiz|2007|pp=69&#8211;70 ''et seq''}}&lt;/ref&gt; The tests used in the reviewed studies were standard critical-thinking tests.

===How argument mapping helps with critical thinking===
The use of argument mapping has occurred within a number of disciplines, such as philosophy, management reporting, military and intelligence analysis, and public debates.&lt;ref name="Applications"/&gt;

''Logical structure'': Argument maps display an argument's logical structure more clearly than does the standard linear way of presenting arguments.

''Critical thinking concepts'': In learning to argument map, students master such key critical thinking concepts as "reason", "objection", "premise", "conclusion", "inference", "rebuttal", "unstated assumption", "co-premise", "strength of evidence", "logical structure", "independent evidence", etc. Mastering such concepts is not just a matter of memorizing their definitions or even being able to apply them correctly; it is also understanding why the distinctions these words mark are important and using that understanding to guide one's reasoning.

''Visualization'': Humans are highly visual and argument mapping may provide students with a basic set of visual schemas with which to understand argument structures.

''More careful reading and listening'': Learning to argument map teaches people to read and listen more carefully, and highlights for them the key questions "What is the logical structure of this argument?" and "How does this sentence fit into the larger structure?" In-depth cognitive processing is thus more likely.

''More careful writing and speaking'': Argument mapping helps people to state their reasoning and evidence more precisely, because the reasoning and evidence must fit explicitly into the map's logical structure.

''Literal and intended meaning'': Often, many statements in an argument do not precisely assert what the author meant. Learning to argument map enhances the complex skill of distinguishing literal from intended meaning.

''Externalization'': Writing something down and reviewing what one has written often helps reveal gaps and clarify one's thinking. Because the logical structure of argument maps is clearer than that of linear prose, the benefits of mapping will exceed those of ordinary writing.

''Anticipating replies'': Important to critical thinking is anticipating objections and considering the plausibility of different rebuttals. Mapping develops this anticipation skill, and so improves analysis.

==Standards==

===Argument Interchange Format===
The Argument Interchange Format, AIF, is an international effort to develop a representational mechanism for exchanging argument resources between research groups, tools, and domains using a semantically rich language.&lt;ref&gt;See the [http://www.arg.dundee.ac.uk/people/chris/publications/2006/aif_final.pdf AIF original draft description] (2006) and the [http://www.argdf.org/source/ArgDF_Ontology.rdfs full AIF-RDF ontology specifications] in [[RDFS]] format.&lt;/ref&gt; AIF-RDF is the extended ontology represented in the [[Resource Description Framework]] Schema (RDFS) semantic language. Though AIF is still something of a moving target, it is settling down.&lt;ref&gt;{{harvnb|Bex|Modgil|Prakken|Reed|2013}}&lt;/ref&gt;

===Legal Knowledge Interchange Format===
The Legal Knowledge Interchange Format (LKIF),&lt;ref&gt;{{harvnb|Boer|Winkels|Vitali|2008}}&lt;/ref&gt; developed in the European ESTRELLA project,&lt;ref&gt;{{cite web |title=Estrella project website |url=http://www.estrellaproject.org/ |website=estrellaproject.org |archiveurl=https://web.archive.org/web/20160212103522/http://www.estrellaproject.org/ |archivedate=2016-02-12 |accessdate=2016-02-24}}&lt;/ref&gt; is an XML schema for rules and arguments, designed with the goal of becoming a standard for representing and interchanging policy, legislation and cases, including their justificatory arguments, in the legal domain. LKIF builds on and uses the [[Web Ontology Language]] (OWL) for representing concepts and includes a reusable basic ontology of legal concepts.

== See also ==
{{Commons category|Argument maps}}
* [[Flow (policy debate)]]
* [[Informal fallacy]]
* [[Information graphics]]
* [[Natural deduction]], a logical system with argument map-like notation

== Notes ==
{{Reflist|colwidth=20em}}

== References ==
* {{cite thesis |type=M.A. thesis |last=&#193;lvarez Ortiz |first=Claudia Mar&#237;a |title=Does philosophy improve critical thinking skills? |url=http://images.austhink.com/pdf/Claudia-Alvarez-thesis.pdf |year=2007 |publisher=Department of Philosophy, [[University of Melbourne]] |oclc=271475715 |ref=harv}}
* {{cite book |last=Beardsley |first=Monroe C. |authorlink=Monroe Beardsley |date=1950 |title=Practical logic |location=New York |publisher=Prentice-Hall |oclc=4318971 |ref=harv}}
* {{cite book |last1=Berg |first1=Timo ter |last2=van Gelder |first2=Tim |authorlink2=Tim van Gelder |last3=Patterson |first3=Fiona |last4=Teppema |first4=Sytske |year=2009 |title=Critical thinking: reasoning and communicating with Rationale |location=Amsterdam |publisher=Pearson Education Benelux |isbn=9043018015 |oclc=301884530 |ref=harv}}
* {{cite book |last=Bex |first=Floris J. |date=2011 |title=Arguments, stories and criminal evidence: a formal hybrid theory |series=Law and philosophy library |volume=92 |location=Dordrecht; New York |publisher=Springer |isbn=9789400701397 |oclc=663950184 |doi=10.1007/978-94-007-0140-3 |ref=harv}}
* {{cite journal |last1=Bex |first1=Floris J. |last2=Modgil |first2=Sanjay |last3=Prakken |first3=Henry |last4=Reed |first4=Chris |title=On logical specifications of the Argument Interchange Format |journal=[[Journal of Logic and Computation]] |volume=23 |issue=5 |pages=951&#8211;989 |year=2013 |doi=10.1093/logcom/exs033 |url=http://www.arg.dundee.ac.uk/people/chris/publications/2013/AIF-ASPIC-JLC-Final.pdf |ref=harv}}
* {{cite book |last1=Boer |first1=Alexander |last2=Winkels |first2=Radboud |last3=Vitali |first3=Fabio |date=2008 |chapter=MetaLex XML and the Legal Knowledge Interchange Format |editor1-last=Casanovas |editor1-first=Pompeu |editor2-last=Sartor |editor2-first=Giovanni |editor3-last=Casellas |editor3-first=N&#250;ria |editor4-last=Rubino |editor4-first=Rossella |title=Computable models of the law: languages, dialogues, games, ontologies |series=Lecture notes in computer science |volume=4884 |location=Berlin; New York |publisher=Springer |pages=21&#8211;41 |isbn=9783540855682 |oclc=244765580 |doi=10.1007/978-3-540-85569-9_2 |chapterurl=https://www.researchgate.net/profile/Radboud_Winkels/publication/225708550_MetaLex_XML_and_the_Legal_Knowledge_Interchange_Format/links/02bfe510239cc79c8d000000.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=B&#246;ttcher |first1=Florian |last2=Meisert |first2=Anke |title=Argumentation in science education: a model-based framework |journal=Science &amp; Education |volume=20 |issue=2 |pages=103&#8211;140 |date=February 2011 |doi=10.1007/s11191-010-9304-5 |ref=harv}}
* {{cite journal |last1=Carrington |first1=Michal |last2=Chen |first2=Richard |last3=Davies |first3=Martin |last4=Kaur |first4=Jagjit |last5=Neville |first5=Benjamin |title=The effectiveness of a single intervention of computer&#8208;aided argument mapping in a marketing and a financial accounting subject |journal=Higher Education Research &amp; Development |volume=30 |issue=3 |pages=387&#8211;403 |date=June 2011 |doi=10.1080/07294360.2011.559197 |url=https://www.researchgate.net/profile/Martin_Davies/publication/232947753_The_effectiveness_of_a_single_intervention_of_computeraided_argument_mapping_in_a_marketing_and_a_financial_accounting_subject/links/0deec51d94fec63bdc000000.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite book |last1=Culmsee |first1=Paul |last2=Awati |first2=Kailash |date=2013 |origyear=2011 |chapter=Chapter 7: Visualising reasoning, and Chapter 8: Argumentation-based rationale |title=The heretic's guide to best practices: the reality of managing complex problems in organisations |location=Bloomington, IN |publisher=iUniverse, Inc. |pages=153&#8211;211 |isbn=9781462058549 |oclc=767703320 |chapterurl=https://books.google.com/books?id=Gb2uuT1zrAAC&amp;pg=PA153 |ref=harv}}
* {{cite journal |last=Davies |first=Martin |title=Computer-aided argument mapping as a tool for teaching critical thinking |journal=[[International Journal of Learning and Media]] |volume=4 |issue=3-4 |pages=79&#8211;84 |date=Summer 2012 |doi=10.1162/IJLM_a_00106 |url=http://www.mitpressjournals.org/doi/full/10.1162/IJLM_a_00106 |ref=harv}}
* {{cite thesis |type=Ph.D. thesis |last=Dwyer |first=Christopher Peter |title=The evaluation of argument mapping as a learning tool |url=http://aran.library.nuigalway.ie/xmlui/bitstream/handle/10379/2617/C.Dwyer-PhD%20Thesis%20Psychology.pdf |year=2011 |publisher=School of Psychology, National University of Ireland, Galway |oclc=812818648 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Grootendorst |first2=Rob |authorlink2=Rob Grootendorst |last3=Snoeck Henkemans |first3=A. Francisca |last4=Blair |first4=J. Anthony |last5=Johnson |first5=Ralph H. |authorlink5=Ralph Johnson (philosopher) |last6=Krabbe |first6=Erik C. W. |last7=Plantin |first7=Christian |last8=Walton |first8=Douglas N. |authorlink8=Doug Walton |last9=Willard |first9=Charles A. |authorlink9=Charles Arthur Willard |last10=Woods |first10=John |authorlink10=John Woods (logician) |date=1996 |title=Fundamentals of argumentation theory: a handbook of historical backgrounds and contemporary developments |location=Mahwah, NJ |publisher=[[Lawrence Erlbaum Associates]] |isbn=0805818618 |oclc=33970847 |doi=10.4324/9780203811306 |ref=harv}}
* {{cite book |last=Facione |first=Peter A. |title=THINK critically |year=2013 |origyear=2011 |edition=2nd |location=Boston |publisher=Pearson |isbn=0205490980 |oclc=770694200 |ref=harv}}
* {{cite book |last=Fisher |first=Alec |title=The logic of real arguments |url=https://books.google.com/books?id=Q2a_RtDZUGgC |year=2004 |origyear=1988 |edition=2nd |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521654815 |oclc=54400059 |doi=10.1017/CBO9780511818455 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Freeman |first=James B. |title=Dialectics and the macrostructure of arguments: a theory of argument structure |url=https://books.google.com/books?id=ScvV9riTOGsC |year=1991 |location=Berlin; New York |publisher=Foris Publications |isbn=3110133903 |oclc=24429943 |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last=van Gelder |first=Tim |authorlink=Tim van Gelder |title=The rationale for Rationale |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=23&#8211;42 |year=2007 |doi=10.1093/lpr/mgm032 |url=http://sites.google.com/site/timvangelder/publications-1/therationaleforrationale/TheRationaleforRationale.pdf |ref=harv}}
* {{cite journal |last=Goodwin |first=Jean |title=Wigmore's chart method |journal=Informal Logic |volume=20 |issue=3 |pages=223&#8211;243 |year=2000 |url=http://windsor.scholarsportal.info/ojs/leddy/index.php/informal_logic/article/view/2278 |ref=harv}}
* {{cite journal |last=Harrell |first=Maralee |title=No computer program required: even pencil-and-paper argument mapping improves critical-thinking skills |journal=[[Teaching Philosophy]] |volume=31 |issue=4 |pages=351&#8211;374 |date=December 2008 |doi=10.5840/teachphil200831437 |url=http://www.hss.cmu.edu/philosophy/harrell/HarrellTeachingPhilosophy2008.pdf |ref=harv}}
* {{cite web |last=Harrell |first=Maralee |title=Creating argument diagrams |date=August 2010 |url=http://www.academia.edu/772321/Creating_Argument_Diagrams |website=[[academia.edu]] |ref=harv}}
* {{cite journal |last=Hilbert |first=Martin |title=The maturing concept of e-democracy: from e-voting and online consultations to democratic value out of jumbled online chatter |journal=[[Journal of Information Technology and Politics]] |date=April 2009 |volume=6 |issue=2 |pages=87&#8211;110 |doi=10.1080/19331680802715242 |url=http://www.martinhilbert.net/e-democracyHilbertJITP.pdf |ref=harv}}
* {{cite journal |last=Hoffmann |first=Michael H. G. |date=November 2015 |title=Reflective argumentation: a cognitive function of arguing |journal=Argumentation |doi=10.1007/s10503-015-9388-9 |ref=harv}}
* {{cite journal |last1=Hoffmann |first1=Michael H. G. |last2=Borenstein |first2=Jason |date=February 2013 |title=Understanding ill-structured engineering ethics problems through a collaborative learning and argument visualization approach |journal=Science and Engineering Ethics |volume=20 |issue=1 |pages=261&#8211;276 |doi=10.1007/s11948-013-9430-y |pmid=23420467 |url=http://works.bepress.com/michael_hoffmann/39/ |ref=harv}}
* {{cite news |last=Holmes |first=Bob |title=Beyond words |url=http://www.newscientist.com/article/mg16321944.700-beyond-words.html |date=10 July 1999 |newspaper=New Scientist |issue=2194 |archiveurl=https://web.archive.org/web/20080928062226/http://www.newscientist.com/article/mg16321944.700-beyond-words.html |archivedate=28 September 2008 |ref=harv}}
* {{cite book |last=Horn |first=Robert E. |authorlink=Robert E. Horn |title=Visual language: global communication for the 21st century |year=1998 |location=Bainbridge Island, WA |publisher=MacroVU, Inc. |isbn=189263709X |oclc=41138655 |ref=harv}}
* {{cite book |last=Kelley |first=David |authorlink=David Kelley |title=The art of reasoning: an introduction to logic and critical thinking |year=2014 |origyear=1988 |edition=4th |location=New York |publisher=W. W. Norton &amp; Company |isbn=0393930785 |oclc=849801096 |ref=harv}}
* {{cite book |editor1-last=Kirschner |editor1-first=Paul Arthur |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Carr |editor3-first=Chad S |title=Visualizing argumentation: software tools for collaborative and educational sense-making |year=2003 |series=Computer supported cooperative work |location=New York |publisher=Springer |isbn=1852336641 |oclc=50676911 |doi=10.1007/978-1-4471-0037-9 |url=https://books.google.com/books?id=dNijwv-my_kC |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Kunsch |first1=David W. |last2=Schnarr |first2=Karin |last3=van Tyle |first3=Russell |title=The use of argument mapping to enhance critical thinking skills in business education |journal=Journal of Education for Business |volume=89 |issue=8 |pages=403&#8211;410 |date=November 2014 |doi=10.1080/08832323.2014.925416 |url= |ref=harv}}
* {{cite journal |last1=Macagno |first1=Fabrizio |last2=Konstantinidou |first2=Aikaterini |title=What students' arguments can tell us: using argumentation schemes in science education |journal=Argumentation |volume=27 |issue=3 |pages=225&#8211;243 |date=August 2013 |doi=10.1007/s10503-012-9284-5 |url=http://ssrn.com/abstract=2185945 |ref=harv}}
* {{cite journal |last1=Metcalfe |first1=Mike |last2=Sastrowardoyo |first2=Saras |date=November 2013 |title=Complex project conceptualisation and argument mapping |journal=International Journal of Project Management |volume=31 |issue=8 |pages=1129&#8211;1138 |doi=10.1016/j.ijproman.2013.01.004 |ref=harv}}
* {{cite book |editor1-last=Okada |editor1-first=Alexandra |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Sherborne |editor3-first=Tony |year=2014 |origyear=2008 |title=Knowledge cartography: software tools and mapping techniques |edition=2nd |series=Advanced information and knowledge processing |location=New York |publisher=Springer |isbn=9781447164692 |oclc=890438015 |doi=10.1007/978-1-4471-6470-8 |url=https://books.google.com/books?id=loa4BAAAQBAJ |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Reed |first1=Chris |last2=Rowe |first2=Glenn |title=A pluralist approach to argument diagramming |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=59&#8211;85 |year=2007 |doi=10.1093/lpr/mgm030 |url=http://lpr.oxfordjournals.org/content/6/1-4/59.full.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Reed |first1=Chris |last2=Walton |first2=Douglas |authorlink2=Doug Walton |last3=Macagno |first3=Fabrizio |title=Argument diagramming in logic, law and artificial intelligence |journal=The Knowledge Engineering Review |volume=22 |issue=1 |pages=1&#8211;22 |date=March 2007 |doi=10.1017/S0269888907001051 |url=https://www.academia.edu/2992281/Argument_Diagramming_in_Logic_Artificial_Intelligence_and_Law |ref=harv}}
* {{cite journal |last1=Scheuer |first1=Oliver |last2=Loll |first2=Frank |last3=Pinkwart |first3=Niels |last4=McLaren |first4=Bruce M. |title=Computer-supported argumentation: a review of the state of the art |journal=International Journal of Computer-Supported Collaborative Learning |volume=5 |issue=1 |pages=43&#8211;102 |year=2010 |doi=10.1007/s11412-009-9080-x |url=http://www.oliver-scheuer.info/publications/ScheuerEtAl-EdArgSystemsReview-IJCSCL-2010.pdf |ref=harv}}
* {{cite book |last=Scriven |first=Michael |authorlink=Michael Scriven |title=Reasoning |year=1976 |location=New York |publisher=McGraw-Hill |isbn=0070558825 |oclc=2800373 |ref=harv}}
* {{cite journal |last1=Simon |first1=Shirley |last2=Erduran |first2=Sibel |last3=Osborne |first3=Jonathan |title=Learning to teach argumentation: research and development in the science classroom |journal=International Journal of Science Education |volume=28 |issue=2-3 |pages=235&#8211;260 |year=2006 |doi=10.1080/09500690500336957 |url=http://cset.stanford.edu/sites/default/files/files/documents/publications/Simon-Learning%20to%20Teach%20Argumentation.pdf |ref=harv}}
* {{cite journal |last=Snoeck Henkemans |first=A. Francisca |date=November 2000 |title=State-of-the-art: the structure of argumentation |journal=Argumentation |volume=14 |issue=4 |pages=447&#8211;473 |doi=10.1023/A:1007800305762 |ref=harv}}
* {{cite book |last=Thomas |first=Stephen N. |date=1997 |origyear=1973 |title=Practical reasoning in natural language |edition=4th |location=Upper Saddle River, NJ |publisher=[[Prentice-Hall]] |isbn=0136782698 |oclc=34745923 |ref=harv}}
* {{cite book |last=Toulmin |first=Stephen E. |authorlink=Stephen Toulmin |title=The uses of argument |url=https://books.google.com/books?id=8UYgegaB1S0C |year=2003 |origyear=1958 |edition=Updated |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521534836 |oclc=57253830 |doi=10.1017/CBO9780511840005 |accessdate=2016-02-24 }}
* {{cite journal |last=Twardy |first=Charles R. |title=Argument maps improve critical thinking |journal=[[Teaching Philosophy]] |volume=27 |issue=2 |pages=95&#8211;116 |date=June 2004 |doi=10.5840/teachphil200427213 |url=http://cogprints.org/3008/1/reasonpaper.pdf |ref=harv}}
* {{cite book |last=Verheij |first=Bart |title=Virtual arguments: on the design of argument assistants for lawyers and other arguers |year=2005 |location=The Hague |publisher=T.M.C. Asser Press |series=Information technology &amp; law series |volume=6 |isbn=9789067041904 |oclc=59617214 |ref=harv}}
* {{cite book |last=Walton |first=Douglas N. |authorlink=Doug Walton |title=Methods of argumentation |url=https://books.google.com/books?id=vaU0AAAAQBAJ |year=2013 |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=1107677335 |oclc=830523850 |doi=10.1017/CBO9781139600187 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Whately |first=Richard |authorlink=Richard Whately |title=Elements of logic: comprising the substance of the article in the Encyclop&#230;dia metropolitana: with additions, &amp;c. |url=https://books.google.com/books?id=5mgAAAAAMAAJ |year=1834 |origyear=1826 |edition=5th |location=London |publisher=B. Fellowes |oclc=1739330 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Wigmore |first=John Henry |authorlink=John Henry Wigmore |title=The principles of judicial proof: as given by logic, psychology, and general experience, and illustrated in judicial trials |url=https://books.google.com/books?id=4ho-AAAAIAAJ |year=1913 |location=Boston |publisher=Little Brown |oclc=1938596 |ref=harv |accessdate=2016-02-24}}

== Further reading ==
* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Garssen |first2=Bart |last3=Krabbe |first3=Erik C. W. |last4=Snoeck Henkemans |first4=A. Francisca |last5=Verheij |first5=Bart |last6=Wagemans |first6=Jean H. M. |date=2014 |title=Handbook of argumentation theory |location=New York |publisher=Springer |isbn=9789048194728 |oclc=871004444 |doi=10.1007/978-90-481-9473-5 |ref=harv}}
* {{cite book|last1=Facione |first1=Peter A. |last2=Facione |first2=Noreen C. |title=Thinking and reasoning in human decision making: the method of argument and heuristic analysis |year=2007 |location=Milbrae, CA |publisher=California Academic Press |isbn=1891557580 |oclc=182039452 |ref=harv}}
* {{cite web |last=van Gelder |first=Tim |authorlink=Tim van Gelder |url=http://timvangelder.com/2009/02/17/what-is-argument-mapping/ |title=What is argument mapping? |publisher=timvangelder.com |date=17 February 2009 |accessdate=12 January 2015 |ref=harv}}
* {{cite book |last=van Gelder |first=Tim |authorlink=Tim van Gelder |date=2015 |chapter=Using argument mapping to improve critical thinking skills |editor1-last=Davies |editor1-first=Martin |editor2-last=Barnett |editor2-first=Ronald |title=The Palgrave handbook of critical thinking in higher education |location=New York |publisher=[[Palgrave Macmillan]] |pages=183&#8211;192 |isbn=9781137378033 |oclc=894935460 |doi=10.1057/9781137378057_12 |ref=harv}}
* {{cite journal |last=Harrell |first=Maralee |title=Using argument diagramming software in the classroom |journal=[[Teaching Philosophy]] |volume=28 |issue=2 |pages=163&#8211;177 |date=June 2005 |doi=10.5840/teachphil200528222 |url=http://www.hss.cmu.edu/philosophy/harrell/ArgumentDiagramsInClassroom.pdf |ref=harv}}
* {{cite journal |last1=Schneider |first1=Jodi |last2=Groza |first2=Tudor |last3=Passant |first3=Alexandre |date=April 2013 |title=A review of argumentation for the social semantic web |journal=Semantic Web |volume=4 |issue=2 |pages=159&#8211;218 |url=http://semantic-web-journal.org/sites/default/files/swj138_0.pdf |ref=harv}}

== External links ==

===Argument mapping software===
*[http://araucaria.computing.dundee.ac.uk/ Araucaria] (open source, cross platform/Java)
*[http://sourceforge.net/projects/argumentative/ Argumentative] (open source, Windows); supports single-user, graphical argumentation
*[http://www.argunet.org/editor/ Argunet] (open source, cross platform)
*[http://compendiuminstitute.net/ Compendium] (open source, cross platform/Java)
*[http://www.phil.cmu.edu/projects/argument_mapping/ iLogos] (cross platform/Java)
*[http://ova.arg-tech.org/ OVA] (Web based, Online Visualisation of Argument)
*[http://www.cs.ie.niigata-u.ac.jp/Research/PIRIKA/PIRIKA.html PIRIKA (PIlot for the RIght Knowledge and Argument)] (open source, Linux, Windows)
*[http://www.vangeldermonk.com/reasoningapp/ The Reasoning PowerPoint App] ([[PowerPoint]] add-in, Windows 2007 or later)

===Online, collaborative software===
*[http://agora.gatech.edu/ AGORA-net] (user interface in English, German, Spanish, Chinese, and Russian)
*[http://arguman.org/ Arguman] (Open source, English, Turkish, and Chinese)
*[http://www.bcisiveonline.com/ bCisiveOnline]
*[http://carneades.github.io/ Carneades] (open source, argument (re)construction, evaluation, mapping and interchange)
*[https://code.google.com/p/collam/ Collam] ([[JavaScript]] library for visualizing argument maps)
*[http://www.debategraph.org/ Debategraph]
*[http://www.truthmapping.com/ TruthMapping]

[[Category:Argument mapping]]
[[Category:Arguments]]
[[Category:Critical thinking]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Problem structuring methods]]</text>
      <sha1>e3g63ewjo08cpvujyl3dxerikbdvivi</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical model</title>
    <ns>0</ns>
    <id>20590</id>
    <revision>
      <id>756102611</id>
      <parentid>756102141</parentid>
      <timestamp>2016-12-22T00:54:50Z</timestamp>
      <contributor>
        <username>Frode54</username>
        <id>28691415</id>
      </contributor>
      <minor />
      <comment>/* Elements of a mathematical model */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32079" xml:space="preserve">{{Distinguish2|the same term used in [[model theory]], a branch of [[mathematical logic]].}}
{{Refimprove|date=May 2008}} 
A '''mathematical model''' is a description of a [[system]] using [[mathematics|mathematical]] concepts and [[Language of mathematics|language]]. The process of developing a mathematical model is termed '''mathematical modeling'''. Mathematical models are used in the [[natural science]]s (such as [[physics]], [[biology]], [[earth science]], [[meteorology]]) and [[engineering]] disciplines (such as [[computer science]], [[artificial intelligence]]), as well as in the [[social sciences]] (such as [[economics]], [[psychology]], [[sociology]], [[political science]]).  [[Physicist]]s, [[engineer]]s, [[statistician]]s, [[operations research]] analysts, and [[economist]]s use mathematical models most extensively. A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.

== Elements of a mathematical model ==

Mathematical models can take many forms, including [[dynamical systems]], [[statistical model]]s, [[differential equations]], or [[Game theory|game theoretic models]].  These and other types of models can overlap, with a given  model involving a variety of abstract structures. In general, mathematical models may include [[model theory|logical models]].  In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments.  Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.

In the [[physical sciences]], the traditional mathematical model contains four major elements. These are
# [[Governing equation]]s
# [[Defining equation (physics)|Defining equation]]s
# [[Constitutive equation]]s
# [[Constraint (mathematics)|Constraint]]s

== Classifications ==
Mathematical models are usually composed of relationships and ''[[variable (mathematics)|variables]]''. Relationships can be described by ''[[Operator (mathematics)|operators]]'', such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system [[parameter (computer programming)|parameters]] of interest, that can be [[Quantification (science)|quantified]]. Several classification criteria can be used for mathematical models according to their structure:
* '''Linear vs. nonlinear:''' If all the operators in a mathematical model exhibit [[linear]]ity, the resulting mathematical model is defined as linear. A model is considered to be nonlinear otherwise. The definition of linearity and nonlinearity is dependent on context, and linear models may have nonlinear expressions in them.  For example, in a [[Linear model|statistical linear model]], it is assumed that a relationship is linear in the parameters, but it may be nonlinear in the predictor variables.  Similarly, a differential equation is said to be linear if it can be written with linear [[differential operator]]s, but it can still have nonlinear expressions in it.  In a [[Optimization (mathematics)|mathematical programming]] model, if the objective functions and constraints are represented entirely by [[linear equation]]s, then the model is regarded as a linear model.  If one or more of the objective functions or constraints are represented with a [[nonlinearity|nonlinear]] equation, then the model is known as a nonlinear model.&lt;br&gt;Nonlinearity, even in fairly simple systems, is often associated with phenomena such as [[Chaos theory|chaos]] and [[irreversibility]].  Although there are exceptions, nonlinear systems and models tend to be more difficult to study than linear ones.  A common approach to nonlinear problems is [[linearization]], but this can be problematic if one is trying to study aspects such as irreversibility, which are strongly tied to nonlinearity.
* '''Static vs. dynamic:''' A ''dynamic'' model  accounts for time-dependent changes in the state of the system, while a ''static'' (or steady-state) model calculates the system in equilibrium, and thus is time-invariant.  Dynamic models typically are represented by [[differential equation]]s or [[difference equation]]s.
* '''Explicit vs. implicit:''' If all of the input parameters of the overall model are known, and the output parameters can be calculated by a finite series of computations, the model is said to be ''explicit''. But sometimes it is the ''output'' parameters which are known, and the corresponding inputs must be solved for by an iterative procedure, such as [[Newton's method]] (if the model is linear) or [[Broyden's method]] (if non-linear). In such a case the model is said to be ''implicit''. For example, a [[jet engine]]'s physical properties such as turbine and nozzle throat areas can be explicitly calculated given a design [[thermodynamic cycle]] (air and fuel flow rates, pressures, and temperatures) at a specific flight condition and power setting, but the engine's operating cycles at other flight conditions and power settings cannot be explicitly calculated from the constant physical properties.
* '''Discrete vs. continuous:''' A [[discrete modeling|discrete model]] treats objects as discrete, such as the particles in a [[molecular model]] or the states in a [[statistical model]]; while a [[continuous model]] represents the objects in a continuous manner, such as the velocity field of fluid in pipe flows, temperatures and stresses in a solid, and electric field that applies continuously over the entire model due to a point charge.
* '''Deterministic vs. probabilistic (stochastic):''' A [[deterministic system|deterministic]] model is one in which every set of variable states is uniquely determined by parameters in the model and by sets of previous states of these variables; therefore, a deterministic model always performs the same way for a given set of initial conditions. Conversely, in a stochastic model&#8212;usually called a "[[statistical model]]"&#8212;randomness is present, and variable states are not described by unique values, but rather by [[probability]] distributions.
* '''Deductive, inductive, or floating:''' A deductive model is a logical structure based on a theory. An inductive model arises from empirical findings and generalization from them. The floating model rests on neither theory nor observation, but is merely the invocation of expected structure. Application of mathematics in social sciences outside of economics has been criticized for unfounded models.&lt;ref&gt;{{cite book |authorlink=Stanislav Andreski |first=Stanislav |last=Andreski |year=1972 |title=Social Sciences as Sorcery |publisher=[[St. Martin&#8217;s Press]] |isbn=0-14-021816-5 }}&lt;/ref&gt; Application of [[catastrophe theory]] in science has been characterized as a floating model.&lt;ref&gt;{{cite book |authorlink=Clifford Truesdell |first=Clifford |last=Truesdell |year=1984 |title=An Idiot&#8217;s Fugitive Essays on Science |pages=121&#8211;7 |publisher=Springer |isbn=3-540-90703-3 }}&lt;/ref&gt;

== Significance in the natural sciences ==
Mathematical models are of great importance in the natural sciences, particularly in [[physics]]. Physical [[theory|theories]] are almost invariably expressed using mathematical models.

Throughout history, more and more accurate mathematical models have been developed. [[Newton's laws of motion|Newton's laws]] accurately describe many everyday phenomena, but at certain limits [[relativity theory]] and [[quantum mechanics]] must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the [[speed of light]]. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the [[de Broglie wavelength]] of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.

It is common to use idealized models in physics to simplify things. Massless ropes, point particles, [[ideal gases]] and the [[particle in a box]] are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, [[Maxwell's equations]] and the [[Schr&#246;dinger equation]]. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by [[molecular orbital]] models that are approximate solutions to the Schr&#246;dinger equation. In [[engineering]], physics models are often made by mathematical methods such as [[finite element analysis]].

Different mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. [[Euclidean geometry]] is much used in classical physics, while [[special relativity]] and [[general relativity]] are examples of theories that use [[geometry|geometries]] which are not Euclidean.

== Some applications==&lt;!--whatever this section is, it isn't background (original heading), and isn't general--&gt;
Since [[prehistory|prehistorical times]] simple models such as [[map]]s and [[Mathematical diagram|diagrams]] have been used.

Often when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in [[simulation]]s.

A mathematical model usually describes a system by a [[Set (mathematics)|set]] of variables and a set of equations that establish relationships between the variables. Variables may be of many types; [[Real number|real]] or [[integer]] numbers, [[Boolean data type|boolean]] values or [[String (computing)|strings]], for example. The variables represent some properties of the system, for example, measured system outputs often in the form of [[Signal (electronics)|signals]], [[Chronometry|timing data]], counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.

== Building blocks ==
In [[business]] and [[engineering]], mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: [[decision theory|decision variables]], [[state variable]]s, [[Exogeny|exogenous]] variables, and [[random variable]]s.

Decision variables are sometimes known as independent variables.  Exogenous variables are sometimes known as [[parameter]]s or [[constant (mathematics)|constant]]s.
The variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables.  Furthermore, the output variables are dependent on the state of the system (represented by the state variables).

[[Goal|Objective]]s and [[constraint (mathematics)|constraint]]s of the system and its users can be represented as [[function (mathematics)|function]]s of the output variables or state variables.  The [[objective function]]s will depend on the perspective of the model's user.  Depending on the context, an objective function is also known as an ''index of performance'', as it is some measure of interest to the user.  Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.

For example, [[economist]]s often apply [[linear algebra]] when using [[input-output model]]s. Complicated mathematical models that have many variables may be consolidated by use of [[vector space|vectors]] where one symbol represents several variables.

== A priori information ==
[[File:Blackbox3D-withGraphs.png|thumb|480px|To analyse something with a typical "black box approach", only  the behavior of the stimulus/response will be accounted for, to infer the (unknown) ''box''. The usual representation of this ''black box system''  is a [[data flow diagram]] centered in the box.]]

Mathematical modeling problems are often classified into [[black box]] or [[White box (software engineering)|white box]] models, according to how much [[a priori (philosophy)|a priori]] information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.

Usually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an [[exponential decay|exponentially decaying]] function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.

In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are [[neural networks]] which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of [[nonlinear system identification]] &lt;ref name="SAB1"&gt;Billings S.A. (2013), ''Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains'', Wiley.&lt;/ref&gt; can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.

=== Subjective information ===
Sometimes it is useful to incorporate subjective information into a mathematical model.  This can be done based on [[Intuition (knowledge)|intuition]], [[experience]], or [[expert opinion]], or based on convenience of mathematical form.  [[Bayesian statistics]] provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a [[prior probability distribution]] (which can be subjective), and then update this distribution based on empirical data.

An example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads.  After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use.  Incorporation of such subjective information might be important to get an accurate estimate of the probability.

== Complexity ==
[[File:Mathematical models for complex systems.jpg|300px|right|thumb|This is a schematic representation of three types of mathematical models of complex systems with the level of their mechanistic understanding.]]
In general, model complexity involves a trade-off between simplicity and accuracy of the model.  [[Occam's razor]] is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable.  While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including [[numerical instability]].  [[Thomas Kuhn]] argues that as science progresses, explanations tend to become more complex before a [[paradigm shift]] offers radical simplification.

For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, [[Isaac Newton|Newton's]] [[classical mechanics]] is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the [[speed of light]], and we study macro-particles only.

== Training ==
Any model which is not pure white-box contains some [[parameter]]s that can be used to fit the model to the system it is intended to describe. If the modeling is done by a [[neural network]] or other [[machine learning]], the optimization of parameters is called ''training'', while the optimization of model hyperparameters is called ''tuning'' and often uses [[cross-validation]]. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by ''[[curve fitting]]''.

== Model evaluation ==
A crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately.  This question can be difficult to answer as it involves several different types of evaluation.

=== Fit to empirical data ===
Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data.  In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters.  An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as [[cross-validation (statistics)|cross-validation]] in statistics.

Defining a [[Metric (mathematics)|metric]] to measure distances between observed and predicted data is a useful tool of assessing model fit.  In statistics, decision theory, and some [[economic model]]s, a [[loss function]] plays a similar role.

While it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model.  In general, more mathematical tools have been developed to test the fit of [[statistical model]]s than models involving [[differential equations]].  Tools from [[non-parametric statistics]] can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.

=== Scope of the model ===
Assessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward.  If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a "typical" set of data.

The question of whether the model describes well the properties of the system between data points is called [[interpolation]], and the same question for events or data points outside the observed data is called [[extrapolation]].

As an example of the typical limitations of the scope of a model, in evaluating Newtonian [[classical mechanics]], we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light.  Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.

=== Philosophical considerations ===
Many types of modeling implicitly involve claims about [[causality]].  This is usually (but not always) true of models involving differential equations.  As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.

An example of such criticism is the argument that the mathematical models of [[Optimal foraging theory]] do not offer insight that goes beyond the common-sense conclusions of [[evolution]] and other basic principles of ecology.&lt;ref&gt;{{Cite journal | last1 = Pyke | first1 = G. H. | doi = 10.1146/annurev.es.15.110184.002515 | title = Optimal Foraging Theory: A Critical Review | journal = Annual Review of Ecology and Systematics | volume = 15 | pages = 523&#8211;575 | year = 1984 | pmid =  | pmc = }}&lt;/ref&gt;

== Examples ==
* One of the popular examples in [[computer science]] is the mathematical models of various machines, an example is the [[deterministic finite automaton]] which is defined as an abstract mathematical concept, but due to the deterministic nature of a DFA, it is implementable in hardware and software for solving various specific problems. For example, the following is a DFA M with a binary alphabet, which requires that the input contains an even number of 0s.

[[File:DFAexample.svg|right|thumb|250px|The [[state diagram]] for ''M'']]
''M'' = (''Q'', &#931;, &#948;, ''q''&lt;sub&gt;0&lt;/sub&gt;, ''F'') where
*''Q'' = {''S''&lt;sub&gt;1&lt;/sub&gt;, ''S''&lt;sub&gt;2&lt;/sub&gt;},
*&#931; = {0, 1},
*''q&lt;sub&gt;0&lt;/sub&gt;'' = ''S''&lt;sub&gt;1&lt;/sub&gt;,
*''F'' = {''S''&lt;sub&gt;1&lt;/sub&gt;}, and
*&#948; is defined by the following [[state transition table]]:
:{| border="1" cell padding="1" cell spacing="0"
| || &lt;center&gt;'''0'''&lt;/center&gt; || &lt;center&gt;'''1'''&lt;/center&gt;
|-
|'''''S''&lt;sub&gt;1&lt;/sub&gt;''' || ''S''&lt;sub&gt;2&lt;/sub&gt; || ''S''&lt;sub&gt;1&lt;/sub&gt;
|-
|'''''S''&lt;sub&gt;2&lt;/sub&gt;''' || ''S''&lt;sub&gt;1&lt;/sub&gt; || ''S''&lt;sub&gt;2&lt;/sub&gt;
|}

The state ''S''&lt;sub&gt;1&lt;/sub&gt; represents that there has been an even number of 0s in the input so far, while ''S''&lt;sub&gt;2&lt;/sub&gt; signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, ''M'' will finish in state ''S''&lt;sub&gt;1&lt;/sub&gt;, an accepting state, so the input string will be accepted.

The language recognized by ''M'' is the [[regular language]] given by the [[regular expression]] 1*( 0 (1*) 0 (1*) )*, where "*" is the [[Kleene star]], e.g., 1* denotes any non-negative number (possibly zero) of symbols "1".

* Many everyday activities carried out without a thought are uses of mathematical models. A geographical [[map projection]] of a region of the earth onto a small, plane surface  is a model&lt;ref&gt;[http://www.landinfo.com/resources_dictionaryMP.htm landinfo.com, definition of map projection]&lt;/ref&gt; which can be used for many purposes such as planning travel.
* Another simple activity is predicting the position of a vehicle from its initial position, direction and speed of travel, using the equation that distance traveled is the product of time and speed. This is known as [[dead reckoning]] when used more formally. Mathematical modeling in this way does not necessarily require formal mathematics; animals have been shown to use dead reckoning.&lt;ref&gt;{{cite book |last=Gallistel |first= |title=The Organization of Learning |location=Cambridge |publisher=The MIT Press |year=1990 |ISBN=0-262-07113-4 }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Whishaw | first1 = I. Q. | last2 = Hines | first2 = D. J. | last3 = Wallace | first3 = D. G. | doi = 10.1016/S0166-4328(01)00359-X | title = Dead reckoning (path integration) requires the hippocampal formation: Evidence from spontaneous exploration and spatial learning tasks in light (allothetic) and dark (idiothetic) tests | journal = Behavioural Brain Research | volume = 127 | issue = 1&#8211;2 | pages = 49&#8211;69 | year = 2001 | pmid =  11718884| pmc = }}&lt;/ref&gt;
* ''[[Population]] Growth''. A simple (though approximate) model of population growth is the [[Malthusian growth model]]. A slightly more realistic and largely used population growth model is the [[logistic function]], and its extensions. 
* ''Individual-based cellular automata models of [[population]] growth''
[[File:Logical deterministic individual-based cellular automata model of single species population growth.gif|left|thumb|150px]]
&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;
* ''Model of a particle  in a potential-field''. In this model we consider a particle as being a point of mass which describes a trajectory in space which is modeled by a function giving its coordinates in space as a function of time. The potential field is given by a function &lt;math&gt;V\!:\mathbb{R}^3\!\rightarrow\mathbb{R}&lt;/math&gt; and the trajectory, that is a function &lt;math&gt;\mathbf{r}\!:\mathbb{R}\rightarrow\mathbb{R}^3&lt;/math&gt;, is the solution of the [[differential equation]]:

::&lt;math&gt; -\frac{\mathrm{d}^2\mathbf{r}(t)}{\mathrm{d}t^2}m=\frac{\partial V[\mathbf{r}(t)]}{\partial x}\mathbf{\hat{x}}+\frac{\partial V[\mathbf{r}(t)]}{\partial y}\mathbf{\hat{y}}+\frac{\partial V[\mathbf{r}(t)]}{\partial z}\mathbf{\hat{z}}, &lt;/math&gt;

that can be written also as:

::&lt;math&gt; m\frac{\mathrm{d}^2\mathbf{r}(t)}{\mathrm{d}t^2}=-\nabla V[\mathbf{r}(t)]. &lt;/math&gt;

:Note this model assumes  the particle is a point mass, which is certainly known to be false in many cases in which we use this model; for example, as a model of planetary motion.

* ''Model of rational behavior for a consumer''.  In this model we assume a consumer faces a choice of ''n'' commodities labeled 1,2,...,''n'' each with a market price ''p''&lt;sub&gt;1&lt;/sub&gt;, ''p''&lt;sub&gt;2&lt;/sub&gt;,..., ''p''&lt;sub&gt;''n''&lt;/sub&gt;. The consumer is assumed to have a ''cardinal'' utility function ''U'' (cardinal in the sense that it assigns numerical values to utilities), depending on the amounts of commodities ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,..., ''x''&lt;sub&gt;''n''&lt;/sub&gt; consumed.  The model further assumes that the consumer has a budget ''M'' which is used to purchase a vector ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,..., ''x''&lt;sub&gt;''n''&lt;/sub&gt; in such a way as to maximize ''U''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;).  The problem of rational behavior in this model then becomes an [[Optimization (mathematics)|optimization]] problem, that is:
:: &lt;math&gt; \max U(x_1,x_2,\ldots, x_n) &lt;/math&gt;
:: subject to:
:: &lt;math&gt; \sum_{i=1}^n p_i x_i \leq M.&lt;/math&gt;
:: &lt;math&gt; x_{i} \geq 0   \; \; \; \forall i \in \{1, 2, \ldots, n \} &lt;/math&gt;
: This model has been used in [[general equilibrium theory]], particularly to show existence and [[Pareto efficiency]] of economic equilibria.  However, the fact that this particular formulation assigns ''numerical values'' to levels of satisfaction is the source of criticism (and even ridicule).  However, it is not an essential ingredient of the theory and again this is an idealization.
* ''[[Neighbour-sensing model]]'' explains the [[mushroom]] formation from the initially chaotic [[fungus|fungal]] network.
* ''[[Computer science]]'': models in Computer Networks, data models, surface model,...
* ''[[Mechanics]]'': movement of rocket model,...

Modeling requires selecting and identifying relevant aspects of a situation in the real world.

== See also ==
{{Portal|Mathematics}}
{{div col|2}}
* [[Agent-based model]]
* [[Cliodynamics]]
* [[Computer simulation]]
* [[Conceptual model]]
* [[Decision engineering]]
* [[Grey box model]]
* [[Mathematical biology]]
* [[Mathematical diagram]]
* [[Mathematical psychology]]
* [[Mathematical sociology]]
* [[Model inversion]]
* [[Microscale and macroscale models]]
* [[Statistical Model]]
* [[System identification]]
* [[TK Solver]] - Rule Based Modeling
{{div col end}}

== References ==
{{Reflist}}

== Further reading ==

===Books===
* Aris, Rutherford [ 1978 ] ( 1994 ). ''Mathematical Modelling Techniques'', New York: Dover. ISBN 0-486-68131-9
* Bender, E.A. [ 1978 ] ( 2000 ). ''An Introduction to Mathematical Modeling'', New York: Dover. ISBN 0-486-41180-X
* Gershenfeld, N. (1998) ''The Nature of Mathematical Modeling'', [[Cambridge University Press]] ISBN 0-521-57095-6 .
* Lin, C.C. &amp; Segel, L.A. ( 1988 ). ''Mathematics Applied to Deterministic Problems in the Natural Sciences'', Philadelphia: SIAM. ISBN 0-89871-229-7

===Specific applications===
* [[Korotayev]] A., Malkov A., Khaltourina D. (2006). [http://cliodynamics.ru/index.php?option=com_content&amp;task=view&amp;id=124&amp;Itemid=70 ''Introduction to Social Macrodynamics: Compact Macromodels of the World System Growth'']. Moscow: [http://urss.ru/cgi-bin/db.pl?cp=&amp;lang=en&amp;blang=en&amp;list=14&amp;page=Book&amp;id=34250 Editorial URSS] ISBN 5-484-00414-4 .
* {{Cite journal | last1 = Peierls | first1 = R. | doi = 10.1080/00107518008210938 | title = Model-making in physics | journal = Contemporary Physics | volume = 21 | pages = 3&#8211;17 | year = 1980 | pmid =  | pmc = |bibcode = 1980ConPh..21....3P }}
* ''[http://anintroductiontoinfectiousdiseasemodelling.com/ An Introduction to Infectious Disease Modelling]'' by Emilia Vynnycky and Richard G White.

== External links ==

;General reference

* Patrone, F. [http://www.fioravante.patrone.name/mat/u-u/en/differential_equations_intro.htm Introduction to modeling via differential equations], with critical remarks.
* [http://plus.maths.org/issue44/package/index.html Plus teacher and student package: Mathematical Modelling.] Brings together all articles on mathematical modeling from ''[[Plus Magazine]]'', the online mathematics magazine produced by the Millennium Mathematics Project at the University of Cambridge.

;Philosophical

* Frigg, R. and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science], in: The Stanford Encyclopedia of Philosophy, (Spring 2006 Edition)
* Griffiths, E. C. (2010) [https://sites.google.com/a/ncsu.edu/emily-griffiths/whatisamodel.pdf What is a model?]

{{Authority control}}

{{DEFAULTSORT:Mathematical Model}}
[[Category:Applied mathematics]]
[[Category:Collective intelligence]]
[[Category:Conceptual models]]
[[Category:Knowledge representation]]
[[Category:Mathematical modeling| ]]
[[Category:Mathematical terminology]]</text>
      <sha1>rynmrsy8u9m5zy4j74jlcegwy2ifo09</sha1>
    </revision>
  </page>
  <page>
    <title>Parallel Tree Contraction</title>
    <ns>0</ns>
    <id>48789236</id>
    <revision>
      <id>751669211</id>
      <parentid>733407508</parentid>
      <timestamp>2016-11-27T04:39:15Z</timestamp>
      <contributor>
        <username>Unready</username>
        <id>8433548</id>
      </contributor>
      <minor />
      <comment>&lt;pre&gt; for no lang</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13629" xml:space="preserve">{{Orphan|date=December 2015}}

In [[computer science]], '''parallel tree contraction''' is a broadly applicable technique for the parallel solution of a large number of [[tree]] problems, and is used as an algorithm design technique for the design of a large number of parallel [[graph (discrete mathematics)|graph]] algorithms.  Parallel tree contraction was introduced by [[Gary L. Miller]] and [[John H. Reif]],&lt;ref name="Miller89book"&gt;[[Gary L. Miller]] and [[John H. Reif]], Parallel Tree Contraction--Part I: Fundamentals., 1989&lt;/ref&gt;  and has subsequently been modified to improve efficiency by X. He and Y. Yesha,&lt;ref&gt;X. He and Y. Yesha, "Binary tree algebraic computation and parallel algorithms for simple graphs.", Journal of Algorithms, 1988, pp 92-113&lt;/ref&gt; Hillel Gazit, Gary L. Miller and Shang-Hua Teng&lt;ref&gt;Hillel Gazit, Gary L. Miller and Shang-Hua Teng, Optimal tree contraction in the EREW model, Springer, 1988&lt;/ref&gt; and many others.&lt;ref&gt;Karl Abrahamson and et al., "A simple parallel tree contraction algorithm.", Journal of Algorithms, 1989, pp 287-302&lt;/ref&gt;

Tree contraction has been used in designing many efficient [[parallel algorithms]], including [[Expression (mathematics)|expression]] evaluation, finding [[lowest common ancestors]], tree isomorphism, [[graph isomorphism]], [[maximal subtree isomorphism]], [[common subexpression elimination]], computing the 3-connected components of a graph, and finding an explicit planar embedding of a [[planar graph]]&lt;ref name="Reif94dynamic"&gt;John H. Reif and Stephen R. Tate, Dynamic parallel tree contraction, Proceedings of the sixth annual ACM symposium on Parallel algorithms and architectures (ACM), 1994&lt;/ref&gt;

Based on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic. This article hereby focuses on a particular solution, which is a variant of the algorithm by Miller and Reif, and its application.

==Introduction==
Over the past several decades there has been significant research on deriving new parallel algorithms for a variety of problems, with the goal of designing highly parallel ([[polylogarithmic depth]]), work-efficient (linear in the sequential running time) algorithms.&lt;ref name="Miller89book" /&gt; For some problems, tree turns out to be a nice solution. Addressing these problems, we can sometimes get more parallelism simply by representing our problem as a tree.

Considering a generic definition of a tree, there is a root vertex, and several child vertices attached to the root.&lt;ref&gt;[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&amp;nbsp;214&#8211;217. Chapters 12&#8211;14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&amp;nbsp;253&#8211;320.&lt;/ref&gt; And the child vertices might have children themselves, and so on so forth. Eventually, the paths come down to leaves, which are defined to be the terminal of a tree. Then based on this generic tree, we can further come up with some special cases: (1) [[balanced binary tree]]; (2) [[linked list]].&lt;ref&gt;[[Donald Knuth]]. ''[[The Art of Computer Programming]]: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&amp;nbsp;308&#8211;423.&lt;/ref&gt; A balanced binary tree has exactly two branches for each vertex except for leaves. This gives a O(log n) bound on the depth of the tree.&lt;ref&gt;{{citation
 | last1 = Ne&#353;et&#345;il | first1 = Jaroslav | author1-link = Jaroslav Ne&#353;et&#345;il
 | last2 = Ossona de Mendez | first2 = Patrice | author2-link = Patrice Ossona de Mendez
 | contribution = Chapter 6. Bounded height trees and tree-depth
 | doi = 10.1007/978-3-642-27875-4
 | isbn = 978-3-642-27874-7
 | location = Heidelberg
 | mr = 2920058
 | pages = 115&#8211;144
 | publisher = Springer
 | series = Algorithms and Combinatorics
 | title = Sparsity: Graphs, Structures, and Algorithms
 | volume = 28
 | year = 2012}}.&lt;/ref&gt; A linked list is also a tree where every vertex has only one child. We can also achieve O(log n) depth using [[symmetry breaking]].&lt;ref&gt;Andrew Goldberg, Serge Plotkin, and Gregory Shannon, Parallel symmetry-breaking in sparse graphs, Proceedings of the nineteenth annual ACM symposium on Theory of computing (ACM), 1987&lt;/ref&gt;

Given the general case of a tree, we would like to keep the bound at O(log n) no matter it is unbalanced or list-like or a mix of both. To address this problem, we make use of an algorithm called [[prefix sum]] by using the [[Euler tour technique]].&lt;ref&gt;[http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf Euler tour trees] - in Lecture Notes in Advanced Data Structures. Prof. Erik Demaine; Scribe: Katherine Lai.&lt;/ref&gt; With the Euler tour technique, a tree could be represented in a flat style, and thus prefix sum could be applied to an arbitrary tree in this format. In fact, prefix sum can be used on any set of values and binary operation which form a group: the binary operation must be associative, every value must have an inverse, and there exists an identity value.

With a bit of thought, we can find some exceptional cases where prefix sum becomes incapable or inefficient. Consider the example of multiplication when the set of values includes 0. Or there are some commonly desired operations are max() and min() which do not have [[inverses]]. The goal is to seek an algorithm which works on all trees, in expected O(n) work and O(log n) depth. In the following sections, a Rake/Compress algorithm will be proposed to fulfill this goal.&lt;ref name="Miller85app"&gt;Gary L. Miller and John H. Reif, Parallel tree contraction and its application, Defense Technical Information Center, 1985&lt;/ref&gt;

==Definitions==

[[File:Rake-1.png|480*360px|thumbnail|right|Fig. 1: Rake Operation]]
[[File:Compress-1.png|480*360px|thumbnail|right|Fig. 2: Compress Operation]]
Before going into the algorithm itself, we first look at a few terminologies that will be used later.

* '''Rake'''&lt;ref name="cmutrees"&gt;[https://www.cs.cmu.edu/afs/cs/academic/class/15499-s09/www/scribe/lec11/lec11.pdf Parallel Algorithms: Tree Operations], Guy Blelloch, Carnegie Mellon University, 2009&lt;/ref&gt; &#8211; Rake step joins every left leaf of binary nodes to the parent. By join, we mean that it undergoes a functional process which achieves the operation we want to make. An example of rake is given in Figure 1.
* '''Compress'''&lt;ref name="cmutrees" /&gt; &#8211; Compress step is actually a sequence of several events: (1) Find an independent set of unary nodes. (Independence here is defined such that no two are neighbors, meaning no parent to child relation) (2) Join each node in independent set with its child (Note that independent set is not unique). An example of compress is given in Figure 2.

And in order to solve actual problems using tree contraction, the algorithm has a structure:

&lt;pre&gt;
Repeat until tree becomes a unary node
{
    Rake;
    Compress;
}
&lt;/pre&gt;


==Analysis==
For the moment, let us assume that all nodes have less than three children, namely binary. Generally speaking, as long as the degree is bounded, the bounds will hold.&lt;ref&gt;MORIHATA, Akimasa, and Kiminori MATSUZAKI, A Parallel Tree Contraction Algorithm on Non-Binary Trees, MATHEMATICAL ENGINEERING
TECHNICAL REPORTS, 2008&lt;/ref&gt; But we will analyze the binary case for simplicity. In the two &#8220;degenerate&#8221; cases listed above, the rake is the best tool for dealing with balanced binary trees, and compress is the best for linked lists. However, arbitrary trees will have to require a combination of these operations. By this combination, we claim a theorem that
* '''Theorem''': After O(log n) expected rake and compress steps, a tree is reduced to a single node.
Now rephrase the tree contraction algorithm as follows:
* Input: A binary tree rooted at r
* Output: A single node
* Operation:  A sequence of contraction steps, each consisting of a rake operation and a compress operation (in any order). The rake operation removes all the leaf nodes in parallel. The compress operation finds an [[Independent set (graph theory)|independent set]] of unary nodes and splice out the selected nodes.
To approach the theorem, we first take a look at a property of a binary tree. Given a binary tree T, we can partition the nodes of T into 3 groups: {{tmath|T_0}} contains all leaf nodes, {{tmath|T_1}} contains all nodes with 1 child, and {{tmath|T_2}} contains all nodes with 2 children. It is easy to see that: &lt;math&gt;V(T) = T_0  \cup T_1 \cup T_2&lt;/math&gt;. Now we propose:
* Claim: &lt;math&gt;|T_0| = |T_2|  + 1&lt;/math&gt;
This claim can be proved by strong induction on the number of nodes. It is easy to see that the base case of n=1 trivially holds. And we further assume the claim also holds for any tree with at most n nodes. Then given a tree with n+1 nodes rooted at r, there appears to be two cases:
# If r has only one subtree, consider the subtree of r. We know that the subtree has the same number of binary nodes and the same number of leaf nodes as the whole tree itself. This is true since the root is a unary node. And based the previous assumption, a unary node does not change either {{tmath|T_0}} or {{tmath|T_2}}.
# If r has two subtrees, we define {{tmath|T_0^L, T_2^L}} to be the leaf nodes and binary nodes in the left subtree, respectively. Similarly, we define the same {{tmath|T_0^R, T_2^R}} for the right subtree. From previous, there is &lt;math&gt;|T_0^L| = |T_2^L| + 1&lt;/math&gt; and &lt;math&gt;|T_0^R| = |T_2^R| + 1&lt;/math&gt;. Also we know that T has &lt;math&gt;|T_0^L| + |T_0^R|&lt;/math&gt; leaf nodes and &lt;math&gt;|T_2^L| + |T_2^R| + 1&lt;/math&gt; binary nodes. Thus, we can derive:

:&lt;math&gt;|T_0^L| + |T_0^R| = |T_2^L| + 1 + |T_2^R| + 1 = (|T_2^L| + |T_2^R| + 1) + 1&lt;/math&gt;

which proves the claim.

Following the claim, we then prove a lemma, which leads us to the theorem.
* Lemma: The number of nodes of after a contraction step is reduced by a constant factor in expectation.
Assume the number of nodes before the contraction to be m, and m' after the contraction. By definition, the rake operation deletes all {{tmath|T_0}} and the compress operation deletes at least 1/4 of {{tmath|T_1}} in expectation. All {{tmath|T_2}} remains. Therefore, we can see:

:&lt;math&gt;E[m'] \leq |T_2| + \tfrac{3}{4}*|T_1| \leq \tfrac{3}{4} + \tfrac{3}{4}*|T_1| + \tfrac{3}{2}*|T_2| = \tfrac{3}{4}(1 + |T_1| + 2*|T_2|) = \tfrac{3}{4}(|T_0| + |T_1| + |T_2|) = \tfrac{3}{4}m&lt;/math&gt;

Finally, based on this lemma, we can conclude that if the nodes are reduced by a constant factor in each iteration, after {{tmath|O(\log n)}}, there will be only one node left.&lt;ref&gt;[https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/scribe/lec9/lecture9.pdf Parallel Algorithms: Analyzing Parallel Tree Contraction], Guy Blelloch, 2007&lt;/ref&gt;

==Applications==

===Expression Evaluation===
To evaluate an expression given as a binary tree (this problem also known as [[binary expression tree]]),&lt;ref&gt;S Buss, Algorithms for boolean formula evaluation and for tree contraction, Arithmetic, Proof Theory, and Computational Complexity, 1993, pp. 96-115&lt;/ref&gt; consider that:
An arithmetic expression is a tree where the leaves have values from some domain and each internal vertex has two children and a label from {+, x, %}. And further assume that these binary operations can be performed in constant time.

We now show the evaluation can be done with parallel tree contraction.&lt;ref&gt;Bader, David A., Sukanya Sreshta, and Nina R. Weisse-Bernstein, Evaluating arithmetic expressions using tree contraction: A fast and scalable parallel implementation for symmetric multiprocessors (SMPs), High Performance Computing&#8212;HiPC 2002. Springer Berlin Heidelberg, 2002, pp. 63-75.&lt;/ref&gt;
* Step 1. Assign expressions to every node. The expression of a leaf is simply the value that it contains. Write L + R, L &#8722; R, or L &#215; R for the operators, where L and R are the values of the expressions in the left and right subtrees, respectively.
* Step 2. When a left (right) child with 0 children is merged into an operator, replace L (R) with the value of the child.
* Step 3. When a node has 1 child, it has an expression that is a function of one variable. When a left (right) child with 1 child is merged into an operator, replace L (R) with the expression and change the variable in the expression to L (R) if appropriate.

In a node with 2 children, the operands in the expression are f(L) and g(R), where f and g are linear functions, and in a node with 1 child, the expression is h(x), where h is a linear function and x is either L or R. We prove this invariant by induction. At the beginning, the invariant is clearly satisfied. There are three types of merges that result in a not fully evaluated expression. (1) A 1-child node is merged into a 2-children node. (2) A leaf is merged into a 2-children node. (3) A 1-child node is merged into a 1-child node. All three types of merges do not change the invariant. Therefore, every merge simply evaluates or composes linear functions, which takes constant time &lt;ref&gt;[http://math.mit.edu/~rpeng/18434/applicationsParallelTreeContraction.pdf Applications of Parallel Tree Contraction], Samuel Yeom, 2015&lt;/ref&gt;

==References==
{{Reflist}}
{{refbegin}}
{{refend}}

==External links==
{{Commons category|Tree structures}}
* [http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf 6.851: Advanced Data Structures] by Prof. Erik Demaine

{{CS-Trees}}

[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]</text>
      <sha1>pm7z4afke6zpabd0n2znolgmshte22e</sha1>
    </revision>
  </page>
  <page>
    <title>Pattern language</title>
    <ns>0</ns>
    <id>182837</id>
    <revision>
      <id>752197416</id>
      <parentid>749932531</parentid>
      <timestamp>2016-11-30T00:37:56Z</timestamp>
      <contributor>
        <username>Quiddity</username>
        <id>210259</id>
      </contributor>
      <comment>/* top */ add year for originating book</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24974" xml:space="preserve">{{About|the structured design approach by architect Christopher Alexander}}
A '''pattern language''' is a method of describing good design practices or patterns of useful organization within a field of expertise. The term was coined by architect [[Christopher Alexander]] and popularized by his 1977 book ''[[A Pattern Language]]''.

A pattern language can also be an attempt to express the deeper wisdom of what brings aliveness within a particular field of human endeavor, through a set of interconnected patterns. Aliveness is one placeholder term for "the quality that has no name": a sense of wholeness, spirit, or grace, that while of varying form, is precise and empirically verifiable.{{cn|date=March 2016}} Some advocates{{who|date=March 2016}} of this design approach claim that ordinary people can use it to successfully solve very large, complex design problems.

==What is a pattern?==
{{See also|Design pattern}}
When a designer designs something &#8211; whether a house, computer program, or lamp &#8211; they must make many decisions about how to solve problems. A single problem is documented with its typical place (the [[syntax]]), and use (the [[grammar]]) with the most common and recognized good solution seen in the wild, like the examples seen in [[dictionary|dictionaries]]. Each such entry is a single [[design pattern]]. Each pattern has a name, a descriptive entry, and some cross-references, much like a dictionary entry. A documented pattern should explain why that solution is good in the pattern's contexts.

Elemental or universal ''patterns'' such as "door" or "partnership" are versatile ideals of design, either as found in experience or for use as components in practice, explicitly described as holistic resolutions of the forces in recurrent contexts and circumstances, whether in architecture, medicine, software development or governance, etc. Patterns might be invented or found and studied, such as the naturally occurring patterns of design that characterize human environments.&lt;ref&gt;Henshaw, J. [http://www.synapse9.com/pub/2015_PURPLSOC-JLHfinalpub.pdf Guiding Patterns of Naturally Occurring Design: Elements. PURPLSOC 2015 proceedings, July 3-5 2015 Krems, Austria] PURPLSOC meeting on the many open scientific questions, e.g. regarding the theoretical background of patterns and the practical implementation of pattern methods in research and teaching.&lt;/ref&gt;

Like all languages, a pattern language has [[vocabulary]], [[syntax]], and [[grammar]] &#8211; but a pattern language applies to some complex activity other than communication. In pattern languages for design, the parts break down in this way:
* The language description &#8211; the ''vocabulary'' &#8211; is a collection of named, described solutions to problems in a field of interest. These are called ''design patterns''. So, for example, the language for architecture describes items like: settlements, buildings, rooms, windows, latches, etc.
* Each solution includes ''syntax'', a description that shows where the solution fits in a larger, more comprehensive or more abstract design. This automatically links the solution into a web of other needed solutions. For example, rooms have ways to get light, and ways to get people in and out.
* The solution includes ''grammar'' that describes how the solution solves a problem or produces a benefit. So, if the benefit is unneeded, the solution is not used. Perhaps that part of the design can be left empty to save money or other resources; if people do not need to wait to enter a room, a simple doorway can replace a waiting room.
* In the language description, grammar and syntax cross index (often with a literal alphabetic index of pattern names) to other named solutions, so the designer can quickly think from one solution to related, needed solutions, and document them in a logical way. In Christopher Alexander's book ''A Pattern Language'', the patterns are in decreasing order by size, with a separate alphabetic index.
* The web of relationships in the index of the language provides many paths through the design process.

This simplifies the design work because designers can start the process from any part of the problem they understand and work toward the unknown parts. At the same time, if the pattern language has worked well for many projects, there is reason to believe that even a designer who does not completely understand the design problem at first will complete the design process, and the result will be usable. For example, skiers coming inside must shed snow and store equipment. The messy snow and boot cleaners should stay outside. The equipment needs care, so the racks should be inside.

==Many patterns form a language==
Just as [[words]] must have [[Grammar|grammatical]] and [[Semantics|semantic]] relationships to each other in order to make a spoken [[language]] useful, design patterns must be related to each other in position and utility order to form a pattern language. Christopher Alexander's work describes a process of decomposition, in which the designer has a problem (perhaps a commercial assignment), selects a solution, then discovers new, smaller problems resulting from the larger solution. Occasionally, the smaller problems have no solution, and a different larger solution must be selected. Eventually all of the remaining design problems are small enough or routine enough to be solved by improvisation by the builders, and the "design" is done.

The actual organizational structure ([[Hierarchy|hierarchical]], [[Iterative method|iterative]], etc.) is left to the discretion of the designer, depending on the problem. This explicitly lets a designer explore a design, starting from some small part. When this happens, it's common for a designer to realize that the problem is actually part of a larger solution. At this point, the design almost always becomes a better design.

In the language, therefore, each pattern has to indicate its relationships to other patterns and to the language as a whole. This gives the designer using the language a great deal of guidance about the related problems that must be solved.

The most difficult part of having an outside expert apply a pattern language is in fact to get a reliable, complete list of the problems to be solved. Of course, the people most familiar with the problems are the people that need a design. So, Alexander famously advocated on-site improvisation by concerned, empowered users,&lt;ref&gt;A Pattern Language, ibid&lt;/ref&gt;&lt;ref&gt;Alexander, Christopher, The Oregon Project&lt;/ref&gt; as a powerful way to form very workable large-scale initial solutions, maximizing the utility of a design, and minimizing the design rework. The desire to empower users of architecture was, in fact, what led Alexander to undertake a pattern language project for architecture in the first place.

==Design problems in a context==
An important aspect of design patterns is to identify and document the key ideas that make a good system different from a poor system (that may be a house, a computer program or an object of daily use), and to assist in the design of future systems. The idea expressed in a pattern should be general enough to be applied in very different systems within its context, but still specific enough to give constructive guidance.

The range of situations in which the problems and solutions addressed in a pattern apply is called its context. An important part in each pattern is to describe this context. Examples can further illustrate how the pattern applies to very different situation.

For instance, Alexander's pattern "A PLACE TO WAIT" addresses bus stops in the same way as waiting rooms in a surgery, while still proposing helpful and constructive solutions. The [[Design Patterns|"Gang-of-Four" book ''Design Patterns'']] by Gamma et al. proposes solutions that are independent of the programming language, and the program's application domain.

Still, the problems and solutions described in a pattern can vary in their level of abstraction and generality on the one side, and specificity on the other side. In the end this depends on the author's preferences. However, even a very abstract pattern will usually contain examples that are, by nature, absolutely concrete and specific.

Patterns can also vary in how far they are proven in the real world. Alexander gives each pattern a rating by zero, one or two stars, indicating how well they are proven in real-world examples. It is generally claimed that all patterns need at least some existing real-world examples. It is, however, conceivable to document yet unimplemented ideas in a pattern-like format.

The patterns in Alexander's book also vary in their level of scale &#8211; some describing how to build a town or neighbourhood, others dealing with individual buildings and the interior of rooms. Alexander sees the low-scale artifacts as constructive elements of the large-scale world, so they can be connected to a [[#Aggregation in an associative network (pattern language)|hierarchic network]].

===Balancing of forces===
A pattern must characterize the problems that it is meant to solve, the context or situation where these problems arise, and the conditions under which the proposed solutions can be recommended.

Often these problems arise from a conflict of different interests or "forces". A pattern emerges as a dialogue that will then help to balance the forces and finally make a decision.

For instance, there could be a pattern suggesting a wireless telephone. The forces would be the need to communicate, and the need to get other things done at the same time (cooking, inspecting the bookshelf). A very specific pattern would be just "WIRELESS TELEPHONE". More general patterns would be "WIRELESS DEVICE" or "SECONDARY ACTIVITY", suggesting that a secondary activity (such as talking on the phone, or inspecting the pockets of your jeans) should not interfere with other activities.

Though quite unspecific in its context, the forces in the "SECONDARY ACTIVITY" pattern are very similar to those in "WIRELESS TELEPHONE". Thus, the competing forces can be seen as part of the essence of a design concept expressed in a pattern.

===Patterns contain their own rationale===
Usually a pattern contains a rationale referring to some given values. For Christopher Alexander, it is most important to think about the people who will come in contact with a piece of architecture. One of his key values is making these people feel more alive. He talks about the "quality without a name" (QWAN).

More generally, we could say that a good system should be accepted, welcomed and happily embraced as an enrichment of daily life by those who are meant to use it, or &#8211; even better &#8211; by all people it affects. For instance, when discussing a street caf&#233;, Alexander discusses the possible desires of a guest, but also mentions people who just walk by.

The same thinking can be applied to technical devices such as telephones and cars, to social structures like a team working on a project, or to the user interface of a computer program. The qualities of a software system, for instance, could be rated by observing whether users spend their time enjoying or struggling with the system.

By focusing on the impacts on human life, we can identify patterns that are independent from changing technology, and thus find "timeless quality" (Alexander).

==Generic structure and layout==
Usually the author of a pattern language or collection chooses a generic structure for all the patterns it contains, breaking each into generic sections like context, problem statement, solution etc.

Christopher Alexander's patterns, for instance, each consist of a short name, a rating (up to two '*' symbols), a sensitizing picture, the context description, the problem statement, a longer part of text with examples and explanations, a solution statement, a sketch and further references. This structure and layout is sometimes referred to as the "Alexandrian form".

Alexander uses a special text layout to mark the different sections of his patterns. For instance, the problem statement and the solution statement are printed in bold font, the latter is always preceded by the "Therefore:" keyword. Some authors instead use explicit labels, which creates some degree of redundancy.

===Meaningful names===
When design is done by a team, pattern names will form a vocabulary they can share. This makes it necessary for pattern names to be easy to remember and highly descriptive. Some examples from Alexander's works are WINDOW PLACE (helps define where windows should go in a room) and A PLACE TO WAIT (helps define the characteristics of bus stops and hospital waiting rooms, for example).

==Aggregation in an associative network (pattern language)==
A pattern language, as conceived by Alexander, contains links from one pattern to another, so when trying to apply one pattern in a project, a designer is pushed to other patterns that are considered helpful in its context.

In Alexander's book, such links are collected in the "references" part, and echoed in the linked pattern's "context" part &#8211; thus the overall structure is a directed graph. A pattern that is linked to in the "references" usually addresses a problem of lower scale, that is suggested as a part of the higher-scale problem. For instance, the "PUBLIC OUTDOOR ROOM" pattern has a reference to "STAIR SEATS".

Even without the pattern description, these links, along with meaningful names, carry a message: When building a place outside where people can spend time ("PUBLIC OUTDOOR ROOM"), consider to surround it by stairs where people can sit ("STAIR SEATS"). If you are planning an office ("WORKSHOPS AND OFFICES"), consider to arrange workspaces in small groups ("SMALL WORKING GROUPS"). Alexander argues that the connections in the network can be considered even more meaningful than the text of the patterns themselves.

The links in Alexander's book clearly result in a hierarchic network. Alexander draws a parallel to the hierarchy of a grammar &#8211; that is one argument for him to speak of a pattern ''language''.

The idea of linking is generally accepted among pattern authors, though the semantic rationale behind the links may vary. Some authors, however, like Gamma et al. in ''[[Design Patterns]]'', make only little use of pattern linking &#8211; possibly because it did not make that much sense for their collection of patterns. In such a case we would speak of a ''pattern catalogue'' rather than a ''pattern language''.&lt;ref name="dearden"&gt;{{cite journal | author = Andy Dearden, Janet Finlay | title = Pattern Languages in HCI: A critical review | date = January 2006 | journal = Human Computer Interaction | volume = 21 | issue = 1 }}&lt;/ref&gt;

===Usage===
Alexander encouraged people who used his system to expand his language with patterns of their own. In order to enable this, his books do not focus strictly on architecture or civil engineering; he also explains the general method of pattern languages. The original concept for the book ''A Pattern Language'' was that it would be published in the form of a 3-ring binder, so that pages could easily be added later; this proved impractical in publishing.&lt;ref&gt;Portland Urban Architecture Research Laboratory
Symposium 2009, presentation by 4 of 6 original authors of ''A Pattern Language''.&lt;/ref&gt;  The pattern language approach has been used to document expertise in diverse fields. Some examples are [[Design pattern (architecture)|architectural patterns]], [[Design pattern (computer science)|computer science patterns]], [[interaction design pattern]]s, [[pedagogical patterns]],  social action patterns, and group facilitation patterns. The pattern language approach has also been recommended as a way to promote [[civic intelligence]] by helping to coordinate actions for diverse people and communities who are working together on significant shared problems (see &lt;ref&gt;Schuler, D. [http://publicsphereproject.org/sites/default/files/Critical%20Enablers%20of%20Civic%20Intelligence.reduced.pdf Choosing Success: Pattern Languages as Critical Enablers of Civic Intelligence]; PUARL Conference, Portland, OR. 2009&lt;/ref&gt; for additional discussion of motivation and rationale as well as examples and experiments).  Alexander's specifications for using pattern languages as well as creating new ones remain influential, and his books are referenced for style by experts in unrelated fields.

It is important to note that notations such as [[Unified Modeling Language|UML]] or the [[flowchart]] symbol collection are not pattern languages. They could more closely be compared to an alphabet: their symbols could be used to document a pattern language, but they are not a language by themselves. A [[recipe]] or other sequential set of steps to be followed, with only one correct path from start to finish, is also not a pattern language. However, the process of designing a new recipe might benefit from the use of a pattern language.

===Simple example of a pattern===
*''Name'': ChocolateChipRatio
*''Context'': You are baking chocolate chip cookies in small batches for family and friends
*''Consider these patterns first'': SugarRatio, FlourRatio, EggRatio
*''Problem'': Determine the optimum ratio of chocolate chips to cookie dough
*''Solution'': Observe that most people consider chocolate to be the best part of the chocolate chip cookie. Also observe that too much chocolate may prevent the cookie from holding together, decreasing its appeal. Since you are cooking in small batches, cost is not a consideration. Therefore, use the maximum amount of chocolate chips that results in a really sturdy cookie.
*''Consider next'': NutRatio or CookingTime or FreezingMethod

==Origin==
[[Christopher Alexander]], an architect and author, coined the term pattern language.&lt;ref&gt;{{Cite book | publisher = [[Oxford University Press]], USA | isbn = 0-19-501919-9 | last = Alexander | first = Christopher | title = A Pattern Language: Towns, Buildings, Construction | year = 1977 | page = 1216}}&lt;/ref&gt; He used it to refer to common problems of the [[design]] and [[construction]] of buildings and towns and how they should be solved. The solutions proposed in the book include suggestions ranging from how cities and towns should be structured to where windows should be placed in a room.

The framework and philosophy of the "pattern language" approach was initially popularized in the book ''[[A Pattern Language]]'' that was written by Christopher Alexander and five colleagues at the Center for Environmental Structure in Berkeley, California in the late 1970s. While ''A Pattern Language'' contains 253 "patterns" from the first pattern, "Independent Regions" (the most general) to the last, "Things from Your Life", Alexander's book ''[[The Timeless Way of Building]]'' goes into more depth about the motivation and purpose of the work. The following definitions of "pattern" and "pattern language" are paraphrased from ''A Pattern Language'':

"A ''pattern'' is a careful description of a perennial solution to a recurring problem within a building context, describing one of the configurations that brings life to a building.

Each pattern describes a problem that occurs over and over again in our environment, and then describes the core solution to that problem, in such a way that you can use the solution a million times over, without ever doing it the same way twice."

A ''pattern language'' is a network of patterns that call upon one another. Patterns help us remember insights and knowledge about design and can be used in combination to create solutions.

== Application domains ==
Christopher Alexander's idea has been adopted in other disciplines, often much more heavily than the original [[Pattern (architecture)|application of patterns to architecture]] as depicted the book ''[[A Pattern Language]]''. Recent examples include [[software design pattern]]s in software engineering and, more generally, [[Architectural pattern (computer science)|architectural patterns in computer science]], as well as [[interaction design pattern]]s. [[Pedagogical patterns]] are used to document good practices in teaching. The book ''Liberating Voices: A Pattern Language for Communication Revolution'', containing [http://www.publicsphereproject.org/patterns/lv 136 patterns] for using information and communication to promote sustainability, democracy and positive social change, was published in 2008. The deck "Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings" was published in 2011. Recently, patterns were also introduced into [[systems architecture]] design.&lt;ref&gt;{{cite web|last=Hein|first=Andreas|title=Adopting Patterns for Space Mission and Space Systems Architecting|url=http://www.academia.edu/2110976/A.M._Hein_Adopting_Patterns_for_Space_Mission_and._Space_Systems_Architecting_|work=5 th International Workshop on System &amp; Concurrent Engineering for Space ApplicationsSECESA 2012|accessdate=2 March 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Hein|first=Andreas|title=Project Icarus: Stakeholder Scenarios for an Interstellar Exploration Program|url=http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM|work=Journal of the British Interplanetary Society, 64, 224-233, 2011|accessdate=2 March 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Cloutier|first=Robert|title=The Concept of Reference Architectures|url=http://www.calimar.com/TheConceptOfReferenceArchitectures.pdf|work=Systems Engineering Vol. 13, No. 1, 2010|accessdate=2 March 2013}}&lt;/ref&gt;  [[Chess]] [[Chess strategy|strategy]] and [[Chess tactics|tactics]] involve many patterns from [[Chess opening|opening]] to [[checkmate]].

==See also==
* [[Feng shui]]
* [[Method engineering]]
* [[Rule of thumb]]
* [[Typology (urban planning and architecture)]]

==References==
{{Reflist}}

==Further reading==
* Christopher Alexander, Sara Ishikawa, Murray Silverstein (1974). 'A Collection of Patterns which Generate Multi-Service Centres' in Declan and Margrit Kennedy (eds.): ''The Inner City.'' Architects Year Book 14, Elek, London. ISBN 0 236 15431 1.
* Alexander, C. (1977). ''[[A Pattern Language: Towns, Buildings, Construction]]''. USA: [[Oxford University Press]]. ISBN 978-0-19-501919-3.
* Alexander, C. (1979). ''The Timeless Way of Building''. USA: Oxford University Press. ISBN 978-0-19-502402-9.
* Schuler, D. (2008). ''Liberating Voices: A Pattern Language for Communication Revolution''. USA: [[MIT Press]]. ISBN 978-0-262-69366-0.
* Leitner, Helmut (2015): ''Pattern Theory: Introduction and Perspectives on the Tracks of Christopher Alexander''. ISBN 1505637430.

==External links==

===About patterns in general===
* [http://www.c2.com/cgi/wiki?TipsForWritingPatternLanguages Tips For Writing Pattern Languages], by [[Ward Cunningham]]
* [http://www.gardenvisit.com/landscape/architecture/3.1-patternlanguage.htm Essay on the pattern language as it relates to urban design]
* [http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM Use of patterns for scenario development for large scale aerospace projects]
* [http://torgronsund.wordpress.com/2010/01/06/lean-startup-business-model-pattern/ Lean Startup Business Model Pattern]
* [http://www.informit.com/articles/printerfriendly.aspx?p=30084 What Is a Quality Use Case?] from the book ''Patterns for Effective Use Cases''
* [http://groupworksdeck.org/what-we-mean-by-pattern Characteristics of group facilitation patterns]

===Online pattern collections===
* [http://www.patternlanguage.com/ patternlanguage.com], by the Center for Environmental Structure
* [http://www.fusedgrid.ca/ Fused Grid] &#8211; A Contemporary Urban Pattern "a collection and synthesis of neighbourhood patterns"
* [http://www.reliableprosperity.net ReliableProsperity.net] &#8211; Patterns for building a "restorative, socially just, and reliably prosperous society"
* [http://www.hcipatterns.org/ hcipatterns.org] &#8211; Patterns for HCI
* [http://www.c2.com/cgi/wiki?PatternIndex The Portland Pattern Repository]
* [http://developer.yahoo.com/ypatterns Yahoo! Design Pattern Library]
* [http://groupworksdeck.org Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings] &#8211; A pattern language of group process
* [http://liveingreatness.com/core-protocols/ The Core Protocols] &#8211; A set of team communication patterns
* [http://www.publicsphereproject.org/patterns/lv Liberating Voices! Pattern Language Project] &#8212; Short versions of patterns available in [http://www.publicsphereproject.org/patterns_arabic Arabic], [http://www.publicsphereproject.org/patterns_chinese Chinese], and [http://www.publicsphereproject.org/patterns_spanish Spanish]

{{DEFAULTSORT:Pattern Language}}

[[Category:Architectural theory]]
[[Category:Cybernetics]]
[[Category:Design]]
[[Category:Knowledge representation]]
[[Category:Linguistics]]

[[fi:Suunnittelumalli]]</text>
      <sha1>jdjl2w2ch2ezm2gr2w2y1uc0oooyrw6</sha1>
    </revision>
  </page>
  <page>
    <title>OntoUML</title>
    <ns>0</ns>
    <id>33378172</id>
    <revision>
      <id>722714629</id>
      <parentid>715170597</parentid>
      <timestamp>2016-05-29T19:18:16Z</timestamp>
      <contributor>
        <ip>194.228.32.214</ip>
      </contributor>
      <comment>an UML -&gt; a UML</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2806" xml:space="preserve">{{Infobox technology standard
| name = OntoUML
| year_started = 2005
| domain = [[Conceptual_model|Conceptual Modeling]]
| base_standards = [[Unified Foundational Ontology (UFO)]]
| related_standards = [[Unified Modeling Language|UML]]
| organization = [[Ontology &amp; Conceptual Modeling Research Group (NEMO)]]
| website = {{URL|http://nemo.inf.ufes.br/}}
}}
'''OntoUML''' is a [[Ontology|ontologically]] well-founded language for [[Ontology_(information_science)|Ontology]]-driven [[Conceptual_model|Conceptual Modeling]]. '''OntoUML''' is built as a [[Unified Modeling Language|UML]] extension based on the [[Unified Foundational Ontology (UFO)]]. UFO was created by Giancarlo Guizzardi in his Ph.D. thesis&lt;ref&gt;{{cite book|last1=Guizzardi|first1=Giancarlo|title=Ontological foundations for structural conceptual models|date=2005|publisher=Enschede: Telematica Instituut Fundamental Research Series|url=http://doc.utwente.nl/50826/1/thesis_Guizzardi.pdf}}&lt;/ref&gt; and used to evaluate and re-design a fragment of the UML 2.0 metamodel. Giancarlo Guizzardi is a lead researcher at the [[Ontology &amp; Conceptual Modeling Research Group (NEMO)]]&lt;ref&gt;{{cite web|title=Ontology &amp; Conceptual Modeling Research Group (NEMO)|url=http://nemo.inf.ufes.br/}}&lt;/ref&gt; located at the [[Federal University of Esp&#237;rito Santo|Federal University of Esp&#237;rito Santo (UFES)]] in [[Vit&#243;ria,_Esp&#237;rito_Santo|Vit&#243;ria]] city, state of [[Esp&#237;rito Santo]], [[Brazil]].

NEMO created an OntoUML infrastructure&lt;ref&gt;{{cite web|title=OntoUML infrastructure|url=http://code.google.com/p/rcarraretto/}}&lt;/ref&gt; using [[Eclipse_Modeling_Framework|Eclipse EMF]] for '''OntoUML''' model manipulation which serve as a basis for its tool support. NEMO has been actively working on tool support for the '''OntoUML''' Conceptual Modeling Language, respectively on:

# extensions of [[Unified Modeling Language|UML]] production-grade tools to support OntoUML, namely, the MDG for [[Enterprise_Architect_(software)|Enterprise Architect]].
# a standalone tool called [[OntoUML Lightweight Editor (OLED)]]&lt;ref&gt;{{cite web|title=OntoUML lightweight editor (OLED) repository|url=https://github.com/nemo-ufes/ontouml-lightweight-editor}}&lt;/ref&gt; to the development, evaluation and implementation of domain ontologies.
# a legacy OntoUML editor&lt;ref&gt;{{cite web|title=Legacy OntoUML editor|url=https://github.com/nemo-ufes/ontouml-editor-eclipse}}&lt;/ref&gt; based on an old version of [[Graphical_Modeling_Framework|Eclipse/GMF]].

Check out a list of all publications of NEMO about ontologies and OntoUML: &lt;ref&gt;{{cite web|title=NEMO publications|url=http://nemo.inf.ufes.br/publications/}}&lt;/ref&gt;

== References ==
&lt;references /&gt;

[[Category:UML tools]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]</text>
      <sha1>crzfhpm2vq2j4uz3fv5s0tfe8kpp5y1</sha1>
    </revision>
  </page>
  <page>
    <title>Moovly</title>
    <ns>0</ns>
    <id>50643936</id>
    <revision>
      <id>750926766</id>
      <parentid>737858018</parentid>
      <timestamp>2016-11-22T08:18:41Z</timestamp>
      <contributor>
        <ip>81.83.30.100</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4273" xml:space="preserve">{{Orphan|date=May 2016}}
{{Infobox company
| name             = Moovly
| type             = [[Public company|Public]]
| key_people       = Brendon Grunewald (Co-Founder and CEO), Geert Coppens (Co-Founder and CTO)
| industry         = [[Internet Marketing]]
| products         = Moovly web-based animation software video creation and generation platform.
| foundation       = {{Start date and age|2012}}
| location_city    = [[Vancouver]]
| location_country = [[Canada]]
| homepage         = {{URL|www.moovly.com}}
}}

'''Moovly''' is a company that provides a cloud-based platform [[Software as a service|(SaaS)]] that enables users to create and generate multimedia content: animated videos, video presentations, animated info graphics and any other video content that includes a mix of animation and motion graphics.&lt;ref&gt;Flanders Today, [http://www.flanderstoday.eu/business/moovly-offers-new-online-tool-develop-multimedia-designs Flanders Today], September 13th, 2013, "Moovly offers new online tool to develop multimedia designs"&lt;/ref&gt;

== History ==
Moovly was originally founded in Belgium in November 2012 by Brendon Grunewald and Geert Coppens with the vision of "becoming the number one platform for engaging customisable multimedia content creation". The company's mission is to "Enable everyone to create engaging multimedia content by making it affordable, Intuitive and Simple".  The company has seen steady subscriber growth from multinational corporations, small and medium business as well as educational institutions.. The company initially secured several rounds of external investment to fund its growth, and in July 2016, listed on the Toronto Venture Stock Exchange as Moovly Media Inc under the symbol MVY (TSX.V: MVY).&lt;ref&gt;Levak, Rachel, [https://www.crunchbase.com/organization/moovly/funding-rounds Crunchbase] , Apr 09, 2015, "Moovly - Funding Rounds |  CrunchBase"&lt;/ref&gt;

== Product ==
Moovly is a cloud based digital media and content creation software platform. Content can be created via various interfaces, including the editor as well as simple, custom-made video generation interfaces.

Using a combination of uploaded images, videos and sounds, as well as a pre-defined library of objects, users are able to quickly assemble new animated content. The final videos or presentations can be downloaded as an [[MP4]] for example, or published on a variety of video platforms.

Moovly provides a feature-rich free license allowing users to create animated videos&lt;ref&gt;Wilson, Li&#233;vano, [http://jsk.stanford.edu/news-notes/2014/10-things-you-need-to-know-when-producing-a-data-animation-for-a-newsroom/ John S. Knight Journalism Fellowships at Stanford], Sep 04th, 2014, "10 things you need to know when producing a data animation for a newsroom"&lt;/ref&gt; that can be exported to [[Facebook]] and [[YouTube]], as well as premium licenses for advanced and professional use. The free videos include the Moovly branding. As an educational tool&lt;ref&gt;Hart, Jane, [http://c4lpt.co.uk/top100tools/moovly/ Centre for Learning &amp; Performance Technologies], Sep 21, 2015, " Top 100 Tools for Learning | Centre for Learning &amp; Performance Technologies"&lt;/ref&gt; and for educational purposes,&lt;ref&gt;Janssens, Mieke, [http://www.klascement.eu/sites/60764/ KlasCement Educational Resources Network], Sep 22, 2015, "Moovly: Create animated videos and presentations | KlasCement Educational Resources Network"&lt;/ref&gt; Moovly offers specific licenses.&lt;ref&gt;[https://www.moovly.com/education-solutions Moovly Website]&lt;/ref&gt;

Companies and brands can use their own library of animated graphics, their own fonts and standard color set.&lt;ref&gt;Waldron, John, [http://www.markitwrite.com/moovly/ markITwrite] , Nov, 2014, "Moovly: Video Animation for Everyone |  markITwrite"&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* {{official website|https://www.moovly.com/}}
* [http://web.tmxmoney.com/company.php?qm_symbol=MVY&amp;locale=EN Toronto Venture Exchange Moovly Page]
* [http://www.bloomberg.com/profiles/companies/1291746D:BB-moovly-nv Bloomberg overview]

[[Category:Animation software]]
[[Category:Companies established in 2012]]
[[Category:Cloud applications]]
[[Category:Computer animation]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]</text>
      <sha1>1exwnuhwzlplp32slxdu3dh7s38ehbc</sha1>
    </revision>
  </page>
  </mediawiki>