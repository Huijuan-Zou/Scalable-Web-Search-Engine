<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Electronic documents</title>
    <ns>14</ns>
    <id>699814</id>
    <revision>
      <id>546483581</id>
      <parentid>525385068</parentid>
      <timestamp>2013-03-23T06:27:20Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 14 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7210821]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="119" xml:space="preserve">[[Category:Documents]]
[[Category:Digital media]]
[[Category:Information retrieval]]
[[Category:Electronic publishing]]</text>
      <sha1>t1806qygmvdgbyf72c9lq6hqh776vz9</sha1>
    </revision>
  </page>
  <page>
    <title>Information needs</title>
    <ns>0</ns>
    <id>11016342</id>
    <revision>
      <id>758495817</id>
      <parentid>753282882</parentid>
      <timestamp>2017-01-05T20:02:38Z</timestamp>
      <contributor>
        <ip>72.239.0.15</ip>
      </contributor>
      <comment>Undid revision 753282882 by [[Special:Contributions/112.201.218.139|112.201.218.139]] ([[User talk:112.201.218.139|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5840" xml:space="preserve">information need is an uncertainty that arises in an individual, and which the individual is believed to be satisfied by information.{{more footnotes|date=June 2015}}
The concept '''Information need''' is seldom, if ever, mentioned in the general literature about [[needs]], but is a common term in the literature of [[information science]]. According to Hj&#248;rland (1997) it is closely related to the concept of [[relevance]]: If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. 

It is often understood as an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The &#8216;information&#8217; and &#8216;need&#8217; in &#8216;information need&#8217; are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:
# The explanation of observed phenomena of information use or expressed need;
# The prediction of instances of information uses;
# The control and thereby improvement of the utilization of information manipulation of essentials conditions.

Information needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.

== Background ==

The concept of information needs was coined by an American information journalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (renamed Journal of the American Society of Information Science and Technology).

In this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.

According to Taylor, information need has four levels:
# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the &#8220;ideal question&#8221; &#8212; the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information
# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.
# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer&#8217;s doubts.
# The question as presented to the information system.

There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).

Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:
# When approached from the point of view of the scientist or technologists, these are studies of scientists&#8217; communication behaviour;
# When approached from the point of view of any communication medium, they are use studies;
# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.

William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:
# The scientist within his culture.
# The scientist within a political system.
# The scientist within a membership group.
# The scientist within a reference group.
# The scientist within an invisible college.
# The scientist within a formal organization.
# The scientist within a work team.
# The scientist within his own head.
# The scientist within a legal/economical system.
# The scientist within a formal.

==See also==
* [[Information retrieval]]
* [[Needs]]

==References==
* Hj&#248;rland, Birger (1997). Information seeking and subject representation. An activity-theoretical approach to information science. Westport, CO: Greenwood Press. 
* Menzel, Herbert. &#8220;Information Needs and Uses in Science and Technology.&#8221; Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.
* Paisley, William J. &#8220;Information Needs and Uses.&#8221; Annual Review of Information Science and Technology, Vol.3, Encyclop&#230;dia Britannica, Inc. Chicago 1968, pp.1-30.
* Taylor, Robert S. &#8220;The Process of Asking Questions&#8221; American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.
* Wilson, T.D. &#8220;On User Studies and Information Needs.&#8221; Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15

[[Category:Information retrieval]]</text>
      <sha1>nn3h4km3zgr2fwj2pmpns1neer6x4b2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Citation indices</title>
    <ns>14</ns>
    <id>24447333</id>
    <revision>
      <id>679712371</id>
      <parentid>678206464</parentid>
      <timestamp>2015-09-06T09:12:40Z</timestamp>
      <contributor>
        <username>Colonies Chris</username>
        <id>577301</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="286" xml:space="preserve">{{Cat main|Citation index}}
{{distinguish|Category:Bibliographic databases and indexes|Category:Citation metrics}}

[[Category:Bibliometrics|Indices]]
[[Category:Reference works]]
[[Category:Indexes]]
[[Category:Information retrieval]]
[[Category:Bibliographic databases and indexes| ]]</text>
      <sha1>1xjzb686lb1dxkxr26wqn5gqxtu4hi7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval evaluation</title>
    <ns>14</ns>
    <id>46965336</id>
    <revision>
      <id>666933492</id>
      <parentid>666713992</parentid>
      <timestamp>2015-06-14T17:58:19Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>The main overview for this category is at {{section link|Information retrieval|Performance and correctness_measures}}.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="154" xml:space="preserve">The main overview for this category is at {{section link|Information retrieval|Performance and correctness_measures}}.

[[Category:Information retrieval]]</text>
      <sha1>m98afgn44mtneoi1kmkjgfk5zton892</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval techniques</title>
    <ns>14</ns>
    <id>46965346</id>
    <revision>
      <id>666714185</id>
      <timestamp>2015-06-13T03:44:13Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34" xml:space="preserve">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval genres</title>
    <ns>14</ns>
    <id>46965446</id>
    <revision>
      <id>666716254</id>
      <timestamp>2015-06-13T04:14:38Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34" xml:space="preserve">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval researchers</title>
    <ns>14</ns>
    <id>46967136</id>
    <revision>
      <id>666733941</id>
      <timestamp>2015-06-13T08:06:20Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>[[WP:AES|&#8592;]]Created page with '[[Category:Computer scientists by field of research|Information retrieval]] [[Category:Information retrieval]]'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="110" xml:space="preserve">[[Category:Computer scientists by field of research|Information retrieval]]
[[Category:Information retrieval]]</text>
      <sha1>3gw4qibpvqdsspfc9rlr58azftr4b7x</sha1>
    </revision>
  </page>
  <page>
    <title>Dwell time (information retrieval)</title>
    <ns>0</ns>
    <id>48317971</id>
    <revision>
      <id>726341772</id>
      <parentid>694217514</parentid>
      <timestamp>2016-06-21T15:24:15Z</timestamp>
      <contributor>
        <ip>92.27.3.65</ip>
      </contributor>
      <comment>Added a new reference to better explain the topic of dwell time</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1197" xml:space="preserve">{{one source|date=October 2015}}

In [[information retrieval]], '''dwell time''' denotes the time which a user spends viewing a document after clicking a link on a [[Search engine results page|search engine results page (SERP)]]. Dwell time is the duration between when a user clicks on a [[search engine]] result, and when the user returns from that result, or the user is otherwise seen to have left the result. Dwell time is a [[Relevance (information retrieval)|relevance]] indicator of the search result correctly satisfying the [[Information needs|intent]] of the user. Short dwell times indicate the user's query intent was not satisfied by viewing the result. Long dwell times indicate the user's query intent was satisfied.&lt;ref&gt;{{Cite web|url=https://blogs.bing.com/webmaster/2011/08/02/how-to-build-quality-content/|title=How To Build Quality Content|publisher=Bing blogs}}&lt;/ref&gt; 

==References==
&lt;references /&gt;2. [https://www.impression.co.uk/blog/4004/dwell-time/ "Understanding dwell time and its impact on search rankings"] Impression Digital
[[Category:Information retrieval]]
[[Category:Information retrieval evaluation]]
[[Category:Internet search engines]]

{{web-software-stub}}</text>
      <sha1>01hndryu1aknulzl4p5fxjodbu3lz9f</sha1>
    </revision>
  </page>
  <page>
    <title>Data bank</title>
    <ns>0</ns>
    <id>40990</id>
    <revision>
      <id>663186027</id>
      <parentid>641573880</parentid>
      <timestamp>2015-05-20T02:08:42Z</timestamp>
      <contributor>
        <username>Hugo999</username>
        <id>3006008</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1261" xml:space="preserve">In [[telecommunication]]s, a '''data bank''' is a repository of information on one or more subjects that is organized in a way that facilitates local or remote information retrieval. A data bank may be either centralized or decentralized.
In computers the data bank is the same as in telecommunication (i.e. it is the repository of data. The data in the data bank can be things such as credit card transactions or it can be any data base of a company where large quantities of queries are being processed on daily bases). 
 
'''Data bank''' may also refer to an organization primarily concerned with the construction and maintenance of a [[database]].

== See also ==

* [[Star Wars Databank]]
* [[Protein Data Bank]]
* [[National Trauma Data Bank]]
* [[memory bank]]
* [[International Tree-Ring Data Bank]]
* [[Hazardous Substances Data Bank]]
* [[electron microscopy data bank]]
* [[Dortmund Data Bank]]
* [[Casio Databank]]
* [[conformational dynamics data bank]]
* [[Databank Systems Limited]] a former New Zealand banking agency

==Sources==
*{{FS1037C MS188}}
*''[[The American Heritage Dictionary of the English Language]], Fourth Edition''. [[Houghton Mifflin]], 2000.

==External links==
{{wiktionary}}

[[Category:Data management]]


{{telecomm-stub}}</text>
      <sha1>idq0yjp8xs0sof0bjtqdn61cjfgdx7r</sha1>
    </revision>
  </page>
  <page>
    <title>Data Transformation Services</title>
    <ns>0</ns>
    <id>1605292</id>
    <revision>
      <id>722695616</id>
      <parentid>681827771</parentid>
      <timestamp>2016-05-29T17:14:54Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* References */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9193" xml:space="preserve">'''Data Transformation Services''', or '''DTS''', is a set of objects and utilities to allow the automation of [[extract, transform, load|extract, transform and load]] operations to or from a database. The objects are DTS packages and their components, and the utilities are called DTS tools. DTS was included with earlier versions of [[Microsoft SQL Server]], and was almost always used with SQL Server databases, although it could be used independently with other databases.

DTS allows data to be transformed and loaded from [[heterogeneous]] sources using [[OLE DB]], [[ODBC]], or text-only files, into any supported [[database]]. DTS can also allow automation of data import or transformation on a scheduled basis, and can perform additional functions such as [[File Transfer Protocol|FTPing]] files and executing external programs. In addition, DTS provides an alternative method of version control and backup for packages when used in conjunction with a version control system, such as [[Microsoft Visual SourceSafe]].
[[Image:DTS Designer screenshot.PNG|right|thumb|300px|Here a DTS package is edited with DTS Designer in [[Windows XP]].]] DTS has been superseded by [[SQL Server Integration Services]] in later releases of Microsoft SQL Server though there was some backwards compatibility and ability to run DTS packages in the new SSIS for a time.

__TOC__
{{clear}}

==History==

In SQL Server versions 6.5 and earlier, [[database administrators]] (DBAs) used [[SQL Server Transfer Manager]] and [[Bulk Copy Program]], included with SQL Server, to transfer data. These tools had significant shortcomings, and many{{quantify|date=May 2014}} DBAs used third-party tools such as [[Pervasive Data Integrator]] to transfer data more flexibly and easily. With the release of SQL Server 7 in 1998, "Data Transformation Services" was packaged with it to replace all these tools.

SQL Server 2000 expanded DTS functionality in several ways. It introduced new types of tasks, including the ability to [[File Transfer Protocol|FTP]] files, move databases or database components, and add messages into [[Microsoft Message Queuing|Microsoft Message Queue]]. DTS packages can be saved as a Visual Basic file in SQL Server 2000, and this can be expanded to save into any COM-compliant language. Microsoft also integrated packages into [[Windows 2000 security]] and made DTS tools more user-friendly;  tasks can accept input and output parameters.

DTS comes with all editions of SQL Server 7 and 2000, but was superseded by [[SQL Server Integration Services]] in the Microsoft SQL Server 2005 release in 2005.

== DTS packages ==
The DTS package is the fundamental logical component of DTS; every DTS object is a [[Information hiding|child component]] of the package. Packages are used whenever one modifies data using DTS. All the [[metadata]] about the data transformation is contained within the package. Packages can be saved directly in a SQL Server, or can be saved in the [[Microsoft Repository]] or in [[Component Object Model|COM]] files. SQL Server 2000 also allows a programmer to save packages in a [[Visual Basic]] or other language file (when stored to a VB file, the package is actually scripted&#8212;that is, a VB script is executed to dynamically create the package objects and its component objects).

A package can contain any number of [[ActiveX Data Objects|connection objects]], but does not have to contain any. These allow the package to read data from any [[OLE DB]]-compliant data source, and can be expanded to handle other sorts of data. The functionality of a package is organized into ''tasks'' and ''steps''.

A DTS Task is a discrete set of functionalities executed as a single step in a DTS package. Each task defines a work item to be performed as part of the data movement and data transformation process or as a job to be executed.

Data Transformation Services supplies a number of tasks that are part of the DTS [[object model]] and that can be accessed graphically through the DTS Designer or accessed programmatically. These tasks, which can be configured individually, cover a wide variety of data copying, data transformation and notification situations. For example, the following types of tasks represent some actions that you can perform by using [[Data transformation service|DTS]]: executing a single SQL statement, sending an email, and transferring a file with FTP.

A step within a DTS package describes the order in which tasks are run and the precedence constraints that describe what to do in the case damage or of failure. These steps can be executed sequentially or in parallel.

Packages can also contain [[global variable]]s which can be used throughout the package. SQL Server 2000 allows input and output parameters for tasks, greatly expanding the usefulness of global variables. DTS packages can be edited, password protected, scheduled for execution, and retrieved by version.

==DTS tools==
DTS tools packaged with SQL Server include the DTS wizards, DTS Designer, and DTS Programming Interfaces.

===DTS wizards===
The DTS [[Wizard (software)|wizards]] can be used to perform simple or common DTS tasks. These include the ''Import/Export Wizard'' and the ''Copy of Database Wizard''. They provide the simplest method of copying data between [[OLE DB]] data sources. There is a great deal of functionality that is not available by merely using a wizard. However, a package created with a wizard can be saved and later altered with one of the other DTS tools.

A ''Create Publishing Wizard'' is also available to schedule packages to run at certain times. This only works if [[SQL Server Agent]] is running; otherwise the package will be scheduled, but will not be executed.

===DTS Designer===
The DTS Designer is a [[graphical tool]] used to build complex DTS Packages with workflows and event-driven logic. DTS Designer can also be used to edit and customize DTS Packages created with the DTS wizard.

Each connection and task in DTS Designer is shown with a specific [[Icon (computing)|icon]]. These icons are joined with precedence constraints, which specify the order and requirements for tasks to be run. One task may run, for instance, only if another task succeeds (or fails). Other tasks may run concurrently.

The DTS Designer has been criticized for having unusual quirks and limitations, such as the inability to visually [[copy and paste]] multiple tasks at one time. Many of these shortcomings have been overcome in [[SQL Server Integration Services]], DTS's successor.

===DTS Query Designer===
A graphical tool used to build [[Information retrieval|queries]] in DTS.

===DTS Run Utility===
DTS Packages can be run from the command line using the DTSRUN Utility.&lt;BR/&gt;
The utility is invoked using the following  syntax: 
&lt;PRE&gt;
dtsrun /S server_name[\instance_name]
        { {/[~]U user_name [/[~]P password]} | /E }
    ]
    {    
        {/[~]N package_name }
        | {/[~]G package_guid_string}
        | {/[~]V package_version_guid_string}
    }
    [/[~]M package_password]
    [/[~]F filename]
    [/[~]R repository_database_name]
    [/A global_variable_name:typeid=value] 
    [/L log_file_name]
    [/W NT_event_log_completion_status]
    [/Z] [/!X] [/!D] [/!Y] [/!C]
]
&lt;/PRE&gt;

When passing in parameters which are mapped to Global Variables, you are required to include the typeid. This is rather difficult to find on the Microsoft site. Below are the TypeIds used in passing in these values.

{| class="wikitable sortable"
! Type !! typeid
|-
| Boolean
| align="right" | 11
|-
| Currency
| align="right" | 6
|-
| Date
| align="right" | 7
|-
| Decimal
| align="right" | 14
|-
| HRESULT
| align="right" | 25
|-
| Int
| align="right" | 22
|-
| Integer (1-byte)
| align="right" | 16
|-
| Integer (8-byte)
| align="right" | 20
|-
| Integer (small)
| align="right" | 2
|-
| LPWSTR
| align="right" | 31
|-
| Pointer
| align="right" | 26
|-
| Real (4-byte)
| align="right" | 4
|-
| Real (8-byte)
| align="right" | 5
|-
| String
| align="right" | 8
|-
| Unsigned int (1-byte)
| align="right" | 17
|-
| Unsigned int (2-byte)
| align="right" | 18
|-
| Unsigned int (4-byte)
| align="right" | 19
|-
| Unsigned int (1-byte)
| align="right" | 21
|-
| Unsigned int
| align="right" | 23
|}

==See also==

* [[OLAP]]
* [[Data warehouse]]
* [[Data mining]]
* [[SQL Server Integration Services]]
* [[Meta Data Services]]

==References==
* {{cite book |author1=Chaffin, Mark |author2=Knight, Brian |author3=Robinson, Todd | title=Professional SQL Server 2000 DTS | publisher=[[Wrox Press]] (Wiley Publishing, Inc.) | year=2003 | isbn=0-7645-4368-7}}

== External links ==
* [http://msdn2.microsoft.com/en-us/library/aa933484(SQL.80).aspx Microsoft SQL Server: Data Transformation Services (DTS)]
* [http://www.sqldts.com/ SQL DTS unique DTS information resource]
* [http://support.microsoft.com/kb/238912 Understanding Microsoft Repository]
* [http://pragmaticworks.com/Resources/webinars/Default.aspx DTS Videos &amp; Training]
* [http://www.softrus.org/dts/ DTS Documenter]

[[Category:Microsoft database software]]
[[Category:Data management]]
[[Category:Extract, transform, load tools]]
[[Category:Microsoft server technology]]</text>
      <sha1>ihca9ivm0abzvype6xbkju92cpudgdt</sha1>
    </revision>
  </page>
  <page>
    <title>Savepoint</title>
    <ns>0</ns>
    <id>1544409</id>
    <revision>
      <id>729770985</id>
      <parentid>677569226</parentid>
      <timestamp>2016-07-14T12:55:07Z</timestamp>
      <contributor>
        <ip>94.116.126.68</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1867" xml:space="preserve">{{For|save points in video games|Saved game}}
{{refimprove|date=September 2014}}

A '''savepoint''' is a way of implementing subtransactions (also known as [[nested transaction]]s) within a [[relational database management system]] by indicating a point within a [[database transaction|transaction]] that can be "[[Rollback (data management)|rolled back to]]" without affecting any work done in the transaction before the savepoint was created. Multiple savepoints can exist within a single transaction. Savepoints are useful for implementing complex error recovery in database applications. If an error occurs in the midst of a multiple-statement transaction, the application may be able to recover from the error (by rolling back to a savepoint) without needing to abort the entire transaction.

A savepoint can be declared by issuing a &lt;code&gt;SAVEPOINT ''name''&lt;/code&gt; statement. All changes made after a savepoint has been declared can be undone by issuing a &lt;code&gt;ROLLBACK TO SAVEPOINT ''name''&lt;/code&gt; command. Issuing &lt;code&gt;RELEASE SAVEPOINT ''name''&lt;/code&gt; will cause the named savepoint to be discarded, but will not otherwise affect anything. Issuing the commands &lt;code&gt;ROLLBACK&lt;/code&gt; or &lt;code&gt;COMMIT&lt;/code&gt; will also discard any savepoints created since the start of the main transaction.[http://docs.oracle.com/cd/B19306_01/appdev.102/b14261/savepoint_statement.htm]

Savepoints are supported in some form or other in database systems like [[PostgreSQL]], [[Oracle database|Oracle]], [[Microsoft SQL Server]], [[MySQL]], [[IBM DB2|DB2]], [[SQLite]] (since 3.6.8), [[Firebird (database server)|Firebird]], [[H2_(DBMS)|H2 Database Engine]], and [[Informix]] (since version 11.50xC3). Savepoints are also defined in the [[SQL#Interoperability_and_standardization|SQL standard]].

{{databases}}

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>fontevph04t7my196thmt8heaweitxv</sha1>
    </revision>
  </page>
  <page>
    <title>Three-phase commit protocol</title>
    <ns>0</ns>
    <id>2044655</id>
    <revision>
      <id>758801696</id>
      <parentid>708444010</parentid>
      <timestamp>2017-01-07T17:41:47Z</timestamp>
      <contributor>
        <username>Animaala</username>
        <id>30048422</id>
      </contributor>
      <comment>/* Coordinator */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5398" xml:space="preserve">In [[computer networking]] and [[database]]s, the '''three-phase commit protocol''' ('''3PC''')&lt;ref name=3PC&gt;{{cite journal
 | last = Skeen
 | first = Dale
 | title = A Formal Model of Crash Recovery in a Distributed System
 | journal = IEEE Transactions on Software Engineering
 | volume = 9
 | issue = 3
 |date=May 1983
 | pages = 219&#8211;228
 | doi = 10.1109/TSE.1983.236608
 | last2 = Stonebraker
 | first2 = M.
}}&lt;/ref&gt; is a [[distributed algorithm]] which lets all nodes in a [[distributed system]] agree to [[Commit (data management)|commit]] a [[database transaction|transaction]].  Unlike the [[two-phase commit protocol]] (2PC) however, 3PC is non-blocking.  Specifically, 3PC places an upper bound on the amount of time required before a transaction either commits or [[Abort (computing)|aborts]].  This property ensures that if a given transaction is attempting to commit via 3PC and holds some [[lock (computer science)|resource locks]], it will release the locks after the timeout.

==Protocol Description==
In describing the protocol, we use terminology similar to that used in the [[two-phase commit protocol]].  Thus we have a single coordinator site leading the transaction and a set of one or more cohorts being directed by the coordinator.

&lt;center&gt;[[Image:Three-phase commit diagram.png]]&lt;/center&gt;

===Coordinator===
#The coordinator receives a transaction request.  If there is a failure at this point, the coordinator aborts the transaction (i.e. upon recovery, it will consider the transaction aborted).  Otherwise, the coordinator sends a canCommit? message to the cohorts and moves to the waiting state.
#If there is a failure, timeout, or if the coordinator receives a No message in the waiting state, the coordinator aborts the transaction and sends an abort message to all cohorts.  Otherwise the coordinator will receive Yes messages from all cohorts within the time window, so it sends preCommit messages to all cohorts and moves to the prepared state.
#If the coordinator succeeds in the prepared state, it will move to the commit state.  However if the coordinator times out while waiting for an acknowledgement from a cohort, it will abort the transaction.  In the case where an acknowledgement is received from the majority of cohorts, the coordinator moves to the commit state as well.

===Cohort===
#The cohort receives a canCommit? message from the coordinator.  If the cohort agrees it sends a Yes message to the coordinator and moves to the prepared state.  Otherwise it sends a No message and aborts.  If there is a failure, it moves to the abort state.
#In the prepared state, if the cohort receives an abort message from the coordinator, fails, or times out waiting for a commit, it aborts.  If the cohort receives a  preCommit message, it sends an '''[[acknowledgement (data networks)|ACK]]''' message back and awaits a final commit or abort.
#If, after a cohort member receives a  preCommit  message, the coordinator fails or times out, the cohort member goes forward with the commit.

==Motivation==
A [[Two-phase commit protocol]] cannot dependably recover from a failure of both the coordinator and a cohort member during the '''Commit phase'''.  If only the coordinator had failed, and no cohort members had received a commit message, it could safely be inferred that
no commit had happened.  If, however, both the coordinator and a cohort member
failed, it is possible that the failed cohort member was the first to be notified, and had
actually done the commit.  Even if a new coordinator is selected, it cannot 
confidently proceed with the operation until it has received an agreement from
all cohort members ... and hence must block until all cohort members respond.

The Three-phase commit protocol eliminates this problem by introducing the Prepared to commit
state.  If the coordinator fails before sending preCommit messages, the cohort will
unanimously agree that the operation was aborted.  The coordinator will not send out a doCommit
message until all cohort members have '''ACK'''ed that they are '''Prepared to commit'''. 
This eliminates the possibility that any cohort member actually completed the 
transaction before all cohort members were aware of the decision to do so 
(an ambiguity that necessitated indefinite blocking in the [[Two-phase commit protocol]]).

==Disadvantages==
The main disadvantage to this algorithm is that it cannot recover in the event the network is segmented in any manner. The original 3PC algorithm assumes a fail-stop model, where processes fail by crashing and crashes can be
accurately detected, and does not work with network partitions or asynchronous communication.

Keidar and Dolev's E3PC&lt;ref name=E3PC&gt;{{cite journal|last=Keidar|first=Idit|author2=Danny Dolev |title=Increasing the Resilience of Distributed and Replicated Database Systems|journal=Journal of Computer and System Sciences (JCSS)|volume=57|issue=3|date=December 1998|pages=309&#8211;324|
url=http://webee.technion.ac.il/~idish/Abstracts/jcss.html|doi=10.1006/jcss.1998.1566}}&lt;/ref&gt; algorithm eliminates this disadvantage.

The protocol requires at least 3 round trips to complete, needing a minimum of 3 round trip times (RTTs). This is potentially a long latency to complete each transaction.

==References==
{{Reflist}}

==See also==
*[[Two-phase commit protocol]]

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>cp52nbp3b3kv308nle4anq4s4isqktr</sha1>
    </revision>
  </page>
  <page>
    <title>Content format</title>
    <ns>0</ns>
    <id>7428842</id>
    <revision>
      <id>727805879</id>
      <parentid>714242064</parentid>
      <timestamp>2016-07-01T10:36:31Z</timestamp>
      <contributor>
        <ip>189.222.175.222</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5457" xml:space="preserve">[[File:Pcm.svg|200px|thumb|right|Graphical representations of electrical data: analog audio content format (red), 4-bit digital pulse code modulated content format (blue).]][[File:Mi Fu-On Calligraphy.jpg|200px|thumb|right|Chinese calligraphy written in a language content format by [[Song Dynasty]] (A.D. 1051-1108) poet [[Mi Fu]].]][[File:12345678901-2-23456 barcode UPC(A).svg|200px|thumb|A series of numbers encoded in a Universal Product Code digital numeric content format.]]A '''content format''' is an [[encoding|encoded]] format for converting a specific type of [[data]] to displayable [[information]]. [[Content (media and publishing)|Content]] formats are used in [[recording]] and [[Telecommunication|transmission]] to prepare data for [[Information processing|observation]] or [[interpreting|interpretation]].&lt;ref&gt;Bob Boiko, ''Content Management Bible,'' Nov 2004 pp:79, 240, 830&lt;/ref&gt;&lt;ref&gt;[[Ann Rockley]], ''Managing Enterprise Content: A Unified Content Strategy,'' Oct 2002 pp:269, 320, 516&lt;/ref&gt; This includes both [[Analog signal|analog]] and [[digitizing|digitized]] content. Content formats may be recorded and read by either natural or manufactured tools and mechanisms.

In addition to converting data to information, a content format may include the [[encryption]] and/or [[Scrambler|scrambling]] of that information.&lt;ref&gt;Jessica Keyes, ''Technology Trendlines,'' Jul  1995 pp:201&lt;/ref&gt; Multiple content formats may be contained within a single section of a [[storage medium]] (e.g. [[Multitrack recording|track]], [[disk sector]], [[computer file]], [[document]], [[page (paper)|page]], [[Column (typography)|column]]) or transmitted via a single [[Channel (communications)|channel]] (e.g. [[wire]], [[carrier wave]]) of a [[transmission medium]]. With [[multimedia]], multiple tracks containing multiple content formats are presented simultaneously. Content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format (e.g. [[spectrogram]], [[pictogram]]). 

Observable data is often known as [[raw data]], or raw content.&lt;ref&gt;Oge Marques and Borko Furht, ''Content-Based Image and Video Retrieval,'' April 2002 pp:15&lt;/ref&gt; A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].

There has been a countless number of content formats throughout history. The following are examples of some common content formats and content format categories (covering: sensory experience, model, and language used for encoding information):
{| width="65%"
|- valign=top
|width="50%"|
*Audio data encoding&lt;ref&gt;David Austerberry, ''The Technology of Video and Audio Streaming, Second Edition,'' Sep 2004 pp: 328&lt;/ref&gt;
**[[Audio coding format]]
**[[Analog signal|Analog]] [[Audio frequency|audio]] data
**[[Stereophonic sound]] formats
**[[Digital audio]] data
**[[Synthesizer]] [[Music sequencer|sequences]]
*Visual data encoding
**[[Art techniques and materials|Hand rendering materials]]
**[[Film speed]] formats
**[[Pixel]] [[coordinates]] data
**[[Color space]] data
**[[Vector graphics|Vector graphic]] [[coordinates]]/[[dimensions]]
**[[Texture mapping]] formats
**[[3D display]] formats
**[[Holographic]] formats
**[[Display resolution]] formatting
*[[Motion graphics]] encoding
**[[Video coding format]]
**[[Frame rate]] data
**[[Video]] data&lt;ref&gt;M. Ghanbari, ''Standard Codecs: Image Compression to Advanced Video Coding,'' Jun 2003 pp:364&lt;/ref&gt;
**[[Computer animation]] formats
*Instruction encoding
**[[Musical notation]]
**[[Computer language]]
**[[Traffic signals]]
|width="50%"|
*[[Language|Natural languages formats]]
**[[Writing system]]s
**[[Phonetic]]
**[[Sign language]]s
*[[Signal (electronics)|Communication signaling formats]]
*[[Code]] formats
*Expert language formats
**[[Graphic organizer]]
**[[Statistical model]]
**[[Table of elements#Standard periodic table|Table of elements]]
**[[DNA sequence]]
**[[Human anatomy]]
**[[Biometrics|Biometric data]]
**[[Chemical formula]]s
**[[Aroma compound]]
**[[Psychoactive drug#Psychoactive drug chart|Drug chart]]
**[[Electromagnetic spectrum]]
**[[Time standard]]
**[[Numerical weather prediction]]
**[[Capital asset pricing model]]
**[[Measures of national income and output|National income and output]]
**[[Celestial coordinate system]]
**[[APP-6a|Military mapping]]
**[[Geographic information system]]
**[[Interstate Highway System]]
|}

==See also==
*[[Communication]]
*[[Representation (arts)]]
*[[Modulation|Content carrier signals]]
*[[Multiplexing|Content multiplexing format]]
*[[Transmission (telecommunications)|Content transmission]]
*[[Wireless|Wireless content transmission]]
*Data storage device
*[[Recording format]]
*[[Encoder]]
*[[Analog television]]: [[NTSC]], [[PAL]] and [[SECAM]]
*[[Information mapping]]

==References==
{{reflist}}

[[Category:Communication]]
[[Category:Media technology]]
[[Category:Data management]]
[[Category:Recording]]
[[Category:Film and video technology]]
[[Category:Sound production technology]]


{{library-stub}}</text>
      <sha1>4yqrpujw4loiubwx4uhsjdj49s703b6</sha1>
    </revision>
  </page>
  <page>
    <title>Master data</title>
    <ns>0</ns>
    <id>7617930</id>
    <revision>
      <id>751501273</id>
      <parentid>747574872</parentid>
      <timestamp>2016-11-26T03:04:06Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http&amp;rarr;https for [[YouTube]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5129" xml:space="preserve">'''Master data''' represents the business objects which are agreed on and shared across the enterprise.&lt;ref&gt;[http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202010/Papers/2B1_EnterpriseMasterDataArchitecture.pdf "ENTERPRISE MASTER DATA ARCHITECTURE: DESIGN DECISIONS AND OPTIONS"], Boris Otto &amp; Alexander Schmidt, Institute of Information Management, University of St. Gallen&lt;/ref&gt; It  can cover relatively static reference data, [[Dynamic data|transactional]], [[Unstructured data|unstructured]], analytical, [[Hierarchical database model|hierarchical]] and [[Metadata|meta]] data.&lt;ref&gt;"What Is Master Data?", Roger Wolter and Kirk Haselden, Microsoft Corporation, [http://msdn.microsoft.com/en-us/library/bb190163.aspx "The What, Why, and How of Master Data Management"], November 2006&lt;/ref&gt; It is the primary focus of the [[Information Technology]] (IT) discipline of [[Master Data Management]] (MDM). 

While master data is often non-transactional in nature, it is not limited to non-transactional data, and often ''supports'' transactional processes and operations. For example, Master data may be about: customers, products, employees, materials, suppliers, and vendors, and it may also cover: sales, documents and aggregated sales.

==Types of master data ==

'''Reference Data''' is the set of permissible values to be used by other (master or transaction) data fields. Reference data normally changes slowly, reflecting changes in the modes of operation of the business, rather than changing in the normal course of business.

'''Master Data''' is a ''single source'' of common business data used across '''multiple''' systems, applications, and/or processes.

'''Enterprise Master Data''' is the ''single source'' of common business data used across '''all''' systems, applications, and processes for an entire enterprise (all departments, divisions, companies, and countries).

'''Market Master Data''' is the ''single source'' of common business data for an entire marketplace. Market master data is used among enterprises within the value chain. An example of Market Master Data is the UPC (Universal Product Code) found on consumer products.

Market Master Data is compatible with enterprise-specific and domain-specific systems, compliant with or linked to industry standards, and incorporated within market research analytics. Market master data also facilitates integration of multiple data sources and literally puts everyone in the market on the same page.

Excerpted from ''Master Data Management for Media: A Call to Action for Business Leaders in Marketing, Advertising, and the Media,'' a Microsoft White Paper by Scott Taylor and Robin Laylin, January 2010

==Master data and Master reference data==
Master data is also called '''Master reference data'''. This is to avoid confusion with the usage of the term Master data for '''[[original data]]''', like an original recording (see also: [[Master Tape]]). Master data is nothing but unique data, i.e., there are no duplicate values.{{Citation needed|date=January 2011}}

'''Material Master Data''' is a specific data set holding structured information about spare parts, raw materials and products within Enterprise Resource Planning (ERP) software. The data is held centrally and used across organisations.

'''Vendor Master''' refers to the centralised location of information pertinent to the Vendor. Often this will include the Legal entity name, Tax identification and contact information.

==Master Data Management==
{{main|Master Data Management}}
Curating and managing master data is key to ensuring master data quality. Analysis and reporting is greatly dependent on the quality of an organization's master data. Master data may either be stored in a central repository, sourced from one or more systems, or referenced centrally using an index. However, when it is used by several functional groups it may be distributed and redundantly stored in different applications across an organization and this copy data may be inconsistent (and if so, inaccurate).&lt;ref&gt;[http://blogs.gartner.com/andrew_white/2014/01/14/the-elephant-in-the-room-master-data-and-application-data/#comments "The Elephant in the Room &#8211; Master Data and Application Data"], Andrew White, Gartner, 14 January 2014&lt;/ref&gt; Thus Master Data should have an agreed-upon view that is shared across the organization. Care should be taken to properly version Master Data, if the need arises to modify it, to avoid issues with distributed copies.

==See also==
*[[Master data management]]
*[[Customer data integration]]
*[[Product information management]]
*[[Data governance]]

== External links==
{{reflist}}
* [http://www.stibosystems.com/Files/Billeder/Stibo%20Systems%20images/UK_Resource_Library_Images/What-is-Master-Data-Management_EN.png What is Master Data?]
* [http://www.semarchy.com/overview/what-is-master-data/ Semarchy: What is Master Data?]
* [http://www.orchestranetworks.com/rdm/ Managing Reference Data (RDM)]
* [https://www.youtube.com/watch?v=2tzVUqWAovg Aaron Zornes: Understanding Reference Data]

[[Category:Data management]]

[[fr:Donn&#233;es de r&#233;f&#233;rence]]</text>
      <sha1>58fvghjbhc8w5f4d6z0a46tfiilefll</sha1>
    </revision>
  </page>
  <page>
    <title>Database administration and automation</title>
    <ns>0</ns>
    <id>8078610</id>
    <revision>
      <id>711470821</id>
      <parentid>706780344</parentid>
      <timestamp>2016-03-23T02:06:54Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* See also */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9462" xml:space="preserve">{{More footnotes|date=March 2011}}
'''Database administration''' is the function of managing and maintaining [[database management system]]s (DBMS) software. Mainstream DBMS software such as [[Oracle database|Oracle]], [[IBM DB2]] and [[Microsoft SQL Server]] need ongoing management. As such, corporations that use DBMS software often hire specialized IT ([[Information technology|Information Technology]]) personnel called [[Database administrator|Database Administrators]] or DBAs.

==DBA Responsibilities==

* Installation, configuration and upgrading of Database server software and related products. 
* Evaluate Database features and Database related products.
* Establish and maintain sound backup and recovery policies and procedures. 
* Take care of the [[Database design]] and implementation. 
* Implement and maintain database security (create and maintain users and roles, assign privileges). 
* [[Database tuning]] and performance monitoring. 
* Application tuning and performance monitoring. 
* Setup and maintain documentation and standards. 
* Plan growth and changes (capacity planning). 
* Work as part of a team and provide 24x7 support when required. 
* Do general technical troubleshooting and give cons.
* Database recovery.

== Types of database administration ==
There are three types of DBAs:

#Systems DBAs (also referred to as Physical DBAs, Operations DBAs or Production Support DBAs): focus on the physical aspects of database administration such as DBMS installation, configuration, patching, upgrades, backups, restores, refreshes, performance optimization, maintenance and disaster recovery.
#Development DBAs: focus on the logical and development aspects of database administration such as [[data model]] design and maintenance, DDL ([[Data Definition Language|data definition language]]) generation, SQL writing and tuning, coding [[stored procedure]]s, collaborating with developers to help choose the most appropriate DBMS feature/functionality and other pre-production activities. 
#Application DBAs: usually found in organizations that have purchased [[Third-party developer|3rd party]] [[application software]] such as ERP ([[enterprise resource planning]]) and CRM ([[customer relationship management]]) systems. Examples of such application software includes [[Oracle Applications]], Siebel and [[PeopleSoft]] (both now part of Oracle Corp.) and SAP. Application DBAs straddle the fence between the DBMS and the application software and are responsible for ensuring that the application is fully optimized for the database and vice versa. They usually manage all the [[Software componentry|application components]] that interact with the database and carry out activities such as application installation and patching, application upgrades, database cloning, building and running data cleanup routines, data load [[process management]], etc.

While individuals usually specialize in one type of database administration, in smaller organizations, it is not uncommon to find a single individual or group performing more than one type of database administration.

== Nature of database administration ==
The degree to which the administration of a database is automated dictates the skills and personnel required to manage databases.  On one end of the spectrum, a system with minimal automation will require significant experienced resources to manage; perhaps 5-10 databases per DBA.  Alternatively an organization might choose to automate a significant amount of the work that could be done manually therefore reducing the skills required to perform tasks.  As automation increases, the personnel needs of the organization splits into highly [[skilled worker]]s to create and manage the automation and a group of lower skilled "line" DBAs who simply execute the automation.

Database administration work is complex, repetitive, time-consuming and requires significant training. Since databases hold valuable and mission-critical data, companies usually look for candidates with multiple years of experience. Database administration often requires DBAs to put in work during off-hours (for example, for planned after hours downtime, in the event of a database-related outage or if performance has been severely degraded). DBAs are commonly well compensated for the long hours

One key skill required and often overlooked when selecting a DBA is database recovery (under disaster recovery).  It is not a case of &#8220;if&#8221; but a case of &#8220;when&#8221; a database suffers a failure, ranging from a simple failure to a full catastrophic failure.  The failure may be data corruption, media failure, or user induced errors.  In either situation the DBA must have the skills to recover the database to a given point in time to prevent a loss of data.  A highly skilled DBA can spend a few minutes or exceedingly long hours to get the database back to the operational point.

== Database administration tools ==
Often, the DBMS software comes with certain tools to help DBAs manage the DBMS. Such tools are called native tools. For example, Microsoft SQL Server comes with SQL Server Management Studio and Oracle has tools such as [[SQL*Plus]] and Oracle Enterprise Manager/Grid Control. In addition, 3rd parties such as BMC, [[Quest Software]], [[Embarcadero Technologies]], [[EMS Database Management Solutions]] and SQL Maestro Group offer GUI tools to monitor the DBMS and help DBAs carry out certain functions inside the database more easily.

Another kind of database software exists to manage the provisioning of new databases and the management of existing databases and their related resources.  The process of creating a new database can consist of hundreds or thousands of unique steps from satisfying prerequisites to configuring backups where each step must be successful before the next can start.  A human cannot be expected to complete this procedure in the same exact way time after time - exactly the goal when multiple databases exist.  As the number of DBAs grows, without automation the number of unique configurations frequently grows to be costly/difficult to support.  All of these complicated procedures can be modeled by the best DBAs into database automation software and executed by the standard DBAs.  Software has been created specifically to improve the reliability and repeatability of these procedures such as [[Stratavia]]'s [[Data Palette]] and [[GridApp Systems]] Clarity.

== The impact of IT automation on database administration ==
Recently, automation has begun to impact this area significantly. Newer technologies such as [[Stratavia]]'s [[Data Palette]] suite and [[GridApp Systems]] Clarity have begun to increase the automation of databases causing the reduction of database related tasks. However at best this only reduces the amount of mundane, repetitive activities and does not eliminate the need for DBAs. The intention of DBA automation is to enable DBAs to focus on more proactive activities around database architecture, deployment, performance and service level management.

''Every database requires a database owner account that can perform all schema management operations. This account is specific to the database and cannot log in to Data Director. You can add database owner accounts after database creation. Data Director users must log in with their database-specific credentials to view the database, its entities, and its data or to perform database management tasks.
Database administrators and application developers can manage databases only if they have appropriate permissions and roles granted to them by the organization administrator. The permissions and roles must be granted on the database group or on the database, and they only apply within the organization in which they are granted.''

== Learning database administration ==
There are several education institutes that offer professional courses, including late-night programs, to allow candidates to learn database administration. Also, DBMS vendors such as Oracle, Microsoft and IBM offer certification programs to help companies to hire qualified DBA practitioners.  College degree in Computer Science or related field is helpful but not necessarily a prerequisite.

==See also==

*[[Column-oriented DBMS]]
*[[Data warehouse]]
*[[Directory service]]
*[[Distributed database management system]]
*[[Hierarchical model]]
*[[Navigational database]]
*[[Network model]]
*[[Object model]]
*[[Object database]] (OODBMS)
*[[Object-relational database]] (ORDBMS)
*[[Run Book Automation]] (RBA)
*[[Relational model]] (RDBMS)
*[[Comparison of relational database management systems]]
*[[Comparison of database tools]]
*[[SQL]] is a language for database management

== External links ==
* {{cite journal | publisher = ACM [[Special Interest Group on Information Retrieval]] | work = SIGIR Forum | volume = 7 | issue = 4 | date = Winter 1972 | pages = 45&#8211;55 | url = http://portal.acm.org/citation.cfm?id=1095495.1095500 | title = A set theoretic data structure and retrieval language }}
* {{cite journal | publisher = [[SIGMOD|ACM Special Interest Group on Management of Data]] | work = SIGMOD Record | volume = 35 | issue = 2 | date = June 2006 | url = http://www.tomandmaria.com/tom/Writing/VeritableBucketOfFactsSIGMOD.pdf | title = Origins of the Data Base Management System | format=PDF | author = Thomas Haigh }}

{{Databases}}
{{FOLDOC}}

[[Category:Database management systems]]
[[Category:Data management]]</text>
      <sha1>h3avt8pd3zctkoezzbdeaovbputl71r</sha1>
    </revision>
  </page>
  <page>
    <title>Content Engineering</title>
    <ns>0</ns>
    <id>7829016</id>
    <revision>
      <id>697741270</id>
      <parentid>695600181</parentid>
      <timestamp>2016-01-01T16:16:10Z</timestamp>
      <contributor>
        <ip>123.201.65.218</ip>
      </contributor>
      <comment>editing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1570" xml:space="preserve">{{Unreferenced|date=May 2009}}
'''Content Engineering''' is a term applied to an engineering speciality dealing with the issues around the use of [[Content (media and publishing)|content]] in computer-facilitated environments.  Content production, [[content management]], content modelling, content conversion, and content use and repurposing are all areas involving this speciality.  It is not a speciality with wide industry recognition and is often performed on an ad hoc basis by members of software development or content production staff, but is beginning to be recognized as a necessary function in any complex content-centric project involving both content production as well as software system development.

Content engineering tends to bridge the gap between groups involved in the production of content ([[Publishing]] and [[Editing|Editorial staff]], [[Marketing]], [[Sales]], [[Human resources|HR]]) and more technologically oriented departments such as [[Software Development]], or [[Information technology|IT]] that put this content to use in web or other software-based environments, and requires an understanding of the issues and processes of both sides.

Typically, Content Engineering involves extensive use of Embedded,  [[XML]] technologies, XML being the most widespread language for representing structured content. [[Content_management_system|Content Management Systems]] are often key technology used in this practice though frequently Content Engineering fills the gap where no formal CMS has been put into place.

[[Category:Data management]]</text>
      <sha1>jbn8ech15k7jv7e2my19994mnfrhs94</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Recording</title>
    <ns>14</ns>
    <id>1479108</id>
    <revision>
      <id>543943325</id>
      <parentid>537137306</parentid>
      <timestamp>2013-03-13T22:34:50Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 9 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6580221]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="116" xml:space="preserve">{{Commons category|Recording}}
{{Cat main|Recording}}

[[Category:Data management]]
[[Category:Information storage]]</text>
      <sha1>44kcubtvwcjez2llpf0aeqk35v2rjgp</sha1>
    </revision>
  </page>
  <page>
    <title>CA Gen</title>
    <ns>0</ns>
    <id>965842</id>
    <revision>
      <id>725386288</id>
      <parentid>725386121</parentid>
      <timestamp>2016-06-15T10:13:11Z</timestamp>
      <contributor>
        <ip>212.167.5.6</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5963" xml:space="preserve">'''CA Gen''' is a [[Computer Aided Software Engineering]] (CASE) application development environment marketed by [[CA Technologies]]. Gen was previously known as '''IEF''' ('''Information Engineering Facility'''), '''Composer by IEF''', '''Composer''', '''COOL:Gen''', '''Advantage:Gen''' and '''AllFusion Gen'''.

The toolset originally supported the [[information engineering]] methodology developed by [[Clive Finkelstein]], [[James Martin (author)|James Martin]] and others in the early 1980s. Early versions supported IBM's [[IBM DB2|DB2]] database, [[IBM 3270|3270]] 'block mode' screens and generated [[COBOL]] code.

In the intervening years the toolset has been expanded to support additional development techniques such as [[Component-based software engineering|component-based development]]; creation of [[Client&#8211;server model|client/server]] and [[web application]]s and generation of [[C (programming language)|C]], [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]]. In addition, other platforms are now supported such as many variants of *ix-like Operating Systems (AIX, HP-UX, Solaris, Linux) as well as Windows.

Its range of supported database technologies have widened to include [[Oracle Database|ORACLE]], [[Microsoft SQL Server]], [[ODBC]], [[Java Database Connectivity|JDBC]] as well as the original DB2.

The toolset is fully integrated - objects identified during analysis carry forward into design without redefinition. All information is stored in a repository (central encyclopedia). The encyclopedia allows for large team development - controlling access so that multiple developers may not change the same object simultaneously.&lt;ref&gt;https://communities.ca.com/web/ca-gen-edge-global-user-community/wiki/-/wiki/EDGE+User+Group+CA+Gen+Wiki/What+is+CA+Gen?&amp;#p_36&lt;/ref&gt;

==Overview==
It was initially produced by [[Texas Instruments]], with input from [[James Martin (author)|James Martin]] and his consultancy firm James Martin Associates, and was based on the Information Engineering Methodology (IEM). The first version was launched in 1987.

IEF became popular among large government departments and public utilities. It initially supported a [[CICS]]/COBOL/DB2 target environment.  However, it now supports a wider range of relational databases and operating systems. IEF was intended to shield the developer from the complexities of building complete multi-tier cross-platform applications.

In 1995, Texas Instruments decided to change their marketing focus for the product. Part of this change included a new name - "Composer".

By 1996, IEF had become a popular tool. However, it was criticized by some IT professionals for being too restrictive, as well as for having a high per-workstation cost ($15K USD). But it is claimed that IEF reduces development time and costs by removing complexity and allowing rapid development of large scale enterprise transaction processing systems.

In 1997, Composer had another change of branding, Texas Instruments sold the [[Texas Instruments Software]] division, including the Composer rights, to [[Sterling Software]]. Sterling software changed the well known name "Information Engineering Facility" to "COOL:Gen". COOL was an acronym for "Common Object Oriented Language" - despite the fact that there was little [[Object-oriented programming|object orientation]] in the product.

In 2000, Sterling Software was acquired by [[Computer Associates]] (now CA). CA has rebranded the product three times to date and the product is still used widely today. Under CA, recent releases of the tool added support for the CA-[[DATACOM/DB|Datacom]] DBMS, the Linux operating system, C# code generation and [[ASP.NET]] web clients. The current version is known as CA Gen - version 8 being released in May 2010, with support for customised web services, and more of the toolset being based around the [[Eclipse (software)|Eclipse framework]].

There are a variety of "add-on" tools available for CA Gen, including Project Phoenix from Jumar - a collection of software tools and services focused on the modernisation and re-platforming of existing/legacy CA Gen applications to new environments,&lt;ref&gt;[http://www.jumar-solutions.com/ Jumar]&lt;/ref&gt; GuardIEn - a [[Configuration Management]] and Developer Productivity Suite,&lt;ref&gt;[http://www.iet.co.uk IET Ltd]&lt;/ref&gt; QAT Wizard,&lt;ref&gt;[http://www.qat.com/qat_wizard.asp QAT Wizard]&lt;/ref&gt; an interview style wizard that takes advantage of the meta model in Gen, products for multi-platform application reporting and XML/SOAP enabling of Gen applications.,&lt;ref&gt;[http://www.canamsoftware.com/ Canam Software Labs]&lt;/ref&gt; and developer productivity tools such as Access Gen, APMConnect, QA Console and Upgrade Console from Response Systems &lt;ref&gt;[http://www.response-systems.com Response Systems]&lt;/ref&gt;
Recently CA GEN has released its latest version 8.5.

==References==
{{Reflist}}

==External links==
* [http://www.uk.capgemini.com/public-sector/tax-welfare/regenerate Capgemini REGENERATE offering] - Support, Update, Migrate
* [http://www.edgeusergroup.org EDGE User Group] - the user group for CA Gen
* [http://www.edgeusergroup.org/wiki CA Gen Wiki] - sponsored by the EDGE User Group
* [http://www.gentalk.biz gentalk.biz] - CA Gen Blog - inactive
* [http://www.qat.com QAT Global] - CA Gen Services and Training Provider (USA)
* [http://www.iet.co.uk IET] - CA Gen Product and Services Provider (UK)
* [http://www.jumar-solutions.com/ Jumar Solutions] - CA Gen Product and Services Provider (UK)
* [http://www.response-systems.com/ Response Systems] - CA Gen Product and Services Provider (UK)
* [http://www.facet.com.au/ Facet Consulting] - CA Gen Services Provider (Australia)
* [http://www.canamsoftware.com/ Canam Software Labs, Inc.] - CA Gen Product and Service Provider (Canada)

[[Category:Computer-aided software engineering tools]]
[[Category:Data management]]
[[Category:CA Technologies]]


Edited by: Sambit Mishra</text>
      <sha1>7rbv8tdyrhzaxki5503od13or719924</sha1>
    </revision>
  </page>
  <page>
    <title>Dynamic knowledge repository</title>
    <ns>0</ns>
    <id>1004008</id>
    <revision>
      <id>593025002</id>
      <parentid>571417452</parentid>
      <timestamp>2014-01-29T22:17:11Z</timestamp>
      <contributor>
        <username>HokeyMang</username>
        <id>20608926</id>
      </contributor>
      <minor />
      <comment>/* Definition */ Small grammatical improvements</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1908" xml:space="preserve">{{refimprove|date=August 2007}}

The '''dynamic knowledge repository''' ('''DKR''') is a concept developed by [[Douglas C. Engelbart]] as a primary strategic focus for allowing humans to address complex problems.{{when|date=September 2011}} Doug has proposed that a DKR will enable us to develop a collective [[IQ]] greater than any individual's IQ. References and discussion of Engelbart's DKR concept are available at the [[Doug Engelbart Institute]].&lt;ref&gt;{{cite web | url=http://www.dougengelbart.org/about/dkrs.html|title=About Dynamic Knowledge Repositories &#8211; an introduction | accessdate=September 15, 2011 | author=Christina Engelbart}}&lt;/ref&gt;

==Definition==
A knowledge repository is a computerized system that systematically captures, organizes and categorizes an organization's knowledge. The repository can be searched and data can be quickly retrieved.

The effective knowledge repositories include factual, conceptual, procedural and meta-cognitive techniques. The key features of knowledge repositories include communication forums.

A knowledge repository can take many forms to "contain" the knowledge it holds. A customer database is a knowledge repository of customer information and insights &#8211; or electronic explicit knowledge. A Library is a knowledge repository of books &#8211; physical explicit knowledge. A community of experts is a knowledge repository of tacit knowledge or experience. The nature of the repository only changes to contain/manage the type of knowledge it holds. A repository (as opposed to an archive) is designed to get knowledge out. It should therefore have some rules of structure, classification, taxonomy, record management, etc., to facilitate user engagement.

==References==
{{Reflist|2}}

== External links==
* [http://dougengelbart.org/ Doug Engelbart Institute]

[[Category:Knowledge representation]]
[[Category:Data management]]


{{compu-storage-stub}}</text>
      <sha1>iat2rpgw89p6fkxges2si1zdxavwc7z</sha1>
    </revision>
  </page>
  <page>
    <title>Client-side persistent data</title>
    <ns>0</ns>
    <id>13150801</id>
    <revision>
      <id>621409809</id>
      <parentid>603840031</parentid>
      <timestamp>2014-08-15T21:48:54Z</timestamp>
      <contributor>
        <username>Jmabel</username>
        <id>28107</id>
      </contributor>
      <comment>/* See also */ + [[Web storage]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1429" xml:space="preserve">'''Client-side persistent data''' or CSPD is a term used in [[computing]] for storing data required by [[web application |web applications]] to complete internet tasks on the [[client-side]] as needed rather than exclusively on the [[Server (computing) |server]]. As a framework it is one solution to the needs of [[Occasionally connected computing]] or OCC.

A major challenge for [[HTTP]] as a [[Stateless server |stateless]] [[Protocol (computing)|protocol]] has been asynchronous tasks. The [[Ajax (programming)|AJAX]] pattern using [[XMLHttpRequest]] was first introduced by [[Microsoft]] in the context of the [[Outlook Web App|Outlook]] e-mail product.

The first CSPD were the [[HTTP cookie |'cookies']] introduced by the [[Netscape]] [[Netscape (web browser)|Navigator]].  [[ActiveX]] components which have entries in the [[Windows registry]] can also be viewed as a form of [[client-side]] [[Persistence (computer science)|persistence]].

==See also==
* [[Occasionally connected computing]]
* [[Curl (programming_language)]]
* [[Ajax (programming)|AJAX]]
* [[HTTP]]
* [[Web storage]]

==External links==
* [http://www.curl.com/developer/faq/cspd/ CSPD]
* [http://safari.ciscopress.com/0596101996/jscript5-CHP-19-SECT-6 Safari] preview
* [http://wp.netscape.com/newsref/std/cookie_spec.html Netscape] on persistent client state

[[Category:Clients (computing)]]
[[Category:Data management]]
[[Category:Web applications]]</text>
      <sha1>0qaq04jf9hzew8j13w65xf8prc2fr73</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft SQL Server Master Data Services</title>
    <ns>0</ns>
    <id>13430116</id>
    <revision>
      <id>736956402</id>
      <parentid>736955850</parentid>
      <timestamp>2016-08-30T22:33:44Z</timestamp>
      <contributor>
        <username>Vpnicholls</username>
        <id>29068868</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7876" xml:space="preserve">{{multiple issues|
{{Advert|date=March 2011}}
{{Update|inaccurate=yes|date=April 2010}}
}}

'''Microsoft SQL Server Master Data Services''' is a [[Master Data Management]] (MDM) product from [[Microsoft]] that ships as a part of the [[Microsoft SQL Server]] relational database management system.&lt;ref&gt;https://msdn.microsoft.com/en-us/library/ms130214.aspx&lt;/ref&gt;  Master Data Services (MDS) is the SQL Server solution for master data management. Master data management (MDM) enables your organization to discover and define non-transactional lists of data, and compile maintainable, reliable master lists. Master Data Services first shipped with Microsoft SQL Server 2008 R2.  Microsoft SQL Server 2016 includes many enhancements to Master Data Services, such as improved performance and security, and the ability to clear transaction logs, create custom indexes, share entity data between different models, and support for many-to-many relationships. For more information, see [https://msdn.microsoft.com/en-us/library/ff929136.aspx What's New in Master Data Services (MDS)]

==Overview==
In Master Data Services, the model is the highest level container in the structure of your master data. You create a model to manage groups of similar data. A model contains one or more entities, and entities contain members that are the data records. An entity is similar to a table.

Like other MDM products, Master Data Services aims to create a centralized data source and keep it synchronized, and thus reduce redundancies, across the applications which process the data.{{cn|date=January 2015}}

Sharing the architectural core with Stratature +EDM, Master Data Services uses a [[Microsoft SQL Server]] database as the physical data store. It is a part of the ''Master Data Hub'', which uses the database to store and manage data [[Entity Data Model|entities]].{{cn|date=January 2015}} It is a database with the software to validate and manage the data, and keep it synchronized with the systems that use the data.&lt;ref name="arch"&gt;{{cite web | url = http://msdn2.microsoft.com/en-us/library/bb410798.aspx | title = Master Data Management (MDM) Hub Architecture | author = Roger Walter | publisher = MSDN TechNet | accessdate = 2007-09-25}}&lt;/ref&gt; The master data hub has to extract the data from the source system, validate, sanitize and shape the data, remove duplicates, and update the hub repositories, as well as synchronize the external sources.&lt;ref name="arch"/&gt; The entity schemas, attributes, data hierarchies, validation rules and access control information are specified as [[metadata]] to the Master Data Services runtime. Master Data Services does not impose any limitation on the data model. Master Data Services also allows custom ''Business rules'', used for validating and sanitizing the data entering the data hub, to be defined, which is then run against the data matching the specified criteria. All changes made to the data are validated against the rules, and a log of the transaction is stored persistently. Violations are logged separately, and optionally the owner is notified, automatically. All the data entities can be [[Revision control system|versioned]].{{cn|date=January 2015}}

Master Data Services allows the master data to be categorized by hierarchical relationships, such as employee data are a subtype of organization data. Hierarchies are generated by relating data attributes. Data can be automatically categorized using rules, and the categories are introspected programmatically. Master Data Services can also expose the data as [[Microsoft SQL Server]] [[view (database)|views]], which can be pulled by any [[SQL]]-compatible client. It uses a role-based access control system to restrict access to the data. The views are generated dynamically, so they contain the latest data entities in the master hub. It can also push out the data by writing to some external journals. Master Data Services also includes a web-based UI for viewing and managing the data. It uses [[AJAX]] in the front-end and [[ASP.NET]] in the back-end.{{cn|date=January 2015}}

Master Data Services also includes certain features not available in the Stratature +EDM product. It gains a [[Web service]] interface to expose the data, as well as an [[API]], which internally uses the exposed web services, exposing the feature set, programmatically, to access and manipulate the data. It also integrates with [[Active Directory]] for authentication purposes. Unlike +EDM, Master Data Services supports [[Unicode]] characters, as well as support multilingual user interfaces.{{cn|date=January 2015}}

There has been a significant [http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever performance increase] in Master Data Services in SQL Server 2016 as well as the Excel Add-In.&lt;ref&gt;http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever&lt;/ref&gt;

== Terminology ==

* ''Model'' is the highest level of an MDS instance. It is the primary container for specific groupings of master data. In many ways it is very similar to the idea of a database. 
* ''Entities'' are containers created within a model. Entities provide a home for members, and are in many ways analogous to database tables. (e.g. Customer)
* ''Members'' are analogous to the records in a database table (Entity) e.g. Will Smith. Members are contained within entities. Each member is made up of two or more attributes. 
* ''Attributes'' are analogous to the columns within a database table (Entity) e.g. Surname. Attributes exist within entities and help describe members (the records within the table). Name and Code attributes are created by default for each entity and serve to describe and uniquely identify leaf members. Attributes can be related to other attributes from other entities which are called 'domain-based' attributes. This is similar to the concept of a foreign key.
Other attributes however, will be of type 'free-form' (most common) or 'file'.
* ''Attribute Groups'' are explicitly defined collections of particular attributes. Say you have an entity "customer" that has 50 attributes &amp;mdash; too much information for many of your users. Attribute groups enable the creation of custom sets of hand-picked attributes that are relevant for specific audiences. (e.g. "customer - delivery details" that would include just their name and last known delivery address). This is very similar to a database view.
*  ''Hierarchies'' organize members into either Derived or Explicit hierarchical structures. Derived hierarchies, as the name suggests, are derived by the MDS engine based on the relationships that exist between attributes. Explicit hierarchies are created by hand using both leaf and consolidated members.
*  ''Business Rules'' can be created and applied against model data to ensure that custom business logic is adhered to. In order to be committed into the system data must pass all business rule validations applied to them. e.g. Within the Customer Entity you may want to create a business rule that ensures all members of the 'Country' Attribute contain either the text "USA" or "Canada". The Business Rule once created and ran will then verify all the data is correct before it accepts it into the approved model.
*  ''Versions'' provide system owners / administrators with the ability to Open, Lock or Commit a particular version of a model and the data contained within it at a particular point in time. As the content within a model varies, grows or shrinks over time versions provide a way of managing metadata so that subscribing systems can access to the correct content.

== References ==
{{Reflist}}

==External links==
*[https://msdn.microsoft.com/en-us/library/ee633763.aspx Microsoft SQL Server 2016 Master Data Services]

[[Category:Data management]]
[[Category:Microsoft software|SQL Server Master Data Services]]
[[Category:2010 software]]</text>
      <sha1>a21wc7c4dtjsyc9bgq9kt6dimnd3htv</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft Office PerformancePoint Server</title>
    <ns>0</ns>
    <id>9562761</id>
    <revision>
      <id>738534516</id>
      <parentid>706312388</parentid>
      <timestamp>2016-09-09T14:54:39Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4336" xml:space="preserve">{{multiple issues|
{{more footnotes|date=August 2013}}
{{refimprove|date=August 2013}}
}}

{{Infobox Software
| name = Microsoft Office PerformancePoint Server
| developer = [[Microsoft]]
| released = {{Start date|2007|11}}
| latest_release_version = 1.0 SP2
| latest_release_date = 2008
| operating_system = [[Microsoft Windows]]
| genre = [[Enterprise Performance Management]]
| license = [[Proprietary software|Proprietary]] [[EULA]]
| website = [https://web.archive.org/web/20071016055516/http://www.microsoft.com/business/performancepoint/ www.microsoft.com/business/performancepoint]
}}
'''Microsoft Office PerformancePoint Server''' is a [[business intelligence]] [[Computer software|software]] product released in 2007 by [[Microsoft]]. Although discontinued in 2009, the dashboard, scorecard, and analytics capabilities of PerformancePoint Server were incorporated into [[Sharepoint 2010|SharePoint 2010]] and later versions.

PerformancePoint Server also provided a planning and budgeting component directly integrated with Excel.

==History==

Microsoft offered preview releases of PerformancePoint Server starting in mid-2006. Previews of the product were formed from [[Business Scorecard Manager 2005]] and the Planning Server component. Acquisitions [[ProClarity Corporation|ProClarity]] and [[Great Plains Software|Great Plains]] brought additional analytics and planning/reporting capabilities, as well as companion products ProClarity 6.3 and [[Microsoft FRx|FRx]].

PerformancePoint Server was officially released in November 2007.

Microsoft discontinued PerformancePoint Server as an independent product in 2009 and folded its dashboard, scorecard and analytics capabilities into PerformancePoint Services in [[SharePoint Server 2010]].&lt;ref&gt;{{cite web |url=http://www.informationweek.com/news/business_intelligence/analytics/showArticle.jhtml?articleID=212902915&amp;subSection=Business+Intelligence |title=Microsoft Makes Sweeping Changes To BI Software Strategy |date=January 27, 2009 |last=Weier |first=Mary Hayes |work=[[InformationWeek]]}}&lt;/ref&gt;

==Monitoring Server Component==
Business monitoring capabilities, including dashboards, scorecards &amp; key performance indicators, navigable reports for deeper analysis, strategy maps, and linked filtering, are provided by PerformancePoint's Monitoring Server component. A Dashboard Designer application that is distributed from Monitoring Server enables business analysts or IT Administrators to:

* create &amp; test data source connections
* create views that use those data connections
* assemble the views into a dashboard
* deploy the dashboard as a [[SharePoint]] page

Dashboard Designer saved content and security information back to the Monitoring Server. Data source connections, such as OLAP cubes or relational tables, were also made through Monitoring Server.
 
After a dashboard has been published to the Monitoring Server database, it would be deployed as a SharePoint page and shared with other users as such. When the pages were opened in a web browser, Monitoring Server updated the data in the views by connecting back to the original data sources.

==Planning Server Component==
PerformancePoint's Planning Server component supported maintenance of logical business models, budget &amp; approval workflows, enterprise data sources, and it followed [[Generally Accepted Accounting Principles]].

Planning Server made use of Excel for input and line-of-business reporting, as well as SQL Server for storing and processing business models.

==Management Reporter Component==
The Management Reporter component was designed to perform financial reporting and can read PerformancePoint Planning models directly. A development kit was also available to allow this component to read other models .{{which|date=August 2013}}

==References==
{{reflist}}

==External links==
* [http://msdn2.microsoft.com/en-us/office/bb660518.aspx PerformancePoint Server 2007 Developer Portal]
* [http://blogs.technet.com/datapuzzle Data Puzzle]
* [http://performancepointinsider.com/blogs/default.aspx PerformancePoint Insider]
* [http://alanwhitehouse.wordpress.com/2009/01/26/pps-planning-being-discontinued/ Performance Point Planning being discontinued]

{{Microsoft Office}}

[[Category:Microsoft Office servers]]
[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>swr63e91wrktyuyert39f6ny54cwk0r</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data structures</title>
    <ns>14</ns>
    <id>691150</id>
    <revision>
      <id>721795166</id>
      <parentid>721793566</parentid>
      <timestamp>2016-05-24T02:53:35Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="703" xml:space="preserve">{{catdiffuse}}

{{Commons cat|Data structures}}
{{Category see also|Data types}}

In [[computer science]], a '''[[data structure]]''' is a way of storing [[data]] in a computer so that it can be used efficiently. Often a carefully chosen data structure will allow a more efficient [[algorithm]] to be used. The choice of the data structure must begin from the choice of an [[abstract data structure]].

{{Cat main|Data structures}}

== See also ==

* [[:Category:Abstract data types]]
* [[:Category:Hashing]]
* [[:Category:Computer file formats]]

[[Category:Algorithms and data structures]]
[[Category:Computer data|Structures]]
[[Category:Data management|Structures]]
[[Category:Computer programming]]</text>
      <sha1>emihnvooajnzusv07pkmismdrynjzv5</sha1>
    </revision>
  </page>
  <page>
    <title>Integration competency center</title>
    <ns>0</ns>
    <id>15014956</id>
    <revision>
      <id>753839619</id>
      <parentid>599128956</parentid>
      <timestamp>2016-12-09T14:05:49Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[data profiling]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12804" xml:space="preserve">{{ad|date=March 2014}}
{{peacock|date=March 2014}}
The '''integration competency center''' (ICC), sometimes referred to as an '''integration center of excellence''' (COE), is a [[shared services|shared service]] function within an organization, particularly large corporate enterprises as well as public sector institutions, for performing methodical [[data integration]], [[system integration]] or [[enterprise application integration]]. 

[[Data integration]] allows companies to access their enterprise data and functions, fragmented across disparate systems, in order to create a combined, accurate, and consistent view of their core information as well as process assets and leverage them across the enterprise to drive business decisions and operations.  System integration is the bringing together of component subsystems into one system and ensuring that they function together effectively. Enterprise application integration enables efficient information exchanges and business process automation across separate computer applications in a [[Cohesion (computer science)|cohesive]] fashion.

== Overview ==
The term may be better understood by examining each of the three words that comprise the acronym. '''Integration''' refers to the objective of the ICC to take a holistic perspective and optimize certain qualities such as cost efficiency, organizational agility and effectiveness, operational risk, customer (internal or external) experience, etc. across multiple functional groups. '''Competency''' refers to the expertise, knowledge or capability that the ICC offers as services.  '''Center''' means that the service is managed or coordinated from a common (central) point independent from the functional areas that it supports. 

Large organizations are usually sub-divided into functional areas such as marketing, sales, distribution, finance, human resources to name just a few. These functional groups have separate operations and are ''vertically integrated'' and are therefore sometimes referred to as "silos" or "stovepipes".  From an organizational perspective, an ICC is a group of people with special skills, who are centrally coordinated, and offer services to accomplish a mission that requires separate functional areas to work together. 

Key objectives of an ICC are:

* Lead and support enterprise integration (data, system and process) projects with the cooperation/coordination of subject matter experts
* Promote Enterprise integration as a formal discipline. For example, data integration will include [[data warehousing]], [[data migration]], [[data quality]] management, data integration for [[service oriented architecture]] deployments, and [[data synchronization]]. Similar system integration will include common messaging services, business service virtualization etc.
* Develop staff specialists in integration processes and operations and leverage their expertise company-wide
* Assess and select integration technology and tools from the marketplace
* Manage integration pilots and projects across the organization
* Optimize integration investments across the enterprise level
* Leverage economies of scale for the integration tools portfolio at enterprise level

ICCs allow companies to:
* Optimize scarce resources by combining integration skills, resources, and processes into one group
* Reduce project delivery times and development and maintenance costs through effectiveness and efficiency
* Improve ROI through creation and reuse of enterprise assets like source definitions, application interfaces, and codified business rules
* Decrease duplication of integration related effort across the enterprise
* Build on past successes instead of reinventing the wheel with each project
* Lower total technology cost of ownership by leveraging technology investments across multiple projects

An ICC may be a temporary group in support of a [[program management|program]] or a permanent part of the organization.  Furthermore, ICC&#8217;s can be established at various scales or levels; within a division of a company, at the enterprise level, or across multiple companies in a supply chain.

== History ==
The term "integration competency center" and its acronym ICC was popularized by Roy Schulte of [[Gartner]] in a series of articles and conference presentations beginning in 2001 with ''The Integration Competency Center'' {{citation missing|date=February 2013}}&lt;!--[Ref SPA-14-0456] mostly useless, cannot be found anywhere--&gt;. He picked up the term from one of his colleagues, Gary Long, who found some of his clients using it (they took the established term &#8220;competency center&#8221; and applied it to integration). Prior to that (from 1997 to 2001) Gartner had been referring to it as the ''central integration team''. The concept itself (even before it was given a label) goes back to 1996 in one of Gartner&#8217;s first reports on integration. {{citation missing|date=February 2013}}

A major milestone was the publication in 2005 of the first book on the topic: ''Integration Competency Center: An Implementation Methodology''&lt;ref name="ICC"&gt;John G. Schmidt and David Lyle (2005), ''Integration Competency Center, An Implementation Methodology,'' ISBN 0-9769163-0-4&lt;/ref&gt; by ''John G. Schmidt'' and ''David Lyle''. The book introduced five ICC organizational models and explored the people, process and technology dimensions of ICC&#8217;s.  Several reviews of the book can be found at [http://blogs.ittoolbox.com/eai/business/archives/soa-competency-center-5731  IT Toolbox] and at [http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304 Amazon]. The concept of integration as a competency in the IT domain has now survived for over 10 years and appears to be picking up momentum and broad-based acceptance. 

These days, ICC&#8217;s are often called, integration center of excellence, SOA center of excellence, the data management center of excellence and other variants. The most advanced ICC's are using [[Lean Integration]] practices to optimize end-to-end processes and to drive continuous improvements. Universities are also beginning to include integration topics in their MBA programs and computer science curricula. For example, The College of Information Sciences and Technology at Penn State University has established a [http://ist.psu.edu/facultyresearch/facilities/eii/  Enterprise Informatics and Integration Center] with the following mission:

"''The Enterprise Informatics and Integration Center (EI&#178;) will actively engage industry, non-profit, and government agency leaders to address critical issues in enterprise processes, knowledge management, and decision making.''"

== Operating models ==
There are a number of ways an ICC can be organized and a wide range of responsibilities with which it can be chartered. The ICC book&lt;ref name="ICC"/&gt; introduced five ICC organizational models and explored the people, process and technology dimensions of ICCs. They include:

=== Best practices ICC ===
The primary function of this ICC model is to document best practices. It does not include a central support or development team to implement those standards across projects, and probably not metadata either. To implement a best practices ICC, companies need a flexible development environment that supports diverse teams and that enables the team to enhance and extend existing systems and processes. Such a team might be a subset of an existing enterprise architecture capability and generally consists of a small number of staff (1-5).

=== Standard services ICC ===
A standard services ICC provides the same knowledge leverage as a best practices ICC, but enforces technical consistency in software development and hardware choices. A standard services ICC focuses on processes, including standardizing and enforcing naming conventions, establishing metadata standards, instituting change management procedures, and providing standards training. This type of ICC also reviews emerging technologies, selects vendors, and manages hardware and software systems.  This style of ICC is often tightly linked with the enterprise architecture team and may be slightly larger than a typical best practices ICC.

=== Shared services ICC ===
A shared services ICC provides a supported technical environment and services ranging from development support all the way through to a help desk for projects in production. This type of ICC is significantly more complex than a best practices or Standard Services model. It establishes processes for knowledge management, including product training, standards enforcement, technology benchmarking, and metadata management, and it facilitates impact analysis, software quality, and effective use of developer resources across projects. The organizational structure of a Shared Services ICC is sometimes referred to as a hybrid or federated model which often includes a small central coordinating team plus dotted-line reporting relationships with multiple distributed teams.

=== Central services ICC ===
A central services ICC controls integration across the enterprise. It carries out the same processes as the other models, but in addition usually has its own budget and a charge-back methodology. It also offers more support for development projects, providing management, development resources, [[data profiling]], data quality, and unit testing. Because a central services ICC is more involved in development activities than the other models, it requires a production operator and a data integration developer.  The staff in a central services ICC does not necessarily need to be a central location and may be distributed geographically; the important distinction is that the staffs have a solid-line reporting relationship to the ICC Director.  The size of these teams can vary and may be as large as 10%-15% of the IT staff in an organization.

=== Self service ICC ===
The self-service ICC represents the highest level of maturity in an organization.  The ICC itself may be almost invisible in that its functions are so ingrained in the day-to-day systems development life-cycle and its operations are so tightly integrated with the infrastructure that it may require only small central team to sustain itself.  This ICC model achieves both a highly efficient operation and provides an environment where independent development and innovation can flourish. This goal is achieved by strict enforcement of a set of application integration standards through automated processes enabled by tools and systems.

== Key challenges ==
ICC as a concept is fairly simple. It is embodiment of the IT management best practices to deliver shared services. However, being an organizational concept, it is far more challenging to implement in practice than the conceptual view because every organization has different DNA and it takes specific personalization/customization effort for ICC that makes the ICC initiative successful. Here are some of the common challenges in ICC establishment journey:
* Change management in terms of technology, processes, organization structure
* Ability of the organization to deal with the pace and quantum of change
* Alignment of stakeholders and process owners for ICC strategy
* Inappropriate ownership level for ICC program and lack of senior management sponsorship
* Highly tactical focus and business program level constraints
* Ignoring foundation elements and jumping to implementation directly
* Inappropriate funding

These issues are important to consider when embarking on the ICC investment since the last leg of the implementation of ICC that's what matters most. Intellectual definition of ICC that is not implemented in the organisation has no real value for the enterprise.

== See also ==
* [[Lean Integration]]

== References ==
&lt;references/&gt;

== Further reading ==
* Maurizio Lenzerini (2002). "Data Integration: A Theoretical Perspective". PODS 2002: 243-246.

== External links ==
* Integration Competency Center book: (http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304)
* [[Informatica]] ICC Blog: (http://blogs.informatica.com/perspectives/category/data-integration/integration-competency-centers/)
* Gartner paper  (http://www.ebizq.net/topics/tech_in_biz/features/5360.html)
* Integration Consortium: (http://www.integrationconsortium.org)
* Infosys ICC Blogs (http://www.infosysblogs.com/bpm-eai/integration_competency_center_icc)
* ICC Handbook (http://www.unthink.fi/Global/PDF/ICC-Handbook.pdf)
* Integration Warstories - article about avoiding ICC pitfalls (http://integrationwarstories.com/2013/10/25/avoiding-pitfalls-of-integration-competency-centers/)

[[Category:Data management]]
[[Category:Software development philosophies]]
[[Category:Information technology]]</text>
      <sha1>k0ccn6uzfeeyybw3nxjjnv4tnc5mm27</sha1>
    </revision>
  </page>
  <page>
    <title>Transaction data</title>
    <ns>0</ns>
    <id>15348791</id>
    <revision>
      <id>748126272</id>
      <parentid>748124215</parentid>
      <timestamp>2016-11-06T13:57:40Z</timestamp>
      <contributor>
        <ip>41.221.99.238</ip>
      </contributor>
      <comment>/* Data Warehousing */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1351" xml:space="preserve">{{Unreferenced|date=July 2010}}

'''Transaction data''' are data describing an event (the change as a result of a [[Transaction processing|transaction]]) and is usually described with verbs. Transaction data always has a time dimension, a numerical value and refers to one or more objects (i.e. the [[reference data]]).

Typical transactions are:
* Financial: orders, invoices, payments
* Work: Plans, activity records
* [[Logistics]]: Deliveries, storage records, travel records, etc.

Typical [[transaction processing system]]s (systems generating transactions) are [[SAP ERP|SAP]] and [[Oracle Financials]].

==Records management==
{{Main|Records management}}
Recording and retaining transactions is called [[records management]]. The record of the transaction is stored in a place where the [[wikt:retention|retention]] can be guaranteed and where data are archived/removed following a [[retention period]]. The format of the transaction can be data (to be stored in a database), but it can also be a document.

==Data Warehousing==
Transaction data can be summarised in a [[Data warehouse]], which helps accessibility and analysis of the data.

==See also==
* [[Data modeling]]
* [[Data architecture]]
* [[Information Lifecycle Management]]
* [[reference data]]

[[Category:Data management]]
[[Category:Transaction processing]]


{{compsci-stub}}</text>
      <sha1>r02veuowjchqvqamcwsedntxf6yfi1j</sha1>
    </revision>
  </page>
  <page>
    <title>Data proliferation</title>
    <ns>0</ns>
    <id>13651081</id>
    <revision>
      <id>753459300</id>
      <parentid>738669492</parentid>
      <timestamp>2016-12-07T08:07:51Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6361" xml:space="preserve">'''Data proliferation''' refers to the prodigious amount of [[data]], [[structured data|structured]] and unstructured, that businesses and governments continue to generate at an unprecedented rate and the [[usability]] problems that result from attempting to store and manage that data. While originally pertaining to problems associated with paper [[documentation]], data proliferation has become a major problem in primary and secondary [[data storage device|data storage]] on computers.

While digital storage has become cheaper, the associated costs, from raw power to maintenance and from metadata to search engines, have not kept up with the proliferation of data. Although the power required to maintain a unit of data has fallen, the cost of facilities which house the digital storage has tended to rise.&lt;ref&gt;{{cite web |url =http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm
|title=Downsizing the digital attic |work=Deloitte Technology Predictions |archiveurl=https://web.archive.org/web/20110722194032/http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm |archivedate=July 22, 2011}}&lt;/ref&gt;

{{rquote|right| At the simplest level, company [[e-mail]] systems spawn large amounts of data. Business e-mail &#8211; some of it important to the enterprise, some much less so &#8211; is estimated to be growing at a rate of 25-30% annually. And whether it&#8217;s relevant or not, the load on the system is being magnified by practices such as multiple addressing and the attaching of large text, audio and even [[video file formats|video file]]s.|IBM Global Technology Services&lt;ref name=IBM&gt;[https://web.archive.org/web/20090206010415/http://www-03.ibm.com/systems/resources/systems_storage_solutions_pdf_toxic_tb.pdf &#8220;The Toxic [[Terabyte]]&#8221;, IBM Global Technology Services, July 2006]&lt;/ref&gt;}}

Data proliferation has been documented as a problem for the [[U.S. military]] since August 1971, in particular regarding the excessive documentation submitted during the acquisition of major weapon systems.&lt;ref name=DODPP&gt;[http://stinet.dtic.mil/oai/oai?&amp;verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0892652 Evolution of the Data Proliferation Problem within Major Air Force Acquisition Programs.]&lt;/ref&gt; Efforts to mitigate data proliferation and the problems associated with it are ongoing.&lt;ref&gt;[http://www.thic.org/pdf/Jun02/dod.rroderique.020612.pdf Data Proliferation: Stop That]&lt;/ref&gt;

==Problems caused==
The problem of data proliferation is affecting all areas of commerce as the result of the availability of relatively inexpensive data storage devices. This has made it very easy to dump data into secondary storage immediately after its window of usability has passed. This masks problems that could gravely affect the profitability of businesses and the efficient functioning of health services, police and security forces, local and national governments, and many other types of organizations.&lt;ref name=IBM /&gt; Data proliferation is problematic for several reasons:
*Difficulty when trying to find and retrieve information. At [[Xerox]], on average it takes employees more than one hour per week to [[document retrieval|find]] hard-copy documents, costing $2,152 a year to manage and store them. For businesses with more than 10 employees, this increases to almost two hours per week at $5,760 per year.&lt;ref&gt;[http://www.itbusiness.ca/it/client/en/home/News.asp?id=40615&amp;cid=13 &#8220;Dealing with data proliferation&#8221;; Vawn Himmelsbach. it business.ca: Canadian Technology News, September 19, 2006]&lt;/ref&gt; In large [[storage network|networks]] of primary and secondary data storage, problems finding electronic data are analogous to problems finding hard copy data.
*[[Data loss]] and legal liability when data is disorganized, not properly replicated, or cannot be found in a timely manner. In April 2005, the [[TD Ameritrade|Ameritrade Holding Corporation]] told 200,000 current and past customers that a [[Magnetic tape data storage|tape]] containing confidential information had been lost or destroyed in transit. In May of the same year, [[Time Warner Incorporated]] reported that 40 tapes containing personal data on 600,000 current and former employees had been lost en route to a storage facility. In March 2005, a Florida judge hearing a $2.7 billion lawsuit against Morgan Stanley issued an "[[adverse inference]] order" against the company for "willful and gross abuse of its discovery obligations." The judge cited Morgan Stanley for repeatedly finding misplaced tapes of e-mail messages long after the company had claimed that it had turned over all such tapes to the court.&lt;ref&gt;[http://www.computerworld.com/printthis/2005/0,4814,103541,00.html &#8220;Data: Lost, Stolen or Strayed&#8221;, Computer World, Security]&lt;/ref&gt;
*Increased manpower requirements to manage increasingly chaotic data storage resources.
*Slower networks and application performance due to excess traffic as users search and search again for the material they need.&lt;ref name=IBM /&gt;
*High cost in terms of the energy resources required to operate storage hardware. A 100 terabyte system will cost up to $35,040 a year to run&#8212;not counting cooling costs.&lt;ref&gt;[http://findarticles.com/p/articles/mi_m0BRZ/is_10_23/ai_111062988 "Power and storage: the hidden cost of ownership&#8221;, Computer Technology Review, October 2003]&lt;/ref&gt;

==Proposed solutions==
*Applications that better utilize modern technology
*Reductions in duplicate data (especially as caused by data movement)
*Improvement of [[metadata]] structures
*Improvement of file and storage transfer structures
*User education and discipline&lt;ref name=DODPP /&gt;
*The implementation of [[Information Lifecycle Management]] solutions to eliminate low-value information as early as possible before putting the rest into actively managed long-term storage in which it can be quickly and cheaply accessed.&lt;ref name=IBM /&gt;

==See also==
*[[Backup]]
* [[Digital Asset Management]]
*[[Disk storage]]
*[[Document management system]]
*[[Hierarchical storage management]]
*[[Information Lifecycle Management]]
*[[Information repository]]
*[[Magnetic tape data storage]]
*[[Retention period|Retention schedule]]

==References==
{{reflist}}

[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Data management]]</text>
      <sha1>qdcq6awrz8huuimiplm5beyd3igxri9</sha1>
    </revision>
  </page>
  <page>
    <title>Sales intelligence</title>
    <ns>0</ns>
    <id>17420819</id>
    <revision>
      <id>737439139</id>
      <parentid>737258376</parentid>
      <timestamp>2016-09-02T19:56:14Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Business intelligence]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3267" xml:space="preserve">'''Sales intelligence''' (SI) refers to technologies, applications and practices for the collection, integration, analysis, and presentation of information to help salespeople keep up to date with clients, [[Prospect research|prospect data]] and drive business. In addition to providing [[Performance metric|metric]]s for win-loss and sales confidence,&lt;ref&gt;[http://chapmanhq.com/solutions/strategic-account-management-sam/metrics-and-measurements Metrics for sales intelligence]&lt;/ref&gt; SI can present contextually relevant customer and product information.

The 2008 survey of 300 companies by the [[Aberdeen Group]]&lt;ref&gt;[http://www.aberdeen.com/Aberdeen-Library/5379/RA-sales-intelligence-nirvana.aspx Sales Intelligence, Aberdeen Group study (2008)]&lt;/ref&gt; show that the recent economic downturn has lengthened traditional sales cycles. As businesses have been forced to reduce spending, sales representatives have been challenged to meet [[Sales quota|quota]]s. Top performing companies have implemented sales intelligence programs to improve the quality and quantity of sales leads. SI contextualizes opportunities by providing relevant industry, corporate and personal information. Frequently SI's fact-based information is integrated or includes [[customer relationship management]] (CRM).

Although some aspects of sales intelligence overlaps business intelligence (BI), SI is specifically designed for the use of salespeople and sales managers. Unlike [[customer relationship management]] (CRM) and traditional [[business intelligence]] (BI) applications, SI provides real-time analysis of current sales data and assists with suggesting and delivering actionable, relevant information.

Sales intelligence solutions are predominantly designed for companies in the [[manufacturing]], [[distribution (business)|distribution]] and [[wholesale]] sectors. These are highly competitive markets, where volumes are high, [[Profit margin|margin]]s are low. (SI) solutions provide unique insight into customer [[buying pattern]]s. By automatically analysing and evaluating these patterns, Sales Intelligence pro-actively identifies and delivers [[up-sell]], [[cross-sell]] and switch-sell opportunities.

==See also==
* [[Analytics]]
* [[Augmented learning]]
* [[Business intelligence tools]]
* [[Dashboards (management information systems)]]
* [[Location intelligence]]
* [[Market intelligence]]
* [[Marketing intelligence]]
* [[Operational Intelligence]]
* [[OODA Loop]]
* [[Predictive analytics]]
* [[Business Intelligence 2.0]]
* [[Process mining]]
* [[Right-time marketing]]
* [[Integrated business planning]]

==References==
{{reflist}}

==External links==
* [http://blog.findable.me/post/52963306183/sales-intelligence-a-short-primer Sales Intelligence A Short Primer]

[[Category:Business intelligence]]
[[Category:Data management]]

[[da:Business intelligence]]
[[de:Business-Intelligence]]
[[es:Inteligencia empresarial]]
[[fr:Informatique d&#233;cisionnelle]]
[[ko:&#44221;&#50689; &#51221;&#48372;&#54617;]]
[[hr:Poslovna inteligencija]]
[[id:Intelijen bisnis]]
[[it:Business intelligence]]
[[lt:Verslo analitika]]
[[nl:Business intelligence]]
[[pl:Business intelligence]]
[[pt:Business intelligence]]
[[ru:Business Intelligence]]
[[fi:Business intelligence]]
[[sv:Business intelligence]]</text>
      <sha1>nwz89ynaa135d2pva4ejfar7dzaxyao</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Document-oriented databases</title>
    <ns>14</ns>
    <id>19642057</id>
    <revision>
      <id>670394070</id>
      <parentid>641828402</parentid>
      <timestamp>2015-07-07T17:54:58Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>{{see also cat|Key-value databases}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="387" xml:space="preserve">A '''document-oriented database''' is a [[database management system]] designed for document-oriented applications
{{Cat main|Document-oriented database}}
{{see also|Document-oriented database#Implementations}}
{{see also cat|Full text databases}}
{{see also cat|Key-value databases}}

[[Category:Data management]]
[[Category:Database management systems]]
[[Category:Types of databases]]</text>
      <sha1>2yilxram2ry5s8ymwzdm4mto1wj1zkx</sha1>
    </revision>
  </page>
  <page>
    <title>Control flow diagram</title>
    <ns>0</ns>
    <id>21084005</id>
    <revision>
      <id>759302694</id>
      <parentid>658301452</parentid>
      <timestamp>2017-01-10T11:04:13Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[Process control]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5520" xml:space="preserve">{{this|flow diagrams in business process modeling{{clarify|reason=From the lead, I'm unable to give a more informative characterization of 'Control flow diagram'.|date=January 2014}}|directed graphs representing the control flow of imperative computer programs|control flow graph}}
[[File:Performance seeking control flow diagram.jpg|thumb|240px|Example of a "performance seeking" control flow diagram.&lt;ref name="GO92"&gt; Glenn B. Gilyard and John S. Orme (1992) [http://www.nasa.gov/centers/dryden/pdf/88262main_H-1808.pdf ''Subsonic Flight Test Evaluationof a Performance Seeking ControlAlgorithm on an F-15 Airplane''] NASA Technical Memorandum 4400.&lt;/ref&gt;]]
A '''control flow diagram''' ('''CFD''') is a [[diagram]] to describe the [[control flow]] of a [[business process]], [[process (engineering)|process]] or review

Control flow diagrams were developed in the 1950s, and are widely used in multiple [[engineering]] disciplines. They are one of the classic [[business process modeling]] methodologies, along with [[flow chart]]s, [[data flow diagram]]s, [[functional flow block diagram]], [[Gantt chart]]s, [[PERT]] diagrams, and [[IDEF]].&lt;ref name="TD03"&gt; Thomas Dufresne &amp; James Martin (2003). [http://mason.gmu.edu/~tdufresn/paper.doc "Process Modeling for E-Business"]. INFS 770 Methods for Information Systems Engineering:  Knowledge Management and E-Business. Spring 2003&lt;/ref&gt;

== Overview ==
A control flow diagram can consist of a subdivision to show sequential steps, with if-then-else conditions, repetition, and/or case conditions. Suitably annotated geometrical figures are used to represent operations, data, or equipment, and arrows are used to indicate the sequential flow from one to another.&lt;ref&gt;[http://www.fda.gov/ora/Inspect_ref/igs/gloss.html FDA glossary of terminology applicable to software development and computerized systems]. Accessed 14 Jan 2008.&lt;/ref&gt;

There are several types of control flow diagrams, for example:
* Change control flow diagram, used in [[project management]]
* Configuration decision control flow diagram, used in [[configuration management]]
* [[Process control]] flow diagram, used in [[process management]]  
* Quality control flow diagram, used in [[quality control]].

In software and systems development control flow diagrams can be used in [[control flow analysis]], [[data flow analysis]], [[algorithm analysis]], and [[simulation]]. Control and data are most applicable for real time and data driven systems. These flow analyses transform logic and data requirements text into graphic flows which are easier to analyze than the text. PERT, state transition, and transaction diagrams are examples of control flow diagrams.&lt;ref&gt;Dolores R. Wallace et al. (1996). [http://hissa.nist.gov/HHRFdata/Artifacts/ITLdoc/234/val-proc.html ''Reference Information for the Software Verification and Validation Process''], NIST Special Publication 500-234.&lt;/ref&gt;

== Types of Control Flow Diagrams ==
=== Process Control Flow Diagram ===
A flow diagram can be developed for the process [[control system]] for each critical activity. Process control is normally a closed cycle in which a [[sensor]] provides information to a process control [[software application]] through a [[communications system]]. The application determines if the sensor information is within the predetermined (or calculated) data parameters and constraints. The results of this comparison are fed to an actuator, which controls the critical component. This [[feedback]] may control the component electronically or may indicate the need for a manual action.&lt;ref name="NIoJ02"&gt; National Institute of Justice (2002). [http://www.ncjrs.gov/txtfiles1/nij/195171.txt '' A Method to Assess the Vulnerability of U.S. Chemical Facilities]''. Series: Special Report.&lt;/ref&gt; 

This closed-cycle process has many checks and balances to ensure that it stays safe. The investigation of how the process control can be subverted is likely to be extensive because all or part of the process control may be
oral instructions to an individual monitoring the process. It may be fully computer controlled and automated, or it may be a hybrid in which only the sensor is automated and the action requires manual intervention. Further, some process control systems may use prior generations of hardware and software, while others are state of the art.&lt;ref name="NIoJ02"/&gt;

=== Performance seeking control flow diagram ===
The figure presents an example of a performance seeking control [[flow diagram]] of the algorithm. The control law consists of estimation, modeling, and optimization processes. In the [[Kalman filter]] estimator, the inputs, outputs, and residuals were recorded. At the compact propulsion system modeling stage, all the estimated inlet and engine parameters were recorded.&lt;ref name="GO92"/&gt;  

In addition to temperatures, pressures, and control positions, such estimated parameters as stall margins, thrust, and drag components were recorded. In the optimization phase, the operating condition constraints, optimal solution, and linear programming health status condition codes were recorded. Finally, the actual commands that were sent to the engine through the DEEC were recorded.&lt;ref name="GO92"/&gt;

== See also ==
* [[Data flow diagram]]
* [[Control flow graph]]
* [[DRAKON]]
* [[Flow process chart]]

== References ==
{{NIST-PD}}
{{reflist}}

[[Category:Information systems]]
[[Category:Data management]]
[[Category:Diagrams]]
[[Category:Systems analysis]]
{{DEFAULTSORT:Control Flow Diagram}}</text>
      <sha1>aolqmm6ooor49a6htq2suku0tqu9j8j</sha1>
    </revision>
  </page>
  <page>
    <title>Isolation (database systems)</title>
    <ns>0</ns>
    <id>325521</id>
    <revision>
      <id>759075398</id>
      <parentid>757215271</parentid>
      <timestamp>2017-01-09T02:47:55Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21326" xml:space="preserve">{{Refimprove|date=January 2009}}
In [[database]] systems, '''isolation''' determines how transaction integrity is visible to other users and systems. For example, when a user is creating a Purchase Order and has created the header, but not the Purchase Order lines, is the header available for other systems/users, carrying out [[Concurrency (computer science)|concurrent]] operations (such as a report on Purchase Orders), to see?

A lower isolation level increases the ability of many users to access data at the same time, but increases the number of concurrency effects (such as dirty reads or lost updates) users might encounter. Conversely, a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.&lt;ref&gt;"Isolation Levels in the Database Engine", Technet, Microsoft, http://technet.microsoft.com/en-us/library/ms189122(v=SQL.105).aspx&lt;/ref&gt;

Isolation is typically defined at database level as a property that defines how/when the changes made by one operation become visible to other. On older systems, it may be implemented systemically, for example through the use of temporary tables. In two-tier systems, a Transaction Processing (TP) manager is required to maintain isolation. In n-tier systems (such as multiple websites attempting to book the last seat on a flight), a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer.&lt;ref&gt;"The Architecture of Transaction Processing Systems", Chapter 23, Evolution of Processing Systems, Department of Computer Science, Stony Brook University, retrieved 20 March 2014, http://www.cs.sunysb.edu/~liu/cse315/23.pdf&lt;/ref&gt;

Isolation is one of the [[ACID]] (Atomicity, Consistency, Isolation, Durability) properties.

==Concurrency control==
[[Concurrency control]] comprises the underlying mechanisms in a [[DBMS]] which handles isolation and guarantees related correctness. It is heavily utilized by the database and storage engines (see above) both to guarantee the correct execution of concurrent transactions, and (different mechanisms) the correctness of other DBMS processes. The transaction-related mechanisms typically constrain the database data access operations' timing ([[Schedule (computer science)|transaction schedules]]) to certain orders characterized as the [[serializability]] and [[recoverability]] schedule properties. Constraining database access operation execution typically means reduced performance (rates of execution), and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints. Often, when possible without harming correctness, the serializability property is compromised for better performance. However, recoverability cannot be compromised, since such typically results in a quick database integrity violation.

[[Two-phase locking]] is the most common transaction concurrency control method in DBMSs, used to provide both serializability and recoverability for correctness. In order to access a database object a transaction first needs to acquire a [[Lock (database)|lock]] for this object. Depending on the access operation type (e.g., reading or writing an object) and on the lock type, acquiring the lock may be blocked and postponed, if another transaction is holding a lock for that object.

==Isolation levels==
Of the four [[ACID]] properties in a [[Database management system|DBMS]] (Database Management System), the isolation property is the one most often relaxed.  When attempting to maintain the highest level of isolation, a DBMS usually acquires [[Lock (database)|locks]] on data or implements [[multiversion concurrency control]], which may result in a loss of [[concurrency (computer science)|concurrency]].  This requires adding logic for the [[software application|application]] to function correctly.

Most DBMSs offer a number of ''transaction isolation levels'', which control the degree of locking that occurs when selecting data. For many database applications, the majority of database transactions can be constructed to avoid requiring high isolation levels (e.g. SERIALIZABLE level), thus reducing the locking overhead for the system.  The programmer must carefully analyze database access code to ensure that any relaxation of isolation does not cause software bugs that are difficult to find. Conversely, if higher isolation levels are used, the possibility of [[deadlock]] is increased, which also requires careful analysis and programming techniques to avoid.

The isolation levels defined by the [[American National Standards Institute|ANSI]]/[[International Organization for Standardization|ISO]] [[SQL]] standard are listed as follows.

===Serializable===
This is the ''highest'' isolation level.

With a lock-based [[concurrency control]] DBMS implementation, [[serializability]] requires read and write locks (acquired on selected data) to be released at the end of the transaction.  Also ''range-locks'' must be acquired when a [[Select (SQL)|SELECT]] query uses a ranged ''WHERE'' clause, especially to avoid the ''phantom reads'' phenomenon (see below).

When using non-lock based concurrency control, no locks are acquired; however, if the system detects a ''write collision'' among several concurrent transactions, only one of them is allowed to commit.  See ''[[snapshot isolation]]'' for more details on this topic.

From : (Second Informal Review Draft) ISO/IEC 9075:1992, Database Language SQL- July 30, 1992:
''The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.''

===Repeatable reads===
In this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction.  However, ''range-locks'' are not managed, so '''''[[Isolation (database systems)#Phantom reads|phantom reads]]''''' can occur.

Also, write skew is possible when one transaction updates column to some color whereas competing transactions updates the same column to some other color(s). In serial execution of the transactions, you should end up with the whole column unicolored whereas repeatable read admits a mixture of colors.&lt;ref&gt;[https://wiki.postgresql.org/wiki/SSI#Simple_Write_Skew Postgresql wiki - SSI]&lt;/ref&gt;

===Read committed===
In this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the [[Select (SQL)|SELECT]] operation is performed (so the ''non-repeatable reads'' phenomenon can occur in this isolation level, as discussed below). As in the previous level, ''range-locks'' are not managed.

Putting it in simpler words, read committed is an isolation level that guarantees that any data read is committed at the moment it is read. It simply restricts the reader from seeing any intermediate, uncommitted, 'dirty' read. It makes no promise whatsoever that if the transaction re-issues the read, it will find the same data; data is free to change after it is read.

===Read uncommitted===
This is the ''lowest'' isolation level. In this level, '''''[[Isolation (database systems)#Dirty reads|dirty reads]]''''' are allowed, so one transaction may see ''not-yet-committed'' changes made by other transactions.

Since each isolation level is stronger than those below, in that no higher isolation level allows an action forbidden by a lower one, the standard permits a DBMS to run a transaction at an isolation level stronger than that requested (e.g., a "Read committed" transaction may actually be performed at a "Repeatable read" isolation level).

==Default isolation level==
The ''default isolation level'' of different [[Database management system|DBMS]]'s varies quite widely. Most databases that feature transactions allow the user to set any isolation level. Some DBMS's also require additional syntax when performing a SELECT statement to acquire locks (e.g. ''SELECT ... FOR UPDATE'' to acquire exclusive write locks on accessed rows).

However, the definitions above have been criticized as being ambiguous, and as not accurately reflecting the isolation provided by many databases:

:This paper shows a number of weaknesses in the anomaly approach to defining isolation levels. The three ANSI phenomena are ambiguous, and even in their loosest interpretations do not exclude some anomalous behavior ... This leads to some counter-intuitive results. In particular, lock-based isolation levels have different characteristics than their ANSI equivalents. This is disconcerting because commercial database systems typically use locking implementations. Additionally, the ANSI phenomena do not distinguish between a number of types of isolation level behavior that are popular in commercial systems.&lt;ref name="sql-isolation"&gt;
{{cite web
| url = http://www.cs.umb.edu/~poneil/iso.pdf
| title = A Critique of ANSI SQL Isolation Levels
| accessdate = 29 July 2012 }}
&lt;/ref&gt;

There are also other criticisms concerning ANSI SQL's isolation definition, in that it encourages implementors to do "bad things":

:... it relies in subtle ways on an assumption that a locking schema is used for concurrency control, as opposed to an optimistic or multi-version concurrency scheme. This implies that the proposed semantics are ''ill-defined''.&lt;ref&gt;{{cite web
| accessdate = 2010-03-09
| publisher = DataStax
| location = www.DataStax.com
| title = Customer testimonials (SimpleGeo, CLOUDSTOCK 2010)
| author = salesforce
| date = 2010-12-06
| url = https://www.youtube.com/v/7J61pPG9j90?version=3
| quote = (see above at about 13:30 minutes of the webcast!)}}&lt;/ref&gt;

==Read phenomena==
The ANSI/ISO standard SQL 92 refers to three different ''read phenomena'' when Transaction 1 reads data that Transaction 2 might have changed.

In the following examples, two transactions take place. In the first, Query 1 is performed. Then, in the second transaction, Query 2 is performed and committed. Finally, in the first transaction, Query 1 is performed again.

The queries use the following data table:

{|class="wikitable"
|+ users
! id !! name !! age
|-
| 1  || Joe  || 20
|-
| 2  || Jill || 25
|}

===Dirty reads===
A ''dirty read'' (aka ''uncommitted dependency'') occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.

Dirty reads work similarly to [[Isolation (database systems)#Non-repeatable reads|non-repeatable reads]]; however, the second transaction would not need to be committed for the first query to return a different result. The only thing that may be prevented in the READ UNCOMMITTED isolation level is updates appearing out of order in the results; that is, earlier updates will always appear in a result set before later updates.

In our example, Transaction 2 changes a row, but does not commit the changes.  Transaction 1 then reads the uncommitted data.  Now if Transaction 2 rolls back its changes (already read by Transaction 1) or updates different changes to the database, then the view of the data may be wrong in the records of Transaction 1.

{|style="font-size: 94%;"
|-
! Transaction 1
! Transaction 2
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 20 */
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
/* No commit here */
&lt;/source&gt;
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 21 */
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
ROLLBACK; /* lock-based DIRTY READ */
&lt;/source&gt;
|}

But in this case no row exists that has an id of 1 and an age of 21.

===Non-repeatable reads===
A ''non-repeatable read'' occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.

''Non-repeatable reads'' phenomenon may occur in a lock-based concurrency control method when read locks are not acquired when performing a [[Select (SQL)|SELECT]], or when the acquired locks on affected rows are released as soon as the SELECT operation is performed.  Under the [[multiversion concurrency control]] method, ''non-repeatable reads'' may occur when the requirement that a transaction affected by a [[commit conflict]] must roll back is relaxed.

{|style="font-size: 94%;"
|-
! Transaction 1
! Transaction 2
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users WHERE id = 1;
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
COMMIT; /* in multiversion concurrency
   control, or lock-based READ COMMITTED */
&lt;/source&gt;
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users WHERE id = 1;
COMMIT; /* lock-based REPEATABLE READ */
&lt;/source&gt;
|}

In this example, Transaction 2 commits successfully, which means that its changes to the row with id 1 should become visible. However, Transaction 1 has already seen a different value for ''age'' in that row. At the SERIALIZABLE and REPEATABLE READ isolation levels, the DBMS must return the old value for the second SELECT. At READ COMMITTED and READ UNCOMMITTED, the DBMS may return the updated value; this is a non-repeatable read.

There are two basic strategies used to prevent non-repeatable reads. The first is to delay the execution of Transaction 2 until Transaction 1 has committed or rolled back. This method is used when locking is used, and produces the serial [[Schedule (computer science)|schedule]] '''T1, T2'''. A serial schedule exhibits ''repeatable reads'' behaviour.

In the other strategy, as used in ''[[multiversion concurrency control]]'', Transaction 2 is permitted to commit first, which provides for better concurrency. However, Transaction 1, which commenced prior to Transaction 2, must continue to operate on a past version of the database&amp;nbsp;&#8212; a snapshot of the moment it was started. When Transaction 1 eventually tries to commit, the DBMS checks if the result of committing Transaction 1 would be equivalent to the schedule '''T1, T2'''. If it is, then Transaction 1 can proceed. If it cannot be seen to be equivalent, however, Transaction 1 must roll back with a serialization failure.

Using a lock-based concurrency control method, at the REPEATABLE READ isolation mode, the row with ID = 1 would be locked, thus blocking Query 2 until the first transaction was committed or rolled back. In READ COMMITTED mode, the second time Query 1 was executed, the age would have changed.

Under multiversion concurrency control, at the SERIALIZABLE isolation level, both SELECT queries see a snapshot of the database taken at the start of Transaction 1. Therefore, they return the same data. However, if Transaction 1 then attempted to UPDATE that row as well, a serialization failure would occur and Transaction 1 would be forced to roll back.

At the READ COMMITTED isolation level, each query sees a snapshot of the database taken at the start of each query. Therefore, they each see different data for the updated row. No serialization failure is possible in this mode (because no promise of serializability is made), and Transaction 1 will not have to be retried.

===Phantom reads===
A ''phantom read'' occurs when, in the course of a transaction, two identical queries are executed, and the collection of rows returned by the second query is different from the first.

This can occur when ''[[range locks]]'' are not acquired on performing a ''[[Select (SQL)|SELECT]] ... WHERE'' operation.
The ''phantom reads'' anomaly is a special case of ''Non-repeatable reads'' when Transaction 1 repeats a ranged ''SELECT ... WHERE'' query and, between both operations, Transaction 2 creates (i.e. [[INSERT]]) new rows (in the target table) which fulfill that ''WHERE'' clause.

{|style="font-size: 95%;"
|-
! Transaction 1
! Transaction 2
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
/* Query 2 */
INSERT INTO users(id,name,age) VALUES ( 3, 'Bob', 27 );
COMMIT;
&lt;/source&gt;
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
COMMIT;
&lt;/source&gt;
|
|}

Note that Transaction 1 executed the same query twice. If the highest level of isolation were maintained, the same set of rows should be returned both times, and indeed that is what is mandated to occur in a database operating at the SQL SERIALIZABLE isolation level. However, at the lesser isolation levels, a different set of rows may be returned the second time.

In the SERIALIZABLE isolation mode, Query 1 would result in all records with age in the range 10 to 30 being locked, thus Query 2 would block until the first transaction was committed. In REPEATABLE READ mode, the range would not be locked, allowing the record to be inserted and the second execution of Query 1 to include the new row in its results.

==Isolation Levels, Read Phenomena and Locks==

===Isolation Levels vs Read Phenomena===
{|class="wikitable"
! Isolation level !! Dirty reads !! Non-repeatable reads !! Phantoms
|-
| Read Uncommitted  || may occur || may occur || may occur
|-
| Read Committed  || - || may occur || may occur
|-
| Repeatable Read || - || - || may occur
|-
| Serializable || - || - || -
|}

Anomaly Serializable is not the same as Serializable. That is, it is necessary, but not sufficient that a Serializable schedule should be free of all three phenomena types.&lt;ref name="sql-isolation"/&gt;

"may occur" means that the isolation level suffers that phenomenon, while "-" means that it does not suffer it.

===Isolation Levels vs Lock Duration ===
{{Citation needed|reason=This is a still a mess. Write operations always place locks until commit, whatever isolation level is used. Queries (selects/reads) never place any kind of lock under Read Uncommitted isolation level. Locks placed by read operations (the only operations that are affected by the isolation level used) should not be referred as Read and Range, but as Data Locks and Predicate Locks. Write operations also place Data and Predicate locks, always until commit, whatever isolation level is used.|date=January 2014}}

In lock-based concurrency control, isolation level determines the duration that locks are held.&lt;br&gt;  '''"C"''' - Denotes that locks are held until the transaction commits.&lt;br&gt;
'''"S"''' - Denotes that the locks are held only during the currently executing statement.  Note that if locks are released after a statement, the underlying data could be changed by another transaction before the current transaction commits, thus creating a violation.

{|class="wikitable"
! Isolation level !! Write Operation !! Read Operation !! Range Operation (...where...)
|-
| Read Uncommitted  || S || S || S
|-
| Read Committed  || C || S || S
|-
| Repeatable Read || C || C || S
|-
| Serializable || C || C || C
|}

==See also==
* [[Atomicity (database systems)|Atomicity]]
* [[Consistency (database systems)|Consistency]]
* [[Durability (database systems)|Durability]]
* [[Lock (database)]]
* [[Optimistic concurrency control]]
* [[Relational Database Management System]]
* [[Snapshot isolation]]

==References==
{{Reflist}}

==External links==
* [http://docs.oracle.com/cd/B12037_01/server.101/b10743/toc.htm  Oracle&#174; Database Concepts], [http://docs.oracle.com/cd/B12037_01/server.101/b10743/consist.htm#sthref1919 chapter 13 Data Concurrency and Consistency, Preventable Phenomena and Transaction Isolation Levels]
* [http://docs.oracle.com/cd/B19306_01/server.102/b14200/toc.htm Oracle&#174; Database SQL Reference], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10.htm#i2068385 chapter 19 SQL Statements: SAVEPOINT to UPDATE], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10005.htm#i2067247 SET TRANSACTION]
&lt;!-- representations in api: java --&gt;
* in [[Java Database Connectivity|JDBC]]: [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#field_summary Connection constant fields], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#getTransactionIsolation() Connection.getTransactionIsolation()], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#setTransactionIsolation(int) Connection.setTransactionIsolation(int)]
* in [[Spring Framework]]: [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Transactional.html @Transactional], [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Isolation.html Isolation]
&lt;!-- representations in api: .NET_Framework --&gt;
* [http://www.bailis.org/blog/when-is-acid-acid-rarely/ P.Bailis. When is "ACID" ACID? Rarely]

{{Authority control}}

{{DEFAULTSORT:Isolation (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>1tifb0noy4j1ylnpqhiecd8jyhmvewh</sha1>
    </revision>
  </page>
  <page>
    <title>Operational system</title>
    <ns>0</ns>
    <id>14190268</id>
    <revision>
      <id>700186629</id>
      <parentid>621313214</parentid>
      <timestamp>2016-01-16T23:31:02Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Synonyms */ caps</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1369" xml:space="preserve">{{distinguish|Operating system}}
An '''operational system''' is a term used in [[data warehousing]] to refer to a system that is used to process the day-to-day transactions of an organization. These systems are designed in a manner that processing of day-to-day transactions is performed efficiently and the integrity of the transactional data is preserved.
== Synonyms ==
Sometimes operational systems are referred to as [[operational database]]s, [[transaction processing system]]s, or [[online transaction processing]] systems (OLTP). However, the use of the last two terms as synonyms may be confusing, because operational systems can be [[batch processing]] systems as well.

Any enterprise must necessarily maintain a lot of data about its operation. This is its "operational data".

{| class="wikitable" border="1"
|-
! Organization
! Probably
|-
| Manufacturing Company 
| Product data
|-
| Bank
| Account Data
|-
| Hospital
| Patient Data
|-
| University
| Student Data
|-
| Government Department
| Planning data
|}

==See also==
* [[Operating system]] (OS)
* [[Data warehouse#Data warehouses versus operational systems|Data warehouses versus operational systems]]

{{database-stub}}

{{DEFAULTSORT:Operational System}}
[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Business intelligence]]</text>
      <sha1>dq3z1auop60p3e1q76xio2bddfsfczz</sha1>
    </revision>
  </page>
  <page>
    <title>Database engine</title>
    <ns>0</ns>
    <id>209503</id>
    <revision>
      <id>745928263</id>
      <parentid>723318473</parentid>
      <timestamp>2016-10-24T06:38:13Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* External links */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8036" xml:space="preserve">A '''database engine''' (or '''storage engine''') is the underlying software component that a [[database management system]] (DBMS) uses to [[create, read, update and delete]] (CRUD) [[data]] from a [[database]]. Most database management systems include their own [[application programming interface]] (API) that allows the user to interact with their underlying engine without going through the user interface of the DBMS.

The term "database engine" is frequently used interchangeably with "[[database server]]" or "database management system". A 'database instance' refers to the processes and memory structures of the running '''database engine'''.

==Storage engines==
Many of the modern DBMS support multiple storage engines within the same database. For example, [[MySQL]] supports [[InnoDB]] as well as [[MyISAM]].

Some storage engines are [[Database transaction|transactional]].

{| class="wikitable"
|-
! Name !! License !! Transactional
|-
| [[Aria (storage engine)|Aria]] || GPL || {{No}}
|-
| BlitzDB || GPL || {{No}}
|-
| [[Falcon (storage engine)|Falcon]] || GPL || {{Yes}}
|-
| [[InnoDB]] || GPL || {{Yes}}
|-
| [[MyISAM]] || GPL || {{No}}
|-
| [[InfiniDB]] || CPL || {{No}}
|-
| [[TokuDB]] || GPL || {{Yes}}
|-
| [[WiredTiger]] || GPL || {{Yes}}
|-
| [[XtraDB]] || GPL || {{Yes}}
|}

Additional engine types include:
*[[Embedded database]] engines
*[[In-memory database]] engines

==Design considerations==
Database bits are laid out in storage in data structures and groupings that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties. Typically the storage itself is designed to meet requirements of various areas that extensively utilize storage, including databases. A DBMS in operation always simultaneously utilizes several storage types (e.g., memory, and external storage), with respective layout methods.

In principle the database storage can be viewed as a [[linear address space]], where every bit of data has its unique address in this address space. In practice, only a very small percentage of addresses are kept as initial reference points (which also requires storage); most data is accessed by indirection using displacement calculations (distance in bits from the reference points) and data structures which define access paths (using pointers) to all needed data in an effective manner, optimized for the needed data access operations.

===Database storage hierarchy===
A database, while in operation, resides simultaneously in several types of storage, forming a [[storage hierarchy]]. By the nature of contemporary computers most of the database part inside a computer that hosts the DBMS resides (partially replicated) in volatile storage. Data (pieces of the database) that are being processed/manipulated reside inside a processor, possibly in [[CPU cache|processor's caches]]. These data are being read from/written to memory, typically through a computer [[Bus (computing)|bus]] (so far typically volatile storage components). Computer memory is communicating data (transferred to/from) external storage, typically through standard storage interfaces or networks (e.g., [[fibre channel]], [[iSCSI]]). A [[Disk array|storage array]], a common external storage unit, typically has storage hierarchy of its own, from a fast cache, typically consisting of (volatile and fast) [[DRAM]], which is connected (again via standard interfaces) to drives, possibly with different speeds, like [[USB flash drive|flash drives]] and magnetic [[disk drive]]s (non-volatile). The drives may be connected to [[magnetic tape]]s, on which typically the least active parts of a large database may reside, or database backup generations.

Typically a correlation exists currently between storage speed and price, while the faster storage is typically volatile.

===Data structures===
{{Main|Database storage structures}}
A data structure is an abstract construct that embeds data in a well defined manner. An efficient data structure allows to manipulate the data in efficient ways. The data manipulation may include data insertion, deletion, updating and retrieval in various modes. A certain data structure type may be very effective in certain operations, and very ineffective in others. A data structure type is selected upon DBMS development to best meet the operations needed for the types of data it contains. Type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in (e.g., speed of access, minimal size of storage chunk accessed, etc.). In some DBMSs database administrators have the flexibility to select among options of data structures to contain user data for performance reasons. Sometimes the data structures have selectable parameters to tune the database performance.

Databases may store data in many data structure types.&lt;ref name="Physical Database Design"&gt;{{harvnb|Lightstone|Teorey|Nadeau|2007}}&lt;/ref&gt; Common examples are the following:
*ordered/unordered [[flat file database|flat files]]
*[[hash table]]s
*[[B+ tree]]s
*[[ISAM]]
*[[heap (data structure)|heaps]]

===Data orientation and clustering===
In contrast to conventional row-orientation, relational databases can also be [[Column-oriented DBMS|column-oriented]] or [[Correlational database|correlational]] in the way they store data in any particular structure.

In general, substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity, being "clustered". This usually allows to retrieve needed related objects from storage in minimum number of input operations (each sometimes substantially time consuming). Even for in-memory databases clustering provides performance advantage due to common utilization of large caches for input-output operations in memory, with similar resulting behavior.

For example, it may be beneficial to cluster a record of an "item" in stock with all its respective "order" records. The decision of whether to cluster certain objects or not depends on the objects' utilization statistics, object sizes, caches sizes, storage types, etc.

===Database indexing===
{{Main|Database index}}
Indexing is a technique some storage engines use for improving database performance. The many types of indexes share the common property that they reduce the need to examine every entry when running a query. In large databases, this can reduce query time/cost by orders of magnitude. The simplest form of index is a sorted list of values that can be searched using a [[binary search]] with an adjacent reference to the location of the entry, analogous to the index in the back of a book. The same data can have multiple indexes (an employee database could be indexed by last name and hire date).

Indexes affect performance, but not results. Database designers can add or remove indexes without changing application logic, reducing maintenance costs as the database grows and database usage evolves.  Indexes can speed up data access, but they consume space in the database, and must be updated each time the data is altered. Indexes therefore can speed data access but slow data maintenance. These two properties determine whether a given index is worth the cost.

==See also==
{{cleanup-merge}} &lt;!--Into chart --&gt;
*[[Architecture of Btrieve#Micro-Kernel Database Engine|Btrieve's Micro-Kernel Database Engine]]
*[[Berkeley DB]]
*[[c-treeACE|c-treeACE Database Engine]]
*[[FLAIM Database Engine]]
*[[Microsoft Jet Database Engine]]
*[[MySQL Cluster]], on the NDB storage engine of [[MySQL]]
*[[NuoDB]]

==References==
{{Reflist}}

==External links==
*http://dev.mysql.com/tech-resources/articles/storage-engine/part_3.html
*[https://books.google.com/books?id=PqZ6QytCemcC&amp;pg=PT287&amp;dq=storage+engines MySQL Administrator's Bible] Chapter 11 "Storage Engines"

{{DEFAULTSORT:Database Engine}}
[[Category:Data management]]
[[Category:Database engines| ]]
[[Category:Database management systems]]</text>
      <sha1>qxtupdiiuy6oik3o6jzdr8wkljvfidn</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Structured storage</title>
    <ns>14</ns>
    <id>25067249</id>
    <revision>
      <id>548068555</id>
      <parentid>389257360</parentid>
      <timestamp>2013-04-01T00:36:50Z</timestamp>
      <contributor>
        <username>Jerome Charles Potts</username>
        <id>562899</id>
      </contributor>
      <comment>removed [[Category:Databases]] using [[WP:HC|HotCat]] entries in here are not necessarily databases</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="61" xml:space="preserve">{{Cat main|Structured storage}}

[[Category:Data management]]</text>
      <sha1>iihgl772os0xqjp9pex5h3mbfuecwl0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Open data</title>
    <ns>14</ns>
    <id>25128034</id>
    <revision>
      <id>641372190</id>
      <parentid>628514013</parentid>
      <timestamp>2015-01-07T07:49:27Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="99" xml:space="preserve">{{cat main}}
[[Category:Open content]]
[[Category:Free software|Data]]
[[Category:Data management]]</text>
      <sha1>jm9rcjkacqe4shupx9drt15yq9mwsqj</sha1>
    </revision>
  </page>
  <page>
    <title>Synthetic data</title>
    <ns>0</ns>
    <id>25270778</id>
    <revision>
      <id>744133740</id>
      <parentid>713098332</parentid>
      <timestamp>2016-10-13T09:34:02Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Applications */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11898" xml:space="preserve">{{Citation style|date=May 2014}}'''Synthetic data''' are "any production data applicable to a given situation that are not obtained by direct measurement" according to the McGraw-Hill Dictionary of Scientific and Technical Terms;&lt;ref name="McGraw"&gt;Synthetic data. (n.d.). ''McGraw-Hill Dictionary of Scientific and Technical Terms''. Retrieved November 29, 2009, from Answers.com Web site: [http://www.answers.com/topic/synthetic-data]&lt;/ref&gt; where Craig S. Mullins, an expert in data management, defines production data as "information that is persistently stored and used by professionals to conduct business processes.".&lt;ref name="Mullins"&gt;Mullins, Craig S. (2009, February 5). ''What is Production Data?'' Message posted to http://www.neon.com/blog/blogs/cmullins/archive/2009/02/05/What-is-Production-Data_3F00_.aspx&lt;/ref&gt;

The creation of synthetic data is an involved process of data [[Anonymity|anonymization]]; that is to say that synthetic data is a [[subset]] of anonymized data.&lt;ref name="MachanavajjhalaEtAl"&gt;{{Cite journal
  | title = Privacy: Theory meets Practice on the Map
  | journal = 2008 IEEE 24th International Conference on Data Engineering
  | doi = 10.1109/ICDE.2008.4497436
  | pages = 277&#8211;286
  | year = 2008
  | last1 = MacHanavajjhala
  | first1 = Ashwin
  | last2 = Kifer
  | first2 = Daniel
  | last3 = Abowd
  | first3 = John
  | last4 = Gehrke
  | first4 = Johannes
  | last5 = Vilhuber
  | first5 = Lars}}&lt;/ref&gt; Synthetic data is used in a variety of fields as a filter for information that would otherwise compromise the [[confidentiality]] of particular aspects of the data. Many times the particular aspects come about in the form of human information (i.e. name, home address, [[IP address]], telephone number, social security number, credit card number, etc.).

== Usefulness ==

Synthetic data are generated to meet specific needs or certain conditions that may not be found in the original, real data.  This can be useful when designing any type of system because the synthetic data are used as a simulation or as a theoretical value, situation, etc.  This allows us to take into account unexpected results and have a basic solution or remedy, if the results prove to be unsatisfactory. Synthetic data are often generated to represent the authentic data and allows a baseline to be set.&lt;ref name="Barse"&gt;Barse, E.L., Kvarnstr&#246;m, H., &amp; Jonsson, E. (2003). ''Synthesizing test data for fraud detection systems.'' Manuscript submitted for publication, Department of Computer Engineering, Chalmbers University of Technology, G&#246;teborg, Sweden. Retrieved from http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1254343&amp;isnumber=28060&lt;/ref&gt; Another use of synthetic data is to protect privacy and confidentiality of authentic data. As stated previously, synthetic data is used in testing and creating many different types of systems; below is a quote from the abstract of an article that describes a software that generates synthetic data for testing fraud detection systems that further explains its use and importance.
"This enables us to create realistic behavior profiles for users and attackers. The data is used to train the [[fraud]] detection system itself, thus creating the necessary adaptation of the system to a specific environment."&lt;ref name="Barse"/&gt;

==History==
The history of the generation of synthetic data dates back to 1993. In 1993, the idea of original fully synthetic data was created by [[Donald Rubin|Rubin]].&lt;ref name="Rubin1993"&gt;{{Cite journal
  | authorlink = Rubin, Donald B.
  | title = Discussion: Statistical Disclosure Limitation
  | journal = Journal of Official Statistics
  | volume = 9
  | pages = 461&#8211;468
  | year = 1993}}
&lt;/ref&gt; Rubin originally designed this to synthesize the Decennial Census long form responses for the short form households. He then released samples that did not include any actual long form records - in this he preserved anonymity of the household.&lt;ref name="Abowd"&gt;
{{Cite web
  | last = Abowd
  | first = John M.
  | title = Confidentiality Protection of Social Science Micro Data: Synthetic Data and Related Methods. [Powerpoint slides]
  | url=http://www.idre.ucla.edu/events/PPT/2006_01_30_abowd_UCLA_synthetic_data_presentation.ppt
  | accessdate = 17 February 2011 }} 
&lt;/ref&gt; Later that year, the idea of original partially synthetic data was created by Little. Little used this idea to synthesize the sensitive values on the public use file.&lt;ref name="Little"&gt;{{Cite journal
  | authorlink = Little, Rod
  | title = Statistical Analysis of Masked Data
  | journal = Journal of Official Statistics
  | volume = 9
  | pages = 407&#8211;426
  | year = 1993}}
&lt;/ref&gt;

In 1994, [[Stephen Fienberg|Fienberg]] came up with the idea of critical refinement, in which he used a parametric posterior predictive distribution (instead of a Bayes bootstrap) to do the sampling.&lt;ref name="Abowd"/&gt; Later, other important contributors to the development of synthetic data generation are [[Trivellore Raghunathan|Raghunathan]], [[Jerry Reiter|Reiter]], [[Donald Rubin|Rubin]], [[John M. Abowd|Abowd]], [[Jim Woodcock|Woodcock]]. Collectively they came up with a solution for how to treat partially synthetic data with missing data. Similarly they came up with the technique of Sequential Regression Multivariate [[Imputation (statistics)|Imputation]].&lt;ref name="Abowd"/&gt;

==Applications==
Synthetic data are used in the process of [[data mining]].  Testing and training [[fraud]] detection systems, confidentiality systems and any type of system is devised using synthetic data. As described previously, synthetic data may seem as just a compilation of &#8220;made up&#8221; data, but there are specific algorithms and generators that are designed to create realistic data.&lt;ref name="Deng"&gt;Deng, R. (2002). ''Information and Communications Security''. Proceedings of the 4th International Conference, ICICS 2002 Singapore, December 2002. Retrieved from https://books.google.com/books?id=6mod7enQa8cC&amp;pg=PA265&amp;dq=%22synthetic+data%22#v=onepage&amp;q=%22synthetic%20data%22&amp;f=false&lt;/ref&gt; This synthetic data assists in teaching a system how to react to certain situations or criteria. Researcher doing [[clinical trials]] or any other research may generate synthetic data to aid in creating a baseline for future studies and testing.  For example, intrusion detection software is tested using synthetic data. This data is a representation of the authentic data and may include intrusion instances that are not found in the authentic data. The synthetic data allows the software to recognize these situations and react accordingly. If synthetic data was not used, the software would only be trained to react to the situations provided by the authentic data and it may not recognize another type of intrusion.&lt;ref name="Barse"/&gt;

Synthetic data is also used to protect the [[privacy]] and [[confidentiality]] of a set of data. Real data contains personal/private/confidential information that a programmer, software creator or research project may not want to be disclosed.&lt;ref name="Abowd2"&gt;Abowd, J.M., &amp; Lane, J. (2004). ''New Approaches to Confidentiality Protection: Synthetic Data, Remote Access and Research Data Centers''. Manuscript submitted for publication, Cornell Institute for Social and Economic Research (CISER), Cornell University, Ithica, New York. Retrieved from http://www.springerlink.com/content/27nud7qx09qurg3p/fulltext.pdf&lt;/ref&gt; Synthetic data holds no personal information and cannot be traced back to any individual; therefore, the use of synthetic data reduces confidentiality and privacy issues.

==Calculations==
Researchers test the framework on synthetic data, which is "the only source of ground truth on which they can objectively assess the performance of their [[algorithm]]s".&lt;sup&gt;10&lt;/sup&gt;

"Synthetic data can be generated with random orientations and positions."&lt;sup&gt;8&lt;/sup&gt;  Datasets can be get fairly complicated. A more complicated dataset can be generated by using a synthesizer build.  To create a synthesizer build, first use the original data to create a model or equation that fits the data the best. This model or equation will be called a synthesizer build. This build can be used to generate more data.&lt;sup&gt;9&lt;/sup&gt;

Constructing a synthesizer build involves constructing a [[statistical model]].  In a [[linear regression]] line example, the original data can be plotted, and a best fit [[linear regression|linear line]] can be created from the data.  This [[linear regression|line]] is a synthesizer created from the original data.  The next step will be generating more synthetic data from the synthesizer build or from this linear line equation.  In this way, the new data can be used for studies and research, and it protects the [[confidentiality]] of the original data.&lt;sup&gt;9&lt;/sup&gt;

David Jensen from the Knowledge Discovery Laboratory mentioned how to generate synthetic data in his "Proximity 4.3 Tutorial" chapter 6: "Researchers frequently need to explore the effects of certain data characteristics on their [[data model]]." To help construct [[data set|datasets]] exhibiting specific properties, such as [[autocorrelation|auto-correlation]] or degree disparity, proximity can generate synthetic data having one of several types of graph structure&lt;sup&gt;10&lt;/sup&gt;:[[random graph]]s that is generated by some [[random process]];[[lattice graph]]s having a ring structure;[[lattice graph]]s having a grid structure, etc.
In all cases, the data generation process follows the same process:
1.	Generate the empty [[Graph (data structure)|graph structure]].
2.	Generate [[Attribute-value system|attribute values]] based on user-supplied prior probabilities.

Since the [[Attribute-value system|attribute values]] of one object may depend on the [[Attribute-value system|attribute values]] of related objects, the attribute generation process assigns values collectively.&lt;sup&gt;10&lt;/sup&gt;

==References==
{{Reflist}}
* Wang, A, Qiu, T, &amp; Shao, L. (2009). ''A Simple Method of Radial Distortion Correction with Centre of Distortion Estimation''. 35. Retrieved from http://www.springerlink.com/content/8180144q56t30314/fulltext.pdf
* Duncan, G. (2006). ''Statistical confidentiality: Is Synthetic Data the Answer?'' Retrieved from http://www.idre.ucla.edu/events/PPT/2006_02_13_duncan_Synthetic_Data.ppt
* Jensen, D. (2004). ''Proximity 4.3 Tutorial Chapter 6.'' Retrieved from http://kdl.cs.umass.edu/proximity/documentation/tutorial/ch06s09.html
* Jackson, C, Murphy, R, &amp; Kova&#711;cevic&#180;, J. (2009). ''Intelligent Acquisition and Learning of Fluorescence Microscope Data Models.'' 18(9), Retrieved from http://www.andrew.cmu.edu/user/jelenak/Repository/08_JacksonMK.pdf
* {{cite book| author=Adam Coates and Blake Carpenter and Carl Case and Sanjeev Satheesh and Bipin Suresh and Tao Wang and David J. Wu and Andrew Y. Ng| chapter=Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning| title=ICDAR| year=2011| pages=440&#8211;445| accessdate=13 May 2014}}

==External links==
* The "DataGenerator" a model-based synthetic data generator: http://finraos.github.io/DataGenerator/
* The ''datgen'' synthetic data generator: http://www.datasetgenerator.com
* Fienberg, S. E. (1994). "Conflicts between the needs for access to statistical information and demands for confidentiality," Journal of Official Statistics 10, 115&#8211;132.
* Little, R (1993). "Statistical Analysis of Masked Data," Journal of Official Statistics, 9, 407-426.
* Raghunathan, T.E., Reiter, J.P., and Rubin, D.B. (2003). "Multiple Imputation for Statistical Disclosure Limitation," Journal of Official Statistics, 19, 1-16.
* Reiter, J.P. (2004). "Simultaneous Use of Multiple Imputation for Missing Data and Disclosure Limitation," Survey Methodology, 30, 235-242.

 	
{{FOLDOC}}
{{Statistics}}

[[Category:Data]]
[[Category:Computer data]]
[[Category:Data management]]</text>
      <sha1>rs61b9y7dwuo3c1rxzo8r1x1o9w0qal</sha1>
    </revision>
  </page>
  <page>
    <title>Recording format</title>
    <ns>0</ns>
    <id>793548</id>
    <revision>
      <id>694556608</id>
      <parentid>563133697</parentid>
      <timestamp>2015-12-10T00:32:33Z</timestamp>
      <contributor>
        <username>SJ Defender</username>
        <id>19403234</id>
      </contributor>
      <minor />
      <comment>Disambiguated: [[drawer]] &#8594; [[drawer (furniture)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3894" xml:space="preserve">{{Unreferenced|date=December 2009}}
[[File:Cylinder Head Sector.svg|thumb|300px|right|A cylinder, head, and sector of a hard drive. The sectors are a recording container format. The digital data on the disks may be both secondary [[Container format (digital)|container file formats]] and raw digital data content formats such as digital audio or ASCII encoded text.]]
[[File:WorldMapLongLat-eq-circles-tropics-non.png|thumb|440px|A map of Earth showing lines of latitude (horizontally) and longitude (vertically). The lines are a grid, a method for dividing and containing recorded [[cartographical]] data. The land masses and oceans are cartographical data in a raw content ([[pictorial]] graphical) format. The text is in an [[alphanumeric]]al symbolic raw content format.]]
A '''recording format ''' is a [[content format|format]] for [[encoder|encoding]] data for storage on a [[storage medium]]. The format can be container information such as [[Cylinder-head-sector|sectors]] on a disk, or user/audience information ([[Content (media and publishing)|content]]) such as [[analog signal|analog]] [[stereo]] [[Sound recording and reproduction|audio]].  Multiple levels of encoding may be achieved in one format. For example, a text encoded page may contain [[HTML]] and [[XML]] encoding, combined in a [[plain text]] file format, using either [[EBCDIC]] or [[ASCII]] character encoding, on a [[Universal Disk Format|UDF]] [[Digital data|digital]]ly formatted disk.  

In [[electronic media]], the primary format is the encoding that requires hardware to interpret (decode) data; while secondary encoding is interpreted by secondary [[signal processing]] methods, usually [[computer software]]. 

==Recording container formats==
A container format is a system for dividing physical storage space or virtual space for data. Data space can be divided evenly by a [[systems of measurement|system of measurement]], or divided unevenly with [[meta data]]. A grid may divide physical or virtual space with physical or virtual (dividers) borders, evenly or unevenly.  Just as a physical container (such as a [[file cabinet]]) is divided by physical borders (such as [[drawer (furniture)|drawer]]s and [[file folder]]s), data space is divided by virtual borders. Meta data such as a [[unit of measurement]], [[Address (geography)|address]], or [[meta tags]] act as virtual borders in a container format. A template may be considered an abstract format for containing a solution as well as the content itself. 

* Systems of measurement
**[[Metric system]]
** [[Geographic coordinate system]]
**[[Grid (page layout)|Page grid]]
* [[Film formats]]
* [[Audio format|Audio data format]]
* [[Video tape|Video tape format]]
* [[Disk format]]
* [[File format]]
* [[Meta data]]
** [[Formatted text|Text formatting]]
** [[Template (file format)|Template]]
** [[Data structure]]

==Raw content formats==
{{Main|content format}}

A raw content format is a system of converting data to displayable [[information]].  Raw content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format. A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].

{{Audio format}}{{Homevid}}

{{DEFAULTSORT:Recording Format}}
[[Category:Communication]]
[[Category:Information science]]
[[Category:Data management]]
[[Category:Film and video technology]]
[[Category:Computer storage media]]
[[Category:Recording]]</text>
      <sha1>41djunpvjeye4mntbzr0yj5z86kf64j</sha1>
    </revision>
  </page>
  <page>
    <title>Data virtualization</title>
    <ns>0</ns>
    <id>26041421</id>
    <revision>
      <id>752147765</id>
      <parentid>751181827</parentid>
      <timestamp>2016-11-29T19:30:21Z</timestamp>
      <contributor>
        <ip>70.114.204.101</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7311" xml:space="preserve">'''Data virtualization''' is any approach to data management that allows an application to retrieve and manipulate data without requiring technical details about the data, such as how it is formatted at source, or where it is physically located.&lt;ref&gt;[http://searchdatamanagement.techtarget.com/definition/data-virtualization "What is Data Virtualization?"], Margaret Rouse, TechTarget.com, retrieved 19 August 2013&lt;/ref&gt;

Unlike the traditional [[extract, transform, load]] ("ETL") process, the data remains in place, and real-time access is given to the source system for the data, thus reducing the risk of data errors and reducing the workload of moving data around that may never be used.

Unlike a [[federated database system]], it does not attempt to impose a single data model on the data (heterogeneous data). The technology also supports the writing of transaction data updates back to the source systems.&lt;ref name="morgan"&gt;[http://www.computerweekly.com/feature/Data-virtualisation-on-rise-as-ETL-alternative-for-data-integration "Data virtualisation on rise as ETL alternative for data integration"] Gareth Morgan, Computer Weekly, retrieved 19 August 2013&lt;/ref&gt;

To resolve differences in source and consumer formats and semantics, various abstraction and transformation techniques are used.
This concept and software is a subset of [[data integration]] and is commonly used within [[business intelligence]], [[service-oriented architecture]] data services, [[cloud computing]], [[enterprise search]], and [[master data management]].

==Examples ==
* The Phone House&#8212;the trading name for the European operations of UK-based mobile phone retail chain [[Carphone Warehouse]]&#8212;implemented [[Denodo]]&#8217;s data virtualization technology between its Spanish subsidiary&#8217;s transactional systems and the Web-based systems of mobile operators.&lt;ref name="morgan"/&gt;
* [[Novartis]], which implemented a data virtualization tool from [[Composite Software]] to enable its researchers to quickly combine data from both internal and external sources into a searchable virtual data store.&lt;ref name="morgan"/&gt;
* The storage-agnostic [http://primarydata.com/ Primary Data] data virtualization platform enables applications, servers, and clients to transparently access data while it is intelligently migrated between direct-attached, network-attached, private and public cloud storage. Server flash memory pioneer [[Fusion-io]] co-founder David Flynn, now Primary Data CTO, saw the need to move data across storage types to maximize efficiency with data virtualization.
* [[Linked Data]] can use a single hyperlink-based [[Data Source Name]] ([[Data Source Name|DSN]]) to provide a connection to a virtual database layer that is internally connected to a variety of back-end data sources using [[ODBC]], [[JDBC]], [[OLE DB]], [[ADO.NET]], [[Service-oriented architecture|SOA]]-style services, and/or [[REST]] patterns.
* [[Database virtualization]] may use a single ODBC-based DSN to provide a connection to a similar virtual database layer.

==Functionality==

Data Virtualization software provides some or all of the following capabilities:

* '''Abstraction''' &#8211;  Abstract the technical aspects of stored data, such as location, storage structure, API, access language, and storage technology. 
* '''Virtualized Data Access''' &#8211; Connect to different data sources and make them accessible from a common logical data access point.
* '''Transformation''' &#8211; Transform, improve quality, reformat, etc. source data for consumer use. 
* '''Data Federation''' &#8211; Combine result sets from across multiple source systems. 
* '''Data Delivery''' &#8211; Publish result sets as views and/or data services executed by client application or users when requested.

Data virtualization software may include functions for development, operation, and/or management.

Benefits include:
* Reduce risk of data errors
* Reduce systems workload through not moving data around
* Increase speed of access to data on a real-time basis
* Significantly reduce development and support time
* Increase governance and reduce risk through the use of policies&lt;ref&gt;[http://www.informatica.com/us/products/data-virtualization/data-services/ "Rapid Access to Disparate Data Across Projects Without Rework"] Informatica, retrieved 19 August 2013&lt;/ref&gt;
* Reduce data storage required&lt;ref&gt;[http://www.zdnet.com/blog/service-oriented/data-virtualization-6-best-practices-to-help-the-business-get-it/7897 Data virtualization: 6 best practices to help the business 'get it'] Joe McKendrick, ZDNet, 27 October 2011&lt;/ref&gt;

Drawbacks include:
* May impact Operational systems response time, particularly if under-scaled to cope with unanticipated user queries or not tuned early on&lt;ref&gt;[http://searchdatamanagement.techtarget.com/news/2240165242/IT-pros-reveal-the-benefits-drawbacks-of-data-virtualization-software|IT pros reveal benefits, drawbacks of data virtualization software"] Mark Brunelli, SearchDataManagement, 11 October 2012&lt;/ref&gt;
* Does not impose a heterogeneous data model, meaning the user has to interpret the data, unless combined with [[Federated database system|Data Federation]] and business understanding of the data&lt;ref name="lawson"&gt;[http://www.itbusinessedge.com/cm/blogs/lawson/the-pros-and-cons-of-data-virtualization/?cs=48794 "The Pros and Cons of Data Virtualization"] Loraine Lawson, BusinessEdge, 7 October 2011&lt;/ref&gt;
* Requires a defined Governance approach to avoid budgeting issues with the shared services
* Not suitable for recording the historic snapshots of data - data warehouse is better for this&lt;ref name="lawson"/&gt;
* Change management "is a huge overhead, as any changes need to be accepted by all applications and users sharing the same virtualization kit"&lt;ref name="lawson"/&gt;

== Technology ==

Some data virtualization technologies include:

*  [[Actifio]] Copy Data Virtualization
*  [[Capsenta]]'s Ultrawrap Platform &lt;ref&gt;https://capsenta.com/&lt;/ref&gt;
*  [[Cisco]] Data Virtualization (formerly [[Composite Software]])
*  [[Denodo|Denodo Platform]]
* DataVirtuality
* Data Virtualization Platform
*  HiperFabric Data Virtualization and Integration
* Stonebond Enterprise Enabler Data Virtualization Platform
* [[Red Hat]] [[JBoss Enterprise Application Platform]] Data Virtualization
* [[XAware]] Data Services

==History==
[[Enterprise information integration]] (EII), first coined by Metamatrix, now known as Red Hat JBoss Data Virtualization, and [[federated database system]]s are terms used by some vendors to describe a core element of data virtualization: the capability to create relational JOINs in a federated VIEW.

==See also==

* [[Data integration]]
* [[Enterprise information integration]] (EII)
* [[Master data management]]
* [[Database virtualization]]
* [[Federated database system|Data Federation]]
* [[Disparate system]]

==References==
{{reflist}}

==Further reading==
* '''Data Virtualization: Going Beyond Traditional Data Integration to Achieve Business Agility''', Judith R. Davis and Robert Eve
* '''Data Virtualization for Business Intelligence Systems: Revolutionizing Data Integration for Data Warehouses''' Rick van der Lans
* '''Data Integration Blueprint and Modeling: Techniques for a Scalable and Sustainable Architecture ''' Anthony Giordano

[[Category:Data management]]</text>
      <sha1>8yq3c8q2a2zntfpfpfhnje3wfl7vb2k</sha1>
    </revision>
  </page>
  <page>
    <title>Jenks natural breaks optimization</title>
    <ns>0</ns>
    <id>25397242</id>
    <revision>
      <id>739548450</id>
      <parentid>669097782</parentid>
      <timestamp>2016-09-15T10:02:07Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>clean up, [[WP:AWB/T|typo(s) fixed]]: 37 year &#8594; 37-year using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6033" xml:space="preserve">The '''Jenks optimization method''', also called the '''Jenks natural breaks classification method''', is a [[data clustering]] method designed to determine the best arrangement of values into different classes. This is done by seeking to minimize each class&#8217;s average deviation from the class mean, while maximizing each class&#8217;s deviation from the means of the other groups. In other words, the method seeks to reduce the [[variance]] within classes and maximize the variance between classes.&lt;ref name="Jenks"&gt;Jenks, George F. 1967. "The Data Model Concept in Statistical Mapping", International Yearbook of Cartography 7: 186&#8211;190.&lt;/ref&gt;&lt;ref name="McMaster"&gt;McMaster, Robert, "In Memoriam: George F. Jenks (1916&#8211;1996)". Cartography and Geographic Information Science. 24(1) p.56-59.&lt;/ref&gt;

==History==

=== George Jenks ===
George Frederick Jenks was a 20th Century American [[cartography|cartographer]]. Graduating with his Ph.D. in agricultural geography from [[Syracuse University]] in 1947, Jenks began his career under the tutelage of [[Richard Edes Harrison|Richard Harrison]], cartographer for [[Time (magazine)|TIME]] and Fortune magazine.&lt;ref name="McMaster2"&gt;McMaster, Robert and McMaster, Susanna. 2002. &#8220;A History of Twentieth-Century American Academic Cartography&#8221;, Cartography and Geographic Information Science. 29(3) p.312-315.&lt;/ref&gt; He joined the faculty of the [[University of Kansas]] in 1949 and began to build the cartography program. During his 37-year tenure at KU, Jenks developed the Cartography program into one of three programs renowned for their graduate education in the field; the others being the [[University of Wisconsin]] and the [[University of Washington]]. Much of his time was spent developing and promoting improved cartographic training techniques and programs. He also spent significant time investigating three-dimensional maps, eye-movement research, [[thematic map]] communication, and [[geostatistics]].&lt;ref name="McMaster" /&gt;&lt;ref name="McMaster2" /&gt;&lt;ref name="CSUN"&gt;CSUN Cartography Specialty Group, [http://www.csun.edu/~hfgeg003/csg/winter97.html Winter 1997 Newsletter]&lt;/ref&gt;

===Development===
Jenks was a cartographer by profession. His work with [[statistics]] grew out of a desire to make [[choropleth map]]s more visually accurate for the viewer. In his paper, ''The Data Model Concept in Statistical Mapping'', he claims that by visualizing data in a three dimensional model cartographers could devise a &#8220;systematic and rational method for preparing choroplethic maps&#8221;.&lt;ref name="Jenks" /&gt; Jenks used the analogy of a &#8220;blanket of error&#8221; to describe the need to use elements other than the mean to generalize data. The three dimensional models were created to help Jenks visualize the difference between data classes. His aim was to generalize the data using as few planes as possible and maintain a constant &#8220;blanket of error&#8221;.

==Method==
The method requires an iterative process. That is, calculations must be repeated using different breaks in the dataset to determine which set of breaks has the smallest in-class [[variance]]. The process is started by dividing the ordered data into groups. Initial group divisions can be arbitrary. There are four steps that must be repeated:
#Calculate the sum of squared deviations between classes (SDBC).
#Calculate the sum of squared deviations from the array mean (SDAM).
#Subtract the SDBC from the SDAM (SDAM-SDBC). This equals the sum of the squared deviations from the class means (SDCM).
#After inspecting each of the SDBC, a decision is made to move one unit from the class with the largest SDBC toward the class with the lowest SDBC.

New class deviations are then calculated, and the process is repeated until the sum of the within class deviations reaches a minimal value.&lt;ref name="Jenks" /&gt;&lt;ref name="ESRI"&gt;ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&amp;d=26442 What is the Jenks Optimization method]&lt;/ref&gt;

Alternatively, all break combinations may be examined, SDCM calculated for each combination, and the combination with the lowest SDCM selected. Since all break combinations are examined, this guarantees that the one with the lowest SDCM is found.

Finally, the GVF statistic (goodness of variance fit) is calculated. GVF is defined as (SDAM - SDCM) / SDAM. GVF ranges from 0 (worst fit) to 1 (perfect fit).

==Uses==

{{main article|Choropleth map}}
Jenks&#8217; goal in developing this method was to create a map that was absolutely accurate, in terms of the representation of data&#8217;s spatial attributes. By following this process, Jenks claims, the &#8220;blanket of error&#8221; can be uniformly distributed across the mapped surface. He developed this with the intention of using relatively few data classes, less than seven, because that was the limit when using monochromatic shading on a choroplethic map.&lt;ref name="Jenks" /&gt;

==Alternative methods==
{{Main article|Cluster analysis}}

Other methods of data classification include [[Head/tail Breaks]], Natural Breaks (without Jenks Optimization), Equal Interval, Quantile, and Standard Deviation.

==See also==
* [[k-means clustering]], a generalization for multivariate data (Jenks natural breaks optimization seems to be one dimensional k-means&lt;ref&gt;[http://www.quantdec.com/SYSEN597/GTKAV/section1/chapter_9.htm]&lt;/ref&gt;).

==References==
{{Reflist}}

==External links==
* ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&amp;d=26442 What is the Jenks Optimization method]
* Volunteered Geographic Information, Daniel Lewis, [http://danieljlewis.org/2010/06/07/jenks-natural-breaks-algorithm-in-python/ Jenks Natural Breaks Algorithm with an implementation in python]
* Object Vision wiki, [http://wiki.objectvision.nl/index.php/Fisher%27s_Natural_Breaks_Classification Fisher's Natural Breaks Classification, a O(k*n*log(n)) algorithm]
* [http://www.ehdp.com/vitalnet/breaks-1.htm What is Jenks Natural Breaks?]

[[Category:Data management]]
[[Category:Cartography]]</text>
      <sha1>rde0ox4vb9d8y79l5w6nwisoi41v9ll</sha1>
    </revision>
  </page>
  <page>
    <title>Data migration</title>
    <ns>0</ns>
    <id>1135408</id>
    <revision>
      <id>756533123</id>
      <parentid>755721938</parentid>
      <timestamp>2016-12-24T23:31:57Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <comment>/* External links */  Removing spam.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8689" xml:space="preserve">{{More footnotes|date=February 2013}}
'''Data migration''' is the process of [[data transfer|transferring]] [[data]] between [[computer data storage|computer storage]] types or [[file format]]s. It is a key consideration for any system implementation, upgrade, or consolidation. Data migration is usually performed programmatically to achieve an ''automated migration'', freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, [[Software modernization|application migration]], website consolidation and [[data center]] relocation.&lt;ref&gt;Janssen C, Data migration, http://www.techopedia.com/definition/1180/data-migration (retrieved 12 August 2013)&lt;/ref&gt;

To achieve an effective data migration procedure, data on the old system is [[data mapping|mapped]] to the new system utilising a design for [[data extraction]] and [[data loading]]. The design relates old [[data format]]s to the new system's formats and requirements. Programmatic data migration may involve many phases but it minimally includes ''data extraction'' where data is read from the old system and ''data loading'' where data is written to the new system.

After loading into the new system, results are subjected to [[data verification]] to determine whether data was accurately translated, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].

Automated and manual data cleaning is commonly performed in migration to improve [[data quality]], eliminate [[data duplication|redundant]] or obsolete information, and match the requirements of the new system.

Data migration phases (design, [[extract, transform, load|extraction]], [[data cleansing|cleansing]], load, verification) for applications of moderate to high complexity are commonly repeated several times before the new system is deployed.

==Categories==

Data is stored on various media in [[Computer file|files]] or [[databases]], and is generated and consumed by [[software applications]] which in turn support [[business processes]]. The need to transfer and convert data can be driven by multiple business requirements and the approach taken to the migration depends on those requirements. Four major migration categories are proposed on this basis.

===Storage migration===
A business may choose to rationalize the physical media to take advantage of more efficient storage technologies. This will result in having to move physical blocks of data from one tape or disk to another, often using [[Storage virtualization|virtualization]] techniques. The data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above.

===Database migration===
{{main article|Schema migration}}
Similarly, it may be necessary to move from one [[database]] vendor to another, or to upgrade the version of database software being used. The latter case is less likely to require a physical data migration, but this can happen with major upgrades. In these cases a physical transformation process may be required since the underlying data format can change significantly. This may or may not affect behavior in the applications layer, depending largely on whether the data manipulation language or protocol has changed &#8211; but modern applications are written to be agnostic to the database technology so that a change from [[Sybase]], [[MySQL]], [[IBM DB2|DB2]] or [[Microsoft SQL Server|SQL Server]] to [[Oracle Database|Oracle]] should only require a testing cycle to be confident that both functional and non-functional performance has not been adversely affected.

===Application migration===
Changing application vendor &#8211; for instance a new [[Customer relationship management|CRM]] or [[Enterprise resource planning|ERP]] platform &#8211; will inevitably involve substantial transformation as almost every application or suite operates on its own specific data model and also interacts with other applications and systems within the [[enterprise application integration]] environment. Furthermore, to allow the application to be sold to the widest possible market, commercial off-the-shelf packages are generally configured for each customer using [[metadata]]. [[Application programming interfaces]] (APIs) may be supplied by vendors to protect the [[data integrity|integrity of the data]] they have to handle.

===Business process migration===
[[Business processes]] operate through a combination of human and application systems actions, often orchestrated by [[business process management]] tools. When these change they can require the movement of data from one store, database or application to another to reflect the changes to the organization and information about customers, products and operations. Examples of such migration drivers are mergers and acquisitions, business optimization and reorganization to attack new markets or respond to competitive threat.

The first two categories of migration are usually routine operational activities that the IT department takes care of without the involvement of the rest of the business. The last two categories directly affect the operational users of processes and applications, are necessarily complex, and delivering them without significant business downtime can be challenging. A highly adaptive approach, concurrent synchronization, a business-oriented audit capability and clear visibility of the migration for stakeholders are likely to be key requirements in such migrations.

===Project versus process===
There is a difference between data migration and [[data integration]] activities. Data migration is a project by means of which data will be moved or copied from one environment to another, and removed or decommissioned in the source. During the migration (which can take place over months or even years), data can flow in multiple directions, and there may be multiple migrations taking place simultaneously. The [[Extract, Transform, Load]] actions will be necessary, although the means of achieving these may not be those traditionally associated with the [[Extract, Transform, Load|ETL]] acronym.

Data integration, by contrast, is a permanent part of the IT architecture, and is responsible for the way data flows between the various applications and data stores - and is a process rather than a project activity. Standard ETL technologies designed to supply data from operational systems to data warehouses would fit within the latter category.

== Migration as a form of digital preservation ==
Migration, which focuses on the digital object itself, is the act of transferring, or rewriting data from an out-of-date medium to a current medium and has for many years been considered the only viable approach to long-term preservation of digital objects.&lt;ref&gt;{{cite journal|author1=van der Hoeven, Jeffrey|author2= Bram Lohman|author3=Remco Verdegem|title=Emulation for Digital Preservation in Practice: The Results|journal=The International Journal of Digital Curation|volume=2|issue=2|year=2007|pages=123-132|url=http://www.ijdc.net/index.php/ijdc/article/view/50|doi=10.2218/ijdc.v2i2.35}}&lt;/ref&gt; Reproducing brittle newspapers onto [[microform|microfilm]] is an example of such migration.

=== Disadvantages ===
* Migration addresses the possible obsolescence of the data carrier, but does not address the fact that certain technologies which run the data may be abandoned altogether, leaving migration useless.
* Time-consuming &#8211; migration is a continual process, which must be repeated every time a medium reaches obsolescence, for all data objects stored on a certain media.
* Costly - an institution must purchase additional data storage media at each migration.&lt;ref&gt;{{cite journal|author=Muira, Gregory|title=Pushing the Boundaries of Traditional Heritage Policy: maintaining long-term access to multimedia content|journal=IFLA Journal|volume=33|year=2007|pages=323-326|url=http://www.ifla.org/files/assets/hq/publications/ifla-journal/ifla-journal-4-2007.pdf}}&lt;/ref&gt;

As a result of the disadvantages listed above, technology professionals have begun to develop alternatives to migration, such as [[emulator|emulation]].

==See also==
* [[Data conversion]]
* [[Data transformation]]
* [[Extract, transform, load]]
* [[System migration]]

==References==
{{reflist}}

== External links ==
* {{dmoz|Computers/Software/Databases/Data_Warehousing/Extraction_and_Transformation|Data Migration}}

{{Authority control}}

[[Category:Data management]]</text>
      <sha1>8znodmubmrbgke8t7rxbscu5hfpv29n</sha1>
    </revision>
  </page>
  <page>
    <title>Content inventory</title>
    <ns>0</ns>
    <id>27255666</id>
    <revision>
      <id>735166545</id>
      <parentid>720256958</parentid>
      <timestamp>2016-08-19T00:18:49Z</timestamp>
      <contributor>
        <username>GermanJoe</username>
        <id>12935443</id>
      </contributor>
      <comment>rmv - spam</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9326" xml:space="preserve">A '''content inventory''' is the process and the result of cataloging the entire contents of a [[website]]. An allied practice&#8212;a [[content audit]]&#8212;is the process of ''evaluating'' that content.&lt;ref name="Halverson"&gt;{{cite web |url= http://www.peachpit.com/articles/article.aspx?p=1388961 |title= Content Strategy for the Web: Why You Must Do a Content Audit |first=Kristina |last=Halvorson |date=August 2009 |accessdate=6 May 2010}}&lt;/ref&gt;&lt;ref name="Baldwin"&gt;{{cite web |url= http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title= Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=29 April 2010}}&lt;/ref&gt;&lt;ref name="Marsh"&gt;{{cite web |url=http://www.hilarymarsh.com/2012/03/12/how-to-do-a-content-audit/ |title=How to do a content audit |first=Hilary |last=Marsh |date=March 2012 |accessdate=2 May 2013}}&lt;/ref&gt; A content inventory and a [[content audit]] are closely related concepts, and they are often conducted in tandem.

==Description==

A content inventory typically includes all information assets on a website, such as [[web page]]s (html), [[meta element]]s (e.g., keywords, description, page title), images, audio and video files, and document files (e.g., .pdf, .doc, .ppt).&lt;ref name="Spencer2006"&gt;{{cite web |url=http://maadmob.net/donna/blog/2006/taking-a-content-inventory |title=Taking a content inventory |first=Donna |last=Spencer |date=January 2006 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Doss2007"&gt;{{cite web |url=http://www.fatpurple.com/2010/02/26/content-inventory/ |title=Content Inventory: Sometimes referred to as Web Content Inventory or Web Audit |first=Glen |last=Doss |date=January 2007 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Jones2009"&gt;{{cite web |url=http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php |title=Content Analysis: A Practical Approach |first=Colleen |last=Jones |date=August 2009 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Leise2007"&gt;{{cite web |url=http://boxesandarrows.com/view/content-analysis |title=Content Analysis Heuristics |first=Fred |last=Leise |date=March 2007 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Baldwin2010"&gt;{{cite web |url=http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title=Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Krozser"&gt;{{cite web |url=http://www.alttags.org/content-management/the-content-inventory-roadmap-to-a-succesful-cms-implementation/ |title= The Content Inventory: Roadmap to a Successful CMS Implementation |first=Kassia |last=Krozser |date=April 2005 |accessdate=27 April 2010}}&lt;/ref&gt; A content inventory is a [[Quantitative research|quantitative analysis]] of a website. It simply logs what is on a website. The content inventory will answer the question: &#8220;What is there?&#8221; and can be the start of a website review.&lt;ref name="GOSS Interactive"&gt;{{cite web |url=http://www.gossinteractive.com/community/whitepapers/conducting-a-website-review-and-implementing-results-for-increased-customer-engagement-and-conversions |title= Conducting a website review and implementing results for increased customer engagement and conversions()|first=GOSS Interactive|date=October 2011 |accessdate=8 October 2011}}&lt;/ref&gt; A related (and sometimes confused term) is a [[content audit]], a [[Qualitative research|qualitative analysis]] of information assets on a website. It is the assessment of that content and its place in relationship to surrounding Web pages and information assets. The content audit will answer the question: &#8220;Is it any good?&#8221;&lt;ref name="Baldwin"/&gt;&lt;ref name="Marsh"/&gt;

Over the years, techniques for creating and managing a content inventory have been developed and refined in the field of website [[content management]].&lt;ref name="Halverson"/&gt;&lt;ref name="Veen2002"&gt;{{cite web |url=http://www.adaptivepath.com/ideas/essays/archives/000040.php |title=Doing a Content Inventory (Or, A Mind-Numbingly Detailed Odyssey Through Your Web Site) |first=Jeffrey |last=Veen |date=June 2002 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Bruns"&gt;{{cite web |url=http://donbruns.net/index.php/how-to-automatically-index-a-content-inventory/ |title= Automatically Index a Content Inventory with GetUXIndex() |first=Don |last=Bruns |date=March 2010 |accessdate=6 May 2010}}&lt;/ref&gt;

A [[spreadsheet]] application (e.g., [[Microsoft Excel]] or [[LibreOffice Calc]]) is the preferred tool for keeping a content inventory; the data can be easily configured and manipulated. Typical categories in a content inventory include the following:

* Link &#8212; The [[URL]] for the page
* Format &#8212; For example, .[[html]], [[.pdf]], [[Microsoft Word|.doc]], [[Microsoft PowerPoint|.ppt]]
* Meta page title &#8212; Page title as it appears in the meta &lt;title&gt; tag
* Meta keywords &#8212; Keywords as they appear in the [[Meta tag#The keywords attribute|meta name="keywords" tag element]]
* Meta description &#8212; Text as it appears in the [[Meta tag#The description attribute|meta name="description" tag element]]
* Content owner &#8212; Person responsible for maintaining page content
* Date page last updated &#8212; Date of last page update
* Audit Comments (or Notes) &#8212; Audit findings and notes

There are other descriptors that may need to be captured on the inventory sheet. Content management experts advise capturing information that might be useful for both short- and long-term purposes. Other information could include:

* the overall topic or area to which the page belongs
* a short description of the information on the page
* when the page was created, date of last revision, and when the next page review is due
* pages this page links to
* pages that link to this page
* page status &#8211; keep, delete, revise, in revision process, planned, being written, being edited, in review, ready for posting, or posted
* rank of page on the website &#8211; is it a top 50 page? a bottom 50 page? Initial efforts might be more focused on those pages that visitors use the most and least.

Other tabs in the inventory workbook can be created to track related information, such as meta keywords, new Web pages to develop, website tools and resources, or content inventories for sub-areas of the main website. Creating a single, shared location for information related to a website can be helpful for all website content managers, writers, editors, and publishers.

Populating the spreadsheet is a painstaking task, but some up-front work can be automated with software, and other tools and resources can assist the audit work.

==Value==

A content inventory and a content audit are performed to understand what is on a website and why it is there. The inventory sheet, once completed and revised as the site is updated with new content and information assets, can also become a resource for help in maintaining [[website governance]].

For an existing website, the information cataloged in a content inventory and content audit will be a resource to help manage all of the information assets on the website.&lt;ref name="Usability"&gt;{{cite web |url=http://www.usability.gov/methods/design_site/inventory.html |title=Content Inventory |date=26 May 2009 |publisher=U.S. Department of Health &amp; Human Services |accessdate=4 May 2010}}&lt;/ref&gt; The information gathered in the inventory can also be used to plan a website re-design or site migration to a [[web content management system]].&lt;ref name="Krozser"/&gt; When planning a new website, a content inventory can be a useful [[project management]] tool: as a guide to map [[information architecture]] and to track new pages, page revision dates, content owners, and so on.

==See also==

* [[Content audit]]
* [[Web content management system]]
* [[Design methods]]
* [[Information architecture]]
* [[Web design]]
* [[Website governance]]

==References==
{{Reflist}}

==Further reading==

* In his article [http://www.boxesandarrows.com/view/a-map-based-approach A Map-Based Approach to a Content Inventory], Patrick Walsh describes how to use [[Microsoft Access]] and Microsoft Excel to link a data attribute with a structural attribute to create &#8220;a tool that can be used throughout the lifetime of a website.&#8221;
* In the article [http://www.louisrosenfeld.com/home/bloug_archive/000448.html The Rolling Content Inventory], author Louis Rosenfeld argues that &#8220;ongoing, partial content inventories&#8221; are more cost-effective and realistic to implement.
* Colleen Jones writes from a UX design perspective in [http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php Content Analysis: A Practical Approach].
* [http://xmlpress.net/content-strategy/audits-and-inventories/ Content Audits and Inventories: A Handbook] is a practical guide to conducting content inventories and audits.

==External links==
* [http://home.snafu.de/tilman/xenulink.html Xenu's Link Sleuth]
* [http://siteorbiter.com/ SiteOrbiter]
* [http://www.webconfs.com/similar-page-checker.php Similar Page Checker]
* [http://www.cryer.co.uk/resources/link_checkers.htm Link Checker Tools]
* [http://www.kevinpnichols.com/downloads/kpn_content_audit.xls Kevin P Nichols' Content Inventory and Audit Template]

{{DEFAULTSORT:Content Inventory}}
[[Category:Data management]]
[[Category:Website management]]
[[Category:Content management systems]]</text>
      <sha1>abffprnh67gnnl90uh9avs7aqm66ufp</sha1>
    </revision>
  </page>
  <page>
    <title>Virtual data room</title>
    <ns>0</ns>
    <id>17879652</id>
    <revision>
      <id>755590300</id>
      <parentid>755583202</parentid>
      <timestamp>2016-12-19T00:34:06Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <comment>Undid revision 755583202 by [[Special:Contributions/Qianding|Qianding]] ([[User talk:Qianding|talk]]) Again, no explanation or reason given for arbitrary list of companies. This sure looks like aggressive spamming.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2855" xml:space="preserve">A '''virtual data room''' (sometimes called a '''VDR''') is an online repository of information that is used for the storing and distribution of documents.  In many cases, a virtual data room is used to facilitate the [[due diligence]] process during an [[M&amp;A]] transaction, [[loan syndication]], or private equity and venture capital transactions.  This due diligence process has traditionally used a physical [[data room]] to accomplish the disclosure of documents. For reasons of cost, efficiency and security, virtual data rooms have widely replaced the more traditional physical data room.&lt;ref&gt;http://www.inc.com/best-industries-2013/jeremy-quittner/virtual-data-rooms.html&lt;/ref&gt;&lt;ref&gt;http://www.forbes.com/pictures/fghj45fjl/2-virtual-data-rooms/&lt;/ref&gt;

A virtual data room is an [[extranet]] to which the bidders and their advisers are given access via the internet. An extranet is essentially a website with limited controlled access, using a secure log-on supplied by the vendor, which can be disabled at any time, by the vendor, if a bidder withdraws. Much of the information released is confidential and restrictions are applied to the viewer&#8217;s ability to release this to third parties (by means of forwarding, copying or printing). This can be effectively applied to protect the data using digital rights management.&lt;ref&gt;http://www.divestopedia.com/definition/836/virtual-data-room-vdr&lt;/ref&gt; 

In the process of [[mergers and acquisitions]] the data room is set up as part of the central repository of data relating to companies or divisions being acquired or sold. The data room enables the interested parties to view information relating to the business in a controlled environment where confidentiality can be preserved. Conventionally this was achieved by establishing a supervised, physical data room in secure premises with controlled access. In most cases, with a physical data room, only one bidder team can access the room at a time. A virtual data room is designed to have the same advantages as a conventional data room (controlling access, viewing, copying and printing, etc.) with fewer disadvantages. Due to their increased efficiency, many businesses and industries have moved to using virtual data rooms instead of physical data rooms. In 2006, a spokesperson for a company which sets up virtual deal rooms was reported claiming that the process reduced the bidding process by about thirty days compared to physical data rooms.&lt;ref&gt;{{cite news|last1=Buckler|first1=Grant|title=A virtual smoke-filled room|url=http://www.theglobeandmail.com/technology/a-virtual-smoke-filled-room/article1110056/|accessdate=4 July 2016|work=The Globe and Mail|date=21 November 2006}}&lt;/ref&gt;

==References==
{{Reflist}}


{{DEFAULTSORT:Virtual Data Room}}
[[Category:Data management]]
[[Category:Disclosure]]
[[Category:Mergers and acquisitions]]</text>
      <sha1>7z2and6wsytwr8fyo4oatlw8xm0it4g</sha1>
    </revision>
  </page>
  <page>
    <title>ISO 8000</title>
    <ns>0</ns>
    <id>20375252</id>
    <revision>
      <id>754083379</id>
      <parentid>754082984</parentid>
      <timestamp>2016-12-10T19:39:33Z</timestamp>
      <contributor>
        <ip>82.137.56.69</ip>
      </contributor>
      <comment>Fix previously added item.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5595" xml:space="preserve">'''[[International Organization for Standardization|ISO]] 8000''', the global standard for ''[[Data Quality and Enterprise Master Data]]'', describes the features and defines the requirements for the Data Quality and Portability of Enterprise Master Data.  Master Data is typically "internal" business information about clients, products and operations.  The standard is currently under development, but is quickly being adopted by many Fortune 500 corporations and certain public agencies involved in the regulation and supervision of financial markets around the world. ISO 8000 is one of the emerging technology standards that large and complex organizations are turning to in order to improve business processes and control operational costs.  The standard will be published as a number of separate documents, which [[International Organization for Standardization|ISO]] calls "parts".

ISO 8000 is being developed by [[ISO TC 184/SC 4|ISO technical committee TC 184, ''Automation systems and integration'', sub-committee SC 4, ''Industrial data'']]. Like other [[International Organization for Standardization|ISO]] and [[International Electrotechnical Commission|IEC]] standards, ISO 8000 is copyrighted and is not freely available.&lt;ref&gt;[http://www.iso.org/iso/copyright.htm ISO copyright policy]&lt;/ref&gt;

== Published parts ==

The following part has already been published:

* ISO/TS 8000-1:2011, ''Data quality &amp;mdash; Part 1: Overview''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50798 ISO catalogue page for ISO/TS 8000-1:2011]&lt;/ref&gt;
* ISO 8000-2:2012, ''Data quality &amp;mdash; Part 2: Vocabulary''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=57436 ISO catalogue page for ISO 8000-2:2012]&lt;/ref&gt;
* ISO 8000-61:2016, ''Data quality &amp;mdash; Part 61: Data quality management: Process reference model''&lt;ref&gt;[http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=63086 ISO catalogue page for ISO-61:2016]&lt;/ref&gt;
* ISO/TS 8000-100:2009, ''Data quality &amp;mdash; Part 100: Master data: Exchange of characteristic data: Overview''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=52129 ISO catalogue page for ISO/TS 8000-100:2009]&lt;/ref&gt;
* ISO 8000-102:2009, ''Data quality &amp;mdash; Part 102: Master data: Exchange of characteristic data: Vocabulary''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50799 ISO catalogue page for ISO 8000-102:2009]&lt;/ref&gt;
* ISO 8000-110:2009, ''Data quality &#8212; Part 110: Master data: Exchange of characteristic data: Syntax, semantic encoding, and conformance to data specification''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51653 ISO catalogue page for ISO 8000-110:2009]&lt;/ref&gt;
* ISO/TS 8000-120:2009, ''Data quality &amp;mdash; Part 120: Master data: Exchange of characteristic data: Provenance''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50801 ISO catalogue page for ISO/TS 8000-120:2009]&lt;/ref&gt;
* ISO/TS 8000-130:2009, ''Data quality &amp;mdash; Part 130: Master data: Exchange of characteristic data: Accuracy''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50802 ISO catalogue page for ISO/TS 8000-130:2009]&lt;/ref&gt;
* ISO/TS 8000-140:2009, ''Data quality &amp;mdash; Part 140: Master data: Exchange of characteristic data: Completeness''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-140:2009]&lt;/ref&gt;
* ISO/TS 8000-150:2011, ''Data quality &amp;mdash; Part 150: Master data: Quality management framework''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-150:2011]&lt;/ref&gt;

== Further reading ==

{{Citation | last=Benson | first=Peter | journal=Real-World Decision Support (RWDS) Journal | year=2009 | volume=3 | issue=4 | title=ISO 8000 Data Quality &#8212; The Fundamentals, Part 1 | url=http://www.ewsolutions.com/resource-center/rwds_folder/rwds-archives/issue.2009-10-12.0790666855/document.2009-10-12.3367922336/view?searchterm=ISO%208000}}

{{Citation | last=Benson | first=Peter | title=NATO Codification System as the foundation for ISO 8000, the International Standard for data quality (Oil IT Journal)| year=2008 | url=http://www.oilit.com/papers/Benson.pdf}}

{{Citation | last=Benson | first=Peter | title=ISO 8000 &amp;mdash; A new international standard for data quality | year=2009 | url=http://www.dataqualitypro.com/data-quality-home/iso-8000-a-new-international-standard-for-data-quality.html}}

{{Citation | last=Benson | first=Peter | title=Peter Benson discusses the certification options of ISO 8000 (Live Recording) | year=2010 | url=http://www.dataqualitypro.com/data-quality-home/peter-benson-discusses-the-certification-options-of-iso-8000.html}}

{{Citation | last=Grantner | first=Emily | title=ISO 8000 &amp;mdash; A Standard for data quality | year=2007 | issue=Oct.-Dec. | journal=Logistics Spectrum | url=http://www.highbeam.com/doc/1P3-1518467381.html}}

{{Citation | last=West | first=Matthew | title=ISO 8000 &amp;mdash; the Emerging Standard for Data Quality | year=2009 | journal=IAIDQ's Information and Data Quality Newsletter | volume=5 | issue=3 | url=http://iaidq.org/publications/west-2009-07.shtml}} (full article requires no cost registration to access)

== References ==
&lt;references/&gt;

{{ISO standards}}

{{DEFAULTSORT:Iso 8000}}
[[Category:ISO standards|#08000]]
[[Category:Data management]]</text>
      <sha1>4d2ca6v0pjcvkesyyuxaqa9sh2x2q13</sha1>
    </revision>
  </page>
  <page>
    <title>Document capture software</title>
    <ns>0</ns>
    <id>23392007</id>
    <revision>
      <id>736154514</id>
      <parentid>736153291</parentid>
      <timestamp>2016-08-25T15:07:44Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>/* Document Capture Software */ rm link farm per [[WP:ELNO]], [[WP:NOT]] a link directory</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6176" xml:space="preserve">{{primary sources|date=August 2009}}
'''Document Capture Software''' refers to applications that provide the ability and feature set to automate the process of [[Image scanner|scanning]] paper documents. Most scanning [[Personal computer hardware|hardware]], both scanners and [[copier]]s, provides the basic ability to scan to any number of [[image file formats]], including: [[PDF]], [[TIFF]], [[JPG]], [[BMP file format|BMP]], etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process.

==Typical features==
Typical features of Document Capture Software include:
* [[Barcode]] recognition
* Patch Code recognition
* Separation
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Optical mark recognition|Optical Mark Recognition (OMR)]]
* Quality Assurance
* Indexing
* Migration

===Goal for Implementation of a Document Capture Solution===
The goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process, and produce metadata along with an image file, and/or OCR text. This information is then migrated to a [[Document management system|Document Management]] or [[Enterprise content management|Enterprise Content Management]] system. These systems often provide a search function, allowing search of the assets based on the produced [[metadata]], and then viewed using [[document imaging]] software.

== Document Capture System Solutions - General ==

===Integration with Document Management System===
{{main|Enterprise content management}}
ECM (Enterprise Content management) and their DMS component (Document Management System) are being adopted by many organizations as a corporate document management system for all types of electronic files, e.g. MS word, PDF ... However, much of the information held by organisations is on paper and this needs to be integrated within the same document repository.

By converting paper documents into digital format through scanning companies can convert paper into image formats such as TIF and JPG and also extract valuable index information or business data from the document using OCR technology. Digital documents and associated metadata can easily be stored in the ECM in a variety of formats. The most popular of these formats is PDF which not only provides an accurate representation of the document but also allows all the OCR text in the document to be stored behind the PDF image. This format is known as PDF with hidden text or text-searchable PDF. This allows users to search for documents by using keywords in the metadata fields or by searching the content of PDF files across the repository.

====Advantages of scanning documents into a ECM/DMS====

Information held on paper is usually just as valuable to organisations as the electronic documents that are generated internally. Often this information represents a large proportion of the day to day correspondence with suppliers and customers. Having the ability to manage and share this information internally through a document management system such as [[SharePoint]] can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire.

Organisations adopting an ECM/DMS  often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails.
For business critical documents, such as purchase orders and supplier invoices, digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems, such as CRM, ERP and Accounting. Scanned invoices can also be routed to managers for payment approval via email or an electronic workflow.

==Distributed Capture Solutions==

Distributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations. A variation of distributed capture is thin-client document capture in which documents are scanned into a central server through the use of web browser.  One of these web-based products was reviewed by AIIM.  They said, "(this product) is a thin-client distributed capture system that streamlines the process of acquiring and creating documents."&lt;ref&gt;Association for Information and Image Management [http://www.aiim.org/community/product-guide/Capture/Prevalent-Software-Quillix "Prevalent Software - Quillix"], accessed August 29, 2011.&lt;/ref&gt;  The streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured.  This includes things like email, fax, or a watched folder.

Jeff Shuey, Director of Business Development at Kodak, makes a distinction between distributed capture and what he calls "remote" capture.  In an article publishing in [[AIIM]], he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server. If, as he points out in his article, the document just needs to be scanned and committed to a [[SharePoint]] system and doesn't need to be sent to some other centralized server, this is just a remote capture situation.&lt;ref&gt;Association for Information and Image Management [http://www.aiim.org/community/blogs/expert/Remote-or-Distributed-Scanning-Are-they-Different "Remote or Distributed Scanning - Are They Different?"], accessed August 29, 2011.&lt;/ref&gt;

There are Document Capture Software comparisons available, featuring some of the most relevant products (EMC Captiva, IBM Datacap, Artsyl Technologies or Ephesoft) and extracting performance facts and their most relevant features.

==References==
&lt;references/&gt;

{{DEFAULTSORT:Document Capture Software}}
[[Category:Artificial intelligence applications]]
[[Category:Optical character recognition]]
[[Category:Data management]]
[[Category:SharePoint]]</text>
      <sha1>24wdyg3kn2dyxodclklvntyct0pcbo0</sha1>
    </revision>
  </page>
  <page>
    <title>Learning object</title>
    <ns>0</ns>
    <id>18126</id>
    <revision>
      <id>744464451</id>
      <parentid>736540516</parentid>
      <timestamp>2016-10-15T11:27:59Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12467" xml:space="preserve">A '''learning object''' is "a collection of content items, practice items, and assessment items that are combined based on a single learning objective".&lt;ref&gt;{{citation|last=Cisco Systems|title=Reusable information object strategy|url=http://www.cisco.com/warp/public/779/ibs/solutions/learning/whitepapers/el_cisco_rio.pdf}}&lt;/ref&gt;  The term is credited to Wayne Hodgins, and dates from a working group in 1994 bearing the name.&lt;ref&gt;{{citation|last=Gerard|first=R.W.|title="Shaping the mind: Computers in education", In N. A. Sciences, Applied Science and Technological Progress|url=https://books.google.com/books?id=BTcrAAAAYAAJ|year=1967}}&lt;/ref&gt; The concept encompassed by 'Learning Objects' is known by numerous other terms, including: content objects, chunks, educational objects, information objects, intelligent objects, knowledge bits, knowledge objects, learning components, media objects, reusable curriculum components, nuggets, reusable information objects, reusable learning objects, testable reusable units of cognition, training components, and units of learning.

The core idea of the use of learning objects is characterized by the following: discoverability, reusability, and interoperability. To support discoverability, learning objects are described by Learning Object Metadata, formalized as IEEE 1484.12 [[Learning object metadata]].&lt;ref&gt;[http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf IEEE 1484.12 Learning Object Metadata]&lt;/ref&gt; To support reusability, the IMS Consortium proposed a series of specifications such as the IMS [[Content package]]. And to support interoperability, the U.S. military's [[Advanced Distributed Learning]] organization created the [[Sharable Content Object Reference Model]].&lt;ref&gt;[http://legacy.adlnet.gov/Technologies/scorm/SCORMSDocuments/2004%204th%20Edition/Overview.aspx SCORM 2004 4th Edition Version 1.1 Overview]&lt;/ref&gt; Learning objects were designed in order to reduce the cost of learning, standardize learning content, and to enable the use and reuse of learning content by learning management systems.&lt;ref&gt;[http://www.irrodl.org/index.php/irrodl/article/view/32/378|Stephen Downes|Learning Objects: Resources For Distance Education Worldwide|The International Review of Research in Open and Distributed Learning|Volume 2 Number 1 2001|Athabasca University Press]&lt;/ref&gt;

==Definitions==
The [[Institute of Electrical and Electronics Engineers]] (IEEE) defines a learning object as "any entity, digital or non-digital, that may be used for learning, education or training".&lt;ref&gt;{{Harvnb|Learning Technology Standards Committee|2002|p=45}}&lt;/ref&gt;

Chiappe defined Learning Objects as: "A digital self-contained and reusable entity, with a clear educational purpose, with at least three internal and editable components: content, learning activities and elements of context. The learning objects must have an external structure of information to facilitate their identification, storage and retrieval: the metadata."&lt;ref&gt;{{Harvnb|Chiappe|Segovia|Rincon|2007|p=8}}.&lt;/ref&gt;

The following definitions focus on the relation between learning object and digital media.  RLO-CETL, a British inter-university Learning Objects Center, defines "reusable learning objects" as "web-based interactive chunks of e-learning designed to explain a stand-alone learning objective".&lt;ref&gt;{{citation|chapter= Learning Objects|url=http://www.rlo-cetl.ac.uk/joomla/index.php?option=com_content&amp;task=view&amp;id=235&amp;Itemid=28|title=RLO-CETL: Reusable Learning Objects|accessdate=2008-04-29}}.&lt;/ref&gt; Daniel Rehak and Robin Mason define it as "a digitized entity which can be used, reused or referenced during technology supported learning".&lt;ref&gt;http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf&lt;/ref&gt;&lt;ref&gt;{{Harvnb|Rehak|Mason|2003|p=}}&lt;/ref&gt;

Adapting a definition from the Wisconsin Online Resource Center, Robert J. Beck suggests that learning objects have the following key characteristics:

* Learning objects are a new way of thinking about learning content. Traditionally, content comes in a several hour chunk.  Learning objects are much smaller units of learning, typically ranging from 2 minutes to 15 minutes.
* Are self-contained &#8211; each learning object can be taken independently
* Are reusable &#8211; a single learning object may be used in multiple contexts for multiple purposes
* Can be aggregated &#8211; learning objects can be grouped into larger collections of content, including traditional course structures
* Are tagged with metadata &#8211; every learning object has descriptive information allowing it to be easily found by a search&lt;ref name="beck"&gt;{{citation|last=Beck|first=Robert J.|chapter=What Are Learning Objects?|url=http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 | title=Learning Objects|publisher=Center for International Education, University of Wisconsin-Milwaukee|accessdate=2008-04-29}}&lt;/ref&gt;

== Components ==
The following is a list of some of the types of information that may be included in a learning object and its metadata:
* General Course Descriptive Data, including: course identifiers, language of content (English, Spanish, etc.), subject area (Maths, Reading, etc.), descriptive text, descriptive keywords
* Life Cycle, including: version, status
* Instructional Content, including: text, web pages, images, sound, video
* Glossary of Terms, including: terms, definition, acronyms
* Quizzes and Assessments, including: questions, answers
* Rights, including: cost, copyrights, restrictions on Use
* Relationships to Other Courses, including prerequisite courses
* Educational Level, including: grade level, age range, typical learning time, and difficulty. [IEEE 1484.12.1:2002]
*Typology as defined by Churchill (2007): presentation, practice, simulation, conceptual models, information, and contextual representation &lt;ref name="Churchill"&gt;Churchill, D. (2007). Towards a useful classification of learning objects. ''Educational Technology Research &amp; Development, 55(5)'', 479-497.&lt;/ref&gt;

==Metadata==
One of the key issues in using learning objects is their identification by search engines or content management systems.{{Citation needed|date=April 2008}}  This is usually facilitated by assigning descriptive [[learning object metadata]]. Just as a book in a library has a record in the [[card catalog]], learning objects must also be tagged with metadata.  The most important pieces of metadata typically associated with a learning object include:
# '''objective:''' The educational objective the learning object is instructing
# '''prerequisites:''' The list of skills (typically represented as objectives) which the learner must know before viewing the learning object
# '''topic:''' Typically represented in a taxonomy, the topic the learning object is instructing
# '''interactivity:''' The [[Interaction Model]] of the learning object.
# '''technology requirements:''' The required system requirements to view the learning object.

==Mutability==

A mutated learning object is, according to Michael Shaw, a learning object that has been "re-purposed and/or re-engineered, changed or simply re-used in some way different from its original intended design". Shaw also introduces the term "contextual learning object", to describe a learning object that has been "designed to have specific meaning and purpose to an intended learner".&lt;ref&gt;{{Harvnb|Shaw|2003}}&lt;/ref&gt;

==Portability==
Before any institution invests a great deal of time and energy into building high-quality e-learning content (which can cost over $10,000 per classroom hour),&lt;ref&gt;Rumble, Greville. 2001. The Cost and Costing of Networked Learning. Journal of Asynchronous Learning Networks, Volume 5, Issue 2.&lt;/ref&gt; it needs to consider how this content can be easily loaded into a [[Learning Management System]]. It is possible for example, to package learning objects with [[SCORM]] specification and load it in [[Moodle]] Learning Management System or [[Desire2Learn]] Learning Environment.

If all of the properties of a course can be precisely defined in a common format, the content can be serialized into a standard format such as [[XML]] and loaded into other systems.  When it is considered that some e-learning courses need to include video, mathematical equations using [[MathML]], chemistry equations using [[Chemical Markup Language|CML]] and other complex structures, the issues become very complex, especially if the systems needs to understand and validate each structure and then place it correctly in a database.{{Citation needed|date=April 2008}}

==Criticism==
In 2001, David Wiley criticized learning object theory in his paper, [https://web.archive.org/web/20041019162710/http:/rclt.usu.edu/whitepapers/paradox.html The Reusability Paradox] which is [http://www.darcynorman.net/2003/08/21/addressing-the-reusability-paradox/ summarized by D'Arcy Norman] as, ''If a learning object is useful in a particular context, by definition it is not reusable in a different context. If a learning object is reusable in many contexts, it isn&#8217;t particularly useful in any.'' 
In [http://www.learningspaces.org/papers/objections.html Three Objections to Learning Objects and E-learning Standards], Norm Friesen, Canada Research Chair in E-Learning Practices at Thompson Rivers University, points out that the word ''neutrality'' in itself implies ''a state or position that is antithetical ... to pedagogy and teaching.''

== See also ==
* [[Intelligent tutoring system]]
* [[North Carolina Learning Object Repository (NCLOR)]]
* [[Serious games]]

==References==
{{reflist|30em}}

==Further reading==
*{{citation|last=Beck|first=Robert J.|title="What Are Learning Objects?", Learning Objects, Center for International Education, University of Wisconsin-Milwaukee, |url= http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 |year= 2009|accessdate= 2009-10-23}}.
*{{citation|last=Learning Technology Standards Committee|title=Draft Standard for Learning Object Metadata. IEEE Standard 1484.12.1|place=New York|publisher=Institute of Electrical and Electronics Engineers|year=2002|url=http://ltsc.ieee.org/wg12/files/LOM_1484_12_1_v1_Final_Draft.pdf| format=PDF| accessdate=2008-04-29}}.
*{{citation|last1=Rehak |first1=Daniel R.|first2=Robin |last2=Mason|chapter=Engaging with the Learning Object Economy|editor-first=Allison|editor-last=Littlejohn|title=Reusing Online Resources: A Sustainable Approach to E-Learning|place= London|publisher= Kogan Page| year=2003| pages=22&#8211;30| isbn=978-0-7494-3949-1}}.
*{{citation|last=Shaw|first=Michael|chapter=(Contextual and Mutated) Learning Objects in the Context of Design, Learning and (Re)Use| url=http://www.shawmultimedia.com/edtech_oct_03.html|title=Teaching and Learning with Technology|date=October 2003|accessdate=2008-04-29}}
*{{citation|first1=Andr&#233;s Chiappe |last1=Laverde |first2=Yasbley Segovia |last2=Cifuentes |first3=Helda Yadira Rinc&#243;n |last3=Rodr&#237;guez|chapter=Toward an instructional design model based on learning objects|editor-first=Springer|editor-last=Boston|title=Educational Technology Research and Development |place=Boston|year= 2007|pages=671&#8211;81|publisher=Springer US |doi=10.1007/s11423-007-9059-0 |issue=6 |volume=55 |issn=1042-1629|id=(Print) {{ISSN|1556-6501}} (Online) |url= http://www.springerlink.com/content/u84w63873vq77h2h/?p=41be7fbeef9648ee9b554f1835112005&amp;pi=6|accessdate=2008-08-21}} Spanish Draf: [http://andreschiappe.blogspot.com/2007/09/que-es-un-objeto-de-aprendizaje-what-is.html ''Blog de Andr&#233;s Chiappe - Objetos de Aprendizaje''].
*{{citation|last=Northrup|first=Pamela|title=Learning Objects for Instruction: Design and Evaluation |place=USA|publisher=Information Science Publishing |year=2007| format=Book}}.
*{{citation|last1=Hunt|first1=John P.|last2=Bernard|first2=Robert|title="An XML-based information architecture for learning content", IBM developerWorks, |url= http://www.ibm.com/developerworks/xml/library/x-dita9a |year= 2005|accessdate= 2005-08-05}}.
*Churchill, D. (2007). Towards a useful classification of learning objects.  ''Educational Technology Research &amp; Development, 55(5)'', 479-497.
Innayah: Creating An Audio Script with Learning Object, unpublished, 2013.

==External links==
* The [http://www4.uwm.edu/cie/learning_objects.cfm?gid=55 Learning Objects] at Milwaukee's Center for International Education.

{{DEFAULTSORT:Learning Object}}
[[Category:Data management]]
[[Category:Educational materials]]
[[Category:Educational technology]]</text>
      <sha1>f4m0pq449v401dcwkl251j3i06otqkt</sha1>
    </revision>
  </page>
  <page>
    <title>Category:NoSQL</title>
    <ns>14</ns>
    <id>29590205</id>
    <revision>
      <id>727132764</id>
      <parentid>546038600</parentid>
      <timestamp>2016-06-26T22:26:27Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="104" xml:space="preserve">{{Cat main|NoSQL}}

[[Category:Databases]] 
[[Category:Data management]]
[[Category:Structured storage]]</text>
      <sha1>rs6jlybdre5hu6v3i3ejr76k9xe4iwt</sha1>
    </revision>
  </page>
  <page>
    <title>Database schema</title>
    <ns>0</ns>
    <id>345937</id>
    <revision>
      <id>762638767</id>
      <parentid>762216814</parentid>
      <timestamp>2017-01-29T23:07:02Z</timestamp>
      <contributor>
        <ip>70.184.214.35</ip>
      </contributor>
      <comment>Alphabetized the categories.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9494" xml:space="preserve">A '''database schema''' ({{IPAc-en|&#712;|s|k|i|.|m|&#601;}} {{respell|SKEE|m&#601;}}) of a [[database system]] is its structure described in a [[formal language]] supported by the [[database management system]] (DBMS). The term "schema" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of [[relational databases]]). The formal definition of a [[database]] schema is a set of formulas (sentences) called [[integrity constraints]] imposed on a database.{{citation needed|date=January 2016}} These integrity constraints ensure compatibility between parts of the schema. All constraints are expressible in the same language. A database can be considered a structure in realization of the [[database language]].&lt;ref name="source1" /&gt; The states of a created [[conceptual schema]] are transformed into an [[Explicit and implicit methods|explicit mapping]], the database schema. This describes how real-world entities are modeled in the database.

"A database schema specifies, based on the [[database administrator]]'s knowledge of possible applications, the facts that can enter the database, or those of interest to the possible [[end-user]]s."&lt;ref name="source3"/&gt; The notion of a database schema plays the same role as the notion of theory in [[predicate calculus]]. A model of this "theory" closely corresponds to a database, which can be seen at any instant of time as a [[mathematical object]]. Thus a schema can contain formulas representing [[Data integrity#Types of integrity constraints|integrity constraints]] specifically for an application and the constraints specifically for a type of database, all expressed in the same database language.&lt;ref name="source1" /&gt; In a [[relational database]], the schema defines the [[Table (database)|tables]], [[Field (computer science)|fields]], [[Relational model|relationship]]s, [[View (database)|view]]s, [[Index (database)|index]]es, [[Software package (installation)|package]]s, [[stored procedure|procedure]]s, [[subroutine|function]]s, [[Queue (data structure)|queue]]s, [[Database trigger|trigger]]s, [[Data type|type]]s, [[sequence]]s, [[materialized view]]s, [[Synonym (database)|synonym]]s, [[database link]]s, [[Directory (file systems)|directories]], [[XML schema]]s, and other elements.

A database generally stores its schema in a [[data dictionary]]. Although a schema is defined in text database language, the term is often used to refer to a graphical depiction of the database structure. In other words, schema is the structure of the database that defines the objects in the database.

In an [[Oracle Database]] system, the term "schema" has a slightly different [[connotation]].

==Ideal requirements for schema integration==
{{See also|Database normalization}}

The requirements listed below influence the detailed structure of schemas that are produced. Certain applications will not require that all of these conditions are met, but these four requirements are the most ideal.

; Overlap preservation
: Each of the overlapping elements specified in the input mapping is also in a database schema relation.&lt;ref name="source2" /&gt;

; Extended overlap preservation
: Source-specific elements that are associated with a source&#8217;s overlapping elements are passed through to the database schema.&lt;ref name="source2" /&gt;

; Normalization
: Independent entities and relationships in the source data should not be grouped together in the same relation in the database schema. In particular, source specific schema elements should not be grouped with overlapping schema elements, if the grouping co-locates independent entities or relationships.&lt;ref name="source2" /&gt;

; Minimality
: If any elements of the database schema are dropped then the database schema is not ideal.&lt;ref name="source2" /&gt;

==Example of two schema integrations==
Suppose we want a mediated (database) schema to integrate two travel databases, Go-travel and Ok-travel.

'''&lt;code&gt;Go-travel&lt;/code&gt;''' has two relations:
&lt;syntaxhighlight lang="text"&gt;
Go-flight(f-num, time, meal(yes/no))
Go-price(f-num, date, price)
&lt;/syntaxhighlight&gt;
(&lt;code&gt;f-num&lt;/code&gt; being the flight number)

'''&lt;code&gt;Ok-travel&lt;/code&gt;''' has just one relation:
&lt;syntaxhighlight lang="text"&gt;
Ok-flight(f-num, date, time, price, nonstop(yes/no))
&lt;/syntaxhighlight&gt;

The overlapping information in Ok-travel&#8217;s and Go-travel&#8217;s schemas could be represented in a mediated schema:&lt;ref name="source2" /&gt;
&lt;syntaxhighlight lang="text"&gt;
Flight(f-num, date, time, price)
&lt;/syntaxhighlight&gt;

== Oracle database specificity ==
In the context of [[Oracle database]]s, a '''schema object''' is a logical [[Database storage structures|data storage structure]].&lt;ref&gt;
{{cite book
|first1= Lance |last1= Ashdown
|first2= Tom  |last2= Kyte
|others= ''et al''.
|title= Oracle Database Concepts 11g Release 2 (11.2)
|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111
|accessdate= 2010-04-14 |date=February 2010
|publisher= Oracle Corporation
|quote= A database schema is a logical container for data structures, called schema objects. Examples of schema objects are tables and indexes. 
}}
&lt;/ref&gt;

An Oracle database associates a separate schema with each database '''user'''.&lt;ref&gt;
{{cite book
|title= Oracle Database Concepts 10g Release 2 (10.2)Part Number B14220-02 
|url= http://docs.oracle.com/cd/B19306_01/server.102/b14220/schema.htm
|accessdate= 2012-11-26
|quote= A schema is a collection of logical structures of data, or schema objects. A schema is owned by a database user and has the same name as that user. Each user owns a single schema. Schema objects can be created and manipulated with SQL. 
}}&lt;/ref&gt;
A schema comprises a collection of schema objects. Examples of schema objects include:

* [[Table (database)|tables]]
* [[View (database)|views]]
* [[sequence]]s
* [[Synonym (database)|synonyms]]
* [[Index (database)|indexes]]
* clusters
* database links
* [[Snapshot (computer storage)|snapshot]]s
* [[stored procedure|procedure]]s
* functions
* packages

On the other hand, non-schema objects may include:&lt;ref&gt;{{cite book
|first1= Lance
|last1= Ashdown
|author1-link=
|first2= Tom
|last2= Kyte
|author2-link=
|others= et al.
|title= Oracle Database Concepts 11g Release 2 (11.2)
|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111
|accessdate= 2010-04-14
|date=February 2010
|publisher= Oracle Corporation
|location=
|isbn=
|quote= Other types of objects are also stored in the database and can be created and manipulated with SQL statements but are not contained in a schema. These objects include database users, roles, contexts, and directory objects.
}}&lt;/ref&gt;

* users
* roles
* contexts
* directory objects

Schema objects do not have a one-to-one correspondence to physical files on disk that store their information. However, [[Oracle database]]s store schema objects logically within a [[tablespace]] of the database. The data of each object is physically contained in one or more of the tablespace's [[datafile]]s. For some objects (such as tables, indexes, and clusters) a [[database administrator]] can specify how much disk space the Oracle [[RDBMS]] allocates for the object within the tablespace's datafiles.

There is no necessary relationship between schemas and tablespaces: a tablespace can contain objects from different schemas, and the objects for a single schema can reside in different tablespaces.

== See also ==
{{too many see alsos|date=July 2013}}
* [[Core architecture data model]] (CADM)
* [[Data definition language]] (DDL)
* [[Database design]]
* [[Data dictionary]]
* [[Data element]]
* [[Data modeling]]
* [[Data mapping]]
* [[Database integrity]]
* [[Entity&#8211;relationship model]]
* [[Knowledge representation and reasoning]]
* [[Object-role modeling]]
* [[Relational algebra]]
* [[Schema matching]]
* [[Three schema approach]]

==References==
{{reflist|refs=
&lt;ref name="source1"&gt;{{cite journal |last=Rybinski |first=H. |year=1987|title=On First-Order-Logic Databases |journal=ACM Transactions on Database Systems |volume=12 |issue=3 |pages=325&#8211;349 |doi= 10.1145/27629.27630}}&lt;/ref&gt;
&lt;ref name="source2"&gt;{{cite journal |last= Pottinger |first=P. |last2=Berstein |first2=P. |year=2008 |title= Schema merging and mapping creation for relational sources |journal= Proceedings of the 11th international conference on Extending database technology: Advances in database technology (EDBT '08) |publisher=ACM |location= New York, NY |pages=73&#8211;84 |doi= 10.1145/1353343.1353357}}&lt;/ref&gt;
&lt;ref name="source3"&gt;{{cite journal |last= Imielinski |first=T. |last2=Lipski |first2=W. |year=1982 |title=A systematic approach to relational database theory |journal= Proceedings of the 1982 ACM SIGMOD international conference on Management of data (SIGMOD '82) |publisher=ACM |location= New York, NY |pages=8&#8211;14 |DOI= 10.1145/582353.582356}}&lt;/ref&gt;
}}

== External links ==
* [https://weblogs.asp.net/scottgu/Tip_2F00_Trick_3A00_-Online-Database-Schema-Samples-Library Tip/Trick: Online Database Schema Samples Library]
* [http://web.archive.org/web/20081217074637/http://msdn.microsoft.com/en-us/library/bb187299%28SQL.80%29.aspx Database Schema Samples]
* [http://web.archive.org/web/20080828210315/http://ciobriefings.com/Publications/WhitePapers/DesigningtheStarSchemaDatabase/tabid/101/Default.aspx Designing the Star Schema Database]

{{DEFAULTSORT:Database Schema}}
[[Category:Data management]]
[[Category:Data modeling]]</text>
      <sha1>866h7bmchhywb54g5v0vjrf7o8g4s4o</sha1>
    </revision>
  </page>
  <page>
    <title>Dashboard (business)</title>
    <ns>0</ns>
    <id>4166591</id>
    <revision>
      <id>745181429</id>
      <parentid>744632161</parentid>
      <timestamp>2016-10-19T18:44:04Z</timestamp>
      <contributor>
        <ip>180.242.28.51</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10076" xml:space="preserve">[[File:3 Dashboards.JPG|thumb|200px|Business Dashboards.]]

'''Dashboards''' often provide at-a-glance views of KPIs ([[key performance indicators]])  relevant to a particular objective or business process (e.g. [[sales]], [[marketing]], [[human resources]], or [[Production (economics)|production]]).&lt;ref&gt;Michael Alexander and John Walkenbach, ''Excel Dashboards and Reports'' (Wiley, 2010)&lt;/ref&gt; In real-world terms, "dashboard" is another name for "progress report" or "report."

Often, the "dashboard" is displayed on a web page that is linked to a database which allows the report to be constantly updated. For example, a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured, or number of failed quality inspections per hour. Similarly, a human resources dashboard may show numbers related to staff recruitment, retention and composition, for example number of open positions, or average days or cost per recruitment.&lt;ref name="Briggs"&gt;{{cite web|url=http://www.targetdashboard.com/site/Dashboard-Best-Practice/Management-Report-and-Dashboard-best-practice-index.aspx|title=Management Reports &amp; Dashboard Best Practice|last=Briggs|first=Jonathan|publisher=Target Dashboard|accessdate=18 February 2013}}&lt;/ref&gt;

The term dashboard originates from the [[automobile]] [[dashboard]] where drivers   monitor the major functions at a glance via the instrument cluster. 

==Benefits==
Digital dashboards allow managers to monitor the contribution of the various departments in their organization. To gauge exactly how well an organization is performing overall, digital dashboards allow you to capture and report specific data points from each department within the organization, thus providing a "snapshot" of performance.

Benefits of using digital dashboards include:&lt;ref name="Briggs" /&gt;
*Visual presentation of performance measures
*Ability to identify and correct negative trends
*Measure efficiencies/inefficiencies
*Ability to generate detailed reports showing new trends
*Ability to make more informed decisions based on collected [[business intelligence]]
*Align strategies and organizational goals
*Saves time compared to running multiple reports
*Gain total visibility of all systems instantly
*Quick identification of data outliers and correlations

==Classification==
Dashboards can be broken down according to role and are either [[strategic]], analytical, operational, or informational.&lt;ref&gt;Steven Few, ''Information Dashboard Design: The Effective Visual Communication of Data'' (O'Reilly, 2006)&lt;/ref&gt; Strategic dashboards support managers at any level in an organization, and provide the quick overview that decision makers need to monitor the health and opportunities of the business. Dashboards of this type focus on high level measures of performance, and forecasts. Strategic dashboards benefit from static snapshots of data (daily, weekly, monthly, and quarterly) that are not constantly changing from one moment to the next. Dashboards for analytical purposes often include more context, comparisons, and history, along with subtler performance evaluators. Analytical dashboards typically support interactions with the data, such as drilling down into the underlying details. Dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment's notice.

==Types of dashboards==

Digital dashboards may be laid out to track the flows inherent in the business processes that they monitor. Graphically, users may see the high-level processes and then [[data drilling|drill down]] into low level data. This level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives.

Three main types of digital dashboard dominate the market today: stand alone software applications, web-browser based applications, and desktop applications also known as [[desktop widgets]]. The last are driven by a [[Software widget|widget engine]].

Specialized dashboards may track all corporate functions. Examples include [[human resources]], [[Recruitment|recruiting]], [[sales]], [[Business operations|operations]], [[security]], [[information technology]], [[project management]], [[customer relationship management]] and many more departmental dashboards. For a smaller organization like a startup a compact startup scorecard dashboard tracks important activities across lot of domains ranging from social media to sales.{{cn|date=September 2016}}

Digital dashboard projects involve business units as the driver and the information technology department as the enabler. The success of digital dashboard projects often depends on the [[measurement|metrics]] that were chosen for monitoring. [[Key performance indicator]]s, [[balanced scorecard]]s, and sales performance figures are some of the content appropriate on business dashboards.

==Dashboards and scoreboards==
Balanced Scoreboards and Dashboards have been linked together as if they were interchangeable. However, although both visually display critical information, the difference is in the format: Scoreboards can open the quality of an operation while dashboards provide calculated direction. 
A balanced scoreboard has what they called a "prescriptive" format. It should always contain these components (Active Strategy)...
*Perspectives &#8211; groupings of high level strategic areas
*Objectives &#8211; verb-noun phrases pulled from a strategy plan
*Measures &#8211; also called Metric or Key Performance Indicators (KPIs)
*Spotlight Indicators &#8211; red, yellow, or green symbols that provide an at-a-glance view of a measure&#8217;s performance.
Each of these sections ensures that a Balanced Scorecard is essentially connected to the businesses critical strategic needs.

The design of a dashboard is more loosely defined.  Dashboards are usually a series of graphics, charts, gauges and other visual indicators that can be monitored and interpreted.  Even when there is a strategic link, on a dashboard, it may not be noticed as such since objectives are not normally present on dashboards.  However, dashboards can be customized to link their graphs and charts to strategic objectives.&lt;ref&gt;ZSL Inc., ''Dashboards Vs Scorecards &#8211; An Insight'' ZSL Inc. (2006)&lt;/ref&gt;

==Design==
Digital dashboard technology is available "out-of-the-box" from many software providers. Some companies however continue to do in-house development and maintenance of dashboard applications. For example, [[GE Aviation]] has developed a proprietary software/portal called "Digital Cockpit" to monitor the trends in aircraft spare parts business.

A good information design will clearly communicate key information to users and makes supporting information easily accessible.&lt;ref&gt;Stacey Barr, ''7 Small Business Dashboard Design Dos and Don'ts'' (Barr, 2010)&lt;/ref&gt;

==Assessing the quality of dashboards==
There are four key elements to a good dashboard:.&lt;ref&gt;Victoria Hetherington, ''Dashboard Demystified: What is a Dashboard?'' (Hetherington, 2009)&lt;/ref&gt;
# Simple, communicates easily
# Minimum distractions...it could cause confusion
# Supports organized business with meaning and useful data
# Applies human visual perception to visual presentation of information

==History==

The idea of digital dashboards followed the study of [[decision support system]]s in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s in the form of [[Executive Information Systems]] (EISs). Due to problems primarily with data refreshing and handling, it was soon realized that the approach wasn&#8217;t practical as information was often incomplete, unreliable, and spread across too many disparate sources.&lt;ref&gt;Steven Few, ''Information Dashboard Design: The Effective Visual Communication of Data'' (O'Reilly, 2006)&lt;/ref&gt; Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and [[online analytical processing]] (OLAP) allowed dashboards to function adequately.{{fact|date=August 2014}} Despite the availability of enabling technologies, the dashboard use didn't become popular until later in that decade, with the rise of [[key performance indicators]] (KPIs), and the introduction of Robert S. Kaplan and David P. Norton's [[Balanced Scorecard]].&lt;ref&gt;[[Wayne W. Eckerson]], ''Performance Dashboards: Measuring, Monitoring, and Managing Your Business'' (Wiley , 2010)&lt;/ref&gt; In the late 1990s, [[Microsoft]] promoted a concept known as the [[Digital Nervous System]] and "digital dashboards" were described as being one leg of that concept.&lt;ref&gt;{{cite web | url=http://www.kmworld.com/Articles/News/Breaking-News/Microsoft-refines-Digital-Dashboard-concept--12189.aspx | title=Microsoft refines Digital Dashboard concept | accessdate=2009-06-09}}&lt;/ref&gt; Today, the use of dashboards forms an important part of Business Performance Management (BPM).

==See also==
* [[Business activity monitoring]]
* [[Complex event processing]]
* [[Corporate performance management]]
* [[Data presentation architecture]]
* [[Enterprise manufacturing intelligence]]
* [[Event stream processing]]
* [[Infographic|Information graphics]]
* [[Information design]]
* [[Scientific visualization]]

==References==
{{reflist}}

==Further reading==
* {{cite book
  |title=Information Dashboard Design
  |last=Few   |first=Stephen
  |publisher=O'Reilly
  |isbn=978-0-596-10016-2
  |date=2006
}}

* {{cite book
  |title=Performance Dashboards: Measuring, Monitoring, and Managing Your Business
  |last=Eckerson|first=Wayne W |author-link=
  |publisher=John Wiley &amp; Sons
  |isbn=978-0-471-77863-9
  |date=2006
}}

{{Data warehouse}}

{{DEFAULTSORT:Dashboard (Business)}}
{{Use dmy dates|date=April 2011}}
[[Category:Business terms]]
[[Category:Computing terminology]]
[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Business software]]
[[Category:Information systems]]</text>
      <sha1>41gzuccbc1s7trmkoddo1ciw267ng96</sha1>
    </revision>
  </page>
  <page>
    <title>Rainbow storage</title>
    <ns>0</ns>
    <id>8098559</id>
    <revision>
      <id>758203483</id>
      <parentid>758200665</parentid>
      <timestamp>2017-01-04T01:57:54Z</timestamp>
      <contributor>
        <username>Doctree</username>
        <id>14110041</id>
      </contributor>
      <comment>/* top */ per WP:NCCAP</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7189" xml:space="preserve">{{notability|date=November 2014}}
'''Rainbow storage''' is a developing [[paper]]-based [[data storage device|data storage]] technique first demonstrated by Indian student Sainul Abideen in November 2006.&lt;ref name="ArabNews"&gt;[http://www.arabnews.com/?page=4&amp;section=0&amp;article=88962&amp;d=18&amp;m=11&amp;y=2006 "Data Can Now Be Stored on Paper"] by M. A. Siraj, ''[[ArabNews]]'' (published November 18, 2006; accessed November 29, 2006)&lt;/ref&gt; Abideen received his [[Master of Computer Applications|MCA]] from [[MES College of Engineering|MES Engineering College]] in [[Kuttipuram]] in [[Kerala]]'s [[Malappuram]] district.

Initial newspaper reports of the technology were disputed by multiple technical sources, although Abideen says those reports were based on a misunderstanding of the technology. The paper meant to demonstrate the capability of storing relatively large amounts of data (and not necessarily in the gigabyte range) using textures and diagrams.&lt;ref name=theinq&gt;[http://www.theinquirer.net/default.aspx?article=36294 Paper storage man misunderstood] &amp;mdash; ''[[The Inquirer]]'' article, 12 December 2006 (retrieved 15 December 2006.&lt;/ref&gt;

The Rainbow data storage technology claims to use [[Geometry|geometric]] shapes such as triangles, circles and squares of various colors to store a large amount of data on ordinary paper or plastic surfaces. This would provide several advantages over current forms of [[Optical disc|optical-]] or [[Magnetic storage|magnetic]] [[data storage device|data storage]] like less environmental pollution due to the biodegradability of paper, low cost and high capacity. Data could be stored on "Rainbow Versatile Disk" (RVD) or plastic/paper cards of any form factor (like SIM cards).&lt;ref name="Techworld.com"&gt;[http://www.techworld.com/storage/news/index.cfm?newsID=7424 "Store 256GB on an A4 sheet"] by Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)&lt;/ref&gt;

==Criticism==
Following the wide media attention this news received, some of the claims have been disputed by various experts.&lt;ref name="ITSoup"&gt; [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper] By ITSoup (published November 25, 2006; accessed November 25, 2006)&lt;/ref&gt;	 &lt;ref name="ArsTechnica"&gt; [http://arstechnica.com/news.ars/post/20061126-8288.html "Can you get 256GB on an A4 sheet? No way!"]  By Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)&lt;/ref&gt;	

Printing at 1,200 dots per inch (DPI) leads to a theoretical maximum of 1,440,000 colored dots per square inch. If a scanner can reliably distinguish between 256 unique colors (thus encoding one byte per dot), the maximum possible storage is approximately 140 megabytes for a sheet of A4 paper&amp;ndash;much lower when the necessary error correction is employed. If the scanner were able to accurately distinguish between 16,777,216 colors (24 bits, or 3 bytes per dot), the capacity would triple, but it still falls well below the media stories' claims of several hundred gigabytes.

Printing this quantity of unique colors would require specialized equipment to generate many [[spot color]]s.  The [[process color]] model used by most printers provides only four colors, with additional colors simulated by a [[halftone|halftone pattern]].

At least one of three things must be true for the claim to be valid:
* The paper must be printed and scanned at a much higher resolution than 1,200 DPI, 	 
* the printer and scanner must be able to accurately produce and distinguish between an extraordinary number of distinct color values, or 	 
* the compression scheme must be a revolutionary [[lossless compression]] algorithm. 	 

The theory is: If Rainbow's "geometric" algorithm is to be encoded and decoded by a computer, it would equally viable to store the compressed data on a conventional disk rather than printing it to paper or other non-digital medium. Printing something as dots on a page rather than bits on a disk will not change the underlying compression ratio, so a lossless compression algorithm that could store 250 gigabytes within a few hundred megabytes of data would be revolutionary indeed. Likewise, data can be compressed with ''any'' algorithm and subsequently printed to paper as colored dots. The amount of data that can be reliably stored in this way is limited by the printer and scanner, as described above.

However Sainul Abideen says that the articles are based on misunderstandings. He claims, it as a method to store data in the form of colour, in any medium where colour can be represented, not only paper. Density of storage in paper will be very small and the density will be depends on the storage medium, capacity of colour representation and retrieval methods etc.

==Demonstrations==
Sainul Abdeen demonstrated his technology to the college and members of the Indian press in the MES College of Engineering computer lab, Kerala, and was able to compress 450 sheets plain text from [[Foolscap folio|foolscap paper]] into a 1 inch square. He also demonstrated a 45-second audio clip compressed using this technology on to an [[ISO 216|A4 sheet]]. Depending on the sampling frequency, bit depth, and audio compression (if any), a 45-second audio clip can consist of anywhere from a few kilobytes to a few megabytes of data.  Abideen claimed that the technology could be extended to 250 gigabytes by using specific materials and devices. {{Fact|date=June 2009}}

This technology is based on two principles:

;Principle I
:&#8220;Every color or color combinations can be converted into some values and from the values the colors or color combinations can be regenerated&#8221;.
;Principle II
:&#8220;Every different color or color combinations will produce different values&#8221;.

==References==
{{reflist}}


==Absolute Rainbow Dots==
Absolute rainbow dots are used to detect errors caused by scratches, and whether any fading has occurred. Absolute rainbow dots are predefined dots carrying a unique value. These dots can be inserted in the rainbow picture in pre-specified areas. If fading occurs these dot values will change accordingly, and at the reproduction stage this can be checked and corrected.
Absolute rainbow dots will be microscopically small so that they occupy very little space in the rainbow picture. These will be colored differently so that each dot will have its own fixed unique value.

==External links==
* [http://www.kerlontech.com/RandD.html Sainul Abideen's home page] (dead)
* [http://www.deccanherald.com/deccanherald/Sep62006/cyberspace163748200695.asp Deccan Herald's article on Rainbow Storage] (dead)
* [http://www.dailytech.com/article.aspx?newsid=5052 Article in DailyTech,]
* [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper]
* [http://www.theregister.co.uk/2006/11/23/rvd_system/ Article in The Register]
*[http://www.idm.net.au/storypages/storydata.asp?id=7749 IDM: Paper Storage Claims A Hoax?]  (dead)

[[Category:Data management]]
[[Category:Vaporware]]</text>
      <sha1>258zidpa1g6gzn5mdi6p82j47tl3m5u</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Database theory</title>
    <ns>14</ns>
    <id>10221974</id>
    <revision>
      <id>547489250</id>
      <parentid>460799758</parentid>
      <timestamp>2013-03-28T19:13:37Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363916]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="134" xml:space="preserve">{{Cat main|database theory}}

[[Category:Areas of computer science]]
[[Category:Databases|Theory]]
[[Category:Data management|Theory]]</text>
      <sha1>n7vxpzshmvizcohpoadxlv240xz1rfp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data centers</title>
    <ns>14</ns>
    <id>24125707</id>
    <revision>
      <id>588264584</id>
      <parentid>547201728</parentid>
      <timestamp>2013-12-29T21:10:37Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor />
      <comment>Adding Commons category link to [[:Commons:Category:Data centers|category with the same name]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="151" xml:space="preserve">{{Commons category|Data centers}}
{{catmain|Data center}}

[[Category:Data management|Centers]]
[[Category:Servers (computing)]]
[[Category:Computers]]</text>
      <sha1>ogk6dlmn54xiyhroes9di4k1vi6r4of</sha1>
    </revision>
  </page>
  <page>
    <title>State transition network</title>
    <ns>0</ns>
    <id>31261582</id>
    <revision>
      <id>635507085</id>
      <parentid>583937780</parentid>
      <timestamp>2014-11-26T13:04:49Z</timestamp>
      <contributor>
        <username>Brirush</username>
        <id>17368641</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="708" xml:space="preserve">{{unreferenced|date=March 2011}}

A '''state transition network''' is a [[diagram]] that is developed from a set of data and charts the [[data flow|flow of data]] from particular data points (called states or nodes) to the next in a probabilistic manner.

==Use==
State transition networks are used in both [[academic]] and [[industry|industrial]] fields. 

==Examples==
State transition networks are a general construct, with more specific examples being augmented transition networks, recursive transition networks, and augmented recursive networks, among others.
==See also==
* [[State transition system]]
* [[Markov network]]
* [[History monoid]]

==References==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>opltjpqygpr4itqiszrg6cqrpy26tmo</sha1>
    </revision>
  </page>
  <page>
    <title>Log trigger</title>
    <ns>0</ns>
    <id>31397529</id>
    <revision>
      <id>760393642</id>
      <parentid>760393573</parentid>
      <timestamp>2017-01-16T18:42:58Z</timestamp>
      <contributor>
        <ip>187.39.108.160</ip>
      </contributor>
      <comment>/* Alternatives */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16705" xml:space="preserve">In [[relational database]]s, the '''Log trigger''' or '''History trigger''' is a mechanism for automatic recording of information about changes inserting or/and updating or/and deleting [[Row (database)|rows]] in a [[Table (database)|database table]].

It is a particular technique for [[Change data capture|change data capturing]], and in [[data warehousing]] for dealing with [[slowly changing dimension]]s.

== Definition ==

Suppose there is a [[Table (database)|table]] which we want to audit. This [[Table (database)|table]] contains the following [[Column (database)|columns]]:

&lt;code&gt;Column1, Column2, ..., Columnn&lt;/code&gt;

The [[Column (database)|column]] &lt;code&gt;Column1&lt;/code&gt; is assumed to be the [[primary key]].

These [[Column (database)|columns]] are defined to have the following types:

&lt;code&gt;Type1, Type2, ..., Typen&lt;/code&gt;

The '''Log Trigger''' works writing the changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) on the [[Table (database)|table]] in another, '''history table''', defined as following:

&lt;syntaxhighlight lang="sql"&gt;
CREATE TABLE HistoryTable (
   Column1   Type1,
   Column2   Type2,
      :        :
   Columnn   Typen,

   StartDate DATETIME,
   EndDate   DATETIME
)
&lt;/syntaxhighlight&gt;

As shown above, this new [[Table (database)|table]] contains the same [[Column (database)|columns]] as the original [[Table (database)|table]], and additionally two new [[Column (database)|columns]] of type &lt;code&gt;DATETIME&lt;/code&gt;: &lt;code&gt;StartDate&lt;/code&gt; and &lt;code&gt;EndDate&lt;/code&gt;. This is known as [[Tuple-versioning|tuple versioning]]. These two additional [[Column (database)|columns]] define a period of time of "validity" of the data associated with a specified entity (the entity of the [[primary key]]), or in other words, it stores how the data were in the period of time between the &lt;code&gt;StartDate&lt;/code&gt; (included) and &lt;code&gt;EndDate&lt;/code&gt; (not included).

For each entity (distinct [[primary key]]) on the original [[Table (database)|table]], the following structure is created in the history [[Table (database)|table]]. Data is shown as example.

[[File:example log trigger.png|center|example]]

Notice that if they are shown chronologically the &lt;code&gt;EndDate&lt;/code&gt; [[Column (database)|column]] of any [[Row (database)|row]] is exactly the &lt;code&gt;StartDate&lt;/code&gt; of its successor (if any). It does not mean that both [[Row (database)|rows]] are common to that point in time, since -by definition- the value of &lt;code&gt;EndDate&lt;/code&gt; is not included.

There are two variants of the '''Log trigger''', depending how the old values (DELETE, UPDATE) and new values (INSERT, UPDATE) are exposed to the trigger (it is RDBMS dependent):

'''Old and new values as fields of a record data structure'''

&lt;syntaxhighlight lang="sql"&gt;
CREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS
DECLARE @Now DATETIME
SET @Now = GETDATE()

/* deleting section */

UPDATE HistoryTable
   SET EndDate = @Now
 WHERE EndDate IS NULL
   AND Column1 = OLD.Column1

/* inserting section */

INSERT INTO HistoryTable (Column1, Column2, ...,Columnn, StartDate, EndDate) 
VALUES (NEW.Column1, NEW.Column2, ..., NEW.Columnn, @Now, NULL)
&lt;/syntaxhighlight&gt;

'''Old and new values as rows of virtual tables'''

&lt;syntaxhighlight lang="sql"&gt;
CREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS
DECLARE @Now DATETIME
SET @Now = GETDATE()

/* deleting section */

UPDATE HistoryTable
   SET EndDate = @Now
  FROM HistoryTable, DELETED
 WHERE HistoryTable.Column1 = DELETED.Column1
   AND HistoryTable.EndDate IS NULL

/* inserting section */

INSERT INTO HistoryTable
       (Column1, Column2, ..., Columnn, StartDate, EndDate)
SELECT (Column1, Column2, ..., Columnn, @Now, NULL)
  FROM INSERTED
&lt;/syntaxhighlight&gt;

=== Compatibility notes ===

* The function &lt;code&gt;GetDate()&lt;/code&gt; is used to get the system date and time, a specific [[Relational database management system|RDBMS]] could either use another function name, or get this information by another way.
* Several [[Relational database management system|RDBMS]] (DB2, MySQL) do not support that the same trigger can be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]). In such a case a trigger must be created for each operation; For an [[Insert (SQL)|INSERT]] operation only the ''inserting section'' must be specified, for a [[Delete (SQL)|DELETE]] operation only the ''deleting section'' must be specified, and for an [[Update (SQL)|UPDATE]] operation both sections must be present, just as it is shown above (the ''deleting section'' first, then the ''inserting section''), because an [[Update (SQL)|UPDATE]] operation is logically represented as a [[Delete (SQL)|DELETE]] operation followed by an [[Insert (SQL)|INSERT]] operation.
* In the code shown, the record data structure containing the old and new values are called &lt;code&gt;OLD&lt;/code&gt; and &lt;code&gt;NEW&lt;/code&gt;. On a specific [[Relational database management system|RDBMS]] they could have different names.
* In the code shown, the virtual tables are called &lt;code&gt;DELETED&lt;/code&gt; and &lt;code&gt;INSERTED&lt;/code&gt;. On a specific [[Relational database management system|RDBMS]] they could have different names. Another [[Relational database management system|RDBMS]] (DB2) even let the name of these logical tables be specified.
* In the code shown, comments are in C/C++ style, they could not be supported by a specific [[Relational database management system|RDBMS]], or a different syntax should be used.
* Several [[Relational database management system|RDBMS]] require that the body of the trigger is enclosed between &lt;code&gt;BEGIN&lt;/code&gt; and &lt;code&gt;END&lt;/code&gt; keywords.

=== [[Data warehousing]] ===

According with the [[slowly changing dimension]] management methodologies, The '''log trigger''' falls into the following:

* [[Slowly changing dimension#Type 2|Type 2]] ([[Tuple-versioning|tuple versioning]] variant)
* [[Slowly changing dimension#Type 4|Type 4]] (use of history tables)

== Implementation in common [[RDBMS]] ==

=== [[IBM DB2]]&lt;ref&gt;"Database Fundamentals" by Nareej Sharma et al. (First Edition, Copyright IBM Corp. 2010)&lt;/ref&gt; ===

* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.
* The old and new values are exposed as fields of a record data structures. The names of these records can be defined, in this example they are named as &lt;code&gt;O&lt;/code&gt; for old values and &lt;code&gt;N&lt;/code&gt; for new values.

&lt;syntaxhighlight lang="sql"&gt;
-- Trigger for INSERT
CREATE TRIGGER Database.TableInsert AFTER INSERT ON Database.OriginalTable
REFERENCING NEW AS N
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);
END;

-- Trigger for DELETE
CREATE TRIGGER Database.TableDelete AFTER DELETE ON Database.OriginalTable
REFERENCING OLD AS O
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   UPDATE Database.HistoryTable
      SET EndDate = Now
    WHERE Column1 = O.Column1
      AND EndDate IS NULL;
END;

-- Trigger for UPDATE
CREATE TRIGGER Database.TableUpdate AFTER UPDATE ON Database.OriginalTable
REFERENCING NEW AS N OLD AS O
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   UPDATE Database.HistoryTable
      SET EndDate = Now
    WHERE Column1 = O.Column1
      AND EndDate IS NULL;

   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);
END;
&lt;/syntaxhighlight&gt;

=== [[Microsoft SQL Server]]&lt;ref&gt;"Microsoft SQL Server 2008 - Database Development" by Thobias Thernstr&#246;m et al. (Microsoft Press, 2009)&lt;/ref&gt; ===

* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.
* Old and new values as rows of virtual tables named &lt;code&gt;DELETED&lt;/code&gt; and &lt;code&gt;INSERTED&lt;/code&gt;.

&lt;syntaxhighlight lang="sql"&gt;
CREATE TRIGGER TableTrigger ON OriginalTable FOR DELETE, INSERT, UPDATE AS

DECLARE @NOW DATETIME
SET @NOW = CURRENT_TIMESTAMP

UPDATE HistoryTable
   SET EndDate = @now
  FROM HistoryTable, DELETED
 WHERE HistoryTable.ColumnID = DELETED.ColumnID
   AND HistoryTable.EndDate IS NULL

INSERT INTO HistoryTable (ColumnID, Column2, ..., Columnn, StartDate, EndDate)
SELECT ColumnID, Column2, ..., Columnn, @NOW, NULL
  FROM INSERTED
&lt;/syntaxhighlight&gt;

=== [[MySQL]] ===

* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.
* The old and new values are exposed as fields of a record data structures called &lt;code&gt;Old&lt;/code&gt; and &lt;code&gt;New&lt;/code&gt;.

&lt;syntaxhighlight lang="sql"&gt;
DELIMITER $$

/* Trigger  for INSERT */
CREATE TRIGGER HistoryTableInsert AFTER INSERT ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();
    
   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);
END;

/* Trigger for DELETE */
CREATE TRIGGER HistoryTableDelete AFTER DELETE ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();
    
   UPDATE HistoryTable
      SET EndDate = N
    WHERE Column1 = OLD.Column1
      AND EndDate IS NULL;
END;

/* Trigger for UPDATE */
CREATE TRIGGER HistoryTableUpdate AFTER UPDATE ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();

   UPDATE HistoryTable
      SET EndDate = N
    WHERE Column1 = OLD.Column1
      AND EndDate IS NULL;

   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);
END;
&lt;/syntaxhighlight&gt;

=== [[Oracle Database|Oracle]] ===

* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.
* The old and new values are exposed as fields of a record data structures called &lt;code&gt;:OLD&lt;/code&gt; and &lt;code&gt;:NEW&lt;/code&gt;.
* It is necessary to test the nullity of the fields of the &lt;code&gt;:NEW&lt;/code&gt; record that define the [[primary key]] (when a [[Delete (SQL)|DELETE]] operation is performed), in order to avoid the insertion of a new row with null values in all columns.

&lt;syntaxhighlight lang="sql"&gt;
CREATE OR REPLACE TRIGGER TableTrigger
AFTER INSERT OR UPDATE OR DELETE ON OriginalTable
FOR EACH ROW
DECLARE Now TIMESTAMP;
BEGIN
   SELECT CURRENT_TIMESTAMP INTO Now FROM Dual;

   UPDATE HistoryTable
      SET EndDate = Now
    WHERE EndDate IS NULL
      AND Column1 = :OLD.Column1;

   IF :NEW.Column1 IS NOT NULL THEN
      INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate) 
      VALUES (:NEW.Column1, :NEW.Column2, ..., :NEW.Columnn, Now, NULL);
   END IF;
END;
&lt;/syntaxhighlight&gt;

== Historic information ==

Typically, [[Database dump|database backups]] are used to store and retrieve historic information. A [[Database dump|database backup]] is a security mechanism, more than an effective way to retrieve ready-to-use historic information.

A (full) [[Database dump|database backup]] is only a snapshot of the data in specific points of time, so we could know the information of each snapshot, but we can know nothing between them. Information in [[Database dump|database backups]] is discrete in time.

Using the '''log trigger''' the information we can know is not discrete but continuous, we can know the exact state of the information in any point of time, only limited to the granularity of time provided with the &lt;code&gt;DATETIME&lt;/code&gt; data type of the [[Relational database management system|RDBMS]] used.

== Advantages ==

* It is simple.
* It is not a commercial product, it works with available features in common [[Relational database management system|RDBMS]].
* It is automatic, once it is created, it works with no further human intervention.
* It is not required to have good knowledge about the tables of the database, or the data model.
* Changes in current programming are not required.
* Changes in the current [[Table (database)|tables]] are not required, because log data of any [[Table (database)|table]] is stored in a different one.
* It works for both programmed and ad hoc statements.
* Only changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) are registered, so the growing rate of the history tables are proportional to the changes.
* It is not necessary to apply the trigger to all the tables on database, it can be applied to certain [[Table (database)|tables]], or certain [[Column (database)|columns]] of a [[Table (database)|table]].

== Disadvantages ==

* It does not automatically store information about the user producing the changes (information system user, not database user). This information might be provided explicitly. It could be enforced in information systems, but not in ad hoc queries.

== Examples of use ==

=== Getting the current version of a table ===

&lt;syntaxhighlight lang="sql"&gt;
SELECT Column1, Column2, ..., Columnn
  FROM HistoryTable
 WHERE EndDate IS NULL
&lt;/syntaxhighlight&gt;

It should return the same resultset of the whole original [[Table (database)|table]].

=== Getting the version of a table in a certain point of time ===

Suppose the &lt;code&gt;@DATE&lt;/code&gt; variable contains the point or time of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT  Column1, Column2, ..., Columnn
  FROM  HistoryTable
 WHERE  @Date &gt;= StartDate
   AND (@Date &lt; EndDate OR EndDate IS NULL)
&lt;/syntaxhighlight&gt;

=== Getting the information of an entity in a certain point of time ===

Suppose the &lt;code&gt;@DATE&lt;/code&gt; variable contains the point or time of interest, and the &lt;code&gt;@KEY&lt;/code&gt; variable contains the [[primary key]] of the entity of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT  Column1, Column2, ..., Columnn
  FROM  HistoryTable
 WHERE  Column1 = @Key
   AND  @Date &gt;= StartDate
   AND (@Date &lt;  EndDate OR EndDate IS NULL)
&lt;/syntaxhighlight&gt;

=== Getting the history of an entity ===

Suppose the &lt;code&gt;@KEY&lt;/code&gt; variable contains the [[primary key]] of the entity of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT Column1, Column2, ..., Columnn, StartDate, EndDate
  FROM HistoryTable
 WHERE Column1 = @Key
 ORDER BY StartDate
&lt;/syntaxhighlight&gt;

=== Getting when and how an entity was created ===

Suppose the &lt;code&gt;@KEY&lt;/code&gt; variable contains the [[primary key]] of the entity of interest.

&lt;syntaxhighlight lang="sql"&gt;
SELECT H2.Column1, H2.Column2, ..., H2.Columnn, H2.StartDate
  FROM HistoryTable AS H2 LEFT OUTER JOIN HistoryTable AS H1
    ON H2.Column1 = H1.Column1
   AND H2.Column1 = @Key
   AND H2.StartDate = H1.EndDate
 WHERE H2.EndDate IS NULL
&lt;/syntaxhighlight&gt;

== Immutability of [[primary key]]s ==

Since the trigger requires that [[primary key]] being the same throughout time, it is desirable to either ensure or maximize its immutability, if a [[primary key]] changed its value, the entity it represents would break its own history.

There are several options to achieve or maximize the [[primary key]] immutability:

* Use of a [[Surrogate Key|surrogate key]] as a [[primary key]]. Since there is no reason to change a value with no meaning other than identity and uniqueness, it would never change.
* Use of an immutable [[natural key]] as a [[primary key]]. In a good database design, a [[natural key]] which can change should not be considered as a "real" [[primary key]].
* Use of a mutable [[natural key]] as a [[primary key]] (it is widely discouraged) where changes are propagated in every place where it is a [[foreign key]]. In such a case, the history table should be also affected.

=== Alternatives ===

Sometimes the [[Slowly changing dimension]] is used as a method, this diagram is an example:
[[File:Scd model.png|frame|right|Scd model]]

== See also ==

* [[RDBMS|Relational database]]
* [[Primary key]]
* [[Natural key]]
* [[Surrogate key]]
* [[Change data capture]]
* [[Slowly changing dimension]]
* [[Tuple-versioning|Tuple versioning]]

== Notes ==

The Log trigger was written by [[Laurence Ruiz Ugalde|Laurence R. Ugalde]] to automatically generate history of transactional databases.

==References==
&lt;references /&gt;

{{DEFAULTSORT:Log Trigger}}
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Data warehousing]]</text>
      <sha1>7pj4vao46yek0q2jq42gjf272potvdf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Object-oriented database management systems</title>
    <ns>14</ns>
    <id>2595964</id>
    <revision>
      <id>435516766</id>
      <parentid>349220324</parentid>
      <timestamp>2011-06-21T19:54:37Z</timestamp>
      <contributor>
        <username>RickBeton</username>
        <id>24644</id>
      </contributor>
      <comment>additional related category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="154" xml:space="preserve">Articles in this category are pure [[object-oriented database management system]]s.

[[Category:Data management]]
[[Category:Database management systems]]</text>
      <sha1>2f4arafq6jbmjrrhxc83mb8xif0t00c</sha1>
    </revision>
  </page>
  <page>
    <title>Cloud Data Management Interface</title>
    <ns>0</ns>
    <id>32115812</id>
    <revision>
      <id>677599436</id>
      <parentid>671762229</parentid>
      <timestamp>2015-08-24T09:54:52Z</timestamp>
      <contributor>
        <ip>79.35.205.170</ip>
      </contributor>
      <comment>Changed latest version (as it is written in www.snia.org/cdmi)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7077" xml:space="preserve">{{Infobox standardref
| title             = Cloud Data Management Interface
| status            = Published
| year_started      = 2009
| version           = 1.1.1
| organization      = [[Storage Networking Industry Association]]
| base_standards    = [[Hypertext Transfer Protocol]]
| related_standards = [[Network File System]]
| abbreviation      = CDMI
| domain            = [[Cloud computing]]
| license           = 
| website           = [http://www.snia.org/cloud CDMI Technical Working Group]
}}

The '''Cloud Data Management Interface''' ('''CDMI''') is a [http://www.snia.org SNIA] standard that specifies a protocol for self-provisioning, administering and accessing [[cloud storage]].&lt;ref&gt;{{cite web|title=Cloud Data Management Interface|url=http://www.snia.org/cdmi|publisher=SNIA|accessdate=26 June 2011}}&lt;/ref&gt;

CDMI defines [[REST]]ful [[HTTP]] operations for assessing the capabilities of the cloud storage system, allocating and accessing containers and objects, managing users and groups, implementing access control, attaching metadata, making arbitrary queries, using persistent queues, specifying retention intervals and holds for compliance purposes, using a logging facility, billing, moving data between cloud systems, and exporting data via other protocols such as [[iSCSI]] and [[Network File System (protocol)|NFS]]. Transport security is obtained via [[Transport Layer Security|TLS]].

==Capabilities==
Compliant implementations must provide access to a set of configuration parameters known as ''capabilities''.
These are either boolean values that represent whether or not a system supports things such as queues, export via other protocols, path-based storage and so on, or numeric values expressing system limits, such as how much metadata may be placed on an object.  As a minimal compliant implementation can be quite small, with few features, clients need to check the cloud storage system for a capability before attempting to use the functionality it represents.

==Containers==
A CDMI client may access objects, including containers, by either name or object id (OID), assuming the CDMI server supports both methods.  When storing objects by name, it is natural to use nested named containers; the resulting structure corresponds exactly to a traditional filesystem directory structure.

==Objects==
Objects are similar to files in a traditional file system, but are enhanced with an increased amount of and capacity for [[metadata]].  As with containers, they may be accessed by either name or OID.  When accessed by name, clients use [[Uniform Resource Locator|URLs]] that contain the full pathname of objects to [[create, read, update and delete]] them. When accessed by OID, the URL specifies an OID string in the '''cdmi-objectid''' container; this container presents a flat name space conformant with standard object storage system semantics.

Subject to system limits, objects may be of any size or type and have arbitrary user-supplied metadata attached to them. Systems that support query allow arbitrary queries to be run against the metadata.

==Domains, Users and Groups==
CDMI supports the concept of a ''domain'', similar in concept to a domain in the [[Windows]] [[Active Directory]] model. Users and groups created in a domain share a common administrative database and are known to each other on a "first name" basis, i.e. without reference to any other domain or system.

Domains also function as containers for usage and billing summary data.

==Access Control==
CDMI exactly follows the [[Access Control List|ACL]] and [[Access Control Entry|ACE]] model used for file authorization operations by [[NFSv4#NFSv4|NFSv4]]. This makes it also compatible with [[Microsoft Windows]] systems.

==Metadata==
CDMI draws much of its metadata model from the [[XAM]] specification. Objects and containers have "storage system metadata", "data system metadata" and arbitrary user specified metadata, in addition to the metadata maintained by an ordinary filesystem (atime etc.).

==Queries==
CDMI specifies a way for systems to support arbitrary queries against CDMI containers, with a rich set of comparison operators, including support for [[regular expression]]s.

==Queues==
CDMI supports the concept of persistent [[FIFO (computing and electronics)|FIFO]] (first-in, first-out) queues. These are useful for job scheduling,  order processing and other tasks in which lists of things must be processed in order.

==Compliance==
Both retention intervals and retention holds are supported by CDMI.  A retention interval consists of a start time and a retention period.  During this time interval, objects are preserved as immutable and may not be deleted. A retention hold is usually placed on an object because of judicial action and has the same effect: objects may not be changed nor deleted until all holds placed on them are removed.

==Logging==
CDMI clients can sign up for logging of system, security and object access events on servers that support it.  This feature allows clients to see events locally as the server logs them.

==Billing==
Summary information suitable for billing clients for on-demand services can be obtained by authorized users from systems that support it.

==Serialization==
Serialization of objects and containers allows export of all data and metadata on a system and importation of that data into another cloud system.

==Foreign Protocols==
CDMI supports export of containers as NFS or CIFS shares.  Clients that mount these shares see the container hierarchy as an ordinary filesystem directory hierarchy, and the objects in the containers as normal files. Metadata outside of ordinary filesystem metadata may or may not be exposed.

Provisioning of iSCSI LUNs is also supported.

== Client SDKs ==
* [http://www.snia.org/forums/csi/programs/CDMIportal CDMI Reference Implementation]
* [https://github.com/scality/Droplet Droplet]
* [https://github.com/livenson/libcdmi-java libcdmi-java]
* [https://github.com/livenson/libcdmi-python libcdmi-python]
* [https://github.com/projectpvg1/.net-SDK .NET SDK]

== See also ==
[[Comparison of CDMI server implementations]]

== References ==
{{reflist}}

== External links ==
* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=40874 ISO-8601]  International Organization for Standardization, "Data elements and interchange formats -- Information interchange -- Representation of dates and times&#8221;, ISO 8601:20044
* [http://www.itu.int/ITU-T/publications/recs.html ITU-T509]  International Telecommunications Union Telecommunication Standardization Sector (ITU-T), Recommendation X.509: Information technology - Open Systems Interconnection - The Directory: Public-key and attribute certificate frameworks, May 2000. Specification and technical corrigenda -
* [http://www.unix.org/version3/ieee_std.html POSIX ERE] The Open Group, Base Specifications Issue 6, IEEE Std 1003.1, 2004 Edition
* [http://www.cloudplugfest.org/ Cloud Interoperability Plugfest project]

[[Category:Cloud storage]]
[[Category:Data management]]</text>
      <sha1>go51qrlype33klzw8jsvbobig4qfpfi</sha1>
    </revision>
  </page>
  <page>
    <title>H-Store</title>
    <ns>0</ns>
    <id>32670994</id>
    <revision>
      <id>739312237</id>
      <parentid>727316634</parentid>
      <timestamp>2016-09-13T23:59:03Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7274" xml:space="preserve">{{Infobox software
| name                   = H-Store
| logo                   = [[File:H-Store-logo.png|80px|H-Store logo]]
| screenshot             =
| caption                =
| developer              = [[Brown University|Brown]], [[Carnegie Mellon University|CMU]], [[Massachusetts Institute of Technology|MIT]], [[Yale University|Yale]]
| latest release version = June 2016
| latest release date    = {{Start date and age|2016|06|03}}
| programming language   = [[C++]], [[Java (programming language)|Java]]
| operating system       = [[Linux]], [[Mac OS X]]
| genre                  = [[Database Management System]]
| license                = [[BSD License]], [[GPL]]
| website                = {{URL|hstore.cs.brown.edu}}
}}

'''H-Store''' is an experimental [[database management system]] (DBMS) designed for [[online transaction processing]] applications that is being developed by a team at [[Brown University]], [[Carnegie Mellon University]], the [[Massachusetts Institute of Technology]], and [[Yale University]].&lt;ref&gt;
{{cite web 
| url = http://hstore.cs.brown.edu
| title = H-Store - Next Generation OLTP DBMS Research
| accessdate = 2011-08-07
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/
| title = Stonebraker's H-Store: There's something happenin' here
| first = David
| last = Van Couvering

| date = 2008-02-18
| &lt;!-- is this one just plain wrong?:--&gt; publication-date = 2011-03-11
| accessdate = 2012-07-18
}}
&lt;/ref&gt;
The system's design was developed in 2007 by database researchers [[Michael Stonebraker]], [[Samuel Madden (MIT)|Sam Madden]], Andy Pavlo and Daniel Abadi.&lt;ref&gt;
{{cite conference
| authorlink = Michael Stonebraker
| first = Mike | last = Stonebraker
| title = The end of an architectural era: (it's time for a complete rewrite)
| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases
| location = Vienna, Austria
| year = 2007
| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf
| format = PDF |display-authors=etal}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
| last1 = Kallman
| first1 = Robert
| last2 = Kimura
| first2 = Hideaki
| last3 = Natkins
| first3 = Jonathan
| last4 = Pavlo
| first4 = Andrew
| last5 = Rasin
| first5 = Alexander
| last6 = Zdonik
| first6 = Stanley
| authorlink6 = Stan Zdonik
| last7 = Jones
| first7 = Evan P. C.
| last8 = Madden
| first8 = Samuel
| authorlink8 = Samuel Madden (MIT)
| last9 = Stonebraker
| first9 = Michael
| authorlink9 = Michael Stonebraker
| last10 = Zhang
| first10 = Yang
| last11 = Hugg
| first11 = John
| last12 = Abadi
| first12 = Daniel J.
| title = H-Store: a high-performance, distributed main memory transaction processing system
| journal = Proc. VLDB Endowment
| year = 2008
| volume = 1
| series = 2
| pages = 1496&#8211;1499
| url = http://hstore.cs.brown.edu/papers/hstore-demo.pdf
| issn = 2150-8097
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/
| title = Mike Stonebraker calls for the complete destruction of the old DBMS order
| first = Curt
| last = Monash
| year = 2008
| publication-date = 2008-02-18
| accessdate  = 2012-07-18
}}
&lt;/ref&gt;

==Architecture==
The significance of the H-Store is that it is the first implementation of a new class of [[Parallel database|parallel database management systems]], called [[NewSQL]],&lt;ref&gt;{{cite web|url=http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |title=How Will The Database Incumbents Respond To NoSQL And NewSQL? |first=Matthew |last=Aslett |publisher=451 Group |publication-date=2011-04-04 |year=2010 |accessdate=2012-07-06 |deadurl=yes |archiveurl=https://web.archive.org/web/20120127202623/http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |archivedate=January 27, 2012 }}
&lt;/ref&gt;&lt;!-- good link, just not supporting H-Store directly, is supporting [[VoltDB]] that is related, but doesn not state the connection: &lt;ref&gt;
{{cite web 
| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext
| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps
| first = Michael
| last = Stonebraker
| publisher = Communications of the ACM
| publication-date = 2011-06-16
| accessdate = 2012-07-06
}}
&lt;/ref&gt; --&gt;that provide the high-throughput and high-availability of [[NoSQL]] systems, but without giving up the [[ACID|transactional guarantees]] of a traditional DBMS.&lt;ref&gt;
{{cite web 
| url = http://preferisco.blogspot.com/2008/03/h-store-new-architectural-era-or-just.html
| title = H-Store - a new architectural era, or just a toy? 
| first = Nigel
| last = Thomas

| date = 2008-03-01
| accessdate = 2012-07-05
}}
&lt;/ref&gt;
Such systems are able to scale out horizontally across multiple machines to improve throughput, as opposed to moving to a more powerful, more expensive machine for a single-node system.&lt;ref&gt;
{{cite web 
| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/
| title = Is H-Store the future of database management systems?
| first = Matthew
| last = Aslett

| date = 2008-03-04
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

H-Store is able to execute [[transaction processing]] with high throughput by forgoing much of legacy architecture of [[IBM System R|System R]]-like systems. For example, H-Store was designed as a [[Parallel database|parallel]], row-storage relational DBMS that runs on a cluster of [[Shared nothing architecture|shared-nothing]], main memory executor nodes.&lt;ref&gt;
{{cite web 
| url = http://hstore.cs.brown.edu/documentation/architecture-overview/
| title = H-Store - Architecture Overview
| accessdate  = 2011-08-07
}}
&lt;/ref&gt;
The database is [[Partition (database)|partitioned]] into disjoint subsets that are assigned to a single-threaded execution engine assigned to one and only one [[Multi-core processor|core]] on a node. Each engine has exclusive access to all of the data at its partition. Because it is single-threaded, only one transaction at a time is able to access the data stored at its partition. Thus, there are no physical locks or latches in the system, and no transaction will stall waiting for another transaction once it is started.&lt;ref&gt;
{{cite web 
| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055
| title = H-Store: Complete destruction of the old DBMS order?
| first = Larry
| last = Dignan
| year = 2008
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

==Licensing==
H-Store is licensed under the [[BSD license]] and [[GPL]] licenses. The commercial version of H-Store's design is [[VoltDB]].&lt;ref&gt;
{{cite web
| url         = http://www.dbms2.com/2009/06/22/h-store-horizontica-voltdb/
| title       = H-Store is now VoltDB
| first       = Curt
| last        = Monash
| year        = 2009
| accessdate  = 2011-07-14
| postscript  = 
}}
&lt;/ref&gt;

==See also==
{{Portal|Free software}}
*[[VoltDB]]
*[[C-Store]]
*[[Transaction processing]]

==References==
{{Reflist}}

==External links==

[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:NewSQL]]</text>
      <sha1>lq1itcnsejsxvpi3snd9901bl7lu1pv</sha1>
    </revision>
  </page>
  <page>
    <title>Information governance</title>
    <ns>0</ns>
    <id>22723009</id>
    <revision>
      <id>756925029</id>
      <parentid>754590586</parentid>
      <timestamp>2016-12-27T18:10:17Z</timestamp>
      <contributor>
        <ip>217.35.252.69</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13911" xml:space="preserve">{{Governance}}

'''Information governance''', or '''IG''', is the set of multi-disciplinary structures, policies, procedures, processes and controls implemented to manage information at an enterprise level, supporting an organization's immediate and future regulatory, legal, risk, environmental and operational requirements. Information governance should determine the balance point between two potentially divergent organizational goals: extracting value from information and reducing the potential risk of information. Information governance reduces organizational risk in the fields of compliance, operational transparency, and reducing expenditures associated with e-discovery and litigation response. An organization can establish a consistent and logical framework for employees to handle data through their information governance policies and procedures. These policies guide proper behavior regarding how organizations and their employees handle electronically stored information ([[Electronically stored information (Federal Rules of Civil Procedure)|ESI]]).&lt;ref&gt;{{cite web|url=http://blogs.gartner.com/debra_logan/2010/01/11/what-is-information-governance-and-why-is-it-so-hard/|title=What is Information Governance? And Why is it So Hard? - Debra Logan|date=11 January 2010|publisher=}}&lt;/ref&gt;&lt;ref&gt;[Kooper, M., Maes, R., and Roos Lindgreen, E. (2011). On the governance of information: Introducing a new concept of governance to support the management of information. International Journal of Information Management, 31(3), 195-200]&lt;/ref&gt;

Information governance encompasses more than traditional [[records management]].  It incorporates [[information security]] and protection, compliance, [[data governance]], [[electronic discovery]], [[risk management]], privacy, data storage and archiving, [[knowledge management]], business operations and management, audit, analytics, IT management, [[master data management]], [[enterprise architecture]], [[business intelligence]], [[big data]], [[data science]], and finance.&lt;ref&gt;{{cite web|url=http://iginitiative.com/igi-publishes-2014-annual-report/|title=IGI PUBLISHES 2014 ANNUAL REPORT - Information Governance Initiative|date=11 August 2014|publisher=}}&lt;/ref&gt;

==History==

===Records management===
Records management deals with the creation, retention and storage and disposition of records.  A record can either be a physical, tangible object, or digital information such as a database, application data, and e-mail.  The [[records life-cycle|lifecycle]] was historically viewed as the point of creation to the eventual disposal of a record.  As data generation exploded in recent decades, and regulations and compliance issues increased, traditional records management failed to keep pace.  A more comprehensive platform for managing records and information became necessary to address all phases of the lifecycle, which led to the advent of information governance.&lt;ref&gt;http://www.arma.org/pdf/WhatIsRIM.pdf&lt;/ref&gt;

In 2003 the Department of Health in England introduced the concept of broad-based information governance into the National Health Service, publishing version 1 of an online performance assessment tool with supporting guidance. The NHS IG Toolkit&lt;ref&gt;{{cite web|url=https://www.igt.hscic.gov.uk/|title=Home|publisher=}}&lt;/ref&gt; is now used by over 30,000 NHS and partner organisations, supported by an e-learning platform with some 650,000 users.

In 2008, [[ARMA International]] introduced the Generally Accepted Recordkeeping Principles&#174;, or "The Principles"&lt;ref&gt;{{cite web|url=http://www.arma.org/principles|title=Generally Accepted Recordkeeping Principles|publisher=}}&lt;/ref&gt; and the subsequent "The Principles" Information Governance Maturity Model.&lt;ref&gt;http://www.arma.org/principles/metrics.cfm&lt;/ref&gt; "The Principles" identify the critical hallmarks of information governance. As such, they apply to all sizes of organizations, in all types of industries, and in both the private and public sectors. Multi-national organizations can also use "The Principles" to establish consistent practices across a variety of business units. ARMA International recognized that a clear statement of "Generally Accepted Recordkeeping Principles&#174;" ("The Principles") would guide:

* CEOs in determining how to protect their organizations in the use of information assets;
* Legislators in crafting legislation meant to hold organizations accountable; and
* Records management professionals in designing comprehensive and effective records management programs.

Information governance goes beyond retention and disposition to include privacy, access controls, and other compliance issues.  In electronic discovery, or e-discovery, relevant data in the form of [[electronically stored information]] is searched for by attorneys and placed on [[legal hold]].  IG includes consideration of how this data is held and controlled for e-discovery, and also provides a platform for defensible disposition and compliance.  Additionally, [[metadata]] often accompanies electronically stored data and can be of great value to the enterprise if stored and managed correctly.

With all of these additional considerations that go beyond traditional records management, IG emerged as a platform for organizations to define policies at the enterprise level, across multiple jurisdictions.  IG then also provides for the enforcement of these policies into the various repositories of information, data, and records.

A coalition of organizations known as Electronic Discovery Reference Model (EDRM), which was founded in 2005 to address issues related to electronic discovery and information governance, subsequently developed, as one of its projects, a resource called the Information Governance Reference Model (IGRM).&lt;ref&gt;{{cite web|author=EDRM|url=http://www.edrm.net/what-is-edrm|title=About EDRM|accessdate=2015-01-21}}&lt;/ref&gt; In 2011, EDRM, in collaboration with ARMA International, published a white paper that describes ''How the Information Governance Reference Model (IGRM) Complements ARMA International&#8217;s Generally Accepted Recordkeeping Principles ("The Principles")''&lt;ref&gt;{{cite book|last=White Paper|title=How the Information Governance Reference Model (IGRM)Complements ARMA International&#8217;s Generally Accepted Recordkeeping Principles|year=2011|publisher=EDRM and ARMA International|pages=15|url=http://www.edrm.net/wp-content/uploads/downloads/2011/12/White-Paper-EDRM-Information-Governance-Reference-Model-IGRM-and-ARMAs-GARP-Principles-12-7-2011.pdf|editor-last=Ledergerber|editor-first=Marcus}}&lt;/ref&gt;  The IGRM illustrates the relationship between key stakeholders and the Information Lifecycle and highlights the transparency required to enable effective governance IGRM v3.0 Update: Privacy &amp; Security Officers As Stakeholders.&lt;ref&gt;[http://www.edrm.net/download/all_projects/igrm/The-Final..-IGRM_v3.0Update-Whitepaper_Oct_2012.pdf IGRM v3.0 Update: Privacy &amp; Security Officers As Stakeholders]&lt;/ref&gt;

Universities and professional associations started to develop information governance training and education programmes. In 2010, Dr Elizabeth Lomas (who had been aligning RM with information security, assurance and risk management models throughout the 2000s) authored distance learning materials for Information Governance modules delivered internationally through Northumbria University. ARMA subsequently started to deliver an Information Governance certification. These initiatives have now been picked up by other Universities, e.g. San Jose State University offers a graduate certificate in information governance, information assurance, and cyber security, and has also incorporated a required course in information governance as part of their 100% online Master of Archives and Records Administration&lt;ref&gt;[http://ischool.sjsu.edu/programs/master-archives-records-administration-mara]]&lt;/ref&gt; (MARA) degree program.

In 2014, [[John Wiley &amp; Sons]] published the first textbook on information governance, "Information Governance: Concepts, Strategies, and Best Practices"&lt;ref&gt;{{cite book|url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118218302.html|title=Information Governance: Concepts, Strategies, and Best Practices|isbn=978-1-118-21830-3|date=April 2014|publisher=John Wiley &amp; Sons}}&lt;/ref&gt; by [[Robert Smallwood]]. Also in 2014, The Information Governance Conference,&lt;ref&gt;Information Governance Conference [http://www.infogovcon.com InfoGovCon.com]&lt;/ref&gt; an annual conference on information governance best practices began and the Information Governance Model&lt;ref&gt;Information Governance Model [http://www.infogovmodel.com InfoGovModel.com]&lt;/ref&gt; was launched at the inaugural event, it is now in use at over 1000 organizations worldwide.

===Organizational structure===
In the past, records managers owned records management, perhaps within a compliance department at an enterprise.  In order to address the broader issues surrounding records management, several other key stakeholders must be involved.  Legal, IT, and Compliance tend to be the departments that touch information governance the most, though certainly other departments might seek representation.  Many enterprises create information governance committees to ensure that all necessary constituents are represented and that all relevant issues are addressed.&lt;ref&gt;{{cite web|url=http://www.law.com/jsp/cc/PubArticleFriendlyCC.jsp?id=1202533945005|title=From the Experts: Information Governance and Its Impact on Litigation|publisher=}}&lt;/ref&gt;

===Tools===
To address retention and disposition, Records Management and Enterprise Content Management applications were developed.  Sometimes detached search engines or homegrown policy definition tools were created.  These were often employed at a departmental or divisional level; rarely were tools used across the enterprise.  While these tools were used to define policies, they lacked the ability to enforce those policies.  Monitoring for compliance with policies was increasingly challenging. Since information governance addresses so much more than traditional records management, several software solutions have emerged to include the vast array of issues facing records managers.

Other available tools include:
* ARMA International [[Www.arma.org/nextlevel|Next Level Information Governance Assessment]] ( Based upon the Generally Accepted Recordkeeping Principles)
* ARMA Generally Accepted Recordkeeping Principles&lt;ref&gt;ARMA International, [http://www.arma.org/r2/generally-accepted-br-recordkeeping-principles "The Principles"], ''ARMA International''&lt;/ref&gt;
* EDRM Information Governance Reference Model&lt;ref&gt;EDRM, [http://www.edrm.net/projects/igrm "Information Governance Reference Model"], ''EDRM''&lt;/ref&gt;
* Information Coalition Information Governance Model&lt;ref&gt;Information Coalition, [http://infocoalition.com/resources/models-methodologies/information-governance-model-infogovmodel "The Information Governance Model"], ''Information Coalition''&lt;/ref&gt;
* NHS Information Governance Toolkit&lt;ref&gt;NHS, [https://www.igt.hscic.gov.uk/ "NHS Information Governance Toolkit"], ''NHS''&lt;/ref&gt;

===Laws and regulations===
Key to IG are the regulations and laws that help to define corporate policies.  Some of these regulations include:
*The Foreign Account Tax Compliance Act, or [[Foreign Account Tax Compliance Act|FATCA]]&lt;ref&gt;{{cite web|url=http://www.irs.gov/businesses/corporations/article/0,,id=236667,00.html|title=Foreign Account Tax Compliance Act|publisher=}}&lt;/ref&gt;
*Payment Card Industry Data Security Standard, or [[Payment Card Industry Data Security Standard|PCI Compliance]]&lt;ref&gt;{{cite web|url=https://www.pcisecuritystandards.org/|title=Official PCI Security Standards Council Site - Verify PCI Compliance, Download Data Security and Credit Card Security Standards|publisher=}}&lt;/ref&gt;
*Health Insurance Portability and Accountability Act, or [[Health Insurance Portability and Accountability Act|HIPAA]]&lt;ref&gt;{{cite web|url=http://www.hhs.gov/hipaa/|title=Health Information Privacy|date=26 August 2015|publisher=}}&lt;/ref&gt;
*Financial Services Modernization Act of 1999, or [[Gramm&#8211;Leach&#8211;Bliley Act|GLBA]]&lt;ref&gt;{{cite web|url=https://www.congress.gov/bill/106th-congress/senate-bill/900|title=S.900 - Gramm-Leach-Bliley Act}}&lt;/ref&gt;
*Sarbanes&#8211;Oxley Act of 2002, or [[Sarbanes&#8211;Oxley|Sarbox or SOX]]&lt;ref&gt;{{cite web|url=https://www.sec.gov/about/laws/soa2002.pdf|title=Sarbanes&#8211;Oxley Act of 2002}}&lt;/ref&gt;
*[[Federal Rules of Civil Procedure]]

===Guidelines===
*[[MoReq2]]&lt;ref&gt;{{cite web|url=http://www.moreq2.eu/|title=Home - MoReq2|publisher=}}&lt;/ref&gt;
*MoReq2010&lt;ref&gt;{{cite web|url=http://moreq2010.eu/|title=Account Suspended|publisher=}}&lt;/ref&gt;
*[[ISO 15489 Information and documentation -- Records management|ISO 15489 Information and Documentation - Records Management]]&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=31908|title=ISO 15489-1:2001 - Information and documentation -- Records management -- Part 1: General|publisher=}}&lt;/ref&gt;
*DoD 5015.2, or [[Design Criteria Standard for Electronic Records Management Software Applications]]&lt;ref&gt;{{cite web|url=http://www.archives.gov/records-mgmt/initiatives/dod-standard-5015-2.html|title=DoD Standard 5015.2|publisher=}}&lt;/ref&gt;

==See also==
*[[Data defined storage]]
* [[Data governance]]
*[[Electronic discovery]]
*[[Enterprise content management]]
*[[Information management]]
*[[Information technology governance]]
*[[Knowledge management]]
*[[National archives]]
*[[Records management]]

==References==
{{reflist|30em}}

==External links==
* [http://www.epa.gov/records/what/quest1.htm EPA 10 Reasons for RM]
* [http://www.druva.com/resources/analyst-reports/governance-takes-central-role-enterprises-shift-to-mobile/]

[[Category:Information governance|*]]
[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Public records]]
[[Category:Data management]]</text>
      <sha1>03aoheqiswrw3e7vlnyfqs003oan6pu</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data mapping</title>
    <ns>14</ns>
    <id>34275960</id>
    <revision>
      <id>469506322</id>
      <timestamp>2012-01-04T13:38:11Z</timestamp>
      <contributor>
        <username>Danim</username>
        <id>14026077</id>
      </contributor>
      <comment>create</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="54" xml:space="preserve">{{catmain|Data mapping}}

[[Category:Data management]]</text>
      <sha1>pk0rhcmjsxd3drsxztdtsdy5r3rkotm</sha1>
    </revision>
  </page>
  <page>
    <title>DMAIC</title>
    <ns>0</ns>
    <id>3733991</id>
    <revision>
      <id>744294342</id>
      <parentid>741433737</parentid>
      <timestamp>2016-10-14T10:04:20Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Additional Steps */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5929" xml:space="preserve">{{sources|date=April 2012}}
'''DMAIC''' (an acronym for ''Define, Measure, Analyze, Improve and Control'') (pronounced ''d&#601;-MAY-ick'') refers to a data-driven improvement cycle used for improving, optimizing and stabilizing business processes and designs. The DMAIC improvement cycle is the core tool used to drive [[Six Sigma]] projects. However, DMAIC is not exclusive to Six Sigma and can be used as the framework for other improvement applications.

==Steps==
DMAIC is an abbreviation of the five improvement steps it comprises: Define, Measure, Analyze, Improve and Control. All of the DMAIC process steps are required and always proceed in the given order.
[[File:DMAICWebdingsII.png|thumbnail|right|400px|The five steps of DMAIC]]

===Define===
The purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline.  This information is typically captured within project charter document.  Write down what you currently know. Seek to clarify facts, set objectives and form the project team. Define the following:

* A problem 
* The customer(s)
* [[Voice of the customer]] (VOC) and  [[Critical to Quality]] (CTQs) &#8212; what are the critical process outputs?

===Measure===
The purpose of this step is to objectively establish current baselines as the basis for improvement.  This is a data collection step, the purpose of which is to establish process performance baselines.  The performance metric baseline(s) from the Measure phase will be compared to the performance metric at the conclusion of the project to determine objectively whether significant improvement has been made.  The team decides on what should be measured and how to measure it. It is usual for teams to invest a lot of effort into assessing the suitability of the proposed measurement systems. Good data is at the heart of the DMAIC process:

===Analyze===
The purpose of this step is to identify, validate and select root cause for elimination.  A large number of potential root causes (process inputs, X) of the project problem are identified via root cause analysis (for example a [[Ishikawa diagram|fishbone diagram]]).  The top 3-4 potential root causes are selected using multi-voting or other consensus tool for further validation.  A data collection plan is created and data are collected to establish the relative contribution of each root causes to the project metric, Y.  This process is repeated until "valid" root causes can be identified.  Within Six Sigma, often complex analysis tools are used. However, it is acceptable to use basic tools if these are appropriate.  Of the "validated" root causes, all or some can be

* List and prioritize potential causes of the problem
* Prioritize the root causes (key process inputs) to pursue in the Improve step
* Identify how the process inputs (Xs) affect the process outputs (Ys).  Data are analyzed to understand the magnitude of contribution of each root cause, X, to the project metric, Y.  Statistical tests using p-values accompanied by Histograms, Pareto charts, and line plots are often used to do this.
* Detailed process maps can be created to help pin-point where in the process the root causes reside, and what might be contributing to the occurrence.

===Improve===
The purpose of this step is to identify, test and implement a solution to the problem; in part or in whole. This depends on the situation. Identify creative solutions to eliminate the key root causes in order to fix and prevent process problems. Use brainstorming or techniques like [[Six Thinking Hats]] and [[Random stimulus|Random Word]]. Some projects can utilize complex analysis tools like DOE ([[Design of Experiments]]), but try to focus on obvious solutions if these are apparent. However, the purpose of this step can also be to find solutions without implementing them.

* Create
* Focus on the simplest and easiest solutions
* Test solutions using [[PDCA|Plan-Do-Check-Act]] (PDCA) cycle
* Based on PDCA results, attempt to anticipate any avoidable risks associated with the "improvement" using [[Failure mode and effects analysis|FMEA]]
* Create a detailed implementation plan
* Deploy improvements

===Control===
The purpose of this step is to sustain the gains.  Monitor the improvements to ensure continued and sustainable success. Create a control plan. Update documents, business process and training records as required.

A [[Control chart]] can be useful during the Control stage to assess the stability of the improvements over time by serving as 1. a guide to continue monitoring the process and 2. provide a response plan for each of the measures being monitored in case the process becomes unstable.

===Replicate and thank the teams===
This is additional to the standard DMAIC steps but it should be considered. Think about replicating the changes in other processes. Share your new knowledge within and outside of your organization.  It is very important to always provide positive morale support to team members in an effort to maximize the effectiveness of DMAIC.

Replicating the improvements, sharing your success and thanking your team members helps build buy-in for future DMAIC or improvement initiatives.

===Additional Steps===
Some organizations add a '''''R'''ecognize'' step at the beginning, which is to recognize the right problem to work on, thus yielding an RDMAIC methodology.&lt;ref name="WebberWallace2006p43"&gt;{{cite book | first1=Larry | last1=Webber | first2=Michael | last2=Wallace | title=Quality Control for Dummies | url=https://books.google.com/books?id=9BWkxto2fcEC&amp;pg=PA43 | accessdate=2012-05-16 | date=15 December 2006 | publisher=For Dummies | isbn=978-0-470-06909-7 | pages=42&#8211;43 }}&lt;/ref&gt;

==See also==
*[[Design for Six Sigma|DFSS]]
*[[Industrial engineering]]
*[[Kaizen]]
*[[PDCA]]
*[[Six Sigma]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Six Sigma]]</text>
      <sha1>4kru109yyc7n246jf8gczh735ctrvwy</sha1>
    </revision>
  </page>
  <page>
    <title>National Information Governance Board for Health and Social Care</title>
    <ns>0</ns>
    <id>34419120</id>
    <revision>
      <id>654169819</id>
      <parentid>653808128</parentid>
      <timestamp>2015-03-30T12:08:55Z</timestamp>
      <contributor>
        <username>PanchoS</username>
        <id>343908</id>
      </contributor>
      <comment>removed [[Category:Health informatics]]; added [[Category:Medical privacy]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6611" xml:space="preserve">The '''National Information Governance Board for Health and Social Care''' (NIGB) was established in the [[United Kingdom]] under section 157&lt;ref&gt;[http://www.legislation.gov.uk/ukpga/2008/14/section/157 Section 157 of the Health and Social Care Act 2008]&lt;/ref&gt; of the Health and Social Care Act 2008, with effect from October 2008, with a range of advisory functions relating to [[information governance]].  From January 2009, the NIGB also gained functions under section 251&lt;ref&gt;[http://www.legislation.gov.uk/ukpga/2006/41/section/251 Section 251 of the NHS Act 2006]&lt;/ref&gt; of the NHS Act 2006 which had previously been held by the [[Patient Information Advisory Group]] (PIAG) until its abolition. These functions were to advise the [[Secretary of State for Health]] on the use of powers to set aside the common law duty of confidentiality in [[England]] where identifiable patient information is needed and where consent is not practicable. From 1 April 2013, the NIGB's functions for monitoring and improving information governance practice have transferred to the [[Care Quality Commission]], which established a National Information Governance Committee to oversee this work. Functions relating to section 251 of the [[National Health Service Act 2006|NHS Act 2006]] (access to people&#8217;s personal and confidential information for research purposes) were transferred to the [[Health Research Authority]]'s Confidentiality Advisory Group.&lt;ref&gt;[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ Webarchive page of the National Information Governance Board for Health and Social Care]&lt;/ref&gt;

==Terms of reference==

The key functions of the NIGB (excerpted from the legislation) were:
&lt;ol type="a"&gt;
&lt;li&gt; to monitor the practice followed by relevant bodies in relation to the processing of relevant information;&lt;/li&gt;
&lt;li&gt; to keep the [[Secretary of State for Health]], and such bodies as the [[Secretary of State for Health]] may designate by direction, informed about the practice being followed by relevant bodies in relation to the processing of relevant information;&lt;/li&gt;
&lt;li&gt; to publish guidance on the practice to be followed in relation to the processing of relevant information;&lt;/li&gt;
&lt;li&gt; to advise the [[Secretary of State for Health]] on particular matters relating to the processing of relevant information by any person; and&lt;/li&gt;
&lt;li&gt; to advise persons who process relevant information on such matters relating to the processing of relevant information by them as the [[Secretary of State for Health]] may from time to time designate by direction.&lt;/li&gt;
&lt;/ol&gt;

The definition of &#8220;relevant information&#8221; in the legislation covers patient information, any other information obtained or generated in the course of the provision of the health service, and any information obtained or generated in the course of the exercise by a local social services authority in [[England]] of its adult social services functions.

==Ethics and Confidentiality Committee==

Some areas of NIGB functions (d) and (e) above had been delegated to the NIGB&#8217;s Ethics and Confidentiality Committee (ECC).  These functions primarily related to applications to use identifiable patient information without consent, in specific circumstances within the bounds of section 251 of the NHS Act 2006.  These applications, which had been considered by PIAG before the NIGB, passed on to the [[Health Research Authority]]'s Confidentiality Advisory Group (CAG) on 1 April 2013.

==Care Record Development Board==

The NIGB had also replaced the Care Record Development Board (CRDB),&lt;ref&gt;[http://www.connectingforhealth.nhs.uk/crdb Care Record Development Board archive page hosted by NHS Connecting for Health]&lt;/ref&gt; which had closed in September 2007. The NIGB had subsequently maintained the NHS Care Record Guarantee which was originally developed by the CRDB and developed a companion Social Care Record Guarantee.

==Members==

The NIGB had consisted of a Chair, a number of Public Members appointed by the NHS Appointments Commission, and a number of Representative Members appointed by the [[Secretary of State for Health]] from a range of stakeholder organisations.  Representatives of several other stakeholder organisations had served as Corresponding Advisers to the NIGB but had not typically attended meetings.  Regular observers at meetings had included representatives from the [[Information Commissioner's Office]] and the devolved UK administrations.

The ECC had consisted of a Chair and a number of Members, all of whom had been appointed by the NIGB with advice from an NHS Appointments Commission approved independent assessor.  The ECC Chair and two ECC Members had also been NIGB Members.

Between 1 June 2011 and 31 March 2013 Dame [[Fiona Caldicott]]&lt;ref&gt;[http://www.connectingforhealth.nhs.uk/newsroom/news/nigbchair Appointment of Fiona Caldicott as new NIGB Chair - press release on NHS Connecting for Health website, June 2011]&lt;/ref&gt; had been Chair of the NIGB, succeeding [[Harry Cayton]] who had chaired the NIGB since its inception.

==Geography==

Members of the NIGB and ECC had been widely distributed nationally but had attended meetings at the NIGB office.  Since September 2011, this had been based at [[Skipton House]], London SE1.  The NIGB&#8217;s staff team had been predominantly based at this office.

==Abolition==

As a result of the [[Health and Social Care Act 2012]] the NIGB was abolished with effect from 1 April 2013. The functions delegated to the ECC with respect to research transferred to the [[Health Research Authority]].&lt;ref&gt;[http://www.hra.nhs.uk/news/2012/12/17/further-update-on-transfer-of-s251-function-from-nigb-to-hra/ Transfer of s251 function from NIGB to HRA]&lt;/ref&gt; The [[NHS Commissioning Board]] is now responsible for providing advice and guidance to NHS bodies. Other functions were transferred to the National Information Governance Committee hosted by the [[Care Quality Commission]].

==References==

{{Reflist}}

==External links==
*[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ NIGB website (archived)]
*[http://www.hra.nhs.uk/ NHS Health Research Authority]
*[http://www.cqc.org.uk/ Care Quality Commission]
*[http://www.nres.nhs.uk National Research Ethics Service]
*[http://www.commissioningboard.nhs.uk/ NHS Commissioning Board]


[[Category:Data management]]
[[Category:Medical privacy]]
[[Category:2008 establishments in the United Kingdom]]
[[Category:Organizations established in 2008]]
[[Category:Governance in the United Kingdom]]
[[Category:Social care in the United Kingdom]]</text>
      <sha1>syiywe3ai2sjykaj74ia2rp6zc0qg0n</sha1>
    </revision>
  </page>
  <page>
    <title>NewSQL</title>
    <ns>0</ns>
    <id>37256799</id>
    <revision>
      <id>752623030</id>
      <parentid>751922209</parentid>
      <timestamp>2016-12-02T09:10:38Z</timestamp>
      <contributor>
        <username>Queenypingcap</username>
        <id>29541323</id>
      </contributor>
      <comment>/* New architectures */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7702" xml:space="preserve">'''NewSQL''' is a class of modern [[relational database management system|relational]] [[database management system]]s that seek to provide the same scalable performance of [[NoSQL]] systems for [[online transaction processing]] (OLTP) read-write workloads while still maintaining the [[ACID]] guarantees of a traditional database system.&lt;ref name="aslett2012"&gt;
{{cite web 
| url = http://cs.brown.edu/courses/cs227/archives/2012/papers/newsql/aslett-newsql.pdf
| title = How Will The Database Incumbents Respond To NoSQL And NewSQL?
| first = Matthew
| last = Aslett
| publisher = 451 Group
| publication-date = 2011-04-04
| year = 2011
| accessdate = 2012-07-06
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext
| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps
| first = Michael
| last = Stonebraker
| publisher = Communications of the ACM Blog
| publication-date = 2011-06-16
| accessdate  = 2012-07-06
}}
&lt;/ref&gt;&lt;ref name="highscalability"&gt;
{{cite web 
| url = http://highscalability.com/blog/2012/9/24/google-spanners-most-surprising-revelation-nosql-is-out-and.html
| title = Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In
| first = Todd
| last = Hoff
| publication-date = 2012-09-24
| accessdate  = 2012-10-07
}}
&lt;/ref&gt;

== History ==
The term was first used by 451 Group analyst Matthew Aslett in a 2011 research paper discussing the rise of new database systems as challengers to established vendors.&lt;ref name="aslett2010" /&gt; Many enterprise systems that handle high-profile data (e.g., financial and order processing systems) also need to be able to scale but are unable to use NoSQL solutions because they cannot give up strong transactional and consistency requirements.&lt;ref name="aslett2010"&gt;{{cite web 
| url = http://blogs.the451group.com/information_management/2011/04/06/what-we-talk-about-when-we-talk-about-newsql/
| title = What we talk about when we talk about NewSQL
| first = Matthew
| last = Aslett
| publisher = 451 Group
| publication-date = 2011-04-06
| year = 2010
| accessdate = 2012-10-07
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://berlinbuzzwords.de/sessions/keynote-0
| title = Building Spanner
| first = Alex
| last = Lloyd
| publisher = Berlin Buzzwords
| publication-date = 2012-06-05
| year = 2012
| accessdate = 2012-10-07
}}&lt;/ref&gt; The only options previously available for these organizations were to either purchase a more powerful single-node machine or develop custom middleware that distributes queries over traditional DBMS nodes. Both approaches are prohibitively expensive and thus are not an option for many. Thus, in this paper, Aslett discusses how NewSQL upstarts are poised to challenge the supremacy of commercial vendors, in particular [[Oracle Database|Oracle]].

== Systems ==
Although NewSQL systems vary greatly in their internal architectures, the two distinguishing features common amongst them is that they all support the [[Relational model|relational data model]] and use [[SQL]] as their primary interface.&lt;ref&gt;{{Cite journal | last1 = Cattell | first1 = R. | title = Scalable SQL and NoSQL data stores | doi = 10.1145/1978915.1978919 | journal = ACM SIGMOD Record | volume = 39 | issue = 4 | pages = 12 | year = 2011 | url = http://cattell.net/datastores/Datastores.pdf| pmid =  | pmc = }}&lt;/ref&gt;
The applications targeted by these NewSQL systems are characterized as having a large number of transactions that (1) are short-lived (i.e., no user stalls), (2) touch a small subset of data using index lookups (i.e., no full table scans or large distributed joins), and (3) are repetitive (i.e. executing the same queries with different inputs).&lt;ref&gt;
{{cite conference
| authorlink = Michael Stonebraker
| first = Mike | last = Stonebraker
| title = The end of an architectural era: (it's time for a complete rewrite
| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases
| location = Vienna, Austria
| year = 2007
| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf
| format = PDF |display-authors=etal}}&lt;/ref&gt; These NewSQL systems achieve high performance and scalability by eschewing much of the legacy architecture of the original [[IBM System R]] design, such as heavyweight [[Algorithms for Recovery and Isolation Exploiting Semantics|recovery]] or [[concurrency control]] algorithms.&lt;ref&gt;{{Cite journal | last1 = Stonebraker | first1 = M. | last2 = Cattell | first2 = R. | doi = 10.1145/1953122.1953144 | title = 10 rules for scalable performance in 'simple operation' datastores | journal = Communications of the ACM | volume = 54 | issue = 6 | pages = 72 | year = 2011 | pmid =  | pmc = }}&lt;/ref&gt; One of the first known NewSQL systems is the [[H-Store]] [[Parallel database|parallel database system]].&lt;ref&gt;
{{cite web 
| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/
| title = Is H-Store the future of database management systems?
| first = Matthew
| last = Aslett
| year = 2008
| publication-date = 2008-03-04
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055
| title = H-Store: Complete destruction of the old DBMS order?
| first = Larry
| last = Dignan
| year = 2008
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

NewSQL systems can be loosely grouped into three categories:
&lt;ref&gt;
{{cite web 
| url = http://www.linuxforu.com/2012/01/newsql-handle-big-data/
| title = NewSQL - The New Way to Handle Big Data
| first =  Prasanna
| last = Venkatesh
| year = 2012
| publication-date = 2012-01-30
| accessdate  = 2012-10-07
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.scalebase.com/the-story-of-newsql/
| title = The NewSQL Market Breakdown
| first = Doron
| last = Levari
| year = 2011
| accessdate  = 2012-04-08
}}
&lt;/ref&gt;

=== New architectures ===
The first type of NewSQL systems are completely new database platforms. These are designed to operate in a distributed cluster of [[Shared nothing architecture|shared-nothing]] nodes, in which each node owns a subset of the data. These databases are often written from scratch with a distributed architecture in mind, and include components such as distributed concurrency control, flow control, and distributed query processing. Example systems in this category are [[Google Spanner]], [[Clustrix]], [[VoltDB]], [[MemSQL]], [[Pivotal Labs|Pivotal]]'s GemFire XD, [[SAP HANA]],&lt;ref&gt;{{cite web|title=SAP HANA|url=http://www.sap.com/pc/tech/data-management/software/extreme-transaction-oltp/index.html|publisher=SAP|accessdate=17 July 2014}}&lt;/ref&gt; [[NuoDB]], [[TiDB]], and [[Trafodion]].&lt;ref&gt;
{{cite web 
| url = http://www.trafodion.org
| title = Trafodion: Transactional SQL-on-HBase
| year = 2014
}}
&lt;/ref&gt;

=== SQL engines ===
The second category are highly optimized [[Database engine|storage engines]] for [[SQL]]. These systems provide the same programming interface as SQL, but scale better than built-in engines, such as [[InnoDB]]. Examples of these new storage engines include [[MySQL Cluster]], [[Infobright]], [[TokuDB]] and the now defunct [[InfiniDB]].

=== Transparent sharding ===
These systems provide a [[Shard (database architecture)|sharding]] [[middleware]] layer to automatically split databases across multiple nodes. [[ScaleBase]] is an example of this type of system.

==See also==
* [[Transaction processing]]
* [[Partition (database)]]

== References ==
{{Reflist|30em}}

{{Databases}}

&lt;!--Categories--&gt;
[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:NewSQL]]</text>
      <sha1>50i33yp6ocxgtqyr9rrz0velbwf4x32</sha1>
    </revision>
  </page>
  <page>
    <title>Category:NewSQL</title>
    <ns>14</ns>
    <id>37256859</id>
    <revision>
      <id>516566229</id>
      <timestamp>2012-10-08T02:19:05Z</timestamp>
      <contributor>
        <username>Apavlo</username>
        <id>1709507</id>
      </contributor>
      <comment>Creating category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="73" xml:space="preserve">{{Cat main|NewSQL}}

[[Category:Databases]] 
[[Category:Data management]]</text>
      <sha1>t8at130tz96tqtxji3yh5gulgbjq3a8</sha1>
    </revision>
  </page>
  <page>
    <title>Data access</title>
    <ns>0</ns>
    <id>1582494</id>
    <revision>
      <id>703967160</id>
      <parentid>697159929</parentid>
      <timestamp>2016-02-08T19:02:20Z</timestamp>
      <contributor>
        <username>Roman.korpachyov</username>
        <id>20212030</id>
      </contributor>
      <minor />
      <comment>Undid revision 682421722 by [[Special:Contributions/89.41.154.245|89.41.154.245]] ([[User talk:89.41.154.245|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2194" xml:space="preserve">{{Refimprove|date=September 2014}}
'''Data access''' typically refers to software and activities related to storing, retrieving, or acting on [[data]] housed in a [[database]] or other [[Information repository|repository]]. Two fundamental types of data access exist:

# [[sequential access]] (as in [[Magnetic tape data storage|magnetic tape]], for example)
# [[random access]] (as in indexed [[Digital media|media]])

Data access crucially involves [[authorization]] to access different data repositories. Data access can help distinguish the abilities of administrators and users. For example, administrators may have the ability to remove, edit and add data, while general users may not even have "read" rights if they lack access to particular information.

Historically, each repository (including each different database, [[file system]], etc.), might require the use of different [[Method (computer science)|methods]] and [[languages]], and many of these repositories stored their content in different and incompatible formats.

Over the years standardized languages, methods, and formats, have developed to serve as interfaces between the often proprietary, and always idiosyncratic, specific languages and methods.  Such standards include [[SQL]] (1974- ), [[ODBC]] (ca 1990- ), [[JDBC]], [[XQuery API for Java|XQJ]], [[ADO.NET]], [[XML]], [[XQuery]], [[XPath]] (1999- ), and [[Web Services]].

Some of these standards enable translation of data from [[unstructured data|unstructured]] (such as HTML or free-text files) to [[structured data|structured]] (such as [[XML]] or [[SQL]]).

Structures such as [[connection string]]s and DBURLs&lt;ref&gt;
{{cite web
| url           = http://www.quickprogrammingtips.com/java/connecting-to-oracle-database-in-java.html
| title         = Connecting to Oracle Database in Java
| accessdate    = 2014-07-18
| quote         = DBURL is of the form [...] jdbc:oracle:thin:@machinename:1521:databasename [...]
}}
&lt;/ref&gt;
can attempt to standardise methods of [[Database connection|connecting to databases]].

== References ==
{{reflist}}

{{DEFAULTSORT:Data Access}}
[[Category:Data management]]
[[Category:Data access technologies| ]]


{{Database-stub}}</text>
      <sha1>rr2m2guq8q1xcgug8tjnboi0mp40gxr</sha1>
    </revision>
  </page>
  <page>
    <title>PL/Perl</title>
    <ns>0</ns>
    <id>2537690</id>
    <revision>
      <id>588743260</id>
      <parentid>568227670</parentid>
      <timestamp>2014-01-02T00:56:19Z</timestamp>
      <contributor>
        <username>Chirlu</username>
        <id>55863</id>
      </contributor>
      <comment>Improve links for database terms</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1731" xml:space="preserve">'''PL/Perl (Procedural Language/Perl)''' is a procedural language supported by the [[PostgreSQL]] [[RDBMS]].

PL/Perl, as an [[imperative programming language]], allows more control than the [[relational algebra]] of [[SQL]].
Programs created in the PL/Perl language are called functions and can use most of the features that the [[Perl|Perl programming language]] provides, including common flow control structures and syntax that has incorporated [[regular expressions]] directly.
These functions can be evaluated as part of a SQL statement, or in response to a [[Database trigger|trigger]] or [[Constraint (database)|rule]].

The design goals of PL/Perl were to create a loadable procedural language that:

* can be used to create functions and trigger procedures,
* adds control structures to the SQL language,
* can perform complex computations,
* can be defined to be either [http://www.postgresql.org/docs/current/static/plperl-trusted.html trusted or untrusted] by the server,
* is easy to use.

PL/Perl is one of many "PL" languages available for PostgreSQL
[[PL/pgSQL]]
[http://gborg.postgresql.org/project/pljava/projdisplay.php PL/Java], 
[http://plphp.commandprompt.com/ plPHP], 
[http://www.postgresql.org/docs/current/interactive/plpython.html PL/Python], 
[http://www.joeconway.com/plr/ PL/R], 
[http://raa.ruby-lang.org/list.rhtml?name=pl-ruby PL/Ruby], 
[http://plsh.projects.postgresql.org/ PL/sh], 
and [http://www.postgresql.org/docs/current/interactive/pltcl.html PL/Tcl].

==References==
* [http://www.postgresql.org/docs/current/static/plperl.html PostgreSQL PL/Perl documentation]

{{DEFAULTSORT:PL Perl}}
[[Category:Data management]]
[[Category:PostgreSQL]]
[[Category:Data-centric programming languages]]</text>
      <sha1>fx057wzvptc3gu3w8ubmbtvi2bq0yzj</sha1>
    </revision>
  </page>
  <page>
    <title>XML database</title>
    <ns>0</ns>
    <id>1442351</id>
    <revision>
      <id>757893342</id>
      <parentid>745145991</parentid>
      <timestamp>2017-01-02T09:30:12Z</timestamp>
      <contributor>
        <ip>2600:100C:B204:22AB:99C3:C902:5C54:31B1</ip>
      </contributor>
      <comment>grammar</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11211" xml:space="preserve">{{multiple issues|
{{refimprove|date=August 2011}}
{{update|date=March 2015}}
}}

An '''XML database''' is a [[data persistence]] software system that allows data to be specified, and sometimes stored, in [[XML]] format. This data can be [[XQuery|queried]], transformed, exported and returned to a calling system. XML databases are a flavor of [[document-oriented database]]s which are in turn a category of [[NoSQL]] database.

== Rationale for XML in databases ==
There are a number of reasons to directly specify data in XML or other document formats such as JSON. For XML in particular, they include:&lt;ref name=nicola2010&gt;{{cite web|last1=Nicola|first1=Matthias|title=5 Reasons for Storing XML in a Database|url=http://nativexmldatabase.com/2010/09/28/5-reasons-for-storing-xml-in-a-database/|website=Native XML Database|accessdate=17 March 2015|date=28 September 2010}}&lt;/ref&gt;
&lt;ref name=feldman2013&gt;{{cite conference|last1=Feldman|first1=Damon|title=Moving from Relational Modeling to XML and MarkLogic Data Models|url=http://www.marklogic.com/resources/slides-moving-from-relational-modeling-to-xml-and-marklogic-data-models/resource_download/presentations/|conference=MarkLogic World|conferenceurl=http://world.marklogic.com/|date=11 April 2013|accessdate=17 March 2015}}&lt;/ref&gt;
* An enterprise may have a lot of XML in an existing standard format
* Data may need to be exposed or ingested as XML, so using another format such as relational forces double-modeling of the data
* XML is very well suited to sparse data, deeply nested data and mixed content (such as text with embedded markup tags)
* XML is human readable whereas relational tables require expertise to access
* Metadata is often available as XML
* Semantic web data is available as RDF/XML

Steve O'Connell gives one reason for the use of XML in databases: the increasingly common use of XML for [[transport layer|data transport]], which has meant that "data is extracted from databases and put into XML documents and vice-versa".&lt;ref name=oconnell2005&gt;{{cite report|author=O'Connell, Steve|work=Advanced Databases Course Notes|title="Section 9.2"|type=Syllabus|date=2005|publisher=[[University of Southampton]]|location=Southampton, England}}&lt;/ref&gt;{{update inline|date=March 2015}} It may prove more efficient (in terms of conversion costs) and easier to store the data in XML format.  In content-based applications, the ability of the native XML database also minimizes the need for extraction or entry of metadata to support searching and navigation.

== XML Enabled databases ==
XML enabled databases typically offer one or more of the following approaches to storing XML within the traditional relational structure:
#XML is stored into a CLOB ([[Character large object]])
#XML is `shredded` into a series of Tables based on a Schema&lt;ref name=oracle&gt;{{cite book|title=Oracle XML DB Developer's Guide, 10''g'' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm|accessdate=17 March 2015|chapter=XML Schema Storage and Query: Basic}}. Section [http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm#i1042421 Creating XMLType Tables and Columns Based on XML Schema]&lt;/ref&gt;
#XML is stored into a native XML Type as defined by ISO Standard 9075-14&lt;ref name=iso9075-2011&gt;{{cite web|title=ISO/IEC 9075-14:2011: Information technology -- Database languages -- SQL -- Part 14: XML-Related Specifications (SQL/XML)|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=53686|publisher=[[International Organization for Standardization]]|accessdate=17 March 2015|date=2011}}&lt;/ref&gt;

RDBMS that support the ISO XML Type are:
#IBM DB2 (pureXML&lt;ref name=db2purexml&gt;{{cite web|title=pureXML overview -- DB2 as an XML database|url=http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.1.0/com.ibm.db2.luw.xml.doc/doc/c0022308.html|website=IBM Knowledge Center|publisher=[[IBM]]|accessdate=17 March 2015}}&lt;/ref&gt;)
#Microsoft SQL Server&lt;ref name=sqlserver2005&gt;{{cite web|title=Using XML in SQL Server|url=https://msdn.microsoft.com/en-us/library/ms190936.aspx|website=Microsoft Developer Network|publisher=[[Microsoft Corporation]]|accessdate=17 March 2015}}&lt;/ref&gt;
#Oracle Database&lt;ref name=oracle2&gt;{{cite book|title=Oracle XML DB Developer's Guide, 10''g'' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb04cre.htm|accessdate=17 March 2015|chapter=XMLType Operations}}&lt;/ref&gt;
#PostgreSQL&lt;ref name=postgresql&gt;{{cite book|title=PostgreSQL 9.0.19 Documentation|chapter-url=http://www.postgresql.org/docs/9.0/static/datatype-xml.html|accessdate=17 March 2015|chapter=8.13. XML Type}}&lt;/ref&gt; &lt;ref&gt;[http://www.postgresql.org/docs/9.0/static/datatype-xml.html PostgreSQL - Data Types - XML Type]&lt;/ref&gt;

Typically an XML enabled database is best suited where the majority of data are non-XML. For datasets where the majority of data are XML, a [[#Native XML databases|native XML database]] is better suited.

=== Example of XML Type Query in IBM DB2 SQL ===
&lt;source lang="sql"&gt;
select
   id, vol, xmlquery('$j/name', passing journal as "j") as name
from
   journals
where 
   xmlexists('$j[licence="CreativeCommons"]', passing journal as "j")
&lt;/source&gt;

== Native XML databases ==
These databases are typically better when much of the data is in XML or other non-relational formats.{{fact|date=August 2015}}
* [[BaseX]]
* [[Berkeley DB]] XML Edition 
* [[eXist]]
* [[MarkLogic Server]]
* [[Qizx]]
* [[Sedna (database)|Sedna]]

All the above databases uses XML as an interface to specify documents as tree structured data that may contain unstructured text, but on disk the data is stored as "optimized binary files." This makes query and retrieval faster. For MarkLogic it also allows XML and JSON to co-exist in one binary format.&lt;ref&gt;{{cite book|last1=Siegel|first1=Erik|last2=Retter|first2=Adam|title=eXist|date=December 2014|publisher=O'Reilly &amp; Associates|isbn=978-1-4493-3710-0|url=https://www.safaribooksonline.com/library/view/exist/9781449337094/ch04.html|accessdate=18 March 2015|chapter=4. Architecture}}&lt;/ref&gt;

Key features of native XML databases include:

* Has an [[XML]] document as at least one fundamental unit of (logical) storage, just as a [[relational database]] has a row in a table as a fundamental unit of (logical) storage.
* Need not have any particular underlying physical storage model. For example, NXDs can use optimized, proprietary storage formats. This is a key aspect of XML databases. Managing XML as large strings is inefficient due to the extra markup in XML. Compressing and indexing XML allows the illusion of directly accessing, querying and transforming XML while gaining the performance advantages of working with optimized binary tree structures.&lt;ref name=kellogg2010&gt;{{cite web|last1=Kellogg|first1=Dave|title=Yes, Virginia, MarkLogic is a NoSQL System|url=http://kellblog.com/2010/04/11/yes-virginia-marklogic-is-a-nosql-system/|website=Kellblog|accessdate=18 March 2015|date=11 April 2010}}&lt;/ref&gt;

The standards for XML querying per W3C recommendation are [[XQuery]] 1.0 and XQuery 3.0.{{citation needed|date=March 2015}} XQuery includes [[XPath]] as a sub-language and XML itself is a valid sub-syntax of XQuery.

In addition to XPath, XML databases support [[XSLT]] as a method of transforming documents or query-results retrieved from the database. XSLT provides a [[declarative language]] written using an XML grammar. It aims to define a set of XPath [[Filter (software)|filter]]s that can transform documents (in part or in whole) into other formats including [[plain text]], XML, or [[HTML]].

But big picture, XML persistence describes only one format in the larger, faster moving [[NoSQL]] movement at this time. Many databases support XML plus other formats, even if XML is internally stored as an optimized, high-performance format and is a first-class citizen within the database. (see Google Trends Link above to see relative popularity of terms).

=== Language features  ===
{| class="wikitable sortable"
|-
! Name
! License
! Native Language
! XQuery 3.0
! XQuery Update
! XQuery Full Text
! EXPath Extensions
! EXQuery Extensions
! XSLT 2.0
|-
| BaseX
| [[BSD License]]
| Java
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| eXist
| [[LGPL|LGPL License]]
| Java
| {{Partial}} || {{Proprietary}} || {{Proprietary}} ||  {{No}} || {{Yes}} || {{Yes}}
|-
| MarkLogic Server
| [[Commercial software|Commercial]]
| C++
| {{Partial}} ||  {{Proprietary}} || {{Proprietary}} || {{No}}  || {{No}} || {{Yes}}
|-
| Qizx
| [[Commercial software|Commercial]]
| Java
| {{Yes}} || {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{Yes}}
|}

=== Supported APIs ===
{| class="wikitable sortable"
|-
! Name
! [[XQuery API for Java|XQJ]]
! XML:DB
! [[Representational State Transfer|RESTful]]
! RESTXQ
! WebDAV
|-
| BaseX
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| eXist
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| MarkLogic Server
| {{Yes}} || {{No}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| Qizx
| {{No}} || {{No}} || {{Yes}} || {{No}} || {{No}}
|-
| Sedna
| {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{No}}
|}

== References ==
{{reflist}}
{{Refbegin}}

== External links ==
* [https://web.archive.org/web/20150906171257/http://db-engines.com/en/ranking/native+xml+dbms DB-Engines Ranking of Native XML DBMS] by popularity, updated monthly
* [http://www.cfoster.net/articles/xmldb-business-case XML Databases - The Business Case, Charles Foster, June 2008] - Talks about the current state of Databases and data persistence, how the current Relational Database model is starting to crack at the seams and gives an insight into a strong alternative for today's requirements.
* [http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-3717 An XML-based Database of Molecular Pathways (2005-06-02)] Speed / Performance comparisons of eXist, X-Hive, Sedna and Qizx/open
* [https://web.archive.org/web/20070922082133/http://swing.felk.cvut.cz/index.php?option=com_docman&amp;task=doc_view&amp;gid=5&amp;Itemid=62 XML Native Database Systems: Review of Sedna, Ozone, NeoCoreXMS] 2006
* [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&amp;toc=comp/mags/ic/2005/02/w2toc.xml&amp;DOI=10.1109/MIC.2005.48 XML Data Stores: Emerging Practices]
* Bhargava, P.; Rajamani, H.; Thaker, S.; Agarwal, A. (2005) ''XML Enabled Relational Databases'', Texas, The University of Texas at Austin.
* [https://web.archive.org/web/20070113224941/http://xmldb-org.sourceforge.net/ Initiative for XML Databases]
* [http://www.rpbourret.com/xml/XMLAndDatabases.htm  XML and Databases, Ronald Bourret, September 2005]
* [https://web.archive.org/web/20071011101718/http://cafe.elharo.com/xml/the-state-of-native-xml-databases/  The State of Native XML Databases, Elliotte Rusty Harold, August 13, 2007]
* {{Official website|https://www.qualcomm.com/qizx|Qualcomm Qizx official website}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
{{Refend}}

{{Database models}}
{{Databases}}

[[Category:XML]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:XML databases| ]]</text>
      <sha1>oibm0ruo4cwpyvkz8vxmq00955tkcsv</sha1>
    </revision>
  </page>
  <page>
    <title>EU Open Data Portal</title>
    <ns>0</ns>
    <id>38138335</id>
    <revision>
      <id>725255762</id>
      <parentid>723334317</parentid>
      <timestamp>2016-06-14T14:42:01Z</timestamp>
      <contributor>
        <ip>158.169.150.5</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6045" xml:space="preserve">{{Infobox Website
| logo = |250px
| url = {{URL|http://data.europa.eu/euodp/}}
| commercial = No
| type = [[public services|Public service]] [[Web portal|portal]] and &lt;br /&gt;institutional information
| registration = Not required
| language = 24 official languages of the EU 
| owner = {{Flag|European Union}}
| content license = Open
| author = [[Publications Office (European Union)|EU Publications Office]]
| launch date = December 2012
}}

The '''EU Open Data Portal''' is the single point of access to a wide range of data held by EU institutions, agencies and other bodies. The portal is a key element of EU open data strategy.

== Legal basis and launch date ==

Launched in December 2012 in beta mode, the portal was formally established by Commission Decision of 12 December 2011 (2011/833/EU) on the reuse of Commission documents to promote accessibility and reuse.&lt;ref name="r1"&gt;{{cite news|url=http://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:32011D0833|title=Commission Decision of 12 December 2011 (2011/833/EU)}}&lt;/ref&gt; 

While the operational management of the portal is the task of the [[Publications Office of the European Union]], implementation of EU open data policy is the responsibility of the [[Directorate General for Communications Networks, Content and Technology]] of the European Commission.

== Features ==

The portal allows anyone to easily search, explore, link, download and reuse the data for commercial or non-commercial purposes, through a catalogue of common metadata. Through this catalogue, users access data stored on the websites of the EU institutions, agencies and other bodies.

Semantic technologies offer new functionalities. The metadata catalogue can be searched via an interactive search engine (Data tab) and through [[SPARQL]] queries (Linked data tab). There is also a showcase of visualisation applications from various EU institutions, agencies and other bodies.

Users can suggest data they would like the portal to be linked to, give feedback on the quality of data obtainable and share information with other users about how they have used it.

The interface is in 24 EU official languages, while most [[metadata]] are currently available in a limited number of languages (English, French and German). Some of the metadata (e.g. names of the data providers, geographical coverage) are in 24 languages following the translation of [[controlled vocabulary]] lists that are used by the portal.&lt;ref name="r3"&gt;{{cite news|url=http://publications.europa.eu/mdr/authority/index.html|title=EU controlled vocabularies}}&lt;/ref&gt;

== Terms of reuse ==

Most data accessible via the EU Open Data Portal are covered by the Europa Legal Notice &lt;ref name="r6"&gt;{{cite news|url=http://ec.europa.eu/geninfo/legal_notices_en.htm|title=Europa Legal Notice}}&lt;/ref&gt; and can be reused free of charge, for commercial and non-commercial purposes, provided that the source is acknowledged. Specific conditions on reuse, related mostly to the protection of third-party intellectual property rights, apply for a very limited amount of data.

== Data available ==

The portal contains a very wide variety of high-value open data across EU policy domains, as also more recently identified by the G8 Open Data Charter. These include the economy, employment, science, environment and education. The number of data providers &#8212; which include [[Eurostat]], the [[European Environment Agency]] and the [[Joint Research Centre]] &#8212; continues to grow.

So far, around 56 EU institutions, bodies or departments (e.g. Eurostat, the European Environment Agency, the Joint Research Centre and other European Commission Directorates General and EU Agencies) have made datasets available, making a total of over 7,800.

In addition to giving access to datasets, the portal also is an easy entry point to a whole range of visualisation applications using EU data. The applications are displayed as much for their information value as for giving examples of what applications can be made using the data.

== Architecture of the portal ==

The portal is built using open source solutions such as the [[Drupal]] content management system and [[CKAN]], the data catalogue software developed by the [[Open Knowledge Foundation]]. It uses Virtuoso as an [[Resource Description Framework|RDF]] database and has a [[SPARQL]] endpoint.

Its metadata catalogue is built on the basis of international standards such as [[Dublin Core]], the data catalogue vocabulary DCAT and the asset description metadata schema [[ADMS]].&lt;ref name="r5"&gt;{{cite news|url=http://ec.europa.eu/digital-agenda/en/open-data-portals|title=Open Data Portals in Europe}}&lt;/ref&gt;

==See also==

*[[Open data]]
*[[Institutions of the European Union]] 
*[[Agencies of the European Union]]
*[[Bodies of the European Union]]
*[[European Data Portal]]

==References==
{{reflist}}

==External links==
* [http://ec.europa.eu/europe2020/index_en.htm Europe 2020 &#8211; Official EU Site]
* [http://ec.europa.eu/digital-agenda/ Digital Agenda for Europe]
* [http://ec.europa.eu/digital-agenda/en/open-data-0 Open Data section of above site]
* [https://joinup.ec.europa.eu/community/ods/description Joinup community on EU open data]
* [http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52011DC0882 Communication &#8216;Open data &#8212; An engine for innovation, growth and transparent governance&#8217;]
* [https://ec.europa.eu/digital-agenda/en/legislative-measures Legal rules on public services information]
* [http://okfn.org/ Open Knowledge Foundation]
* [http://dublincore.org/ Dublin Core]
* [http://latc-project.eu/ Publication and usage of linked data on the Web]
* [http://datacatalogs.org/group/eu-official Data catalogues]
* [http://5stardata.info/ Open data classification by [[Tim Berners Lee]]] 
* [http://opendatachallenge.org/ Open Data Challenge (now over)]

[[Category:European Commission]]
[[Category:Open data]]
[[Category:Transparency (behavior)]]
[[Category:Open government]]
[[Category:Semantic Web]]
[[Category:Data management]]
[[Category:Creative Commons]]</text>
      <sha1>lzt69kb0mi6wqgg3jtfdl5iox0h7cue</sha1>
    </revision>
  </page>
  <page>
    <title>Approximate inference</title>
    <ns>0</ns>
    <id>39019965</id>
    <revision>
      <id>603934296</id>
      <parentid>564359964</parentid>
      <timestamp>2014-04-12T22:03:41Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>fixed [[Help:CS1 errors#bad_date|CS1 errors: dates]] to meet [[MOS:DATEFORMAT]] (also [[WP:AWB/GF|General fixes]]) using [[Project:AWB|AWB]] (10069)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1158" xml:space="preserve">'''Approximate inference''' methods make it possible to learn realistic models from [[big data]] by trading off computation time for accuracy, when exact learning and [[inference]] are [[computationally intractable]].

==Major methods classes ==

*[[Variational Bayesian method]]s 
*[[Expectation propagation]]
*[[Markov random field]]s 
*[[Bayesian network]]s
**[[Variational message passing]]
*loopy and generalized [[belief propagation]] 
&lt;ref&gt;{{cite journal|url=http://academic.research.microsoft.com/Paper/14666.aspx|title=Approximate Inference and Constrained Optimization|journal=Uncertainty in Artificial Intelligence - UAI|pages=313&#8211;320|year=2003}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://mlg.eng.cam.ac.uk/zoubin/approx.html|title=Approximate Inference|accessdate=2013-07-15}}&lt;/ref&gt;

==See also==
*[[Statistical inference]]
*[[fuzzy logic]]
*[[data mining]]

==References==
{{reflist}}

==External links==
*{{cite web|url=http://videolectures.net/mlss09uk_minka_ai/|title=Machine Learning Summer School (MLSS), Cambridge 2009, Approximate Inference|author= Tom Minka, Microsoft Research|date=Nov 2, 2009|type=video lecture}}

[[Category:Data management]]</text>
      <sha1>4mc17z7p7molc85ufvs8evuhpxr0hfy</sha1>
    </revision>
  </page>
  <page>
    <title>Address space</title>
    <ns>0</ns>
    <id>507144</id>
    <revision>
      <id>759279035</id>
      <parentid>759276544</parentid>
      <timestamp>2017-01-10T06:49:50Z</timestamp>
      <contributor>
        <username>Flyer22 Reborn</username>
        <id>4293477</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/41.189.162.22|41.189.162.22]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5148" xml:space="preserve">{{About|a concept used universally in computing|addressing specifically the main memory|Memory address}}
{{refimprove|date=December 2011}}

In [[computing]], an '''address space''' defines a range of discrete addresses, each of which may correspond to a [[network host]], [[peripheral device]], [[disk sector]], a [[computer data storage|memory]] cell or other logical or physical entity.

For software programs to save and retrieve stored data, each unit of data must have an address where it can be individually located or else the program will be unable to find and manipulate the data. The number of address spaces available will depend on the underlying address structure and these will usually be limited by the computer architecture being used.

Address spaces are created by combining enough uniquely identified qualifiers to make an address unambiguous (within a particular address space). For a person's physical address, the ''address space'' would be a combination of locations, such as a neighborhood, town, city, or country. Some elements of an address space may be the same&#8211; but if any element in the address is different than addresses in said space will reference different entities. An example could be that there are multiple buildings at the same address of "32 Main Street" but in different towns, demonstrating that different towns have different, although similarly arranged, [[street address]] spaces.

An address space usually provides (or allows) a partitioning to several regions according to the [[mathematical structure]] it has. In the case of [[total order]], as for [[memory address]]es, these are simply [[interval (mathematics)|chunks]]. Some nested domains hierarchy appears in the case of [[arborescence (graph theory)|directed ordered tree]] as for the [[Domain Name System]] or a [[directory structure]]; this is similar to the hierarchical design of [[postal address]]es. In the [[Internet]], for example, the [[Internet Assigned Numbers Authority]] (IANA) allocates ranges of [[IP address]]es to various registries in order to enable them to each manage their parts of the global Internet address space.&lt;ref&gt;{{cite web|url=http://www.iana.org/assignments/ipv4-address-space/ |title= IPv4 Address Space Registry |date=March 11, 2009 |publisher=Internet Assigned Numbers Authority (IANA) |accessdate= September 1, 2011}}&lt;/ref&gt;

==Examples==
Uses of addresses include, but are not limited to the following:
* [[Memory address]]es for [[main memory]], [[memory-mapped I/O]], as well as for [[virtual memory]];
* Device addresses on an [[expansion bus]];
* [[disk sector|Sector]] addressing for [[disk drive]]s;
* [[File name]]s on a particular [[volume (computing)|volume]];
* Various kinds of network host addresses in [[computer network]]s;
* [[Uniform resource locator]]s in the Internet.

== Address mapping and translation ==
[[File:CNFTL9.JPG|thumb|Illustration of translation from logical block addressing to physical geometry]]
Another common feature of address spaces are [[map (mathematics)|mappings and translations]], often forming numerous layers. This usually means that some higher-level address must be translated to lower-level ones in some way. 
For example, [[file system]] on a [[logical disk]] operates [[one-dimensional array|linear]] sector numbers, which have to be translated to ''absolute'' [[Logical block addressing|LBA]] sector addresses, in simple cases, via [[addition]] of the partition's first sector address. Then, for a disk drive connected via [[Parallel ATA]], each of them must be converted to ''logical'' (means fake) [[cylinder-head-sector]] address due to the interface historical shortcomings. It is converted back to LBA by the disk [[controller (computing)|controller]] and then, finally, to ''physical'' [[cylinder (disk drive)|cylinder]], [[disk head|head]] and [[track (disk drive)|sector]] numbers.

The [[Domain Name System]] maps its names to (and from) network-specific addresses (usually IP addresses), which in turn may be mapped to [[link layer]] network addresses via [[Address Resolution Protocol]]. Also, [[network address translation]] may occur on the edge of ''different'' IP spaces, such as a [[local area network]] and the Internet.

[[File:Virtual address space and physical address space relationship.svg|thumb|Virtual address space and physical address space relationship]]
An iconic example of virtual-to-physical address translation is [[virtual memory]], where different [[Page (computer memory)|pages]] of [[virtual address space]] map either to [[paging|page file]] or to main memory [[physical address]] space. It is possible that several numerically different virtual addresses all refer to one physical address and hence to the same physical byte of [[Random access memory|RAM]]. It is also possible that a single virtual address maps to zero, one, [[CPU cache#Homonym and synonym problems|or more than one]] physical address.

== See also ==
* [[Linear address space]]
* [[Name space]]
* [[Virtualization]]

== References ==
{{Reflist}}

[[Category:Computing terminology]]
[[Category:Data management]]
[[Category:Computer architecture]]</text>
      <sha1>ccs2vb2l5zp8050k06eumey469b764m</sha1>
    </revision>
  </page>
  <page>
    <title>Data thinking</title>
    <ns>0</ns>
    <id>40598793</id>
    <revision>
      <id>751623617</id>
      <parentid>584787059</parentid>
      <timestamp>2016-11-26T21:48:57Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="647" xml:space="preserve">{{unreferenced|date=September 2013}}
'''Data thinking''' is the generic mental pattern observed during the processes of picking a subject to start with, identifying its parts or components, organizing and describing them in an informative fashion that is relevant to what motivated and initiated the whole processes.

The term was created by Mario Faria and Rogerio Panigassi in 2013 when they were writing a book about data science, [[data analysis|data analytics]], data management and how data practitioners were able to achieve their goals.

Mario Faria is one of the first [[Chief Data Officer]]s in the world.



[[Category:Data management]]</text>
      <sha1>5k54s6sjbsvor48c51ocx3cdx3n1fre</sha1>
    </revision>
  </page>
  <page>
    <title>ISO/IEC JTC 1/SC 32</title>
    <ns>0</ns>
    <id>41418778</id>
    <revision>
      <id>671602052</id>
      <parentid>671601888</parentid>
      <timestamp>2015-07-15T19:37:22Z</timestamp>
      <contributor>
        <username>Katieburkhardt</username>
        <id>25402509</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14915" xml:space="preserve">'''ISO/IEC JTC 1/SC 32 Data management and interchange''' is a [[standardization]] subcommittee of the Joint Technical Committee [[ISO/IEC JTC1|ISO/IEC JTC 1]] of the [[International Organization for Standardization]] (ISO) and the [[International Electrotechnical Commission]] (IEC), which develops and facilitates standards within the field of data management and interchange. The international [[Secretariat (administrative office)|secretariat]] of ISO/IEC JTC 1/SC 32 is the [[American National Standards Institute]] (ANSI) located in the United States.&lt;ref name=countries&gt;{{cite web| title=ISO/IEC JTC 1/SC 32 - Data management and interchange| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee_participation.htm?commid=45342| author=ISO| accessdate=2013-10-03}}&lt;/ref&gt;

==History==
ISO/IEC JTC 1/SC 32 was formed in 1997, as a combination of the following three ISO/IEC JTC 1 subgroups: ISO/IEC JTC 1/SC 21/WG 3, Database; ISO/IEC JTC 1/SC 14, Data elements; and ISO/IEC JTC 1/SC 30, Open-edi. The new subcommittee was established with the intention of developing, and facilitating the development of, standards for data management within local and distributed information system environments.&lt;ref name=briefing&gt;{{cite news| title=Information technology: ISO/IEC JTC 1/SC 32, Data Management and Interchange| type=Briefings| author1=Cannan, Stephen| author2=Melton, Jim| journal=ISO Bulletin| date=January 2000| url=http://jtc1sc32.org/doc/N0601-0650/32N0607.pdf| pages=3&#8211;4| volume=31| issue=1}}&lt;/ref&gt; ISO/IEC JTC 1/SC 32 was originally made up of five working groups (WGs), though ISO/IEC JTC 1/SC 32/WG 5, Database access and interchange, was disbanded in March 2002.&lt;ref&gt;{{cite report| type=Business Plan Draft| title=Draft Business Plan for ISO/IEC JTC 1/SC32, Data Management and Interchange| author=Mann, Douglas| accessdate=2013-10-04| url=http://jtc1sc32.org/doc/N0751-0800/32N0783.pdf| date=2002-04-04|page=4}}&lt;/ref&gt; The four other original working groups of the subcommittee are currently active, although the title of ISO/IEC JTC 1/SC 32/WG 1 was changed from Open-edi to its current title, e-Business.&lt;ref name=briefing/&gt;

==Scope==
The scope of ISO/IEC JTC 1/SC 32 is &#8220;Standards for data management within and among local and distributed information systems environments. SC 32 provides enabling technologies to promote harmonization of data management facilities across sector-specific areas. Specifically, SC32 standards include:&#8221;&lt;ref name=business2012&gt;{{cite report| type=Business Plan| url=http://jtc1info.org/wp-content/uploads/2013/03/SC-32-Business-Plan-2012.pdf| accessdate=2013-10-03| author=Melton, Jim| date=2012-10-02| title=Business Plan for JTC1/SC32: 2012-2013}}&lt;/ref&gt;
* Reference models and frameworks for the coordination of existing and emerging standards
* Definition of data domains, data types, and data structures, and their associated semantics
* Languages, services, and protocols for persistent storage, concurrent access and concurrent update, and interchange of data
* Methods, languages, services, and protocols to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce

==Structure==
ISO/IEC JTC 1/SC 32 is made up of four active working groups, each of which carries out specific tasks in standards development within the field of data management and interchange. As a response to changing standardization needs, working groups of ISO/IEC JTC 1/SC 32 can be disbanded if their area of work is no longer applicable, or established if new working areas arise. The focus of each working group is described in the group&#8217;s terms of reference. Active working groups of ISO/IEC JTC 1/SC 32 are:&lt;ref name=business2012/&gt;&lt;ref&gt;{{cite web| title=ISO/IEC JTC 1/SC 32 Data management and interchange| author=ISO| accessdate=2013-10-03| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342}}&lt;/ref&gt;
{| class="wikitable" width="60%"
! width="20%" | Working Group
! width="40%" | Working Area
|-
| ISO/IEC JTC 1/SC 32/WG 1 || [[Electronic business|e-Business]]
|-
|ISO/IEC JTC 1/SC 32/WG 2	|| [[Metadata]]
|-
|ISO/IEC JTC 1/SC 32/WG 3	|| [[Database#Database languages|Database languages]]
|-
|ISO/IEC JTC 1/SC 32/WG 4 || [[SQL]] Multimedia and application packages
|-
|}

==Collaborations==
ISO/IEC JTC 1/SC 32 works in close collaboration with a number of other organizations or subcommittees, both internal and external to ISO or IEC, in order to avoid conflicting or duplicative work. Organizations internal to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:&lt;ref name=business2012/&gt;&lt;ref&gt;{{cite web| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342| title=ISO/IEC JTC 1/SC 32 Data management and interchange| accessdate=2013-10-03| author=ISO}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=SC32 Liaison Organizations| accessdate=2013-10-03| author=ISO/IEC JTC 1/SC 32| url=http://jtc1sc32.org/}}&lt;/ref&gt;
* [[ISO/IEC JTC 1/SC 7]], Software and systems engineering
* [[ISO/IEC JTC 1/SC 25]], Interconnection of information technology equipment
* [[ISO/IEC JTC 1/SC 38]], Cloud Computing and Distributed Platforms
* ISO/TC 12, Quantities and units
* [[ISO/TC 37]], Terminology and other language and content resources 
* ISO/TC 37/SC 2, Terminographical and lexicographical working methods
* ISO/TC 37/SC 3, Systems to manage terminology, knowledge and content
* ISO/TC 37/SC 4, Language resource management
* ISO/TC 46/SC 4, Technical interoperability
* ISO/TC 46/SC 11, Archives/records management
* ISO/TC 68/SC 2, Financial Services, security
* ISO/TC 127, Earth-moving machinery
* ISO/TC 154, Processes, data elements and documents in commerce, industry and administration
* ISO/TC 184, Automation systems and integration
* [[ISO TC 184/SC 4|ISO/TC 184/SC 4]], Industrial data
* ISO/TC 204, Intelligent transport systems
* [[ISO/TC 211]], Geographic information/Geomatics
* [[ISO/TC 215]], Health informatics
* ISO/TC 232, Learning services outside formal education

Some organizations external to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:
* [[Conf&#233;d&#233;ration Internationale des Soci&#233;t&#233;s d&#180;Auteurs et Compositeurs|International Confederation of Societies of Authors and Composers]] (CISAC)
* [[Dublin Core Metadata Initiative]] (DCMI)
* [[EUROSTAT]]
* [[International Telecommunications Satellite Organization]] (ITSO)
* [[International Telecommunication Union|ITU]]
* [[Infoterm]]
* [[Object Management Group]] (OMG)
* [[Society for Worldwide Interbank Financial Telecommunication]] (SWIFT)
* [[UN/CEFACT]]
* [[United Nations Economic Commission for Europe]] (UNECE)
* [[World Meteorological Organization]] (WMO)
* [[W3C]]

==Member countries==
Countries pay a fee to ISO to be members of subcommittees.&lt;ref&gt;{{cite manual| url=http://www.iso.org/iso/iso_membership_manual_2012.pdf| pages=-18| chapter=III. What Help Can I Get from the ISO Central Secretariat?| title=ISO Membership Manual| author=ISO| date=June 2012| accessdate=2013-07-12| publisher=ISO}}&lt;/ref&gt;

The 14 "P" (participating) members of ISO/IEC JTC 1/SC 32 are: Canada, China, Czech Republic, C&#244;te d'Ivoire, Egypt, Finland, Germany, India, Japan, Republic of Korea, Portugal, Russian Federation, United Kingdom, and United States.&lt;ref name=countries/&gt;

The 22 "O" (observing) members of ISO/IEC JTC 1/SC 32 are: Australia, Austria, Belgium, Bosnia and Herzegovina, France, Ghana, Hungary, Iceland, Indonesia, Islamic Republic of Iran, Ireland, Italy, Kazakhstan, Luxembourg, Netherlands, Norway, Poland, Romania, Serbia, Spain, Switzerland, and Turkey.

==Published standards==
ISO/IEC JTC 1/SC 32 standards are meant to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce.&lt;ref name=briefing/&gt; ISO/IEC JTC 1/SC 32 currently has 74 published standards within the field of data management and interchange, including:&lt;ref&gt;{{cite web| title=ISO/IEC JTC 1/SC 32| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_tc_browse.htm?commid=45342&amp;published=on| accessdate=2013-10-03| author=ISO}}&lt;/ref&gt;&lt;ref&gt;{{cite web| publisher=ISO| url=http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html| title=Freely Available Standards| accessdate=2013-09-26}}&lt;/ref&gt;
{| class="wikitable sortable" width="100%"
! data-sort-type="number" width="14%" | ISO/IEC Standard
! width="29%" | Title
! width="6%" | Status
! width="49%" | Description
! width= "2%" | WG
|-
|data-sort-value="14662"|ISO/IEC 14662 [http://standards.iso.org/ittf/licence.html free] || Information technology &#8211; Open-edi reference model || Published (2010) || Specifies the framework for coordinating the integration of existing International Standards and the development of future International Standards for the interworking of Open-edi parties through Open-edi&lt;ref&gt;{{cite journal| title=ISO/IEC 14662| date=2010-02-15| author=ISO| edition=3| page=1}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=ISO/IEC 14662:2010| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55290| date=2010-02-02| author=ISO}}&lt;/ref&gt; || 1
|-
|data-sort-value="15944"|ISO/IEC 15944-1 [http://standards.iso.org/ittf/licence.html free] || Information technology &#8211; Business Operational View &#8211; Part 1: Operational aspects of Open-edi for implementation || Published (2011) || Allows constraints, including legal requirements, commercial and/or international trade and contract terms, public policy, and laws and regulations, to be defined and integrated into Open-edi through the business operational view (BOV)&lt;ref&gt;{{cite journal| title=ISO/IEC 15944-1| date=2011-08-01| author=ISO| edition=2| page=1}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=ISO/IEC 15944-1:2011| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55289| date=2011-07-21| author=ISO}}&lt;/ref&gt; || 1
|-
|data-sort-value="11179"|[[ISO/IEC 11179]]-3 [http://standards.iso.org/ittf/licence.html free] || Information technology &#8211; [[metadata registry|Metadata registries]] (MDR) &#8211; Part 3: Registry metamodel and basic attributes || Published (2013) || Specifies the structure of a metadata registry in the form of a conceptual data model and specifies basic attributes which are required to describe metadata items&lt;ref&gt;{{cite journal| page=1| date=2003-02-15| title=ISO/IEC 11179-3| author=ISO| edition=2}}&lt;/ref&gt;&lt;ref&gt;{{cite web| title=ISO/IEC 11179-3:2013| accessdate=2013-10-03| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=50340| date=2013-02-12| author=ISO}}&lt;/ref&gt; || 2
|-
|data-sort-value="20943"| ISO/IEC TR 20943-1 [http://standards.iso.org/ittf/licence.html free] || Information technology &#8211; Procedures for achieving metadata registry content consistency &#8211; Part 1: Data elements || Published (2003) || &#8220;Describes a set of procedures for the consistent registration of data elements and their attributes in a registry.&#8221;&lt;ref&gt;{{cite journal| title=ISO/IEC TR 20943-1| author=ISO| page=1| date=2003-08-01| edition=1}}&lt;/ref&gt;&lt;ref&gt;{{cite web| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=34343| date=2012-12-19| title=ISO/IEC TR 20943-1:2003| author=ISO}}&lt;/ref&gt; || 2
|-
|data-sort-value="20944"| ISO/IEC 20944-1 || Information technology &#8211; Metadata Registries Interoperability and Bindings (MDR-IB) &#8211; Part 1: Framework, common vocabulary, and common provisions for conformance || Published (2013) || Contains the overview, framework, common vocabulary, and common provisions for conformance for the ISO/IEC 20944 series, which provides the bindings and their interoperability for MDRs&lt;ref&gt;{{cite web| author=ISO| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=51914| date=2013-01-08| accessdate=2013-10-04| title=ISO/IEC 20944-1:2013}}&lt;/ref&gt; || 2
|-
|data-sort-value="19502"|ISO/IEC 19502 || Information technology &#8211; [[Meta-Object Facility|Meta Object Facility (MOF)]] || Published (2005) || Defines a metamodel using MOF, and a set of interfaces using Open Distributed Processing (ODP) that can be used to define and manipulate a set of interoperable metamodels and their corresponding models&lt;ref&gt;{{cite web| title=ISO/IEC 19502:2005| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32621| date=2011-03-17| author=ISO}}&lt;/ref&gt; || 2
|-
|data-sort-value="19773"|ISO/IEC 19773 || Information technology &#8211; Metadata Registries (MDR) modules || Published (2011) || Specifies small modules of data to be used or reused in applications&lt;ref&gt;{{cite web| url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=41769| date=2011-09-01| accessdate=2013-10-04| author=ISO| title=ISO/IEC 19773:2011}}&lt;/ref&gt; || 2
|-
|data-sort-value="09075"|ISO/IEC 9075-1 [http://standards.iso.org/ittf/licence.html free] || Information technology &#8211; Database languages &#8211; [[SQL#Standardization|SQL]] &#8211; Part 1: Framework (SQL/Framework) || Published (2011) || Defines the conceptual framework to specify the grammar of SQL and the result of processing statements in that language by an SQL-implementation&lt;ref&gt;{{cite journal| title=ISO/IEC 9075-1| page=1| date=2008-07-15| edition=3| author=ISO}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53681| date=2013-02-04| accessdate=2013-10-04| author=ISO| title=ISO/IEC 9075-1:2011}}&lt;/ref&gt; || 3
|-
|data-sort-value="13249"|ISO/IEC 13249-3 || Information technology &#8211; Database languages &#8211; SQL multimedia and application packages &#8211; Part 3: Spatial || Published (2011) || &#8220;Defines spatial user-defined types, routines, and schemas for generic spatial data handling.&#8221;&lt;ref&gt;{{cite web| date=2011-08-22| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53698| accessdate=2013-10-04| author=ISO| title=ISO/IEC 13249-3:2011}}&lt;/ref&gt; || 4
|-
|}

==See also==
* [[ISO/IEC JTC1]]
* [[List of International Organization for Standardization standards|List of ISO standards]]
* [[American National Standards Institute]]
* [[International Organization for Standardization]]
* [[International Electrotechnical Commission]]

==References==
{{Reflist|2}}

==External links==
* [http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342 ISO/IEC JTC 1/SC 32 page at ISO]

{{DEFAULTSORT:ISO IEC JTC1 SC32}}
[[Category:ISO/IEC JTC1 subcommittees|#032]]
[[Category:Standards organizations]]
[[Category:Data management]]
[[Category:Data interchange standards]]</text>
      <sha1>akoaiv8ccfnfkxv9v49ctbfr8vswa0b</sha1>
    </revision>
  </page>
  <page>
    <title>ANSI 834 Enrollment Implementation Format</title>
    <ns>0</ns>
    <id>42783850</id>
    <revision>
      <id>760868602</id>
      <parentid>760742706</parentid>
      <timestamp>2017-01-19T15:16:18Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2401" xml:space="preserve">{{Orphan|date=June 2014}}
Edi 834 files.
[[American National Standards Institute|ANSI]] 834 EDI Enrollment Implementation [[File format|Format]] is a standard format for electronically exchanging health plan enrollment data between employers and [[health insurance]] carriers. An 834 file contains a string of data elements and each data element represents a fact, such as a subscriber&#8217;s name, hire date, etc. The entire string is called a data segment. [[Health Insurance Portability and Accountability Act|The Health Insurance Portability and Accountability Act (HIPAA)]] requires that all health plans or health insurance carriers accept a standard enrollment format, ANSI 834A Version 5010. The ANSI 834A is the national standard for electronic enrollment and maintenance health plan.

The 834 is used to transfer enrollment information from the sponsor of the [[insurance]] coverage, benefits, or policy to a payer. The intent of this implementation guide is to meet the [[health care]] industry's specific need for the initial enrollment and subsequent maintenance of individuals who are enrolled in insurance products. This implementation guide specifically addresses the enrollment and maintenance of health care products only. One or more separate guides may be developed for life, flexible spending, and retirement products.

An example layout of an ANSI 834A Version 5010 file is shown below.

'''Sample File Output'''&lt;br /&gt;
INS*Y*18*030*XN*A*E**FT~&lt;br /&gt;
REF*OF*152239999~&lt;br /&gt;
REF*1L*Blue~&lt;br /&gt;
DTP*336*D8*20070101~&lt;br /&gt;
NM1*IL*1*BLUTH*LUCILLE****34*152239999~&lt;br /&gt;
N3*224 N DES PLAINES*6TH FLOOR~&lt;br /&gt;
N4*CHICAGO*IL*60661*USA~&lt;br /&gt;
DMG*D8*19720121*F*M~&lt;br /&gt;
HD*030**VIS**EMP~&lt;br /&gt;
DTP*348*D8*20111016~&lt;br /&gt;
INS*N*19*030*XN*A*E***N*N~&lt;br /&gt;
REF*OF*152239999~&lt;br /&gt;
REF*1L*Blue~&lt;br /&gt;
DTP*357*D8*20111015~&lt;br /&gt;
NM1*IL*1*BLUTH*BUSTER~&lt;br /&gt;
N3*224 N DES PLAINES*6TH FLOOR~&lt;br /&gt;
N4*CHICAGO*IL*60661*USA~&lt;br /&gt;
DMG*D**19911015*M-HD*030**VIS~&lt;br /&gt;
DTP*348*D8*20110101~&lt;br /&gt;
DTP*349*D8*20111015~

==See also==
* [[X12 Document List]]

==References==
{{reflist}}
* [http://getworkforce.com/ansi-834-file-layout/ "ANSI 834 File Layout"]
* [http://getworkforce.com/ansi-834-file-layout/ "Guardian Electronic User Guide 834 Enrollment and Maintenance"]
* [http://www.1edisource.com/transaction-sets?tset=834 "EDI 834 Benefit Enrollment and Maintenance"]

[[Category:Data management]]</text>
      <sha1>nxkmw9as3ynru7oy5nsxlcab39yp909</sha1>
    </revision>
  </page>
  <page>
    <title>BBC Genome Project</title>
    <ns>0</ns>
    <id>44172733</id>
    <revision>
      <id>762519208</id>
      <parentid>762519193</parentid>
      <timestamp>2017-01-29T09:13:56Z</timestamp>
      <contributor>
        <username>Tim!</username>
        <id>203786</id>
      </contributor>
      <comment>added [[Category:Databases in the United Kingdom]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5276" xml:space="preserve">[[File:BBC Genome Logo.png|thumb|BBC Genome Logo]]

The '''BBC Genome Project''' is a digitised searchable database of programme listings from the [[Radio Times]] from the first issue in 1923, to 2009.&lt;ref name=About&gt;{{cite web|title=About this project|url=http://genome.ch.bbc.co.uk/about|publisher=[[BBC]]|accessdate=21 October 2014}}&lt;/ref&gt; 

==History==
===Prior===
BBC Genome is not the BBC's first online searchable database; in April 2006 the BBC gave the public access to Infax, the BBC's programme database. Infax contained around 900,000 entries, but not every programme ever broadcast, and it ceased operation in December 2007.&lt;ref name="About Infax"&gt;{{cite web|title=About This Prototype|url=http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|publisher=[[BBC]]|accessdate=2 February 2016|archiveurl=https://web.archive.org/web/20060613100552/http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|archivedate=13 June 2006}}&lt;/ref&gt; The front page of the website is still available to see via the [[Internet Archive]] [https://web.archive.org/web/20060512054648/http://open.bbc.co.uk/catalogue/infax here.] After Infax ceased, a message on the website said that it would be incorporating in the information into individual programme pages.&lt;ref name="Prototype End"&gt;{{cite web|title=This experimental prototype trial has now concluded|url=http://www.bbc.co.uk/archive/catalogue_offline.shtml|publisher=[[BBC]]|accessdate=2 February 2016}}&lt;/ref&gt; In 2012, it was replaced by the database Fabric but this is only for internal use within the BBC.

==''Radio Times''== 
[[File:BBC Genome OCR error.jpg|thumb|Screenshot of an OCR error (since corrected) in Genome. The text, "Uza TarbuclC's Christmas", should read "[[Liza Tarbuck]]'s Christmas".]]
In December 2012, the [[BBC]] completed a digitisation exercise, scanning the listings from [[Radio Times]] of all BBC programmes 1923-2009 from an entire run of about 4,500 copies of the magazine.&lt;ref name="Kelion"&gt;{{cite web|url=http://www.bbc.co.uk/news/technology-20625884|title=BBC finishes Radio Times archive digitisation effort|last=Kelion|first=Leo|work=[[BBC Online]]|accessdate=20 January 2013}}&lt;/ref&gt; They identified around five million programmes, involving 8.5 million actors, presenters, writers and technical staff.&lt;ref name="Kelion" /&gt; The listings are as published, in advance, and so do not include late changes or cancellations.

The issues were scanned at high resolution, producing [[TIF]] images and [[Optical Character Recognition]] (OCR) was then used to turn the text from the page into searchable text on the Genome database.&lt;ref name=About/&gt;

BBC Genome was released for public use on 15 October 2014.&lt;ref name="Hilbish"&gt;{{cite web|url=http://www.bbc.co.uk/blogs/aboutthebbc/posts/Genome-The-Radio-Times-Archive-is-now-live|title=Genome &#8211; Radio Times archive now live|last=Bishop|first=Hilary|work=[[BBC Online]]|accessdate=15 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Sweney|first=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|newspaper=Guardian|date=16 October 2014}} &lt;/ref&gt;

The aim of this project is to allow researchers to be able to find out information easier and to help [[BBC Archives]] to build up a picture of what exists and what is currently missing from the archive.&lt;ref&gt;{{cite web|title=BBC's Genome Project offers radio and TV archive listings|url=http://www.bbc.co.uk/news/technology-29643662|publisher=[[BBC]]|accessdate=21 October 2014|date=16 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Sweney|first1=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|publisher=[[The Guardian]]|accessdate=21 October 2014|date=16 October 2014}}&lt;/ref&gt; Corrections to OCR errors and changes to advertised schedules are being [[Crowdsourcing|crowdsourced]],&lt;ref name="Hilbish" /&gt; with over 180,000 user generated edits accepted as of January 2017. &lt;ref&gt;{{Cite web|url=http://genome.ch.bbc.co.uk/schedules/bbcone/london/1964-04-20|title=BBC One London - 20 April 1964 - BBC Genome|website=genome.ch.bbc.co.uk|access-date=2017-01-09}}&lt;/ref&gt; 

Each listing entry has a unique identifier which may be expressed as a URL. For example, the very first screening of ''[[Doctor Who]]'' is http://genome.ch.bbc.co.uk/8f81c193ba224e84981f353cae480d49 A broadcast programme may have more than one such identifier, if it was screened (and thus listed) on repeat occasions.

==See also==
*[[BBC Archives]]
* [[Timeline of the BBC]]

==References==
{{Reflist|2}}

==External links==

{{Wikidata property|P1573|BBC Genome identifiers}}

*[http://genome.ch.bbc.co.uk/ BBC Genome website]
*[http://www.bbc.co.uk/blogs/genome BBC Genome blog] 
*[http://twitter.com/bbcgenome/ BBC Genome on Twitter] 
*[http://www.facebook.com/bbcgenome/ BBC Genome on Facebook]

{{BBC}}

[[Category:BBC]]
[[Category:BBC New Media|Archives]]
[[Category:Data management]]
[[Category:Broadcasting websites]]
[[Category:British websites]]
[[Category:History of television in the United Kingdom]]
[[Category:History of radio]]
[[Category:BBC history]]
[[Category:Databases in the United Kingdom]]</text>
      <sha1>jaawtz7kv2x0n2jc0bmq8br0l5dy9qp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information privacy</title>
    <ns>14</ns>
    <id>44363114</id>
    <revision>
      <id>633318258</id>
      <timestamp>2014-11-11T01:41:38Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <comment>Robot: Moved from Category:Data privacy. Authors: Alex299006, DHN-bot, 213.223.20.191, 62.147.39.198, EmausBot, Addbot, Elmondo21st, Escarbot, TCY, C.L&#246;ser, SporkBot, Ugur Basak Bot, Eliyak, Pnm, Qu3a, Maurreen, Good Olfactory, Hooperbloob, ArnoldRein...</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="177" xml:space="preserve">{{Cat main}}

[[Category:Data management|Privacy]]
[[Category:Computer law|Privacy]]
[[Category:Privacy]]
[[Category:Computer security|Privacy]]
[[Category:Information|Privacy]]</text>
      <sha1>q4g66ivyer72zhhl9eaphjhi5pe856c</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Extract, transform, load tools</title>
    <ns>14</ns>
    <id>44363181</id>
    <revision>
      <id>633318793</id>
      <timestamp>2014-11-11T01:45:57Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <comment>Robot: Moved from Category:ETL tools. Authors: DePiep, TXiKiBoT, Addbot, EmausBot, DSisyphBot, JonHarder, 203.91.193.5, Luckas-bot, CG janitor, 203.116.208.9, Good Olfactory, Stevage, 88.217.121.60</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="201" xml:space="preserve">'''Extract, transform, load tools''' are software packages that facilitate the performing of [[Extract, transform, load|ETL]] tasks.

[[Category:Data management]]
[[Category:Data warehousing products]]</text>
      <sha1>tsdhmcux8y8jmna0iwhvk5ye6zpz8ql</sha1>
    </revision>
  </page>
  <page>
    <title>SciDB</title>
    <ns>0</ns>
    <id>38334530</id>
    <revision>
      <id>706251516</id>
      <parentid>701800207</parentid>
      <timestamp>2016-02-22T08:18:36Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <comment>[[Help:Cat-a-lot|Cat-a-lot]]: Moving from [[Category:Software using the AGPL license]] to [[Category:Software using the GNU AGPL license]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2813" xml:space="preserve">{{Infobox software
| title       = SciDB
| developer   = Paradigm4
| publisher   =
| genre       = Database management system
| license     = [[Affero General Public License|AGPL]] v3&lt;ref&gt;[http://www.scidb.org/forum/viewtopic.php?f=16&amp;t=364 Download &amp; licensing]&lt;/ref&gt;
| first       =
| last        =
| coauthors   =
| released    = 2008
| website     = {{URL|http://www.paradigm4.com/}}
}}
{{Distinguish2|[http://scidb.sourceforge.net/ Scidb: Chess database]}}

'''SciDB'''&lt;ref&gt;{{cite web | url = http://www.scidb.org/ | title=SciDB website}}&lt;/ref&gt; is an array database designed for multidimensional data management and analytics common to scientific, geospatial, financial, and industrial applications.  It is developed by Paradigm4, co-founded by [[Michael Stonebraker]].

==History and Characteristics==
[[Michael Stonebraker]] co-created SciDB where, he claims, arrays are 100 or so times faster than a RDBMS on this class of problem.&lt;ref&gt;{{cite web | url = http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview | title=Stonebraker interview}}&lt;/ref&gt; It is swapping rows and columns for mathematical arrays that put fewer restrictions on the data and can work in any number of dimensions unlike the conventionally widely used [[relational database management system]] model, in which each [[Relation (database)|relation]] supports only one dimension of records.

According to a Strata Conference presentation on SciDB,&lt;ref&gt;{{cite web |url=http://strataconf.com/stratany2011/public/schedule/detail/21376 |title=Big Data and Big Analytics: SciDB is not Hadoop}}&lt;/ref&gt; it natively supports:
* An array data model for efficient storage and manipulation of larger-than-memory multi-dimensional arrays.
* Data versioning and provenance to allow tracking results back to original supporting data.
* What-if modeling, back-testing, and re-analysis.
* Massive scale math on the arrays for linear algebra and analytics.
* Uncertainty can be modeled by associating error-bars with data.
* Efficient storage.

==See also==
&lt;!-- Please do not list specific implementations here! --&gt;
* [[Comparison of object database management systems]]
* [[Comparison of structured storage software]]

== References ==
{{Reflist|2}}

==External links==
* [http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview/ Michael Stonebraker interview]
* [http://itknowledgeexchange.techtarget.com/soa-talk/stonebraker-sees-high-programming-overhead-for-nosql/ Stonebraker sees high programming overhead for NoSQL]

[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Document-oriented databases]]
[[Category:Distributed computing architecture]]
[[Category:Free database management systems]]
[[Category:Structured storage]]
[[Category:NoSQL]]
[[Category:Software using the GNU AGPL license]]</text>
      <sha1>3zalu0yyeryiam7p078jwbxnu13b44m</sha1>
    </revision>
  </page>
  <page>
    <title>Media aggregation platform</title>
    <ns>0</ns>
    <id>46201390</id>
    <revision>
      <id>723519281</id>
      <parentid>711264718</parentid>
      <timestamp>2016-06-03T14:30:57Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* top */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2980" xml:space="preserve">A '''Media Aggregation Platform''' or '''Media Aggregation Portal''' (MAP) is an over-the-top service for distributing web-based streaming media content from multiple sources to a large audience. MAPs consist of networks of sources who host their own content which viewers can choose and access directly from a larger variety of content to choose from than a single source can offer.&lt;ref&gt;{{cite web | url=https://medium.com/@bmobley/over-the-top-of-ott-need-a-map-9931096775c2 | title=Over the Top of OTT&#8230; Need a MAP? | publisher=[[Medium.com]] | accessdate=23 March 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite book | title=Building Next-Generation Converged Networks: Theory and Practice | publisher=CRC Press |author1=Al-Sakib Khan Pathan |author2=Muhammad Mostafa Monowar |author3=Zubair Md. Fadlullah | year=2013 | isbn=1466507616}}&lt;/ref&gt; The service is used by content providers, looking to extend the reach of their content.

Unlike multichannel video programming distributor ([[MVPD]]) or multiple-system operator (MSO), MAPs rely on the Internet rather than cables or satellite. As more network television channels have moved online in the early 21st century,&lt;ref&gt;{{cite web | url=http://www.exchange4media.com/59377_ott-platforms-to-be-key-growth-area-for-tv-broadcasters.html | title=OTT platforms to be key growth area for TV broadcasters | accessdate=23 March 2015}}&lt;/ref&gt; joining web-native channels like [[Netflix]], MAPs aggregates content the way that MSOs and MVPDs have used cable, and to a lesser extent satellite and IPTV infrastructure. There are companies that offer a similar service for free, including [[Yidio]] and TV.com, while others charge a subscription fee like as [[FreeCast Inc]]'s Rabbit TV Plus.&lt;ref&gt;{{cite web | url=http://rabbittvplus.com/ | title=FreeCast Inc Rabbit TV Plus | accessdate=23 March 2015}}&lt;/ref&gt; When compared with MSOs and MVPDs, MPAs network have much lower cost due to lack of physical infrastructure. The majority of revenues from their services is retained by the content creators and revenues are from advertisements, [[pay-per-view]], and subscription-based content offerings instead of by licensing and reselling content. MAPs service consumers directly with the content source and they purchase content directly from its source, without the markup added by a middleman.&lt;ref&gt;{{cite web | url=https://www.ncta.com/industry-data | title=NCTA Industry Data | accessdate=23 March 2015}}&lt;/ref&gt;

==See also==
* [[Multichannel video programming distributor|Multichannel Video Programming Distributor (MVPD)]]
* [[Multiple system operator|Multiple System Operator (MSO)]]
* [[Internet protocol television|Internet Protocol Television (IPTV)]]
* [[Over-the-top content|Over-the-top (OTT)]]
* [[Over-the-air television|Over-the-air (OTA)]]
* [[Video on demand|Video on demand (VOD)]] 
* [[Broadcast networks|Broadcast Networks]] 
* [[Internet Television]]
* [[Streaming Media]]
* [[Pay TV]]

==References==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>548lw4ztr4kisy9wmjuxnr2rjp2mbyv</sha1>
    </revision>
  </page>
  <page>
    <title>Data architect</title>
    <ns>0</ns>
    <id>46362818</id>
    <revision>
      <id>741240349</id>
      <parentid>739031003</parentid>
      <timestamp>2016-09-26T07:40:00Z</timestamp>
      <contributor>
        <ip>2400:4020:89C7:2F00:5C4A:79A7:ADBF:EDDD</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4742" xml:space="preserve">{{expert-subject|Computer science|talk=Copyright issues and need for expert attention}}
A ''data architect'' is a practitioner of [[data architecture]], an information technology discipline concerned with designing, creating, deploying and managing an organization's data architecture. Data architects define how the data will be stored, consumed, integrated and managed by different data entities and IT systems, as well as any applications using or processing that data in some way.&lt;ref&gt;{{cite web|title=Definition of Data Architect|url=http://stage.web.techopedia.com/definition/29452/data-architect|website=Techopedia}}&lt;/ref&gt;  It is closely allied with [[business architecture]] and is considered to be one of the four domains of [[enterprise architecture]].

==Role==
According to the Data Management Body of Knowledge,&lt;ref&gt;{{cite web|title=Data Management Body of Knowledge|url=http://www.dama.org/content/body-knowledge|publisher=Data Management Association}}&lt;/ref&gt; the data architect &#8220;provides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements, and aligns with enterprise strategy and related business architecture.&#8221;

According to the Open Group Architecture Framework (TOGAF), a data architect is expected to set data architecture principles, create models of data that enable the implementation of the intended business architecture, create diagrams showing key data entities, and create an inventory of the data needed to implement the architecture vision.&lt;ref name=TOGAF&gt;{{cite book|title=The Open Group Architectural Framework (TOGAF 9.1)|publisher=The Open Group|location=Chapter 10 - Data Architecture|url=http://pubs.opengroup.org/architecture/togaf9-doc/arch/chap10.html|accessdate=1 March 2015}}&lt;/ref&gt;

==Responsibilities==
# Organizes data at the macro level (i.e. which subject areas are managed in which goldensources) 
# Organizes data at the micro level, data models, for a new application. 
# Provides a logical data model as a standard for the goldensource and for consuming applications to inherit. 
# Provides a logical data model with elements and business rules needed for the creation of DQ rules.

==Skills==
Bob Lambert, Director of Data Architecture at consulting firm CapTech, describes the necessary skills of a Data Architect as follows:&lt;ref&gt;{{cite web|last1=Lambert|first1=Bob|title=Skills of a Data Architect|url=http://www.captechconsulting.com/blog/bob-lambert/skills-the-data-architect|website=Captech}}&lt;/ref&gt;

* Foundation in systems development: the data architect should understand the system development life cycle; software project management approaches; and requirements, design, and test techniques. The data architect is asked to conceptualize and influence application and interface projects, and therefore must understand what advice to give and where to plug in to steer toward desirable outcomes.
* Depth in data modeling and database design: This is the core skill of the data architect, and the most requested in data architect job descriptions. The effective data architect is sound across all phases of data modeling, from conceptualization to database optimization. In his experience this skill extends to SQL development and perhaps database administration.
* Breadth in established and emerging data technologies: In addition to depth in established data management and reporting technologies, the data architect is either experienced or conversant in emerging tools like columnar and NoSQL databases, predictive analytics, data visualization, and unstructured data. While not necessarily deep in all of these technologies, the data architect hopefully is experienced in one or more, and must understand them sufficiently to guide the organization in understanding and adopting them.
* Ability to conceive and portray the big data picture: When the data architect initiates, evaluates, and influences projects he or she does so from the perspective of the entire organization. The data architect maps the systems and interfaces used to manage data, sets standards for data management, analyzes current state and conceives desired future state, and conceives projects needed to close the gap between current state and future goals.
* Ability to astutely operate in the organization: Well respected and influential, Able to emphasize methodology, modeling, and governance, Technologically and politically neutral, Articulate, persuasive, and a good salesperson, and Enthusiastic

==References==
{{reflist}}

==See also==
* [[Data Architecture]]
* [[Information Architect]]
* [[Enterprise Architecture]]

[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Data security]]</text>
      <sha1>1pynysiny7kdpvw2tywx6vkd2m0292f</sha1>
    </revision>
  </page>
  <page>
    <title>Data availability</title>
    <ns>0</ns>
    <id>46877023</id>
    <revision>
      <id>751536610</id>
      <parentid>751376809</parentid>
      <timestamp>2016-11-26T09:24:23Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/136.186.67.94|136.186.67.94]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2803" xml:space="preserve">{{cleanup reorganize|date=June 2015}}
Data availability&lt;ref&gt;http://searchstorage.techtarget.com/definition/data-availability&lt;/ref&gt; is a term used by computer storage manufacturers and storage service providers (SSPs) to describe products and services that ensure that data continues to be available at a required level of performance in situations ranging from normal through "disastrous."

Anytime a server loses power, for example, it has to reboot, recover data and repair corrupted data. The time it takes to recover, known as the mean time to recover (MTR), could be minutes, hours or days.&lt;ref&gt;http://blog.schneider-electric.com/datacenter/2012/10/12/understanding-data-center-reliability-availability-and-the-cost-of-downtime/&lt;/ref&gt;

==Data Center Standards==
The two organizations in the United States that publish data center standards are the [[Telecommunications Industry Association]] (TIA) and the [[Uptime Institute]].

===TIA - Data Center Standards===
See wiki entry on [[TIA-942]].

===Uptime Institute - Data Center Tier Standards===
'''Tier I Requirements'''&lt;ref&gt;http://www.firstcomm.com/overview-of-data-center-availability-tiers/&lt;/ref&gt;
* Single non-redundant distribution path serving the IT equipment
*  Non-redundant capacity components
* Basic site infrastructure with expected availability of 99.671%

'''Tier II Requirements'''
* Meets or exceeds all Tier I requirements
* Redundant site infrastructure capacity components with expected availability of 99.741%

'''Tier III Requirements'''
* Meets or exceeds all Tier I and Tier II requirements
* Multiple independent distribution paths serving the IT equipment
* All IT equipment must be dual-powered and fully compatible with the topology of a site&#8217;s architecture
* Concurrently maintainable site infrastructure with expected availability of 99.982%

'''Tier IV Requirements'''
* Meets or exceeds all Tier I, Tier II and Tier III requirements
* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems
* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%

The Uptime Institute&#8217;s tier system allows for the following minutes of downtime annually:
* Tier I (99.671% minimum uptime) (1729 minutes maximum annual downtime)
* Tier II (99.741% minimum uptime) (1361 minutes maximum annual downtime)
* Tier III (99.982% minimum uptime) (95 minutes maximum annual downtime)
* Tier IV (99.995% minimum uptime) (26 minutes maximum annual downtime)

==See also==
* Data Center Tiers [[Data center#Data center tiers|Data center tiers]]

==References==
{{reflist}}

[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]</text>
      <sha1>eyw1evyl91100yk9uk86y68gwjw8m1j</sha1>
    </revision>
  </page>
  <page>
    <title>Online transaction processing</title>
    <ns>0</ns>
    <id>2329992</id>
    <revision>
      <id>759139904</id>
      <parentid>759139867</parentid>
      <timestamp>2017-01-09T13:07:19Z</timestamp>
      <contributor>
        <username>CyanoTex</username>
        <id>24347243</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/203.99.197.212|203.99.197.212]] ([[User talk:203.99.197.212|talk]]) to last revision by Alymamlani. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7608" xml:space="preserve">'''Online transaction processing''', or '''OLTP''', is a class of [[information system]]s that facilitate and manage transaction-oriented applications, typically for data entry and retrieval [[transaction processing]].

The term is somewhat ambiguous; some understand a "transaction" in the context of computer or [[database transactions]], while others (such as the [[Transaction Processing Performance Council]]) define it in terms of business or [[financial transaction|commercial transactions]].&lt;ref&gt;[http://www.tpc.org/ Transaction Processing Performance Council website]&lt;/ref&gt; OLTP has also been used to refer to processing in which the system responds immediately to user requests. An [[automated teller machine]] (ATM) for a bank is an example of a commercial transaction processing application. Online transaction processing applications are high throughput and insert or update-intensive in database management. These applications are used concurrently by hundreds of users. The key goals of OLTP applications are availability, speed, concurrency and recoverability.&lt;ref&gt;[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]&lt;/ref&gt; Reduced paper trails and the faster, more accurate forecast for revenues and expenses are both examples of how OLTP makes things simpler for businesses. However, like many modern online information technology solutions, some systems require offline maintenance, which further affects the cost&#8211;benefit analysis of on line transaction processing system.

OLTP is typically contrasted to [[Online analytical processing|OLAP]] (online analytical processing), which is generally characterized by much more complex queries, in a smaller volume, for the purpose of business intelligence or reporting rather than to process transactions. Whereas OLTP systems process all kinds of queries (read, insert, update and delete), OLAP is generally optimized for read only and might not even support other kinds of queries.

==Overview==
OLTP system is a popular data processing system in today's enterprises.  Some examples of OLTP systems include order entry, retail sales, and financial transaction systems.&lt;ref&gt;What is an OLTP System[http://docs.oracle.com/cd/E11882_01/server.112/e25523/part_oltp.htm]&lt;/ref&gt;  On line transaction processing system increasingly requires support for transactions that span a network and may include more than one company. For this reason, modern on line transaction processing software use client or server processing and brokering software that allows transactions to run on different computer platforms in a network.

In large applications, efficient OLTP may depend on sophisticated transaction management software (such as [[CICS]]) and/or [[database]] optimization tactics to facilitate the processing of large numbers of concurrent updates to an OLTP-oriented database.

For even more demanding decentralized database systems, OLTP brokering programs can distribute transaction processing among multiple computers on a [[computer network|network]]. OLTP is often integrated into [[service-oriented architecture]] (SOA) and [[Web service]]s.

On line transaction processing (OLTP) involves gathering input information, processing the information and updating existing information to reflect the gathered and processed information. As of today, most organizations use a database management system to support OLTP. OLTP is carried in a client server system.

On line transaction process concerns about concurrency and atomicity.  Concurrency controls guarantee that two users accessing the same data in the database system will not be able to change that data or the user has to wait until the other user has finished processing, before changing that piece of data.  Atomicity controls guarantee that all the steps in transaction are completed successfully as a group. That is, if any steps between the transaction fail, all other steps must fail also.&lt;ref&gt;
[http://technet.microsoft.com/en-us/library/ms187669(v=sql.105).aspx On line Transaction Processing vs. Decision Support]&lt;/ref&gt;

==Systems design==
To build an OLTP system, a designer must know that the large number of concurrent users does not interfere with the system's performance.  To increase the performance of OLTP system, designer must avoid the excessive use of indexes and clusters.

The following elements are crucial for the performance of OLTP systems:&lt;ref&gt;[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]&lt;/ref&gt;
*Rollback segments
:Rollback segments are the portions of database that record the actions of transactions in the event that a transaction is rolled back.  Rollback segments provide read consistency, roll back transactions, and recover the database.&lt;ref&gt;[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76956/rollbak.htm Rollback]&lt;/ref&gt;
*Clusters
:A cluster is a [[Database schema|schema]] that contains one or more tables that have one or more columns in common.  Clustering tables in database improves the performance of [[Join (SQL)|join]] operation.&lt;ref&gt;[http://www.iselfschooling.com/mc4articles/mc4cluster.htm cluster table]&lt;/ref&gt;
*Discrete transactions
:All changes to the data are deferred until the transaction commits during a discrete transaction.  It can improve the performance of short, non-distributed transaction.&lt;ref&gt;[http://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/transac.htm Discrete Transactions]&lt;/ref&gt;
*[[Block (data storage)]] size
:The data block size should be a multiple of the operating system's block size within the maximum limit to avoid unnecessary I/O.&lt;ref&gt;[http://docs.oracle.com/cd/B10500_01/server.920/a96524/c03block.htm Data Block]&lt;/ref&gt;
*[[Buffer cache]] size
:To avoid unnecessary resource consumption, tune [[SQL]] statements to use the database buffer cache.&lt;ref&gt;[http://docs.oracle.com/cd/E16655_01/server.121/e15857/tune_buffer_cache.htm#TGDBA294 Database buffer Cache]&lt;/ref&gt;
*[[Dynamic allocation]] of space to tables and rollback segments
*[[Transaction processing]] monitors and the multi-threaded server
:A transaction processing monitor is used for coordination of services.  It is like an operating system and does the coordination at a high level of granularity and can span multiple computing devices.&lt;ref&gt;[http://c2.com/cgi/wiki?TransactionProcessingMonitor Transaction processing monitor]&lt;/ref&gt;
*[[Partition (database)]]
:Partition increases performance for sites that have regular transactions while still maintain availability and security.&lt;ref&gt;[[Partition (database)|Partition]]&lt;/ref&gt;
*[[Database tuning]]
:With database tuning, OLTP system can maximize its performance as efficiently and rapidly as possible.

==Contrasted to==
*[[Batch processing]]
*[[Grid computing]]

==See also==
*[[On line analytical processing]] (OLAP)
*[[Transaction processing]]
*[[Database transaction]]

==References==
&lt;references /&gt;

==External links==
{{Wiktionary|OLTP}}
*[http://hstore.cs.brown.edu H-Store Project] (architectural and application shifts affecting OLTP performance)
*[http://www.ibm.com/cics IBM CICS official website]
*[http://www.tpc.org/ Transaction Processing Performance Council]
*[http://dbms.knowledgehills.com/What-is-Online-Transaction-Processing-(OLTP)-Schema/a32p2 OLTP Schema]
*[http://www.amazon.com/dp/1558601902 Transaction Processing: Concepts &amp; Techniques Management]

{{Databases}}

{{DEFAULTSORT:On line Transaction Processing}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]</text>
      <sha1>kjl3z3a5dn4a51ynvfwzc5k1artttrj</sha1>
    </revision>
  </page>
  <page>
    <title>WCF Data Services</title>
    <ns>0</ns>
    <id>10988544</id>
    <revision>
      <id>723199784</id>
      <parentid>635858836</parentid>
      <timestamp>2016-06-01T16:41:08Z</timestamp>
      <contributor>
        <username>GrahamHardy</username>
        <id>2956291</id>
      </contributor>
      <minor />
      <comment>- defsort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7369" xml:space="preserve">{{Primary sources|date=November 2010}}
'''[[Windows Communication Foundation|WCF]] Data Services''' (formerly '''ADO.NET Data Services''',&lt;ref&gt;{{cite web|url=http://blogs.msdn.com/astoriateam/archive/2009/11/17/simplifying-our-n-tier-development-platform-making-3-things-1-thing.aspx|title=Simplifying our n-tier development platform: making 3 things 1 thing|date=2009-11-17|accessdate=2009-12-17|publisher=ADO.NET Data Services Team Blog}}&lt;/ref&gt; codename '''"Astoria"''')&lt;ref&gt;{{cite web | url = http://blogs.msdn.com/data/archive/2007/12/10/ado-net-data-services-ctp-released.aspx | title = ADO.NET Data Services CTP Released! | accessdate = 2007-11-12}}&lt;/ref&gt; is a platform for what [[Microsoft]] calls ''Data Services''. It is actually a combination of the runtime and a [[web service]] through which the services are exposed. In addition, it also includes the '''Data Services Toolkit''' which lets Astoria Data Services be created from within [[ASP.NET]] itself. The Astoria project was announced at [[MIX (Microsoft)|MIX]] 2007, and the first developer preview was made available on April 30, 2007. The first [[Software release life cycle#Beta|CTP]] was made available as a part of the [[ASP.NET]] 3.5 Extensions Preview. The final version was released as part of [[Service Pack]] 1 of the [[.NET Framework 3.5]] on August 11, 2008.  The name change from ADO.NET Data Services to WCF data Services was announced at the 2009 [[Professional Developers Conference|PDC]].

==Overview==
WCF Data Services exposes data, represented as [[ADO.NET#Entity Framework|Entity Data Model]] (EDM) objects, via web services accessed over [[HTTP]]. The data can be addressed using a [[Representational State Transfer|REST]]-like [[URI]]. The data service, when accessed via the HTTP GET method with such a URI, will return the data. The web service can be configured to return the data in either plain [[XML]], [[JSON]] or [[Resource Description Framework|RDF+XML]]. In the initial release, formats like [[RSS]] and [[ATOM]] are not supported, though they may be in the future. In addition, using other HTTP methods like PUT, POST or DELETE, the data can be updated as well. POST can be used to create new entities, PUT for updating an entity, and DELETE for deleting an entity.

==Description==

Windows Communication Foundation (WCF) comes to the rescue when we find ourselves not able to achieve what we want to achieve using web services, i.e., other protocols support and even duplex communication. With WCF, we can define our service once and then configure it in such a way that it can be used via HTTP, TCP, IPC, and even Message Queues. We can consume Web Services using server side scripts (ASP.NET), JavaScript Object Notations (JSON), and even REST (Representational State Transfer).

'''Understanding the basics'''

When we say that a WCF service can be used to communicate using different protocols and from different kinds of applications, we will need to understand how we can achieve this. If we want to use a WCF service from an application, then we have three major questions:

'''1.'''Where is the WCF service located from a client's perspective?
'''2.'''How can a client access the service, i.e., protocols and message formats?
'''3.'''What is the functionality that a service is providing to the clients?

Once we have the answer to these three questions, then creating and consuming the WCF service will be a lot easier for us. The WCF service has the concept of endpoints. A WCF service provides endpoints which client applications can use to communicate with the WCF service. The answer to these above questions is what is known as the ABC of WCF services and in fact are the main components of a WCF service. So let's tackle each question one by one.

'''Address:''' Like a webservice, a WCF service also provides a URI which can be used by clients to get to the WCF service. This URI is called as the Address of the WCF service. This will solve the first problem of "where to locate the WCF service?" for us.

'''Binding:''' Once we are able to locate the WCF service, we should think about how to communicate with the service (protocol wise). The binding is what defines how the WCF service handles the communication. It could also define other communication parameters like message encoding, etc. This will solve the second problem of "how to communicate with the WCF service?" for us.

'''Contract:''' Now the only question we are left up with is about the functionalities that a WCF service provides. Contract is what defines the public data and interfaces that WCF service provides to the clients.


The URIs representing the data will contain the physical location of the service, as well as the service name. In addition, it will also need to specify an EDM Entity-Set or a specific entity instance, as in respectively
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection&lt;/nowiki&gt;
or
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection[SomeArtist]&lt;/nowiki&gt;
The former will list all entities in the ''Collection'' set whereas the latter will list only for the entity which is indexed by ''SomeArtist''.

In addition, the URIs can also specify a traversal of a relationship in the Entity Data Model. For example,
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection[SomeSong]/Genre&lt;/nowiki&gt;
traverses the relationship ''Genre'' (in SQL parlance, joins with the ''Genre'' table) and retrieves all instances of ''Genre'' that are associated with the entity ''SomeSong''. Simple predicates can also be specified in the URI, like
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection[SomeArtist]/ReleaseDate[Year eq 2006]&lt;/nowiki&gt;
will fetch the items that are indexed by ''SomeArtist'' and had their ''release'' in ''2006''. Filtering and partition information can also be encoded in the URL as
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection?$orderby=ReleaseDate&amp;$skip=100&amp;$top=50&lt;/nowiki&gt;
It is important to note that although the presence of skip and top keywords indicate paging support, in Data Services version 1 there is no method of determining the number of records available and thus impossible to determine how many pages there may be.  The [[Open Data Protocol|OData]] 2.0 spec adds support for the '''$count''' path segment (to return just a count of entities) and '''$inlineCount''' (to retrieve a page worth of entities and a total count without a separate round-trip....).&lt;ref&gt;http://msdn.microsoft.com/en-us/library/ee373845.aspx&lt;/ref&gt;

==References==
{{Reflist}}
{{Refbegin}}
* {{cite web | url = http://blogs.msdn.com/pablo/archive/2007/04/30/codename-astoria-data-services-for-the-web.aspx | title = Codename "Astoria": Data Services for the Web | accessdate = 2007-04-30}}
* [http://astoria.mslivelabs.com/ ADO.NET Data Services Framework (formerly "Project Astoria")]
{{Refend}}

==External links==
*[http://msdn.microsoft.com/en-us/library/cc907912.aspx Using Microsoft ADO.NET Data Services]
*[http://www.asp.net/downloads/3.5-extensions/ ASP.NET 3.5 Extensions Preview]
*[http://blogs.msdn.com/astoriateam/ ADO.NET Data Services (Project Astoria) Team Blog]
*[http://entmag.com/news/article.asp?EditorialsID=9105 Access Cloud Data with Astoria: ENT News Online]

{{.NET Framework}}

[[Category:Data management]]
[[Category:Web services]]
[[Category:ADO.NET Data Access technologies]]
[[Category:.NET Framework]]</text>
      <sha1>nsmfky1f7eu997ud8fz9vty1knfd3xx</sha1>
    </revision>
  </page>
  <page>
    <title>Data discovery</title>
    <ns>0</ns>
    <id>40008710</id>
    <revision>
      <id>754355898</id>
      <parentid>754355780</parentid>
      <timestamp>2016-12-12T05:53:17Z</timestamp>
      <contributor>
        <ip>220.225.34.211</ip>
      </contributor>
      <comment>/* Players */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2172" xml:space="preserve">'''Data discovery''' is a [[business intelligence]] architecture aimed at creating and using interactive reports and explorable [[data]] from multiple sources. According to the [[United States]] information technology research and advisory firm [[Gartner]] "Data discovery has become a mainstream architecture in 2012".&lt;ref&gt;Kern, J., (2013), [http://www.information-management.com/news/data-discovery-saas-lead-bi-market-review-10024484-1.html Data Discovery, SaaS Lead BI Market Review], ''Information Management''/&lt;/ref&gt;

==Definition==
Data discovery is a user-driven process of searching for patterns or specific items in a data set.  Data discovery applications use visual tools such as geographical maps, pivot-tables, and heat-maps to make the process of finding patterns or specific items rapid and intuitive.  Data discovery may leverage statistical and [[data mining]] techniques to accomplish these goals.

==Data discovery and business intelligence (BI)==
Data discovery is a type of [[business intelligence]] in that they both provide the end-user with an application that visualizes [[data]].  Traditional BI covered dashboards, static and parameterized reports, and pivot tables.  Visualization of data in traditional BI incorporated standard charting, KPIs, and limited graphical representation and interactivity. BI is undergoing transformation in capabilities it offers, with a focus on end-user data analysis and discovery, access to larger volumes of data and an ability to create high fidelity presentations of information. 

==Players==
Data Discovery vendors include: [[Tableau_Software|Tableau]], [[Qlik]],  [[TIBCO_Software|TIBCO Spotfire]], Microsoft Power BI, [[MicroStrategy]], SAP (Lumira), Platfora, Datameer, ClearStory Data, [[AnswerRocket]], and Datawatch.&lt;ref&gt;The Rise of Data Discovery Has Set the Stage for a Major Strategic Shift in the BI and Analytics Platform Market 
15 June 2015 G00277789 
Analyst(s): Josh Parenteau | Rita L. Sallam | Cindi Howson 
&lt;/ref&gt;

==See also==
* [[Business intelligence]]
* [[Business intelligence tools]]

==References==
{{Reflist|30em}}

[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>h5y914vy4swbi06z5d2jng2t77qal1m</sha1>
    </revision>
  </page>
  <page>
    <title>Bright Computing</title>
    <ns>0</ns>
    <id>50639093</id>
    <revision>
      <id>760522769</id>
      <parentid>760234817</parentid>
      <timestamp>2017-01-17T14:59:29Z</timestamp>
      <contributor>
        <username>Ec9exc</username>
        <id>30124234</id>
      </contributor>
      <comment>Corrected misleading claim about OpenStack, accidentally introduced on 21 December</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21981" xml:space="preserve">{{Infobox company
| name = Bright Computing
| logo = 
| logo_size = 
| logo_alt = 
| logo_caption = 
| logo_padding = 
| image = 
| image_size = 
| image_alt = 
| image_caption = 
| type = [[Privately held company|Private]]
| founded = 2009 &lt;!-- if known: {{start date and age|YYYY|MM|DD}} in [[city]], [[state]], [[country]] --&gt;
| founder = {{Unbulleted list|Matthijs van Leeuwen|Alex Ninaber}}
| hq_location = {{Unbulleted list|[[Amsterdam, The Netherlands]]|[[San Jose, California]]}}
| hq_location_city = 
| hq_location_country = 
| area_served = Global &lt;!-- or: | areas_served = --&gt;
| key_people = {{Unbulleted list|Bill Wagner ([[Chief executive officer|CEO]])|Martijn de Vries ([[Chief technology officer|CTO]])|Kristin Hansen ([[Chief marketing officer|CMO]])|Bill Griffin ([[Chief financial officer|CFO]])}}
| industry = [[Enterprise software]]
| products = {{Unbulleted list|Bright Cluster Manager for HPC|Bright Cluster Manager for Big Data|Bright OpenStack}}
| brands = 
| services = 
| former_name = ClusterVision (spin-off) 
| website = {{URL|brightcomputing.com}} &lt;!-- or: | homepage = --&gt;&lt;!-- {{URL|example.com}} --&gt;
}}
'''Bright Computing''', Inc. is a developer of [[software]] for deploying and managing [[Supercomputer|high-performance]] (HPC) clusters, [[big data]] clusters, and [[OpenStack]] in [[data center]]s and using [[cloud computing]].&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2016/02/03/24601/|title=Create Mixed HPC/Big Data Clusters Today Says Bright Computing|date=2016-02-03|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt;

== History ==
Bright Computing was founded by Matthijs van Leeuwen in 2009, who spun the company out of ClusterVision, which he had co-founded with Alex Ninaber and Arijan Sauer. Alex and Matthijs had worked together at UK&#8217;s Compusys, which was one of the first companies to commercially build HPC clusters.&lt;ref&gt;{{Cite web|url=http://www.bloomberg.com/research/stocks/private/person.asp?personId=282144720&amp;privcapId=115561075&amp;previousCapId=115561075&amp;previousTitle=Bright%2520Computing,%2520Inc.|title=Matthijs van Leeuwen: Executive Profile &amp; Biography - Businessweek|website=www.bloomberg.com|access-date=2016-05-24}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt; They left Compusys in 2002 to start ClusterVision in the [[Netherlands]], after determining there was a growing market for building and managing supercomputer clusters using off-the-shelf hardware components and open source software, tied together with their own customized scripts.&lt;ref&gt;{{Cite web|url=http://www.clustervision.com/content/management-team|title=ClusterVision - Europe's Leading HPC and Cloud Specialist|website=ClusterVision|language=en-GB|access-date=2016-05-24}}&lt;/ref&gt; ClusterVision also provided delivery and installation support services for HPC clusters at universities and government entities.&lt;ref&gt;{{Cite web|url=http://clustervision.com/about-the-company/|title=About the Company - ClusterVision|website=ClusterVision|language=en-GB|access-date=2016-05-24}}&lt;/ref&gt;

In 2004, Martijn de Vries joined ClusterVision and began development of cluster management software. The software was made available to customers in 2008, under the name ClusterVisionOS v4.&lt;ref&gt;{{Cite web|url=http://www.beowulf.org/pipermail/beowulf/2008-September/023265.html|title=Roll your own cluster management system with ClusterVisionOS v4 was: [Beowulf] What services do you run on your cluster nodes?|last=holway|first=andrew|date=2008-09-23|access-date=2016-05-24}}&lt;/ref&gt;

In 2009, Bright Computing was spun out of ClusterVision. ClusterVisionOS was renamed Bright Cluster Manager, and van Leeuwen was named Bright Computing&#8217;s CEO.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt;

In 2010, [[ING Group|ING Corporate Investments]] made a $2.5 million investment in Bright Computing. In 2014, [[Draper Fisher Jurvetson]] (DFJ) (US), [[DFJ Esprit]] (UK), Prime Ventures (NL), and [[ING Group|ING Corporate Investments]] invested $14.5 million in Bright Computing. At that time, Bright Computing and ClusterVision were completely separated.&lt;ref&gt;{{Cite web|url=https://www.crunchbase.com/organization/ing-corporate-investments#/entity|title=ING Corporate Investments {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-05-24}}&lt;/ref&gt;

In February 2016, Bright appointed Bill Wagner as chief executive officer. Matthijs van Leeuwen became chief strategy officer and board member.&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2016/02/bright-computing-names-bill-wagner-as-chief-executive-officer/|title=Bright Computing Names Bill Wagner as CEO - insideHPC|date=2016-02-16|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

== Customers  ==
Early customers included [[Boeing]],&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2011/12/boeing-consolidates-on-brite-cluster-manager/|title=Boeing Consolidates on Bright Cluster Manager - insideHPC|date=2011-12-06|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt; [[Sandia National Laboratories]],&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/news/sandia-labs-adopts-bright-cluster-manager|title=Sandia National Laboratories Adopts Bright Cluster Manager to Manage Departmental Clusters|last=Staff|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt; [[Virginia Tech]],&lt;ref&gt;{{Cite web|url=https://finance.yahoo.com/news/virginia-bioinformatics-institute-selects-bright-151500615.html|title=Virginia Bioinformatics Institute Selects Bright Cluster Manager for Big Data to Test New Research Methods|website=Yahoo Finance|access-date=2016-05-24}}&lt;/ref&gt; [[Hewlett Packard Enterprise Services|Hewlett Packard]],&lt;ref&gt;{{Cite web|url=http://www8.hp.com/h20195/v2/GetPDF.aspx/4AA5-2604ENW.pdf|title=HPC Accelerates SMBs|last=|first=|date=|website=|publisher=Hewlett-Packard Development Company, L.P.|access-date=2007-05-24}}&lt;/ref&gt; [[National Security Agency|NSA]], and [[Drexel University]]. Many early customers were introduced through resellers, including SICORP,&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2010/02/bright-computing-and-sicorp-sign-reseller-agreement/|title=Bright Computing And SICORP Sign Reseller Agreement - insideHPC|date=2010-02-22|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt; [[Cray]],&lt;ref&gt;{{Cite web|url=http://www.cray.com/company/collaboration/partners|title=Partner Relationships {{!}} Cray|website=www.cray.com|access-date=2016-05-24}}&lt;/ref&gt; [[Dell]],&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2015/11/11/MN54237|title=Bright Computing Announces Integration with Dell PowerEdge Servers for HPC Environments - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt; Appro, and Advanced HPC.&lt;ref&gt;{{Cite web|url=http://www.advancedhpc.com/high_performance_servers/gpu_computing/bright_computing.html|title=Advanced HPC - GPU Computing - GPU Software - Bright Computing|website=www.advancedhpc.com|access-date=2016-05-24}}&lt;/ref&gt;

By 2014, the company estimated 400 customers, including more than 20 [[Fortune 500]] Companies.&lt;ref&gt;{{Cite web|url=http://www.primeventures.com/portfolio/bright-computing/|title=Prime Ventures - Bright Computing|website=www.primeventures.com|access-date=2016-05-25}}&lt;/ref&gt;

== Products and services ==
Bright Cluster Manager for HPC lets customers deploy and manage complete clusters. It provides management for the hardware, the operating system, the HPC software, and users.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-roll-new-line-products/|title=Bright Computing to Roll Out New Line of Products|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

In 2014, Bright Computing introduced Bright Cluster Manager for [[Apache Hadoop]] clusters.&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/04/03/MN96527|title=Bright Computing Announces Full Support for Apache Hadoop at the Hadoop Summit Europe in Amsterdam - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt; It was later extended to support [[Apache Spark]] and other big data applications, and was renamed, Bright Cluster Manager for Big Data. It can be used as a complete solution or to manage big data software distributions from leading vendors, including [[Cloudera]] and [[Hortonworks]]. It also has several features that allow the combination of Big Data and HPC workloads on the same cluster.&lt;ref&gt;{{Cite web|url=http://www.deskeng.com/de/bright-computing-releases-bright-cluster-manager-7-2/|title=Bright Computing Releases Bright Cluster Manager 7.2|date=2016-01-22|website=Desktop Engineering|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

In 2014, the company announced Bright OpenStack, software to deploy, provision, and manage [[OpenStack]]-based private cloud infrastructures.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-announces-bright-openstack-integration-talligent-openbook-billing-software-openstack-clouds/|title=Bright Computing Announces Bright OpenStack Integration With Talligent Software|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

In January 2016, version 7.2 was released. The updates supported containers using [[Docker (software)|Docker]], improved integration with [[Puppet (software)|Puppet]] and job-based metrics.&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-highlights-version-7-2-bright-cluster-manager-software-solution-enterprisehpc-2016/|title=Bright Computing Highlights Version 7.2 of Bright Cluster Manager Software Solutions at EnterpriseHPC 2016|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

Bright Cluster Manager software is frequently sold through [[original equipment manufacturer]] (OEM) resellers, including Dell and Cray.&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/usa|title=USA|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
Technology partners include:
{{Div col|3}}
* Adaptive Computing&lt;ref&gt;{{Cite web|url=http://www.adaptivecomputing.com/news/adaptive-computing-bright-computing-deepen-product-integration-enhance-provisioning-workflow-optimization-technical-computing-environments/|title=Adaptive Computing and Bright Computing Deepen Product Integration to Enhance Provisioning and Workflow Optimization in Technical Computing Environments - Adaptive Computing|date=2014-06-24|website=Adaptive Computing|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* Allinea&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2015/09/bright-computing-collaborates-on-openhpec-accelerator-suite/|title=Bright Computing Collaborates on OpenHPEC Accelerator Suite|date=2015-09-16|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Altair Engineering|Altair]]&lt;ref&gt;{{Cite web|url=http://www.prweb.com/releases/2011/9/prweb8788932.htm|title=Bright Computing Now Resells Altair PBS Professional Workload Manager, Delivering HPC Operational Efficiencies and Cost Savings|website=PRWeb|access-date=2016-05-24}}&lt;/ref&gt;
* [[Amazon Web Services|Amazon]]&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2011/11/08/bright_computing_bursts_into_cloud/|title=Bright Computing Bursts Into Cloud|date=2011-11-08|website=HPCwire|access-date=2016-05-24}}&lt;/ref&gt;
* [[Ansys]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-ansys|title=Technology Partner - Ansys|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Cavium]]&lt;ref&gt;{{Cite web|url=http://www.cavium.com/newsevents_Cavium-Collaboration-with-BrightComputing.html|title=Cavium Announces Collaboration with Bright Computing to Support the ThunderX Processor Family|website=www.cavium.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Cisco Systems|Cisco]]&lt;ref&gt;{{Cite web|url=https://gigaom.com/2014/07/28/bright-computing-takes-in-14-5m-to-push-its-cluster-management-smarts/|title=Bright Computing takes in $14.5M to push its cluster management smarts|last=Darrow|first=Barb|date=2014-07-28|website=gigaom.com|access-date=2016-05-24}}&lt;/ref&gt;
* Cloudera&lt;ref&gt;{{Cite web|url=http://insidebigdata.com/2014/04/10/bright-computing-achieves-cloudera-certification-bright-cluster-manager/|title=Bright Computing Achieves Cloudera Certification for Bright Cluster Manager|last=Brueckner|first=Rich|date=2014-04-10|website=insideBIGDATA|access-date=2016-05-24}}&lt;/ref&gt;
* Cray&lt;ref&gt;{{Cite web|url=https://cug.org/proceedings/cug2015_proceedings/includes/files/pap176-file2.pdf|title=Cray User Group - Bright Cluster Manager Presentation|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;
* DDN Storage&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-ddn|title=Technology Partner - DDN|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Dell&lt;ref&gt;{{Cite web|url=http://insidehpc.com/2015/12/dellbright/|title=Bright Cluster Manager Integrates with Dell PowerEdge Servers for HPC Environments - insideHPC|date=2015-12-09|website=insideHPC|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Fujitsu]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-fujitsu|title=Technology Partner - Fujitsu|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Hewlett Packard Enterprise&lt;ref&gt;{{Cite web|url=http://www.datacenterknowledge.com/archives/2014/07/29/bright-cluster-management-raises-dough/|title=Bright Networks Cluster Management Gets $14.5m Round|date=2014-07-29|website=Data Center Knowledge|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* Hortonworks&lt;ref&gt;{{Cite web|url=http://hortonworks.com/partner/bright-computing/|title=Bright Computing partners with Hortonworks for Hadoop|website=Hortonworks|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Huawei]]&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/09/16/MN12537|title=Huawei signs global reseller agreement with Bright Computing - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt;
* [[IBM]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-ibm|title=Technology Partner - IBM|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Inspur]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-inspur|title=Technology Partner - Inspur|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Intel]]&lt;ref&gt;{{Cite web|url=https://ctovision.com/2015/08/bright-computing-integrated-intel-enterprise-edition-lustre/|title=Bright Computing Integrated with Intel Enterprise Edition for Lustre - CTOvision.com|date=2015-08-13|website=CTOvision.com|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* [[Lenovo]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-lenovo|title=Technology Partner - Lenovo|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Magma&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-magma|title=Technology Partner - Magma|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Mellanox Technologies]]&lt;ref&gt;{{Cite web|url=http://www.openstack.org/news/view/214/bright-computing-announces-bright-openstack%25E2%2584%25A2-integration-with-mellanox-technologies-infiniband-switches-and-virtual-extensible-lan-(vxlan)-offloading|title=&#187; OpenStack Open Source Cloud Computing Software|website=www.openstack.org|access-date=2016-05-24}}&lt;/ref&gt;
* [[Microsoft]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-microsoft|title=Technology Partner - Microsoft|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* NICE&lt;ref&gt;{{Cite web|url=https://www.nice-software.com/partners/bright-computing|title=Bright Computing - NICE|website=www.nice-software.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Nvidia|NVidia]]&lt;ref&gt;{{Cite web|url=https://developer.nvidia.com/bright-cluster-manager|title=Bright Cluster Manager|date=2012-01-12|website=NVIDIA Developer|access-date=2016-05-24}}&lt;/ref&gt;
* [[Red Hat|Redhat]]&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-redhat|title=Technology Partner - RedHat|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* Runtime Design Automation&lt;ref&gt;{{Cite web|url=http://www.brightcomputing.com/technology-partner-runtime|title=Technology Partner - Runtime|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Silicon Graphics International|SGI]]&lt;ref&gt;{{Cite web|url=https://www.sgi.com/partners/technology/partners.html|title=SGI - Partners: Technology Partners: SGI Partnerships|website=www.sgi.com|access-date=2016-05-24}}&lt;/ref&gt;
* [[Supermicro]]&lt;ref&gt;{{Cite web|url=http://www.montana.edu/rci/HyaliteCluster.html|title=Hyalite Cluster - Research Computing Group {{!}} Montana State University|website=www.montana.edu|access-date=2016-05-24}}&lt;/ref&gt;
* [[SUSE]]&lt;ref&gt;{{Citation|last=SUSE|title=Bright Computing and SUSE share common values system|date=2015-10-29|url=https://www.youtube.com/watch?v=4WwAKlOBhBo|accessdate=2016-05-24}}&lt;/ref&gt;
* [[Taligent]]&lt;ref&gt;{{Cite web|url=https://finance.yahoo.com/news/bright-computing-announces-bright-openstack-073000503.html|title=Bright Computing Announces Bright OpenStack&#8482; Integration with Talligent Openbook Billing Software for OpenStack Clouds|website=Yahoo Finance|access-date=2016-05-24}}&lt;/ref&gt;
* [[Univa]]&lt;ref&gt;{{Cite web|url=http://www.prweb.com/releases/2012/10/prweb9961985.htm|title=Bright Computing and Univa Offer Combined Cluster &amp; Workload Management Solution|website=PRWeb|access-date=2016-05-24}}&lt;/ref&gt;
{{Div col end}}

Bright Computing was covered by [[Software Magazine]]&lt;ref&gt;{{Cite web|url=http://www.softwaremag.com/bright-computing-releases-version-7-2-of-bright-cluster-manager-for-hpc-bright-cluster-manager-for-big-data-and-bright-openstack/|title=Bright Computing Releases Version 7.2 of Bright Cluster Manager for HPC, Bright Cluster Manager for Big Data, and Bright OpenStack &#8211;|website=www.softwaremag.com|access-date=2016-05-24}}&lt;/ref&gt; and [[Yahoo! Finance]],&lt;ref&gt;{{Cite web|url=https://finance.yahoo.com/news/bright-computing-showcase-bright-openstack-162500598.html|title=Bright Computing to Showcase Bright OpenStack&#8482; and HPC Cluster-as-a-Service (CaaS) at 2016 OpenStack Summit in Austin|website=Yahoo Finance|access-date=2016-05-24}}&lt;/ref&gt; among other publications.

== Awards ==
In 2016, Bright Computing was awarded a &#8364;1.5M [[Horizon 2020]] SME Instrument grant from the [[European Commission]].&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-receives-horizon-2020-grant-european-commission/|title=Bright Computing Receives Horizon 2020 Grant From European Commission|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

Bright Computing was one of only 33 grant recipients from 960 submitted proposals.&lt;ref&gt;{{Cite web|url=https://ec.europa.eu/easme/en/horizons-2020-sme-instrument|title=Horizon 2020's SME Instrument - EASME - European Commission|website=EASME|access-date=2016-05-24}}&lt;/ref&gt; In its category only 5 out of 260 grants were awarded.&lt;ref&gt;{{Cite web|url=https://ec.europa.eu/digital-single-market/en/open-disruptive-innovation-0|title=Open Disruptive Innovation - Digital Single Market - European Commission|website=Digital Single Market|access-date=2016-05-24}}&lt;/ref&gt;

* 2015 HPCwire Editor&#8217;s Choice Award for &#8220;Best HPC Cluster Solution or Technology."&lt;ref&gt;{{Cite web|url=http://www.hpcwire.com/2015-hpcwire-readers-choice-awards/|title=2015 HPCwire Awards &#8211; Readers&#8217; &amp; Editors&#8217; Choice - HPCwire|website=HPCwire|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;
* Main Software 50 &#8220;Highest Growth&#8221; award winner, 2013.&lt;ref&gt;{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2013/11/14/MN16669|title=Bright Computing Wins Main Software 50 "Highest Growth" Award - The Business Journals|website=The Business Journals|access-date=2016-05-24}}&lt;/ref&gt;
* [[Deloitte]] Technology Fast50 &#8220;Rising Star 2013&#8221; Award Winner.&lt;ref&gt;{{Cite web|url=http://www2.deloitte.com/be/en/pages/about-deloitte/articles/ayden-wins-fast50.html|title=Adyen wins Deloitte Technology Fast50 {{!}} Deloitte Belgium {{!}} TMT {{!}} News, press release|website=Deloitte Belgium|access-date=2016-05-24}}&lt;/ref&gt;
* Bio-IT World Conference &amp; Expo &#8216;13, Boston, MA, Winner of &#8220;IT Hardware &amp; Infrastructure&#8221; category of the &#8220;Best of Show Award&#8221; program.&lt;ref&gt;{{Cite web|url=http://www.bio-itworld.com/2013/4/10/2013-bio-it-world-best-show-winners-named.html|title=2013 Bio-IT World Best of Show Winners Named - Bio-IT World|access-date=2016-05-24}}&lt;/ref&gt;
* [[Red Herring (magazine)|Red Herring]] Top 100 Global Award, 2013.&lt;ref&gt;{{Cite web|url=http://www.redherring.com/events/rhna/2013-rhnawinners/|title=2013 Top 100 North America: Winners &#8212; Red Herring|website=Red Herring|language=en-US|access-date=2016-05-24}}&lt;/ref&gt;

== References ==
{{reflist|30em}}

==Further reading==
* Morgan, Timothy Prickett (June 20, 2011). [http://www.theregister.co.uk/2011/06/20/bright_cluster_manager_5_2/ "Bright Computing revs up cluster manager"]. ''[[The Register]]''.
* Morgan, Timothy Prickett (November 8, 2011). [http://www.theregister.co.uk/2011/11/08/bright_cluster_manager_amazon_cloud_bursting/ "Bright Computing bursts HPC to EC2 clouds"]. ''The Register''.

[[Category:Big data companies]]
[[Category:Cloud computing]]
[[Category:Cloud infrastructure]]
[[Category:Cluster computing]]
[[Category:Data management]]
[[Category:Supercomputers]]</text>
      <sha1>7xmi6fj4y2mx9qedyusntrkapx8e4uy</sha1>
    </revision>
  </page>
  <page>
    <title>Digital obsolescence</title>
    <ns>0</ns>
    <id>934683</id>
    <revision>
      <id>758142348</id>
      <parentid>758141932</parentid>
      <timestamp>2017-01-03T18:47:13Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <minor />
      <comment>fixed broken (duplicate) reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8857" xml:space="preserve">{{Outdated|date=January 2015}}[[File:VCF 2010 Domesday tray open.jpg|thumb|300px|A Domesday Project machine with its modified [[Laserdisc]]. The Domesday Project was published in 1986.]]
'''Digital obsolescence''' is a situation where a digital resource is no longer readable because of its archaic format: the physical media, the reader (required to read the media), the hardware, or the software that runs on it is no longer available.&lt;ref name='national-archives'&gt;{{cite web | last = | first = | authorlink = | title =Managing Digital Obsolescence Risks | work = | publisher =The National Archives | date =April 2009 | url = http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf| format = pdf| doi = | accessdate = | archiveurl =http://webarchive.nationalarchives.gov.uk/+/http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf | archivedate = 28 Jun 2011}}&lt;/ref&gt; 

A prime example of this is the [[BBC Domesday Project]] from the 1980s, although its data was eventually recovered after a significant amount of effort. [[Cornell University]] Library&#8217;s [http://www.icpsr.umich.edu/dpm/dpm-eng/eng_index.html digital preservation tutorial] (now hosted by [[ICPSR]]) has a timeline of obsolete media formats, called the [http://www.icpsr.umich.edu/dpm/dpm-eng/oldmedia/index.html &#8220;Chamber of Horrors&#8221;], that shows how rapidly new technologies are created and cast aside.

==Introduction==
The rapid evolution and proliferation of different kinds of [[computer hardware]], modes of digital encoding, [[operating systems]] and general or specialized [[software]] ensures that digital obsolescence will become a problem in the future.&lt;ref&gt;Rothenberg, J. (1998). [http://www.clir.org/pubs/reports/rothenberg/introduction.html#longevity Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation]&lt;/ref&gt; Many versions of word-processing programs, data-storage media, standards for encoding images and films are considered "standards" for some time, but in the end are always replaced by new versions of the software or completely new hardware. Files meant to be read or edited with a certain program (for example [[Microsoft Word]]) will be unreadable in other programs, and as operating systems and hardware move on, even old versions of programs developed by the same company become impossible to use on the new platform (for instance, older versions of [[Microsoft Works]], before Works 4.5, cannot be run under [[Windows 2000]] or later). 

Early attention was brought to the challenges of preserving [[machine-readable data]] by the work of [[Charles M Dollar]] in the 1970s, but it was only during the 1990s that libraries and archives came to appreciate the significance of the problem&lt;ref&gt;Hedstrom, M. (1995). [http://www.uky.edu/%7Ekiernan/DL/hedstrom.html Digital Preservation: A Time Bomb for Digital Libraries]&lt;/ref&gt; and has been discussed among professionals in those branches, though so far without any obvious solutions other than continual forward-migration of files and information to the latest data-storage standards. File formats should be widespread, backward compatible, often upgraded, and, ideally, open format. In 2002, the National Initiative for a Networked Cultural Heritage cited&lt;ref&gt;National Initiative for a Networked Cultural Heritage. (2002). [http://www.nyu.edu/its/humanities/ninchguide/V/ NINCH Guide to Good Practice in the Digital Representation and Management of Cultural Heritage Materials]&lt;/ref&gt; the following as &#8220;de facto&#8221; formats that are unlikely to be rendered obsolete in the near future:  uncompressed [[TIFF]]  and [[ASCII]] and [[Rich Text Format|RTF]] (for text).

In order to prevent this from happening, it is important that an institution regularly evaluate and explore its current technologies and evaluate its long term business model.&lt;ref name='national-archives'/&gt;

== Types ==
Digital objects are vulnerable to three types of obsolescence:&lt;ref&gt;{{cite web|publisher=National Archives of Australia|title=Obsolescence &#8211; a key challenge in the digital age|url=http://www.naa.gov.au/records-management/agency/preserve/e-preservation/obsolescence.aspx|accessdate=17 March 2014}}&lt;/ref&gt;
# '''Physical media''': the physical carrier of the digital file becomes obsolete; e.g. 8 inch floppy disks, which are no longer commercially available. 
# '''Hardware''': the hardware needed to access the digital file becomes obsolete; e.g. floppy disk drive, which computers are no longer manufactured with.
# '''Software''': the software needed to access the digital file becomes obsolete; e.g. [[WordStar]], a word processor popular in the 1980s which used a [[Open data|closed data]] format and is no longer readily available.

== Strategies ==
Any organization that has digital records should assess its records to identify any potential risks for file format obsolescence. The Library of Congress maintains [http://www.digitalpreservation.gov/formats/intro/intro.shtml Sustainability of Digital Formats], which includes technical details about many different format types. The UK National Archives maintains an online registry of file formats called [http://www.nationalarchives.gov.uk/PRONOM/Default.aspx PRONOM].

In its 2014 agenda, the National Digital Stewardship Alliance recommended developing File Format Action Plans: "it is important to shift from more abstract considerations about file format obsolescence to develop actionable strategies for monitoring and mining information about the heterogeneous digital files the organizations are managing."&lt;ref&gt;{{cite web|publisher=National Digital Stewardship Alliance|title=National Agenda for Digital Stewardship 2014|url=http://www.digitalpreservation.gov/ndsa/documents/2014NationalAgenda.pdf|accessdate=17 March 2014|date=2014}}&lt;/ref&gt; 

File Format Action Plans are documents internal to an organization which list the type of digital files in its holdings and assess what actions should be taken to ensure its ongoing accessibility.&lt;ref&gt;{{cite web|last=Owens|first=Trevor|title=File Format Action Plans in Theory and Practice|url=http://blogs.loc.gov/digitalpreservation/2014/01/file-format-action-plans-in-theory-and-practice/|accessdate=17 March 2014|date=6 January 2014}}&lt;/ref&gt;  Examples include the [http://fclaweb.fcla.edu/node/795 Florida Digital Archive Action Plan] and University of Michigan's [http://deepblue.lib.umich.edu/static/about/deepbluepreservation.html Deep Blue Preservation and Format Support Policy].

== Copyright issues ==
Untangling [[copyright]] issues also presented a significant challenge for projects attempting to overcome the obsolescence issues related to the BBC Domesday Project. In addition to copyright surrounding the many contributions made by the estimated 1 million people who took part in the project, there are also copyright issues that relate to the technologies employed. It is likely that the Domesday Project will not be completely free of copyright restrictions until at least 2090, unless copyright laws are revised for earlier [[Copyright term|expiration]] of software into [[Public domain software|public domain]].&lt;ref&gt;{{Cite web |url=http://www2.si.umich.edu/CAMILEON/reports/IPRreport.doc|title= The CAMiLEON Project: Legal issues arising from the work aiming to preserve elements of the interactive multimedia work entitled "The BBC Domesday Project."|first= Andrew|last= Charlesworth|date= 5 November 2002|publisher= Information Law and Technology Unit, University of Hull|location= Kingston upon Hull|format= Microsoft Word|accessdate= 23 March 2011}}&lt;/ref&gt;

==Intentional obsolescence==
In some cases, obsolete technologies are used in a deliberate attempt to avoid data intrusion in a strategy known as "[[security through obsolescence]]".&lt;ref&gt;{{cite news | url=http://www.linux.com/articles/23313 | title=Security through obsolescence | first=Robin | last=Miller | publisher=Linux.com | date=2002-06-06 | accessdate=2008-07-18}}&lt;/ref&gt;

==See also==
* [[Obsolescence]]
* [[Digital preservation]]
* [[Digital Dark Age]]
* [[CAMiLEON]]
* [[Emulation (computing)]]
* [[M-DISC]]

==References==
{{reflist|30em}}

==External links==
* [http://www.digitalpreservation.gov/formats/ The Library of Congress, Sustainability of Digital Formats]
* [[Wired Magazine]]: [http://wired-vig.wired.com/wired/archive/6.09/saved.html What death can't destroy and how to digitize it]
* [https://www.icpsr.umich.edu/icpsrweb/content/datamanagement/preservation/ Digital Preservation at ICPSR]

{{DigitalPreservation}}
{{DEFAULTSORT:Digital Obsolescence}}
[[Category:Data management]]
[[Category:Digital libraries]]
[[Category:Digital preservation]]
[[Category:Future problems]]
[[Category:Obsolescence]]
[[Category:Records management]]</text>
      <sha1>o35mx4ql30kgkhu9nv103f3ki883xq6</sha1>
    </revision>
  </page>
  <page>
    <title>Physical data model</title>
    <ns>0</ns>
    <id>1030540</id>
    <revision>
      <id>749352529</id>
      <parentid>738527426</parentid>
      <timestamp>2016-11-13T22:29:58Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4043" xml:space="preserve">{{Refimprove|date=April 2008}}
[[File:Physical Data Model Options.jpg|thumb|320px|Physical Data Model Options.&lt;ref name="WH05"&gt;[http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF FEA Consolidated Reference Model Document]. whitehouse.gov May 2005. p.91.  {{webarchive |url=https://web.archive.org/web/20100705040628/http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF |date=July 5, 2010 }}&lt;/ref&gt;]]

A '''physical data model''' (or '''[[database design]]''') is a representation of a data design as implemented, or intended to be implemented, in a [[database management system]].  In the [[Project lifecycle | lifecycle of a project]] it typically derives from a [[logical data model]], though it may be [[reverse-engineer]]ed from a given [[database]] implementation.  A complete physical data model will include all the [[database artifact]]s required to create [[relationships between table]]s or to achieve performance goals, such as [[index (database)|index]]es, constraint definitions, linking tables, [[partitioned table]]s or [[cluster (computing)|cluster]]s.  Analysts can usually use a  physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system.

{{As of | 2012}} seven main databases dominate the commercial marketplace: [[Informix Dynamic Server|Informix]], [[Oracle Database|Oracle]], [[PostgreSQL|Postgres]], [[Microsoft SQL Server|SQL Server]], [[Sybase]], [[IBM DB2|DB2]] and [[MySQL]]. Other RDBMS systems tend either to be legacy databases or used within academia such as universities or further education colleges. Physical data models for each implementation would differ significantly, not least due to underlying [[operating system | operating-system]] requirements that may sit underneath them.  For example: SQL Server runs only on [[Microsoft Windows]] operating-systems, while Oracle and MySQL can run on Solaris, Linux and other UNIX-based operating-systems as well as on Windows. This means that the disk requirements, security requirements and many other aspects of a physical data model will be influenced by the RDBMS that a [[database administrator]] (or an organization) chooses to use.

==Physical schema==
''Physical schema'' is a term used in [[data management]] to describe how [[data]] is to be represented and stored (files, indices, ''et al.'') in [[secondary storage]] using a particular [[database management system]] (DBMS) (e.g., Oracle RDBMS, Sybase SQL Server, etc.).

In the [[ANSI-SPARC Architecture|ANSI/SPARC Architecture]] [[three schema approach]], the ''internal schema'' is the view of data that involved data management technology.  This is as opposed to an ''external schema'' that reflects an individual's view of the data, or the ''[[conceptual schema]]'' that is the integration of a set of external schemas.

Subsequently{{Citation needed|date=June 2012}} the internal schema was recognized to have two parts:

The [[logical schema]] was the way data were represented to conform to the constraints of a particular approach to database management.  At that time the choices were hierarchical and network.  Describing the logical schema, however, still did not describe how physically data would be stored on disk drives.  That is the domain of the ''physical schema''.  Now logical schemas describe data in terms of relational ''tables and columns'', object-oriented ''classes'', and [[XML]] ''tags''.

A single set of tables, for example, can be implemented in numerous ways, up to and including an architecture where table rows are maintained on computers in different countries.

==See also==
*[[Database schema]]
*[[Logical schema]]

==References==
{{Reflist}}

{{DEFAULTSORT:Physical Data Model}}
[[Category:Data modeling]]
[[Category:Data management]]
[http://www.whitehouse.gov/sites/default/files/omb/assets/fea_docs/FEA_CRM_v23_Final_Oct_2007_Revised.pdf FEA Consolidated Reference Model Document] (whitehouse.gov) Oct 2007.

[[ja:&#12473;&#12461;&#12540;&#12510; (&#12487;&#12540;&#12479;&#12505;&#12540;&#12473;)]]</text>
      <sha1>g8bfysxvwtxvpkswduzugck0czz8erz</sha1>
    </revision>
  </page>
  <page>
    <title>Data</title>
    <ns>0</ns>
    <id>18985040</id>
    <revision>
      <id>762555663</id>
      <parentid>762555662</parentid>
      <timestamp>2017-01-29T14:33:47Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/41.48.16.44|41.48.16.44]] to version by LilyKitty. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2914878) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11607" xml:space="preserve">{{about||data in computer science|Data (computing)|other uses}}
{{pp-move-indef}}
[[File:Data types - en.svg|thumb|right|200px|Some of the different types of data.]]
'''Data''' ({{IPAc-en|&#712;|d|e&#618;|t|&#601;}} {{respell|DAY|t&#601;}}, {{IPAc-en|&#712;|d|&#230;|t|&#601;}} {{respell|DA|t&#601;}}, or {{IPAc-en|&#712;|d|&#593;&#720;|t|&#601;}} {{respell|DAH|t&#601;}})&lt;ref&gt;The pronunciation {{IPAc-en|&#712;|d|e&#618;|t|&#601;}} {{respell|DAY|t&#601;}} is widespread throughout most varieties of English. The pronunciation {{IPAc-en|&#712;|d|&#230;|t|&#601;}} {{respell|DA|t&#601;}} is chiefly [[Hiberno-English|Irish]] and [[American English|North American]]. The pronunciation {{IPAc-en|&#712;|d|&#593;&#720;|t|&#601;}} {{respell|DAH|t&#601;}} is chiefly [[Australian English|Australian]], [[New Zealand English|New Zealand]] and [[South African English|South African]]. Each pronunciation may be realized differently depending on the dialect/language of the speaker.&lt;/ref&gt; is a [[set (mathematics)|set]] of values of [[Qualitative data|qualitative]] or [[Quantitative data|quantitative]] variables. An example of qualitative data would be an [[anthropologist]]'s handwritten notes about her interviews with people of an Indigenous tribe. Pieces of data are individual pieces of [[information]]. While the concept of data is commonly associated with [[scientific research]], data is collected by a huge range of organizations and institutions, including businesses (e.g., sales data, revenue, profits, [[stock price]]), governments (e.g., [[crime rate]]s, [[unemployment rate]]s, [[literacy]] rates) and non-governmental organizations (e.g., censuses of the number of [[homelessness|homeless people]] by non-profit organizations).

Data is [[measurement|measured]], [[data reporting|collected and reported]], and [[data analysis|analyzed]], whereupon it can be [[data visualization|visualized]] using graphs, images or other analysis tools. Data as a general [[concept]] refers to the fact that some existing [[information]] or [[knowledge]] is ''[[Knowledge representation and reasoning|represented]]'' or ''[[code]]d'' in some form suitable for better usage or [[data processing|processing]]. ''[[Raw data]]'' ("unprocessed data") is a collection of [[number]]s or [[character (computing)|characters]] before it has been "cleaned" and corrected by researchers. Raw data needs to be corrected to remove [[outlier]]s or obvious instrument or data entry errors (e.g., a thermometer reading from an outdoor Arctic location recording a tropical temperature).  Data processing commonly occurs by stages, and the "processed data" from one stage may be considered the "raw data" of the next stage. [[Field work|Field data]] is raw data that is collected in an uncontrolled "[[in situ]]" environment. [[Experimental data]] is data that is generated within the context of a scientific investigation by observation and recording. Data has been described as the new [[Petroleum|oil]] of the [[digital economy]].&lt;ref&gt;[https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Data Is the New Oil of the Digital Economy]&lt;/ref&gt;&lt;ref&gt;[https://spotlessdata.com/blog/data-new-oil Data is the new Oil]&lt;/ref&gt;

== Etymology and terminology ==
The first English use of the word "data" is from the 1640s. Using the word "data" to mean "transmittable and storable computer information" was first done in 1946. The expression "data processing" was first used in 1954.&lt;ref name="eol"&gt;http://www.etymonline.com/index.php?term=data&lt;/ref&gt;

The [[Data (word)|Latin word ''data'']] is the plural of ''datum'', "(thing) given," neuter past participle of ''dare'' "to give".&lt;ref name="eol"/&gt;  Data may be used as a plural noun in this sense, with some writers in the 2010s using ''datum'' in the singular and ''data'' for plural. In the 2010s, though, in non-specialist, everyday writing, "data" is most commonly used in the singular, as a [[mass noun]] (like "information", "sand" or "rain").&lt;ref&gt;{{cite web|last=Hickey |first=Walt |url=http://fivethirtyeight.com/datalab/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/ |title=Elitist, Superfluous, Or Popular? We Polled Americans on the Oxford Comma |publisher=FiveThirtyEight |date=2014-06-17 |accessdate=2015-05-04}}&lt;/ref&gt;

== Meaning ==
Data, [[information]], [[knowledge]] and [[wisdom]] are closely related concepts, but each has its own role in relation to the other, and each term has its own meaning. Data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. &lt;ref&gt;{{cite web|title=Joint Publication 2-0, Joint Intelligence|url=http://www.dtic.mil/doctrine/new_pubs/jp2_0.pdf|work=Defense Technical Information Center (DTIC)|publisher=Department of Defense|accessdate=February 22, 2013|pages=GL-11|date=22 June 2007}}&lt;/ref&gt; [[Knowledge]] is derived from extensive amounts of experience dealing with information on a subject. For example, the height of [[Mount Everest]] is generally considered data. The height can be recorded precisely with an [[altimeter]] and entered into a database. This data may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to make a decision about the best method to climb it. Using an understanding based on experience climbing mountains to advise persons on the way to reach Mount Everest's peak may be seen as "knowledge". Some complement the series "data", "information" and "knowledge" with "wisdom", which would mean the status of a person in possession of a certain "knowledge" who also knows under which circumstances is good to use it.

Data is the least abstract concept, information the next least, and knowledge the most abstract.&lt;ref&gt;{{cite web|author=Akash Mitra|year=2011|title=Classifying data for successful modeling|url=http://www.dwbiconcepts.com/data-warehousing/12-data-modelling/101-classifying-data-for-successful-modeling.html}}&lt;/ref&gt; Data becomes information by interpretation; e.g., the height of Mount Everest is generally considered "data", a book on Mount Everest geological characteristics may be considered "information", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered "knowledge". "Information" bears a diversity of meanings that ranges from everyday usage to technical use. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation.&lt;!--given by nupur seth--&gt; Beynon-Davies uses the concept of a [[sign]] to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.&lt;ref&gt;{{cite book|author=P. Beynon-Davies|year=2002|title=Information Systems: An introduction to  informatics in organisations|publisher=[[Palgrave Macmillan]] |location=Basingstoke, UK|isbn=0-333-96390-3}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author=P. Beynon-Davies|year=2009|title=Business information systems|publisher=Palgrave |location=Basingstoke, UK|isbn=978-0-230-20368-6}}&lt;/ref&gt;

Before the development of computing devices and machines, only people could collect data and impose patterns on it. Since the development of computing devices and machines, these devices can also collect data. In the 2010s, computers are widely used in many fields to collect data and sort or process it, in disciplines ranging from [[marketing]], analysis of [[social services]] usage by citizens to scientific research. These patterns in data are seen as information which can be used to enhance knowledge. These patterns may be interpreted as "[[truth]]" (though "truth" can be a subjective concept), and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken.&lt;ref&gt;{{cite book|author=Sharon Daniel|title=The Database: An Aesthetics of Dignity}}&lt;/ref&gt;

Mechanical computing devices are classified according to the means by which they represent data. An [[analog computer]] represents a datum as a voltage, distance, position, or other physical quantity. A [[Computer|digital computer]] represents a piece of data as a sequence of symbols drawn from a fixed [[alphabet]]. The most common digital computers use a binary alphabet, that is, an alphabet of two characters, typically denoted "0" and "1". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A [[computer program]] is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably [[Lisp (programming language)|Lisp]] and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish [[metadata]], that is, a description of other data. A similar yet earlier term for metadata is "ancillary data."  The prototypical example of metadata is the library catalog, which is a description of the contents of books.

== In other fields ==
Though data is also increasingly used in other fields, it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as "given". Peter Checkland introduced the term ''capta'' (from the Latin ''capered'', &#8220;to take&#8221;) to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented.&lt;ref&gt;{{cite book | author = P. Checkland and S. Holwell | title = Information, Systems, and Information Systems: Making Sense of the Field. | year = 1998 | publisher = John Wiley &amp; Sons | location = Chichester, West Sussex | isbn = 0-471-95820-4 | pages = 86&#8211;89  }}&lt;/ref&gt; [[Johanna Drucker]] has argued that since the humanities affirm knowledge production as "situated, partial, and constitutive," using ''data'' may introduce assumptions that are counterproductive, for example that phenomena are discrete or are observer-independent.&lt;ref&gt;{{cite web
 |author=Johanna Drucker
 |year=2011
 |title=Humanities Approaches to Graphical Display
 |url=http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html
}}&lt;/ref&gt; The term ''capta'', which emphasizes the act of observation as constitutive, is offered as an alternative to ''data'' for visual representations in the humanities.

== See also ==
{{div col|5}}
* [[Biological data]]
* [[Data acquisition]]
* [[Data analysis]]
* [[Data cable]]
* [[Dark data]]
* [[Data domain]]
* [[Data element]]
* [[Data farming]]
* [[Data governance]]
* [[Data integrity]]
* [[Data maintenance]]
* [[Data management]]
* [[Data mining]]
* [[Data modeling]]
* [[Data visualization]]
* [[Computer data processing]]
* [[Data publication]]
* [[Information privacy|Data protection]]
* [[Data remanence]]
* [[Data set]]
* [[Data warehouse]]
* [[Database]]
* [[Datasheet]]
* [[Environmental data rescue]]
* [[Fieldwork]]
* [[Metadata]]
* [[Open data]]
* [[Scientific data archiving]]
* [[Statistics]]
* [[Computer memory]]
* [[Data structure]]
* [[Raw Data]]
* [[Secondary Data]]
{{div col end}}

== References ==
{{FOLDOC}}
{{Reflist}}

== External links ==
{{Wiktionary}}
* [http://purl.org/nxg/note/singular-data Data is a singular noun] (a detailed assessment)

{{Statistics}}

[[Category:Computer data| ]]
[[Category:Data| ]]
[[Category:Data management]]</text>
      <sha1>g0jivpy60x98e6s0l0iweihoxpcsqfn</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Statistical data agreements</title>
    <ns>14</ns>
    <id>24105895</id>
    <revision>
      <id>753884296</id>
      <parentid>720923276</parentid>
      <timestamp>2016-12-09T18:41:46Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="93" xml:space="preserve">[[Category:Statistical data|Agreements]]
[[Category:Agreements]]
[[Category:Data management]]</text>
      <sha1>fehtp2uw6tomdlwa1epm5pqevh40c1p</sha1>
    </revision>
  </page>
  <page>
    <title>Data storage device</title>
    <ns>0</ns>
    <id>28174</id>
    <revision>
      <id>762116593</id>
      <parentid>762116376</parentid>
      <timestamp>2017-01-26T19:23:54Z</timestamp>
      <contributor>
        <ip>86.153.3.18</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6307" xml:space="preserve">[[File:PersonalStorageDevices.agr.jpg|thumb|Many different consumer electronic devices can store data.]]
[[File:EdisonPhonograph.jpg|thumb|Edison cylinder phonograph ca. 1899. The phonograph cylinder is a storage medium. The phonograph may be considered a storage device.]]
[[File:Reel-to-reel recorder tc-630.jpg|thumb|On a reel-to-reel tape recorder (Sony TC-630), the recorder is data storage equipment and the magnetic tape is a data storage medium.]]
[[File:RNA-comparedto-DNA thymineAndUracilCorrected.png|thumb|upright|[[RNA]] might be the oldest [[data]] storage medium.&lt;ref&gt;{{cite journal|title=The RNA World|journal=[[Nature (journal)|Nature]]|first=Walter|last=Gilbert|authorlink=Walter Gilbert|date=Feb 1986|pages=618|volume=319|doi=10.1038/319618a0|issue=6055|bibcode=1986Natur.319..618G}}&lt;/ref&gt;]]

A '''data storage device''' is a device for [[recording]] (storing) [[information]] (data). Recording can be done using virtually any form of [[energy]], spanning from manual muscle power in [[handwriting]], to acoustic vibrations in [[phonograph]]ic recording, to electromagnetic energy modulating [[magnetic tape]] and [[optical disc]]s.

A storage device may hold information, process information, or both. A device that only holds information is a recording [[Medium (communication)|medium]]. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.

Electronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require [[Visual perception|vision]] and a brain to read data fall into this category.  Electromagnetic data may be stored in either an  analog [[data]] or [[digital data]] format on a variety of media. This type of data is considered to be [[Machine-readable medium|electronically encoded]] data, whether it is electronically stored in a  [[semiconductor]] [[Computer data storage|device]], for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of [[computer data storage]]) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) [[microcircuit]]s are [[volatile memory]], for it vanishes if power is removed.

Except for [[barcode]]s, [[optical character recognition]] (OCR), and [[magnetic ink character recognition]] (MICR) data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium.&lt;ref&gt;{{Cite web|url=https://www.seas.gwu.edu/~shmuel/WORK/Differences/Chapter%203%20-%20Sources.pdf|title=The Difference between Electronic and Paper Documents|last=Rotenstreich|first=Shmuel|website=Seas.GWU.edu|publisher=The George Washington University|access-date=12 April 2016}}&lt;/ref&gt;

==Global capacity, digitization, and trends==
In a recent study in [[Science (journal)|''Science'']] it was estimated that the world's technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) [[exabyte]]s in 1986, to 295 (optimally compressed) [[exabyte]]s in 2007,&lt;ref name="HilbertLopez2011"&gt;{{cite journal | last1 = Hilbert | first1 = Martin | last2 = L&#243;pez | first2 = Priscila | year = 2011 | title = The World's Technological Capacity to Store, Communicate, and Compute Information | journal = [[Science (journal)|Science]] | volume = 332 | issue = 6025| pages = 60&#8211;65 | doi=10.1126/science.1200970 | pmid=21310967}}; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html&lt;/ref&gt; and doubles roughly every three years.&lt;ref name="Hilbertvideo2011"&gt;[http://ideas.economist.com/video/giant-sifting-sound-0 "video animation on The World&#8217;s Technological Capacity to Store, Communicate, and Compute Information from 1986 to 2010]&lt;/ref&gt;

It is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information digitally than on analog storage devices.&lt;ref name="HilbertLopez2011" /&gt;

==See also==
{{colbegin||22em}}
* [[Archival science]]
* [[Blank media tax]]
* [[Computer data storage]]
* [[Content format]]
* [[Data transmission]]
* [[Digital Data Storage|Digital Data Storage (DDS)]]
* [[Digital preservation]]
* [[Disk drive performance characteristics]]
* [[Format war]]
* [[Flip-flop (electronics)]]
* [[IOPS]]
* [[Library]]
* [[Media controls]]
* [[Medium format (film)]]
* [[Memristor]]
* [[Nanodot]]
* [[Nonlinear medium]] ([[random access]])
* [[Recording format]]
* [[Semiconductor memory]]
* [[Telecommunication]]
{{colend}}

==References==
{{Reflist|30em}}

==Further reading==
* {{cite journal |last = Bennett |first=John C. | title = 'JISC/NPO Studies on the Preservation of Electronic Materials: A Framework of Data Types and Formats, and Issues Affecting the Long Term Preservation of Digital Material | publisher = British Library Research and Innovation Report 50 | year = 1997 | url = http://www.ukoln.ac.uk/services/papers/bl/jisc-npo50/bennet.html }}
* [http://www.zetta.net/history-of-computer-storage/ History of Computer Storage from 1928 to 2013]
* [http://www.remosoftware.com/info/history-of-storage-from-cave-paintings-to-electrons/ History of Storage from Cave Paintings to Electrons]
* [[Plant-based digital data storage]]

==External links==
* [http://ns1758.ca/winch/winchest.html Historical Notes about the Cost of Hard Drive Storage Sp]
* @[http://ns1758.ca/winch/winchest.html ace]
* [https://www.securedatarecovery.com/infographics/the-evolution-of-data-storage The Evolution of Data Storage]

{{Magnetic storage media}}
{{Optical storage media}}
{{Paper data storage media}}
{{Primary storage technologies}}

{{Authority control}}

[[Category:Computer storage devices]]
[[Category:Data management]]
[[Category:Film and video technology]]
[[Category:Media technology]]
[[Category:Recording]]
[[Category:Sound production technology]]
[[Category:Storage media]]</text>
      <sha1>dcw3ob9igccpbqcytaa4jbtadgix2ll</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer storage media</title>
    <ns>14</ns>
    <id>891409</id>
    <revision>
      <id>734498872</id>
      <parentid>724823490</parentid>
      <timestamp>2016-08-14T19:27:41Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category Computer storage to [[:Category:Computer data storage]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="464" xml:space="preserve">{{Cat main|Computer storage media}}
This category refers to [[digital media]] used in [[computer storage]] devices.  Examples of such media include (a) magnetic disks, cards, tapes, and drums, (b) punched cards and paper tapes, (c) optical disks, (d) barcodes and (e) magnetic ink characters.


{{Commons cat|Computer storage media}}

[[Category:Electronic documents]]
[[Category:Storage media]]
[[Category:Digital media]]
[[Category:Computer data storage| Media]]</text>
      <sha1>g6fx64ou7zxk6algnkh8jpvt7cqvjyb</sha1>
    </revision>
  </page>
  <page>
    <title>Xplor International</title>
    <ns>0</ns>
    <id>17762769</id>
    <revision>
      <id>761461298</id>
      <parentid>736922667</parentid>
      <timestamp>2017-01-23T03:10:04Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor />
      <comment>Fixed a spelling error.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4028" xml:space="preserve">{{about|the trade association|the software package|X-PLOR}}
{{Refimprove|date=June 2008}}
'''Xplor International'''&lt;ref&gt;'''Technology Trends: Xplor Directions Survey''',
Dr. Keith Davidson PhD, edp, ''Enterprise Journal, 1998''. Example of Xplor industry research on [[transaction documents]] and [[laser printing]] trends http://esj.com/article.aspx?ID=10229843143PM&lt;/ref&gt; also known as '''The Electronic Document Systems Association''' is an international [[trade association]] specifically focused on the issues of [[transaction document]]s. Transaction documents are legally relevant documents that are either printed and mailed or are electronically delivered e.g. [[Bill (payment)|bills]], [[bank statements]], [[insurance policies]] etc.

The acronym XPLOR was derived from ''Xerox Printer Liaison ORganization'', the original association name. Xplor expanded its mission in 1983 to include other vendors' technology and adopted the acronym as the organization's name.

== History ==
Xplor International was founded in 1980 as a trade association specifically for transaction document applications, due to the difference in emphasis on variable data. Originally a user group for the [[Xerox 9700]] laser printer, they reshaped its mission in the early 1980s to address the entire transaction document industry. Hardware companies like [[IBM]], [[Siemens]] (later [[Oc&#233;]]), [[Hewlett Packard]], [[Pitney Bowes]], [[Bell &amp; Howell]], and [[Xerox]] have been actively involved as have software companies like Image Sciences (later Docucorp International), Document Sciences, [[Cincom Systems]], [[GMC Software AG|GMC Software Technology]], Xenos, Crawford Technologies, supported Xplor in order to promote a venue for the issues that are unique to the creation of transaction documents.

In the 1990s, Xplor began to shift from solely document &#8220;printing&#8221; to document &#8220;printing and presentation&#8221;, as transaction documents came to be presented on the Web.

==Membership==
Xplor&#8217;s membership of users and vendors is worldwide, with approximately 45% of the membership in the early 2000s being outside the US.&lt;ref&gt;William J. 'Bill' McCalpin edp, former General Manager of Xplor International, 2008&lt;/ref&gt;

== Xplor honours and awards ==
Xplor awards various technology providers with awards each year, including:

*The Technology Application Award is presented to an individual, a company, or an organization to recognize outstanding achievement in the imaginative application of current technology and/or unique implementation of existing [[electronic document]] systems.
*The Innovator of the Year Award honors an individual, company, or organization that has conceived and developed an original concept leading to a significant advancement in the industry. The "Innovator" has advanced a new program product or technology that notably enhances the capabilities of [[electronic document]] systems.
*The Xplorer of the Year is Xplor International's most prestigious award; it honors significant service to the Association, dedication to the Xplor mission, and notable achievement promoting the interest of the [[electronic document]] systems industry.
*The Brian Platte Lifetime Achievement Award, established in 2007, is given to an individual whose efforts and contributions have significantly changed the course and development of the digital document industry.

=== Electronic Document Professional ===

Xplor manages the [[Electronic Document Professional]] (EDP) certification program for people experienced in [[electronic document]] systems and/or application development.

==Associations in related fields==
* [[Association for Information and Image Management]], the association for electronic content management
* [[Association of Records Managers and Administrators]], the association for records management professionals

==External links==
* [http://www.xplor.org Xplor International] website

== References ==

{{Reflist}}

[[Category:Electronic documents]]
[[Category:International trade associations]]</text>
      <sha1>fujpocswp0fu3ymcg8uvh7h69mrwpbt</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Office software</title>
    <ns>14</ns>
    <id>21137368</id>
    <revision>
      <id>548884230</id>
      <parentid>545545650</parentid>
      <timestamp>2013-04-05T19:40:17Z</timestamp>
      <contributor>
        <username>Chobot</username>
        <id>259798</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q6158391|]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="124" xml:space="preserve">{{Commons category|Office suites}}
[[Category:Office work]]
[[Category:Business software]]
[[Category:Electronic documents]]</text>
      <sha1>ddsjci9h6yme1tj9rni7od0bf3g0tls</sha1>
    </revision>
  </page>
  <page>
    <title>Novell Vibe</title>
    <ns>0</ns>
    <id>25023858</id>
    <revision>
      <id>742795326</id>
      <parentid>706957544</parentid>
      <timestamp>2016-10-05T21:01:01Z</timestamp>
      <contributor>
        <username>Reinvolve</username>
        <id>22870028</id>
      </contributor>
      <minor />
      <comment>caps</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5504" xml:space="preserve">{{multiple issues |{{Notability|Products|date=April 2012}}
{{refimprove|date=April 2012}}
{{advert|date=November 2012}}
}}

{{Infobox software
|name                       = Novell Vibe
|logo                       = 
|screenshot                 = 
|caption                    = 
|collapsible                = 
|author                     = 
|developer                  = [[Novell]]
|released                   = {{Start date|2008|06|25}} &lt;!-- {{Start date|YYYY|MM|DD}} --&gt;
|discontinued               = 
|latest release version     = 
|latest release date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|latest preview version     = 
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         = 
|programming language       = 
|operating system           = 
|platform                   =
|size                       = 
|language                   = 
|status                     = 
|genre                      = [[Web application]]
|license                    = Proprietary
|website                    = [http://www.novell.com/products/vibe/  Novell Vibe]
}}

'''Novell Vibe''' is a web-based team collaboration platform developed by [[Novell]], and was initially released by Novell in June 2008 under the name of Novell Teaming. Novell Vibe is a collaboration platform that can serve as a knowledge repository, [[document management system]], project collaboration hub, process automation machine, corporate [[intranet]] or [[extranet]]. Users can upload, manage, comment on, and edit content in a secure manner. Supported content includes documents, calendars, discussion forums, wikis, blogs, tasks, and more.

Document management functionality allows for document versions, approvals, and document life cycle tracking. Users can download and modify pre-built custom web pages and workflows free of charge from the Vibe Resource Library.&lt;ref&gt;{{cite web|url=http://www.novell.com/products/vibe/resource-library/ |title=Novell Vibe Resource Library |publisher=Novell.com |date= |accessdate=2012-04-12}}&lt;/ref&gt;

==History==
Novell Vibe is the result of a merging of two products in November 2010: Novell Teaming and Novell Pulse.&lt;ref&gt;http://www.novell.com/communities/node/12257/great-vibes-paths-merging-novell-teaming-and-novell-pulse&lt;/ref&gt;

Novell Teaming began as SiteScape Forum. When Novell Acquired SiteScape in 2008, the name was changed to Novell Teaming.{{citation needed|date=August 2012}}

Created in 2009, Novell Pulse was a communication tool based on the [[Google Wave Federation Protocol]].&lt;ref&gt;http://www.scala-lang.org/node/6618&lt;/ref&gt;

===Server===
Any combination of Linux and Windows servers can run the Vibe application. Furthermore, MySQL, MS SQL, and Oracle databases are supported.

===End-User operating systems===
Windows, Linux, Mac, iOS or Android

===Browsers===
Firefox, Internet Explorer, Safari, or Chrome

===Mobile===
Any mobile device that has a browser can access the Vibe site. Native iOS &lt;ref&gt;https://itunes.apple.com/us/app/novell-vibe/id476653054?mt=8&lt;/ref&gt; and Android &lt;ref&gt;https://play.google.com/store/apps/details?id=com.novell.vibe.android&lt;/ref&gt; apps are available for free download in the app stores.

===Microsoft Office integration===
Word, PowerPoint, and Excel (versions 2013,2010 and 2007) on the Windows operating system are supported.

==Interoperability==
Vibe can be used in conjunction with various other software products, such as [[Novell Access Manager]], [[Novell GroupWise]], [[Skype]], and [[YouTube]]. Novell Vibe integrates with an LDAP directory for authentication.

==Extendability==
Vibe administrators can extend the Vibe software by creating [[software extensions]], remote applications, or [[JAR (file format)]] files that enhance the power and usefulness of the Vibe software to create a custom experience for users.

Software extensions enable third-party developers to create abilities which extend an application. Vibe administrators or Vibe developers can create custom extensions (add-ons) to enhance Vibe. For example, you might have an extension that enables Flash video support in Vibe.

===Remote applications===
A remote application is a program that runs on a remote server and delivers data for use on the Novell Vibe site (such as data from a remote database). For example, Vibe administrators or Vibe developers could set up a remote application for Twitter that displays all of a user's Twitter entries in Vibe.

Unlike creating an extension for Vibe, creating a remote application does not modify the Vibe software.

== Open-source solutions ==
Not all of these projects implement all of the features Novell Vibe has to offer as well as Vibe is missing some features these products have:

* [[Kablink|Kablink Vibe]] - an open source version of Novell Vibe
* [[Redmine]]/[[ChiliProject]]
* [[trac]]
* [[Feng Office Community Edition]]
* [[Open Workbench]]
* [[OpenProj]]

''see also'': [[:Category:Free project management software|Wikipedia category for free project management software]]

==References==
{{Reflist}}

==External links==
* [http://www.novell.com/products/vibe/resource-library/  Novell Vibe product page]
* [http://kabtim.ru//  Kablink-Vibe]
{{Novell}}

[[Category:Novell software]]
[[Category:Proprietary wiki software]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Internet Protocol based network software]]
[[Category:Blog software]]</text>
      <sha1>feld695rwhwtr8chf1lh2bptmf231v4</sha1>
    </revision>
  </page>
  <page>
    <title>DataCite</title>
    <ns>0</ns>
    <id>26964279</id>
    <revision>
      <id>758768395</id>
      <parentid>753456648</parentid>
      <timestamp>2017-01-07T13:41:27Z</timestamp>
      <contributor>
        <username>Egon Willighagen</username>
        <id>17845450</id>
      </contributor>
      <comment>/* Background */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7006" xml:space="preserve">[[File:DataCite logo.png|thumb|DataCite's logo.]]

'''DataCite''' is an international [[not-for-profit]] organization which aims to improve [[data citation]] in order to:
*establish easier access to research data on the Internet
*increase acceptance of research data as legitimate, citable contributions to the scholarly record
*support data archiving that will permit results to be verified and re-purposed for future study.&lt;ref&gt;{{cite web|publisher=DataCite|title=What is DataCite?|url=http://www.datacite.org/whatisdatacite|accessdate=17 March 2014}}&lt;/ref&gt; 

==Background==
In August 2009 a paper was published laying out an approach for a global registration agency for research data.&lt;ref&gt;{{cite web|title=Approach for a joint global registration agency for research data|doi=10.3233/ISU-2009-0595}}&lt;!--| accessdate=2011-05-23--&gt;&lt;/ref&gt; DataCite was subsequently founded in London on 1 December 2009&lt;ref&gt;{{cite journal|last1=Neumann|first1=Janna|last2=Brase|first2=Jan|title=DataCite and DOI names for research data|journal=Journal of Computer-Aided Molecular Design|date=20 July 2014|volume=28|issue=10|pages=1035&#8211;1041|doi=10.1007/s10822-014-9776-5}}&lt;/ref&gt; by organisations from 6 countries: the [[British Library]]; the Technical Information Center of Denmark (DTIC); the [[TU Delft]] Library from the Netherlands; the National Research Council&#8217;s [[Canada Institute for Scientific and Technical Information]] (NRC-CISTI); the [[California Digital Library]] (University of California Curation Center);&lt;ref&gt;{{cite web|url=http://webarchives.cdlib.org/sw1st7gf68/http://www.universityofcalifornia.edu/news/article/23055 |title=University of California becomes founding member of Datacite |accessdate=2014-05-05 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; [[Purdue University]] (USA);&lt;ref&gt;{{cite web |url= http://blogs.lib.purdue.edu/news/2010/02/16/purdue-libraries-a-founding-member-of-international-cooperative-to-advance-research/ | title=Purdue Libraries becomes founding member of Datacite | accessdate=2010-04-15}}&lt;/ref&gt; and the German National Library of Science and Technology (TIB).&lt;ref&gt;{{cite web|url=http://www.tib-hannover.de/en/the-tib/news/news/id/133/ | title=TIB becomes founding member of Datacite | accessdate=2010-04-15}}&lt;/ref&gt;

After the founding of DataCite, leading research libraries and information centres converged for the first official members&#8217; convention in Paris on 5 February 2010. The inclusion of five further members was approved in the office of the International Council for Science (ICSU): Australian National Data Service (ANDS);&lt;ref&gt;{{cite web|url=http://ands.org.au/guides/doi.html | title=ANDS joins Datacite | accessdate=2010-04-15}}&lt;/ref&gt; Deutsche Zentralbibliothek f&#252;r Medizin (ZB MED); GESIS - Leibniz-Institut f&#252;r Sozialwissenschaften; French Institute for Scientific and Technical Information (INIST);&lt;ref&gt;{{cite web|url=http://www.inist.fr/spip.php?article66 |title=Inist join Datacite consortium |accessdate=2010-04-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20100309010702/http://www.inist.fr:80/spip.php?article66 |archivedate=2010-03-09 |df= }}&lt;/ref&gt; and Eidgen&#246;ssische Technische Hochschule (ETH) Z&#252;rich.

== Technical ==

The primary means of establishing easier access to research data is by DataCite members assigning persistent identifiers, such as [[digital object identifier]]s (DOIs), to data sets. Although currently leveraging the well-established DOI infrastructure, DataCite takes an open approach to identifiers, and considers other systems and services that help forward its objectives.&lt;ref&gt;{{cite web|publisher=DataCite |title=What do we do? |url=http://www.datacite.org/whatdowedo |accessdate=17 March 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140317195125/http://www.datacite.org/whatdowedo |archivedate=17 March 2014 |df= }}&lt;/ref&gt; 

DataCite's recommended format for a data citation is: 
*Creator (PublicationYear): Title. Publisher. Identifier
OR
*Creator (PublicationYear): Title. Version. Publisher. ResourceType. Identifier

DataCite recommends that DOI names are displayed as linkable, permanent URLs.&lt;ref&gt;{{cite web|publisher=DataCite|title=Why cite data?|url=http://www.datacite.org/whycitedata|accessdate=17 March 2014}}&lt;/ref&gt;

Third-party tools allow the migration of content to and from other services such as ODIN, for [[ORCID]]&lt;ref name="ODIN"&gt;{{cite web |url=http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/|title=New ORCID-integrated data citation tool |last=Thorisson |first=Gudmundur |date=2013-05-13 |publisher=ODIN Project |accessdate=7 May 2014}}&lt;/ref&gt;

==Members==
* Australia:
** [[Australian National Data Service]] - ANDS
* Canada:
** [[National Research Council (Canada)|National Research Council Canada]] - NRC-CNRC
* China:
** [[Beijing Genomics Institute]] - BGI 
* Denmark:
** Technical Information Center of Denmark (DTU Library)
* Estonia:
** [[University of Tartu]] (/UT)
* France:
** [[Institut de l'information scientifique et technique]] - INIST-CNRS
* Germany:
** [[German National Library of Economics]] - ZBW
** [[German National Library of Medicine]] - ZB MED
** [[German National Library of Science and Technology]] - TIB
** Leibniz Institute for the Social Sciences - GESIS
** G&#246;ttingen State and University Library - [[G&#246;ttingen State and University Library|SUB]]
** Gesellschaft f&#252;r wissenschaftliche Datenverarbeitung mbH G&#246;ttingen - GWDG 
* Hungary:
** Library and Information Centre, Hungarian Academy of Sciences - MTA KIK
* International:
** [[World Data System|ICSU World Data System]] - ICSU-WDS 
* Italy:
** [[Conference of Italian University Rectors]] - CRUI
* Japan:
** Japan Link Center - JaLC
* Netherlands:
** [[TU Delft]] Library
* Norway:
** [[BIBSYS]]
* Republic of Korea:
** [[Korea Institute of Science and Technology Information]] - KISTI 
* South Africa:
** South African Environmental Observation Network - SAEON
* Sweden:
** Swedish National Data Service - SND
* Switzerland:
** [[CERN]] - European Organization for Nuclear Research
** [[Swiss Federal Institute of Technology Zurich]] - ETH
* Thailand:
** National Research Council of Thailand - NRCT
* United Kingdom:
** [[The British Library]] - BL
** [[Digital Curation Centre]] 
* United States:
** [[California Digital Library]] - CDL
** [[OSTI|Office of Scientific and Technical Information, US Department of Energy]] - OSTI
** [[Purdue University|Purdue University Libraries]] - PUL
** [[Inter-university Consortium for Political and Social Research]] - ICPSR 
** [[Harvard University Library]] 
** [[Institute of Electrical and Electronics Engineers]] - IEEE 

==References==
&lt;references /&gt;

== External links ==
* [http://www.datacite.org/ Official '''DataCite''' website]

[[Category:Academic publishing]]
[[Category:Data publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
[[Category:Non-profit organizations]]
[[Category:Non-profit technology]]</text>
      <sha1>13dvg4pw8smr4skoh1vpqc59ur1je6z</sha1>
    </revision>
  </page>
  <page>
    <title>Structured document</title>
    <ns>0</ns>
    <id>23524003</id>
    <revision>
      <id>738692612</id>
      <parentid>602852155</parentid>
      <timestamp>2016-09-10T14:13:56Z</timestamp>
      <contributor>
        <username>Owen Ambur</username>
        <id>3035628</id>
      </contributor>
      <comment>/* See also */ Added link to Machine-Readable Documents</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5049" xml:space="preserve">{{Refimprove|date=April 2014}}
A '''structured document''' is an [[electronic document]] where some method of [[embedded coding]], such as [[markup language|mark-up]], is used to give the whole, and parts, of the document various structural meanings according to a [[Database schema|schema]]. A structured document whose mark-up doesn't break the schema and is designed to conform to and which obeys the [[syntax]] rules of its [[markup language|mark-up language]] is "well-formed".

{{Quote|The [[Standard Generalized Markup Language]] (SGML) has pioneered the concept of structured documents|[http://www.w3.org/People/Janne/porject/paper.html Multi-purpose publishing using HTML, XML, and CSS], [http://www.w3.org/People/howcome/ H&#229;kon Wium Lie] &amp; [http://www.w3.org/People/Janne/ Janne Saarela]}}

As of 2009 the most widely used [[markup language]], in all its evolving forms, is [[HTML]], which is used to structure documents according to various [[Document Type Definition|Document Type Definition (DTD)]] schema defined and described by the [[W3C]], which continually reviews, refines and evolves the specifications.

{{Quote|[[XML]] is the universal format for structured documents and data on the Web|[http://www.w3.org/MarkUp/#related XHTML2 Working Group], [[W3C]]}}

==Structural semantics==
In writing structured documents the focus is on encoding the logical structure of a document, with no explicit concern in the structural markup for its presentation to humans by printed pages, screens or other means. Structured documents, especially well formed ones, can easily be processed by computer systems to extract and present [[metadata]] about the document. In most Wikipedia articles for example, a table of contents is automatically generated from the different heading tags in the body of the document. Popular [[word processor]]s can have such a function available.

In [[HTML]] a part of the logical structure of a document may be the document body; &lt;code&gt;&lt;nowiki&gt;&lt;body&gt;&lt;/nowiki&gt;&lt;/code&gt;, containing a first level heading; &lt;code&gt;&lt;nowiki&gt;&lt;h1&gt;&lt;/nowiki&gt;&lt;/code&gt;, and a paragraph; &lt;code&gt;&lt;nowiki&gt;&lt;p&gt;&lt;/nowiki&gt;&lt;/code&gt;.

&lt;code&gt;&lt;nowiki&gt;&lt;body&gt;&lt;/nowiki&gt;&lt;/code&gt;
:&lt;code&gt;&lt;nowiki&gt;&lt;h1&gt;Structured document&lt;/h1&gt;&lt;/nowiki&gt;&lt;/code&gt;
:&lt;code&gt;&lt;nowiki&gt;&lt;p&gt;A &lt;strong class="selflink"&gt;structured document&lt;/strong&gt; is an &lt;a href="/wiki/Electronic_document" title="Electronic document"&gt;electronic document&lt;/a&gt; where some method of &lt;a href="/w/index.php?title=Embedded_coding&amp;amp;action=edit&amp;amp;redlink=1" class="new" title="Embedded coding (page does not exist)"&gt;embedded coding&lt;/a&gt;, such as &lt;a href="/wiki/Markup_language" title="Markup language"&gt;markup&lt;/a&gt;, is used to give the whole, and parts, of the document various structural meanings according to a &lt;a href="/wiki/Schema" title="Schema"&gt;schema&lt;/a&gt;.&lt;/nowiki&gt;&lt;/code&gt;&lt;code&gt;&lt;nowiki&gt;&lt;/p&gt;&lt;/nowiki&gt;&lt;/code&gt;
&lt;code&gt;&lt;nowiki&gt;&lt;/body&gt;&lt;/nowiki&gt;&lt;/code&gt;

One of the most attractive features of structured documents is that they can be reused in many contexts and presented in various ways on mobile phones, TV screens, speech synthesisers, and any other device which can be programmed to process them.

=== Other semantics ===
Other meaning can be ascribed to text which isn't structural. In the [[HTML]] fragment above, there is semantic markup which has nothing to do with structure; the first of these, the &lt;code&gt;&lt;nowiki&gt;&lt;strong&gt;&lt;/nowiki&gt;&lt;/code&gt; tag, means that the enclosed text should be given a strong emphasis. In visual terms this is equivalent to the bold, &lt;code&gt;&lt;nowiki&gt;&lt;b&gt;&lt;/nowiki&gt;&lt;/code&gt; tag, but in speech synthesisers this means a voice inflection giving strong emphasis is used. The term [[semantic markup]] excludes markup like the bold tag which has no meaning other than an instruction to a visual display. The strong tag means that the presentation of the enclosed text should have a strong emphasis in all presentation forms, not just visual.&lt;br /&gt;
The anchor &lt;code&gt;&lt;nowiki&gt;&lt;a&gt;&lt;/nowiki&gt;&lt;/code&gt; tag is a more obvious example of semantic markup unconcerned with structure, with its href attribute set it means that the text it surrounds is a [[hyperlink]].

[[HTML]] from early on has also had tags which gave presentational semantics, i.e. there were tags to give '''bold''' (&lt;code&gt;&lt;nowiki&gt;&lt;b&gt;&lt;/nowiki&gt;&lt;/code&gt;)or ''italic'' (&lt;code&gt;&lt;nowiki&gt;&lt;i&gt;&lt;/nowiki&gt;&lt;/code&gt;) text, or to alter &lt;small&gt;font sizes&lt;/small&gt; or which had other effects on the presentation.&lt;ref&gt;{{cite web|url=http://www.w3.org/MarkUp/draft-ietf-iiir-html-01.txt|accessdate=5 March 2014}}&lt;/ref&gt; Modern versions of markup languages discourage such markup in favour of [[Style sheet language|style sheets]]. Different style sheets can be attached to any markup, semantic or presentational, to produce different presentations. In [[HTML]], tags such as; &lt;code&gt;&lt;nowiki&gt;&lt;a&gt;, &lt;blockquote&gt;, &lt;em&gt;, &lt;strong&gt;&lt;/nowiki&gt;&lt;/code&gt; and others do not have a structural meaning, but do have a meaning.

== See also ==
* [[Document processor]]
* [[Machine-Readable Documents]]

 {{reflist}}

{{DEFAULTSORT:Structured Document}}
[[Category:Electronic documents]]</text>
      <sha1>a8axve109ysslvg3a75t4601b11kd4y</sha1>
    </revision>
  </page>
  <page>
    <title>Computable Document Format</title>
    <ns>0</ns>
    <id>32785726</id>
    <revision>
      <id>753685907</id>
      <parentid>752057522</parentid>
      <timestamp>2016-12-08T17:16:06Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4462" xml:space="preserve">{{Infobox file format
| name                   = Computable Document Format
| logo                   = [[Image:WolframCDFLogoSmall.png]]
| icon                   = [[Image:WolframCDFLogoSmall.png]]
| iconcaption            = 
| screenshot             =  
| caption                =  
| extension              = .cdf
| mime                   = application/cdf
| typecode               =  
| uniform type           = com.wolfram.cdf
| magic                  =  
| owner                  = [[Wolfram Research]]
| released               = {{Start date|2011|07|21}}&lt;!-- {{Start date|YYYY|mm|dd|df=yes}} --&gt;
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|mm|dd|df=yes}} --&gt;
| genre                  =
| container for          =
| contained by           =
| extended from          =
| extended to            =
| standard               =  
| free                   =
| url                    = [http://www.wolfram.com/cdf Computable Document Format]
}}
'''Computable Document Format''' ('''CDF''') is an electronic document format&lt;ref&gt;[http://www.telegraph.co.uk/technology/news/8561619/Wolfram-Alpha-creator-plans-to-delete-the-PDF.html Wolfram Alpha Creator plans to delete the PDF] The Telegraph (UK)&lt;/ref&gt; designed to allow easy authoring&lt;ref&gt;[http://www.pcworld.com/businesscenter/article/236202/wolfram_makes_data_interactive.html Wolfram makes data interactive] PC World&lt;/ref&gt; of dynamically generated interactive content. CDF is a published public format&lt;ref&gt;{{cite web|title=About CDFs|url=http://www.wolfram.com/cdf/faq/#aboutcdf|publisher=[[Wolfram Research]]}}&lt;/ref&gt; created by [[Wolfram Research]].&lt;ref name=thinq11/&gt;

==Features==
Computable document format supports [[GUI]] elements such as sliders, menus and buttons. Content is updated using embedded computation in response to GUI interaction. Contents can include formatted text, tables, images, sounds and animations. CDF supports [[Mathematica]] typesetting and technical notation.&lt;ref&gt;[http://www.zdnet.com/blog/btl/wolfram-launches-new-document-format-meet-cdf/52917 Wolfram Launches new document format. Meet CDF] ZDNet&lt;/ref&gt; Paginated layout, structured drill down layout and slide-show mode are supported. Styles can be controlled using a [[Cascading Style Sheets|cascading style sheet]].

==Reading==
CDF files can be read using a proprietary [[CDF Player]] with a restrictive license, which can be downloaded free of charge from Wolfram Research.&lt;ref name=thinq11&gt;[http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ Wolfram punts expanded medium for technical docs] {{webarchive |url=https://web.archive.org/web/20110725121540/http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ |date=July 25, 2011 }} ThinQ&lt;/ref&gt;

==Authoring==
CDF Files can be created using [[Mathematica]].  Online authoring tools are planned.&lt;ref name=thinq11/&gt;&lt;ref&gt;[http://www.cio.com.au/article/394473/wolfram_makes_data_interactive/ Wolfram makes data interactive] CIO, 21 July 2011&lt;/ref&gt;

==Uses==
Computable Document Format has been used in electronic books by [[Pearson Education]],&lt;ref&gt;[http://www.schoollibraryjournal.com/slj/home/891371-312/wolfram_launches_pdf_killer.html.csp Wolfram launches PDF Killer] School Library Journal&lt;/ref&gt;&lt;ref&gt;[http://www.pearsonhighered.com/briggscochran1einfo/ Briggs Cochrane Calculus]&lt;/ref&gt; to provide the content for the [[Wolfram Demonstrations Project]], and to add client-side interactivity to [[Wolfram Alpha]].&lt;ref&gt;[http://thenextweb.com/apps/2011/08/12/wolfram-alpha-adds-powerful-interactive-search-results/ WolframAlpha adds powerful interactive search results] The Next Web, 12 August 2011&lt;/ref&gt;&lt;ref&gt;[http://www.pcpro.co.uk/news/enterprise/368815/wolfram-launches-its-own-interactive-document-format Wolfram Launches its own interactive document format] PC Pro, July 2011&lt;/ref&gt;

== See also ==
* [[List of numerical analysis software]]
* [[Comparison of numerical analysis software]]

== References ==
{{Reflist}}

== External links ==
* [http://www.wolfram.com/cdf/ Wolfram Research CDF]
* [http://www.wolfram.com/cdf-player/ CDF Player download]

{{Graphics file formats}}
{{Ebooks}} 
{{Wolfram Research}}&lt;!--navbox--&gt;

[[Category:2011 introductions]]
[[Category:Wolfram Research]]
[[Category:Electronic documents]]
[[Category:Open formats]]
[[Category:Page description languages]]
[[Category:Vector graphics]]
[[Category:Computer file formats]]
[[Category:Digital press]]</text>
      <sha1>oe1x29fe66r0j7mxb6c9c50g5bz8ynf</sha1>
    </revision>
  </page>
  <page>
    <title>ViXra</title>
    <ns>0</ns>
    <id>32834692</id>
    <revision>
      <id>746631893</id>
      <parentid>746631435</parentid>
      <timestamp>2016-10-28T15:35:57Z</timestamp>
      <contributor>
        <username>Trilliant</username>
        <id>27603315</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2509" xml:space="preserve">{{lowercase title}}{{Infobox Website
|name           = viXra
|logo           = 
|screenshot     = 
|caption        = 
|url            = [http://viXra.org/ viXra.org]
|alexa          = 
|commercial     = No
|type           = Science
|language       = English
|registration   = 
|owner          = 
|author         = 
|launch date    = 
|current status = Online
|revenue        = 
|slogan         =
}}

'''viXra''' is an electronic [[e-print]] archive set up by independent physicist Philip Gibbs as an alternative to the dominant [[arXiv]] service operated by [[Cornell University]].

==Description==
Although dominated by physics and mathematics submissions, viXra aims to cover topics across the whole scientific community. It accepts submissions without requiring authors to have an academic affiliation and without any threshold for quality.&lt;ref&gt;"[http://blogs.nature.com/news/2009/07/whats_arxiv_spelled_backwards.html What&#8217;s arXiv spelled backwards? A new place to publish]". ''Nature News Blog''. 16 July 2009.&lt;/ref&gt; The e-prints on viXra are grouped into seven broad categories: physics, mathematics, computational science, biology, chemistry, humanities, and other areas.&lt;ref name="vixra"&gt;{{cite web |url=http://vixra.org/ |title=ViXra.org open e-print archive |work=viXra.org | accessdate=22 August 2011}}&lt;/ref&gt; Anyone may post anything on viXra, though house rules do prohibit &#8220;vulgar, libellous [sic], plagiaristic or dangerously misleading&#8221; content.&lt;ref&gt;http://nautil.us/issue/41/selection/what-counts-as-science&lt;/ref&gt;

Gibbs' original motivation for starting the archive was to cater for researchers who believed that their preprints had been unfairly rejected or reclassified by the arXiv moderators.&lt;ref name="pw"&gt;{{cite journal |title=Fledgling site challenges arXiv server |work=[[Physics World]] |date=15 July 2009 |url= http://physicsworld.com/cws/article/news/39845}}&lt;/ref&gt; As of 2013 it had already over 4000 preprints&lt;ref&gt;{{citation|title=A Good Year for viXra|first=Philip E.|last=Gibbs|journal=Prespacetime Journal|volume=4|issue=1|year=2013|pages=87&#8211;90|url=http://prespacetime.com/index.php/pst/article/view/482}}.&lt;/ref&gt; and in October, 2016 the number had grown to 16,214.&lt;ref&gt;Official site (front page)&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* [http://vixra.org/ Official website]

[[Category:Eprint archives]]
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Online archives]]


{{website-stub}}</text>
      <sha1>bv6frstx9jrf502z5hukyij46z3ejln</sha1>
    </revision>
  </page>
  <page>
    <title>Lettrs</title>
    <ns>0</ns>
    <id>39130775</id>
    <revision>
      <id>760064908</id>
      <parentid>746352852</parentid>
      <timestamp>2017-01-14T19:37:54Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* History */HTTP&amp;rarr;HTTPS for [[Yahoo!]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11408" xml:space="preserve">{{lowercase title}}
{{Infobox software
| name         = lettrs
| logo         = [[File:Lettrs logo Square.jpg|200px]]
| released     = {{Start date|2012}}
| developer   = [[Drew Bartkiewicz]] (CEO)
| status             = Active
| operating system   = [[IOS|Apple iOS]], [[Android (operating system)|Android]]
| genre              = [[Social Networking]]
| website  =  {{URL|http://www.lettrs.com}}
}}

'''lettrs''' is a global [[mobile app|mobile application]] and [[social network]] that allows users to compose and send mobile messages privately or publicly.&lt;ref name=crunch&gt;{{cite web |url=https://www.crunchbase.com/organization/lett-rs |title= lettrs |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=CrunchBase.com |publisher=AOL Inc |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=bweek&gt;{{cite web |url=http://www.businessweek.com/articles/2014-01-30/youve-sent-mail-a-letter-writing-app-forces-users-to-slow-down |title=You've Sent Mail: A Letter-Writing App Forces Users to Slow Down |last1=Leonard |first1=Devin |date=30 January 2014 |website=BusinessWeek.com |publisher=Bloomberg LP |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=think&gt;{{cite web |url=http://customerthink.com/lettrs_launches_platform_to_organize_and_deliver_the_world_s_letters_in_the_cloud/ |title=lettrs Launches Platform to Organize and Deliver the World&#8217;s Letters in the Cloud |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=21 December 2012 |website=CustomerThink.com |publisher=Customer Think Corporation |accessdate=26 January 2015}}&lt;/ref&gt; The lettrs app converts mobile voice, data and pictures to digital personal and public messages via its text and mobile delivery inventions.&lt;ref name=mash1&gt;{{cite web |url=http://mashable.com/2013/04/23/lettrs-mobile-app/ |title=Lettrs App Lets You Send Snail Mail From Your iPhone |last1=Petronzio |first1=Matt |date=23 April 2013 |website=Mashable.com |publisher=Mashable Inc |accessdate=26 January 2015}}&lt;/ref&gt;

lettrs is headquartered in [[New York City]] and [[Drew Bartkiewicz]] is the company&#8217;s CEO and co-founder.&lt;ref name=crunch&gt;&lt;/ref&gt; In 2015, [[Mark Jung]] was named the company [[Chairman]].&lt;ref name=trutower2&gt;{{cite web |url=http://www.trutower.com/2015/05/06/lettrs-chat-app-chairman-woman-note-23902/ |title=lettrs Messaging App Announces New Chairman and Launch of "Woman Of Note" Collection |last1=Nay |first1=Josh Robert |date=6 May 2015 |website=trutower.com |publisher=TruTower |access-date=3 July 2015}}&lt;/ref&gt;&lt;ref name=techco&gt;{{cite web |url=http://tech.co/lettrs-women-of-note-2015-05 |title=lettrs: Bringing Hand Written Notes to the Digital World |last1=Schmidt |first1=Will |date=6 May 2015 |website=tech.co |publisher=TechCo |access-date=3 July 2015}}&lt;/ref&gt; lettrs has a global user base in 174 companies,&lt;ref name=mobile&gt;{{cite web |url=http://www.adweek.com/socialtimes/messaging-app-lettrs-launches-socialstamps-advertisers-charities/554471?red=im |title=Messaging App Lettrs Launches SocialStamps for Advertisers, Charities |last1=Shaul |first1=Brandy |date=9 December 2014 |website=Social Times |publisher=Prometheus Global Media |accessdate=26 January 2015}}&lt;/ref&gt; over 1 million downloads and has been featured in several media outlets, including [[The Wall Street Journal]], [[CBS]] and [[NPR]].&lt;ref name=crunch/&gt;&lt;ref name=postal&gt;{{cite web |url=http://postalvision2020.com/postalvision-2020-3-0/speakers-3-0-conference/drew-bartkiewicz/ |title=Drew Bartkiewicz |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=PostalVision2020.com |publisher=Ursa Major Associates, LLC |accessdate=26 January 2015}}&lt;/ref&gt;

==History==
lettrs was established in 2012 by technology entrepreneur [[Drew Bartkiewicz]].&lt;ref name=bweek/&gt;&lt;ref name=think/&gt;&lt;ref name=postal/&gt; Bartkiewicz came up with the idea for the company in 2008&lt;ref name=bweek/&gt; after being inspired by his grandmother&#8217;s letter writing&lt;ref name=yahoo&gt;{{cite web |url=https://news.yahoo.com/lettrs-brings-snail-mail-back-future-230223676.html |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Yahoo News |publisher=Mashable Inc |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=mash2&gt;{{cite web |url=http://mashable.com/2012/06/19/lettrs/ |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Nashable |publisher=Mashable Inc |accessdate=26 January 2015}}&lt;/ref&gt; and his own experiences during his service in the military.&lt;ref name=bweek/&gt;&lt;ref name=think/&gt;&lt;ref name=parcel&gt;{{cite web |url=http://postandparcel.info/54661/in-depth/innovation-in-depth/postalvision-sets-sights-on-congress-and-younger-americans/ |title=PostalVision sets sights on Congress and younger Americans |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 March 2013 |website=PostandParcel.info |publisher=Post &amp; Parcel |accessdate=26 January 2015}}&lt;/ref&gt; lettrs was officially established the summer of 2012 with the help of his wife, Araceli Bartkiewicz, and children,&lt;ref name=postal/&gt; though it was not launched as a global platform from its [[Software release life cycle#Beta|beta]] phase until December 2012.&lt;ref name=think/&gt;&lt;ref name=nextweb&gt;{{cite web |url=http://thenextweb.com/apps/2013/04/23/word-up-lettrs-launches-on-ios/ |title=TNW Pick of the Day: Lettrs turns your iPhone into a personal writing desk, transcriber and post office |last1=Sawers |first1=Paul |date=23 April 2013 |website=The Next Web |publisher=The Next Web, Inc |accessdate=26 January 2015}}&lt;/ref&gt;

Bartkiewicz introduced the lettrs mobile application at the PostalVision 2020/3.0 conference in [[Washington, D.C.]] in 2013.&lt;ref name=postal/&gt;&lt;ref name=mash1/&gt;&lt;ref name=nextweb/&gt; The Android version was released in July 2014,&lt;ref name=android&gt;{{cite web |url=http://www.androidcentral.com/lettrs-app-comes-android-more-personal-messages |title=The lettrs app comes to Android for more personal messages |last1=Callaham |first1=John |date=14 June 2014 |website=AndroidCentral.com |publisher=Mobile Nations |accessdate=26 January 2015}}&lt;/ref&gt; followed by a re-release of the iOS app in October.&lt;ref name=apple&gt;{{cite web |url=http://www.prweb.com/releases/2014/10/prweb12261924.htm |title=lettrs Raises $1.5M, Releases First Native iPad and Popular New iPhone App |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=21 October 2014 |website=PRWeb.com |publisher=Vocus PRW Holdings, LLC |accessdate=26 January 2015}}&lt;/ref&gt;

==Features==
lettrs provides a [[mobile phone|mobile]] platform for customers to create and deliver mobile letters in 80 translated languages with a selection of writing themes, proprietary &#8220;SocialStamps&#8221; and styles.&lt;ref name=think/&gt;&lt;ref name=mnn&gt;{{cite web |url=http://www.mnn.com/lifestyle/responsible-living/blogs/miss-sending-letters-try-lettrs |title=Miss sending letters? Try lettrs! |last1=Vartan |first1=Starre |date=8 February 2013 |website=Mother Nature Network |publisher=MNN Holding Company, LLC |accessdate=26 January 2015}}&lt;/ref&gt; It facilitates both private messaging and public posting of signed, translated and networked mobile-to-mobile letters.&lt;ref name=crunch/&gt;

The signature service of lettrs is the translation of letter messages in real time complete with original user signatures and selectable SocialStamps. The lettrs mobile network is able to translate an original digital letter in up to 80 languages. Users may also share open letters and the lettrs stamps across other major social networks.&lt;ref name=bweek/&gt;&lt;ref name=mash1/&gt;&lt;ref name=yahoo/&gt;&lt;ref name=parcel/&gt;

In December 2014 the company introduced a feature named SocialStamps that allows users to add a customized stamp to a letter. At the feature&#8217;s launch, the company offered 47 different stamps with plans to issue new stamps monthly. As part of the release the lettrs 2014 Persons of Note stamps on the lettrs network featured [[Michelle Phan]], [[Narendra Modi]], [[Bob Woodruff]] of [[ABC News]] and [[Stanley A. McChrystal]].&lt;ref name=mobile/&gt;&lt;ref name=stamps&gt;{{cite web |url=http://venturebeat.com/2014/12/09/lettrs-com-calls-postage-stamps-into-social-duty-for-its-old-style-letters/ |title=Lettrs calls postage stamps into social duty for its old-style letters |last1=Levine |first1=Barry |date=9 December 2014 |website=VentureBeat.com |publisher=VentureBeat |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=marketer&gt;{{cite web |url=http://www.mobilemarketer.com/cms/news/social-networks/19316.html |title=United Way of New York City leverages lettrs' SocialStamps for fundraising |last1=Samuely |first1=Alex |date=9 December 2014 |website=MobileMarketer.com |publisher=Napean LLC |accessdate=26 January 2015}}&lt;/ref&gt;

Users can share letters and the SocialStamps via [[Facebook]] and [[Twitter]].&lt;ref name=think/&gt;&lt;ref name=nextweb/&gt; lettrs also integrates with [[Google+]] and [[Instagram]] so that users may broaden the distribution of their letters beyond the mobile app.&lt;ref name=bweek/&gt; Users can also pen open public letters or petitions for supporting causes, persons, or brands.&lt;ref name=think/&gt;&lt;ref name=nextweb/&gt;

lettrs conducted its first Hollywood movie integration in April 2015 with Relativity Media. The company released stamps featuring images from the movie ''[[Desert Dancer]]''.&lt;ref name=trutower/&gt; In May 2015, lettrs released the "Women of Note" stamp collection. It featured 12 notable women including [[Michelle Obama]], [[Queen Rania of Jordan]], [[Shakira]], [[Michelle Bachelet]], [[Laura Bush]], [[Sonia Gandhi]], [[Ellen DeGeneres]] and [[Angelina Jolie]].&lt;ref name=trutower2/&gt;&lt;ref name=techco/&gt;

==Recognition and partnerships==
In 2014, Google selected lettrs as one of the Best Android Apps of the year.&lt;ref name=time&gt;{{cite web |url=http://time.com/3611709/best-android-google-play-apps-2014/ |title=Google Says These Are 2014's Best Android Apps |last1=Luckerson |first1=Victor |date=1 December 2014 |website=Time.com |publisher=Time Inc |accessdate=1 February 2015}}&lt;/ref&gt;&lt;ref name=techco/&gt;

lettrs has worked with the [[United Service Organizations|USO]], [[Aspen Institute]], and the [[United Way]].&lt;ref name=trutower2/&gt; In 2014, the company published the first digitally sourced book of letters, ''Poetguese''. The book contains a foreword by author [[Paulo Coelho]] with all proceeds donated to charity.&lt;ref name=broadway&gt;{{cite web |url=http://www.broadwayworld.com/bwwbooks/article/Lettrs-Announces-POETGUESE-20141008 |title=lettrs Announces Poetguese |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=8 October 2014 |website=broadwayworld.com |publisher=Wisdom Digital Media |access-date=3 July 2015}}&lt;/ref&gt;

lettrs established lettrs Foundation, an organization that partners with schools and non-profits to improve literacy through social networking, including partnerships with the United Way and Aspen Institute.&lt;ref name=trutower&gt;{{cite web |url=http://www.trutower.com/2014/07/14/lettrs-letter-writing-messaging-app-launch-on-android/ |title=lettrs Platform Launches on Android, Bringing Handwritten Letters Back to the Mainstream |last1=Nay |first1=Josh Robert |date=14 July 2014 |website=TruTower.com |publisher=TruTower |accessdate=1 February 2015}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*{{Official site|http://about.lettrs.com}}
*[http://www.lettrsfoundation.org lettrs Foundation]

[[Category:Mobile social software]]
[[Category:Postal system]]
[[Category:Postal services]]
[[Category:Internet terminology]]
[[Category:Electronic documents]]</text>
      <sha1>tqjpucdsrh20pi62bos80bk46yzuu3v</sha1>
    </revision>
  </page>
  <page>
    <title>Zathura (document viewer)</title>
    <ns>0</ns>
    <id>46505081</id>
    <revision>
      <id>741958483</id>
      <parentid>741955753</parentid>
      <timestamp>2016-09-30T19:19:03Z</timestamp>
      <contributor>
        <username>B-21</username>
        <id>5486213</id>
      </contributor>
      <comment>added Source Mage package historical entry</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5948" xml:space="preserve">{{notability|Products|date=April 2015}}
{{Infobox software
| name                   = Zathura
| screenshot             = Zathura Screenshot.png
| caption = Screenshot of zathura viewing a PDF file in Arch Linux.
| author                 = Moritz Lipp, Sebastian Ramacher
| developer              = pwmt&lt;ref&gt;{{cite web|url=https://pwmt.org |title=Programs With Movie Titles}}&lt;/ref&gt;
| released               = {{Start date|2009|09|18}}
| latest release version = 0.3.6
| latest release date    = {{Release date|2016|04|18}}&lt;ref&gt;{{cite web |url=https://pwmt.org/news/zathura-0-3-6/ |title=ZATHURA 0.3.6 |website=pwmt.org |date=2016-04-18 |accessdate=2016-08-28}}&lt;/ref&gt;
| programming language   = [[C (programming language)|C]]
| operating system       = [[Unix-like]]
| status                 = Active
| genre                  = Document viewer
| license                = [[Free software license|Free software]]
| website                = {{URL|pwmt.org/projects/zathura}}
}}

'''Zathura''' is a [[Free software|free]], [[Plug-in (computing)|plugin-based]] [[document viewer]]. Plugins are available for [[Portable Document Format|PDF]] (via [[Poppler_(software)|poppler]] or [[MuPDF]]), [[PostScript]], [[DjVu]], and [[EPUB]]. It was written to be lightweight and controlled with [[Vim (text editor)|vim]]-like keybindings. Zathura's customizability makes it well-liked by many Linux users.&lt;ref&gt;{{cite web|url=http://www.maketecheasier.com/8-alternative-pdf-readers-for-your-consideration/|title=MakeTechEasier list of alternative PDF viewers|access-date=24 April 2015}}&lt;/ref&gt;

Zathura has a mature, well-established codebase and a large development team.&lt;ref&gt;{{cite web|url=https://www.openhub.net/p/zathura-pdf-viewer|title=OpenHUB analysis of Zathura PDF Viewer|access-date=24 April 2015}}&lt;/ref&gt; It has official packages available in [[Arch linux]],&lt;ref&gt;{{cite web|url=https://www.archlinux.org/packages/community/x86_64/zathura/|title=Arch Linux zathura package}}&lt;/ref&gt;
[[Debian]],&lt;ref&gt;{{cite web|url=https://packages.debian.org/en/sid/zathura|title=Debian zathura package}}&lt;/ref&gt;
[[Fedora (operating system)|Fedora]],&lt;ref&gt;{{cite web|url=http://pkgs.org/altlinux-sisyphus/classic-x86_64/zathura-devel-0.3.3-alt1.x86_64.rpm.html|title=Fedora zathura package}}&lt;/ref&gt;
[[Gentoo linux|Gentoo]],&lt;ref&gt;{{cite web|url=https://packages.gentoo.org/package/app-text/zathura|title=Gentoo zathura package}}&lt;/ref&gt;
[[Ubuntu (operating system)|Ubuntu]],&lt;ref&gt;{{cite web|url=http://packages.ubuntu.com/precise/zathura|title=Ubuntu zathura package}}&lt;/ref&gt;
[[Source Mage GNU/Linux]],&lt;ref&gt;{{cite web|url=http://download.sourcemage.org/codex/test/doc/zathura/|title=Source Mage zathura package}}&lt;/ref&gt;
[[OpenBSD]],&lt;ref&gt;{{cite web|url=http://openports.se/textproc/zathura|title=OpenBSD zathura package}}&lt;/ref&gt;
and [[Mac OS X]].&lt;ref&gt;{{cite web|url=https://www.macports.org/ports.php?by=name&amp;substr=zathura|title=MacPorts zathura package}}&lt;/ref&gt;

Zathura was named after the [[Zathura (film)|film]] of the same name.&lt;ref&gt;https://git.pwmt.org/groups/pwmt&lt;/ref&gt;

== History ==

Development on Zathura began on 12 August 2009.&lt;ref&gt;{{cite web|url=https://github.com/pwmt/zathura/commit/0eeb457bea2f93983e556d07028c2cfdb49b898c|title=Zathura initial commit}}&lt;/ref&gt; On 18 September 2009, version 0.0.1 was announced to the Arch Linux community.&lt;ref&gt;{{cite web|url=https://bbs.archlinux.org/viewtopic.php?id=80458|title=zathura - a document viewer}}&lt;/ref&gt;

Zathura has been an official Arch Linux package since April 2010.&lt;ref&gt;{{cite web|url=https://projects.archlinux.org/svntogit/community.git/log/trunk?h=packages/zathura|title=Arch Linux package history for Zathura}}&lt;/ref&gt; Same year, by the end of July it was imported into Source Mage test grimoire.&lt;ref&gt;{{cite web|url=http://scmweb.sourcemage.org/?p=smgl/grimoire.git;a=commit;h=c0963f5c0a65a0536d21a03a528ffaff4245cce7|title=zathura package in Source Mage}}&lt;/ref&gt; It has been an official Debian package since at least 2011, as part of Debian Squeeze.&lt;ref&gt;{{cite web|url=https://packages.debian.org/squeeze/zathura|title=Debian Squeeze package for Zathura}}&lt;/ref&gt;

== Features ==

Zathura automatically reloads documents. When working in compiled documents such as those written in [[LaTeX]], Zathura will refresh the output whenever compilation takes place. Zathura has the option of enabling [[inverse search]] (using "synctex").&lt;ref&gt;{{cite web|url=https://wiki.math.cmu.edu/iki/wiki/tips/20140310-zathura-fsearch.html|title=LaTeX forward/inverse searches with Zathura}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://gist.github.com/vext01/16df5bd48019d451e078|title=Vim+Zathura+Synctex}}&lt;/ref&gt;

Zathura can adjust the document to best-fit or to fit width, and it can rotate pages. It can view pages side-by-side and has a fullscreen mode. Pages can also be recolored to have a black background and white foreground.

Zathura can search for text and copy text to the [[X Window selection|primary X selection]]. It supports bookmarks and can open encrypted files.

The behavior and appearance of Zathura can be customised using a [[configuration file]]. Zathura has the ability to execute external [[List of command-line interpreters#Unix-like systems|shell]] commands. It can be opened in tabs using {{URL|http://tools.suckless.org/tabbed/|tabbed}}.&lt;ref&gt;{{cite web|url=http://taitran.ca/vim/latex/markdown/2015/03/11/vim-latex-and-markdown-preview-scripts.html|title=Vim, Latex and Markdown preview scripts}}&lt;/ref&gt;

== See also ==
{{Portal|Free software}}

* [[List of PDF software]]
* [[Zathura (film)]]

==References==
{{Reflist|30em}}

==External links==
* {{official website|http://pwmt.org/projects/zathura/}}
* {{URL|https://wiki.archlinux.org/index.php/List_of_applications/Documents#Graphical_2|Arch Linux list of document viewers}}

{{PDF readers}}

[[Category:Free PDF readers]]
[[Category:PostScript]]
[[Category:Free software programmed in C]]
[[Category:Office software that uses GTK+]]
[[Category:Electronic documents]]</text>
      <sha1>tq080uk0yruhsuag6o1mkcie88dj81j</sha1>
    </revision>
  </page>
  <page>
    <title>Data paper</title>
    <ns>0</ns>
    <id>42801446</id>
    <redirect title="Data publishing" />
    <revision>
      <id>727129301</id>
      <parentid>716138266</parentid>
      <timestamp>2016-06-26T21:57:08Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="100" xml:space="preserve">#REDIRECT [[Data publishing#Paper]]


[[Category:Data publishing]]
[[Category:Electronic documents]]</text>
      <sha1>silxgqknuobdhob719e5rgz9jdqcdqb</sha1>
    </revision>
  </page>
  <page>
    <title>Social Sciences Citation Index</title>
    <ns>0</ns>
    <id>6853403</id>
    <revision>
      <id>761250910</id>
      <parentid>760300945</parentid>
      <timestamp>2017-01-21T22:19:39Z</timestamp>
      <contributor>
        <username>Riceissa</username>
        <id>20698937</id>
      </contributor>
      <comment>/* External links */ link was broken</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3149" xml:space="preserve">{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 
| languages = 
| providers = 
| cost = 
| disciplines = Social sciences
| depth = Index &amp; citation indexing 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/social-sciences-citation-index.html
| titles = http://ip-science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=SS
}}
The '''Social Sciences Citation Index''' ('''SSCI''') is a commercial [[citation index]] product of [[Thomson Reuters]]' Healthcare &amp; Science division. It was developed by the [[Institute for Scientific Information]] from the [[Science Citation Index]].

==Overview==
The SSCI citation database covers some 3,000 of the world's leading [[academic journals]] in the [[social sciences]] across more than 50 [[academic discipline|disciplines]].&lt;ref&gt;{{cite web |title=Social Sciences Citation Index |url=http://scientific.thomson.com/products/ssci/ |accessdate=2008-06-11}}&lt;/ref&gt; It is made available online through the [[Web of Science]] service for a fee. The database records which articles are cited by other articles.

==Criticism==
[[Philip Altbach]] has criticised the Social Sciences Citation Index of favouring English-language journals generally and American journals specifically, while greatly underrepresenting journals in non-English languages.&lt;ref&gt;{{cite book |last= Altbach|first=Philip |authorlink= Philip Altbach |year=2005 |article=Academic Challenges: The American Professoriate in Comparative Perspective |title=The Professoriate: Profile of a Profession |url= |location=Dortrecht |publisher=Springer |pages=147&#8211;165 |isbn= |author-link= }}&lt;/ref&gt;

In 2004, economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.&lt;ref&gt;Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box&#8212;with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134&#8211;165.&lt;/ref&gt;

==See also==
* [[Arts and Humanities Citation Index]]
* [[Science Citation Index]]

==References==
{{reflist}}

==External links==
*{{Official website|http://ip-science.thomsonreuters.com/mjl/publist_ssci.pdf}}
*[http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]

{{Thomson Reuters}}

[[Category:Thomson Reuters]]
[[Category:Social sciences literature]]
[[Category:Citation indices]]
[[Category:Social science journals| ]]

{{database-stub}}
{{sci-stub}}</text>
      <sha1>6iwpse9ifa7aj6wnq3cu6pjopk5l23j</sha1>
    </revision>
  </page>
  <page>
    <title>Science Citation Index</title>
    <ns>0</ns>
    <id>6852678</id>
    <revision>
      <id>760552398</id>
      <parentid>760304433</parentid>
      <timestamp>2017-01-17T18:26:14Z</timestamp>
      <contributor>
        <username>Schlind</username>
        <id>25113919</id>
      </contributor>
      <comment>updated</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10341" xml:space="preserve">{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 2000-present
| languages = 
| providers = 
| cost = 
| disciplines = Science, medicine, and technology
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 

| updates = 
| p_title = 
| p_dates = 
| ISSN = 0036-827X
| web = http://thomsonreuters.com/science-citation-index-expanded/
| titles = 
}}
The '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]]).&lt;ref name=dimension&gt;
{{cite journal
|doi=10.1126/science.122.3159.108
|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas
|url=http://ije.oxfordjournals.org/content/35/5/1123.full
|format=Free web article download
|year=1955
|last1=Garfield
|first1=E.
|journal=Science
|volume=122
|issue=3159
|pages=108&#8211;11
|pmid=14385826|bibcode=1955Sci...122..108G
}}&lt;/ref&gt;&lt;ref name=evolve&gt;
{{cite journal
 |last = Garfield 
 |first = Eugene
 |doi=10.2436/20.1501.01.10
 |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf
 |format=Free PDF download
 |title=The evolution of the Science Citation Index|doi-broken-date = 2017-01-16
 }} International microbiology '''10.'''1 (2010): 65-69.&lt;/ref&gt;&lt;ref name=gOverview&gt;
{{cite web
 | last = Garfield 
 | first = Eugene
 | authorlink =
 | coauthors =
 | title = Science Citation Index
 | work = Science Citation Index 1961
 | publisher = Garfield Library - UPenn
 | date = 1963
 | url = http://garfield.library.upenn.edu/papers/80.pdf
 | format = Free PDF download
 | doi =
 | accessdate = 2013-05-27}} 
* Originally published by the Institute of Scientific Information in 1964
* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.&lt;/ref&gt;&lt;ref name=history-cite-indexing&gt;
{{cite web
 | title =History of Citation Indexing 
 | work =Needs of researchers create demand for citation indexing 
 | publisher =Thomson Ruters 
 | date =November 2010 
 | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ 
 | format =Free HTML download 
 | accessdate =2010-11-04}}&lt;/ref&gt; The larger version ('''Science Citation Index Expanded''') covers more than 8,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternatively described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.&lt;ref name=Expanded&gt;
{{cite web 
|url=https://www.thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/science-citation-index-expanded.html 
|title=Science Citation Index Expanded 
|work= |accessdate=2017-01-17}}&lt;/ref&gt;&lt;ref name=wetland&gt;{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)&lt;/ref&gt;&lt;ref name=shan&gt;
{{cite journal 
| doi= 10.1007/s11192-012-0837-z 
|title= The top-cited research works in the Science Citation Index Expanded 
|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf 
| year= 2012 
| last1= Ho 
| first1= Yuh-Shan 
| journal= Scientometrics 
| volume= 94 
| issue= 3 
| page= 1297}}&lt;/ref&gt;

The index is made available online through different platforms, such as the [[Web of Science]]&lt;ref name=AtoZ&gt;{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}&lt;/ref&gt;&lt;ref&gt;[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]&lt;/ref&gt; and SciSearch.&lt;ref&gt;{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}&lt;/ref&gt; (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",&lt;ref name=SpCI&gt;
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ 
|title=Specialty Citation Indexes 
|work= |accessdate=2009-08-30}}&lt;/ref&gt; 
such as the '''Neuroscience Citation Index'''&lt;ref name=NCI&gt;
{{cite web 
|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD 
|title=Journal Search - Science - |work= |accessdate=2009-08-30}}&lt;/ref&gt; and the '''Chemistry Citation Index'''.&lt;ref&gt;{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD 
|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}&lt;/ref&gt;

==Chemistry Citation Index==
The Chemistry Citation Index was first introduced by Eugene Garfield, a chemist by training. His original "search examples were based on [his] experience as a chemist".&lt;ref name=Garcci/&gt; In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.&lt;ref name=Garcci&gt;Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.&lt;/ref&gt;

By 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.&lt;ref&gt;
[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&amp;VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.&lt;/ref&gt; 

One 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.&lt;ref&gt;
{{cite journal
|doi=10.1007/BF02016348
|title=Science citation index and chemistry
|year=1980
|last1=Dewitt
|first1=T. W.
|last2=Nicholson
|first2=R. S.
|last3=Wilson
|first3=M. K.
|journal=Scientometrics
|volume=2
|issue=4
|page=265}}&lt;/ref&gt;

==See also==
* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.
* [[Impact factor]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.

==References==
{{Reflist|30em}}

==Further reading==
*{{cite journal
|doi= 10.1002/aris.1440360102
|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf
|title=Scholarly Communication and Bibliometrics
|year= 2005
|last1= Borgman
|first1= Christine L.
|last2= Furner
|first2= Jonathan
|journal= Annual Review of Information Science and Technology
|volume= 36
|issue= 1 
|pages=3&#8211;72}}

*{{cite journal
|doi= 10.1002/asi.20677
|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf
|format= Free PDF download
|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar
|year= 2007
|last1= Meho
|first1= Lokman I.
|last2= Yang
|first2= Kiduk
|journal= Journal of the American Society for Information Science and Technology
|volume= 58
|issue= 13
|page= 2105}}

*{{cite journal
|doi= 10.1002/asi.5090140304
|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf
|format= Free PDF download
|title= New factors in the evaluation of scientific literature through citation indexing
|year= 1963
|last1= Garfield
|first1= E.
|last2= Sher
|first2= I. H.
|journal= American Documentation
|volume= 14
|issue= 3
|page= 195}}

*{{cite journal
|doi= 10.1038/227669a0
|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf
|format= Free PDF download
|title= Citation Indexing for Studying Science
|year= 1970
|last1= Garfield
|first1= E.
|journal= Nature
|volume= 227
|issue= 5259
|pages= 669&#8211;71
|pmid= 4914589|bibcode= 1970Natur.227..669G
}}

*{{cite book
 | last =Garfield
 | first =Eugene 
 | authorlink =
 | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities
 | publisher =Wiley-Interscience
 | series = Information Sciences Series
 | edition = 1st
 | origyear = 1979| year = 1983
 | location = New York
 | isbn =9780894950247}}

==External links==
* [http://scientific.thomson.com/products/wos/ Introduction to SCI]
* [http://science.thomsonreuters.com/mjl/ Master journal list]
* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. 
* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. 
* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&amp;VER=E Chemistry Citation Index]. Chinweb.

{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]</text>
      <sha1>2sfjgeetu0x6qz4eab70liaq02ylaa2</sha1>
    </revision>
  </page>
  <page>
    <title>Conference Proceedings Citation Index</title>
    <ns>0</ns>
    <id>47141591</id>
    <revision>
      <id>670682707</id>
      <parentid>669755873</parentid>
      <timestamp>2015-07-09T14:36:31Z</timestamp>
      <contributor>
        <username>Derek R Bullamore</username>
        <id>698799</id>
      </contributor>
      <comment>Filling in 1 references using [[WP:REFLINKS|Reflinks]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1357" xml:space="preserve">{{Incomplete|date=July 2015}}
{{infobox bibliographic database
| title = Conference Proceedings Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 
| languages = 
| providers = 
| cost = 
| disciplines = 
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/
| titles = 
}}
The '''Conference Proceedings Citation Index''' ('''CPCI''') is a [[citation index]] produced by [[Thomson Reuters]] covering [[conference proceedings]].&lt;ref&gt;{{cite web|url=http://wokinfo.com/media/pdf/proceedingswhtpaper.pdf |format=PDF |title=White Paper : Conference Proceddings and Their Impact on Global Research |publisher=Wokinfo.com |accessdate=2015-07-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/cpciessay/|title=CPCI Essay - IP &amp; Science - Thomson Reuters|author=Thomson Reuters|publisher=Wokinfo.com|accessdate=2015-07-09}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{official website|http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/}}

{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
[[Category:Conference proceedings]]</text>
      <sha1>lt2ut3s2escbshe7x36xpyong89bo2i</sha1>
    </revision>
  </page>
  <page>
    <title>ScienceOpen</title>
    <ns>0</ns>
    <id>48598824</id>
    <revision>
      <id>757592311</id>
      <parentid>733034837</parentid>
      <timestamp>2016-12-31T15:35:24Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>/* top */changed {{Notability}} to {{Notability|Organizations}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11635" xml:space="preserve">{{Multiple issues|
{{notability|Organizations|date=February 2016}}
{{refimprove|date=February 2016}}
}}

{{Infobox organization
| name                = ScienceOpen
| native_name         = 
| native_name_lang    = 
| named_after         = 
| image               =
| image_size          = 300px
| alt                 =  ScienceOpen's logo
| caption             = 
| map                 = 
| map_size            = 
| map_alt             = 
| map_caption         = 
| map2                = 
| map2_size           = 
| map2_alt            = 
| map2_caption        = 
| abbreviation        = 
| motto               = 
| predecessor         = 
| merged              = 
| successor           = 
| formation           = &lt;!-- use {{start date|YYYY|MM|DD|df=y}} --&gt;
| founder             = 
| founding_location   = 
| extinction          = &lt;!-- use {{end date and age|YYYY|MM|DD}} --&gt;
| merger              = 
| type                = 
| tax_id              = &lt;!-- or | vat_id = (for European organizations) --&gt;
| registration_id     = &lt;!-- for non-profit org --&gt;
| status              = 
| purpose             = 
| headquarters        = [[Berlin]], [[Germany]]
| location            = 
| coords              = &lt;!-- {{coord|LAT|LON|display=inline,title}} --&gt;
| region              = 
| services            = 
| products            = 
| methods             = 
| fields              = 
| membership          = 
| membership_year     = 
| language            = 
| owner               = &lt;!-- or | owners = --&gt;
| sec_gen             = 
| leader_title        = 
| leader_name         = 
| leader_title2       = 
| leader_name2        = 
| leader_title3       = 
| leader_name3        = 
| leader_title4       = 
| leader_name4        = 
| board_of_directors  = 
| key_people          = 
| main_organ          = 
| parent_organization = 
| subsidiaries        = 
| secessions          = 
| affiliations        = 
| budget              = 
| budget_year         = 
| revenue             = 
| revenue_year        = 
| disbursements       = 
| expenses            = 
| expenses_year       = 
| endowment           = 
| staff               = 
| staff_year          = 
| volunteers          = 
| volunteers_year     = 
| slogan              = 
| mission             = 
| website             = {{URL|scienceopen.com}}
| remarks             = 
| formerly            = 
| footnotes           = 
}}
'''ScienceOpen''' is a privately owned discovery and research network with three roles:  aggregation, [[Open access|open access publishing]] and the evaluation of scholarly literature in all scholarly disciplines.

== History ==
ScienceOpen began in 2013&lt;ref&gt;{{Cite web|title = OA interviews: Alexander Grossmann, ScienceOpen - Open-access publishing - Research Information|url = http://www.researchinformation.info/features/feature.php?feature_id=473|website = www.researchinformation.info|accessdate = 2015-11-19}}&lt;/ref&gt; when Alexander Grossmann, a professor of Publishing Management at the [[Leipzig University of Applied Sciences]] and former publishing director at scientific house [http://www.degruyter.com/dg/page/15/the-publishing-house De Gruyter], and Tibor Tscheke, president and CEO of content management systems company [http://ovitas.com/ Ovitas], decided to start a platform. Their idea was to allow researchers to share scientific information, both formally by publishing articles and participating in [https://futureofscipub.wordpress.com/open-post-publication-peer-review/ post-publication peer review], and informally by reviewing their colleagues&#8217; work, providing endorsements and comments, and by updating their own papers.

Its beta version was introduced in November 2013, and release 1.0 launched in May 2014.&lt;ref&gt;{{Cite web|title = ScienceOpen: the next wave of Open Access? - EuroScientist Webzine|url = http://www.euroscientist.com/scienceopen-next-wave-open-access|website = EuroScientist Webzine|accessdate = 2015-11-19|language = en-US}}&lt;/ref&gt; As of September 2015 the site has 10 million articles and records&lt;ref&gt;{{Cite web|title = Open and Shut?: The OA Interviews: ScienceOpen&#8217;s Alexander Grossmann|url = http://poynder.blogspot.com/2015/11/the-oa-interviews-scienceopens.html|website = Open and Shut?|date = 2015-11-16|accessdate = 2015-11-19|first = Richard|last = Poynder}}&lt;/ref&gt; from [[PubMed Central]], [[ArXiv]], [[PubMed]] and ScienceOpen, and a publicly available citation index which is free for researchers to use wherever they are and is provided at no cost to libraries, which in February 2016 was dubbed the Open Citation Index.&lt;ref&gt;http://blog.scienceopen.com/2016/02/the-open-citation-index/&lt;/ref&gt; All content on the platform is available for post-publication peer review by scientific members with five or more peer-reviewed publications on their [[ORCID]], and all articles can be publicly commented on by members with one or more items.

ScienceOpen appoints members of the research community as Collection Editors&lt;ref&gt;{{Cite web|title = ScienceOpen Collections|url = http://about.scienceopen.com/scienceopen-collections/#more-390|website = About ScienceOpen|accessdate = 2015-11-19|language = en-US}}&lt;/ref&gt; who curate articles from multiple publishers in any topic. [[Thieme Medical Publishers|Thieme]], a German medical publisher, mirrors three open access journals [https://www.scienceopen.com/collection/Thieme on the platform].

The organization is based in Berlin and has a technical office in Boston. It is a member of [[CrossRef]], [[ORCID]], the [[Open Access Scholarly Publishers Association]]&lt;ref&gt;{{Cite web|url=http://oaspa.org/member/scienceopen/|title=Member Record: ScienceOpen|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; and the [[Directory of Open Access Journals]]. The company was designated as one of  &#8220;10 to Watch&#8221; by research advisory firm Outsell in its report &#8220;[http://img.en25.com/Web/CopyrightClearanceCenterInc/%7Bfc9f07ac-b2c9-4cd7-b763-2f21e0c6e94b%7D_Outsell_2015_Open_Access_Report.pdf Open Access 2015: Market Size, Share, Forecast, and Trends].&#8221;

In 2015, Tscheke provided further clarification of ScienceOpen&#8217;s focus on aggregation and filtering content.&lt;ref&gt;{{Cite web|title = There&#8217;s more to Open Access than APCs, right? &#8211; ScienceOpen Blog|url = http://blog.scienceopen.com/2015/10/theres-more-to-open-access-than-apcs-right/|website = blog.scienceopen.com|accessdate = 2015-11-19}}&lt;/ref&gt;

== Business model ==
ScienceOpen publishes articles of almost any type and from any research field, including the social sciences and humanities. This includes primary research articles, opinion papers, posters, case studies, negative results, and data publications. To fund article publication, ScienceOpen charges a publication fee ($800 as of this time of writing, in 2015) to be paid by the author or the author&#8217;s employer, funder or library. This is for a post-publication peer review process and publication after editorial control,&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/06/review-instructions-for-scienceopen/|title=Review Instructions for ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; and is facilitated through integration with [[ORCID]].&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/06/orcid-integration-at-scienceopen/|title=ORCID integration at ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; Additionally, authors can opt to use the 'Peer Review by Endorsement' model,&lt;ref&gt;{{cite journal |last= Jan Velterop |date= 29 September 2015 |title= Peer review &#8211; issues, limitations, and future development |url= https://www.scienceopen.com/document?15&amp;vid=1dcfbe69-c30c-4eaa-a003-948c9700da40 |journal= ScienceOpen Research |doi= 10.14293/S2199-1006.1.SOR-EDU.AYXIPS.v1 |access-date=14 June 2016}}&lt;/ref&gt; in which peer review is conducted prior to submission, and for a fee of $400. Included in this fee are up to two article revisions within 12 months, with full version control. Revised versions have a new DOI so that it is easier to link back to cited versions. A partial or full fee waiver is available to those who demonstrate need. [[Poster session]] publishing is free. All published articles are published via a [[Creative Commons]] Attribution 4.0 International Public License.&lt;ref&gt;{{Cite web|url=http://about.scienceopen.com/open-access-explanation-of-cc-by-license/|title=Open Access License Agreement|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

From 2016, ScienceOpen has started partnering with publishers to offer advanced indexing services. In June 2016 they partnered with [[SciELO]], the largest publisher in Latin America{{Cite web|title = ScienceOpen helps to put scientific research in a global context with more than 15 million article records &#8211; ScienceOpen Blog|url = http://blog.scienceopen.com/2016/06/scienceopen-helps-to-put-scientific-research-in-a-global-context-with-more-than-15-million-article-records/|website = blog.scienceopen.com|accessdate = 2016-06-14}}. Additional publishing partners include Higher Education Press&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/04/higher-education-press-indexing-partnership-with-scienceopen/|title=Higher Education Press indexing partnership with ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; and The Italian Society of Victimology.&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/06/welcome-to-the-italian-society-of-victimology/|title=Welcome to the Italian Society of Victimology|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; In September 2015, ScienceOpen hit the 10 million article record mark,&lt;ref&gt;{{Cite web|url=http://www.prnewswire.com/news-releases/scienceopen-hits-the-10-million-article-mark-527671151.html|title=ScienceOpen Hits the 10 Million Article Mark|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; and as of 27 June 2016 has more than 13 million records.

Every research article on ScienceOpen has a traceable genealogy through citations, a public peer review process, and social interaction tracked by [[altmetrics]], which they call research "context".&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/05/why-context-is-important-for-research/|title=Why &#8216;context&#8217; is important for research|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; The technology behind the ScienceOpen platform is provided by Ovitas.

ScienceOpen are findable on [https://www.youtube.com/user/ScienceOpen YouTube], [https://www.linkedin.com/company/scienceopen-inc- LinkedIn], [https://www.facebook.com/scienceopen/ Facebook], and [https://twitter.com/science_open Twitter].

== Publications ==
* [https://www.scienceopen.com/collection/scienceopen_research ScienceOpen Research], ISSN [https://www.worldcat.org/search?q=n2%3A2199-1006&amp;qt=results_page 2199-1006]
* [https://www.scienceopen.com/collection/scienceopen_posters ScienceOpen Posters], ISSN [https://www.worldcat.org/search?q=n2%3A2199-8442&amp;qt=results_page 2199-8442]

== Headquarters ==
ScienceOpen has its headquarters located at Pappelallee 78-79, 10437 Berlin and its technical hub is at 60 Mall Rd., Burlington, Mass.

== See also ==
* [[Open Access Scholarly Publishers Association]] (OASPA)
* [[Directory of Open Access Journals]] (DOAJ)
* [[Registry of Open Access Repositories Mandatory Archiving Policies]] (ROARMAP)

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.scienceopen.com}}

[[Category:Open access publishers]]
[[Category:Open access (publishing)]]
[[Category:Scholarly communication]]
[[Category:Citation indices]]</text>
      <sha1>m90rf9cano1dutysci467eyu29raiyu</sha1>
    </revision>
  </page>
  <page>
    <title>World Radio TV Handbook</title>
    <ns>0</ns>
    <id>1536556</id>
    <revision>
      <id>745298325</id>
      <parentid>620207475</parentid>
      <timestamp>2016-10-20T10:03:23Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Publications */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3122" xml:space="preserve">{{redirect|WRTH|the radio station|WRTH (FM)}}
The '''''World Radio TV Handbook''''', also known as '''''WRTH''''', is a [[directory (databases)|directory]] of virtually every [[Radio station|radio]] and [[TV station]] on Earth, published yearly. It was started in 1947 by [[Oluf Lund Johansen]] (1891&#8211;1975) as the ''World Radio Handbook'' (WRH).&lt;ref&gt;[http://oz6gh.byethost33.com/lund_johansen.htm O. Lund-Johansen], presented by OZ6GH.&lt;/ref&gt; The word "TV" was added to the title in 1965, when [[Jens M. Frost]] (1919&#8211;1999) took over as editor.&lt;ref&gt;[http://www.dswci.org/specials/membersofhonour/jens_frost.html DSWCI Member of Honour:  Jens M. Frost]&lt;/ref&gt; It had then already included data for [[television broadcasting]] for some years. After the 40th edition in 1986, Frost handed over editorship to [[Andrew G. Sennitt|Andrew G. (Andy) Sennitt]].&lt;ref&gt;[http://www.agsmedia.nl/body_who.html Andy Sennitt], own presentation.&lt;/ref&gt;

The first edition that bears an edition number is the 4th edition, published in 1949. The three previous editions appear to have been:
* the 1st edition, marked "Winter Ed. 1947" on the cover and completed in November 1947
* the 2nd edition, marked "1948 (May-November)" on the cover and completed in May 1948
* the 3rd edition, marked "1948-49" on the cover and completed in November 1948.

Summer Supplements appear to have been issued from 1959 through 1971. From 1959 through 1966 they were called the Summer Supplement. From 1967 through 1971 they were called the Summer Edition.

Through the 1969 edition, the WRTH indicated the date on which the manuscript was completed.

Issues with covers in Danish are known to have been available for the years 1948 May-November (2d ed.), 1950-51 (5th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), 1952 (6th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), and probably others. The 1952 English ed., which is completely in English, has an extra page with world times and agents, and ads in English which are sometimes different from the ads in the Danish edition. Also, the 1953 ed. mentions the availability of a German edition.

[[Oluf Lund Johansen]] published, in conjunction with [[Libreria Hispanoamericana]] of [[Barcelona]], Spain, a [[softbound]] Spanish-language version of the 1960 WRTH. The book was printed in Spain and called ''Guia Mundial de Radio y Television'', and carried the WRTH logo at the time as well as all the editorial references contained in the English-language version. 

Hardbound editions are known to have been available for the years 1963 through 1966, 1968, 1969, and 1975-1978, and probably others.

== Publications ==
* Gilbert, Sean; Nelson, John; Jacobs, George, [https://books.google.com/books?id=IBu8NHvC4fMC&amp;printsec=frontcover ''World Radio TV Handbook 2007''], Watson-Guptill, 2006. ISBN 0-9535864-9-9.

==References==
{{reflist}}

== External links ==
* http://www.wrth.com/

[[Category:Radio organizations]]
[[Category:Television organizations]]
[[Category:International broadcasting]]
[[Category:Directories]]
[[Category:1945 introductions]]</text>
      <sha1>3gtsdqcx7h3ctetf090q0i6u3k6n213</sha1>
    </revision>
  </page>
  <page>
    <title>MiM</title>
    <ns>0</ns>
    <id>10926747</id>
    <revision>
      <id>601740140</id>
      <parentid>524168445</parentid>
      <timestamp>2014-03-29T00:58:24Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10065)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="910" xml:space="preserve">{{Other uses|MIM (disambiguation)}}
{{Orphan|date=February 2009}}
'''MIM''' stands for Music Industry Manual. It was founded in 1996 by James Robertson and in its first year was called The Promoter's Handbook. The Promoters Handbook was a reference manual for the [[dance music]] industry including [[DJ]]s agents, [[nightclub]]s and unusual venues, promoters, flyer designers. The following year its title was changed to give it a broader appeal.

It still caters for the dance music industry, but is now fully international with over 100,000 contacts from countries as remote as [[Azerbaijan]] to developed nations. Whilst the focus is still on DJ and club culture it has over 100 categories including bar designer, music lawyers, event management.

==External links==
* [http://www.mim.dj Official web site]

{{primary sources|date=August 2007}}

[[Category:Directories]]
[[Category:Electronic dance music]]</text>
      <sha1>fz1k66wq8yr2yzed7o4obbyy09t8fdl</sha1>
    </revision>
  </page>
  <page>
    <title>American Art Directory</title>
    <ns>0</ns>
    <id>21099152</id>
    <revision>
      <id>743781910</id>
      <parentid>740762621</parentid>
      <timestamp>2016-10-11T06:56:16Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4042" xml:space="preserve">{{Infobox Magazine
| title           = American Art Directory
| image_file      = American Art Directory logo, 1898.svg
| image_size      = 140px
| image_caption   = ''Frontispiece from 1898 volume''
| editor          = 
| editor_title    = 
| previous_editor = 
| staff_writer    = 
| frequency       = [[Annual publication|Annual]]
| circulation     = 
| category        = 
| company         = 
| publisher       = 
| firstdate       = 1898
| country         = 
| based           = 
| language        = [[English language|English]]
| website         = http://www.americanartdir.com/
| issn            = 0065-6968
}}

The '''''American Art Directory''''' is a yearly publication covering [[art museum]]s, [[Arts centre|arts centers]], and [[Art school|art educational institutions]] as well as news, obituaries, book and magazine publications, etc. related to the artistic community in the [[United States]].  Established in 1898, it was originally entitled ''American Art Annual''.

Art consultant, advisor, author, and independent appraiser [[Alan Bamberger]] describes the Directory as "...a required reference for art museums, libraries, arts organizations, art schools, and corporations with art holdings."&lt;ref&gt;{{citation
 |first1=Alan S. 
 |last1=Bamberger 
 |title=Who's Who In American Art, Official Museum Directory, American Art Directory 
 |publisher=ArtBusiness.com 
 |accessdate=2009-01-14 
 |url=http://www.artbusiness.com/revs0608.html 
 |deadurl=bot: unknown 
 |archiveurl=http://www.webcitation.org/5dphZr5S6?url=http%3A%2F%2Fwww.artbusiness.com%2Frevs0608.html 
 |archivedate=2009-01-15 
 |df= 
}}, archived by [[WebCite]] &lt;/ref&gt;

A yearly feature is the "Review of the Year" article discussing the touring exhibitions, commissions, grants to organizations, construction starts at museums and other facilities, and various other events that occur within the art community.

Initially the directory was the work of the [[New York City|New York]] area artist [[Florence Nightingale Levy]] and published by [[Macmillan Publishers|The Macmillan Company]].&lt;ref name="NYTObit"&gt;{{Citation
  | title = NOTES AND NEWS.; Items Gathered During This Week's Tour of the Publishing Houses.
  | newspaper = [[The New York Times]]
  | date = April 1, 1899
  | url = http://query.nytimes.com/gst/abstract.html?res=9407E1DF1538E433A25752C0A9629C94689ED7CF}}&lt;/ref&gt;  The [[American Federation of Arts]], with which Mrs. Levy was associated and which she would later become the president of, was founded in 1909&lt;ref&gt;{{citation
  | title = About the AFA
  | publisher = [[American Federation of Arts]]
  | accessdate = 2009-01-14
  | url = http://www.afaweb.org/about/}}&lt;/ref&gt; and in 1913 the directory became an official publication of that organization.&lt;ref name="Torchbearers"&gt;{{citation
  | first1 = Karen J.
  | last1 = Blair
  | authorlink1 = Karen J. Blair
  | title = The Torchbearers: Women and Their Amateur Arts Associations in America, 1890-1930
  | publisher = [[Indiana University Press]]
  | location = [[Bloomington, Indiana|Bloomington]]
  | year = 1994
  | isbn = 978-0-253-31192-4
  | page = 80
  | oclc = 27677514
  | url = https://books.google.com/books?id=wP5pq2aBYBAC&amp;printsec=frontcover#PPA80,M1}}&lt;/ref&gt;  It later became the independent publication it exists as currently.

In 1952 the ''American Art Annual'' was split into two separate publications, ''[[Who's Who in American Art]]'' and the ''American Art Directory''.&lt;ref&gt;{{citation
  | title = Who&#8217;s Who in American Art
  | publisher = [[R. R. Bowker]]
  | location = [[New York City|New York]]
  | year = 1953
  | issn = 0000-0191}}&lt;/ref&gt;

==References==
{{Reflist}}
&lt;div style="text-align:center;height:3em;"&gt;&amp;#160;&lt;/div&gt;

==External links==
* [http://www.americanartdir.com/ Official website]

[[Category:Books about visual art]]
[[Category:Annual magazines]]
[[Category:Magazines established in 1898]]
[[Category:Directories]]
[[Category:Arts in the United States]]
[[Category:American arts magazines]]


{{art-mag-stub}}
{{US-arts-org-stub}}
{{art-book-stub}}</text>
      <sha1>julw8nnntiu5p0to5shwga9og65e70c</sha1>
    </revision>
  </page>
  <page>
    <title>Bulmer (directories)</title>
    <ns>0</ns>
    <id>5853521</id>
    <revision>
      <id>558920617</id>
      <parentid>557540683</parentid>
      <timestamp>2013-06-08T15:30:47Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor />
      <comment>clean up / checkwiki error #52 (category not at the end) using [[Project:AWB|AWB]] (9241)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1748" xml:space="preserve">'''Bulmer''' was a [[Victorian era|Victorian]] [[historian]], [[surveying|surveyor]], [[cartographer]] and compiler of [[Yellow Pages|directories]]. His directories provided a history and geography of a particular area. The directories listed and described all parishes; listed trades and professions and provided a helpful street index with the names of residents, together with other local information. Data CDs of Bulmer Directories are available from publishers in the UK.&lt;ref&gt;[[S&amp;N Genealogy Supplies|S&amp;N Publishing]]&lt;/ref&gt;

==List of directories==
* Bulmer's History, Topography and Directory of East Cumberland, 1883
* Bulmer's History, Topography and Directory of West Cumberland, 1884.
* Bulmer's History, Directory and Topography of Westmorland, 1885
* Bulmer's History, Topography and Directory of Northumberland (Hexham Division), 1886
* Bulmer's History, Topography and Directory of Northumberland (Tyneside, Wansbeck and Berwick Divisions), 1887
* Bulmer's History, and Directory of Newcastle upon Tyne, 1887
* Bulmer's History, and Directory of North Yorkshire, 1890 (two Volumes)
* Bulmer's History, Topography and Directory of East Yorkshire and Hull, 1892
* Bulmer's Directory of Cumberland, 1901
* T. Bulmer: History, Topography and Directory of Westmorland, 1906
* T Bulmer: History, Topography and Directory of Furness and Cartmel, 1911
* Bulmer's History, Topography and Directory of Furness, Cartmel and Egremont division of Cumbria, 1911
* T Bulmer: History, Topography and Directory of Lancaster and district, 1912
* J Bulmer: History, Topography and Directory of Lancaster and district, 1913

== References ==

{{reflist}}

{{DEFAULTSORT:Bulmer (Directories)}}
[[Category:Directories]]


{{UK-hist-stub}}
{{Ref-book-stub}}</text>
      <sha1>slkz3r559cqzz40wmh98zekzwlstw9a</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directory assistance services</title>
    <ns>14</ns>
    <id>28706812</id>
    <revision>
      <id>410106072</id>
      <parentid>382986994</parentid>
      <timestamp>2011-01-26T04:02:55Z</timestamp>
      <contributor>
        <username>Pnm</username>
        <id>5795</id>
      </contributor>
      <comment>added [[Category:Information by telephone]] using [[Help:Gadget-HotCat|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="143" xml:space="preserve">[[Category:Directories]]
[[Category:Telephone services]]
[[Category:Telephone service enhanced features]]
[[Category:Information by telephone]]</text>
      <sha1>c06q36jxdhug7iqrdpjbp00lfufwnzb</sha1>
    </revision>
  </page>
  <page>
    <title>Pigot's Directory</title>
    <ns>0</ns>
    <id>20147149</id>
    <revision>
      <id>743989939</id>
      <parentid>712825473</parentid>
      <timestamp>2016-10-12T12:33:18Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* List of Pigot&#8217;s Trade Directories by date */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3659" xml:space="preserve">[[File:PigotDirectory1839Kent.jpg|thumb|An example page from Pigot's 1839 directory of Kent, Surrey and Sussex. This page has information on Bromley and Canterbury]]
'''Pigot's Directory''' was a major British [[Trade directory|directory]] started in 1814 by [[James Pigot]].&lt;ref&gt;{{cite web | url=http://www.hertfordshire-genealogy.co.uk/data/directories/directories-pigot.htm | title=Trade directories in Hertfordshire}}&lt;/ref&gt;

Pigot's Directories covered England, Scotland, and Wales in the period before official Civil Registration began and are a valuable source of information regarding all major professions, nobility, gentry, clergy, trades and occupations including taverns and public houses and much more are listed. There are even timetables of the coaches and carriers that served a town.

Parishes are listed for each area with useful information including the number of inhabitants, a geographical description and the main trades and industries of the area or town.

{{TOC right}}

==List of Pigot&#8217;s Trade Directories by date==
* {{cite book |url=https://books.google.com/books?id=4llGAAAAYAAJ |title= Commercial Directory for 1818-19-20 |location=Manchester |publisher=James Pigot |year=1818 }}
* {{Citation |publication-place = London |publisher = J. Pigot &amp; Co. |url =https://books.google.com/books?id=zBEHAAAAQAAJ |title = Pigot &amp; Co.'s metropolitan guide &amp; book of reference to every street, court, lane, passage, alley and public building, in the cities of London &amp; Westminster, the borough of Southwark, and their respective suburbs |publication-date = 1824 }}
* {{cite book |url= https://books.google.com/books?id=hdMHAAAAQAAJ |title= Pigot &amp; Co.'s National Commercial Directory for 1828-9 |location=London |publisher=James Pigot }}
*{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =J. Pigot &amp; Co. }}

==List of Pigot&#8217;s Trade Directories by geographic coverage==
*[[Bedfordshire]] 1839
*[[Cambridgeshire]] 1839
*[[Cambridgeshire]] 1830
*[[Derbyshire]] 1835
*[[Durham, England|Durham]] 1834
*[[Essex]] 1839
*[[Herefordshire]] 1835
*[[Hertfordshire]] 1839
*[[Huntingdonshire]] 1830
*[[Huntingdonshire]] 1839
*[[Kent]] 1839
*[[Leicestershire]] 1835
*[[Lincolnshire]] 1835
*[[London]] 1839
*[[Middlesex]] 1839
*[[Monmouthshire (historic)|Monmouthshire]] 1835
*[[Norfolk]] 1839
*[[North Wales]] 1835
*[[Northumberland]] 1828
*[[Northumberland]] 1834
*[[Nottinghamshire]] 1835
*[[Rutlandshire]] 1835
*[[Shropshire]] 1835
*[[South Wales]] 1835
*[[Staffordshire]] 1835
*[[Suffolk]] 1830
*[[Suffolk]] 1839
*[[Surrey]] 1839
*[[Sussex]] 1839
*[[Sussex]] 1840
*[[Warwickshire]] 1835
*[[Worcestershire]] 1835

==References==
{{reflist}}

==External links==
* {{citation |title=Historical Directories - England &amp; Wales |publisher=[[University of Leicester]] |location=UK |url=http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4}}. Includes digitized Pigot's &amp; Slater's directories for England &amp; Wales, various dates
* {{citation |title=Historical Directories - Scotland |publisher=[[National Library of Scotland]] |location=UK |url=http://www.nls.uk/family-history/directories/post-office/index.cfm?place=Scotland }}. Includes digitized Pigot's &amp; Slater's directories for Scotland, various dates

[[Category:Directories]]


{{ref-book-stub}}</text>
      <sha1>5h7qfuctqsjn228pou7vjz0175py7l5</sha1>
    </revision>
  </page>
  <page>
    <title>Army List</title>
    <ns>0</ns>
    <id>31790826</id>
    <revision>
      <id>731540468</id>
      <parentid>687562000</parentid>
      <timestamp>2016-07-26T00:39:40Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3094" xml:space="preserve">The '''''Army List''''' is a list (or more accurately seven series of lists) of serving regular, [[militia]] or territorial [[British Army]] officers, kept in one form or another, since 1702.

Manuscript lists of army officers were kept from 1702&#8211;1752, the first official list being published in 1740.

==Regular army==
* Annual Army Lists, 1754&#8211;1879 (WO 65)
* Quarterly Army Lists (First Series), 1879&#8211;1922
* Half-Yearly Army Lists, 1923 - Feb 1950 (From 1947, annual, despite the name)
* Modern Army Lists, 1951-Ongoing
** Part 1; serving officers.
** Part 2; retired officers, as of 2011 four-yearly
** Part 3; the Gradation List, a short biography of officers, a restricted publication  not generally available.

==Other lists==
* Monthly Army Lists, 1798-June 1940. Officers of colonial, militia and territorial units are included.
* Quarterly Army Lists (Second Series), July 1940-December 1950. These superseded the Monthly Army Lists, and, for the remainder of [[World War II]] were not published but  produced as confidential documents, monthly or bi-monthly until December 1943 and quarterly until April 1947, then three times a year, April, August and December.
* [[British Home Guard|Home Guard]] List, 1939&#8211;1945
* Militia Lists - various militia lists pertaining to the eighteenth and nineteenth centuries are extant.
* ''[[Hart's Army List]]'', an unofficial list, produced between 1839 and 1915, containing details of war service which the official lists started covering only in 1881.

==Further reading and bibliography==
* ''The army lists of the [[Roundheads and Cavaliers]]: containing the names of the officers in the Royal and Parliamentary armies of 1642'', [[Edward Peacock (antiquary)|Edward Peacock]] (ed) (1874)
* ''English army lists and commission registers, 1661&#8211;1714'', [[Charles Dalton]] (ed) (1892&#8211;1904)
* [[Henry George Hart]], ''Hart's army list : the new army list exhibiting the rank, standing, and various services of every officer in the Army on full pay'' (1839-)
* William Spencer, ''Army service records of the First World War'' (seventh edition, 2006)

==See also==
* [[Navy List]]
* [[Crockford's Clerical Directory]]
*

==References==
{{Reflist}}

==External links==
* [https://books.google.com/books?id=p_BfsBzDzWYC The 1740 Army List] at [[google books]]
* [http://discovery.nationalarchives.gov.uk/SearchUI/Collection/Display?iaid=C14273&amp;parentiaid=C543 War Office: Printed Annual Army Lists 1754-1879 (WO 65) - download for free]
* Digitised copies of 'Quarterly army lists' from [http://digital.nls.uk/97136046 1913 to 1919] and from [http://digital.nls.uk/97136048 1940 to 1946] at [[National Library of Scotland]]
* Digitised copies of 'Half-yearly army lists' from [http://digital.nls.uk/97136047 1938 to 1941] at National Library of Scotland
* [http://www.nationalarchives.gov.uk/records/research-guides/british-army-lists.htm British Army Lists] ([[National Archives]]' Research Guide)
* [https://archive.org/details/nlsarmylists Hart's Army List] at the [[Internet Archive]]

[[Category:Directories]]
[[Category:British Army]]</text>
      <sha1>stwgxgnsj29ohxtwqaf899lsl8i5004</sha1>
    </revision>
  </page>
  <page>
    <title>Corporate Technology Directory</title>
    <ns>0</ns>
    <id>33232179</id>
    <revision>
      <id>746857071</id>
      <parentid>692300833</parentid>
      <timestamp>2016-10-30T00:24:01Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1024" xml:space="preserve">The '''Corporate Technology Directory''' also known as the '''CorpTech directory of technology companies''' was a [[Trade directory|directory]] of technology companies published from 1986&lt;!-- maybe 1987 --&gt;-2004 by [[CorpTech]]. It listed thousands of technology companies including software, services, and hardware as well as developers.

The directory was later made available in digital form as a cd&lt;ref name="SterlingBracken1998"&gt;{{cite book|last1=Sterling|first1=Christopher H.|last2=Bracken|first2=James K.|last3=Hill|first3=Susan M.|title=Mass communications research resources: an annotated guide|url=https://books.google.com/books?id=kwOo6BiWiFkC&amp;pg=PA10|accessdate=27 September 2011|year=1998|publisher=Psychology Press|isbn=978-0-8058-2024-9|pages=10&#8211;}}&lt;/ref&gt; and subsequently database subscription.

==See also==
* [[Major Information Technology Companies of the World]]

==References==
{{reflist}}
&lt;!-- https://books.google.com/books?id=96XpVBf6pvAC&amp;pg=PA246 --&gt;

{{technology-stub}}

[[Category:Directories]]</text>
      <sha1>nfty50kh02sfc01jm82d1fdotppi5sl</sha1>
    </revision>
  </page>
  <page>
    <title>Royal Almanac</title>
    <ns>0</ns>
    <id>31485471</id>
    <revision>
      <id>729621945</id>
      <parentid>625425900</parentid>
      <timestamp>2016-07-13T13:27:35Z</timestamp>
      <contributor>
        <username>LouisAlain</username>
        <id>14909828</id>
      </contributor>
      <comment>link to new article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34262" xml:space="preserve">{{unreferenced|date=May 2011}}

The '''Royal Almanac''' is a French administrative directory founded in 1683 by the [[bookseller]] Laurent d'Houry, which appeared under this title from 1700 to 1792, and under other titles until 1919.

He presented each year in the official [[order of precedence]], the list of members of the [[royal family]] of [[France]], the princes of blood, and the main body of the kingdom, great crown officers, senior clerics, abbots of large abbeys (with income of each abbey), marshals of France, colonels and general officers, ambassadors and consuls of France, presidents of the main courts, state councilors, bankers, etc..

Despite the fact that he could present indigestible because of the many lists that he was composed, he enjoyed a wide circulation with a readership consisting primarily of financial, political and all persons who had an interest in knowing the administrative organization of France.

Although his edition is due to the initiative of a private publisher, included in the lists of the Almanac was a royal official and abuse were therefore punished. Thus, a [[Poitou|Poitevin]], Pierre Joly, was interned in the [[Bastille]] at the end of the eighteenth century to have usurped the banking profession by being registered as such in the Almanach Royal.

His edition was in regular format in-8 o editor with a binder leather adorned with a sprinkling of [[fleur de lys]] gold.

==History==
===Founded at the request of King===
Laurent d'Houry imagines a calendar or [[Almanac]] 1683. The first edition contained only a few pages with a calendar and omens for the coming year. The last edition in this form, in 1699, already shows some lists that foreshadow the upcoming Royal Almanac. Thus we find lists of counselors of state with their ordinary homes, the commissioners of the Board, auditors general and stewards of finances, the Chancellor, archbishops and bishops of France, universities, and the list of major exhibitions, sessions of the courts of [[Paris]] and the log of the Palace, and finally addresses the messengers and items indicating the day of departure.

In 1699, [[Louis XIV]] asked him what the author describes in detail his work. Here as his widow explains these beginnings :

:"Louis XIV, who wanted this glorious memory Almanac, made him ask the author, who had the honor to present to Her Majesty's what induced him to give the title of Royal Almanac, &amp; to make it his principal occupation of this work. "

The same year [[Louis Tribouillet]], chaplain of the king and canon of [[Meaux]], publishes its State of France. This book describes in detail the functioning of the Court of the King, all his ministers, the treatments they receive, the various expenses of the state, clergy, etc..

The first edition of the Almanac Royal appears in 1700, at the same time as another book, Calendar of the Court of John Colombat, one of the printers of the King. Parisians have a choice between three books with similar content: the Almanach Royal Houry, Calendar of the Court of Colombat and the State of France Tribouillet. At that time, the yard around Louis XIV is highly stratified and since the expansion of the [[Palace of Versailles]] in 1684, it continues to grow. In this context, recognition of peers is a valuable asset "if someone has just placed a new post is an overflow of praise in its favor during the floods and the Chapel (...) but c is that while envy and jealousy talk like adulation.. One can understand the need to maintain directories so that everyone can follow the evolution of all these people. The multiplicity of these publications so says the king's will want to "officially" referencing his [[courtier]]s to charges created to keep beside him at [[Palace of Versailles|Versailles]], and maintain the jealousies of each other.

Even if the king gave his approval, publishing such a book is not without risk. In December 1708, Laurent d'Houry is being prosecuted for having established a [[printing press]] in his house and forced to sell its equipment two months later. Then, in February 1716, he was imprisoned in the Bastille on complaint of the [[Earl of Stair]], the British Ambassador, "to be disrespectful in his almanac, the King George by not naming him not as king of England, or rather from Great Britain, and mention the king as son of Jacques II Stuart, exiled to St. Germain.

===Affirmation of a monopoly===
The Almanac and the Royal Court Calendar coexist peacefully for ten years and a lot of money to their authors, but from 1710, Laurent d'Houry integrates more and more topics like the book Colombat Biblio. The abscess broke out in 1717 when Houry Almanac releases its Abstract that will follow the format of the Calendar of the Court and simultaneously filing a [[lawsuit]] against its competitor. A Judgement of Solomon is made: if it is allowed to Houry now to continue the publication of its abstract and to counterfeit the Court Calendar, Colombat is obliged to freeze its calendar format and forbidden 'expand content. This stops any changes in the calendar of the Court, leaving the way open to the Almanac.

Upon the death of Laurent Houry in 1725, his family is [[destitute]]. Revenues from sales of the Almanac are not sufficient to cover expenses of printing and bookselling. In these circumstances, his widow, Elizabeth Dubois, took over the business. Their son Charles-Maurice, who had hitherto been a mere [[proofreader]] of the Almanac, is trying to evict her mother and she is suing cons. It prepares the edition of 1726 but a ruling forbade him to publish it in his name alone. The ruling of 11 December 1726 forbids even disturb the affairs of his mother and to participate directly or indirectly to the development of the Almanac. That is why Charles-Maurice is mentioned as editor of the Almanac on the edition of 1726.

In 1731, she filed a new lawsuit against Colombat which increased its schedule despite the prohibition of 1717. Unsuccessful, she resumed the publication of the Abstract and Colombat complaint in turn, she then accepts to abandon the publication of the Abstract "if returned to Colombat format from 1718. "The disputes have become extinct with the death of the parties.

The privilege of the Almanac is about to expire, Charles-Maurice d'Houry tries one last time to seize it, but a royal letter of 27 March 1744 confirmed definitively Andr&#233;-Fran&#231;ois Le Breton as sole heir.

==A family of hegemony 131 years==
===Directorate of Andr&#233; Le Breton===
In 1728, the widow of his grandson, Houry combines son Andr&#233;-Fran&#231;ois Le Breton, who was 18 years old and an orphan under the [[guardianship]] of Charles-Maurice d'Houry. Andrew Francis had inherited, according to the will of the estate of Lawrence Houry, half of the rights of the Almanac, and his widow, the rest.

Under his leadership, the Almanac takes a new breath and adds new sections, which sometimes does not go without punishment. For example, in 1768, he has trouble with [[Voltaire]], who sent him a letter incendiary:

:"I say as much to Le Breton, the Almanach royal printer: I'll pay him Almanac point that sold me this year. He had the rudeness to say that Mr. President... Mr. advisor..., remains in the cul-de-sac to Menard, in the cul-de-sac of the White Coats in the ass -de-sac de l'Orangerie. (...) How can you say that president remains in a serious ass?"

In 1773, Le Breton moved his print shop in a wing of the former Hotel d'Alegre, at 13 rue de Hautefeuille, he acquired William Louis Joly de Fleury and was previously occupied by Ambassador Portugal.

In the late eighteenth century, the weather is bad and bad [[wheat]] harvests. The price of this staple increases disproportionately. Recently, a rumor that the government would have the [[monopoly]] wheat, thus perpetuating the high cost of food. This rumor became official when accidentally in his edition of 1774, Le Breton added a "Treasurer's grain account of the King" in the person of Sr. Mirlavaud. The edition of the Almanac had yet been proofread and approved by the Chancellor, but was still sentenced to close his shop for 3 months and publish a revised edition, without the line in question,.

In 1777, Le Breton was again accused of inserting information deemed subversive. It has, according to its critics, cited "the Floral &amp; Pranks Verg&#232;s &amp; Vaucresson, among the prosecutors and attorneys general of the [[Parliament of Paris]], who had been involved in a reform of parliament made by Maupeou against the venality of Parliament desired by [[Louis XV]], but annulled by [[Louis XVI]]. In rebuke, Le Breton was sentenced to " carton "section on the Almanacs that had not yet been sold, and replacement cost, the Almanac issue of those who so request it."

He died on 4 October 1779 and his cousin, Charles-Laurent Houry, son of Charles-Maurice d'Houry, took over the business.

===French Revolution===
The privilege granted to the family of Houry for the Almanac has been threatened in 1789 when [[Camille Desmoulins]], in his speech at the Lantern to the Parisians, says it will cease in favor of Baldwin, another Parisian publisher. This threat has not been brought into effect since the Almanac remained in Houry. Looking at the publications of the time, we can nevertheless see that Baldwin got the impressions of the [[National Assembly (French Revolution)|National Assembly]] and other organs of state.

===Last generation Houry===
Following the death of Joan Nera, widow of Laurent-Charles Houry, the Almanac is echoed by Jean-Fran&#231;ois-No&#235;l Debure, husband of Anne-Charlotte d'Houry, their daughter. Debure is from a prominent and wealthy family of Parisian booksellers, especially combined with the Didot family. It is a printer since 1784 with the title's printer [[Duke of Orleans]].

Debure takes time printing of the family of Houry, but his other business is in trouble and he is forced to file for [[bankruptcy]]. To keep the property inherited from his family, Anne-Charlotte d'Houry hires a separation procedure. In November 1791, the bankruptcy is declared and it is opposed to the [[creditor]]s to preserve his legacy, and this opposition is futile and a ruling allows creditors to seize his furniture, but that does not appear to have been necessary because a subsequent decision allows him to recover property that creditors have not taken her husband.

Francois-Jean-Noel Debure dies 1802 in [[Loiret]]. However, it is focused died from 1795 through various sources. Maybe it was just left without leaving an address.

Stephen Lawrence Testu worked as a clerk in the family home Debure since 1788, and had gradually won the confidence of the household. Because of the absence of Mr. Debure, Anne-Charlotte is alone with his two son. Despite their age difference, he is 20 years younger than she, she married Testu July 1795. Testu few highlights its knowledge in the profession to convince him to transfer the management of printing. It accepts in 1797 and offered him priority over the rights of the Almanac in exchange for a perpetual [[Annuity (European financial arrangements)|annuity]] of 800 francs, then she completely abandons the Almanac. This influx of money seems to turn the head Testu who play games and learn to enjoy the easy life, neglects the direction of its establishment and constantly running out of money, he contracted many loans that gradually ruining his business. Relations were strained with his wife because he left the marital home in September 1801 and the only ties that bind the couple are now linked to multiple trials they s'intentent.

In 1810, Testu secretly sold the rights of the Almanac in which he partnered with Guyot. Anne-Charlotte d'Houry opposes this sale she saw as a usurpation, but loses the case in 1812. She gets in return a pension of 1 200 francs Testu does not pay. Indeed, a decree of 1820 declares the debtor more than 90 000 francs... In 1814, due to the large sums invested by Guyot in the case, an order confirming the owner of the Almanac, a copy of this order is also printed at the end of the following books. Testu still gets Guyot repayment of its debts and an annuity of 2,400 francs.

Guyot dismisses Testu business in 1820 and continues even to pay his annuity. The latter, again running out of money turned in 1823 against his wife, calls it reaches the marital home and she pays all household expenses, or alternatively, that she pays rent of 6000 francs. Judges d&#233;boutent Testu the marital home, since he himself had deserted 22 years earlier and has no housing to offer his wife even though she already lives in a very beautiful but still require his wife, yet very rich, to pay him a pension of 1,800 francs by invoking the solidarity between spouses.

The [[hegemony]] of the family of Houry on the Almanac established in 1683 has finally ceased in 1814 when, by order, the company is transferred to the association Guyot-Testu. Anne-Charlotte d'Houry died 22 July 1828 aged about 83 years.

===Judgement of publishing===
In 1867, edition of the Almanac is transferred to the widow Berger-Levrault, who had already published the Yearbook of the French empire diplomatic, and military Yearbook of the French empire, both published as the Almanac according to documents provided by the administration.

The edition of the Almanac stops definitively in 1919 after four years of interruption due to war, the latter number includes the years 1915 to 1919. Not found in the literature the reasons that prompted the shutdown of publication, but it can be assumed that the combination of very large volume of the book (more than 1650 pages in 1900) and the hard times that the [[Economics]] and French policy at that time was to make the management of such a volume of information extremely complex and unprofitable for the publisher. It is also possible that the new government formed after elections in 1919 no longer supported the development of the Almanac.

==Changing content==
===The topics in the Almanac===
The Almanac or Calendar, as he was called in its early editions, was just a simple calendar which were associated topics on astronomical events, the days of fairs, the newspaper of the Palace, the residence of messengers The departure of the mails, the price of currencies and the list of collectors' offices. After his presentation to the king in 1699, many items are constantly being added including the clergy, the royal family of France, then the families of other sovereign nations, officers, ambassadors, etc..

In 1705, Houry added to the list of knights of the Holy Spirit and peer and marshal of France. In 1707, it is the state of the clergy and, in 1712, the birth of kings, princes and princesses of Europe. After the death of Louis XIV, the Duke of Orleans, became the Regent, is added to the list of members of the royal family of the members of the [[House of Orleans]]. Later, he put more of his own, the full house of the Queen and princes.

It is not possible to describe all items contained in an almanac as there, so the contents of 1780 covers ten pages:

The Almanac also stands abreast of scientific advances. In the middle of the eighteenth century, improving the accuracy of clocks and many wealthy fans begin to observe and study the stars. It is indispensable to know precisely the difference between true solar time of [[sundial]]s, and mean solar time clocks, especially since the advent of clocks seconds. This is the equation of the pendulum, also called the equation of time, the table is added shortly before 1750.

With the [[French Revolution]], the Almanac exchange of title and its content is modified to match the new institutions.

The abolition of all distinctions requires overhaul the topics, timing of the vulgar era is replaced by the [[French Republican Calendar|Republican calendar]], the place reserved for kings and princes of Europe is replaced by a note on the friendly powers of France, the administrative organs of royal power are replaced by new ministries.

The content changes again with the reforms of the Consulate and the Empire, the Restoration, the [[Hundred Days]], the [[July Monarchy]], the [[French Second Republic|Second Republic]] followed the [[Second French Empire|Second Empire]] and the [[French Third Republic|Third Republic]] who sees the end of the edition of Almanac. In each case, the bindings are supplied with the times.

As the number of entries is increasing, the number of pages follows the same trend: they numbered one hundred in 1699, nearly five-hundred in 1760, and seven-hundred just before the French Revolution. The course of a thousand pages is taken in 1840 to over 1000-6-cents in 1900. On average, about thirty names are listed per page, the total number of people or places listed annually in the tens of thousands, but no table patronymic does quickly find a particular name.

All changes in the Almanac makes it a very useful book for historians that may follow, year after year, ministries and other administrative bodies, movements of people in these offices, and retail organization public services to a resident of Paris (such as places of mailboxes, timetables and fares for ticks and royal messengers ).

===Chronology of the 237 years of publishing Almanacs===
After the death of Laurent Houry, his descendants continued his work until 1814. The edition was continued until 1919. It would be tedious to describe in words the evolution of the Almanac of the 237 years that have elapsed since the first edition by Laurent d'Houry in 1683, hence the choice of this table layout.

Throughout its existence, the Almanac has crossed 11 schemes political editor changed the title 14 times and 9 times.

==Publication==
===Collecting information===
Since its inception in 1700, following a royal demand, the Almanac invented by Laurent d'Houry aims to be an official handbook.

Until the French Revolution, contributors are cordially invited to provide information to the bookseller, as pointed note of the printer in the first pages of the Almanac. In 1771, for example, we read in the Journal History of the Revolution that the Bar Association in the person of a certain Gerbier, asserted that "there would be no change in the order of the table, and that it would be printed in the Almanac as Royal was last year, leaving out only the dead."

With the French Revolution, the order was given to government to provide all information to the publisher. In 1802, Testu gets even exclusivity.

Later, the collection of information for the Almanac is even part of the operating budget of the ministries and can be seen in order of December 31, 1844 signed by [[Louis-Philippe I]] "on the organization of the Ministry Administration Navy "Article 6 of which list the items in the budget" the formation of the Royal Almanac.

====Typography====
The print quality improves significantly when Laurent d'Houry became printer. It multiplies the bands and tail-lights to decorate and titles for sections. The Almanac is still very poor prints because the image is not its goal. Only that the reader can find are patterns explaining the oppositions of the planets and eclipses are present every year, and the map of [[departments of France]] editions of 1791 and 1792.

Despite the short time to prepare the book, the printer treats the presentation and uses in the case of many variations in size and shape of characters for easy reading of long lists, special characters to emphasize certain lines, compositions tables or columns and clusters in braces.

That Le Breton, grand son of Laurent Houry, who brings more to the book. It increases greatly and restructures the Almanac, and also improves its presentation in order to preserve readability. Many notes are added to guide the reader and help in understanding the operation of certain administrative bodies.

The Almanacs modern nineteenth-century advantage of technological advances. Cartoon characters are modernized and the use of fonts to graphics customizable multiplies, sometimes to excess: we can count at least 7 in 11 fonts fonts differ on the cover page of the National Almanac 1850 printed by Guyot et Scribe!

Announcements, the ancestors of [[advertising]], are introduced by the publisher Berger-Levrault in the late nineteenth century.

====Well-to-shoot====
The deadline for submitting this information to the editor is set to "first ten days of October (or November). The last-minute changes are incorporated in an erratum end of the book. When they are too large, they may even delay the release. In late December, an event is sent to the administration for approving the content. This approval is required before the sale.

It leaves only two months to integrate the information of the year in the text of the previous edition and call all of the pages before submitting the book for the right to shoot. The editing step, at least for the test in 1706, has not been done with great care as can be seen by very many shells and mistakes which have crept into the [[table of contents]] presented in thumbnail to the right.

Once the administrative agreement obtained, it is inserted end of the book, the Almanac is stapled or bound and is then distributed to customers at the end of the year.

===Printing===
Early Almanacs were not printed by Laurent d'Houry. The Almanac of 1706 and is printed by Jacques Vincent, installed Huchette street, at the sign of the Angel. November 15, 1712, Laurent Houry became printer and immediately began printing his work. Then all the almanacs will be printed by their publishers.

====Draw====
There is no source that explains the draw of the Almanac. The only figures available are the annual rents generated by sales.

In 1782, Mercier said a pension of more than 40 000 francs. Diderot, at the same time, puts the figure at 30,000 pounds. For a price of sale of 5 to 6 pounds, the draw must necessarily be greater than about 7500 almanacs.

Around 1820, during the trials that have brought the widow and Debure Testu, income of the Almanac was estimated between 25 and 30 000 francs. In 1834, another almanac, the Almanac of France, said that its cost is 35 cents for a sale price of 50 cents. Booksellers then purchase the item at prices of 38 cents, to resell a suggested retail price of 50 cents. The publisher earns so 3 cents per pound sold, the bookseller earns 12 (minus shipping costs, dependent). If one considers - arbitrarily - a four Almanac is sold directly into the library Testu (priced at 10 francs 50) the remainder being passed through intermediaries, we can prorate that to generate an annual income of 30 000 francs Testu must sell approximately 25 000.

In the absence of more precise information, we can only estimate at about 15,000, the number of copies sold per year between the late eighteenth and early nineteenth century.

====Binding====
The almanac is sold either stapled or bound by the printer. The [[paperback]] version allows the purchaser to connect his book as he wishes, and so it is possible to find books with bindings very ornate, with lace, weapons of families, many colors brightened or gilding Biblio 24, etc..

The bound version provided by the printer is usually presented in a bound in calf or Morocco,{{clarify|date=August 2012}} full, and lilies in the boxes back. With the revolution, the lilies are replaced with Phrygian caps in roundels Library 25.

====Distribution====
The Almanac is normally available from the bookseller, but it can also be found in the province in other bookstores that serve as intermediaries, for example in 1816, at Pesche, bookseller at Le Mans Ref 17, or by correspondence through the [[Sorbonne]], as did Voltaire Ref 18.

===Readership===
The Almanac has a very great interest because of the number of subjects it addresses the organization of the French administration. In 1785, Mairobert wrote that "the Almanach Royal is in the hands of everyone and is among the Princes, the King's office, the foreign ministers would cater Ref 19. Louis-Sebastien Mercier in a pamphlet, the Tableau de Paris that stands in 1782 Ref 20 explains that "Those who are thrown into the paths of ambition, study Almanac Royal with serious attention," "more a beautiful royal consult the almanac to see if her lover is a lieutenant or sergeant,... ", that" everyone is buying the almanac to find out exactly where they stand. "And finally" even Fontenelle said, that it was the book which contained the greatest truths."

Adages use the almanac as a reference. According to Jean-Fran&#231;ois de La Harpe is "the only book to read to get rich is the Royal Almanac Ref 21, [[Jean-Joseph Regnault-Warin]] uses the phrase" having the memory of a Royal Almanac Ref 22 " or the Memoirs of the Academy of hawkers Ref 23 explains that "it is enough to read the Almanac for education."

In the eyes of [[justice]], the book can be used as a basis for comparison: during a police investigation in 1824 Ref 24, a defendant defends himself by explaining that the volume of documents he was accused of having carried "could be equal to that of a royal Almanac almanac or a related trade."

Whether to have a certain level of resources to purchase this item, the customer extended beyond the financial and political world.

==Competition==
The Royal Almanac is competing at its inception with the Almanac of the Court of Colombat who can not make it evolve since 1717. In fact, bibliographers consider that the Royal Almanac is one of the "oldest and most helpful Ref 25". If it essentially describes the royal court and the Parisian institutions, other major cities also have their almanacs, such as the city of Lyon equally voluminous Ref 26. The Almanac is however considered a reference book. In 1780, a notice of a bookseller named Desnos inserted at the end of the Gazette of 27 offers for courts Ref 8 pounds to "the statesman, letters, and generally all persons attached to the service of the King (... ) Almanach Royal, Calendar of the Court, said Colombat, Mignone Strennas-Note 22, Ref 28, the State Military Note 23, the four connected units, with shelves &amp; stylus to write, which makes the closure ": the Royal Almanac ranks first in the collection.

===The Court Calendar===
Since 1717, the Calendar of the Court can not change, its sections are limited to an ephemeris of the celestial motions (30 years) increased by astronomical tables with sky conditions, and timing of the court to the family and royal house, lists of boards, departments and secretaries of state finances, births and deaths of kings, queens, princes and princesses of Europe, the knights of various orders, the archbishops and bishops of the kingdom and Cardinals of the Sacred College.

It is primarily sought for its ephemeris of the celestial motions and astronomical tables of events

===The Almanac of Commerce===
The Almanac of Business, published by S&#233;bastien Bottin in the eighteenth century contains, besides the addresses of shops in Paris, many useful statistics financiers. It is supplementary to the Almanach royal, which concerns only the French administration.

===The State of France===
Some have criticized the Almanach Royal of being a [[plagiarism]] of the State of France, another administrative directory, the first publication seems to have been made in 1619 and is still published in the middle of the eighteenth century Ref 30. However, the edition of 1736 of the State of France said it was a "periodical whose audience has applied for renewal from time to time, and had been published until 1699, 1702, 1708, 1712, 1718 and 1727 Ref 31. The latest editions of 1727 and 1736 five volumes contain over 500 pages each. Offices are described down to the smallest detail Note 25, the state of France is a companion volume of the Almanach royal use by those who wish to deepen their knowledge on the functioning of the French administration.

==Examples of information contained in the Almanac==
Further details concerning the organization of the administration of the French state, and persons who occupied positions, many other topics are discussed in the Almanacs, for example in the eighteenth century:

===The cost of construction in Paris===
This section is only found in the Almanacs of the early eighteenth century, and stops just before 1726.

There are prices for masonry, carpentry and joinery, roofing, locksmithing, painting and glazing that are usually in Paris, for example:

:"Walls of circular pits, with layers of stone studded with low excess moilon quilted 18 inch thick, 22 pounds fquare fathom, and more in proportion to the depth of the wells, or other difficulties that may encounter."

With these data, the historian is able to quantify fully the construction of a building in Paris at that time.

===The official ceremonies===
The Almanac explains in great detail some official ceremonies:
* Opening Ceremony of the Annual Courthouse
:"The Entry of Parliament is the day after the S. Martin, 12 November, which day Presidents in red dresses holding their furs &amp; Mortar Note 26, &amp; Gowns Gentlemen Consultants red, after attending the solemn Mass that are usually said by a Bishop in the grand hall of the Palace, receive oaths of Lawyers &amp; Counsel. The first president made this day a speech to thank those who celebrated the Mass, which responds to him by another harangue Note 27."
* Procession of the University, whose description takes three pages of the Almanac Note 28
:"The Rector of the University at the end of its Rector, who regularly is only three months, indicating a general procession which assists the whole body. It is a ceremony that deserves to be seen. We will mark the place here What the doctors take the four faculties Note 29 that comprise the university, all the graduates of these faculties, with the Religious Orders Note 30. Procession from the Church of Religious Trinitarians, otherwise known as Maturin. (...) The procession is closed by the booksellers, papermakers, bookbinders, Parcherminiers, illuminators, writers swear by the University."

The detailed description of the ceremonies to stop mid-eighteenth century to make room for a still more comprehensive directory. A reference is then made towards the end of the book "guides for all kinds of ceremonies to be observed in the receipt of any office or employment whether in dress or in the Sword."

===Transportation===
Transportation of persons is ensured by the coaches, carriages, wagons and other carriages. Found in the Almanac schedules and rates of major roads.

In 1715, a passenger wishing to travel from Paris to [[Caen]] will go rue Saint-Denis on Monday at six o'clock in the morning. He has previously "sent his clothes the night before early." Fifteen years later, the starting time is advanced to 5 am in summer and in 1750, the departure is 5 hours throughout the year. In 1780, two flights are scheduled Tuesdays and Fridays at 23:30 and the journey takes two days. A van, slower, except Sunday at noon and made the trip in four and a half days in summer and five days in winter. In 1790, transportation is now provided by the General Department of stagecoaches and mail royal France. Three coaches liaise on Tuesday, Thursday and Sunday and the van on Sunday. The departure is now Notre-Dame-des-Victoires.

Rates are rarely reported but in 1725 and 1761 is 18 pounds per person tournaments. He is 21 pounds in 1770 to reach 42 pounds in 1790 (fortunately for the traveler, it is stated that the "sleeping bag weighing 10 pounds is" free").

===Company guards of the King Pumps===
In 1716, the king appoints Fran&#231;ois Perrier Dumouriez as Director General of public pump to remedy fire, without the audience is obliged to pay anything. In 1722, he founded the Compagnie des Gardes Pumps du Roy, under the direction of the same. This company later became the Brigade of firefighters in Paris Note 32.

The Almanac of 1719 lists these pumps and their wardens and deputy wardens. We then learned that a brigade is made up of four guards and four sub-custodians who are responsible for maintaining the material deposited in each district. What became three years later the Society of King's Guards Pumps were not at that time that 41 people, 17 pumps distributed in groups of 8 men and 4 or 3 pumps in the City Hall, the convent of the Grands Augustins The Carmelite convent in the Place Maubert, Convent of Mercy, and the Fathers of Little Place des Victoires, in addition to a pump at the Director General of the pumps, Rue Mazarine. Except Dumouriez guards pumps are not professional fire but shoemakers, carpenters, locksmiths, etc..

==Considerations bibliophiles==
===Availability===
Almanacs are found regularly in auctions and in the antiquarian booksellers. Given their importance documentary and the fact that there are beautiful copies, these books are particularly sought after by historians, writers, bibliophiles and enthusiasts.

Volumes in the first round of the seventeenth century often exceed several thousand euros Ref 32, the other is generally negotiated between a few tens and five hundred euros, sometimes more, depending on their rarity, condition and quality bookbinding Note 33. Just over half are however available for free download on Gallica or Google Books:

===Notes handwritten readers===
Almanacs contain some handwritten notes left by their readers. The value of the book can then be influenced upward or downward depending on the quality and content of these notes, and especially the person who wrote them - when you can identify it. They are usually found on page intentionally left blank for the ephemeris. Some of these notes can provide very interesting information, such as notes written on the page in August 1715 a copy of the BNF. It relates the circumstances of the death of Louis XIV, who was suffering from gangrene Note 34:

:"We thought the death dez Roy Lundy 25. He marched
:better a day or two quoyque hopeless. It
:died after having suffered much and with great
:Patience on Sunday September 1, r t is 8 pm Morning
:M r le Duc d'Orleans went to Parl t and was declared
:Regent on 2. September e"

==References==
{{Reflist}}

[[Category:Historiography of France]]
[[Category:18th-century books]]
[[Category:Directories]]</text>
      <sha1>kvw919kfshuntua849gdkt623w30s77</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Yellow pages</title>
    <ns>14</ns>
    <id>36911912</id>
    <revision>
      <id>660702741</id>
      <parentid>546401675</parentid>
      <timestamp>2015-05-04T03:47:46Z</timestamp>
      <contributor>
        <username>Jdaloner</username>
        <id>4460044</id>
      </contributor>
      <comment>Removed tag for "Wikipedia categories named after texts" category since that category is for *specific* texts, and this category is for a general type of text.. Removed "NOGALLERY" tag.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="120" xml:space="preserve">{{Commons category|Yellow pages}}
{{Cat main|Yellow pages}}

[[Category:Directories]]
[[Category:Advertising by medium]]</text>
      <sha1>lcw303d802aa27ieuqe2ct86jmwweic</sha1>
    </revision>
  </page>
  <page>
    <title>Novel &amp; Short Story Writer's Market</title>
    <ns>0</ns>
    <id>7520269</id>
    <revision>
      <id>742117597</id>
      <parentid>732502581</parentid>
      <timestamp>2016-10-01T20:30:35Z</timestamp>
      <contributor>
        <ip>2600:100F:B025:E0B9:6089:9B2D:F1CB:CB77</ip>
      </contributor>
      <comment>Outdated.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2475" xml:space="preserve">{{italic title}}
[[File:Mosko.jpg|thumb|200px|right|''Novel &amp; Short Story Writer's Market'']]

'''''Novel &amp; Short Story Writer's Market''''' (''NSSWM'') is an annual resource guide for fiction writers that compiles hundreds of listings for book publishers, magazines literary agents, writing contests, and conferences. ''NSSWM'' is published by [[Writer's Digest Books]] and usually hits bookstores around August of each year.

==The Market Listings==
For 26 years, ''NSSWM'' has listed hundreds of U.S. and international magazines and book publishers who are open to submissions from fiction writers. Listings provide current contact information, editorial needs, schedules, submission guidelines, and payment and contract terms. All listings are updated annually.

==The Articles==
In addition to the market listings, the book contains interviews with and essays by best-selling and award-winning writers, as well as editors and agents.

==Writer's Digest Books==
[[File:TheFaulknerPortable.jpg|350px|right|thumb|A copy of the 1939 edition of ''Writer's Market'' rests next to William Faulkner's [[Underwood Typewriter Company|Underwood]] Universal Portable typewriter in his office at his home, [[Rowan Oak]], which is now maintained by the [[University of Mississippi]] in [[Oxford, Mississippi|Oxford]] as a museum.]]''Novel &amp; Short Story Writer's Market'' is one of eight "[[Market (economics)|market]] books" published each year by [[Writer's Digest Books]] - the most famous of which is ''[[Writer's Market]]'', a book that lists thousands of magazine and book publishers listings for writers. Others include: ''Photographer's Market'', ''Children's Writer's and Illustrator's Market'', ''Guide to Literary Agents'', ''Artist and Graphic Designer's Market'', ''Poet's Market'' and ''Songwriter's Market''. Each book is designed to give creatives instructions on how to submit work for publication.

==See also==
* [[Publishing]] 
* ''[[Writer's Digest]]''
* ''[[Writer's Market]]''
* ''[[Writers' &amp; Artists' Yearbook]]''
* [[Literary agent#Querying|query]]
* [[royalties]]
* [[Authors Guild]]

==External links==
* [http://www.writersdigest.com/competitions Official site for the competitions of Writer's Digest Books]
* [http://www.writersdigest.com ''Writer's Digest'' magazine official site]
* [http://www.fwpublications.com F+W Publications - parent company of Writer's Digest Books]

{{DEFAULTSORT:Novel and Short Story Writer's Market}}
[[Category:Directories]]</text>
      <sha1>oih5zgjucsj5wt7jbl5qq94x787xgr7</sha1>
    </revision>
  </page>
  <page>
    <title>Association of Directory Publishers</title>
    <ns>0</ns>
    <id>39023454</id>
    <revision>
      <id>743554364</id>
      <parentid>741346072</parentid>
      <timestamp>2016-10-10T05:00:48Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* History */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5312" xml:space="preserve">The '''Association of Directory Publishers''' (ADP), is an international [[trade association]] founded in 1898 and is headquartered in [[Traverse City, Michigan]].&lt;ref name=associations2012&gt;{{cite book |title= Encyclopedia of Associations |issn=0071-0202 |volume= 1 |edition=51st |year= 2012 |page= 318 |via=[[Boston Public Library]] Reference &amp; Reader's Advisory Department }}&lt;!--|accessdate=April 8, 2013 --&gt;&lt;/ref&gt; 

==About==
ADP is the oldest international trade association serving the Yellow Pages industry.  The Association represents the various interests of its membership which includes publishers of print, online and mobile directories, Certified Marketing Representatives (CMRs), advertising agencies and suppliers to the Yellow Pages and local search industry.

ADP represents the $35 billion Yellow Pages industry known as the original "local search engine" that brings buyers to sellers at the exact moment they are ready to buy.

The Association helps its members expand their businesses by offering them services and tools targeted to assisting them in achieving their clients' advertising objectives. ADP offers a wide variety of research, marketing and sales materials created with information from leading organization that are developed specifically to help members increase their company's bottom line.

ADP is a unique Association because of the governance structure of one company, one vote.  Every publisher from the smallest to largest has an equal opportunity to determine the leadership and direction of the Association.  ADP represents member companies of all sizes and from numerous countries.

==History==
The group formed in 1898 as the '''Association of American Directory Publishers,''' headquartered in New York. It aimed "to improve the [[Reference work|directory]] business."&lt;ref&gt;{{citation |url=https://books.google.com/books?id=gt7UAAAAMAAJ |year=1908 |work=Boyd's Directory of Harrisburg |title=(Advertisement for the Association of American Directory Publishers)}}&lt;/ref&gt; It changed its name to the '''Association of North American Directory Publishers''' in 1919.&lt;ref&gt;{{citation |title=Printers' Ink |location=NY |date=September 11, 1919 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.worldcat.org/identities/lccn-no2011-132108 |title=Association of North American Directory Publishers |work=WorldCat |publisher=[[OCLC]] |accessdate=April 5, 2013 }}&lt;/ref&gt; It has held annual meetings starting in 1899 and has published the ''Directory Bulletin''.&lt;ref&gt;{{citation |title=Directory Bulletin |volume =1 |year=1901 |location=Milwaukee |publisher=Association of American Directory Publishers |url=https://books.google.com/books?id=2zTZAAAAMAAJ }}&lt;/ref&gt; Officers have included George W. Overton and [[Ralph Lane Polk]].&lt;ref name=members1921 /&gt; Among the members in the 1920s:&lt;ref name=members1921&gt;{{citation |chapter=Members of Association of North American Directory Publishers |year=1921 |url=https://books.google.com/books?id=qG4UAAAAYAAJ&amp;pg=PA480 |title=Manchester Directory |publisher=Sampson &amp; Murdock Co. }}&lt;/ref&gt;

{{Col-begin}}
{{Col-1-of-3}}
* Action Pages
* Atkinson Erie Directory Company
* Atlanta City Directory Company
* W.H. Boyd Company
* Burch Directory Company
* Caron Directory Company
* Chicago Directory Company
* J.W. Clement Company
* Cleveland Directory Company
* Connelly Directory Company
* Fitzgerald Directory Company
* Gate City Directory Company
* Hartford Printing Company
* Henderson Directories Ltd.
* Hill Directory Company
{{Col-2-of-3}}
* C.E. Howe Company
* Kimball Directory Company
* Leshnick Directory Company
* Los Angeles Directory Company
* John Lovell &amp; Son Ltd.
* McCoy Directory Company
* H.A. Manning Company
* Maritime Directory Company
* Henry M. Meek Publishing Company
* Might Directories Ltd.
* Minneapolis Directory Company
* Piedmont Directory Company
* [[R.L. Polk &amp; Company]]
{{Col-3-of-3}}
* Polk-Gould Directory Company
* Polk-Husted Directory Company
* Polk-McAvoy Directory Company
* Polk's Southern Directory Company
* Portland Directory Company
* Price &amp; Lee Company
* W.L. Richmond
* Roberts Bros Company
* Sampson &amp; Murdock Company
* Soards Directory Company 
* Utica Directory Publishing Company
* Williams Directory Company&lt;ref&gt;{{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |url = http://hdl.handle.net/2027/nyp.33433082423645 |title = The development and growth of city directories |publication-date = 1913 }}&lt;/ref&gt;
* John F. Worley Directory Company
* Wright Directory Company
{{Col-end}}

In 1992 the group renamed itself the "Association of Directory Publishers."&lt;ref name=associations2012 /&gt;

==References==
{{Reflist|30em}}

==Further reading==
* {{citation |title=Pacific Bell fends off feisty competitors seeking confidential Yellow Pages data |work=San Francisco Business Times |date=March 1, 1991 }}

==External links==
* [http://www.adp.org/ Official website]
* {{cite web |url=http://www.adp.org/committees |title=Our Industry: Timeline |publisher=Association of Directory Publishers }}

[[Category:Organizations established in 1898]]
[[Category:1898 establishments in the United States]]
[[Category:Professional associations based in the United States]]
[[Category:Publishing organizations]]
[[Category:Directories]]
[[Category:Yellow pages]]</text>
      <sha1>kg7zfb5vw7asx2xrih55e9qx74q5wby</sha1>
    </revision>
  </page>
  <page>
    <title>Women Environmental Artists Directory</title>
    <ns>0</ns>
    <id>41307188</id>
    <revision>
      <id>758603968</id>
      <parentid>740025150</parentid>
      <timestamp>2017-01-06T13:00:36Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[greywater]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5766" xml:space="preserve">The '''Women Environmental Artists Directory''' (WEAD) focuses on promoting environmental and [[Social justice]] art. &lt;ref&gt;{{cite web | url=http://weadartists.org/about-us | title=About Us | publisher=Women Environmental Artists Directory | accessdate=2013-08-12}}&lt;/ref&gt; WEAD was founded in 1996 by Jo Hanson, Susan Leibovitz Steinman and Estelle Akamine.&lt;ref&gt;{{cite web | url=http://greenmuseum.org/generic_content.php?ct_id=285 | title=JO HANSON: Pioneering Environmental Artist Dies in San Francisco | publisher=Green Museum | accessdate=2013-08-12 | last=Leibovitz Steinman | first=Susan}}&lt;/ref&gt; 

WEAD has been listed among the best projects relating to [[Environmental art]],&lt;ref&gt;{{cite web | url=http://www.andrew.cmu.edu/user/md2z/greenarts/artprojects.html | title=Green Arts Web: Artists &amp; Projects | publisher=Carnegie Mellon University | accessdate=2013-08-12}}&lt;/ref&gt; and has sponsored a number of exhibits about activist eco art.&lt;ref&gt;{{cite web|title=Earthly Concerns, Activist EcoArt curated by WEAD|url=http://www.usfca.edu/uploadedFiles/Destinations/Library/thacher/archive/Earthly%20Concerns.pdf|publisher=University of San Francisco|accessdate=2013-08-12}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=CONVERGENCE/DIVERGENCE SYMPOSIUM|url=http://www.losmedanos.edu/art/archive.aspx|publisher=Los Medanos College|accessdate=2013-08-12}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=WEAD East I Women and the Environment|url=http://www.kbcc.cuny.edu/artgallery/Pages/ewead.aspx|publisher=Kingsborough Community College|accessdate=2013-08-12}}&lt;/ref&gt; 

One of the co-founders, Ms. Steinman, is considered a leader in the eco art field and has participated in roundtables and artists in residences programs,&lt;ref&gt;{{cite web|title=Artist Talk with Susan Steinman|url=http://goddard.edu/news-events/events/artist-talk-susan-steinman|publisher=Goddard College|accessdate=2013-08-12}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=Eco Art Video Salon|url=http://www.berkeleyartcenter.org/programs_Q4-2010.html|publisher=Berkeley Arts Center|accessdate=2013-08-12}}&lt;/ref&gt; and is listed in the sculptor directory of the International Sculpture Center.&lt;ref&gt;{{cite web|title=Sculptor Susan Leibovitz Steinman|url=http://www.sculpture.org/portfolio/sculptorPage.php?sculptor_id=1000451|publisher=International Sculpture Center|accessdate=2013-08-12}}&lt;/ref&gt;  Another co-founder, Jo Hanson, was instrumental in founding an EPA Artist in Residence Program, which was aimed at educating the public about recycling. Another of the WEAD co-founders, Estelle Akamine, was also one of the artists in residence.&lt;ref&gt;{{cite web|title=Recology&#8217;s Artist in Residence|url=http://www.epa.gov/wastes/conserve/smm/web-academy/2011/feb11.htm|publisher=US Environmental Protection Agency|accessdate=2013-08-12}}&lt;/ref&gt; Ms. Akamine's work has also been featured at the Museum of Craft and Folk Art museum store&lt;ref&gt;{{cite web|title=Museum Store|url=http://www.mocfa.org/store/artists.htm|publisher=Museum of Craft and Folk Art|accessdate=2013-08-12}}&lt;/ref&gt; and has lectured at a textile lecture series.&lt;ref&gt;{{cite web|last=Valoma|first=Deborah|title=Textiles Lecture Series Archive|url=https://www.cca.edu/news/2012/08/27/textiles-lecture-archive|publisher=California College for the Arts|accessdate=2013-08-12}}&lt;/ref&gt; All three co-founders were featured in a discussion about women artists of the American West whose art was about current social concerns.&lt;ref&gt;{{cite web|last=Cohn|first=Terri|title=Nature, Culture and Public Space|url=http://www.cla.purdue.edu/WAAW/Cohn/index.html|publisher=Purdue University|accessdate=2013-08-12}}&lt;/ref&gt; 

The directory lists a wide variety of [[Woman artists]], such as [[Marina DeBris]], a [[trashion]] artist, [[Betty Beaumont]], often called a pioneer of environmental art, and Shai Zakai.

WEAD also published a magazine, which focuses on such topics as dirty water, and the legacy of atomic energy. A recent guest editor was Dr. Elizabeth Dougherty, founder of Wholly H2O, and speaker at events such as Pacific Gas and Electric Company conference on [[Water conservation]]&lt;ref&gt;{{cite web|title=2010 Water Conservation Showcase Speakers Save Water by Going Paperless!|url=http://www.pge.com/pec/water/presentations.shtml|publisher=Pacific Gas and Electric Company|accessdate=2013-08-12}}&lt;/ref&gt; and Toulumne County's conference on [[greywater]].&lt;ref&gt;{{cite web|title=Greywater in California:  Designing, Managing, Monitoring|url=http://portal.co.tuolumne.ca.us/psp/ps/TUP_HS_ENVIR_HEALTH/ENTP/c/TU_DEPT_MENU.TUOCM_HTML_COMP.GBL?action=U&amp;CONTENT_PNM=EMPLOYEE&amp;CATGID=2651|publisher=TUOLUMNE COUNTY ENVIRONMENTAL HEALTH|accessdate=2013-08-12}}&lt;/ref&gt; Linda Weintraub was a contributor to a recent issue of the WEAD magazine. Ms. Weintraub is the author of well known books on art and activism&lt;ref&gt;{{cite web|title=Drop Dead Gorgeous: Beauty and the Aesthetics of Activism|url=http://artsci.ucla.edu/?q=events/art-activism-linda-weintraub|publisher=UCLA Art Sci Center|accessdate=2013-08-12}}&lt;/ref&gt; such as "To Life!"&lt;ref&gt;{{cite web|title=To Life! Eco Art in Pursuit of a Sustainable Planet|url=http://www.ucpress.edu/book.php?isbn=9780520273627|publisher=University of California Press|accessdate=2013-08-12}}&lt;/ref&gt;  and is an eco art activist.&lt;ref&gt;{{cite web|last=Lambe|first=Claire|title=An Interview with Linda Weintraub&#8201;&#8211;&#8201;Curator of &#8220;Dear Mother Nature: Hudson Valley Artists 2012&#8221; at The Dorsky|url=http://www.rollmagazine.com/an-interview-with-linda-weintraub-%E2%80%93-curator-of-%E2%80%9Cdear-mother-nature-hudson-valley-artists-2012%E2%80%9D-at-the-dorsky/|publisher=Roll Magazine, Mark Gruber Gallery|accessdate=2013-08-12}}&lt;/ref&gt; 

==References==
{{reflist}}

[[Category:1996 introductions]]
[[Category:Directories]]
[[Category:Environmental art]]
[[Category:Women artists]]</text>
      <sha1>ho11bcza76t0r5851fkr8tz8oa2wv74</sha1>
    </revision>
  </page>
  <page>
    <title>Deutsches Geschlechterbuch</title>
    <ns>0</ns>
    <id>41858428</id>
    <revision>
      <id>683767666</id>
      <parentid>609164339</parentid>
      <timestamp>2015-10-02T10:53:03Z</timestamp>
      <contributor>
        <username>Vanasan</username>
        <id>17280035</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2037" xml:space="preserve">{{italic title}}
The '''''Deutsches Geschlechterbuch''''', until 1943 known as the '''''Genealogisches Handbuch b&#252;rgerlicher Familien''''', is a major German genealogical handbook of [[Bourgeoisie|bourgeois]] or [[patrician (post-Roman Europe)|patrician]] families. It is the bourgeois and patrician equivalent of the ''[[Genealogisches Handbuch des Adels]]'' and the former ''[[Almanach de Gotha]]''. It includes genealogies and coats of arms of the included families. The ''Genealogisches Handbuch b&#252;rgerlicher Familien'' was started in 1889 and prior to 1943, 119 volumes covering around 1,200 families were published under the original title. From 1956, the series were continued under the title ''Deutsches Geschlechterbuch''. In 2007, the 219th and latest volume was published. In total, around 4,000 families have been covered.

The ''Hamburgisches Geschlechterbuch'', comprising 17 volumes on the [[Hanseaten (class)|Hanseatic]] families of Hamburg, is an integral part of the work, and is regarded as the most comprehensive reference work of its kind on a single city.&lt;ref&gt;Hildegard von Marchthaler: ''Die Bedeutung des Hamburger Geschlechterbuchs f&#252;r Hamburgs Bev&#246;lkerungskunde und Geschichte'', in: ''Hamburgisches Geschlechterbuch'', Bd. 9, Limburg an der Lahn 1961, S. XXIII&lt;/ref&gt;

The publication has been highly influential and inspired several similar publications, such as ''[[Nederland's Patriciaat]]''. To some extent it corresponds to ''[[Burke's Landed Gentry]]'' in the United Kingdom, although it could also be said to be the equivalent of ''[[Burke's Peerage]]'' in its coverage of [[Hanseaten (class)|Hanseatic]] and patrician families who comprised the highest class in the former city-republics.

==References==
{{reflist}}

==Bibliography==
*Genealogisches Handbuch b&#252;rgerlicher Familien (1889&#8211;1943)
*Deutsches Geschlechterbuch (1956-)

[[Category:Biographical dictionaries]]
[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Publications established in 1889]]


{{bio-dict-stub}}</text>
      <sha1>2gfj2xmvvg96h4db12hi5go2aax3ffy</sha1>
    </revision>
  </page>
  <page>
    <title>Address book</title>
    <ns>0</ns>
    <id>442661</id>
    <revision>
      <id>757793505</id>
      <parentid>747704276</parentid>
      <timestamp>2017-01-01T20:00:37Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2016 July 2|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4629" xml:space="preserve">{{redirect|Address Book|the Apple Inc. software|Address Book (application)}}
{{unreferenced|date=February 2012}}
[[File:Address book 1.jpg|thumb|A blank page in a typical paper address book]]
[[File:Jack L. Warner's address book - National Museum of American History - DSC06088.JPG|thumb|[[Jack L. Warner]]'s address book on display at the [[National Museum of American History]]]]
An '''address book''' or a '''name and address book''' ('''NAB''') is a [[book]] or a [[database]] used for storing entries called '''contacts'''. Each contact entry usually consists of a few standard [[Field (computer science)|fields]] (for example: first name, last name, company name, [[address (geography)|address]], [[telephone]] number, [[e-mail]] address, [[fax]] number, [[mobile phone]] number). Most such systems store the details in alphabetical order of people's names, although in [[paper]]-based address books entries can easily end up out of order as the owner inserts details of more individuals or as people move. Many address books use small [[ring binder]]s that allow adding, removing and shuffling of pages to make room.

== Little black book ==
A related term that has entered the popular [[lexicon]] is '''little black book''' (or simply '''black book'''). Such books are used as [[courtship|dating]] guides, listing people who the owner has dated in the past or hopes to in the future, and details of their various relationships. More explicit variations are guides for [[sexual partner]]s. It is unclear how prevalent this is in practice or when it originated, though such books have been mentioned in many pieces of [[popular culture]]. For example, the 1953 film version of ''[[Kiss Me, Kate]]'' features a musical scene in which [[Howard Keel]]'s character laments the loss of the social life he enjoyed before marriage, naming numerous female romantic encounters while perusing a miniature black book. More recently, the mid-2000s [[Guinness Brewmasters]] advertising campaign features the "little black book" as an invention of one of the brewmasters.

== Software address book ==
[[File:X-office-address-book.svg|thumb|A digital address book icon]]
Address books can also appear as [[software]] designed for this purpose, such as the [[Address Book (application)|"Address Book"]] application included with [[Apple Inc.]]'s [[Mac OS X]]. Simple address books have been incorporated into [[e-mail]] software for many years, though more advanced versions have emerged in the 1990s and beyond; and also in [[mobile phone]]s.

A [[personal information manager]] (PIM) integrates an address book, [[calendar]], task list, and sometimes other features.

Entries can be imported and exported from the software in order to transfer them between programs or computers. The common file formats for these operations are:
* [[LDAP Data Interchange Format|LDIF]] (*.ldif, *.ldi)
* Tab delimited (*.tab, *.txt)
* [[Comma-separated values|Comma-separated]] (*.csv)
* [[vCard]] (*.vcf)

Individual entries are frequently transferred as [[vCard]]s (*.vcf), which are roughly comparable to physical [[business card]]s. And some software applications like [[Lotus Notes]] and Open Contacts can handle a vCard file containing multiple vCard records.

== Online address book ==
An online address book typically enables users to create their own web page (or profile page) which is then indexed by search engines like Google and Yahoo. This in turn enables users to be found by other people via a search of their name and then contacted via their web page containing their personal information. Ability to find people registered with online address books via search engine searches usually varies according to the commonness of the name and the amount of results for the name. Typically users of such systems can synchronize their contact details with other users that they know to ensure that their contact information is kept up to date.

== Network address book ==
Currently, most people have many different address books: their email accounts, their mobile phone, and the "friends lists" on their social networks. A network address book allows them to organize and manage all of their address books through a single interface and share their contacts across their different address books and social networks.

== See also ==
{{colbegin|3}}
* [[Calendaring software]]
* [[Contact list]]
* [[Mobile social address book]]
* [[Personal information manager]]
* [[Rolodex]]
* [[Suvorin directories]]
* [[Telephone directory]]
* [[Windows Address Book]]
{{colend}}

{{Authority control}}
[[Category:Office equipment]]
[[Category:Directories]]</text>
      <sha1>r56tjvaj2v05f0nomvkxevw50605pit</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directory services</title>
    <ns>14</ns>
    <id>30937577</id>
    <revision>
      <id>604573040</id>
      <parentid>552678615</parentid>
      <timestamp>2014-04-17T09:44:26Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="210" xml:space="preserve">{{cat main|Directory service}}

[[Category:Directories]]
[[Category:Computer access control protocols]]
[[Category:Access control software]]
[[Category:Network service]]
[[Category:Database management systems]]</text>
      <sha1>mesxuugdchzdz7ub39jw103mleqsvbd</sha1>
    </revision>
  </page>
  <page>
    <title>Directory of Open Access Journals</title>
    <ns>0</ns>
    <id>2241822</id>
    <revision>
      <id>761373219</id>
      <parentid>761372997</parentid>
      <timestamp>2017-01-22T16:56:48Z</timestamp>
      <contributor>
        <username>Marchitelli</username>
        <id>11251957</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5368" xml:space="preserve">{{Infobox website
|name            = Directory of Open Access Journals
|logo            = DOAJ logo.jpg
|logocaption     =
|screenshot      =
|collapsible     =
|collapsetext    =
|caption         =
|url             = {{URL|https://doaj.org/}}
|slogan          =
|commercial      =No
|type            =
|registration    =
|language        =English
|content license =
|owner           =
|author          =
|launch date     = &lt;!--{{Start date and age|YYYY|MM|DD|df=yes/no}}--&gt;
|alexa           = 58,591 (as of October 2015)&lt;ref&gt;{{cite web|title=Ranking for Doaj.org|url=http://www.alexa.com/siteinfo/doaj.org|accessdate=2015-10-20|work=[[Alexa.com]]}}&lt;/ref&gt;
|revenue         =
|current status  =Online
|footnotes       =
}}

The '''Directory of Open Access Journals''' ('''DOAJ''') is a [[website]] that lists [[open access journal]]s and is maintained by Infrastructure Services for Open Access (IS4OA).&lt;ref&gt;{{cite web |url=http://www.is4oa.org/ |title=Infrastructure Services for Open Access |publisher=Infrastructure Services for Open Access C.I.C. |accessdate=2013-03-05}}&lt;/ref&gt; The project defines open access journals as [[scientific journal|scientific]] and [[academic journal|scholarly journal]]s that meet high quality standards by exercising [[peer review]] or editorial quality control and "use a funding model that does not charge readers or their institutions for access."&lt;ref name=aboutdoaj/&gt; The [[Budapest Open Access Initiative]]'s definition of [[Open access (publishing)|open access]] is used to define required rights given to users, for the journal to be included in the DOAJ, as the rights to "read, download, copy, distribute, print, search, or link to the full texts of these articles".&lt;ref name=aboutdoaj/&gt;&lt;ref&gt;The BOAI definition is at "[http://www.earlham.edu/~peters/fos/boaifaq.htm#openaccess Budapest Open Access Initiative: Frequently Asked Questions]".&lt;/ref&gt; The aim of DOAJ is to "increase the visibility and ease of use of open access scientific and scholarly journals thereby promoting their increased usage and impact."&lt;ref name=aboutdoaj&gt;{{cite web |url= http://doaj.org/about |title=About |work=Directory of Open Access Journals |accessdate=2015-04-14}}&lt;/ref&gt;

As of March 2015, the database contained records for 10,000 journals.&lt;ref&gt;{{cite web |url= http://sparc.arl.org/blog/doaj-introduces-new-standards |title=Directory of Open Access Journals introduces new standards to help community address quality concerns |first=Caralee |last=Adams |date=5 March 2015 |publisher=SPARC |accessdate=2015-04-14}}&lt;/ref&gt; An average of four journals were being added each day in 2012.&lt;ref&gt;{{cite web|title=DOAJ Statistics |url= http://www.doaj.org/doaj?func=newTitles&amp;uiLanguage=en&amp;fromDate=1970-01-01+00:00:00&amp;orderedBy=J.first_added |archiveurl= https://web.archive.org/web/20120404125652/http://www.doaj.org/doaj?func=newTitles&amp;uiLanguage=en&amp;fromDate=1970-01-01+00:00:00&amp;orderedBy=J.first_added |archivedate= 2012-04-04 |accessdate=2013-01-06 |work=Directory of Open Access Journals}}&lt;/ref&gt;

In May 2016, DOAJ announced that they had removed approximately 3,300 journals from their database to provide better reliability on the content listed on it.&lt;ref&gt;{{cite journal |last=Marchitelli|first= Andrea|last2=Galimberti |first2=Paola |last3=Bollini |first3=Andrea|last4=Mitchell |first4=Dominic|date= January 2017 |title=
Helping journals to improve their publishing standards: a data analysis of DOAJ new criteria effects |url=http://leo.cineca.it/index.php/jlis/article/view/12052|journal= JLIS.it|volume=8 |issue=1 |pages= 39-49|doi=10.4403/jlis.it-12052|access-date=2017-01-22 }}&lt;/ref&gt;

The journals that were removed can reapply as part of an ongoing procedure. &lt;ref&gt;{{Cite web|url=https://doajournals.wordpress.com/2016/05/09/doaj-to-remove-approximately-3300-journals/|title=DOAJ to remove approximately 3300 journals|last=DOAJ|date=2016-05-09|website=News Service|access-date=2016-09-24}}&lt;/ref&gt; As of September 2016, the database now contains 9,216 journals. &lt;ref&gt;{{Cite web|url=https://doaj.org/|title=Directory of Open Access Journals|last=DOAJ|access-date=2016-09-24}}&lt;/ref&gt;

==History==
The [[Open Society Institute]] funded various open access related projects after the Budapest Open Access Initiative; the Directory was one of those projects.&lt;ref&gt;{{cite book|last=Crawford|first=Walt|title=Open access : what you need to know now|publisher=American Library Association|location=Chicago|isbn=9780838911068|page=13}}&lt;/ref&gt; The idea for the DOAJ came out of discussions at the first Nordic Conference on
Scholarly Communication in 2002, [[Lund University]] became the organization to set up and maintain the DOAJ.&lt;ref&gt;{{Cite journal | last1 = Hedlund | first1 = T. | last2 = Rabow | first2 = I. | doi = 10.1087/2009303 | title = Scholarly publishing and open access in the Nordic countries | journal = Learned Publishing | volume = 22 | issue = 3 | pages = 177-186| year = 2009 | pmid =  | pmc = }}&lt;/ref&gt; It continued to do so  until January 2013, when Infrastructure Services for Open Access (IS4OA) took over.

== See also ==
* [[List of open-access journals]]
*[[Open Access Scholarly Publishers Association]]

== References ==
&lt;references/&gt;

== External links ==
* {{official website|https://doaj.org/}}
{{Open access navbox}}

[[Category:Open access (publishing)]]
[[Category:Open access journals| ]]
[[Category:Directories]]</text>
      <sha1>g8x51pc6ahboxcfctwsf3ivam12cn8g</sha1>
    </revision>
  </page>
  <page>
    <title>Lighthouse Directory</title>
    <ns>0</ns>
    <id>47962147</id>
    <redirect title="Lists of lighthouses and lightvessels" />
    <revision>
      <id>741664733</id>
      <parentid>741402946</parentid>
      <timestamp>2016-09-28T23:05:30Z</timestamp>
      <contributor>
        <username>De728631</username>
        <id>4919722</id>
      </contributor>
      <comment>this is where we have an article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="93" xml:space="preserve">#REDIRECT [[Lists of lighthouses and lightvessels]]
{{R from merge}}
[[Category:Directories]]</text>
      <sha1>mwnnukqe44lzsfwq1ih7yjm3sirpk42</sha1>
    </revision>
  </page>
  <page>
    <title>Web search query</title>
    <ns>0</ns>
    <id>11525372</id>
    <revision>
      <id>724066361</id>
      <parentid>720529338</parentid>
      <timestamp>2016-06-06T23:00:26Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10948" xml:space="preserve">A '''web search query''' is a query that a user enters into a [[web search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].

== Types ==
There are three broad categories that cover most web search queries: informational, navigational, and transactional.&lt;ref&gt;Broder, A. (2002). A taxonomy of Web search. SIGIR Forum, 36(2), 3&#8211;10.&lt;/ref&gt; These are also called "do, know, go."&lt;ref&gt;{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}&lt;/ref&gt; Although this model of searching was not theoretically derived, the classification has been  empirically validated with actual search engine queries.&lt;ref&gt;Jansen, B. J., Booth, D., and Spink, A. (2008) [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_user_intent.pdf Determining the informational, navigational, and transactional intent of Web queries], Information Processing &amp; Management. 44(3), 1251-1266.&lt;/ref&gt;

* '''Informational queries''' &#8211; Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.
* '''Navigational queries''' &#8211; Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').
* '''Transactional queries''' &#8211; Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.

Search engines often support a fourth type of query that is used far less frequently:

* '''Connectivity queries''' &#8211; Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).&lt;ref&gt;{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}&lt;/ref&gt;

== Characteristics ==

Most commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.&lt;ref&gt;Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]&lt;/ref&gt; Nevertheless, research studies appeared in 1998.&lt;ref&gt;Jansen, B. J., Spink, A., Bateman, J., and Saracevic, T. 1998. [https://faculty.ist.psu.edu/jjansen/academic/jansen_sigir_forum.pdf Real life information retrieval: A study of user queries on the web]. SIGIR Forum, 32(1), 5 -17.&lt;/ref&gt;&lt;ref&gt;Silverstein, C., Henzinger, M., Marais, H., &amp; Moricz, M. (1999). Analysis of a very large Web search engine query log. SIGIR Forum,
33(1), 6&#8211;12.&lt;/ref&gt; Later, a study in 2001&lt;ref&gt;{{cite journal|author1=Amanda Spink |author2=Dietmar Wolfram |author3=Major B. J. Jansen |author4=Tefko Saracevic | year = 2001 | title = [https://faculty.ist.psu.edu/jjansen/academic/jansen_public_queries.pdf Searching the web: The public and their queries] | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226&#8211;234 | doi = 10.1002/1097-4571(2000)9999:9999&lt;::AID-ASI1591&gt;3.3.CO;2-I }}&lt;/ref&gt; analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:

* The average length of a search query was 2.4 terms. 
* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. 
* Close to half of the users examined only the first one or two pages of results (10 results per page).
* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).
* The top four most frequently used terms were, '' (empty search), and, of, ''and'' sex.

A study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).&lt;ref&gt;{{cite conference |author1=Mark Sanderson  |author2=Janet Kohler  |lastauthoramp=yes | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}&lt;/ref&gt; Studies also show that, in addition to short queries (i.e., queries with few terms), there are also predictable patterns to how users change their queries.&lt;ref&gt;Jansen, B. J., Booth, D. L., &amp; Spink, A. (2009). [[Patterns of query modification during Web searchinhttps://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_patterns_query_reformulation.pdf|Patterns of query modification during Web searchin]]g. Journal of the American Society for Information Science and Technology. 60(3), 557-570. 60(7), 1358-1371.&lt;/ref&gt;

A 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.&lt;ref&gt;{{cite conference |author1=Jaime Teevan |author2=Eytan Adar |author3=Rosie Jones |author4=Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703&#8211;704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}&lt;/ref&gt; This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries &lt;ref&gt;http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx&lt;/ref&gt;

In addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. &gt; 100 million queries) are used most often, while the remaining terms are used less often individually.&lt;ref name="baezayates1"&gt;{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7&#8211;22 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}&lt;/ref&gt; This example of the [[Pareto principle]] (or ''80&#8211;20 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching. In addition, studies have been conducted on discovering linguistically-oriented attributes that can recognize if a web query is navigational, informational or transactional.&lt;ref&gt;{{cite journal | author = Alejandro Figueroa | year = 2015 | title = Exploring effective features for recognizing the user intent behind web queries | booktitle = Computers in Industry | pages = 162&#8211;169 | volume = 68 | publisher = Elsevier | url = https://www.researchgate.net/publication/271911317_Exploring_effective_features_for_recognizing_the_user_intent_behind_web_queries}}&lt;/ref&gt;

But in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.&lt;ref&gt;{{cite journal |author1=Mona Taghavi |author2=Ahmed Patel |author3=Nikita Schmidt |author4=Christopher Wills |author5=Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards &amp; Interfaces | pages = 162&#8211;170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}&lt;/ref&gt; Google has implemented the [[Google Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (i.e. "where is the nearest coffee shop?").&lt;ref&gt;{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google "Hummingbird" Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}&lt;/ref&gt; 
For longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.&lt;ref&gt;{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}&lt;/ref&gt; For multi-sentence queries where keywords statistics and [[Tf&#8211;idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.&lt;ref&gt;{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets
for Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037
}}&lt;/ref&gt;

== Structured queries ==
With search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as &lt;code&gt;vehicles OR cars OR automobiles&lt;/code&gt;. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as &lt;code&gt;(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)&lt;/code&gt; is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.&lt;ref&gt;{{Cite web
|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf
|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness
|author1=Vojkan Mihajlovi&#263; |author2=Djoerd Hiemstra |author3=Henk Ernst Blok |author4=Peter M.G. Apers |postscript=&lt;!--None--&gt;}}&lt;/ref&gt;

== See also ==
* [[Information retrieval]]
* [[Web search engine]]
* [[Web query classification]]
* [[Taxonomy for search engines]]

== References ==
{{reflist|2}}

{{Internet search}}

[[Category:Internet search]]</text>
      <sha1>ar5jnedoolddh2swp3jprfwspmyfonr</sha1>
    </revision>
  </page>
  <page>
    <title>Mystery Seeker</title>
    <ns>0</ns>
    <id>25176438</id>
    <revision>
      <id>743413388</id>
      <parentid>689145227</parentid>
      <timestamp>2016-10-09T15:03:03Z</timestamp>
      <contributor>
        <username>Prell</username>
        <id>50792</id>
      </contributor>
      <comment>Site doesn't return a usable response.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4081" xml:space="preserve">{{Infobox website
| name               = Mystery Seeker
| logo               = MYSTERYSEEKER logo.jpeg
| logo_alt           = The mysteryseeker.com logo
| logocaption       = The logo found at [http://www.mysteryseeker.com mysteryseeker.com], decorated with fog and a moon in the background.
| screenshot         = mysteryseeker.com screenshot.jpeg
| collapsible        = y
| screenshot_alt     = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com]
| caption            = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com].
| url                =  {{URL|www.mysteryseeker.com}}
| slogan             = &#8220;What will you search for?&#8221;
| commercial         = &lt;!-- "Yes", "No" or leave blank --&gt;
| type               = Search engine
| registration       = None
| language           = [[English language|English]]
| content_license    = &amp;copy; Mystery Seeker, 2009
| owner              = Mystery Seeker
| launch_date        = {{start date and age|2009|10|02|df=yes}}
| alexa              = {{decrease}} [http://www.alexa.com/siteinfo/mysteryseeker.com 3110015] (Global, November 2015)
| current_status     = Online but defunct 
| footnotes          = 
}}
'''Mystery Seeker''' is a website based on the [[Google]] search engine.&lt;ref name="Chivers 2009" /&gt; Until November 30, 2009, the website was known as '''Mystery Google''', but on December 1, 2009, the name changed to '''Mystery Seeker'''. It has been featured in a number of technology blogs.&lt;ref&gt;{{cite news| url=http://www.huffingtonpost.com/2009/10/12/mystery-google-surprise-y_n_318089.html | work=Huffington Post | first=Bianca | last=Bosker | title=Mystery Google: Surprise Yourself With Someone Else's Search Results | date=October 12, 2009}}&lt;/ref&gt;&lt;ref&gt;[http://mashable.com/2009/10/12/mystery-google/ Mystery Google: The &#8220;I&#8217;m Feeling Lucky&#8221; Button Re-imagined]&lt;/ref&gt;&lt;ref&gt;[http://www.geekologie.com/2009/11/i_wasnt_looking_for_that_myste.php I Wasn't Looking For That: Mystery Google Gives You Previous Person's Search Query | Geekologie]&lt;/ref&gt; Upon a search query, Mystery Seeker returns the results from the previous search, so &#8220;you get what the person before you searched for.&#8221;&lt;ref name="Chivers 2009"&gt;{{Cite news|url=http://www.telegraph.co.uk/technology/google/6316140/Mystery-google-returns-other-peoples-search-results.html|title=Mystery Google returns other people's search results|accessdate=2009-11-23|publisher=The Telegraph|date=13 Oct 2009|author=Tom Chivers|location=London}}&lt;/ref&gt;

There is a trend among the people on Mystery Seeker to add so-called "missions", where the next user is asked to do something. For example, "Your mission is to copy and paste this until you see it again. Then and only then will you be a true ninja".&lt;ref&gt;[http://www.softsailor.com/news/12457-how-to-receivegive-google-mystery-missions-and-why-they-are-fun.html Tech Source]&lt;/ref&gt; Other examples of possible missions include telling someone you love them, sending someone a get well card, mailing a banana to someone, etc. There are also references to [[MyLifeIsAverage|MLIA]]. Due to the high number of posted missions involving phone numbers, Mystery Seeker received enough complaints to remove phone numbers from the site. However, the developers are testing Mystery Missions Beta in order to allow the continuance of missions.

A number of phrases yield intentional responses ([[easter egg (media)|easter egg]]s).

In November 2009 Mystery Seeker had 440,000 unique visitors,&lt;ref&gt;[http://siteanalytics.compete.com/mysterygoogle.com/ mysterygoogle.com UVs for November 2012 | Compete]&lt;/ref&gt; making it one of the most highly trafficked social entertainment sites online.

Google has not commented on any possible connection to the site.&lt;ref name="Chivers 2009" /&gt; The [[domain name]] ''www.mysterygoogle.com'' is registered to a private registrant {{As of|2009|10|2|lc=on}}.&lt;ref&gt;http://whois.domaintools.com/mysterygoogle.com&lt;/ref&gt;

== References ==
{{Reflist|30em}}

== External links ==
* [http://www.mysteryseeker.com/ Mystery Seeker site]

[[Category:Websites]]
[[Category:Internet search]]</text>
      <sha1>os6rq49fxawe28x2dld864au0ij5t1k</sha1>
    </revision>
  </page>
  <page>
    <title>Google (verb)</title>
    <ns>0</ns>
    <id>375665</id>
    <revision>
      <id>759870388</id>
      <parentid>759688628</parentid>
      <timestamp>2017-01-13T16:30:56Z</timestamp>
      <contributor>
        <username>AnotherOnymous</username>
        <id>8638379</id>
      </contributor>
      <comment>/* Causes */ Copyedit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9857" xml:space="preserve">{{cleanup|reason= Substantive entry outdated, summary does not refer to content in body of entry, body reads like dictionary entry for 'ungoogleable')|date=October 2016}}

{{about|the verb|the use of the verb in cricket|Googly|other uses|Google (disambiguation)}}
{{redirect|Googled|the book of the same name|Googled: The End of the World as We Know It}}
{{wiktionary|google}}
As a result of the increasing popularity and dominance of the [[Google Search|Google search engine]],&lt;ref&gt;{{cite web |last=Burns |first=Enid |date=June 19, 2007 |url=http://searchenginewatch.com/showPage.html?page=3626208 |title=Top 10 Search Providers, April 2007 |publisher=SearchEngineWatch.com |accessdate=2007-08-11 }}&lt;/ref&gt; usage of the [[transitive verb]]&lt;ref&gt;{{cite web|url=http://www.merriam-webster.com/dictionary/google |title=Google - Definition and More from the Free Merriam-Webster Dictionary |publisher=Merriam-webster.com |date= |accessdate=2011-09-19}}&lt;/ref&gt; '''to google''' (also spelled '''Google''') grew ubiquitously. The [[neologism]] commonly refers to searching for information on the [[World Wide Web]], regardless of which [[search engine]] is used.&lt;ref&gt;{{cite web|url=http://www.thelinguafile.com/2013/02/how-google-became-verb.html |title=How Google Became a Verb |publisher=The Lingua File - The Language Blog |date= |accessdate=2013-11-22}}&lt;/ref&gt; The [[American Dialect Society]] chose it as the "most useful word of 2002."&lt;ref&gt;{{cite web |date=January 13, 2003 |url=http://www.americandialect.org/index.php/amerdial/2002_words_of_the_y/ |title=2002 Words of the Year |publisher=American Dialect Society |accessdate=2007-08-11 }}&lt;/ref&gt; It was added to the ''[[Oxford English Dictionary]]'' on June 15, 2006,&lt;ref&gt;Bylund, Anders. "[http://www.fool.com/investing/dividends-income/2006/07/05/to-google-or-not-to-google.aspx To Google or Not to Google]." ''[[The Motley Fool]].'' July 5, 2006. Retrieved on March 28, 2007.&lt;/ref&gt; and to the eleventh edition of the ''[[Merriam-Webster|Merriam-Webster Collegiate Dictionary]]'' in July 2006.&lt;ref&gt;Harris, Scott D. "[http://www.mercurynews.com/mld/mercurynews/business/14985574.htm Dictionary adds verb: to google]." ''[[San Jose Mercury News]].'' July 7, 2006. Retrieved on July 7, 2006.&lt;/ref&gt;

==Etymology==
The first recorded usage of ''google'' used as a [[participle]], thus supposing an [[intransitive verb]], was on July 8, 1998, by [[Google]] co-founder [[Larry Page]] himself, who wrote on a mailing list: "Have fun and keep googling!"&lt;ref&gt;{{cite web |last=Page |first=Larry |authorlink=Larry Page |date=July 8, 1998 |url=http://www.egroups.com/group/google-friends/3.html |title=Google Search Engine: New Features |publisher=Google Friends Mailing List |accessdate=2007-08-06 |archiveurl=https://web.archive.org/web/19991009052012/http://www.egroups.com/group/google-friends/3.html |archivedate=1999-10-09 }}&lt;/ref&gt; Its earliest known use (as a transitive verb) on American television was in the "[[Help (Buffy episode)|Help]]" episode of ''[[Buffy the Vampire Slayer (TV series)|Buffy the Vampire Slayer]]'' (October 15, 2002), when [[Willow Rosenberg|Willow]] asked [[Buffy Summers|Buffy]], "Have you googled her yet?"&lt;ref&gt;{{Cite book |title=Digital Wars: Apple, Google, Microsoft and the Battle for the Internet |last=Arthur |first=Charles |year=2012 |publisher=Kogan Page Publishers |location= |isbn= |page=48 |url=https://books.google.com/books?id=IXiYi-dQenEC&amp;pg=PA48#v=onepage&amp;q&amp;f=false |accessdate=January 2, 2013 }}&lt;/ref&gt;
&lt;!-- Fearing the [[generic trademark|genericizing]] and potential loss of its [[trademark]], Google has discouraged use of the word as a verb, particularly when used as a synonym for general web searching. --&gt; 
On February 23, 2003,&lt;ref&gt;{{cite web |last=McFedries |first=Paul |date=February 23, 2003 |url=http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0302D&amp;L=ads-l&amp;P=R2450 |title=Google trademark concerns |publisher=American Dialect Society Mailing List |accessdate=2007-08-11 }}&lt;/ref&gt; the company sent a [[cease and desist]] letter to [[Paul McFedries]], creator of [[Word Spy]], a website that tracks [[neologism]]s.&lt;ref&gt;Duffy, Jonathan. "[http://news.bbc.co.uk/2/hi/uk_news/3006486.stm Google calls in the 'language police']." ''[[BBC News]].'' June 20, 2003. Retrieved on July 7, 2006.&lt;/ref&gt; In an article in the ''[[Washington Post]]'', Frank Ahrens discussed the letter he received from a Google lawyer that demonstrated "appropriate" and "inappropriate" ways to use the verb "google".&lt;ref&gt;{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2006/08/04/AR2006080401536.html|title=So Google Is No Brand X, but What Is 'Genericide'?|author=Frank Ahrens|date=2006-08-05|accessdate=2006-08-05|publisher=Washington Post}}&lt;/ref&gt; It was reported that, in response to this concern, [[lexicographer]]s for the ''Merriam-Webster Collegiate Dictionary'' lowercased the actual entry for the word, ''google'', while maintaining the capitalization of the search engine in their definition, "to use the [[Google search|Google search engine]] to seek online information" (a concern which did not deter the Oxford editors from preserving the history of both "cases").&lt;ref&gt;Noon, Chris. "[http://www.forbes.com/2006/07/06/page-brin-google-cx_cn_0706autofacescan01.html Brin, Page See 'Google' Take Its Place In Dictionary]." ''[[Forbes]].'' July 6, 2006. Retrieved on July 7, 2006.&lt;/ref&gt; On October 25, 2006, Google sent a request to the public requesting that "You should please only use 'Google' when you&#8217;re actually referring to Google Inc. and our services."&lt;ref&gt;{{cite web |last=Krantz |first=Michael |date=October 25, 2006 |url=http://googleblog.blogspot.com/2006/10/do-you-google.html |title=Do you "Google?" |publisher=The Official Google Blog |accessdate=2007-08-11 }}&lt;/ref&gt;

==Ungoogleable==
{{main|Censorship by Google|Deep Web (search indexing)}}
{{wiktionary|unGoogleable}}
Ungoogleable, (or unGoogleable) is a term for something that cannot be "googled" &#8211; i.e. it is a term for something that cannot be found easily using the [[Google Search]] [[web search engine]]. It is increasingly used to mean something that cannot be found using any web search engine.&lt;ref&gt;{{cite news| url=http://www.bbc.co.uk/news/magazine-21956743 | title=Who, What, Why: What is 'ungoogleable'? | publisher=[[BBC]] |work=[[BBC News Magazine]] | date=27 March 2013 | accessdate=5 April 2013 }}&lt;/ref&gt;

In 2013 the [[Swedish Language Council]] attempted to include the [[Swedish language|Swedish]] version of the word ("''[[:sv:Ogooglebar|ogooglebar]]''") in its list of new words, but Google objected to the definition not being specifically related to Google, and the Council was forced to briefly remove it to avoid a legal confrontation with Google.&lt;ref&gt;{{cite news| url=http://www.bbc.co.uk/news/world-europe-21944834 | title=Google gets ungoogleable off Sweden's new word list | first=Sean | last=Fanning | publisher=[[BBC]] | work=[[BBC News]] | date=26 March 2013 | accessdate=5 April 2013 }}&lt;/ref&gt;&lt;ref&gt;{{cite news| url=http://www.independent.co.uk/news/world/europe/ungoogleable-removed-from-list-of-swedish-words-after-row-over-definition-with-google-8550096.html | title='Ungoogleable' removed from list of Swedish words after row over definition with Google: California based search engine giant asked Swedish to amend definition | first=Rob | last=Williams | newspaper=[[The Independent]] | date=26 March 2013 | accessdate=5 April 2013 }}&lt;/ref&gt;

===Causes===
Google Search generally ignores punctuation and [[letter case]] even when using the "quotation" operator to denote exact searches.&lt;ref&gt;[https://support.google.com/websearch/answer/2466433?hl=en Search operators - Search Help]&lt;/ref&gt; Thus, Google may not be able to differentiate terms for which punctuation impacts meaning{{--}}for example, "man eating chicken" and "man-eating chicken" (the former meaning a human who is consuming chicken meat and the latter a chicken that eats humans). Because Google treats upper and lower case letters as one and the same, it also is unable to differentiate between the pronoun ''[[he]]'' and the surname ''[[He (surname)|He]]'', which, when combined with its disregard for punctuation, could bury results for an obscure person named &lt;code&gt;"Thomas He"&lt;/code&gt; among results such as:
:&lt;q&gt;... Assisted by '''Thomas, he''' was able to provide incontrovertible proof of this theory, and in so doing, he gained wide recognition in the medical ...&lt;/q&gt;&lt;ref&gt;[https://www.google.com/webhp?hl=en&amp;sa=X#hl=en&amp;q=%22Thomas+He%22 "Thomas He" - Google Search]&lt;/ref&gt;

The above also exemplifies how Google's [[PageRank]] algorithm, which sorts results by "importance", could also cause something to become ungoogleable: results for those with the 17th most common Chinese surname&lt;ref&gt;{{cite web|url=http://cdn.theatlantic.com/newsroom/img/posts/2013/10/chinassurnames/0886ab335.jpg |title=China's Surnames |publisher=Cdn.theatlantic.com |accessdate=2016-07-12}}&lt;/ref&gt; are difficult to separate from results containing the 16th [[most common words in English|most common word in English]]. In other words, a specific subject may be ungoogleable because its results are a [[wikt:needle in a haystack|needle in a haystack]] of results for a more "important" term.

==See also==
{{Portal|Internet}}
* [[grep#Usage as a verb|grep]]&lt;!--lowercase--&gt;
* [[Swedish_Language_Council#Controversy|Ogooglebar, Swedish for Ungoogleable]]
* [[Photo manipulation#Photoshopping|Photoshop (verb)]], a similar neologism referring to digital photo editing

==References==
{{reflist|30em}}

{{Google Inc.}}

{{DEFAULTSORT:Google (Verb)}}
[[Category:Google]]
[[Category:Verbs]]
[[Category:Internet terminology]]
[[Category:Internet search]]
[[Category:Words coined in the 1990s]]
[[Category:Computer-related introductions in 1998]]

[[ja:Google#&#27966;&#29983;&#35486;]]
[[ru:Google (&#1082;&#1086;&#1084;&#1087;&#1072;&#1085;&#1080;&#1103;)#to google]]</text>
      <sha1>5r8iaa12icr25yaq7bpur64wneeyuuv</sha1>
    </revision>
  </page>
  <page>
    <title>Figaro Systems</title>
    <ns>0</ns>
    <id>17910258</id>
    <revision>
      <id>757564542</id>
      <parentid>669709633</parentid>
      <timestamp>2016-12-31T11:24:26Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10891" xml:space="preserve">{{Infobox company
|name = Figaro Systems, Inc.
|logo = [[Image:Figaro-logo.png|Figaro logo]]
|type = [[Privately held company|Private]]
|foundation = 1993
|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]
|location_country =[[United States]]
|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]
|homepage = [http://www.figarosystems.com figarosystems.com]
}}

'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 &lt;ref&gt;Andrew Webb, &#8220;Opera Subtitle Firm Eyes New Game,&#8221; ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]&lt;/ref&gt;
by Patrick Markle, [[Geoff Webb]], and Ron Erkman  &lt;ref name="figaro-systems.com"/&gt; and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.
&lt;ref&gt;[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, &#8220;Nothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,&#8221;] ''Albuquerque Journal'', June 4, 2006&lt;/ref&gt;

==History==
Figaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.&lt;ref&gt;[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don't get it",] ''New Mexico Business Weekly'', April 8, 2005&lt;/ref&gt;

The [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.&lt;ref name="figaro-systems.com"&gt;[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]&lt;/ref&gt; Markle, Webb, and Erkman were further reinforced by their understanding of technology&#8217;s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.&lt;ref&gt;[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf &#8220;[[User-friendly]] art: In-seat text displays that subtitle and translate&#8221;, ''Auditoria'', May 2007]&lt;/ref&gt; Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.
&lt;ref&gt;[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=11&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=figaro.ASNM.&amp;OS=AN/figaro&amp;RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]&lt;/ref&gt;&lt;ref&gt;[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]&lt;/ref&gt;

Philanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.&lt;ref&gt;[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," ''[[New York Magazine]]'', January 14, 2002]&lt;/ref&gt;&lt;ref&gt;[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," ''The Free Encyclopedia'']&lt;/ref&gt;  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.

In 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." &lt;ref&gt;[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com&lt;/ref&gt;

==Products and technology==
The company&#8217;s products are known variously as seat back titles, [[surtitles]],
&lt;ref&gt;[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/&lt;/ref&gt; [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.

Opera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]
&lt;ref&gt;[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov&lt;/ref&gt; although the software enables the reading of the libretto in any [[written language]].
&lt;ref name="entertanmentengineering.com"&gt;[http://www.entertanmentengineering.com/v4.issue04/page.06.html  &#8220;Giving the Opera a New Voice,&#8221;] ''Entertainment Engineering," Volume 4, Issue 2, p. 6&lt;/ref&gt; Translation is provided by one screen and delivery system per person.&lt;ref&gt;[http://www.figarosystems.com  Figaro Systems Official Website]&lt;/ref&gt;

Typically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues&#8217; existing systems as well as Figaro Systems' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.&lt;ref name="entertanmentengineering.com"/&gt;

The company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens 
&lt;ref&gt;[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=figaro.ASNM.&amp;OS=AN/figaro&amp;RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database&lt;/ref&gt; for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.

==Venues==
In the US, the company&#8217;s systems are in use in the [[Ellie Caulkins Opera House]] 
&lt;ref&gt;[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,&lt;/ref&gt; in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,&lt;ref&gt;[https://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],&lt;/ref&gt; the [[Brooklyn Academy of Music]]&lt;ref&gt;[http://www.appliancemagazine.com/editorial.php?article=1768&amp;zone=210&amp;first=1  &#8220;An Operatic Performance,&#8221; ''Appliance Magazine'', June 2007],&lt;/ref&gt; the [[Metropolitan Opera]], New York, where it is called "MetTitles"),&lt;ref&gt;[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],&lt;/ref&gt; the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.&lt;ref name="figaro-systems.com"/&gt;

In the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].
&lt;ref&gt;[http://www.entertainmentengineering.com/v4.issue04/page.06.html &#8220;Giving the Opera a New Voice,&#8221; ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com&lt;/ref&gt;

==Awards==
In 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories&#8217; Technology Commercialization Award for its Simultext system.&lt;ref&gt;[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.&lt;/ref&gt;
In 2008, the company&#8217;s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.

==References==
{{Reflist}}

[[Category:Assistive technology]]
[[Category:Companies based in Santa Fe, New Mexico]]
[[Category:Companies established in 1993]]
[[Category:Educational technology companies]]
[[Category:Information retrieval organizations]]
[[Category:Privately held companies based in New Mexico]]
[[Category:Software companies based in New Mexico]]</text>
      <sha1>gyaxrxh3bhd23tjb5drzwk6myfo52j8</sha1>
    </revision>
  </page>
  <page>
    <title>European Conference on Information Retrieval</title>
    <ns>0</ns>
    <id>10328235</id>
    <revision>
      <id>696281305</id>
      <parentid>673371570</parentid>
      <timestamp>2015-12-22T03:03:09Z</timestamp>
      <contributor>
        <username>Jfoley-cs</username>
        <id>20336486</id>
      </contributor>
      <comment>Update to consolidate past/present and update to current list of locations.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3234" xml:space="preserve">The '''European Conference on Information Retrieval''' (ECIR) is the main 
European research conference for the presentation of new results in the field of [[information retrieval]] (IR).
It is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).
      
The event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was 
held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has
alternated between the United Kingdom and continental Europe. To mark the metamorphosis
from a small informal colloquium to a major event in the IR research calendar, the 
BCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,
ECIR has continued to grow and has become the major European forum for the discussion
of research in the field of Information Retrieval.

Some of the topics dealt with include:
* IR models, techniques, and algorithms
* IR applications
* IR system architectures
* Test and evaluation methods for IR
* [[Natural Language Processing]] for IR
* Distributed IR
* Multimedia and cross-media IR

==Time and Location==

Traditionally, the ECIR is held in Spring, near the Easter weekend. A list of locations and planned venues are presented below.

* [[Padova, Italy]], 2016 [http://ecir2016.dei.unipd.it/]
* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]
* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]
* [[Moscow, Russia]], 2013 [http://ecir2013.org/]
* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]
* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]
* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]
* [[Toulouse]], 2009 [http://ecir09.irit.fr/]
* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]
* [[Rome]], 2007 [http://ecir2007.fub.it/]
* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]
* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]
* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]
* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]
* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*
* [[Darmstadt]], 2001* (organized by GMD)
* [[Cambridge]], 2000* (organized by Microsoft Research)
* [[Glasgow]], 1999*
* [[Grenoble]], 1998*
* [[Aberdeen, Scotland|Aberdeen]], 1997*
* [[Manchester]], 1996*
* [[Crewe]], 1995* (organized by Manchester Metropolitan University)
* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)
* [[Glasgow]], 1993* (organized by Strathclyde University)
* [[Lancaster, Lancashire|Lancaster]], 1992*
* [[Lancaster, Lancashire|Lancaster]], 1991*
* [[Huddersfield]], 1990*
* [[Huddersfield]], 1989*
* [[Huddersfield]], 1988*
* [[Glasgow]], 1987*
* [[Glasgow]], 1986*
* [[Bradford]], 1985*
* [[Bradford]], 1984*
* [[Sheffield]], 1983*
* [[Sheffield]], 1982*
* [[Birmingham]], 1981*
* [[Leeds]], 1980*
* [[Leeds]], 1979*

&lt;br /&gt; *as the Annual Colloquium on Information Retrieval Research

==External links==
* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]

[[Category:Information retrieval organizations]]
[[Category:Computer science conferences]]</text>
      <sha1>riqu9nbt26tklccsku5oewp3cs1l4qn</sha1>
    </revision>
  </page>
  <page>
    <title>Coveo</title>
    <ns>0</ns>
    <id>16001013</id>
    <revision>
      <id>747841320</id>
      <parentid>701612031</parentid>
      <timestamp>2016-11-04T17:59:10Z</timestamp>
      <contributor>
        <ip>38.104.140.214</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3058" xml:space="preserve">{{Infobox company
| name = Coveo Solutions Inc.
| logo = [[Image:Coveo logo.png|120px]]
| type = Private
| slogan = 
| foundation =  2005
| location_city = [[Quebec City]], [[Canada]]
| key_people = Louis T&#234;tu, Chairman and CEO &lt;br /&gt;Laurent Simoneau, President and CTO
| num_employees =200+
| industry = [[Enterprise search]]
| products = Coveo Search &amp; Relevance Platform,&lt;br /&gt;Coveo for Sitecore,&lt;br /&gt;Coveo for Salesforce
| homepage = http://www.coveo.com
}}

'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for [[Salesforce.com]], Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.

==History==
Coveo Solutions Inc. was founded in 2005 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.&lt;ref&gt;http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/&lt;/ref&gt;

==Products==
'''Coveo Search &amp; Relevance Platform'''

Coveo Search &amp; Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.

'''Coveo for Sitecore'''

Coveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore&#8217;s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.

'''Coveo for Salesforce'''

Coveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.

==Customers==
Coveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley &amp; Aldrich, GEICO, Lockheed Martin, P&amp;G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.&lt;ref&gt;{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}&lt;/ref&gt; These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}

==References==
{{reflist}}

==External links==
* [http://www.coveo.com/ Coveo.com]

[[Category:Companies based in Quebec City]]
[[Category:Information retrieval organizations]]
[[Category:BlackBerry development software]]</text>
      <sha1>gia8fu1hkzihglmcfe8koywrlqm7vvn</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Yandex</title>
    <ns>14</ns>
    <id>31871742</id>
    <revision>
      <id>666734407</id>
      <parentid>664470321</parentid>
      <timestamp>2015-06-13T08:12:11Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>added [[Category:Information retrieval organizations]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="495" xml:space="preserve">{{commonscat|Yandex}}
{{Cat main|Yandex}}

[[Category:Wikipedia categories named after information technology companies]]
[[Category:Wikipedia categories named after companies of Russia]]
[[Category:Web portals]]
[[Category:Internet search engines]]
[[Category:Internet companies of Russia]]
[[Category:Companies listed on NASDAQ]]
[[Category:Internet in Russia]]
[[Category:Internet properties established in 1997]]
[[Category:Russian websites]]
[[Category:Information retrieval organizations]]</text>
      <sha1>plohxsti2uz2mb9oqdzn1052x0hhtxj</sha1>
    </revision>
  </page>
  <page>
    <title>Smartlogic</title>
    <ns>0</ns>
    <id>39083726</id>
    <revision>
      <id>731887173</id>
      <parentid>714748020</parentid>
      <timestamp>2016-07-28T04:37:50Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>USA is deprecated, per [[MOS:NOTUSA]], and correct [[MOS:OVERLINK|overlinking]] of common places using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3133" xml:space="preserve">{{Multiple issues|
{{advert|date=June 2015}}
{{COI|date=June 2015}}
{{notability|Companies|date=November 2015}}
}}
{{Infobox company |
name = Smartlogic  |
logo =  |
slogan = "The Content Intelligence Company" |
type = [[Privately held company|Private]] |
foundation = 2006 |
location = United States, UK | 
area_served = Global |
industry = [[Information retrieval]] |
products = Semaphore Cloud, Semaphore Ontology Editor, Semaphore Classification Server, Semaphore Semantic Enhancement Server, Advanced Language Packs, Search Appliance Framework, Text Miner, Classification Review Tool, Classification Analysis Tool  | 
num_employees  = 55|
homepage = http://www.smartlogic.com
}}

'''Smartlogic ''' is a [[software company]] which specializes in developing [[information retrieval]], [[text analytics]] and [[knowledge management]] solutions.

==History==
Smartlogic was founded in the United Kingdom in 2006. It is a privately held company and has offices in San Jose, CA; Alexandria, VA; Cambridge, MA and London, UK. The company develops and sells a suite of products; Semaphore Ontology Editor, Classification Server, Advanced Language Packs, Semantic Enhancement Server, Text Miner, Classification Review tool, and Classification Analysis tool.

==Products==

===Semaphore Ontology Editor===
Semaphore Ontology Editor is a web-based tool used to build taxonomies, ontologies, controlled vocabularies as well as other knowledge organization systems. Models are used by organizations to enhance the capabilities of enterprise search engines,&lt;ref&gt;[http://www.cmswire.com/events/item/webinar-leverage-metadata-to-drive-critical-business-processes-022370.php] CMSWire Leverage Metadata to Drive Critical Business Processes&lt;/ref&gt; content management and workflow systems deployed by clients to augment and enhance their investment.

===Semaphore Classification Server===
Semaphore Classification Server uses the model structure from Semaphore Ontology Editor and auto classifies unstructured information assets by applying metadata tags to the unstructured information.

===Semaphore Advanced Language Packs===

===Semantic Enhancement Server===

==Integrations==
Semaphore integrates with [[Microsoft Sharepoint]],&lt;ref&gt;[http://www.cmswire.com/cms/information-management/sharepoint-2013-office-365-get-semantic-search-with-smartlogic-semaphore-018353.php] CMSWire SharePoint 2013 Office 365 Get Semantic Search with Smartlogic Semaphore&lt;/ref&gt; [[Google Search Appliance]],&lt;ref&gt;[https://www.google.com/enterprise/marketplace/viewVendorListings?vendorId=33&amp;pli=1] Google Enterprise Catalogue&lt;/ref&gt; [[Apache Solr]],&lt;ref&gt;[http://www.flatironssolutions.com/blog/alfresco-semaphore-integration/] Alfresco-Semaphore Integration&lt;/ref&gt; FAST ESP&lt;ref&gt;[http://arnoldit.com/wordpress/2009/10/23/smartlogic-and-fast-esp-integration/] Stephen E. Arnold - Beyond Search&lt;/ref&gt; and others.

==References==
{{Reflist}}

==External links==
* [http://www.smartlogic.com/ Smartlogic]

[[Category:Software companies of the United Kingdom]]
[[Category:Information retrieval organizations]]
[[Category:Analytics companies]]
[[Category:Knowledge management]]</text>
      <sha1>5vzqowbooya9gblfepzmhdozctlyr6e</sha1>
    </revision>
  </page>
  <page>
    <title>Gerard Salton Award</title>
    <ns>0</ns>
    <id>1981660</id>
    <revision>
      <id>677373770</id>
      <parentid>675414620</parentid>
      <timestamp>2015-08-22T20:49:52Z</timestamp>
      <contributor>
        <username>Hiemstra</username>
        <id>5757030</id>
      </contributor>
      <minor />
      <comment>/* Chronological honorees and lectures */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2993" xml:space="preserve">The '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].

==Chronological honorees and lectures==
* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."
* 1988 - [[Karen Sp&#228;rck Jones]], [[University of Cambridge]] : "A look back and a look forward."
* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."
* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"
* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." 
* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."&lt;BR&gt;'''For ...''' ''"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."''
* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."&lt;BR&gt;'''For ...''' ''"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."''
* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	"Quantum haystacks."
* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."
* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."
* 2015 - [[Nicholas J. Belkin]], [[Rutgers University]]: &#8220;People, Interacting with Information&#8221;

==External links==
* [http://www.acm.org/sigir/ ACM SIGIR homepage]
* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]

[[Category:Association for Computing Machinery]]
[[Category:Computer science awards]]
[[Category:Information retrieval organizations]]</text>
      <sha1>7p103c98c2iwzcil5sre072cnj75pqi</sha1>
    </revision>
  </page>
  <page>
    <title>Ness Computing</title>
    <ns>0</ns>
    <id>32567205</id>
    <revision>
      <id>762161003</id>
      <parentid>716722231</parentid>
      <timestamp>2017-01-27T01:24:41Z</timestamp>
      <contributor>
        <ip>2601:647:4D03:3CA7:351F:EDB6:72D7:FA03</ip>
      </contributor>
      <comment>clean up text and remove some marketing-y phrases</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1862" xml:space="preserve">{{Notability|Companies|date=July 2011}}

'''Ness Computing''' was a personal search company. It was acquired by OpenTable in March 2014 and was shut down later that year.&lt;ref&gt;{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}&lt;/ref&gt; 

It was founded in October 2009 by Corey Reese,&lt;ref&gt;http://www.linkedin.com/in/coreyreese&lt;/ref&gt; Paul Twohey,&lt;ref&gt;http://www.linkedin.com/in/twohey&lt;/ref&gt; Nikhil Raghavan,&lt;ref&gt;http://www.linkedin.com/in/nikhilraghavan&lt;/ref&gt; and Steven Schlansker.&lt;ref&gt;http://www.linkedin.com/in/stevenschlansker&lt;/ref&gt; The company was headquartered in Los Altos, California.

Ness aimed to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. The company referred to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that used [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that served up results based on these signals. 

The free Ness Dining App (for iPhone) was referred to as the [[Netflix]] &lt;ref&gt;http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php&lt;/ref&gt; or [[Pandora Radio|Pandora]] &lt;ref&gt;http://gigaom.com/2011/08/25/ness-restaurant-app/&lt;/ref&gt; for restaurants. Based on a user's ratings and preferences, the service delivered recommendations for a particular time, location, price range, and cuisine preference. Users could view the menu for a place via SinglePlatform,&lt;ref&gt;http://www.singleplatform.com/&lt;/ref&gt; browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]].

==References==
{{Reflist}}

[[Category:Information retrieval organizations]]
[[Category:Software companies based in California]]</text>
      <sha1>2pwqy0zraygr7snf2btr953mb6emq85</sha1>
    </revision>
  </page>
  <page>
    <title>ChemRefer</title>
    <ns>0</ns>
    <id>11242818</id>
    <revision>
      <id>750662252</id>
      <parentid>696391061</parentid>
      <timestamp>2016-11-21T03:14:37Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 4 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3153" xml:space="preserve">{{Orphan|date=February 2009}}
{{Infobox website
| name = Chemrefer
| logo = [[Image:Chemrefer.png]]
| screenshot = 
| caption = 
| url = http://www.chemrefer.com
| commercial = Yes
| type = [[Search engine]]
| language = English
| registration = Not Applicable
| owner = ChemRefer Limited
| author = William James Griffiths
| launch date = 2006
| current status = Offline
| revenue = 
}}
'''ChemRefer''' is a service that allows searching of freely available and full-text chemical and pharmaceutical literature that is published by authoritative sources.&lt;ref&gt;{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}&lt;/ref&gt;

Features include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.

ChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.

==See also==
*[[Google Scholar]]
*[[Windows Live Academic]]
*[[BASE (search engine)|BASE]]
*[[PubMed]]

==References==
{{reflist}}

==External links==
===Recommendations &amp; reviews===
*[https://web.archive.org/web/20060902072725/http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]
*[https://web.archive.org/web/20070804051550/http://infoweb.nrl.navy.mil:80/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]
*[https://web.archive.org/web/20070212122105/http://www.mta.ca:80/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]
*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine
*[https://web.archive.org/web/20080917155607/http://recherche-technologie.wallonie.be:80/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium
*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki

===Background===
*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine
*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College

[[Category:Scholarly search services]]
[[Category:Chemistry literature]]
[[Category:Information retrieval systems]]
[[Category:Open access projects]]

{{searchengine-website-stub}}</text>
      <sha1>8750da31coonzcsb326q2qohodeyxt4</sha1>
    </revision>
  </page>
  <page>
    <title>Ptx (Unix)</title>
    <ns>0</ns>
    <id>1442890</id>
    <revision>
      <id>744303325</id>
      <parentid>666706695</parentid>
      <timestamp>2016-10-14T11:30:07Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="602" xml:space="preserve">{{Unreferenced stub|auto=yes|date=December 2009}}
{{Lowercase|title=ptx}}
'''ptx''' is a [[Unix utility]], named for the ''[[permuted index]]'' which can perform the function of the [[Keyword in Context]] (KWIC) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, [[thesaurus]]es, or web sites to aid in locating entries of interest.

==See also==
* [[Concordancer]]

[[Category:Information retrieval systems]]
[[Category:Unix text processing utilities]]


{{Unix-stub}}</text>
      <sha1>99vz846szdr0goe3urfsc1ttejc1gce</sha1>
    </revision>
  </page>
  <page>
    <title>Contextual Query Language</title>
    <ns>0</ns>
    <id>9672320</id>
    <revision>
      <id>666712419</id>
      <parentid>599561646</parentid>
      <timestamp>2015-06-13T03:21:05Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2682" xml:space="preserve">'''Contextual Query Language''' (CQL), previously known as '''Common Query Language''',&lt;ref&gt;[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress&lt;/ref&gt; is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].

== Examples of query syntax ==

Simple queries:

&lt;blockquote&gt;&lt;tt&gt;dinosaur&lt;br/&gt;
"complete dinosaur"&lt;br/&gt;
title = "complete dinosaur"&lt;br/&gt;
title exact "the complete dinosaur"&lt;/tt&gt;&lt;/blockquote&gt;

Queries using [[Boolean logic]]:

&lt;blockquote&gt;&lt;tt&gt;dinosaur or bird&lt;br/&gt;
Palomar assignment and "ice age"&lt;br/&gt;
dinosaur not reptile&lt;br/&gt;
dinosaur and bird or dinobird&lt;br/&gt;
(bird or dinosaur) and (feathers or scales)&lt;br/&gt;
"feathered dinosaur" and (yixian or jehol)&lt;/tt&gt;&lt;/blockquote&gt;

Queries accessing [[index (publishing)|publication indexes]]:

&lt;blockquote&gt;&lt;tt&gt;publicationYear &lt; 1980&lt;br/&gt;
lengthOfFemur &gt; 2.4&lt;br/&gt;
bioMass &gt;= 100&lt;/tt&gt;&lt;/blockquote&gt;

Queries based on the proximity of words to each other in a document:

&lt;blockquote&gt;&lt;tt&gt;ribs prox/distance&lt;=5 chevrons&lt;br/&gt;
ribs prox/unit=sentence chevrons&lt;br/&gt;
ribs prox/distance&gt;0/unit=paragraph chevrons&lt;/tt&gt;&lt;/blockquote&gt;

Queries across multiple [[Dimension (data warehouse)|dimensions]]:

&lt;blockquote&gt;&lt;tt&gt;date within "2002 2005"&lt;br/&gt;
dateRange encloses 2003&lt;/tt&gt;&lt;/blockquote&gt;

Queries based on [[Relevance (information retrieval)|relevance]]:

&lt;blockquote&gt;&lt;tt&gt;subject any/relevant "fish frog"&lt;br/&gt;
subject any/rel.lr "fish frog"&lt;/tt&gt;&lt;/blockquote&gt;

The latter example specifies using a specific [[algorithm]] for [[logistic regression]].&lt;ref&gt;[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://www.loc.gov/standards/sru/cql/ CQL home page]
* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]
* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]

{{Query languages}}

{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}
{{LOC-stub}}

[[Category:Information retrieval systems]]
[[Category:Library science]]
[[Category:Library of Congress]]
[[Category:Query languages]]
[[Category:Knowledge representation languages]]</text>
      <sha1>0m9p20h4ri08w133ag6qbrq8hiz9kp1</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of enterprise search software</title>
    <ns>0</ns>
    <id>41829011</id>
    <revision>
      <id>750936293</id>
      <parentid>749627004</parentid>
      <timestamp>2016-11-22T10:15:02Z</timestamp>
      <contributor>
        <username>Elise lowry</username>
        <id>29603353</id>
      </contributor>
      <comment>/* General information */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="56167" xml:space="preserve">{{Cleanup-list|date=June 2014}}

The following tables compare the major [[List of enterprise search vendors|enterprise search software]] vendors in their classes.

== General information ==

{| class="wikitable sortable"
|-
! Product !! formerly k.a. !! Vendor !! [[Software release life cycle|Stable release]] !! Update!! Platforms !! API !! Target Customer !! Software License !! Open source !! Multilingual !! Website
|-
|Mindbreeze InSpire
|Mindbreeze InSpire
|Mindbreeze 
|2016 Summer Release
|October 17th, 2016
|Windows Server Linux
|REST, SOAP, .NET, Java, Push API
|Information Insight, Knowledge management for departments/organizations, Big Data Search &amp; Analytics
|?
|No
|Yes (including languages like CJK)
|www.mindbreeze.com 
|-
| [[Lookeen Desktop Search#Lookeen Server|Lookeen Server]]&lt;ref&gt;[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]&lt;/ref&gt; || Lookeen Server || [[Axonic Informationssysteme GmbH]] || 1.3.1.1118 || April 2014 || Windows Server || [[.NET Framework|.NET]] || ? || ? || {{no}} || {{yes}} || http://www.lookeen-server.com
|-
| intergator || intergator Enterprise Search || [[intergator|interface projects GmbH]] || 5.3 || March 2016 || Windows Server, Linux Server || Java, Groovy und XML/JSON || Enterprise Search,Knowledge-Management, Content Analytics, Big Data || {{Yes}} || {{No}} || {{Yes}} || http://www.intergator.de 
|-
| Coveo Enterprise Search || Coveo Platform || [[Coveo|Coveo Solutions Inc]] || 7.0 || February 6, 2014 || Windows || [[REST]], [[SOAP]], [[.NET Framework|.NET]] || Web customer service, customer interaction hubs || ? || {{no}} || {{yes|Yes, multilingual user interfaces}} || http://www.coveo.com/en/advanced-enterprise-search
|-
| 3RDi Search || 3RDi Search || The Digital Group Inc || 1.0 || NA || Generic || Supported || Enterprise Search, Knowledge Management, Big Data, BI, Analytics || Commercial || {{no}} || {{Yes}} || http://www.3rdisearch.com
|-
| Endeca Guided Search || ? || [[Oracle Corporation|Oracle]] || 6.2.2 || March 2012 || ? || ? || ? || ? || ? || ? || http://www.oracle.com/us/products/applications/commerce/endeca/endeca-guided-search/overview/index.html
|-
| EXALEAD CloudView || ? || [[Dassault Syst&#232;mes]] || R2014 || July 2013 || Windows Server, Linux Server || Push API (PAPI) || ? || ? || {{no}} || {{yes|Yes, &lt;br /&gt;125 languages supported}} || http://www.3ds.com/products-services/exalead/
|-
|Datafari
|
|France Labs
|2.2.1
|April 2016
|Linux Server, Windows (for test)
|REST
|Enterprise Search, Knowledge management, Big Data, BI, Analytics
|No
|Yes
|Yes
|&lt;nowiki&gt;http://www.datafari.com/en&lt;/nowiki&gt;
|-
| FAST for SharePoint 2010 (F4SP) || [[Fast Search &amp; Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?
|-
| SharePoint 2013 || [[Fast Search &amp; Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?
|-
| [[Google Search Appliance]] (GSA) || ? || [[Google]] || 7.4 || March 2015 || ? || [[.NET Framework|.NET]], [[Java]] || ? || ? || ? || {{yes}} || https://www.google.com/enterprise/search/products/gsa.html
|-
| HP IDOL&lt;ref&gt;[http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/ HP IDOL]&lt;/ref&gt; || . || [[Hewlett Packard Enterprise|HPE BigData]] || 11 || March 2016 || Windows, HP-UX, Linux, Solaris || [[SOAP]], [[REST]] || big data and analytics, Enterprise search, Video, Image or Audio analytics, Knowledge-Management, Info-Governance || Yes, licensed by volume. Starting on 250Gb of Metadata || No || {{yes}} || http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/
|-
| Haven Search OnDemand&lt;ref&gt;[http://search.havenondemand.com/ Haven Search OnDemand]&lt;/ref&gt; || . || [[Hewlett Packard Enterprise|HPE BigData]] || 20160329|| March 2016 || SAAS Offering || [[REST]] || ? || Consumption-based pricing. || No || {{yes}} || http://search.havenondemand.com
|-
|-
| [[Funnelback]] Search || Panoptic Search || [[Funnelback]] || 15.6 || June 2016 || SaaS Offering, Windows server, Linux server || [[REST]] || Enterprise search, website search, vertical search || Document- and server-based licensing || {{no}} || {{yes|Yes.  Multi-lingual support includes indexing, querying and localised UIs}} || https://www.funnelback.com
|-
| IBM Infosphere Data Explorer || [[Vivisimo|Vivisimo Velocity]] || [[IBM]] || ? || ? || ? || ? || big data and analytics projects || ? || ? || {{yes}} || http://www-01.ibm.com/software/data/information-optimization/
|-
| [[Swiftype]] Search || [[Swiftype]] || [[Swiftype]] || ? || September 2014 || Windows and Linux, MacOS || [[REST]] APIs || websites and mobile applications || ? No{{No}} || ? || https://swiftype.com
|-
| Lucidworks Fusion || N/A || [[Lucidworks|Lucidworks Inc.]] || 2.1.4 || March 2016 || Windows, Linux, MacOS || [[REST]] APIs || Enterprise search, online retail, search-based data analytics || Licensed by CPU cores || {{No}} ||{{yes}}|| https://lucidworks.com/products/fusion/

|-
| Perceptive Search/ISYS || Enterprise Server || [[Lexmark|Lexmark Perceptive Software]] || 4.2 || ? || Windows, Linux, Solaris, Mac OS, HP-UX and AIX || [[REST]], [[SOAP]] || ? || ? || ? || {{yes}} || http://www.perceptivesoftware.com/products/perceptive-search
|-
| Secure Enterprise Search (SES) || ? || [[Oracle Corporation|Oracle]] || 11.2.2.2 || January 2014 || Windows, Linux (Oracle, RedHat and SUSE, 32-bit and 64 bit), Solaris || ? || ? || ? || ? || ? || http://www.oracle.com/technetwork/search/oses/documentation/ses-096384.html
|-
| RAVN Connect ||   || RAVN Systems || 3.3 || February 2014 || Windows Server, Linux Server || REST API || ? || ? || {{no}} || {{yes}} || http://www.ravn.co.uk/capabilities/enterprise-search/
|}

== Features ==

=== Content Collection &amp; Indexing ===

This section compares the ability of the products to collect and index content, both textual and non textual, from different data source types and document types (formats).

==== Indexing and connectivity ====

This is about indexing pipeline tools and processes; included connectors, support for connectors, etc.

===== Web-based =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server&lt;ref&gt;[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]&lt;/ref&gt;!! HP IDOL&lt;ref&gt;[http://www.ndm.net/archiving/HP-Autonomy/information-connectivity Autonomy Information Connectivity]&lt;/ref&gt;!! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://www.coveo.com/en/platform-features-connect#connectorsSection Coveo connectors]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Swiftype Search !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search&lt;ref&gt;[http://www.oracle.com/technetwork/search/oses/overview/ses11222ds-1969734.pdf SES 11.2.2.2 Datasheet - Oracle]&lt;/ref&gt;!! RAVN Connect !! intergator !! Funnelback Search
|-
| [[HTTP]] || For crawling of Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || ? || {{yes}} || {{yes}} ||  {{yes}}
|-
| [[HTTPS]] || For crawling of secured Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}}  
|-
| [[XML]] || For indexing any XML-compliant data source. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}} 
|-
|}

===== File-based =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2 !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| [[NetWare File System|Netware file systems]]  || For incremental indexing of Netware file systems. || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}
|-
| [[Samba]]/[[Unix File System|Unix file systems]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Windows File System|Windows file systems]], Windows NT Filesystems (NTFS) ||  || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
|}

===== Archiving/Directory =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What Connectors Are Available With Coveo]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| LDAP || For indexing a company directory stored on a LDAP (v2 or v3) server. || ? ||  {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| [[Microsoft Active Directory]] || Supports Microsoft Active Directory. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| [[Symantec Enterprise Vault]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}
|-
|}

===== Messaging =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server || HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Akonix || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| [[Facetime]] || . || ? || {{yes}}|| ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| [[IBM Lotus Connections]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[IMAP]] || For indexing e-mail messages and attached files stored on an IMAP server. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[IBM Lotus Notes]] || For indexing e-mail messages and attached files stored on a Lotus notes server. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CLN)}} 
|?|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[Microsoft Exchange Server|Microsoft Exchange]] || Ability to retrieve and index e-mail messages and attached files &lt;br /&gt; (Mailboxes, Public Folders). || {{yes}} || {{yes}} || {{yes|Yes, &lt;br /&gt; Exchange 2003/2007/2010/2013 Servers}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CXG)}} 
|Yes|| ? || ? || {{yes|Yes &lt;br /&gt;(via Adhere Solutions partner)}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; Exchange 2003 Servers}} || {{yes}}|| {{yes}}
|-
| [[Microsoft Exchange Server|Microsoft Exchange Online]] || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || . 
|Yes (IMAP)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[NNTP]] || For real-time indexing of Usenet news groups. || ? ||  ? || ? || ? || ? || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
| [[Gmail]] || . || ? || ? || {{yes}} || ? || {{yes}} || . 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
|}

===== [[Content management system|CMS]], [[Document management system|DMS]] &amp; Social =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL&lt;ref&gt;[http://www8.hp.com/us/en/software-solutions/asset/software-asset-viewer.html?asset=1997065&amp;module=1970565&amp;docname=4AA5-8715ENW&amp;page=1970341 KeyView IDOL - Product Brief]&lt;/ref&gt;!! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What connectors are available with Coveo]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Alfresco (software)|Alfresco]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but no native support}} 
|Yes|| ? || ? || {{yes|Yes (via Incentro partner)}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| [[EMC Documentum|EMC Documentum Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CDO)}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}}  || {{yes}}|| {{yes}}
|-
| [[EPiServer|EpiServer]] || . || ? || {{yes}} || {{yes}} || ? || ? || {{no}} 
|?|| ? || ? || ? || ? || {{no}} || ? || ?  || ?|| {{yes}}
|-
| IBM Content Manager || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || {{yes|Yes &lt;br /&gt;(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}
|-
| IBM FileNet || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes (P8)|| ? || ? || {{yes|Yes &lt;br /&gt;(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}
|-
| [[WebSphere|IBM WebSphere]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, Informatica PowerCenter 9.x connectivity}} 
|?|| ? || ? || {{yes|Yes (via Adhere Solutions partner)}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Jalios || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CJA)}} 
|?|| ? || ? || ? || ? || ? || ? || ?  || ?|| ?
|-
| [[SharePoint]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CXG)}} 
|Yes|| ? || ? || {{yes|Microsoft SharePoint Portal Server &lt;br /&gt; Microsoft SharePoint Services}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| SharePoint Online || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[Dropbox (service)|Dropbox]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| Windows File Share || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| [[Google Docs]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}
|-
| Jive || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}
|-
| [[Plumtree Software|Plumtree]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{no}} || ? || ? || ?|| {{yes}}
|-
| Lithium || . || ? || ? || {{yes}} || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ?
|-
| [[Confluence]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ?  || ? || {{yes}} 
|-
| [[Twitter]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} 
|-
| [[Facebook]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|No|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| ? 
|-
| [[LinkedIn]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? 
|-
|}

===== Databases =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://www.arnoldit.com/search-wizards-speak/coveo.html Coveo Solutions Inc., An Interview with Laurent Simoneau]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| [[IBM DB2]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter &lt;br /&gt; DB2 for Linux}} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[JDBC]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; translates [[SQL database]] fields &lt;br /&gt; into XML documents &lt;br /&gt; than are then indexed together &lt;br /&gt;  with the document metadata}} 
|Yes|| ? || ? || Yes || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft SQL Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[MySQL]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[ODBC]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Oracle RDBMS]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{?}}
|-
| [[Sybase]] || . || ? || {{yes}} || ? || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|?|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
|}

===== [[Customer relationship management|CRM]], [[Enterprise resource planning|ERP]], [[Product lifecycle management|PLM]], [[Business intelligence|BI]] =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance&lt;ref&gt;[https://www.google.com/enterprise/marketplace/viewListing?productListingId=3905631+7212827882376498737 IBM DB2 Content Manager OD Connector for Google Search Appliance, by Adhere Solutions]&lt;/ref&gt;!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Salesforce.com|Salesforce]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CSF)}} 
|Yes (via partner)|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ? || {{yes}} || ?
|-
| [[SAP Business Suite|SAP]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} 
|-
| [[Siebel Systems|Siebel]]/Oracle || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, ETL:Informatica }} 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[Microsoft Dynamics]] || . || ? || ? || {{yes}} || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} 
|-
| [[Business Objects|Business Object]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[IBM]] [[Cognos]] || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[Informatica|Informatica PowerCenter]] || . || ? || ? || ? || ? || ? || {{yes|Yes &lt;br /&gt; (trigram:CJA)}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ? || ? 
|-
| [[MicroStrategy]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes (via partners)}} || ? || ? 
|-
| [[Windchill (software)|PTC Windchill]] || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ? ||{{yes}}
|-
| [[ENOVIA MatrixOne|ENOVIA]] || Support for MatrixOne/Enovia data. || ? || {{yes}} || ? || ? || ? || {{yes|Yes &lt;br /&gt; (trigram:CEN)}} 
|?|| ? || ? || {{yes|Yes (via partner)}} || ? || ? || ? || ? || ?|| {{yes}}
|-
|}

===== Miscellaneous =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Jira (software)|Jira]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}
|-
| [[GitHub]] || . || ? || ? || ? || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?
|-
| [[Slack (software)|Slack]] || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?
|-
| [[Mantis Bug Tracker|Mantis]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
| [[JD Edwards]]/Oracle || EnterpriseOne, World. || ? || {{yes}} || ? || ? || ? || {{yes|Yes &lt;br /&gt;ETL:Informatica PowerCenter }} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
|-
| [[PeopleSoft]]/Oracle || . || ? || {{yes}} || ? || ? || ? || {{yes|Yes &lt;br /&gt;ETL:Informatica PowerCenter}} 
|?|| ? || ? || ? || ? || ? || ? || {{coming soon}} || ?|| ?
|-
|}

==== Supported Formats ====

{| class="wikitable sortable"
|-
! File type !! Description !! Lookeen Server !! HP IDOL  !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/CES/7.0/User/Supported_File_Formats.htm Coveo Platform 7, Supported File Formats]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView&lt;ref&gt;[http://3ds.exalead.com/software/common/pdfs/products/cloudview/Exalead-Connectors-and-Formats.pdf Exalead Cloudview Connectors + Formats]&lt;/ref&gt;
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Adobe [[PDF]] || Includes Adobe Acrobat or other PDF documents. || {{yes}} || {{yes}} || {{yes|Yes, &lt;br /&gt; Version 1.0 to 1.7}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Web pages || [[HTML]], [[XHTML]], etc. || {{yes}} || {{yes|Yes, &lt;br /&gt; versions v.3, 4, 5}} || {{yes|.asp, .aspx, .cgi, .col, &lt;br /&gt; .dochtml, .dothtml, .fphtml, &lt;br /&gt; .hta, .htm, .html, .jsp, .php, &lt;br /&gt; .pothtml, .ppthtml, .shtm, &lt;br /&gt; .shtml, .xlshtml}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; versions v.4.01 and above, and [[XHTML]]}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[XML]] || Extensible Markup Language (.xml) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; any [[Document type definition|DTD]]}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Text || Raw text. || {{yes}} || {{yes}} || {{yes|.ascx, .bat, .cmd, .config, &lt;br /&gt; .csv, .dic, .exc, .inf, .ini, &lt;br /&gt; .js, .jsl, .log, .nfo, .scp, &lt;br /&gt; .sdl, .sln, .txt, .vbdproj, &lt;br /&gt; .vbs, .vdp, .vdproj, .vjp, &lt;br /&gt; .vjsproj, .vjsprojdata, &lt;br /&gt; .wsdl, .wsf, .wtx, .xsd &lt;br /&gt; ANSI, ASCII, Unicode}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Excel]] || Microsoft Excel Charts (.xls), &lt;br /&gt; Microsoft Excel XML (.xlsx, .xltm, .xltx) &lt;br /&gt; Others (.xlam, .xlb, .xlm, .xlsm) || {{yes}} || {{yes}} || {{yes|&lt;br /&gt; Version 5.0, 95(7.0), 97, 2000, XP, 2003, 2007, 2010. &lt;br /&gt; Indexes Excel 2010 attachments.}} || {{yes}} || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Word]] || Microsoft Word (.doc) &lt;br /&gt; Microsoft Word XML (.docx, .dotx, .dotm) || {{yes}} || {{yes|Yes &lt;br /&gt; for Mac, Windows (multiple versions)}} || {{yes|Version 6.0, 6.0 (for MAC), 95 (7.0), 97, 98 (for MAC), 2000, &lt;br /&gt; XP, 2003, 2007, 2010 &lt;br /&gt; Indexes Word 2010 attachments.}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft PowerPoint]] || .pot, .potm, .potx, .ppam,  &lt;br /&gt; .pps, .ppsm, .ppsx, .ppt,  &lt;br /&gt; .pptm, .pptx || {{yes}} || {{yes}} || {{yes|Yes, &lt;br /&gt; Indexes PowerPoint 2010 attachments.}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Access]] || MDB || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Project]] || MPP || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{no}}
|-
| [[Microsoft Visio]] || VSD || ? || {{yes|Yes &lt;br /&gt; (multiple versions)}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Microsoft Outlook]] || Message, archives, and templates (.msg, .oft, .pst) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[MIME|MIME documents]] || Multipurpose Internet Mail Extension. || {{yes}} || {{yes|Yes, &lt;br /&gt; Microsoft Outlook Express Mac and PC (multiple versions) (.eml)}} || {{yes|.email, .eml, .ews, .mime &lt;br /&gt; MIME converter available with CES 7.0.5935+}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[StarOffice|Sun StarOffice]] || . || ? || ? || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || ?|| {{yes}} || {{no}}
|-
| Lotus 1-2-3 || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Lotus Freehand || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Corel WordPerfect]] || Corel WordPerfect Linux (.wps) &lt;br /&gt; Corel WordPerfect Macintosh (.wps) &lt;br /&gt; Corel WordPerfect Windows (.wo) &lt;br /&gt; Corel WordPerfect Windows (.wpd) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; version 6 and 7}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Archive files || . || ? || {{yes|7Z, DMG, HQX, BIZIP2, GZ, ISO, JAR, &lt;br /&gt; EMX, BIN, BKF, CAB, LZH/LHA &lt;br /&gt; ZIP, RAR, RTFD, TAR, Z, UUE &lt;br /&gt; multiple versions.}} || ? || ? || {{yes}} || {{yes|RAR, ZIP}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| RTF || Rich Text Format || {{yes}} || {{yes|Yes, &lt;br /&gt;multiple versions.}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Image files &lt;br /&gt; (text extraction) ||  || {{yes}} || ? || {{yes|Yes, &lt;br /&gt; .bmp, .jpeg, .max, .pcx/.dcx, &lt;br /&gt; .pdf, .png, .tiff, .tiff-fx &lt;br /&gt; requires the Optical Character Recognition (OCR) module.}} || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| {{yes}} || {{no}}
|-
| Images &lt;br /&gt; (metadata extraction) || Creation of thumbnail. || {{yes}} || {{yes}} || {{yes|.bmp, .emf, .exif, .gif, &lt;br /&gt; .icon, .jpeg, .png, .tiff, &lt;br /&gt; .wmf}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; JPEG, PNG, GIF, PNG}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Audio &lt;br /&gt; (text extraction) ||  || ? || ? || {{yes|.aif, .aifc, .aiff, .asf, .au, &lt;br /&gt; .cda .mid, .midi, .mp1, .mp3, &lt;br /&gt; .mpga, .rmi, .snd, .wav , .wma &lt;br /&gt; Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Audio (metadata) || Creation of thumbnail. || ? || ? || ? || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; MP3, OGG.}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Video ||  || ? || {{yes|.264, video/3gpp, .3mm, .4mp ... &lt;br /&gt; .avi, .m1v, .mov, .mp2, ...}} || {{yes|.avi, .m1v, .mov, .mp2, .mp2v, &lt;br /&gt; .mpa, .mpeg, .mpg, .mpv2 .qt, &lt;br /&gt; .rec, .rm, .rnx, .wm, .wmv &lt;br /&gt; Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? 
|Yes (metadata)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata extraction.}}
|-
| MacroMedia Flash || . || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; MacroMedia Flash text section and hypertext links}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| Autovue 2D &amp; 3D CAD files || . || ? || ? || ? || ?  || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; Open CAD files directly inside}} Autovue || ?|| {{yes}} || {{no}}
|-
| AutoCAD Drawing || AutoCAD Drawing (DWF), AutoCAD Drawing Exchange (DXF) || ? || {{yes|Yes, &lt;br /&gt; (multiple versions)}} || ? || ? || ? || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[CATIA]] versions 4 and 5 || Drawing document (CATDrawing), Part document (CATPart), &lt;br /&gt;  Assembly document (CATProduct), Model (V4 only) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; any 5 version}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| [[SolidWorks]] v1 || Drawing (.slddrw), Assembly (.sldasm), &lt;br /&gt; Part (.sldprt) || ? ||{{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; 2003 to 2013 releases}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Pro/ENGINEER]] || Part (.prt), Assembly (.asm) || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; any release from R16 to 2001, from WF1 to WF5}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| CAD Open formats || . || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; [[IGES]] version 5.2 and 5.3, [[STEP-File]]}} 
|Yes (metadata from the DWG CAD format)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata from DWG CAD}}
|-
|}

==== Text Analytics ====

{| class="wikitable sortable"
|-
! Linguistics !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance&lt;ref&gt;[http://www.bainsight.com/resources/Google-vs-Microsoft-FAST-Search-Whitepaper.pdf Microsoft and Google - BA Insight]&lt;/ref&gt;!! IBM Infosphere Data Explorer !! Lucidworks Fusion!! Perceptive Search !! Secure Enterprise Search&lt;ref&gt;[http://docs.oracle.com/cd/E14507_01/admin.1112/e14130.pdf Oracle Secure Enterprise Search Administrator's Guide]&lt;/ref&gt;!! RAVN Connect !! intergator !! Funnelback Search
|-
| Language detection || Ability to identify the languages at indexing time. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Synonyms/stemming  || Ability to treat as synonyms variations of keywords. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Entity extraction|Named Entity Extraction]] || Ability to automatically extract entities such as persons, locations and organizations from indexed content. || ? || {{yes|Yes  &lt;br /&gt; named "Eduction"}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes|Yes &lt;br/&gt;With automatic entity linking}}|| {{yes}} || {{yes}}
|-
| Stop words || Ability to exclude stop words (e.g. 'an', 'the') in order to improve relevance. || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Audio &amp; Video Analytics ====

{| class="wikitable sortable"
|-
! Multimedia !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Audio analytics || Ability to understand topics being discussed, genders and emotional tones of speech, music, etc. || ? || ? || ? || ? || ? || ? 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes|Yes &lt;br/&gt; speech, topics, music, keyword spotting }}|| ? 
|-
| Video analytics || Ability to understand the content of the video without relying on metadata (e.g. key framing, facial identification, logo recognition, etc.) || ? || {{yes}} || ? || ? || ? || ? 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| ? 
|-
| Image analytics || Ability to detect patterns in image (e.g. faces, bodies, gender, age range, expression, etc.) || ? || {{yes}} || ? || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
|}

=== Search Experience ===

This section compares the ability of the products to :
* Enable the user to enter and execute the query
* Present the data to the user within seconds after the query is parsed and processed so that the user can find what he seeks quickly and act on it.

==== Search Language ====

{| class="wikitable sortable"
|-
! Query Parser !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/User/search_prefix_and_operators.htm Search Prefixes and Operators]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect&lt;ref&gt;[http://www.ravn.co.uk/wp-content/uploads/2014/02/CORE-Whitepaper-W.V.pdf RAVN Connect, CORE White paper]&lt;/ref&gt;!! intergator !! Funnelback Search &lt;ref&gt;https://docs.funnelback.com/query_language_help.html&lt;/ref&gt;
|-
| Wildcard search || Does the system use the asterisk ("*") and question mark ("?") character as a wildcard? || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Fuzzy search || Does the system offer phonetic and approximate spelling search? (distinctions of syntax and semantics) || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Exact phrase search || Does the system enable to find words as a phrase? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Proximity search (text)|Proximity search]] || Support for advanced proximity operators - NEAR, BEFORE, AFTER. || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Range search || Ability to match all terms which are lexically between square brackets ("[]") and curly braces ("{}"). || ? || ? || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Boosting a term || Automatic bigram and trigram relevancy boosting. || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Boolean search || Does the system interprets Boolean operators? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Graph search || Does the system keep relationships between fields and allows searching for them (while enabling full-text search)? || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
|}

==== Usability ====

===== Search Query =====

This is the process of searching (querying).

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://www.coveo.com/en/news-releases/Coveo-Reports-Accelerated-Demand-for-Search-and-Relevance-Technology Coveo Reports Accelerated Demand for Search &amp; Relevance Technology in 2013]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Auto-complete || Does the system provide an automatic query guidance in the search box while typing? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}&lt;ref&gt;https://docs.funnelback.com/auto_completion_collection_cfg.html&lt;/ref&gt;
|-
| Spell-checking || Does the system checks if the words in the query are spelled correctly and suggest corrections? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}} &lt;ref&gt;https://docs.funnelback.com/spelling_suggestions.html&lt;/ref&gt;
|-
| [[Federated search]] || The ability to send the same query simultaneously to several searchable sources. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}&lt;ref&gt;https://docs.funnelback.com/ui_modern_extra_searches_collection_cfg.html&lt;/ref&gt;
|-
| Advanced search page || Does the system allow users to perform complex and sophisticated queries? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}
|-
|}

===== Result-list =====

This is the process of scanning the content of any document directly from the result lists.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL&lt;ref&gt;[http://www.ndm.net/archiving/pdf/20130902_PI_B_HP_AUTN_IDOL10_web.pdf Autonomy KeyView IDOL - Product Brief - Ndm.net]&lt;/ref&gt;!! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance&lt;ref&gt;[http://static.googleusercontent.com/media/www.google.com/fr//support/enterprise/static/gsa/docs/admin/70/gsa_doc_set/quick_start/quick_start.pdf Getting the Most from Your Google Search Appliance]&lt;/ref&gt;!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator
|-
| Relevance ranking || Ability to find the highest quality and most relevant documents and bring them to the top of a search results list. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes|Yes &lt;br /&gt; Google Site Search factors in more than 100 variables &lt;br /&gt; for each query}} || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}
|-
| Find similar || Ability to find similar links. || {{yes}} || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}
|-
| Hit highlighting || Ability to highlight query key terms within the document in search result. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| Summarization &lt;br /&gt; (view as HTML) || Does the system offer content preview in the search result, so that users can judge relevance of results? || {{yes}} || {{yes}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| {{partial|*only available for Office file types (Powerpoint, Excel, Visio, etc.)}} || {{partial}} || {{yes|Yes &lt;br /&gt; converts over 220 file formats into HTML}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
|  || Does the system enable to copy/paste from within the preview? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Thumbnails and preview || Ability to generate thumbnails for a large amount of different file types. || ? || {{yes|Yes &lt;br /&gt; 100+ doc types}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but low resolution for CATProducts &lt;br /&gt; no thumbnails for Pro/E assemblies}} 
|Yes|| {{partial|*only available for Office file types (.[[DOCX|DOCx]], .[[PPTX|PPTx]]) &lt;br /&gt; First page thumbnail preview}} || {{partial}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Full document graphical preview || Ability to access the content of any document without having to open a windows client application. || ? || ? || ? || ? || {{yes}} || {{yes|Yes &lt;br /&gt; regardless of the file's original application}} 
|Yes|| {{partial|*only available for PowerPoint file types}} || {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes &lt;br/&gt; Asynchronous loading}}|| {{yes}}
|-
| Document comparison || Ability to compare. || ? ||{{yes|Yes &lt;br /&gt; for version management, signature identification, among other features}} || ? || ? || {{yes}} || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Clustering || Ability to dynamically organize search results into groups. || ? || {{yes}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes|Yes &lt;br /&gt; group search results by topic}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Sort by fields || Ability to sort all results by order of date or other attribute. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; &#8222;hard sort&#8223;}} || {{yes}}|| {{yes}}
|-
|}

==== Faceted Navigation ====

This is the process of browsing the content by narrowing search results quickly in clicking filters that refine results based on related categories, so that users extract more meaning and insight from the content.

{| class="wikitable sortable"
|-
! [[Faceted Search|Facets]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/user/about_facets.htm Coveo Facets]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Multiple filters || Does the system enable the user to filter results in selecting multiple facet values? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}
|-
| Facet values and counts || Ability to display the term and the number of documents containing that term in the search results. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[Faceted classification|Hierarchical]]- and range facets || . || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| Date, number and string types || Ability to filter by date/time, number and string data types. || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
|}

==== Social and collaborative ====

This is the process of asking social network.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010&lt;ref&gt;[http://www.hcsolutions.at/produkte/ontolica/Documents/Ontolica_2010_feature_matrix.pdf Search Solution for Microsoft SharePoint]&lt;/ref&gt;!! SharePoint 2013&lt;ref&gt;[http://www.slideshare.net/SurfRay/new-sharepoint-server-2013-search-features Introduction to SharePoint 2013 Search]&lt;/ref&gt;!! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search&lt;ref&gt;[http://www.perceptivesoftware.com/images/psi_ds_perceptiveenterprisesearch.pdf Perceptive Enterprise Search - Product Datasheet]&lt;/ref&gt;!! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Search result tagging || Ability to improve relevancy by creating or adding existing tags. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|No|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes|Yes&lt;br/&gt;Real time}}|| {{yes}} || {{no|No. Deprecated in v14.0.1}}
|-
| Tag searching || Ability to search for a tag. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Personalization/ Audience targeting || Ability to deliver more accurate targeted results. || ? || {{yes| Yes, &lt;br /&gt; browse histories, content contributors, and interactions, etc}}  || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || {{yes| Yes, &lt;br /&gt; source, date, metadata and entities biaising }} || ? || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Expertise location || Ability to find experts in users organization by searching on related keywords. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Saved search || Ability to save searches. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|No|| {{no}} || {{no}} || ? || {{yes}} || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Saved alerts || Ability to save alerts in order to notified when new content matching your queries has been added to the system. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || {{no}} || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Saved RSS feeds || Ability to save RSS feeds. || ? || ? || {{yes}} || ? || {{yes}} || ? 
|No|| ? || {{no}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Mobile support ====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari
!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Mobile search || Does the system support mobile device access and search? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| Mobile UI || Does the system detect the user device (desktop, smartphone, tablet, etc.) and adapt itself based on it?  || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Admin UI and Default Public UI use responsive designs.}}
|-
| Geolocation || Does the system enable from the end-user&#8217;s geolocation to provide additional context to filter? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? || {{yes}}
|-
| Compatibility || Is the system compatible with iOS. || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}
|-
|  || Is the system compatible with Android? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}
|-
|  || Is the system compatible with Windows Phone? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || ?
|-
|}

=== Administration &amp; Architecture ===

This section compares the flexibility in the underlying architecture, application development, the scalability and the administrative services of the products.

==== Management &amp; Search analytics ====

This table is about the ability to report on usage and activity (most popular queries, documents not found, etc.)

{| class="wikitable sortable"
|-
! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search&lt;ref&gt;[http://docs.oracle.com/cd/E35643_01/Workbench.211/pdf/WorkbenchUserGuide.pdf Endeca&#174; Workbench: User's Guide - Oracle Documentation]&lt;/ref&gt;!! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Web based administration interfaces &lt;br /&gt; (HTML) || . || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Search statistics || Does the system collect search statistics? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Search reports || Does the system enable to report search statistics? || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; most popular queries}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Portal usage reports || Does the system enable to report on portal usage? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; popular navigation}} || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Document storage reports || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{no}} || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; for documents not found}} || ?|| {{yes}} || {{yes}}
|-
| Custom reports || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|Yes|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Click Scoring || Ability to improve relevancy by enabling to track which results are most often clicked. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Interface flexibility ====

This is about tools to customize the interface,so that it adds value to any industry or business process.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator !! Funnelback Search
|-
| Standard based open interface || Does the system support all client platforms? || ? || {{yes|Yes, &lt;br /&gt; HTTP and XML/JSON}} || ? || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; XML, JSON and HTTP}} || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes.  HTML, JSON, XML, RSS and OpenSearch.}}
|-
| Page Layout Helper || Ability to change easily to global attributes (logo, fonts, header, and footer) and to the look of the Search Box and Search Results. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| {{no|No, action menu on search results not configurable}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Stylesheet editor || Ability to make more extensive changes using [[XSLT]] stylesheet. || ? || ? || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; Full customization with CSS or Java API &amp; XML}}  || {{yes}}
|Yes|| {{yes}} || ? || {{yes|Yes &lt;br /&gt;XSLT stylesheet editor}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes|Yes. Full results customisation via [[FreeMarker]]&lt;ref&gt;https://docs.funnelback.com/freemarker.html&lt;/ref&gt; and CSS/JS. Cached copies of XML templated via XSLT&lt;ref&gt;https://docs.funnelback.com/xslt_processing.html&lt;/ref&gt;}}
|-
|}

==== Scalability ====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/coveo_scalability_model.htm Scalability]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator
|-
| Index capacity || How many total documents can be indexed into the system? || up to 10 Million || ? || ? || ? || ? || ? 
|?|| Up to 100 Million || Up to 100 Million || ? || ? || ? || ? || ? || Scalable to billions|| Scalable to billions
|-
| Indexing rate || How rapidly documents can be added or reprocessed into the index? || real time || ? || ? || ? || ? || ? 
|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || ? || per requirement, scalable || ?
|-
| Query-processing speed || How many queries per second ([[Queries per second|QPS]]) the engine can process? || ? || 2,000 &lt;br /&gt; across all indexed data with sub-second response times || ? || ? || ? || ? 
|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || scalable || || Depands on the datasource. (sub)-second times 
|-
|}

==== Platform readiness ====

{| class="wikitable sortable"
|-
! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| General system requirement || Minimum of available disk space. || 100 MB || ? || ? || ? || ? || 25 GB 
|?|| ? || ? || ? || ? || ? || ? || ? || 10 GB || 25 GB
|-
|  || Minimum RAM. || 8GB || ? || ? || ? || ? || 8 GB 
|?|| ? || ? || ? || ? || ? || ? || ? || 8 GB || 4 GB
|-
|  || Hard drives to store the data files. || ? || ? || ? || ? || ? || SCSI, SAS, SAN, over FC or SSD disks &lt;br /&gt; (as opposed to SATA disks) 
|?|| ? || ? || ? || ? || ? || ? || ? || optimised for throughput&lt;br/&gt; SCSI, SAS, SAN || SCSI, SAS, SAN, SSD, HDD
|-
|}

=== Vendor Intangibles ===

This section compares each software investment.

{| class="wikitable sortable"
|-
! Investment !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect || intergator
|-
| Hardware costs || Total costs of servers. || 0 || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ? || Depands on the base licence
|-
| Installation costs || . || ? ||? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| License costs || . || ? ||? || ? || ? || ? || ? 
|0|| ? || ? || 500,000 Documents &#8211; $28,387 &lt;br /&gt; 1 million documents &#8211; $66,236 &lt;br /&gt; Upgrade from 1 million to 2 million documents &#8211; $1,971 &lt;br /&gt; 2 Million documents &#8211; $123,010 &lt;br /&gt; 2 Million documents &#8211; $123,010 &lt;br /&gt; 3 Million documents &#8211; $113, 548 &lt;br /&gt; 3 Million documents &#8211; $158,967 &lt;br /&gt; 5 Million documents &#8211; $433,766 &lt;br /&gt; 10 Million documents &#8211; $305,066 &lt;br /&gt; 10 Million &#8211; $423,913 &lt;br /&gt; 15 Million documents &#8211; $533,896 &lt;br /&gt; 15 Million documents with hot backup &#8211; $615,053 &lt;br /&gt; 30 Million documents with hot backup &#8211; $993,548 || ? || ? || ? || ? || ?|| ?
|-
| Annual Maintenance || Annual Maintenance fees. || ? || ? || ? || ? || ? || ? 
|Per server|| ? || ? || ? || ? || ? || ? || ?|| ?|| ?
|-
|}

== References ==

{{reflist}}

== First version ==

[[Category:Information retrieval systems]]</text>
      <sha1>nh4dm2u5yxeajo2zl139f5mcmeiyv0f</sha1>
    </revision>
  </page>
  <page>
    <title>Trip (search engine)</title>
    <ns>0</ns>
    <id>14069461</id>
    <revision>
      <id>729838341</id>
      <parentid>666716945</parentid>
      <timestamp>2016-07-14T22:11:08Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>/* Users */USA is deprecated, per [[MOS:NOTUSA]], and correct [[MOS:OVERLINK|overlinking]] of common places using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3853" xml:space="preserve">{{Infobox software
|name = Trip
|logo =
|screenshot =
|caption =
|developer = Trip Database Ltd
|latest_release_version =
|latest_release_date =
|latest_preview_version =
|latest_preview_date =
|operating_system =
|genre = [[Search engine]]
|language =
|license = [[Freeware]]
|website = [http://www.tripdatabase.com Trip]
}}

'''Trip''' is a free [[Illness|clinical]] [[search engine]].  Its primary function is to help [[clinician]]s identify the best available evidence with which to answer clinical questions.  Its roots are firmly in the world of [[evidence-based medicine]].

==History==
The site was created in 1997 as a search tool to help the staff of ATTRACT&lt;ref&gt;[http://www.attract.wales.nhs.uk ATTRACT]&lt;/ref&gt; answer clinical questions for [[General practitioner|GP]]s in [[Gwent (county)|Gwent]], [[South Wales]].  Shortly afterwards ''[[Bandolier (journal)|Bandolier]]'' highlighted the Trip Database and this helped establish the site.  In 2003, after a period of steady growth, Trip became a subscription-only service.  This was abandoned In September 2006 and since then the growth in usage has been significant. Originally "Trip" stood for Turning Research Into Practice, but the system is now simply called Trip.&lt;ref&gt;{{cite web |url= http://www.tripdatabase.com/about |title=About |work=Trip |publisher=Trip Database Ltd |accessdate=3 April 2013}}&lt;/ref&gt;

==Process==
The core to Trip&#8217;s system is the identification and incorporation of new evidence.  The people behind Trip are heavily involved in clinical question answering systems (e.g., [[NLH Q&amp;A Service]]).  Therefore, if resources are identified that are useful in the Q&amp;A process they tend to be added to Trip.

==Users==
A site survey (September 2007) showed that the site was searched over 500,000 times per month, with 69% from [[health professional]]s and 31% from members of the public. Of the health professionals around 43% are doctors.  Most users come from either the United Kingdom or the United States.  In September 2008 the site was searched 1.4 million times. To date the site has been searched over 100 millions times.

==Recent updates==
At the end of 2012 Trip had a major upgrade which saw significant new enhancements:

* New content - widening the coverage
* New design
* Advanced search
* PICO search - to help users formulate focused searches
* Improved filtering
* Search history/timeline - recording all a user activity on the site
* Related articles

==Education tracker==
Trip has an education tracker which allows users to record their activity on Trip which can then be used, subject to local regulations, for revalidation/re-licensing.

==Future areas of work==
Trip is exploring numerous innovative technologies to improve the site, these include:
* Link out to full-text articles via Trip.
* RCT database.
* Rapid (within a week) systematic review quality reviews.
* Learning from users prior use of the site and that of similar users to improve search results.

==Trip Answers==
In November 2008, Trip released a new website, Trip Answers.  This is a repository of clinical Q&amp;As from a variety of Q&amp;A services. At launch it had over 5,000 Q&amp;As and currently has over 6,300.  This content has been integrated into Trip.

==References==
{{reflist}}

==External links==
* [http://www.tripdatabase.com Trip]
* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1852632/ Using the Turning Research Into Practice (TRIP) database: how do clinicians really search?] an evaluation of the website.
* [http://libguides.lhl.uab.edu/content.php?pid=108596&amp;sid=1099056 Reviews: From Systematic to Narrative] review of the site
* [http://guides.library.manoa.hawaii.edu/content.php?pid=250484&amp;sid=2157277 Evidence Based Pyramid] a pictorial representation of TRIP's approach to the evidence

[[Category:Medical websites]]
[[Category:Information retrieval systems]]</text>
      <sha1>j5dyq9hft2degfoqu79xgr9alxmyjct</sha1>
    </revision>
  </page>
  <page>
    <title>Collaborative search engine</title>
    <ns>0</ns>
    <id>22101925</id>
    <revision>
      <id>733812882</id>
      <parentid>725540619</parentid>
      <timestamp>2016-08-10T08:04:20Z</timestamp>
      <contributor>
        <username>Klamann</username>
        <id>10223845</id>
      </contributor>
      <comment>/* Community of practice */ jumper 2.0 has been renamed to apexkb</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14749" xml:space="preserve">{{Recommender systems}}
'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].

== Models of collaboration ==

Collaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization
&lt;ref name=Golo2007&gt;{{citation
 | title = Collaborative Exploratory Search
 | year = 2007
 | author = Golovchinsky Gene
 | author2 = Pickens Jeremy
 | journal = Proceedings of HCIR 2007 workshop
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf
}}&lt;/ref&gt; and depth of mediation 
,&lt;ref name=Pickens2008&gt;{{citation
 | title = Collaborative Exploratory Search
 | year = 2008
 | author = Pickens Jeremy
 | author2 = Golovchinsky Gene
 | author3 = Shah Chirag
 | author4 = Qvarfordt Pernilla
 | author5 = Back Maribeth
 | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval
 | pages = 315&#8211;322
 | volume = 
 | issue = 
 | doi = 10.1145/1390334.1390389
 | isbn = 
 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389
| chapter = Algorithmic mediation for collaborative exploratory search
 }}&lt;/ref&gt; task vs. trait,&lt;ref name=Morris2008&gt;{{citation
 | contribution = Understanding Groups&#8217; Properties as a Means of Improving Collaborative Search Systems
 | year = 2008
 | author = Morris Meredith
 | author2 = Teevan Jaime
 | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf
}}&lt;/ref&gt; and division of labor and sharing of knowledge.&lt;ref name=Foley2008&gt;{{citation
 | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval
 | year = 2008
 | author = Foley Colum
 | booktitle = PhD Thesis, Dublin City University
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf
}}&lt;/ref&gt;

=== Explicit vs. implicit collaboration ===

Implicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,&lt;ref name=Smith2003&gt;{{citation
 | title = Collaborative Web Search
 | year = 2003
 | author = Barry Smyth
 | author2 = Evelyn Balfe
 | author3 = Peter Briggs
 | author4 = Maurice Coyle
 | author5 = Jill Freyne
 | journal = IJCAI
 | pages = 1417&#8211;1419
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,&lt;ref name=Glance2001&gt;{{citation
 | title = Community search assistant
 | year = 2001
 | author = Natalie S. Glance
 | journal = Workshop on AI for Web Search AAAI'02
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; the CSE of Burghardt et al.,&lt;ref name=BurghardtWI2008&gt;{{citation
 | title = Discovering the Scope of Privacy Needs in Collaborative Search
 | year = 2008
 | author = Thorben Burghardt
 | author2 = Erik Buchmann
 | author3 = Klemens B&#246;hm
 | journal = Web Intelligence (WI)
 | pages = 
 910| volume = 
 | issue = 
 | doi = 10.1109/WIIAT.2008.165
 | isbn = 
 978-0-7695-3496-1}}&lt;/ref&gt; and the works of Longo et al.
&lt;ref name=Longo2009a&gt;{{citation
 | title = Toward Social Search - From Explicit to Implicit Collaboration to Predict Users' Interests
 | year = 2009
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = ''[[Webist]]'' 2009 - Proceedings of the Fifth International Conference                on Web Information Systems and Technologies, Lisbon, Portugal,                March 23&#8211;26, 2009
 | pages = 693&#8211;696
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-989-8111-81-4
 | url = 
}}&lt;/ref&gt; 
&lt;ref name=Longo2010&gt;{{citation
 | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time
 | year = 2010
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = Transaction Computational Collective Intelligence II
 | pages = 46&#8211;69
 | volume = 2
 | issue = 
 | doi = 10.1007/978-3-642-17155-0_3
 | isbn = 
 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/
| series = Lecture Notes in Computer Science
 }}&lt;/ref&gt; 
&lt;ref name=Longo2009b&gt;{{citation
 | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search
 | year = 2009
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = Computational Collective Intelligence. Semantic Web, Social                Networks and Multiagent Systems, First International Conference,                ICCCI 2009, Wroclaw, Poland, October 5&#8211;7, 2009. Proceedings
 | pages = 63&#8211;74
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-3-642-04440-3
 | url = http://dl.acm.org/citation.cfm?id=1692026
}}&lt;/ref&gt; 
all represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.

Explicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether&lt;ref name=Morris2007&gt;{{citation
 | title = SearchTogether: An Interface for Collaborative Web Search
 | year = 2007
 | author = Meredith Ringel Morris
 | author2 = Eric Horvitz
 | journal = UIST
| url = http://portal.acm.org/citation.cfm?id=1294211.1294215
}}&lt;/ref&gt; published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.&lt;ref name=Redy2008&gt;{{citation
 | title = The Role of Communication in Collaborative Information Searching
 | year = 2008
 | author = Madhu C. Reddy
 | author2 = Bernhard J. Jansen
 | author3 = Rashmi Krishnappa
 | journal = ASTIS
}}&lt;/ref&gt; (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,&lt;ref name="Smith2003"/&gt; the Community Search Assistant,&lt;ref name="Glance2001"/&gt; and the CSE of Burghardt et al.&lt;ref name="BurghardtWI2008" /&gt; Cerciamo &lt;ref name=Pickens2008 /&gt; supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.

However, in Papagelis et al.&lt;ref name=Papagelis2007&gt;{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis| author2 = Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88&#8211;98| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}&lt;/ref&gt; terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.

=== Community of practice  ===

Recent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.&lt;ref name=Rohini&amp;Ambati&gt;{{citation
 | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries
 | year = 2002
 | author = Rohini U
 | author2 = Vamshi Ambati
 | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}&lt;/ref&gt;

Collaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.&lt;ref name=Coyle2008&gt;{{citation
 | title = Social Aspects of a Collaborative, Community-Based Search Network
 | editor4-first = Eelco
 | editor3-first = Pearl
 | editor2-first = Judy
 | editor1-first = Wolfgang
 | year = 2008
 | editor1-last = Nejdl
 | author = Maurice Coyle
 | author2 = Barry Smyth
 | last-author-amp = yes
 | journal = Adaptive Hypermedia and Adaptive Web-Based Systems
 | pages =  103&#8211;112  
 | volume = 5149/2008
 | issue = 
 | series = | doi = 10.1007/978-3-540-70987-9
 | isbn = 978-3-540-70984-8
 | url = http://portal.acm.org/citation.cfm?id=1485050
 | editor2-last = Kay
 | editor4-last = Herder
 | editor3-last = Pu| display-editors = 3}}&lt;/ref&gt; The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project [[ApexKB]] (previously known as Jumper 2.0).&lt;ref name=Jumper2010&gt;{{citation
 | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features
 | year = 2010
 | author = Jumper Networks Inc.
 | journal = Press release
 | pages = 
 | volume =
 | issue = 
 | doi =
 | isbn =
 | url = http://www.trilexnet.com/labs/jumper}}&lt;/ref&gt;

=== Depth of mediation ===

This refers to the degree that the CSE mediates search.&lt;ref name=Pickens2008 /&gt; SearchTogether&lt;ref name=Morris2007 /&gt; is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo&lt;ref name=Pickens2008 /&gt; and recommendation systems such as I-Spy&lt;ref name=Smith2003 /&gt; keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.

=== Task vs. trait ===

This model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.&lt;ref name=Morris2008 /&gt;

== Privacy-aware collaborative search engines ==

Search terms and links clicked that are shared among users reveal their interests, habits, social
relations and intentions.&lt;ref name=EUArticle29&gt;{{citation
 | title = Article 29 EU Data Protection Working Party
 | year = 2008
 | author = Data Protection Working Party
 | journal = EU
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. 
&lt;ref name="Morris2007"/&gt;&lt;ref name=Smith2005&gt;{{citation
 | title = A Live-User Evaluation of Collaborative Web Search
 | year = 2005
 | author = Barry Smyth
 | author2 = Evelyn Balfe
 | author3 = Oisin Boydell
 | author4 = Keith Bradley
 | author5 = Peter Briggs
 | author6 = Maurice Coyle
 | author7 = Jill Freyne
 | journal = IJCAI
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt;
&lt;ref name=Smith2006&gt;{{citation
 | title = Anonymous personalization in collaborative web search
 | year = 2005
 | author = Smyth, Barry
 | author2 = Balfe, Evelyn
 | last-author-amp = yes
 | journal = Inf. Retr.
 | pages = 165&#8211;190
 | volume = 9
 | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = 
 | url = 
}}&lt;/ref&gt;
&lt;ref name=Jung2004&gt;{{citation
 | title = Applying Collaborative Filtering for Efficient Document Search
 | year = 2004
 | author = Seikyung Jung
 | author2 = Juntae Kim
 | author3 = Herlocker, JL
 | journal = Inf. Retr.
 | pages = 640&#8211;643
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.

As CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.&lt;ref name=BurghardtCC2008&gt;{{citation
 | title = Collaborative Search And User Privacy: How Can They Be Reconciled?
 | year = 2008
 | author = Thorben Burghardt
 | author2 = Erik Buchmann
 | author3 = Klemens B&#246;hm
 | author4 = Chris Clifton
 | journal = CollaborateCom
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://dbis.ipd.uni-karlsruhe.de/1184.php
}}&lt;/ref&gt; They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.

== References ==
{{reflist|2}}
{{Internet search}}

[[Category:Information retrieval systems]]</text>
      <sha1>agj9vnwxx8zguckzd8ak74sc4hhysja</sha1>
    </revision>
  </page>
  <page>
    <title>S&#248;rensen&#8211;Dice coefficient</title>
    <ns>0</ns>
    <id>9701718</id>
    <revision>
      <id>753370759</id>
      <parentid>753370690</parentid>
      <timestamp>2016-12-06T19:41:17Z</timestamp>
      <contributor>
        <username>Nnescio</username>
        <id>12634958</id>
      </contributor>
      <comment>/* Formula */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8611" xml:space="preserve">The '''S&#248;rensen&#8211;Dice index''', also known by other names (see [[S&#248;rensen&#8211;Dice_coefficient#Name|Name]], below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald S&#248;rensen]]&lt;ref&gt;{{cite journal |last=S&#248;rensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1&#8211;34 |doi= }}&lt;/ref&gt; and [[Lee Raymond Dice]],&lt;ref&gt;{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297&#8211;302 |doi=10.2307/1932409 }}&lt;/ref&gt; who published in 1948 and 1945 respectively.
The S&#248;rensen&#8211;Dice is also known as [[F1 score]] or Dice similarity coefficient (DSC).

==Name==
The index is known by several other names, usually '''S&#248;rensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for S&#248;rensen are Sorenson, Soerenson index and S&#246;renson index, and all three can also be seen with the &#8211;sen ending.

Other names include:
*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index&lt;ref name ="gallagher"/&gt;

==Formula==
S&#248;rensen's original formula was intended to be applied to presence/absence data, and is

:&lt;math&gt; QS =  \frac{2 |X \cap Y|}{|X|+ |Y|}&lt;/math&gt;

where |''X''| and |''Y''| are the numbers of elements in the two samples. Based on what is written here,

:&lt;math&gt; DSC = \frac{2 TP}{2 TP + FP + FN}&lt;/math&gt;,

as compared with the Jaccard index, which omits true negatives from both the numerator and the denominator. QS is the quotient of similarity and ranges between 0 and&amp;nbsp;1.&lt;ref&gt;http://www.sekj.org/PDF/anbf40/anbf40-415.pdf&lt;/ref&gt; It can be viewed as a similarity measure over sets.

Similarly to the [[Jaccard index]], the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':

:&lt;math&gt;s_v = \frac{2 | A \cdot B |}{| A |^2 + | B |^2} &lt;/math&gt;

which gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.

For sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :&lt;ref&gt;{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979
|title=Information Retrieval
|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}&lt;/ref&gt;

When taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:&lt;ref&gt;{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003
|title=Cognates Can Improve Statistical Translation Models
|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
|pages=46&#8211;48 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}&lt;/ref&gt;

:&lt;math&gt;s = \frac{2 n_t}{n_x + n_y}&lt;/math&gt;

where ''n''&lt;sub&gt;''t''&lt;/sub&gt; is the number of character bigrams found in both strings, ''n''&lt;sub&gt;''x''&lt;/sub&gt; is the number of bigrams in string ''x'' and ''n''&lt;sub&gt;''y''&lt;/sub&gt; is the number of bigrams in string ''y''. For example, to calculate the similarity between:

:&lt;code&gt;night&lt;/code&gt;
:&lt;code&gt;nacht&lt;/code&gt;

We would find the set of bigrams in each word:
:{&lt;code&gt;ni&lt;/code&gt;,&lt;code&gt;ig&lt;/code&gt;,&lt;code&gt;gh&lt;/code&gt;,&lt;code&gt;ht&lt;/code&gt;}
:{&lt;code&gt;na&lt;/code&gt;,&lt;code&gt;ac&lt;/code&gt;,&lt;code&gt;ch&lt;/code&gt;,&lt;code&gt;ht&lt;/code&gt;}

Each set has four elements, and the intersection of these two sets has only one element: &lt;code&gt;ht&lt;/code&gt;.

Inserting these numbers into the formula, we calculate, ''s''&amp;nbsp;=&amp;nbsp;(2&amp;nbsp;&#183;&amp;nbsp;1)&amp;nbsp;/&amp;nbsp;(4&amp;nbsp;+&amp;nbsp;4)&amp;nbsp;=&amp;nbsp;0.25.

==Difference from Jaccard ==
This coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.&lt;ref name ="gallagher"/&gt;

The function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function

:&lt;math&gt;d = 1 -  \frac{2 | X \cap Y |}{| X | + | Y |} &lt;/math&gt;

is not a proper distance metric as it does not possess the property of [[triangle inequality]].&lt;ref name ="gallagher"&gt;Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&amp;rep=rep1&amp;type=pdf COMPAH Documentation], University of Massachusetts, Boston&lt;/ref&gt; The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.

==Applications==
The S&#248;rensen&#8211;Dice coefficient is useful for ecological community data (e.g. Looman &amp; Campbell, 1960&lt;ref&gt;[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409&#8211;416.]&lt;/ref&gt;). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s&lt;ref&gt;[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123&#8211;131.]&lt;/ref&gt;). As compared to [[Euclidean distance]], S&#248;rensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.&lt;ref&gt;McCune, Bruce &amp; Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.&lt;/ref&gt; Recently the Dice score (and its variations, e.g. logDice taking a logarithm of it) has become popular in computer [[lexicography]] for measuring the lexical association score of two given words.&lt;ref&gt;[http://nlp.fi.muni.cz/raslan/2008/raslan08.pdf#page=14 Rychl&#253;, P. (2008) A lexicographer-friendly association score. Proceedings of the Second Workshop on Recent Advances in Slavonic Natural Language Processing RASLAN 2008: 6&#8211;9]&lt;/ref&gt; It is also commonly used in [[Image segmentation]], in particular for comparing algorithm output against reference masks in medical applications{{Citation needed|reason=Some seminal works need to be cited to show how Dice coefficient is used|date=December 2016}}.

==Abundance version==
The expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:
* Quantitative S&#248;rensen&#8211;Dice index&lt;ref name ="gallagher"/&gt;
* Quantitative S&#248;rensen index&lt;ref name ="gallagher"/&gt;
* Quantitative Dice index&lt;ref name ="gallagher"/&gt;
* [[Bray&#8211;Curtis dissimilarity|Bray&#8211;Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')&lt;ref name ="gallagher"/&gt;
* [[Jan Czekanowski|Czekanowski]]'s quantitative index&lt;ref name ="gallagher"/&gt;
* Steinhaus index&lt;ref name ="gallagher"/&gt;
* [[E. C. Pielou|Pielou]]'s percentage similarity&lt;ref name ="gallagher"/&gt;
* 1 minus the [[Hellinger distance]]&lt;ref&gt;{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1957 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326&#8211;349 |doi=10.2307/1942268 }}&lt;/ref&gt;

==See also==
* [[Correlation]]
* [[Jaccard index]]
* [[Hamming distance]]
* [[Mantel test]]
* [[Morisita's overlap index]]
* [[Most frequent k characters]]
* [[Overlap coefficient]]
* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])
* [[Tversky index]]
* [[Universal adaptive strategy theory (UAST)]]

==References==
{{reflist}}

==External links==
{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}

{{DEFAULTSORT:Sorensen-Dice coefficient}}
[[Category:Information retrieval evaluation]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>3pwfykke2qfsyvf38zrfx4m8vbiqu5q</sha1>
    </revision>
  </page>
  <page>
    <title>Overlap coefficient</title>
    <ns>0</ns>
    <id>22049756</id>
    <revision>
      <id>725310648</id>
      <parentid>721860843</parentid>
      <timestamp>2016-06-14T21:54:34Z</timestamp>
      <contributor>
        <username>GermanJoe</username>
        <id>12935443</id>
      </contributor>
      <comment>+ tag, ref section, - SPA spam</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="721" xml:space="preserve">{{unreferenced|date=June 2016}}
The '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the smaller of the size of the two sets:

:&lt;math&gt;\mathrm{overlap}(X,Y) = \frac{| X \cap Y | }{\min(|X|,|Y|)}&lt;/math&gt;

If set ''X'' is a [[subset]] of ''Y'' or the converse then the overlap coefficient is equal to one.

==References==
&lt;references /&gt;

[[Category:Information retrieval techniques]]
[[Category:Information retrieval evaluation]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>5v4i27p6yzl6rl4pa7y4ghgm6rel1s8</sha1>
    </revision>
  </page>
  <page>
    <title>Discounted cumulative gain</title>
    <ns>0</ns>
    <id>19542049</id>
    <revision>
      <id>753666505</id>
      <parentid>753665876</parentid>
      <timestamp>2016-12-08T15:08:55Z</timestamp>
      <contributor>
        <username>Antoine-sac</username>
        <id>29832403</id>
      </contributor>
      <comment>/* Example */ Removed unnecessary (thus confusing) emphasis on rel_1</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10866" xml:space="preserve">'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.&lt;ref&gt;Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422&#8211;446 (2002)&lt;/ref&gt;

== Overview ==

Two assumptions are made in using DCG and its related measures.

# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)
# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.

DCG originates from an earlier, more primitive, measure called Cumulative Gain.

=== Cumulative Gain ===

Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{CG_{p}} = \sum_{i=1}^{p} rel_{i} &lt;/math&gt;

Where &lt;math&gt;rel_{i}&lt;/math&gt; is the graded relevance of the result at position &lt;math&gt;i&lt;/math&gt;.

The value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document &lt;math&gt;d_{i}&lt;/math&gt; above a higher ranked, less relevant, document &lt;math&gt;d_{j}&lt;/math&gt; does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.

=== Discounted Cumulative Gain ===

The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:&lt;ref name="stanfordireval"&gt;{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}&lt;/ref&gt;

:&lt;math&gt; \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{rel_{i}}{\log_{2}(i+1)} = rel_1 + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}(i+1)} &lt;/math&gt;

Previously there has not been  any theoretically sound justification for using a [[logarithm]]ic reduction factor&lt;ref name=CMS2009&gt;{{cite book | title=Search Engines: Information Retrieval in Practice |author1=B. Croft |author2=D. Metzler |author3=T. Strohman |year=2010 | publisher=''Addison Wesley"}}&lt;/ref&gt; other than the fact that it produces a smooth reduction. But Wang et al. (2013)&lt;ref&gt;Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of Normalized Discounted Cumulative Gain (NDCG) Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).&lt;/ref&gt; give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner.

An alternative formulation of DCG&lt;ref&gt;Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363&lt;/ref&gt; places stronger emphasis on retrieving relevant documents:

:&lt;math&gt; \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} &lt;/math&gt;

The latter formula is commonly used in industry including major web search companies&lt;ref name="stanfordireval"/&gt; and data science competition platform such as Kaggle.&lt;ref&gt;{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}&lt;/ref&gt;

These two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]];&lt;ref name=CMS2009/&gt;{{rp|320}} &lt;math&gt;rel_{i} \in \{0,1\}&lt;/math&gt;.

Note that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.&lt;!-- Not very clear, does it affect or no the value of DCG? --&gt;

=== Normalized DCG ===

Search result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of &lt;math&gt;p&lt;/math&gt; should be normalized across queries. This is done by sorting all '''relevant''' documents in the corpus by their relative relevance, producing the maximum possible DCG through position &lt;math&gt;p&lt;/math&gt;, also called Ideal DCG (IDCG) through that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG_{p}} &lt;/math&gt;,

where:

:&lt;math&gt; \mathrm{IDCG_{p}} = \sum_{i=1}^{|REL|} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} &lt;/math&gt;

and |REL| represents the list of relevant documents (ordered by their relevance) in the corpus up to position p.

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.

== Example ==

Presented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning not relevant, 3 meaning highly relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as

:&lt;math&gt; D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} &lt;/math&gt;

the user provides the following relevance scores:

:&lt;math&gt; 3, 2, 3, 0, 1, 2 &lt;/math&gt;

That is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:

:&lt;math&gt; \mathrm{CG_{6}} = \sum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11&lt;/math&gt;

Changing the order of any two documents does not affect the CG measure. If &lt;math&gt;D_3&lt;/math&gt; and &lt;math&gt;D_4&lt;/math&gt; are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:


{| class="wikitable" border="1"
|-
! &lt;math&gt;i&lt;/math&gt;
! &lt;math&gt;rel_{i}&lt;/math&gt;
! &lt;math&gt;\log_{2}(i+1)&lt;/math&gt;
! &lt;math&gt; \frac{rel_{i}}{\log_{2}(i+1)} &lt;/math&gt;
|-
| 1
| 3
| 1
| 3
|-
| 2
| 2
| 1.585
| 1.262
|-
| 3
| 3
| 2
| 1.5
|-
| 4
| 0
| 2.322
| 0
|-
| 5
| 1
| 2.585
| 0.387
|-
| 6
| 2
| 2.807
| 0.712
|}

So the &lt;math&gt;DCG_{6}&lt;/math&gt; of this ranking is:

:&lt;math&gt; \mathrm{DCG_{6}} = \sum_{i=1}^{6} \frac{rel_{i}}{\log_{2}(i+1)} = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861&lt;/math&gt;

Now a switch of &lt;math&gt;D_3&lt;/math&gt; and &lt;math&gt;D_4&lt;/math&gt; results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.

The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.

To normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:

:&lt;math&gt; 3, 3, 2, 2, 1, 0 &lt;/math&gt;

The DCG of this ideal ordering, or ''IDCG (Ideal DCG)'' , is then:

:&lt;math&gt; \mathrm{IDCG_{6}} = 7.141 &lt;/math&gt;

And so the nDCG for this query is given as:

:&lt;math&gt; \mathrm{nDCG_{6}} = \frac{DCG_{6}}{IDCG_{6}} = \frac{6.861}{7.141} = 0.961 &lt;/math&gt;

== Limitations ==
# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores &lt;math&gt; 1,1,1 &lt;/math&gt; and &lt;math&gt; 1,1,1,0 &lt;/math&gt; respectively, both would be considered equally good even if the latter contains a bad result. One way to take into account this limitation is to use &lt;math&gt;1 - 2^{rel_{i}}&lt;/math&gt; in the numerator for scores for which we want to penalize and &lt;math&gt;2^{rel_{i}} - 1&lt;/math&gt; for all others. For example, for the ranking judgments &lt;math&gt;Excellent, Fair, Bad&lt;/math&gt; one might use numerical scores &lt;math&gt;1,0,-1&lt;/math&gt; instead of &lt;math&gt;2,1,0&lt;/math&gt;.
# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores &lt;math&gt; 1,1,1 &lt;/math&gt; and &lt;math&gt; 1,1,1,1,1 &lt;/math&gt; respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores &lt;math&gt; 1,1,1,0,0 &lt;/math&gt; and &lt;math&gt; 1,1,1,1,1 &lt;/math&gt; and quote nDCG as nDCG@5.
# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.

== References ==
{{Reflist|1}}

[[Category:Information retrieval evaluation]]</text>
      <sha1>9525lsrvg9khdq6kf8yhnp4r4rr1916</sha1>
    </revision>
  </page>
  <page>
    <title>Universal IR Evaluation</title>
    <ns>0</ns>
    <id>26591446</id>
    <revision>
      <id>716876883</id>
      <parentid>641384455</parentid>
      <timestamp>2016-04-24T12:38:09Z</timestamp>
      <contributor>
        <username>Noyster</username>
        <id>19396915</id>
      </contributor>
      <comment>add lead sentence</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3840" xml:space="preserve">{{Multiple issues|
{{refimprove|date=April 2011}}
{{orphan|date=April 2010}}
}}

In [[computer science]], ''universal [[Information retrieval evaluation|IR (information retrieval) evaluation]]'' aims to develop measures of database retrieval performance that shall be comparable across all information retrieval tasks.

==Measures of "relevance"==
[[Information retrieval evaluation|IR (information retrieval) evaluation]] begins whenever a user submits a query (search term) to a [[database]]. If the user is able to determine the [[Relevance (information retrieval)|relevance]] of each document in the database (relevant or not relevant), then for each query, the complete set of documents is naturally divided into four distinct (mutually exclusive) subsets: relevant documents that are retrieved, not relevant documents that are retrieved, relevant documents that are not retrieved, and not relevant documents that are not retrieved. These four subsets (of documents) are denoted by the letters a,b,c,d respectively and are called Swets variables, named after their inventor.&lt;ref&gt;Swets, J.A. (1969). Effectiveness of information retrieval methods. ''American Documentation, 20''(1), 72-89.&lt;/ref&gt;

In addition to the Swets definitions, four relevance metrics have also been defined: [[Precision (information retrieval)|Precision]] refers to the fraction of relevant documents that are retrieved (a/(a+b)), and [[Precision (information retrieval)|Recall]] refers to the fraction of retrieved documents that are relevant (a/(a+c)). These are the most commonly used and well-known relevance metrics found in the IR evaluation literature. Two less commonly used metrics include the Fallout, i.e., the fraction of not relevant documents that are retrieved (b/(b+d)), and the Miss, which refers to the fraction of relevant documents that are not retrieved (c/(c+d)) during any given search.

==Universal IR evaluation techniques==
Universal IR evaluation addresses the mathematical possibilities and relationships among the four relevance metrics Precision, Recall, Fallout and Miss, denoted by P, R, F and M, respectively. One aspect of the problem involves finding a mathematical derivation of a complete set of universal IR evaluation points.&lt;ref&gt;Schatkun, M. (2010). A Second look at Egghe's universal IR surface and a simple derivation of a complete set of universal IR evaluation points. ''Information Processing &amp; Management, 46''(1), 110-114.&lt;/ref&gt; The complete set of 16 points, each one a quadruple of the form (P,R,F,M), describes all the possible universal IR outcomes. For example, many of us have had the experience of querying a database and not retrieving any documents at all. In this case, the Precision would take on the undetermined form 0/0, the Recall and Fallout would both be zero, and the Miss would be any value greater than zero and less than one (assuming a mix of relevant and not relevant documents were in the database, none of which were retrieved). This universal IR evaluation point would thus be denoted by (0/0, 0, 0, M), which represents only one of the 16 possible universal IR outcomes.

The mathematics of universal IR evaluation is a fairly new subject since the relevance metrics P,R,F,M were not analyzed collectively until recently (within the past decade). A lot of the theoretical groundwork has already been formulated, but new insights in this area await discovery. For a detailed mathematical analysis, a query in the [[ScienceDirect]] database for "universal IR evaluation" retrieves several relevant peer-reviewed papers.

==See also==
* [[Information retrieval]]
* [[Web search query]]

==References==
{{Reflist}}

==External links==
* [http://www.sciencedirect.com Science Direct]

{{DEFAULTSORT:Universal Ir Evaluation}}
[[Category:Databases]]
[[Category:Information retrieval evaluation]]</text>
      <sha1>3k9c67chjch0ddw1mi1r68hbwm7d1vd</sha1>
    </revision>
  </page>
  <page>
    <title>Controlled vocabulary</title>
    <ns>0</ns>
    <id>1850719</id>
    <revision>
      <id>753986026</id>
      <parentid>752248354</parentid>
      <timestamp>2016-12-10T05:37:44Z</timestamp>
      <contributor>
        <username>Rjlabs</username>
        <id>325437</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16719" xml:space="preserve">{{refimprove|date=June 2012}}

'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]],&lt;ref&gt;[https://web.archive.org/web/20101204132228/http://www.imresources.fit.qut.edu.au:80/vocab/ Controlled Vocabularies]  Links to examples of thesauri and classification schemes.&lt;/ref&gt;&lt;ref&gt;[https://web.archive.org/web/20090314094707/http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies]  Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.&lt;/ref&gt; [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction.

== In library and information science ==

In [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.&lt;ref&gt;Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].&lt;/ref&gt;&lt;ref&gt;Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]&lt;/ref&gt; Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.

For example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms&#8212;subject headings in this case&#8212;have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (''cockroach'' versus ''Periplaneta americana''), and choices between synonyms (''automobile'' versus ''car''), among other difficult issues.

Choices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).

Controlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term ''pool'' has to be qualified to refer to either ''swimming pool'' or the game ''pool'' to ensure that each authorized term or heading refers to only one concept.

There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.

Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.

For example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "[[Hypernym|Broader term]]" and "[[Hyponym|Narrow term]]".

The [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.

Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.

Controlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].

== Indexing languages ==

There are three main types of indexing languages.

* Controlled indexing language &#8211; only approved terms can be used by the indexer to describe the document
* [[Natural language]] indexing language &#8211; any term from the document in question can be used to describe the document
* Free indexing language &#8211; any term (not only from the document) can be used to describe the document

When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document.

In recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.

Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word [[football (word)|''football'']] for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|association football]], which also happens to be called ''[[soccer]]'' in several countries. The word ''football'' is also applied to [[rugby football]] ([[rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.

Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).

In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.

However, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.

This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.

Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.

On the other hand, free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.

Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.

The use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.

Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.

==Applications==
Controlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.

In large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.

Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].

It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.&lt;ref&gt;Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].&lt;/ref&gt; To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.&lt;ref&gt;Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].&lt;/ref&gt;

Controlled vocabularies of the [[Semantic Web]] define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of &#8220;Person&#8221;, such as the Friend of a Friend ([[FOAF]]) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of [[Schema.org]].&lt;ref&gt;{{cite web |url=http://schema.org/Person |title=The Person vocabulary of Schema.org |accessdate=13 March 2015}}&lt;/ref&gt; Similarly, a book can be described using the Book vocabulary of [[Schema.org]]&lt;ref&gt;{{cite web |url=http://schema.org/Book |title=The Book vocabulary of Schema.org |accessdate=13 March 2015}}&lt;/ref&gt; and general publication terms from the [[Dublin Core]] vocabulary,&lt;ref&gt;{{cite web |url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |accessdate=13 March 2015}}&lt;/ref&gt; an event with the Event vocabulary of [[Schema.org]],&lt;ref&gt;{{cite web |url=http://schema.org/Event |title=The Event vocabulary of Schema.org |accessdate=13 March 2015}}&lt;/ref&gt; and so on.

To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, [[Microdata (HTML)|HTML5 Microdata]], or [[JSON-LD]] in the markup, or [[Resource Description Framework|RDF]] serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files.

==See also==
*[[Authority control]]
*[[Controlled natural language]]
*[[IMS VDEX|IMS Vocabulary Definition Exchange]]
*[[Named-entity recognition]]
*[[Nomenclature]]
*[[Ontology (computer science)]]
*[[Terminology]]
*[[Thesaurus]]
*[[Universal Data Element Framework]]
*[[Vocabulary-based transformation]]

==References==
{{Reflist|2}}

==External links==
* [http://www.controlledvocabulary.com/ controlledvocabulary.com] &#8212; explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.
* [http://www.photo-keywords.com/ photo-keywords.com/] &#8212; useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.
* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]

{{Lexicography}}

[[Category:Information retrieval techniques]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]
[[Category:Controlled vocabularies]]
[[Category:Information science]]</text>
      <sha1>peb0omh8lyh05h6afppxoruwpxfgqa7</sha1>
    </revision>
  </page>
  <page>
    <title>Search suggest drop-down list</title>
    <ns>0</ns>
    <id>23344134</id>
    <revision>
      <id>762009683</id>
      <parentid>756061903</parentid>
      <timestamp>2017-01-26T03:23:31Z</timestamp>
      <contributor>
        <username>Connor Behan</username>
        <id>2139896</id>
      </contributor>
      <minor />
      <comment>ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8636" xml:space="preserve">A '''search suggest drop-down list''' is a [[Query language|query]] feature used in [[computing]] to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed into a [[text box]]. Before the query is complete, a[[drop-down list]] with the suggested completions appears to provide options to select. The suggested queries then enable the searcher to complete the required search quickly. As a form of [[Autocomplete|autocompletion]], the suggestion list is distinct from [[web browsing history|search history]] in that it attempts to be predictive even when the user is searching for the first time. Data may come from popular searches, sponsors, geographic location or other sources.&lt;ref&gt;{{cite web|url=http://www.thingsontop.com/googles-new-search-suggestions-may-kill-your-website-158.html|title=Google's new search suggestions may kill your website|first=Vegard|last=Sandvoid|publisher=Things On Top|date=2008-12-14|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://diegobasch.com/search-for-obama-on-facebook-and-you-get-romney|title=Search for Obama on Facebook and you get Romney|first=Diego|last=Basch|date=2012-09-19|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref name="se-land"&gt;{{cite web|url=http://searchengineland.com/how-google-instant-autocomplete-suggestions-work-62592|title=How Google Instant's autocomplete suggestions work|first=Danny|last=Sullivan|publisher=Search Engine Land|date=2011-04-06|accessdate=2016-08-03}}&lt;/ref&gt; These lists are used by [[operating system]]s, [[web browsers]] and various [[website]]s, particularly [[search engine]]s. Search suggestions are common with a 2014 survey finding that over 80% of [[e-commerce]] websites included them.&lt;ref&gt;{{cite web|url=https://www.smashingmagazine.com/2014/08/the-current-state-of-e-commerce-search/|title=The current state of e-commerce search|first=Christian|last=Holt|publisher=Smash Magazine|date=2014-08-18|accessdate=2016-08-03}}&lt;/ref&gt;

The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from a [[database]]. [[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].

Although not the first deployment of search suggestions, [[Google Suggest]] is one of the most prominent. Four years before it was considered stable, the feature was developed in 2004 by Google engineer Kevin Gibbs and the name was chosen by [[Marissa Mayer]].&lt;ref&gt;{{cite web|url=http://allthingsd.com/20130823/nearly-a-decade-later-the-autocomplete-origin-story-kevin-gibbs-and-google-suggest/|title=Nearly a Decade Later, the Autocomplete Origin Story: Kevin Gibbs and Google Suggest|first=Liz|last=Gannes|publisher=All Things D|date=2013-08-23|accessdate=2016-08-03}}&lt;/ref&gt; Google, and other large search companies, maintain a blacklist that prevents the display of queries that could be interpreted as violating their [[social responsibility]]. Despite this, the company regularly receives complaints that several popular suggestions, or suggestions whose positions have been inflated by [[Internet bot|bots]], should be added to this list.&lt;ref name="se-land" /&gt;&lt;ref&gt;{{cite web|url=https://utopiaordystopia.com/2015/02/22/truth-and-prediction-in-the-dataclysm/|title=Truth and Prediction in the Dataclysm|first=Rick|last=Searle|publisher=Utopia or Dystopia|date=2015-02-22|accessdate=2016-08-03}}&lt;/ref&gt; The [[Electronic Frontier Foundation]]'s [[Jillian York]] has criticized [[Apple Computers|Apple]]'s blacklist for including words that are merely provocative.&lt;ref&gt;{{cite web|url=http://www.thedailybeast.com/articles/2013/07/16/the-apple-kill-list-what-your-iphone-doesn-t-want-you-to-type.html|title=The Apple 'Kill List': What your iPhone doesn't want you to type|first=Michael|last=Keller|publisher=The Daily Beast|date=2013-07-16|accessdate=2016-08-03}}&lt;/ref&gt;

One example of a project using suggested queries to expose societal attitudes was a 2013 ad series called ''The Autocomplete Truth'' by [[UN Women]]. The campaign showed several gender stereotypes being displayed as popular searches by Google Suggest.&lt;ref&gt;{{cite web|url=http://www.adweek.com/adfreak/after-viral-success-inequality-ads-creators-say-they-will-expand-campaign-153363|title=After viral succes of inequality ads, creators say they will expand campaign|first=David|last=Griner|publisher=Ad Week|date=2013-10-24|accessdate=2016-08-03}}&lt;/ref&gt; Another was a story by [[Bad Astronomy]] that revealed a distrustful perspective on scientists in the suggestion box.&lt;ref&gt;{{cite web|url=http://www.slate.com/blogs/bad_astronomy/2013/12/04/search_engine_bias_scientists_are.html|title="Scientists are..."|first=Phil|last=Plait|publisher=Slate|date=2013-12-04|accessdate=2016-08-03}}&lt;/ref&gt; Additionally, cases related to [[libel]] laws have posited that suggestions may inspire people to associate specific names with specific alleged crimes when they would not have otherwise.&lt;ref name="japan"&gt;{{cite web|url=http://www.tamingthebeast.net/blog/online-world/google-autocomplete-angst.htm|title=Some Folks *Really* Hate Autocomplete|first=Michael|last=Bloch|publisher=Taming The Beast|date=2012-03-27|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|url=http://ijlit.oxfordjournals.org/content/23/3/261.full|title=Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm|first1=Stavroula|last1=Karapapa|first2=Maurizio|last2=Borghi|journal=International Journal of Law and Information Technology|volume=23|pages=261-289|year=2015|accessdate=2016-08-03}}&lt;/ref&gt;

Some users have criticized the fact that suggestion-enabled text boxes, unlike the [[web forms]] of static HTML, send data about each keystroke to a central server.&lt;ref&gt;{{cite web|url=http://thekeesh.com/2011/08/who-does-facebook-think-you-are-searching-for/|title=Who does Facebook think you are searching for?|first=Jeremy|last=Keeshin|publisher=The Keesh|date=2011-08-18|accessdate=2016-08-03}}&lt;/ref&gt; Such data has the potential to [[keystroke dynamics|identify specific people]]. This has caused at least one [[Mozilla Firefox]] developer to opine that "users mostly dislike search suggestions".&lt;ref&gt;{{cite web|url=https://bugzilla.mozilla.org/show_bug.cgi?id=1189719|title=Recall and display search history within main browser UI|first=Richard|last=Newman|publisher=Mozilla|date=2015-08-25|accessdate=2016-08-03}}&lt;/ref&gt; Apart from the privacy debate, some users have expressed negative reception over the usefulness of search autocompletion.&lt;ref name="japan" /&gt;&lt;ref&gt;{{cite web|url=http://arnoldit.com/wordpress/2012/09/10/google-autocomplete-is-smart-help-a-hindrance/|title=Google Autocomplete: Is Smart Help A Hindrance?|first=Stephen|last=Arnold|publisher=Beyond Search|date=2012-09-09|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.quora.com/How-do-very-strange-stupid-auto-complete-statements-appear-while-searching-on-Google-Do-people-actually-do-these-kind-of-searches-or-are-they-pun-intended|title=How do very strange/stupid auto-complete statements appear while searching on Google? Do people actually do these kind of searches or are they pun intended?|first=Seshal|last=Jain|publisher=Quora|date=2015-05-02|accessdate=2016-08-03}}&lt;/ref&gt; Specifically, the sudden appearance of a suggestion box in some programs has been compared to the behaviour of a [[pop-up ad]].&lt;ref&gt;{{cite web|url=http://martesmartes.blogspot.com/2008/07/disabling-openoffices-stupid.html|title=Disabling Open Office's Stupid Autocomplete|first=Jeff|last=Martens|publisher=Martes-Martes|date=2008-07-09|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://mikesmithers.wordpress.com/2012/01/28/turning-off-code-completion-in-sqldeveloper-a-grumpy-old-man-fights-back/|title=Turning off code completion in SQLDeveloper &amp;mdash; A grumpy old man fights back|first=Mike|last=Smithers|publisher=The Anti-Kyte|date=2012-01-28|accessdate=2016-08-03}}&lt;/ref&gt;

==See also==
*[[Autocomplete]]
*[[Search engine (computing)]]
*[[Search box]]
*[[Search algorithm]]
*[[Censorship by Google#Search suggestions|Censorship by Google &#167; Search suggestions]]

==References==
{{Reflist}}

{{DEFAULTSORT:Search Suggest Drop-Down List}}
[[Category:Information retrieval techniques]]</text>
      <sha1>sg1khoj7kpgqw7315poot0npjijws35</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Vector space model</title>
    <ns>14</ns>
    <id>36475839</id>
    <revision>
      <id>666717303</id>
      <parentid>503023195</parentid>
      <timestamp>2015-06-13T04:30:48Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="45" xml:space="preserve">[[Category:Information retrieval techniques]]</text>
      <sha1>d5cl9w748m6esl94ihxfydykzgembzh</sha1>
    </revision>
  </page>
  <page>
    <title>Extended Boolean model</title>
    <ns>0</ns>
    <id>25271852</id>
    <revision>
      <id>735217565</id>
      <parentid>735217500</parentid>
      <timestamp>2016-08-19T08:55:27Z</timestamp>
      <contributor>
        <ip>14.139.180.72</ip>
      </contributor>
      <comment>Undid revision 735217500 by [[Special:Contributions/14.139.180.72|14.139.180.72]] ([[User talk:14.139.180.72|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7211" xml:space="preserve">The '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.&lt;ref&gt;	
{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last1=Salton | first1=Gerard | first2=Edward A. | last2=Fox | first3= Harry | last3=Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}&lt;/ref&gt;

Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.

==Definitions==
In the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.

The weight of term {{math|''K&lt;sub&gt;x&lt;/sub&gt;''}} associated with document {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} is measured by its normalized [[Term frequency]] and can be defined as:

&lt;math&gt;
w_{x,j}=f_{x,j}*\frac{Idf_{x}}{max_{i}Idf_{i}}
&lt;/math&gt;

where {{math|''Idf&lt;sub&gt;x&lt;/sub&gt;''}} is [[inverse document frequency]].

The weight vector associated with document {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} can be represented as:

&lt;math&gt;\mathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \ldots, w_{i,j}]&lt;/math&gt;

==The 2 Dimensions Example==
{{multiple image
 | width     = 150
 | image1    = 2D_Extended_Boolean_model_OR_example.png
 | alt1      = Figure 1
 | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;or; ''K&lt;sub&gt;y&lt;/sub&gt;'')}} with documents {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} and {{math|''d''&lt;sub&gt;''j''+1&lt;/sub&gt;}}.
 | image2    = 2D_Extended_Boolean_model_AND_example.png
 | alt2      = Figure 2
 | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;and; ''K&lt;sub&gt;y&lt;/sub&gt;'')}} with documents {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} and {{math|''d''&lt;sub&gt;''j''+1&lt;/sub&gt;}}.
}}

Considering the space composed of two terms {{math|''K&lt;sub&gt;x&lt;/sub&gt;''}} and {{math|''K&lt;sub&gt;y&lt;/sub&gt;''}} only, the corresponding term weights are {{math|''w''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''w''&lt;sub&gt;2&lt;/sub&gt;}}.&lt;ref&gt;[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]&lt;/ref&gt;  Thus, for query {{math|''q&lt;sub&gt;or&lt;/sub&gt;'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;or; ''K&lt;sub&gt;y&lt;/sub&gt;'')}}, we can calculate the similarity with the following formula:
 
&lt;math&gt;sim(q_{or},d)=\sqrt{\frac{w_1^2+w_2^2}{2}}&lt;/math&gt;

For query {{math|''q&lt;sub&gt;and&lt;/sub&gt;'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;and; ''K&lt;sub&gt;y&lt;/sub&gt;'')}}, we can use:

&lt;math&gt;sim(q_{and},d)=1-\sqrt{\frac{(1-w_1)^2+(1-w_2)^2}{2}}&lt;/math&gt;

==Generalizing the idea and P-norms==
We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.

This can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &amp;le; ''p'' &amp;le; &amp;infin;}} is a new parameter.&lt;ref&gt;{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}&lt;/ref&gt;

*A generalized conjunctive query is given by:
:&lt;math&gt;q_{or}=k_1 \lor^p k_2 \lor^p .... \lor^p k_t  &lt;/math&gt;

*The similarity of &lt;math&gt;q_{or}&lt;/math&gt; and &lt;math&gt;d_j&lt;/math&gt; can be defined as:
''':&lt;math&gt;sim(q_{or},d_j)=\sqrt[p]{\frac{w_1^p+w_2^p+....+w_t^p}{t}}&lt;/math&gt;'''

*A generalized disjunctive query is given by:
:&lt;math&gt;q_{and}=k_1 \land^p k_2 \land^p .... \land^p k_t  &lt;/math&gt;

*The similarity of &lt;math&gt;q_{and}&lt;/math&gt; and &lt;math&gt;d_j&lt;/math&gt; can be defined as:
:&lt;math&gt;sim(q_{and},d_j)=1-\sqrt[p]{\frac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}&lt;/math&gt;

==Examples==
Consider the query {{math|''q'' {{=}} (''K''&lt;sub&gt;1&lt;/sub&gt; &amp;and; ''K''&lt;sub&gt;2&lt;/sub&gt;) &amp;or; ''K''&lt;sub&gt;3&lt;/sub&gt;}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:

&lt;math&gt;sim(q,d)=\sqrt[p]{\frac{(1-\sqrt[p]{(\frac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}&lt;/math&gt;

==Improvements over the Standard Boolean Model==

Lee and Fox&lt;ref&gt;{{citation | last1=Lee | first1=W. C. | first2=E. A. | last2=Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries | url = http://eprints.cs.vt.edu/archive/00000112/01/TR-88-27.pdf}}&lt;/ref&gt; compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.
Using P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.&lt;br&gt;
The P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.

==Further reading==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]
* [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6VC8-454T5MS-2&amp;_user=513551&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1117914301&amp;_rerunOrigin=google&amp;_acct=C000025338&amp;_version=1&amp;_urlVersion=0&amp;_userid=513551&amp;md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | first2=S. | last2=Betrabet | first3=M. | last3=Koushik | first4=W. | last4=Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}
* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first1= Lucie | last1= Skorkovsk&#225; | first2=Pavel | last2=Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}

==See also==
*[[Information retrieval]]

==References==
{{reflist}}

{{DEFAULTSORT:Extended Boolean Model}}
[[Category:Information retrieval techniques]]</text>
      <sha1>dqzp4lhnpncigjmr8w05tm6yesy2tbw</sha1>
    </revision>
  </page>
  <page>
    <title>Subsetting</title>
    <ns>0</ns>
    <id>3231582</id>
    <revision>
      <id>745810698</id>
      <parentid>701827927</parentid>
      <timestamp>2016-10-23T12:53:07Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1132" xml:space="preserve">In research communities (for example, [[earth science]]s, [[astronomy]], [[business]], and [[government]]), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client&#8212;server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.

Subsetting may be favorable for the following reasons:&lt;ref name="Institute2012"&gt;{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=https://books.google.com/books?id=OE0UfAhit4kC&amp;pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}&lt;/ref&gt;
* restrict or divide the time range
* select [[Cross-sectional data|cross section]]s of data
* select particular kinds of [[time series]]
* exclude particular observations

==References==
{{reflist}}


==External links==
*[http://www.subset.org/index.jsp Subset.org]

[[Category:Information retrieval techniques]]

{{Statistics-stub}}</text>
      <sha1>ehtfvn3ht2czhn8f3f65c82ilqs52y5</sha1>
    </revision>
  </page>
  <page>
    <title>Statistical semantics</title>
    <ns>0</ns>
    <id>7271261</id>
    <revision>
      <id>753523331</id>
      <parentid>753522318</parentid>
      <timestamp>2016-12-07T18:13:23Z</timestamp>
      <contributor>
        <ip>82.2.1.89</ip>
      </contributor>
      <comment>/* External links */ Removed low value external links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13412" xml:space="preserve">{{linguistics}}
'''Statistical semantics''' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}&lt;!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation--&gt;. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?

==History==

The term ''statistical semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].&lt;ref&gt;{{harvnb|Weaver|1955}}&lt;/ref&gt; He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].&lt;ref&gt;{{harvnb|Firth|1957}}&lt;/ref&gt; This assumption is known in [[linguistics]] as the [[distributional hypothesis]].&lt;ref&gt;{{harvnb|Sahlgren|2008}}&lt;/ref&gt; Emile Delavenay defined ''statistical semantics'' as the "Statistical study of meanings of words and their frequency and order of recurrence."&lt;ref&gt;{{harvnb|Delavenay|1960}}&lt;/ref&gt; "[[George Furnas|Furnas]] et al. 1983" is frequently cited as a foundational contribution to statistical semantics.&lt;ref&gt;{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}&lt;/ref&gt;  An early success in the field was [[latent semantic analysis]].

==Applications==

Research in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:
* Measuring the [[Semantic similarity|similarity in word meanings]]&lt;ref&gt;{{harvnb|Lund|Burgess|Atchley|1995}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Landauer|Dumais|1997}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|McDonald|Ramscar|2001}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Terra|Clarke|2003}}&lt;/ref&gt;
* Measuring the similarity in word relations &lt;ref&gt;{{harvnb|Turney|2006}}&lt;/ref&gt;
* Modeling [[similarity-based generalization]]&lt;ref&gt;{{harvnb|Yarlett|2008}}&lt;/ref&gt;
* Discovering words with a given relation&lt;ref&gt;{{harvnb|Hearst|1992}}&lt;/ref&gt;
* Classifying relations between words&lt;ref&gt;{{harvnb|Turney|Littman|2005}}&lt;/ref&gt;
* Extracting keywords from documents&lt;ref&gt;{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Turney|2000}}&lt;/ref&gt;
* Measuring the cohesiveness of text&lt;ref&gt;{{harvnb|Turney|2003}}&lt;/ref&gt;
* Discovering the different senses of words&lt;ref&gt;{{harvnb|Pantel|Lin|2002}}&lt;/ref&gt;
* Distinguishing the different senses of words&lt;ref&gt;{{harvnb|Turney|2004}}&lt;/ref&gt;
* Subcognitive aspects of words&lt;ref&gt;{{harvnb|Turney|2001}}&lt;/ref&gt;
* Distinguishing praise from criticism&lt;ref&gt;{{harvnb|Turney|Littman|2003}}&lt;/ref&gt;

==Related fields==

Statistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].

Many of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.&lt;ref&gt;{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}&lt;/ref&gt;

==See also==
{{Portal|Linguistics}}
{{div col|3}}
*[[Co-occurrence]]
*[[Computational linguistics]]
*[[Information retrieval]]
*[[Latent semantic analysis]]
*[[Latent semantic indexing]]
*[[Natural language processing]]
*[[Semantic analytics]]
*[[Semantic similarity]]
*[[Text corpus]]
*[[Text mining]]
*[[Web mining]]
{{div col end}}

==References==
{{reflist|2}}

===Sources===
{{refbegin}}
* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}
* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1&#8211;32 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}
*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}
* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668&#8211;673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | citeseerx = 10.1.1.148.3598 | ref = harv }}
* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = https://web.archive.org/web/*/http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753&#8211;1806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}
* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539&#8211;545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | citeseerx = 10.1.1.36.701 | ref = harv }}
* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211&#8211;240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | citeseerx = 10.1.1.184.4759 | ref = harv | doi=10.1037/0033-295x.104.2.211}}
* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660&#8211;665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}
* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611&#8211;616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | citeseerx = 10.1.1.104.7535 | ref = harv }}
* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613&#8211;619 | citeseerx = 10.1.1.12.6771 | doi = 10.1145/775047.775138 | ref = harv }}
* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33&#8211;53 | ref = harv}}
* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244&#8211;251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | citeseerx = 10.1.1.12.9041 | doi = 10.3115/1073445.1073477 | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303&#8211;336 | arxiv = cs/0212020 | citeseerx = 10.1.1.11.1829 | doi = 10.1023/A:1009976227802 | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409&#8211;419 | arxiv = cs/0212015 | citeseerx = 10.1.1.12.8734 | ref = harv | doi=10.1080/09528130110100270}}
* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434&#8211;439 | arxiv = cs/0308033 | citeseerx = 10.1.1.100.3751 | ref = harv }}
* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239&#8211;242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379&#8211;416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | citeseerx = 10.1.1.75.8007 | ref = harv }}
* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] | volume = 21 | issue = 4 | pages = 315&#8211;346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | citeseerx = 10.1.1.9.6425 | doi = 10.1145/944012.944013 | ref = harv }}
* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1&#8211;3 | pages = 251&#8211;278 | arxiv = cs/0508103 | citeseerx = 10.1.1.90.9819 | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}
* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482&#8211;489 | arxiv = cs/0309035 | citeseerx = 10.1.1.5.2939 | url = http://cogprints.org/3163/ | ref = harv }}
* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15&#8211;23 | ref = harv }}
* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}
{{refend}}


{{DEFAULTSORT:Statistical Semantics}}
[[Category:Artificial intelligence applications]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]
[[Category:Semantics]]
[[Category:Statistical natural language processing]]
[[Category:Applied statistics]]</text>
      <sha1>1y8u3rtxu122ixzhvlcjv2umaflm40q</sha1>
    </revision>
  </page>
  <page>
    <title>Literature-based discovery</title>
    <ns>0</ns>
    <id>31149053</id>
    <revision>
      <id>666859024</id>
      <parentid>654885635</parentid>
      <timestamp>2015-06-14T05:29:24Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3200" xml:space="preserve">'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. 

Literature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".&lt;ref&gt;{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526&#8211;557 | doi=10.1353/pbm.1988.0009}}&lt;/ref&gt; It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].

==Swanson linking==
[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]
''Swanson linking'' is a term proposed in 2003&lt;ref&gt;Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111&#8211;135. As quoted by Bekhuis&lt;/ref&gt; that refers to connecting two pieces of knowledge previously thought to be unrelated.&lt;ref&gt;{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}&lt;/ref&gt; For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.

==See also==
*[[Arrowsmith System]]
*[[Implicature]]
*[[Latent semantic indexing]]
*[[Metaphor]]

==References==
* Chen, Ran; Hongfei Lin &amp; Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&amp;nbsp;9958&#8211;9964.
*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&amp;coll=DL&amp;dl=GUIDE&amp;CFID=23143258&amp;CFTOKEN=52033794 ACM DL]

; Further readings
* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&amp;nbsp;156. ISBN 0-8371-9485-7.

; Footnotes
{{reflist}}

[[Category:Information retrieval techniques]]
[[Category:Medical research]]


{{science-stub}}</text>
      <sha1>ou3v0a629slw4sic5bh7zhhn1npizvt</sha1>
    </revision>
  </page>
  <page>
    <title>Fuzzy retrieval</title>
    <ns>0</ns>
    <id>25935906</id>
    <revision>
      <id>745299586</id>
      <parentid>745299532</parentid>
      <timestamp>2016-10-20T10:15:29Z</timestamp>
      <contributor>
        <username>564dude</username>
        <id>10738273</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8396" xml:space="preserve">'''Fuzzy retrieval''' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.

==Mixed Min and Max model (MMM)==

In fuzzy-set theory, an element has a varying degree of membership, say ''d&lt;sub&gt;A&lt;/sub&gt;'', to a given set ''A'' instead of the traditional membership choice (is an element/is not an element).&lt;br /&gt;
In MMM&lt;ref&gt;{{citation | last1=Fox | first1=E. A. | author2=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}&lt;/ref&gt; each index term has a fuzzy set associated with it. A document's weight with respect to an index term ''A'' is considered to be the degree of membership of the document in the fuzzy set associated with ''A''. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:&lt;br/&gt;
:&lt;math&gt;d_{A\cap B}= min(d_A, d_B)&lt;/math&gt;
:&lt;math&gt;d_{A\cup B}= max(d_A,d_B)&lt;/math&gt;

According to this, documents that should be retrieved for a query of the form ''A or B'', should be in the fuzzy set associated with the union of the two sets ''A'' and ''B''. Similarly, the documents that should be retrieved for a query of the form ''A and B'', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the ''or'' query to be ''max(d&lt;sub&gt;A&lt;/sub&gt;, d&lt;sub&gt;B&lt;/sub&gt;)'' and the similarity of the document to the ''and'' query to be ''min(d&lt;sub&gt;A&lt;/sub&gt;, d&lt;sub&gt;B&lt;/sub&gt;)''. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the ''min'' and ''max'' document weights.

Given a document ''D'' with index-term weights ''d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;'' for terms ''A&lt;sub&gt;1&lt;/sub&gt;, A&lt;sub&gt;2&lt;/sub&gt;, ..., A&lt;sub&gt;n&lt;/sub&gt;'', and the queries:

''Q&lt;sub&gt;or&lt;/sub&gt; = (A&lt;sub&gt;1&lt;/sub&gt; or A&lt;sub&gt;2&lt;/sub&gt; or ... or A&lt;sub&gt;n&lt;/sub&gt;)''&lt;br /&gt;
''Q&lt;sub&gt;and&lt;/sub&gt; = (A&lt;sub&gt;1&lt;/sub&gt; and A&lt;sub&gt;2&lt;/sub&gt; and ... and A&lt;sub&gt;n&lt;/sub&gt;)''

the query-document similarity in the MMM model is computed as follows:

''SlM(Q&lt;sub&gt;or&lt;/sub&gt;, D) = C&lt;sub&gt;or1&lt;/sub&gt; * max(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;) + C&lt;sub&gt;or2&lt;/sub&gt; * min(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;)''&lt;br /&gt;
''SlM(Q&lt;sub&gt;and&lt;/sub&gt;, D) = C&lt;sub&gt;and1&lt;/sub&gt; * min(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;) + C&lt;sub&gt;and2&lt;/sub&gt; * max(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt; ..., d&lt;sub&gt;An&lt;/sub&gt;)''

where ''C&lt;sub&gt;or1&lt;/sub&gt;, C&lt;sub&gt;or2&lt;/sub&gt;'' are "softness" coefficients for the ''or'' operator, and ''C&lt;sub&gt;and1&lt;/sub&gt;, C&lt;sub&gt;and2&lt;/sub&gt;'' are softness coefficients for the ''and'' operator. Since we would like to give the maximum of the document weights more importance while considering an ''or'' query and the minimum more importance while considering an ''and'' query, generally we have ''C&lt;sub&gt;or1&lt;/sub&gt; &gt; C&lt;sub&gt;or2&lt;/sub&gt; and C&lt;sub&gt;and1&lt;/sub&gt; &gt; C&lt;sub&gt;and2&lt;/sub&gt;''. For simplicity it is generally assumed that ''C&lt;sub&gt;or1&lt;/sub&gt; = 1 - C&lt;sub&gt;or2&lt;/sub&gt;'' and ''C&lt;sub&gt;and1&lt;/sub&gt; = 1 - C&lt;sub&gt;and2&lt;/sub&gt;''.

Lee and Fox&lt;ref name="leefox"&gt;{{citation | last1=Lee | first1=W. C. | author2=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}&lt;/ref&gt; experiments indicate that the best performance usually occurs with ''C&lt;sub&gt;and1&lt;/sub&gt;'' in the range [0.5, 0.8] and with ''C&lt;sub&gt;or1&lt;/sub&gt;'' &gt; 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].

==Paice model==

The Paice model&lt;ref&gt;{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}&lt;/ref&gt; is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:

:&lt;math&gt;S(D,Q) = \sum_{i=1}^n\frac{r^{i-1}*w_{di}}{\sum_{j=1}^n r^{j-1}}&lt;/math&gt;

where ''r'' is a constant coefficient and ''w&lt;sub&gt;di&lt;/sub&gt;'' is arranged in ascending order for ''and'' queries and descending order for ''or'' queries. When n = 2 the Paice model shows the same behavior as the MMM model.

The experiments of Lee and Fox&lt;ref name="leefox"/&gt; have shown that setting the ''r'' to 1.0 for ''and'' queries and 0.7 for ''or'' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of ''min'' or ''max'' of a set of term weights each time an ''and'' or ''or'' clause is considered, which can be done in ''O(n)''. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an ''and'' clause or an ''or'' clause is being considered. This requires at least an ''0(n log n)'' sorting algorithm. A good deal of floating point calculation is needed too.

==Improvements over the Standard Boolean model==
Lee and Fox&lt;ref name="leefox"/&gt; compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:
{| class="wikitable"
|-
!
! CISI
! CACM
! INSPEC
|-
! MMM
| 68%
| 109%
| 195%
|-
! Paice
| 77%
| 104%
| 206%
|}

These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.

==Recent work==

Recently '''Kang ''et al.'''.&lt;ref&gt;{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last1=Kang | first1=Bo-Yeong | author2=Dae-Won Kim |author3=Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}&lt;/ref&gt; have devised a fuzzy retrieval system indexed by concept identification.

If we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.&lt;br /&gt;
They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.

Zadrozny&lt;ref&gt;{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first1=S&#322;awomir | last1=Zadrozny | last2=Nowacka | first2=Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}&lt;/ref&gt; revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:
* assuming linguistic terms as importance weights of keywords also in documents
* taking into account the uncertainty concerning the representation of documents and queries
* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh&#8217;s fuzzy logic (calculus of linguistic statements)
* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries

The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.

==See also==
*[[Information retrieval]]

==Further reading==
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | author2=S. Betrabet | author3=M. Koushik | author4=W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}

==References==
{{reflist}}

{{DEFAULTSORT:Fuzzy Retrieval}}
[[Category:Information retrieval techniques]]</text>
      <sha1>p1d5venl6w978oshhxl2qtoplhul3ov</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted search</title>
    <ns>0</ns>
    <id>10715937</id>
    <revision>
      <id>762163166</id>
      <parentid>761809870</parentid>
      <timestamp>2017-01-27T01:39:57Z</timestamp>
      <contributor>
        <ip>47.208.26.15</ip>
      </contributor>
      <comment>removed irrelevant self-promotion</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5439" xml:space="preserve">'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.&lt;ref name="Faceted Search"&gt;[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan &amp; Claypool, 2009&lt;/ref&gt;

Facets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.

Within the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].&lt;ref name="sigir06"&gt;[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]&lt;/ref&gt;

==Development==

The [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:
&lt;blockquote&gt;
The web search world, since its very beginning, has offered two paradigms:
*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.
*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. 
Over the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].&lt;ref name="sigir06"&gt;[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]&lt;/ref&gt;
&lt;/blockquote&gt;

==Mass market use==

Faceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] provide software for implementing faceted search applications.

Online retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. &lt;ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)"&gt;[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.&lt;/ref&gt; Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.

==Libraries and information science==

In 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.&lt;ref name="Major classification systems : the Dewey Centennial"&gt;[https://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]&lt;/ref&gt;

Modern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system. The [[CiteSeerX]] project&lt;ref&gt;[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.&lt;/ref&gt; at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.

==See also==
* [[Enterprise search]]
* [[Exploratory search]]
* [[Faceted classification]]
* [[Human&#8211;computer information retrieval]]
* [[Information extraction]]
* [[NoSQL]]

==References==
&lt;References/&gt;

{{DEFAULTSORT:Faceted Search}}
[[Category:Information retrieval techniques]]</text>
      <sha1>679qro0109cm5m20aloozugpv228i91</sha1>
    </revision>
  </page>
  <page>
    <title>Personalization</title>
    <ns>0</ns>
    <id>1656760</id>
    <revision>
      <id>759758478</id>
      <parentid>758632459</parentid>
      <timestamp>2017-01-13T00:52:40Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor />
      <comment>Fixed a typo.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14711" xml:space="preserve">{{Multiple issues|
{{cleanup-reorganize|date=June 2008}}
{{refimprove|date=September 2015}}
{{tone|date=September 2015}}
}}

'''Personalization''', sometimes known as '''customization''', consists of tailoring a service or a product to accommodate specific individuals, sometimes tied to groups or segments of individuals.  A wide variety of organizations use personalization to improve [[customer satisfaction]], digital sales conversion, marketing results, branding, and improved website metrics as well as for [[advertising]]. Personalization is a key element in [[social media]] and [[recommender system]]s.

==Web pages==
[[Web page]]s can be personalized based on the characteristics (interests, social category, context, etc.), actions (click on button, open a link, etc.), intent (make a purchase, check status of an entity), or any other parameter that can be identified and associated with an individual, therefore providing them with a tailored user experience.  Note that the experience is rarely simply accommodation of the user but a relationship between the user and the desires of the site designers in driving specific actions to achieve objectives (e.g. Increase sales conversion on a page).  The term ''customization'' is often used when the site only uses explicit data such as product ratings or user preferences.

Technically, web personalization can be achieved by associating a visitor segment with a predefined action. Customizing the user experience based on behavioural, contextual and technical data is proven to have a positive impact on conversion rate optimization efforts. Associated actions can range from changing the content of a webpage, presenting a modal display, presenting interstitials, triggering a personalized email or even automating a phone call to the user.

According to a 2014 study from research firm Econsultancy, less than 30% of [[e-commerce]] websites have invested in the field of web personalization. However, many companies now offer services for web personalization as well as web and email recommendation systems that are based on personalization or anonymously-collected user behaviours.&lt;ref name=behaviours&gt;[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html ''Wall Street Journal'', &#8220;On the Web's Cutting Edge, Anonymity in Name Only&#8221;], August 4, 2010&lt;/ref&gt; According to a study done by Compass, e-commerce websites that use personalization can see an increase in revenue of as much as 29%. &lt;ref name=29%&gt;[http://blog.compass.co/improving-ecommerce-retention-revenue-personalization/ ''Compass Blog'', &#8220;Improving Ecommerce Retention and Revenue with Personalization&#8221;], August 11, 2016&lt;/ref&gt;

There are many categories of web personalization including
# Behavioral
# Contextual
# Technical
# Historic data
# Collaboratively filtered

There are several camps in defining and executing web personalization.  A few broad methods for web personalization may include:
# Implicit
# Explicit
# Hybrid

With implicit personalization, the web personalization is performed based on the different categories mentioned above. It can also be learned from direct interactions with the user based on implicit data, such as items purchased or pages viewed.&lt;ref&gt;{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}&lt;/ref&gt; With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system. Hybrid personalization combines the above two approaches to leverage the ''best of both worlds''.

Web personalization is can be linked to the notion of [[Adaptive hypermedia]] (AH). The main difference is that the former would usually work on what is considered an "open corpus hypermedia," whilst the latter would traditionally work on "closed corpus hypermedia." However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.

Personalization is also being considered for use in less overtly commercial applications to improve the user experience online.&lt;ref&gt;[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March &#8211; 3 April 2004. Archives &amp; Museum Informatics, pages 63&#8211;78, 2004.&lt;/ref&gt; Internet activist [[Eli Pariser]] has documented that search engines like [[Google]] and [[Yahoo! News]] give different results to different people (even when logged out). He also points out social media site [[Facebook]] changes user's friend feeds based on what it thinks they want to see. Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.

On an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role. The term "customization" in this context refers to the ability of users to modify the page layout or specify what content should be displayed.

==Digital media==
Another aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards. One such example is Ordnance Survey Open Data.&lt;ref&gt;{{cite news| url=https://www.theguardian.com/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}&lt;/ref&gt; Data made available in this way is structured to allow it to be inter-connected and re-used by third parties.&lt;ref&gt;{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}&lt;/ref&gt;

Data available from a user's personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].

Current [[open data]] standards on the Web include:
# [[Attention Profiling Mark-up Language]] (APML)
# DataPortability
# [[OpenID]]
# [[OpenSocial]]

== Mobile phones ==

Over time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.&lt;ref&gt;May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.&lt;/ref&gt;

==Print media==
{{main|Mail merge}}

In print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients' information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient's demographics or interests using fields within the database, such as "first name", "last name", "company", etc.

The term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing|variable data printing (VDP)]]. This allows for full image and text variability within a printed book.
With the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.

== Promotional merchandise ==
Promotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children's storybooks&#8212;wherein the child becomes the [[protagonist]], with the name and image of the child personalized&#8212;are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained.

== 3D printing ==
3D printing is a production method that allows to create unique and personalized items on a global scale. Personalized apparel and accessories, such as jewellery, are increasing in popularity.&lt;ref&gt;{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&amp;h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015}}&lt;/ref&gt; This kind of customization is also relevant in other areas like Consumer Electronics&lt;ref&gt;{{Cite web|url=http://www.3ders.org/articles/20160121-philips-launches-worlds-first-personalized-3d-printed-face-shaver-for-limited-edition-run.html|title=Philips launches the world's first personalized, 3D printed face shaver for limited edition run|website=3ders.org|language=en-US|access-date=2016-03-02}}&lt;/ref&gt; and Retail.&lt;ref&gt;{{Cite web|url=http://twikblog.twikit.com/belgian-3d-company-twikit-brings-3d-customization-french-retail/|title=Twikit brings 3D customization to French retail.|website=Twikit Blog {{!}} 3D Customization, 3D Printing|language=en-US|access-date=2016-03-02}}&lt;/ref&gt; By combining 3D printing with complex software a product can easily be customized by an end-user.

== Mass personalization ==

{{tone|section|date=January 2011}}

Mass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.&lt;ref&gt;{{cite web|url=http://www.answers.com/personalization&amp;r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}&lt;/ref&gt; From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers' individual specific needs with manufacturers' customization capabilities.&lt;ref&gt;	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167&lt;/ref&gt; The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.&lt;ref&gt;Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.&lt;/ref&gt;

A website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.&lt;ref&gt;{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}&lt;/ref&gt;

[[Behavioral targeting]] represents a concept that is similar to mass personalization.

== Predictive personalization ==

Predictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.&lt;ref&gt;{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}&lt;/ref&gt;  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.

== Map personalization ==
{{Expand section|date=September 2015}}Digital [[Web mapping|web maps]] are also being personalized. [[Google Maps]] change the content of the map based on previous searches and other profile information.&lt;ref&gt;{{Cite web|title = The Next Frontier For Google Maps Is Personalization|url = http://social.techcrunch.com/2013/02/01/the-next-frontier-for-google-maps-is-personalization/|website = TechCrunch|accessdate = 2015-09-13|first = Frederic|last = Lardinois}}&lt;/ref&gt; Technology writer [[Evgeny Morozov]] has criticized map personalization as a threat to [[public space]].&lt;ref&gt;{{Cite news|title = My Map or Yours?|url = http://www.slate.com/articles/technology/future_tense/2013/05/google_maps_personalization_will_hurt_public_space_and_engagement.html|newspaper = Slate|date = 2013-05-28|access-date = 2015-09-13|issn = 1091-2339|language = en|first = Evgeny|last = Morozov}}&lt;/ref&gt;

==See also==
* [[Adaptation (computer science)]]
* [[Mass customization]]
* [[Adaptive hypermedia]]
* [[Behavioral targeting]]
* [[Bespoke]]
* [[Collaborative filtering]]
* [[Configurator]]
* [[Personalized learning]]
* [[Preorder economy]]
* [[Real-time marketing]]
* [[Recommendation system]]
* [[User modeling]]

==References==
{{reflist|2}}

==External links==
* [http://www.iimcp.org International Institute on Mass Customization &amp; Personalization which organizes MCP, a biannual conference on customization and personalization]
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''

[[Category:Human&#8211;computer interaction]]
[[Category:World Wide Web]]
[[Category:User interface techniques]]
[[Category:Usability|Personas]]
[[Category:Types of marketing]]
[[Category:Information retrieval techniques]]</text>
      <sha1>4o61uootwufctou2edfvdobhvups9g3</sha1>
    </revision>
  </page>
  <page>
    <title>Preference learning</title>
    <ns>0</ns>
    <id>34072838</id>
    <revision>
      <id>761915121</id>
      <parentid>747619165</parentid>
      <timestamp>2017-01-25T15:47:54Z</timestamp>
      <contributor>
        <ip>2A02:8109:8A40:54A4:30F5:4A84:F46D:95A2</ip>
      </contributor>
      <comment>made the english clearer</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9148" xml:space="preserve">'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.&lt;/ref&gt; In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.

While the concept of preference learning has been emerged for some time in many fields such as [[economics]],&lt;ref name="SHOG00" /&gt; it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.&lt;ref name="WEB:WORKSHOP" /&gt;

==Tasks==

The main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':&lt;ref name="FURN11" /&gt;

===Label ranking===

In label ranking, the model has an instance space &lt;math&gt;X=\{x_i\}\,\!&lt;/math&gt; and a finite set of labels &lt;math&gt;Y=\{y_i|i=1,2,\cdots,k\}\,\!&lt;/math&gt;. The preference information is given in the form &lt;math&gt;y_i \succ_{x} y_j\,\!&lt;/math&gt; indicating instance &lt;math&gt;x\,\!&lt;/math&gt; shows preference in &lt;math&gt;y_i\,\!&lt;/math&gt; rather than &lt;math&gt;y_j\,\!&lt;/math&gt;. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.

It was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:&lt;ref name="HARP03" /&gt; if a training instance &lt;math&gt;x\,\!&lt;/math&gt; is labeled as class &lt;math&gt;y_i\,\!&lt;/math&gt;, it implies that &lt;math&gt;\forall j \neq i, y_i \succ_{x} y_j\,\!&lt;/math&gt;. In the [[Multi-label classification|multi-label]] case, &lt;math&gt;x\,\!&lt;/math&gt; is associated with a set of labels &lt;math&gt;L \subseteq Y\,\!&lt;/math&gt; and thus the model can extract a set of preference information &lt;math&gt;\{y_i \succ_{x} y_j | y_i \in L, y_j \in Y\backslash L\}\,\!&lt;/math&gt;. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.

===Instance ranking===

Instance ranking also has the instance space &lt;math&gt;X\,\!&lt;/math&gt; and label set &lt;math&gt;Y\,\!&lt;/math&gt;. In this task, labels are defined to have a fixed order &lt;math&gt;y_1 \succ y_2 \succ \cdots \succ y_k\,\!&lt;/math&gt; and each instance &lt;math&gt;x_l\,\!&lt;/math&gt; is associated with a label &lt;math&gt;y_l\,\!&lt;/math&gt;. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.

===Object ranking===

Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form &lt;math&gt;x_i \succ x_j\,\!&lt;/math&gt; and the model should find out a ranking order among instances.

==Techniques==

There are two practical representations of the preference information &lt;math&gt;A \succ B\,\!&lt;/math&gt;. One is assigning &lt;math&gt;A\,\!&lt;/math&gt; and &lt;math&gt;B\,\!&lt;/math&gt; with two real numbers &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt; respectively such that &lt;math&gt;a &gt; b\,\!&lt;/math&gt;. Another one is assigning a binary value &lt;math&gt;V(A,B) \in \{0,1\}\,\!&lt;/math&gt; for all pairs &lt;math&gt;(A,B)\,\!&lt;/math&gt; denoting whether &lt;math&gt;A \succ B\,\!&lt;/math&gt; or &lt;math&gt;B \succ A\,\!&lt;/math&gt;. Corresponding to these two different representations, there are two different techniques applied to the learning process.

===Utility function===

If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function &lt;math&gt;f: X \times Y \rightarrow \mathbb{R}\,\!&lt;/math&gt; such that &lt;math&gt;y_i \succ_x y_j \Rightarrow f(x,y_i) &gt; f(x,y_j)\,\!&lt;/math&gt;. For instance ranking and object ranking, the mapping is a function &lt;math&gt;f: X \rightarrow \mathbb{R}\,\!&lt;/math&gt;.

Finding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.

===Preference relations===

The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. F&#252;rnkranz, Johannes and H&#252;llermeier proposed this approach in label ranking problem.&lt;ref name="FURN03" /&gt; For object ranking, there is an early approach by Cohen et al.&lt;ref name="COHE98" /&gt;

Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.&lt;ref name="FURN03" /&gt;

==Uses==

Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.&lt;ref name="LIU09" /&gt;

Another application of preference learning is [[recommender systems]].&lt;ref name="GEMM09" /&gt; Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.

==See also==
*[[Learning to rank]]

==References==

{{Reflist|
refs=

&lt;ref name="SHOG00"&gt;{{
cite journal
|last       = Shogren
|first      = Jason F. |author2=List, John A. |author3=Hayes, Dermot J.
|year       = 2000
|title      = Preference Learning in Consecutive Experimental Auctions
|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm
|journal    = American Journal of Agricultural Economics
|volume     = 82
|pages      = 1016&#8211;1021
|doi=10.1111/0002-9092.00099
}}&lt;/ref&gt;

&lt;ref name="WEB:WORKSHOP"&gt;{{
cite web
|title      = Preference learning workshops
|url        = http://www.preference-learning.org/#Workshops
}}&lt;/ref&gt;

&lt;ref name="FURN11"&gt;{{
cite book
|last       = F&amp;uuml;rnkranz
|first      = Johannes
|coauthors  = H&amp;uuml;llermeier, Eyke
|year       = 2011
|title      = Preference Learning
|url        = https://books.google.com/books?id=nc3XcH9XSgYC
|chapter    = Preference Learning: An Introduction
|chapterurl = https://books.google.com/books?id=nc3XcH9XSgYC&amp;pg=PA4
|publisher  = Springer-Verlag New York, Inc.
|pages      = 3&#8211;8
|isbn       = 978-3-642-14124-9
}}&lt;/ref&gt;

&lt;ref name="HARP03"&gt;{{
cite journal
|last       = Har-peled
|first      = Sariel |author2=Roth, Dan |author3=Zimak, Dav
|year       = 2003
|title      = Constraint classification for multiclass classification and ranking
|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02
|pages      = 785&#8211;792
}}&lt;/ref&gt;

&lt;ref name="FURN03"&gt;{{
cite journal
|last       = F&amp;uuml;rnkranz
|first      = Johannes
|coauthors  = H&amp;uuml;llermeier, Eyke
|year       = 2003
|title      = Pairwise Preference Learning and Ranking
|journal    = Proceedings of the 14th European Conference on Machine Learning
|pages      = 145&#8211;156
}}&lt;/ref&gt;

&lt;ref name="COHE98"&gt;{{
cite journal
|last       = Cohen
|first      = William W. |author2=Schapire, Robert E. |author3=Singer, Yoram
|year       = 1998
|title      = Learning to order things
|url        = http://dl.acm.org/citation.cfm?id=302528.302736
|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems
|pages      = 451&#8211;457
}}&lt;/ref&gt;

&lt;ref name="LIU09"&gt;{{
cite journal
|last       = Liu
|first      = Tie-Yan
|year       = 2009
|title      = Learning to Rank for Information Retrieval
|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304
|journal    = Foundations and Trends in Information Retrieval
|volume     = 3
|issue      = 3
|pages      = 225&#8211;331
|doi        = 10.1561/1500000016
}}&lt;/ref&gt;

&lt;ref name="GEMM09"&gt;{{
cite journal
|last       = Gemmis
|first      = Marco De
|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni 
|year       = 2009
|title      = Preference Learning in Recommender Systems
|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45
|journal    = PREFERENCE LEARNING
|volume     = 41
|pages      = 387&#8211;407
|doi=10.1007/978-3-642-14125-6_18
}}&lt;/ref&gt;

}}

==External links==
*[http://www.preference-learning.org/ Preference Learning site]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]</text>
      <sha1>1zke4qcza7oaylt47fe8dk0e4201sj2</sha1>
    </revision>
  </page>
  <page>
    <title>Cluster labeling</title>
    <ns>0</ns>
    <id>25202953</id>
    <revision>
      <id>752038667</id>
      <parentid>751539775</parentid>
      <timestamp>2016-11-29T04:25:18Z</timestamp>
      <contributor>
        <username>Ryk72</username>
        <id>20425983</id>
      </contributor>
      <comment>Disambiguated: [[fusion]] &#8594; [[wikt:fusion]]; ce; spc</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10522" xml:space="preserve">In [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster and distinguish the clusters from each other.

==Differential cluster labeling==
Differential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html&gt;.&lt;/ref&gt;

===Pointwise mutual information===

{{Main article|Pointwise mutual information}}

In the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:

&lt;math&gt;I(X, Y) = \sum_{x\in X}{ \sum_{y\in Y} {p(x, y)log_2\left(\frac{p(x, y)}{p_1(x)p_2(y)}\right)}}&lt;/math&gt;

where ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p&lt;sub&gt;1&lt;/sub&gt;(x)'' is the probability distribution of X, and ''p&lt;sub&gt;2&lt;/sub&gt;(y)'' is the probability distribution of Y.

In the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html&gt;.&lt;/ref&gt;  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

&lt;math&gt;I(C, T) = \sum_{c\in {0, 1}}{ \sum_{t\in {0, 1}} {p(C = c, T = t)log_2\left(\frac{p(C = c, T = t)}{p(C = c)p(T = t)}\right)}}&lt;/math&gt;

In this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.

===Chi-Squared Selection===
{{Main article|Pearson's chi-squared test}}
The Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:

&lt;math&gt;X^2 = \sum_{a \in A}{\sum_{b \in B}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}&lt;/math&gt;

where ''O&lt;sub&gt;a,b&lt;/sub&gt;'' is the ''observed'' frequency of a and b co-occurring, and ''E&lt;sub&gt;a,b&lt;/sub&gt;'' is the ''expected'' frequency of co-occurrence.

In the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

&lt;math&gt;X^2 = \sum_{a \in {0,1}}{\sum_{b \in {0,1}}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}&lt;/math&gt;

For example, ''O&lt;sub&gt;1,0&lt;/sub&gt;'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E&lt;sub&gt;1,0&lt;/sub&gt;'' is the expected number of documents that are in a particular cluster but don't contain a certain term.
Our initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html&gt;.&lt;/ref&gt;

''E&lt;sub&gt;1,0&lt;/sub&gt; = N * P(C = 1) * P(T = 0)''

where N is the total number of documents in the collection.

==Cluster-Internal Labeling==
Cluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.
Cluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.

===Centroid Labels===
{{Main article|Vector space model}}
A frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.
One downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.

===Contextualized centroid labels===
A simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection.&lt;ref&gt;Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters&#8217; contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155&lt;/ref&gt;
In this approach, a term-term co-occurrence matrix referred as &lt;math&gt;T_k&lt;/math&gt; is first built for each cluster &lt;math&gt;S_k&lt;/math&gt;. Each cell represents the number of times term &lt;math&gt;i&lt;/math&gt; co-occurs with term &lt;math&gt;j&lt;/math&gt; within a certain window of text (a sentence, a paragraph, etc.)
In a second stage, a similarity matrix &lt;math&gt;T_k^{sim}&lt;/math&gt; is obtained by multiplying &lt;math&gt;T_k&lt;/math&gt; with its transpose. We have &lt;math&gt;T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})&lt;/math&gt;. Being the dot product of two normalized vectors &lt;math&gt;\tilde{t}_{i}&lt;/math&gt; and &lt;math&gt;\tilde{t}_{j}&lt;/math&gt;, &lt;math&gt;t_{{sim}_{ij}}&lt;/math&gt; denotes the cosine similarity between terms &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;. The so obtained &lt;math&gt;T_k^{sim}&lt;/math&gt; can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.

===Title labels===
An alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.

===External knowledge labels===
Cluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.&lt;ref&gt;David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146&lt;/ref&gt; In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.

=== Combining Several Cluster Labelers ===
The cluster labels of several different cluster labelers can be further combined to obtain better labels. 
For example, [[Linear Regression]] can be used to learn an optimal combination of labeler scores.&lt;ref&gt;David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146&lt;/ref&gt; A more sophisticated technique is based on a [[wikt:fusion|fusion]] approach and analysis of the cluster labels decision stability of various labelers.&lt;ref&gt;Haggai Roitman, Shay Hummel, Michal Shmueli-Scheuer. [http://dl.acm.org/citation.cfm?id=2609465 A fusion approach to cluster labeling.] SIGIR 2014: 883-886&lt;/ref&gt;

==External links==
* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]
* [http://www.cs.cmu.edu/~callan/Papers/dgo06-puck.pdf Automatically Labeling Hierarchical Clusters]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Cluster Labeling}}
[[Category:Information retrieval techniques]]</text>
      <sha1>hb72the6uunib3ub4tupkrny17f5qfm</sha1>
    </revision>
  </page>
  <page>
    <title>Collaborative filtering</title>
    <ns>0</ns>
    <id>480289</id>
    <revision>
      <id>760205549</id>
      <parentid>758697402</parentid>
      <timestamp>2017-01-15T16:20:37Z</timestamp>
      <contributor>
        <ip>46.217.7.105</ip>
      </contributor>
      <comment>/* Memory-based */ removed comma</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26610" xml:space="preserve">{{external links|date=November 2013}}
{{Use dmy dates|date=June 2013}}
{{Recommender systems}}
[[File:Collaborative filtering.gif|300px|thumb|

This image shows an example of predicting of the user's rating using [[Collaborative software|collaborative]] filtering. At first, people rate different items (like videos, images, games). After that, the system is making [[prediction]]s about user's rating for an item, which the user hasn't rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won't like the video.]]

'''Collaborative filtering''' ('''CF''') is a technique used by [[recommender system]]s.&lt;ref name="handbook"&gt;Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35&lt;/ref&gt; Collaborative filtering has two senses, a narrow one and a more general one.&lt;ref name=recommender&gt;{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will|authorlink1=Loren Terveen}}&lt;/ref&gt;  

In the newer, narrower sense, collaborative filtering is a method of making automatic [[prediction]]s (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from [[crowdsourcing|many users]] (collaborating). The underlying assumption of the collaborative filtering approach is that if a person ''A'' has the same opinion as a person ''B'' on an issue, A is more likely to have B's opinion on a different issue ''x'' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).&lt;ref&gt;[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV &amp; VOD Recommendations] {{webarchive |url=https://web.archive.org/web/20120606225352/http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations |date=6 June 2012 }}&lt;/ref&gt; Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.

In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.&lt;ref name="recommender" /&gt;  Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.

==Introduction==
The [[internet growth|growth]] of the [[Internet]] has made it much more difficult to effectively [[information extraction|extract useful information]] from all the available [[online information]]. The overwhelming amount of data necessitates  mechanisms for efficient [[information filtering]]. Collaborative filtering is one of the techniques used for dealing with this problem.

The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Collaborative filtering encompasses techniques for matching people with similar interests and making [[recommender system|recommendations]] on this basis.

Collaborative filtering algorithms often require (1) users' active participation, (2) an easy way  to represent users' interests, and (3) algorithms that are able to match people with similar interests.

Typically, the workflow of a collaborative filtering system is:
# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user's interest in the corresponding domain.
# The system matches this user's ratings against other users'  and finds the people with most "similar" tastes.
# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)
A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.

==Methodology==

[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]

Collaborative filtering systems have many forms, but many common systems can be reduced to two steps:
# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).
# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user
This falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].

Alternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:
# Build an item-item matrix determining relationships between pairs of items
# Infer the tastes of the current user by examining the matrix and matching that user's data
See, for example, the [[Slope One]] item-based collaborative filtering family.

Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.

Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].

==Types==

===Memory-based===
This approach uses user rating data to compute the similarity between users or items. This is used for making recommendations. This was an early approach used in many commercial systems. It's effective and easy to implement. Typical examples of this approach are neighbourhood-based CF and item-based/user-based top-N recommendations. For example, in user based approaches, the value of ratings user 'u' gives to item 'i' is calculated as an aggregation of some similar users' rating of the item:
:&lt;math&gt;r_{u,i} = \operatorname{aggr}_{u^\prime \in U} r_{u^\prime, i}&lt;/math&gt;

where 'U' denotes the set of top 'N' users that are most similar to user 'u' who rated item 'i'. Some examples of the aggregation function includes:
:&lt;math&gt;r_{u,i} = \frac{1}{N}\sum\limits_{u^\prime \in U}r_{u^\prime, i}&lt;/math&gt;
:&lt;math&gt;r_{u,i} = k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)r_{u^\prime, i}&lt;/math&gt;
:&lt;math&gt;r_{u,i} = \bar{r_u} +  k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)(r_{u^\prime, i}-\bar{r_{u^\prime}} )&lt;/math&gt;

where k is a normalizing factor defined as &lt;math&gt;k =1/\sum_{u^\prime \in U}|\operatorname{simil}(u,u^\prime)| &lt;/math&gt;. and &lt;math&gt;\bar{r_u}&lt;/math&gt; is the average rating of user u for all the items rated by u.

The neighborhood-based algorithm calculates the similarity between two users or items produces a prediction for the user by taking the [[weighted average]] of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple measures, such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.

The Pearson correlation similarity of two users x, y is defined as 
:&lt;math&gt; \operatorname{simil}(x,y) = \frac{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})(r_{y,i}-\bar{r_y})}{\sqrt{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})^2\sum\limits_{i \in I_{xy}}(r_{y,i}-\bar{r_y})^2}} &lt;/math&gt;

where I&lt;sub&gt;xy&lt;/sub&gt; is the set of items rated by both user x and user y.

The cosine-based approach defines the cosine-similarity between two users x and y as:&lt;ref name="Breese1999"&gt;John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id=231&amp;proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998 {{webarchive |url=https://web.archive.org/web/20131019134152/http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id=231&amp;proceeding_id=14 |date=19 October 2013 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{simil}(x,y) = \cos(\vec x,\vec y) = \frac{\vec x \cdot \vec y}{||\vec x|| \times ||\vec y||} = \frac{\sum\limits_{i \in I_{xy}}r_{x,i}r_{y,i}}{\sqrt{\sum\limits_{i \in I_{x}}r_{x,i}^2}\sqrt{\sum\limits_{i \in I_{y}}r_{y,i}^2}}&lt;/math&gt;

The user based top-N recommendation algorithm uses a similarity-based vector model to identify the k most similar users to an active user. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.

The advantages with this approach include: the explainability of the results, which is an important aspect of recommendation systems; easy creation and use; easy facilitation of new data; content-independence of the items being recommended; good scaling with co-rated items.

There are also several disadvantages with this approach. Its performance decreases when [[sparsity|data gets sparse]], which occurs frequently with web-related items. This hinders the [[scalability]] of this approach and creates problems with large datasets. Although it can efficiently handle new users because it relies on a [[data structure]], adding new items becomes more complicated since that representation usually relies on a specific [[vector space]]. Adding new items requires inclusion of the new item and the re-insertion of all the elements in the structure.

===Model-based===
Models are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], multiple multiplicative factor, [[latent Dirichlet allocation]] and [[Markov decision process]] based models.&lt;ref name="Suetal2009"&gt;Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.&lt;/ref&gt;

This approach has a more holistic goal to uncover latent factors that explain observed ratings.&lt;ref&gt;[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] {{webarchive |url=https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 |date=23 October 2010 }}&lt;/ref&gt; Most of the models are based on creating a classification or clustering technique to identify the user based on the training set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].

There are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.

The disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.

===Hybrid===
A number of applications combine the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches and improve prediction performance. Importantly, they overcome the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.&lt;ref&gt;{{cite journal | url = http://www.sciencedirect.com/science/article/pii/S0020025512002587 | doi=10.1016/j.ins.2012.04.012 | volume=208 | title=Kernel-Mapping Recommender system algorithms | journal=Information Sciences | pages=81&#8211;104}}
&lt;/ref&gt; Usually most commercial recommender systems are hybrid, for example, the Google news recommender system.&lt;ref&gt;{{cite web|url=http://dl.acm.org/citation.cfm?id=1242610|title=Google news personalization|publisher=}}&lt;/ref&gt;

==Application on social web==
Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.&lt;ref&gt;[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]&lt;/ref&gt;

One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Reddit]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.

Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.

===Problems===
A collaborative filtering system does not necessarily succeed in automatically matching content to one's preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user's preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.

==Challenges==

===Data sparsity===
In practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.

One typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users' past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.

Similarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.

===Scalability===
As the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers &lt;math&gt;O(M)&lt;/math&gt; and millions of items &lt;math&gt;O(N)&lt;/math&gt;, a CF algorithm with the complexity of &lt;math&gt;n&lt;/math&gt; is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.&lt;ref name="twitterwtf"&gt;Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web&lt;/ref&gt;

===Synonyms===
[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.

For example, the seemingly different items "children movie" and "children film" are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the [[Latent Dirichlet Allocation]] technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}

===Gray sheep===
Gray sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.

===Shilling attacks===
In a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.

===Diversity and the long tail===
Collaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."&lt;ref&gt;{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984 | doi = 10.1287/mnsc.1080.0974 }}&lt;/ref&gt; Several collaborative filtering algorithms have been developed to promote diversity and the "[[long tail]]" by recommending novel, unexpected,&lt;ref&gt;{{cite journal| last1= Adamopoulos | first1= Panagiotis | first2= Alexander |last2= Tuzhilin | title=On Unexpectedness in Recommender Systems: Or How to Better Expect the Unexpected|journal=ACM Transactions on Intelligent Systems and Technology |date=January 2015|url=http://dl.acm.org/citation.cfm?id=2559952 | doi = 10.1145/2559952}}&lt;/ref&gt; and serendipitous items.&lt;ref&gt;{{cite journal| last1= Adamopoulos | first1= Panagiotis | title=Beyond rating prediction accuracy: on new perspectives in recommender systems|journal=Proceedings of the 7th ACM conference on Recommender systems |date=October 2013|url=http://dl.acm.org/citation.cfm?id=2508073| doi = 10.1145/2507157.2508073}}&lt;/ref&gt;

==Innovations==
{{Prose|date=May 2012}}
* New algorithms have been developed for CF as a result of the [[Netflix prize]].
* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.
* [[Robust collaborative filtering]], where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.&lt;ref&gt;{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}&lt;/ref&gt;

==See also==
{{div col|3}}
* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]
* [[Cold start]]
* [[Collaborative model]]
* [[Collaborative search engine]]
* [[Collective intelligence]]
* [[Customer engagement]]
* [[Delegative Democracy]], the same principle applied to voting rather than filtering
* [[Enterprise bookmarking]]
* [[Firefly (website)]], a defunct website which was based on collaborative filtering
* [[Filter bubble]]
* [[Preference elicitation]]
* [[Recommendation system]]
* [[Relevance (information retrieval)]]
* [[Reputation system]]
* [[Robust collaborative filtering]]
* [[Similarity search]]
* [[Slope One]]
* [[Social translucence]]
{{div col end}}

==References==
{{Reflist|30em}}

==External links==
*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf ''Beyond Recommender Systems: Helping People Help Each Other''], page 12, 2001
*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.
*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005
*[https://web.archive.org/web/20060527214435/http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems] ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])
*[http://www.grouplens.org/publications.html GroupLens research papers].
*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&amp;nbsp;187&#8211;192, Edmonton, Canada, July 2002.
*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]
*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]
*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M
*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web
*[https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)
*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]
*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]
*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]

{{Authority control}}

{{DEFAULTSORT:Collaborative Filtering}}
[[Category:Collaboration]]
[[Category:Collaborative software]]
[[Category:Collective intelligence]]
[[Category:Information retrieval techniques]]
[[Category:Recommender systems]]
[[Category:Social information processing]]</text>
      <sha1>fdv7lym2htjpz0kqr5bcqk5d7au19pk</sha1>
    </revision>
  </page>
  <page>
    <title>Webometrics</title>
    <ns>0</ns>
    <id>703145</id>
    <revision>
      <id>724066476</id>
      <parentid>715051625</parentid>
      <timestamp>2016-06-06T23:01:18Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* Bibliography */refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4691" xml:space="preserve">{{For|Webometrics Ranking of World Universities|Webometrics Ranking of World Universities}}
{{refimprove|date=May 2014}}
The science of '''webometrics''' (also '''cybermetrics''') tries to measure the [[World Wide Web]] to get knowledge about the number and types of [[hyperlink]]s, structure of the World Wide Web and usage patterns. According to Bj&#246;rneborn and Ingwersen (2004), the definition of '''webometrics''' is "the study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [[Bibliometrics|bibliometric]] and [[informetrics|informetric]] approaches." The term ''webometrics'' was first coined by Almind and Ingwersen (1997). A second definition of webometrics has also been introduced, "the study of web-based content with primarily quantitative methods for social science research goals using techniques that are not specific to one field of study" (Thelwall, 2009), which emphasizes the development of applied methods for use in the wider social sciences. The purpose of this alternative definition was to help publicize appropriate methods outside of the information science discipline rather than to replace the original definition within information science.

Similar scientific fields are [[Bibliometrics]], [[Informetrics]], [[Scientometrics]], [[Virtual ethnography]], and [[Web mining]].
[[File:Site based graph relationship.jpg|thumb|Site based graph relationship. The idea was taken from paper "Web-communicator creation costs sharing problem as a cooperative game"{{sfn|Mazalov|Pechnikov|Chirkov|Chuyko|2010|p=189}}]]

One relatively straightforward measure is the "Web Impact Factor" (WIF) introduced by Ingwersen (1998). The WIF measure may be defined as the number of web pages in a web site receiving links from other web sites, divided by the number of web pages published in the site that are accessible to the crawler. However the use of WIF has been disregarded due to the mathematical artifacts derived from power law distributions of these variables. Other similar indicators using size of the institution instead of number of webpages have been proved more useful.

== See also ==
* [[Altmetrics]]
* [[Impact factor]]
* [[PageRank]]
* [[Network mapping]]
* [[Search engine]]
* [[Webometrics Ranking of World Universities]]

== References ==
&lt;references /&gt;

== Bibliography ==

* {{cite journal |author1=Tomas C. Almind  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 1997 | title = Informetric analyses on the World Wide Web: Methodological approaches to 'webometrics' | journal = Journal of Documentation | volume = 53 | issue = 4 | pages = 404&#8211;426 | doi = 10.1108/EUM0000000007205}}
* {{cite journal |author1=Lennart Bj&#246;rneborn  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 2004 | title = Toward a basic framework for webometrics | journal = Journal of the American Society for Information Science and Technology | volume = 55 | issue = 14 | pages = 1216&#8211;1227 | url = http://www3.interscience.wiley.com/cgi-bin/abstract/109594194/ABSTRACT | doi = 10.1002/asi.20077}}
*{{cite journal |author = Peter Ingwersen | year = 1998 |title = The calculation of web impact factors | journal = Journal of Documentation |volume = 54 |issue = 2 | pages = 236&#8211;243 |doi = 10.1108/EUM0000000007167}}
*{{cite journal |author1=Mike Thelwall |author2=Liwen Vaughan |author3=Lennart Bj&#246;rneborn | year = 2005 |title = Webometrics | journal = Annual Review of Information Science and Technology |volume = 39 | pages = 81&#8211;135 |doi = 10.1002/aris.1440390110}}
* {{cite book |author=Mike Thelwall |title= Introduction to Webometrics: Quantitative Web Research for the Social Sciences |publisher= Morgan &amp; Claypool |year= 2009 |isbn= 978-1-59829-993-9 |url=http://www.morganclaypool.com/doi/abs/10.2200/S00176ED1V01Y200903ICR004}}
* {{cite conference 
|url            = http://www.mtas.ru/upload/library/UBS30112.pdf
|title          = Web-communicator creation costs sharing problem as a cooperative game (in Russian)
|last1=Mazalov |first1= Vladimir
|last2=Pechnikov |first2=Andrey
|last3=Chirkov |first3=Alexandr
|last4=Chuyko |first4=Julia
|year           = 2010
|booktitle      = &#1059;&#1087;&#1088;&#1072;&#1074;&#1083;&#1077;&#1085;&#1080;&#1077; &#1073;&#1086;&#1083;&#1100;&#1096;&#1080;&#1084;&#1080; &#1089;&#1080;&#1089;&#1090;&#1077;&#1084;&#1072;&#1084;&#1080;: &#1089;&#1073;&#1086;&#1088;&#1085;&#1080;&#1082; &#1090;&#1088;&#1091;&#1076;&#1086;&#1074;
|pages          = 
|location       = 
|ref = harv
}}

* {{cite web
 |url        = http://eprints.rclis.org/7554/
 |title      = Webometrics: ten years of expansion
 |last       = Ingwersen
 |first      = Peter
 |year       = 2006
 |accessdate = 2013
 |ref = harv
}}

[[Category:World Wide Web]]
[[Category:Information science]]
[[Category:Information retrieval techniques]]


{{web-stub}}

[[pt:Webometria]]</text>
      <sha1>gi450mzk7ntg00b01u9r56nsoy8b2xy</sha1>
    </revision>
  </page>
  <page>
    <title>Personalized search</title>
    <ns>0</ns>
    <id>28010520</id>
    <revision>
      <id>761045176</id>
      <parentid>761045145</parentid>
      <timestamp>2017-01-20T15:23:38Z</timestamp>
      <contributor>
        <ip>168.184.14.110</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33188" xml:space="preserve">{{Multiple issues|
{{essay-like|date=January 2015}}
{{original research|date=January 2015}}
}}

'''Personalized search''' refers to [[web search]] experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to [[personalizing]] search results, one involving modifying the user's query and the other re-ranking search results.&lt;ref&gt;{{cite journal|last=Pitokow|first=James|author2=Hinrich Sch&#252;tze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM |year=2002|volume=45|issue=9|pages=50&#8211;55|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}&lt;/ref&gt;

==History==

[[Google]] introduced personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search implemented for all users, not only those with a Google account. There is not very much information on how exactly Google personalizes their searches; however, it is believed that they use user language, location, and [[web history]].&lt;ref&gt;{{cite conference | url=http://personalization.ccs.neu.edu/paper.pdf | title=Measuring Personalization of Web Search | year=2013 | archiveurl=https://web.archive.org/web/20130425195202/http://personalization.ccs.neu.edu/paper.pdf | archivedate=April 25, 2013 | deadurl=y|author1=Aniko Hannak|author2=Piotr Sapiezynski|author3=Arash Molavi Kakhki|author4=Balachander Krishnamurthy|author5=David Lazer|author6=Alan Mislove|author7=Christo Wilson}}&lt;/ref&gt;

Early [[search engine]]s, like [[Google]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by Google, has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."&lt;ref name=Remerowski&gt;{{cite AV media| last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}&lt;/ref&gt; Using mathematical algorithms, search engines are now able to return results based on the number of links to and from sites; the more links a site has, the higher it is placed on the page.&lt;ref name=Remerowski/&gt; Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.&lt;ref name=Simpson&gt;{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969&#8211;982}}&lt;/ref&gt; If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results indicating that those near the top are more relevant to a user's wants than those below.&lt;ref name=Simpson/&gt;

While many search engines take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.&lt;ref&gt;{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237&#8211;251|doi=10.1002/asi.20477}}&lt;/ref&gt; However, user-supplied information can be difficult to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.&lt;ref&gt;{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287&#8211;296}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581&#8211;590}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Shen|first=X. |author2=Tan, B. |author3=Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824&#8211;831}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675&#8211;684}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415&#8211;422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}&lt;/ref&gt;

There are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization&lt;ref&gt;{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}&lt;/ref&gt;). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. If a user accesses the same site through a search result from Google many times, it believes that they like that page. So when users carry out certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if a user is signed out, Google may personalize their results because it keeps a 180-day record of what a particular web browser has searched for, linked to a cookie in that browser.&lt;ref&gt;{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}&lt;/ref&gt;

In order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University compared an aggregate set of searches from logged in users against a [[control group]]. The research team found that 11.7% of results show differences due to personalization; however, this varies widely by [[Web search query|search query]] and result ranking position.&lt;ref name=Briggs&gt;{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|date=24 June 2013|accessdate=21 April 2014}}&lt;/ref&gt; Of various factors tested, the two that had measurable impact were being logged in with a Google account and the [[IP address]] of the searching users. It should also be noted that results with high degrees of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if a user searched for "used car sales", Google may produce results of local car dealerships in their area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.&lt;ref name=Briggs/&gt;

When measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when a user performs a search and follow it with a subsequent search, the results of the second search is influenced by the first search. A noteworthy point is that the top-ranked [[URL]]s are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.&lt;ref name=Briggs/&gt;

==The filter bubble==
{{Main article|Filter bubble}}

Several concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by [[bias]]ing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome". As a result, people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).&lt;ref name=Pariser&gt;{{cite book| url=http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf | title=The Filter Bubble | archiveurl=https://web.archive.org/web/20131228150122/http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf|archivedate=December 28, 2013|author=E. Pariser|year=2011}}&lt;/ref&gt;

The methods of personalization, and how useful it is to "promote" certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the filter bubble is created. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.&lt;ref&gt;{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal of Pattern Recognition &amp; Artificial Intelligence |year=2007|pages=183&#8211;205}}&lt;/ref&gt;

An area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information ([[selective exposure]]). This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.&lt;ref name=Pariser/&gt; While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.&lt;ref name="Consumer Watchdog"&gt;{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|date=2 June 2010|accessdate= 27 April 2014|work=Consumer Watchdog}}&lt;/ref&gt;

Many search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.&lt;ref&gt;{{cite journal|last=Wai-Tin|first=Kenneth|author2=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEEE Transactions on Knowledge and Data Engineering|year=2010|volume=22|issue=7|pages=969&#8211;982|doi=10.1109/tkde.2009.144}}&lt;/ref&gt;

The feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.&lt;ref&gt;[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.&lt;/ref&gt; An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.&lt;ref name=Pariser/&gt;

Some have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].&lt;ref&gt;{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}&lt;/ref&gt;

==The case of Google==
{{Main article|Google Personalized Search}}

An important example of search personalization is [[Google]]. There are a host of Google applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived of a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.&lt;ref&gt;{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20&#8211;31}}&lt;/ref&gt; that keeps track of all the information directly under one's name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the ''[[New York Times]]''. The high level of personalization that was available with Google played a significant part in helping remain the world's most favorite search engine.

One example of Google's ability to personalize searches is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information one want. The concern, however, is that the very important information can be held back because it does not match the criteria that the program sets for the particular user. This can create the "[[filter bubble]]" as described earlier.&lt;ref name=Pariser/&gt;

An interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.
Google's popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although having Google personalize search results based on what users searched previously may have its benefits, there are negatives that come with it.&lt;ref&gt;{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}&lt;/ref&gt;
With the power from this information, Google has chosen to enter other sectors it owned, such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.

Using search personalization, Google has doubled its video market share to about eighty percent. The legal definition of a [[monopoly]] is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.

The analytical firm Experian Hitwise stated that since 2007, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from 31% in 2007 to 10% in 2010 and Yahoo Images has gone from 12% to 7%. It becomes apparent that the decline of these companies has come because of Google's increase in market share from 43% in 2007 to about 55% in 2009.

It can be said that Google is more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google's influence because their numbers went from 1.3 million unique visitors to 11.9 unique visitors in one month. That kind of growth can only come with the change of a process.

In the end, there are two common themes with all of these graphs. The first is that Google's market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around 2007, which is around the time that Google began to use its "Universal Search" method.&lt;ref&gt;{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}&lt;/ref&gt;

==Benefits==

One of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human ability to process information has not expanded much.&lt;ref name=Diehl&gt;{{cite journal|author=Diehl, K.|year=2003|title=Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making|journal=Advances In Consumer Research|volume=30|issue=1|pages=166&#8211;169}}&lt;/ref&gt; When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show a [[positive correlation]] between personalized search and the quality of consumers' decisions.

The first study was conducted by Kristin Diehl from the [[University of South Carolina]]. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that 'consumers make worse choices because lower search costs cause them to consider inferior options.' It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.&lt;ref name=Diehl/&gt; The study by Gerald Haubl from the [[University of Alberta]] and Benedict G.C. Dellaert from [[Maastricht University]] mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers' decision quality and reduced the number of products inspected.&lt;ref name=Diehl/&gt;

==Models==

Personalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.&lt;ref&gt;{{cite book|author1=Coyle, M.|author2=Smyth, B.|lastauthoramp=y|year=2007|chapter=Information recovery and discovery in collaborative web search|title=Advances in Information Retrieval|pp=356&#8211;367|doi=10.1007/978-3-540-71496-5_33|isbn=978-3-540-71494-1|series=Lecture Notes in Computer Science}}&lt;/ref&gt; Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.

The first model available is based on the users' historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.

There is another way to personalize search results. In Bracha Shapira and Boaz Zabar's "Personalized Search: Integrating Collaboration and Social Networks", Shapira and Zabar focused on a model that utilizes a [[recommendation system]].&lt;ref&gt;{{cite journal|author1=Shapira, B.|author2=Zabar, B.|lastauthoramp=y|year=2011|title=Personalized search: Integrating collaboration and social networks|journal=Journal of the American Society for Information Science &amp; Technology|volume=62|issue=1|pages=146&#8211;160|doi=10.1002/asi.21446}}&lt;/ref&gt; This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.

Recent paper &#8220;[https://arxiv.org/abs/1612.03597 Search personalization with embeddings]&#8221; shows that a new embedding model for search personalization, where users are embedded on a topical interest space, produces better search results than strong learning-to-rank models.

==Disadvantages==

While there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users' search engine results to material that aligns with the users' interests and history. It limits the users' ability to become exposed to material that would be relevant to the user's search query but due to the fact that some of this material differs from the user's interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. "Objectivity matters little when you know what you are looking for, but its lack is problematic when you do not".&lt;ref&gt;{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426&#8211;445|doi=10.1111/j.1467-9973.2012.01759.x}}&lt;/ref&gt; Another criticism of search personalization is that it limits a core function of the web: the collection and sharing of information. Search personalization prevents users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user's search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue, the user's search results will reflect that. The user may not be shown both sides of the issue and miss potentially important information if the user's interests lean to one side or another. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users, even though each user entered the same search query. According to Bates, "only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on".&lt;ref&gt;{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|journal=Online|volume=35|issue=6|pages=64}}&lt;/ref&gt; If search personalization was not active, all the results in theory should have been the same stories in an identical order.

Another disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling their users' internet interests and histories to other companies. This raises a privacy issue concerning whether people are comfortable with companies gathering and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.

==Sites that use it==

E. Pariser, author of ''The Filter Bubble'', explains how there are differences that search personalization has on both [[Facebook]] and Google. Facebook implements personalization when it comes to the amount of things people share and what pages they "like". An individual's [[social interaction]]s, whose profile they visit the most, who they message or chat with are all indicators that are used when Facebook uses personalization. Rather than what people share being an indicator of what is filtered out, Google takes into consideration what we "click" to filter out what comes up in our searches. In addition, Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and users share what other people want to see. Even while [[tag (metadata)|tag]]ging photographs, Facebook uses personalization and [[face recognition]] that will automatically assign a name to face. In terms of Google, users are provided similar websites and resources based on what they initially click on. There are even other websites that use the filter tactic to better adhere to user preferences. For example, [[Netflix]] also judges from the users search history to suggest movies that they may be interested in for the future. There are sites like [[Amazon.com|Amazon]] and personal [[shopping site]]s also use other peoples history in order to serve their interests better. [[Twitter]] also uses personalization by "suggesting" other people to follow. In addition, based on who one "follows", "tweets" and "retweets" at, Twitter filters out suggestions most relevant to the user.  [[Mark Zuckerberg]], founder of Facebook, believed that people only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn't true. Although personalized search may seem helpful, it is not a very accurate representation of any person. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles. There are many sites being used for different purposes and that do not make up one person's [[Personal identity|identity]] at all, but provide false representations instead.&lt;ref name=Pariser/&gt;

==Online shopping==
{{main article|Online shopping}}
Search engines such as Google and Yahoo! utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual's web clicks, search engines can use personalized search to put advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in [[brick-and-mortar]] stores. These types of products and services are called long tail items.&lt;ref&gt;{{cite journal|author=Badke, William|title=Personalization and Information Literacy|journal=Online|volume=36|issue=1|page=47|date=February 2012}}&lt;/ref&gt; Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.

Aside from aiding consumers and businesses in finding one another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.&lt;ref"Consumer Watchdog"/&gt; In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word "shoes" using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer's queries.

==References==
{{reflist|30em}}

{{DEFAULTSORT:Personalized search}}
[[Category:Information retrieval techniques]]
[[Category:Internet search engines|*Personalized search]]
[[Category:Internet terminology]]
[[Category:Personalized search| ]]</text>
      <sha1>fl52676pccm6tci3b5zmtkds6s1gnrc</sha1>
    </revision>
  </page>
  <page>
    <title>Adversarial information retrieval</title>
    <ns>0</ns>
    <id>11486091</id>
    <revision>
      <id>686895664</id>
      <parentid>686778986</parentid>
      <timestamp>2015-10-22T00:57:49Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <comment>[[WP:CHECKWIKI]] error fix #95. Editor's signature or link to user space. Do [[Wikipedia:GENFIXES|general fixes]] and cleanup if needed. - using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3216" xml:space="preserve">'''Adversarial information retrieval''' ('''adversarial IR''') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.

On the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], [[click fraud]],&lt;ref&gt;Jansen, B. J. (2007) [https://faculty.ist.psu.edu/jjansen/academic/jansen_click_fraud.pdf Click fraud]. IEEE Computer. 40(7), 85-86.&lt;/ref&gt; and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].&lt;ref&gt;B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]&lt;/ref&gt;

Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.

== Topics ==
Topics related to Web spam (spamdexing):

* [[Link spam]]
* [[Keyword spamming]]
* [[Cloaking]]
* Malicious tagging
* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]

Other topics:
* [[Click fraud]] detection
* Reverse engineering of  [[search engine]]'s [[ranking]] algorithm
* Web [[content filtering]]
* [[Ad filtering|Advertisement blocking]]
* Stealth [[web crawling|crawling]]
*[[Troll (Internet)]]
* Malicious tagging or voting in [[social networks]]
* [[Astroturfing]]
* [[Sockpuppetry]]

== History ==
The term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.&lt;ref&gt;D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]&lt;/ref&gt;

== See also ==
*[[Spamdexing]]
*[[Information retrieval]]

== References ==
{{reflist}}

== External links ==
*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web
*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection
*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection

{{DEFAULTSORT:Adversarial Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Internet fraud]]</text>
      <sha1>2aitq56yh0wqp2fqf6md6sqxtskjz4p</sha1>
    </revision>
  </page>
  <page>
    <title>Desktop search</title>
    <ns>0</ns>
    <id>1274156</id>
    <revision>
      <id>748763325</id>
      <parentid>748763108</parentid>
      <timestamp>2016-11-10T06:37:06Z</timestamp>
      <contributor>
        <ip>41.130.82.43</ip>
      </contributor>
      <comment>/* Platforms &amp; their histories */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18453" xml:space="preserve">{{Multiple issues|
{{technical|date=October 2014}}
{{manual||date=October 2016}}
}}
[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]
'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.

One of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.

A variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.  Most desktop search programs are standalone applications, whereas a few also provide search capabilities in an [[integrated writing environment]] (IWE).

Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users need to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] &#8212; the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.&lt;ref&gt;{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.&lt;/ref&gt;  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.

Companies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index &#8212; but not preview &#8212; items they should not even know exist.{{Citation needed|date = November 2009}}

Historically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.

== Technologies ==
Most desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,&lt;ref&gt;{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}&lt;/ref&gt; which performs searches over only filenames &amp;mdash; not the files' contents &amp;mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,&lt;ref&gt;{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}&lt;/ref&gt; which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.

Desktop search tools typically collect three types of information about files:
* file and folder names
* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]
* file content (for supported types of documents only)

To search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.

Long-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.&lt;ref&gt;{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/video-search.html|title=The current state of video search|author=Niall Kennedy|date=17 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html|title=The current state of audio search|author=Niall Kennedy|date=15 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}&lt;/ref&gt;

The sector attracted considerable attention from the struggle between Microsoft and Google.&lt;ref&gt;{{cite web|url=http://news.bbc.co.uk/1/hi/technology/3952285.stm|title=BBC NEWS - Technology - Search wars hit desktop computers|work=bbc.co.uk|accessdate=24 June 2015}}&lt;/ref&gt; According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.&lt;ref&gt;{{cite web|url=http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/|title=SearchMax|work=goebelgroup.com|accessdate=24 June 2015}}&lt;/ref&gt;

As of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more.&lt;ref&gt;[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]&lt;/ref&gt;

Desktop search products are software alternatives to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more.,&lt;ref&gt;[http://www.brianmadden.com/blogs/brianmadden/archive/2015/03/11/what-do-you-do-for-desktop-search-in-vdi-and-rdsh.aspx  &#8222;What do you do for desktop search in VDI and RDSH?&#8220;]. Blogpost by Brian Madden on brainmadden.com. Retrieved on March 25, 2015.&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://venturebeat.com/2008/06/02/lookeen-offers-a-new-way-way-for-outlook-users-to-search/|title=Lookeen offers a new way for Outlook users to search|author=Anthony Ha|date=2 June 2008|work=VentureBeat|accessdate=8 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/|title=X1 rises again with Desktop Search 8, Virtual Edition|author=Robert L. Mitchell|date=8 May 2013|work=Computerworld|accessdate=24 June 2015}}&lt;/ref&gt;

==Platforms &amp; their histories==
There are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS &amp; [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.

=== Windows ===
Today's [[Windows Search]] replaced WDS ([[Windows Desktop Search]]). WDS, in turn, replaced [[Indexing Service]]. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"&lt;ref&gt;{{cite web|url=https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx|title=Indexing Service|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}&lt;/ref&gt; Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.&lt;ref&gt;{{cite web|url=https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx|title=Indexing with Microsoft Index Server|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}&lt;/ref&gt; This made indexing large amounts of files require extremely powerful hardware and very long wait times.

In 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.&lt;ref&gt;{{cite web|url=http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|archiveurl=https://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|title=Windows Search: Technical FAQ|archivedate=24 September 2011|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}&lt;/ref&gt; Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.

With the release of [[Windows Vista]] came Windows Search 3.1. Unlike its predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the [[RAM]] and [[CPU]] requirements were greatly reduced, cutting back indexing times immensely. Windows Search 4.0 is currently running on all PCs with [[Windows 7]] and up.

=== Mac OS ===
Mac OS was the first to implement Desktop Search with its [[AppleSearch]] search engine, allowing users to fully search all documents within their Macintosh computer, including file format types, meta-data on those files, and content within the files. AppleSearch was a [[Client&#8211;server model|client/server application]], and as such required a server separate from the main device in order to function. The biggest issue with AppleSearch were its large resource requirements: "AppleSearch requires at least a 68040 processor and 5MB of RAM."&lt;ref&gt;{{cite web|url=http://infomotions.com/musings/tricks/manuscript/1600-0001.html|title=AppleSearch|work=infomotions.com|accessdate=24 June 2015}}&lt;/ref&gt; At the time, a Macintosh computer with these specifications was priced at approximately $1400; equivalent to $2050 in 2015.&lt;ref&gt;{{cite web|url=http://stats.areppim.com/calc/calc_usdlrxdeflator.php|title=Converter of current to real US dollars - using the GDP deflator|author=eduardo casais|work=areppim.com|accessdate=24 June 2015}}&lt;/ref&gt; On top of this, the software itself cost an additional $1400 for a single license.

In 1997, [[Sherlock (software)|Sherlock]] was released alongside Mac OS 8.5. Sherlock (named after the famous fictional detective [[Sherlock Holmes]]) was integrated into Mac OS's file browser&amp;nbsp;&#8211; [[Finder (software)|Finder]]. Sherlock extended the desktop search function to the World Wide Web, allowing users to search both locally and externally. Adding additional functions&#8212;such as internet access&#8212;to Sherlock was relatively simple, as this was done through plugins written as plain text files. Sherlock was included in every release of Mac OS from [[Mac OS 8]], before being deprecated and replaced by [[Spotlight (software)|Spotlight]] and [[Dashboard (Mac OS)|Dashboard]] in [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It was officially removed in [[Mac OS X Leopard|Mac OS X 10.5 Leopard]]

[[Spotlight (software)|Spotlight]] was released in 2005 as part of [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It is a Selection-based search tool, which means the user invokes a query using only the mouse. Spotlight allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage, and uses a built-in calculator and Oxford American Dictionary to offer quick access to small calculations and word definitions.&lt;ref&gt;{{cite web|url=http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html|title=Apple - Press Info - Apple to Ship Mac OS X "Tiger" on April 29|work=apple.com|accessdate=24 June 2015}}&lt;/ref&gt; While Spotlight initially has a long startup time, this decreases as the hard disk is indexed. As files are added by the user, the index is constantly updated in the background using minimal CPU &amp; RAM resources.

=== Linux ===
There are a wide range of desktop search options for Linux users, depending upon the skill level of the user, their preference to use desktop tools which tightly integrate into their desktop environment, command-shell functionality (often with advanced scripting options), or browser-based users interfaces to locally running software.  In addition, many users create their own indexing from a variety of indexing packages (e.g. one which does extraction and indexing of PDF/DOC/DOCX/[[OpenDocument|ODT]] documents well, another search engine which works w/ vcard, LDAP, and other directory/contact databases, as well as the conventional &lt;tt&gt;find&lt;/tt&gt; and &lt;tt&gt;locate&lt;/tt&gt; commands.

====Ubuntu====
The [[Ubuntu distribution]] is a popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using [[Tracker (search software)|Tracker]]&lt;ref&gt;{{cite web|url=http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/|title=A first look at Tracker 0.6.0|work=Ars Technica|accessdate=24 June 2015}}&lt;/ref&gt; desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX-based systems. Tracker, released in late 2007, was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting and meta-data matching, but support for searching through emails and instant messages was added. Years later, in 2014 [[Recoll]]&lt;ref&gt;{{cite web|url=http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING|title=Recoll user manual|work=lesbonscomptes.com|accessdate=24 June 2015}}&lt;/ref&gt; was added to Linux distributions, it works with other search programs such as Tracker and [[Beagle (software)|Beagle]] to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. A major advantage of Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example, you could search for just file types or by content.&lt;ref&gt;{{cite web|url=http://archive09.linux.com/feature/114283|title=Linux.com|work=linux.com|accessdate=24 June 2015}}&lt;/ref&gt;

====[[openSUSE]]&lt;ref&gt;http://www.opensuse.org/&lt;/ref&gt;====
&lt;!--TODO! Prior desktop search before KDE 3.5--&gt;
Starting with [[KDE4]], the [[NEPOMUK (software)|NEPOMUK]] was introduced.  It provided the ability to index a wide range of desktop content, email, and use semantic web technologies (e.g. [[Resource Description Framework|RDF]]) to annotate the database.  The introduction faced a few glitches, much of which seemed to be based on the [[triplestore]].  Performance improved (at least for queries) by switching the backend to a stripped own version of the [[Virtuoso]] Open Source Edition, however indexing remained a common user complaint.  
Based on user feedback, the Nepomuk indexing and search has been replaced with the Baloo framework&lt;ref&gt;https://community.kde.org/Baloo&lt;/ref&gt; based on [[Xapian]].

==See also==
*[[List of search engines#Desktop search engines|List of desktop search engines]]

== References ==
{{reflist|2}}

{{Navigationbox Desktopsearch}}

{{DEFAULTSORT:Desktop Search}}
[[Category:Desktop search engines| ]]
[[Category:Information retrieval genres]]</text>
      <sha1>i4l5ki9rr4s02u3wj3ttbxbtlymyq8g</sha1>
    </revision>
  </page>
  <page>
    <title>Multimedia search</title>
    <ns>0</ns>
    <id>5987236</id>
    <revision>
      <id>735904849</id>
      <parentid>735904754</parentid>
      <timestamp>2016-08-23T21:41:14Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Audio search engine */ move link, c/e</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3643" xml:space="preserve">'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.
Multimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.
We can distinguish two methodologies in multimedia search:
*'''Metadata search''': the search is made on the layers of [[metadata]].
* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.

==Metadata search==

Search is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.

There are three processes which should be done in this method:
*'''[[Multimedia information retrieval#Feature extraction methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.
*'''[[Multimedia information retrieval#Feature extraction methods|Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])
*'''[[Multimedia information retrieval#Categorization methods|Categorization of media descriptions]]''' into classes.

==Query by example==

In [[query by example]], the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often, it's used [[Search engine indexing|audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:
*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].
*Compare descriptors of the query and our database&#8217;s media.
*List the media sorted by maximum coincidence.

==Multimedia search engine==
There are two big search families, in function of the content:
* [[Visual search engine]]
*[[Audio search engine]]

===Visual search engine===
Inside this family we can distinguish two topics: [[image search]] and [[video search]]

*'''[[Image search]]''': Although usually it's used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example, [[QR codes]].
*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.

===Audio search engine===
There are different methods of [[Audio search engine|audio searching]]:
*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].
*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album&#8230;) . There are some programs of [[music recognition]], for example [[Shazam (service)|Shazam]] or [[SoundHound]].

==See also==
*''[[Journal of Multimedia]]''
*[[List of search engines#Multimedia|List of search engines]]
*[[Multimedia]]
*[[Multimedia information retrieval]]
*[[Search engine indexing]]
*[[Streaming media]]
*[[Video search engine]]

==External links==

[[Category:Information retrieval genres]]
[[Category:Multimedia]]</text>
      <sha1>qan1tcazolqdrw3k2rawn61dtvmeo6c</sha1>
    </revision>
  </page>
  <page>
    <title>Cognitive models of information retrieval</title>
    <ns>0</ns>
    <id>24963841</id>
    <revision>
      <id>756183083</id>
      <parentid>723050831</parentid>
      <timestamp>2016-12-22T15:00:23Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>rm disguised advert</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5000" xml:space="preserve">{{Orphan|date=September 2012}}

'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.

==Berrypicking==
One way of understanding how users search for information has been described by [[Marcia Bates]]&lt;ref&gt;[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." https://pages.gseis.ucla.edu/faculty/bates/berrypicking.html&lt;/ref&gt; at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.&lt;ref&gt;[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.&lt;/ref&gt;

Bates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.

==Exploratory Search==
Researchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.&lt;ref&gt;Qu, Yan &amp; Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"&lt;/ref&gt; Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.

==Natural language searching==

Another way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.&lt;ref&gt;Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm&lt;/ref&gt;  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.

==Notes==
{{Reflist}}

[[Category:Information retrieval genres]]
[[Category:Cognitive modeling]]</text>
      <sha1>pu3jxdkwsvvcsu1kh1ne7cadpz7g9ec</sha1>
    </revision>
  </page>
  <page>
    <title>Noisy text analytics</title>
    <ns>0</ns>
    <id>6026708</id>
    <revision>
      <id>723138276</id>
      <parentid>715585657</parentid>
      <timestamp>2016-06-01T07:08:15Z</timestamp>
      <contributor>
        <username>I dream of horses</username>
        <id>9676078</id>
      </contributor>
      <minor />
      <comment>/* top */clean up, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6330" xml:space="preserve">{{multiple issues|
{{COI|date=December 2015}}
{{notability|date=December 2015}}
{{Orphan|date=June 2016}}
}}
'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as &#8220;um&#8221; and &#8220;uh&#8221; and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today&#8217;s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.

== Techniques for noisy text analysis ==
Missing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[part-of-speech tagging]]
and [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.

== Possible source of noisy text ==
* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.
* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.
* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.
* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.

== References ==
*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&amp;pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]
*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. &amp; Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND), 2007; Hyderabad, India."].
*"L. V. Subramaniam, S. Roy, T. A. Faruquie, S. Negi, A survey of types of text noise and techniques to handle noisy text. In: Third Workshop on Analytics for Noisy Unstructured Text Data (AND), 2009".
&lt;references /&gt;

==See also==
* [[Text analytics]]
* [[Information extraction]]
* [[Computational linguistics]]
* [[Natural language processing]]
* [[Named entity recognition]]
* [[Text mining]]
* [[Automatic summarization]]
* [[Statistical classification]]
* [[Data quality]]

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval genres]]
[[Category:Statistical natural language processing]]</text>
      <sha1>mq5obexvm5nhbb56y6g4bcqomz058bz</sha1>
    </revision>
  </page>
  <page>
    <title>Information retrieval applications</title>
    <ns>0</ns>
    <id>13324645</id>
    <revision>
      <id>684413359</id>
      <parentid>666861561</parentid>
      <timestamp>2015-10-06T13:57:36Z</timestamp>
      <contributor>
        <ip>195.251.3.6</ip>
      </contributor>
      <comment>/* General applications of information retrieval */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1476" xml:space="preserve">Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):

==General applications of information retrieval==
* [[Digital libraries]]
*  [[Information filtering]]
** [[Recommender systems]]
*  Media search
** Blog search
** [[Image retrieval]]
** [[3D retrieval]]
** [[Music information retrieval|Music retrieval]]
** News search
** Speech retrieval
** Video retrieval
* [[Search engines]]
** [[Site search]]
** [[Desktop search]]
** [[Enterprise search]]
** [[Federated search]]
** [[Mobile search]]
** [[Social search]]
** [[Web search engine|Web search]]

==Domain specific applications of information retrieval==
* Expert search finding
* Genomic information retrieval
* [[Geographic information retrieval]]
*  Information retrieval for chemical structures
* Information retrieval in [[software engineering]]
* [[Legal information retrieval]]
* [[Vertical search]]

==Other retrieval methods==
Methods/Techniques in which [[information retrieval]] techniques are employed include:
* [[Adversarial information retrieval]]
* [[Automatic summarization]]
**[[Multi-document summarization]]
* [[Compound term processing]]
* [[Cross-language information retrieval|Cross-lingual retrieval]]
* [[Document classification]]
* [[Spam filtering]]
* [[Question answering]]

== See also ==
* [[Information retrieval]]

{{DEFAULTSORT:Information Retrieval Applications}}
[[Category:Information retrieval genres|*]]</text>
      <sha1>pjjmag6bnccmh6l7r85tg4d3ip8smnb</sha1>
    </revision>
  </page>
  <page>
    <title>Gerard Salton</title>
    <ns>0</ns>
    <id>509624</id>
    <revision>
      <id>758638559</id>
      <parentid>728275225</parentid>
      <timestamp>2017-01-06T17:42:35Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <comment>See also 'List of pioneers in computer science'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7684" xml:space="preserve">{{Infobox scientist
| name              = Gerard Salton
| birth_date        = {{birth date|1927|03|08}}
| birth_place       = [[Nuremberg]]
| death_date        = {{death date and age|1995|08|28 |1927|03|08}}
| death_place       = 
| fields            = [[information retrieval]]
| workplaces        = [[Cornell University]]
| alma_mater        = [[Harvard University]]
| thesis_title      = An automatic data processing system for public utility revenue accounting
| thesis_url        = http://hollis.harvard.edu/?itemid=%7Clibrary/m/aleph%7C003918090
| thesis_year       = 1958
| doctoral_advisor  = [[Howard Aiken]]
| doctoral_students = [[Amit Singhal]]
| known_for         = the father of information retrieval&lt;ref name=father-IR /&gt; &lt;br&gt; [[Gerard Salton Award]]
}}
'''Gerard A. "Gerry" Salton''' (8 March 1927 in [[Nuremberg]] &#8211; 28 August 1995), was a Professor of [[Computer Science]] at [[Cornell University]].  Salton was perhaps the leading computer scientist working in the field of [[information retrieval]] during his time, and "the father of information retrieval".&lt;ref name=father-IR&gt;{{cite web |url=http://www.cs.cornell.edu/gries/40brochure/pg24_25.pdf |title=The father of information retrieval |last1= |first1= |last2= |first2= |date= |website=cs.cornell.edu |publisher= |quote= a founding member of the department and the father of information retrieval. |access-date=10 March 2015}}&lt;/ref&gt;  His group at Cornell developed the [[SMART Information Retrieval System]], which he initiated when he was at Harvard.

Salton was born Gerhard Anton Sahlmann on March 8, 1927 in [[Nuremberg, Germany]].  He received a Bachelor's (1950) and Master's (1952) degree in mathematics from [[Brooklyn College]], and a Ph.D. from [[Harvard University|Harvard]] in [[Applied Mathematics]] in 1958, the last of [[Howard Aiken]]'s doctoral students, and taught there until 1965, when he joined [[Cornell University]] and co-founded its department of Computer Science.

Salton was perhaps most well known for developing the now widely used [[vector space model]] for Information Retrieval.&lt;ref&gt;{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Wong | first2 = A. | last3 = Yang | first3 = C. S. | doi = 10.1145/361219.361220 | title = A vector space model for automatic indexing | journal = Communications of the ACM | volume = 18 | issue = 11 | pages = 613 | year = 1975 | pmid =  | pmc = }}&lt;/ref&gt;  In this model, both documents and queries are represented as vectors of term counts, and the similarity between a document and a query is given by the cosine between the term vector and the document vector.  In this paper, he also introduced [[TF-IDF]], or term-frequency-inverse-document frequency, a model in which the score of a term in a document is the ratio of the number of terms in that document divided by the frequency of the number of documents in which that term occurs. (The concept of inverse document frequency, a measure of specificity, had been introduced in 1972 by [[Karen Sp&#228;rck Jones|Karen Sparck-Jones]].&lt;ref&gt;{{Cite journal | last1 = Sp&#228;rck Jones | first1 = K. | authorlink1 = Karen Sp&#228;rck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11&#8211;21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}&lt;/ref&gt;) Later in life, he became interested in automatic text summarization and analysis,&lt;ref&gt;{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Allan | first2 = J. | last3 = Buckley | first3 = C. | last4 = Singhal | first4 = A. | title = Automatic Analysis, Theme Generation, and Summarization of Machine-Readable Texts | doi = 10.1126/science.264.5164.1421 | journal = Science | volume = 264 | issue = 5164 | pages = 1421&#8211;1426 | year = 1994 | pmid =  17838425| pmc = }}&lt;/ref&gt; as well as automatic hypertext generation.&lt;ref&gt;{{cite web|url=http://www.cs.cornell.edu/Info/Department/Annual95/Faculty/Salton.html |title=Gerard Salton |publisher=Cs.cornell.edu |date= |accessdate=2013-09-14}}&lt;/ref&gt;  He published over 150 research articles and 5 books during his life.

Salton was editor-in-chief of the [[Communications of the ACM]] and the [[Journal of the ACM]], and chaired [[Special Interest Group on Information Retrieval]] (SIGIR).  He was an associate editor of the [[ACM Transactions on Information Systems]]. He was an [[List of Fellows of the Association for Computing Machinery|ACM Fellow]] (elected 1995),&lt;ref name=fellow-acm&gt;{{cite web |url=http://awards.acm.org/award_winners/salton_2316166.cfm |title=Gerard Salton ACM Fellows  1995 |last1= |first1= |last2= |first2= |date= |website=acm.org |publisher= |quote=contributions over 30 years to information organization and retrieval |access-date=10 March 2015}}&lt;/ref&gt; received an Award of Merit from the [[American Society for Information Science]] (1989), and was the first recipient of the SIGIR Award for outstanding contributions to study of information retrieval (1983) -- now called the [[Gerard Salton Award]].

==Bibliography==
*Salton, ''Automatic Information Organization and Retrieval'', 1968.
*{{cite book
 | author     = Gerard Salton
 | title      = A Theory of Indexing
 | publisher  = Society for Industrial and Applied Mathematics
 | year       = 1975
 | page      = 56
}}
*--- and Michael J. McGill, ''Introduction to modern information retrieval'', 1983.  ISBN 0-07-054484-0
*{{cite book
 | author     = Gerard Salton
 | title      = Automatic Text Processing
 | publisher  = Addison-Wesley Publishing Company
 | year       = 1989
 | page      = 530
 | isbn       = 0-201-12227-8
}}
*[http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salton:Gerard.html DBLP Bibliography]
* G. Salton, A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613&#8211;620. ''(Article in which a vector space model was presented)''

==See also==
* [[List of pioneers in computer science]]

==References==
{{Reflist}}

==External links==
* [http://www.cs.cornell.edu/Info/Department/Annual96/Beginning/salton.html In Memoriam]
* [http://blog.tomevslin.com/2006/01/search_down_mem.html Fractals of Change: Search Down Memory Lane]
* [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] - This 2004 Library Trends paper by David Dubin serves as a historical review of the metamorphosis of the term discrimination value model (TDV) into the vector space model as an information retrieval model (VSM as an IR model). This paper calls into question what the Information Retrieval research community believed Salton's vector space model was originally intended to model. What much later became an information retrieval model was originally a data-centric mathematical&#8211;computational model used as an explanatory device. In addition, Dubin's paper points out that a 1975 Salton paper oft cited does not exist but is probably a combination of two other papers, neither of which actually refers to the VSM as an IR model.

{{Authority control}}

{{DEFAULTSORT:Salton, Gerard}}
[[Category:1927 births]]
[[Category:1995 deaths]]
[[Category:American computer scientists]]
[[Category:Harvard University alumni]]
[[Category:Harvard University faculty]]
[[Category:Cornell University faculty]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Guggenheim Fellows]]
[[Category:Information retrieval researchers]]</text>
      <sha1>eerm70yrnhtib3iex8p9gy1s3t2bkk1</sha1>
    </revision>
  </page>
  <page>
    <title>Cyril Cleverdon</title>
    <ns>0</ns>
    <id>20632884</id>
    <revision>
      <id>754009445</id>
      <parentid>735921474</parentid>
      <timestamp>2016-12-10T10:19:39Z</timestamp>
      <contributor>
        <username>Sfan00 IMG</username>
        <id>4906524</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4861" xml:space="preserve">{{Infobox scientist
| name = Cyril Cleverdon
| image =
| caption = 
| birth_date = {{birth date|1914|9|9|df=y}}
| birth_place = [[Bristol]], [[United Kingdom|UK]]
| death_date = {{death date and age|1997|12|4|1914|9|9|df=y}}
| death_place = [[Cranfield]], [[United Kingdom|UK]]
| residence = United Kingdom
| nationality = British
| field = Computer Science
| work_institution = [[Cranfield Institute of Technology]]
| known_for  = work on the evaluation of information retrieval systems
| prizes = Professional Award of the Special Libraries Association (1962), Award of Merit of the American Society for Information Science (1971), The [[Gerard Salton Award]] of the Special Interest Group on Information Retrieval of the Association for Computing Machinery (1991)
}}
'''Cyril Cleverdon''' (9 September 1914 &#8211; 4 December 1997) was a [[United Kingdom|British]] librarian and computer scientist who is best known for his work on the evaluation of [[information retrieval]] systems.

Cyril Cleverdon was born in [[Bristol]], [[England]]. He worked at the Bristol Libraries from 1932 to 1938, and from 1938 to 1946 he was the librarian of the Engine Division of the Bristol Aeroplane Co. Ltd. In 1946 he was appointed librarian of the College of Aeronautics at Cranfield (later the [[Cranfield Institute of Technology]] and [[Cranfield University]]), where he served until his retirement in 1979, the last two years as professor of Information Transfer Studies.

With the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:
# a collection of documents,
# a set of user requests or queries, and 
# a set of relevance judgments&#8212;that is, a set of documents judged to be [[Relevance (information retrieval)|relevant]] to each query. 
Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: [[Precision (information retrieval)|precision]] and [[Recall (information retrieval)|recall]]. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful [[Text Retrieval Conference]] series that began in 1992.

Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the [[automatic indexing]] as done in today's [[search engine]]s. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:

''This conclusion is so controversial and so unexpected that it is bound to throw considerable doubt on the methods which have been used (...) A complete recheck has failed to reveal any discrepancies (...) there is no other course except to attempt to explain the results which seem to offend against every canon on which we were trained as librarians.'' 

Cyril Cleverdon also ran, for many years, the Cranfield conferences, which provided a major international forum for discussion of ideas and research in information retrieval. This function was taken over by the [[Special Interest Group on Information Retrieval|SIGIR]] conferences in the 1970s.

==References==
* {{cite journal|author=Cyril Cleverdon|title=Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems|publisher=The College of Aeronautics, Cranfield|year=1960|url=http://www.sigir.org/museum/pdfs/Report_on_the_Testing_and_Analysis_of_an_Investigation_Into_the_Comparative_Efficiency_of_Indexing_Systems/pdfs/frontmatter.pdf}}
* Cyril Cleverdon and Michael Keen, Factors Determining the Performance of Indexing Systems, Volume 2, ''The College of Aeronautics, Cranfield'', 1966
* Stephen Robertson, In Memoriam Cyril W. Cleverdon, ''Journal of the American Society for Information Science 49''(10):866, 1998

{{DEFAULTSORT:Cleverdon, Cyril}}
[[Category:1914 births]]
[[Category:1997 deaths]]
[[Category:British computer scientists]]
[[Category:English librarians]]
[[Category:People associated with Cranfield University]]
[[Category:People from Bristol]]
[[Category:Information retrieval researchers]]</text>
      <sha1>ebt0x6rdxbqj1hwyon4ae7l6xhrcv04</sha1>
    </revision>
  </page>
  <page>
    <title>C. J. van Rijsbergen</title>
    <ns>0</ns>
    <id>1514191</id>
    <revision>
      <id>705677791</id>
      <parentid>666734175</parentid>
      <timestamp>2016-02-18T22:31:26Z</timestamp>
      <contributor>
        <username>KasparBot</username>
        <id>24420788</id>
      </contributor>
      <comment>migrating [[Wikipedia:Persondata|Persondata]] to Wikidata, [[toollabs:kasparbot/persondata/|please help]], see [[toollabs:kasparbot/persondata/challenge.php/article/C. J. van Rijsbergen|challenges for this article]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3647" xml:space="preserve">{{Use dmy dates|date=March 2014}}
{{Use British English|date=March 2014}}
{{multiple issues|
{{BLP sources|date=January 2012}}
{{More footnotes|date=January 2012}}
}}

{{Infobox scientist
|name              = Cornelis Joost van Rijsbergen
|image             = C J van Rijsbergen.jpg
|image_size        =
|caption           = C. J. "Keith" van Rijsbergen
|birth_date        = {{birth year and age|1943}}
|birth_place       = [[Rotterdam]]
|residence         = 
|citizenship       =
|nationality       = 
|fields            = [[Information Retrieval]]
|workplaces        = [[Monash University]], [[University of Glasgow]]
|alma_mater        = [[University of Western Australia]], [[University of Cambridge]]
|doctoral_advisor  = 
|academic_advisors =
|doctoral_students =
|notable_students  =
|known_for         = 
|author_abbrev_bot =
|author_abbrev_zoo =
|influences        =
|influenced        =
|awards            =
|signature         = &lt;!--(filename only)--&gt;
|footnotes         =
}}

'''C. J. "Keith" van Rijsbergen''' [[FREng]]&lt;ref name=fellow&gt;{{cite web|title=List of Fellows|url=http://www.raeng.org.uk/about-us/people-council-committees/the-fellowship/list-of-fellows}}&lt;/ref&gt; ('''Cornelis Joost van Rijsbergen''') (born 1943) is a professor of [[computer science]] and the leader of the Glasgow Information Retrieval Group based at the [[University of Glasgow]]. He is one of the founders of modern [[Information Retrieval]] and the author of the seminal monograph ''Information Retrieval'' and of the textbook ''The Geometry of Information Retrieval''.

He was born in [[Rotterdam]], and educated in the [[Netherlands]], [[Indonesia]], [[Namibia]] and [[Australia]].
His first degree is in mathematics from the [[University of Western Australia]], and in 1972 he completed a
PhD in computer science at the [[University of Cambridge]].
He spent three years lecturing in information retrieval and artificial intelligence at [[Monash University]]
before returning to [[University of Cambridge|Cambridge]] to hold a [[Royal Society]] Information Research Fellowship. 
In 1980 he was appointed to the chair of computer science at [[University College Dublin]];
from there he moved in 1986 to [[Glasgow University]].
Since 2007 he has been Chairman of the Scientific Board of the [[Information Retrieval Facility]].

==Awards and honors==
In 2003 he was inducted as a Fellow of the [[Association for Computing Machinery]]. In 2004 he was awarded the [[Tony Kent Strix award]].
In 2004 he was appointed a [[Fellow]]&lt;ref name=fellow /&gt; of the [[Royal Academy of Engineering]].&lt;ref name=fellow /&gt;
In 2006, he was awarded the [[Gerard Salton Award]] for ''Quantum haystacks''.

==See also==
*[[F1 score]]

==References==
{{Reflist}}

==External links==
*[http://www.dcs.gla.ac.uk/~keith/ C. J. "Keith" van Rijsbergen - The University of Glasgow]
*[http://ir.dcs.gla.ac.uk/ Glasgow Information Retrieval Group]
*[http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval book - C. J. van Rijsbergen 1979]
*[http://www.ir-facility.org/ Information Retrieval Facility]
*{{worldcat id|id=lccn-n83-236586}}
* [http://www.alanmacfarlane.com/ancestors/rijsbergen.htm Keith van Rijsbergen interviewed by Alan Macfarlane 15 July 2009 (film)]

{{Authority control}}

{{DEFAULTSORT:Rijsbergen, C. J. van}}
[[Category:1943 births]]
[[Category:Living people]]
[[Category:Dutch computer scientists]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:People from Rotterdam]]
[[Category:University of Western Australia alumni]]
[[Category:Information retrieval researchers]]


{{Netherlands-scientist-stub}}
{{compu-scientist-stub}}</text>
      <sha1>7cwi9xv7deved6pq65739qdqud7kboy</sha1>
    </revision>
  </page>
  <page>
    <title>Meta Content Framework</title>
    <ns>0</ns>
    <id>1053030</id>
    <revision>
      <id>713093369</id>
      <parentid>694907063</parentid>
      <timestamp>2016-04-01T22:17:38Z</timestamp>
      <contributor>
        <username>Cyberbot II</username>
        <id>16283967</id>
      </contributor>
      <comment>Rescuing 2 sources. #IABot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2749" xml:space="preserve">'''Meta Content Framework''' ('''MCF''') is a specification of a [[content format]] for structuring [[metadata]] about [[web site]]s and other [[data]].

==History==
MCF was developed by [[Ramanathan V. Guha]] at [[Apple Advanced Technology Group|Apple Computer's Advanced Technology Group]] between 1995 and 1997. Rooted in [[Knowledge representation and reasoning | knowledge-representation]] systems such as [[CycL]], [[KRL (programming language)| KRL]], and [[Knowledge Interchange Format|KIF]], it sought to describe objects, their attributes, and the relationships between them.&lt;ref name=hammersley&gt;{{Cite book| publisher = O'Reilly| isbn = 978-0-596-00383-8| last = Hammersley| first = Ben| title = Content Syndication with RSS| location = Sebastopol| date = 2003| page=2}}&lt;/ref&gt;

One application of MCF was [[HotSauce]], also developed by Guha while at Apple. It generated a [[3D computer graphics|3D]] [[visualization (graphic)|visualization]] of a web site's table of contents, based on MCF descriptions. By late 1996, a few hundred sites were creating MCF files and Apple HotSauce allowed users to browse these MCF representations in 3D.&lt;ref name=hammersley /&gt;

When the research project was discontinued, Guha left Apple for [[Netscape Communications Corporation|Netscape]], where, in collaboration with [[Tim Bray]], he adapted MCF to use [[XML]]&lt;ref&gt;{{Cite conference
| publisher = W3C
| last = Guha
| first = R V
|author2=Tim Bray
 | title = Meta Content Framework Using XML
| accessdate = 2014-09-14
| date = 1997-06-06
| url = http://www.w3.org/TR/NOTE-MCF-XML/
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|last1=Guha |first1=R.V. |last2=Bray |first2=Tim |title=Meta Content Framework Using XML |work=Netscape |accessdate=2015-12-12 |date=1997-06-13 |url=http://developer.netscape.com/mcf.html |deadurl=yes |archiveurl=https://web.archive.org/web/19970615144715/http://developer.netscape.com/mcf.html |archivedate=June 15, 1997 }}&lt;/ref&gt; and created the first version of the [[Resource Description Framework]] (RDF).&lt;ref&gt;{{Cite web|last=Andreessen |first=Marc |title=Innovators of the Net: R.V. Guha and RDF |work=Netscape |accessdate=2014-09-14 |date=1999-01-08 |url=http://wp.netscape.com/columns/techvision/innovators_rg.html |deadurl=yes |archiveurl=https://web.archive.org/web/20080205163659/http://wp.netscape.com/columns/techvision/innovators_rg.html |archivedate=February 5, 2008 }}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://www.textuality.com/mcf/MCF-tutorial.html MCF Tutorial] (using XML syntax)
*[http://www.guha.com/mcf/ Guha MCF site]
*[http://downlode.org/Etext/MCF/towards_a_theory_of_metacontent.html The metacontent concept]

[[Category:Knowledge representation]]
[[Category:Apple Inc. software]]

{{compu-AI-stub}}</text>
      <sha1>l2qkcs9dli230ql43s5oyh3rpfny723</sha1>
    </revision>
  </page>
  <page>
    <title>Nippon Decimal Classification</title>
    <ns>0</ns>
    <id>2471086</id>
    <revision>
      <id>688030250</id>
      <parentid>688003149</parentid>
      <timestamp>2015-10-29T05:11:53Z</timestamp>
      <contributor>
        <username>Alex Sims</username>
        <id>240189</id>
      </contributor>
      <comment>Unsure what was meant, but not correct. Undid revision 562404720 by [[Special:Contributions/Mukeshwar|Mukeshwar]] ([[User talk:Mukeshwar|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4276" xml:space="preserve">The '''Nippon Decimal Classification''' ('''NDC''', also called the '''Nippon Decimal System''') is a system of [[library classification]] developed for mainly Japanese language books maintained by the Japan Library Association since 1956. It is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. The system is based upon using each successive digit to divide into nine divisions with the digit zero used for those not belonging to any of the divisions.

== Main classes ==
The system is made up of ten categories:

* 000 General
* 100 [[Philosophy]]
* 200 [[History]]
* 300 [[Social sciences]]
* 400 [[Natural sciences]]
* 500 [[Technology]] and [[engineering]]
* 600 [[Industry]] and [[commerce]]
* 700 [[Arts]]
* 800 [[Language]]
* 900 [[Literature]]

== Description of the classes ==

*000 General
**010 [[Libraries]], [[Library and information science|Library &amp; information science]]
**020 [[Books]], [[Bibliography]]
**030 [[Encyclopaedia]]s
**040 General collected [[essays]]
**050 General [[Periodical literature|serial publications]]
**060 [[Organizations]]
**070 [[Journalism]], [[Newspapers]]
**080 General collections
**090 [[Rare books]], Local collections, [[Special collections]]
*100 Philosophy
**110 Special treatises on philosophy
**120 [[Oriental philosophy]]
**130 [[Western philosophy]]
**140 [[Psychology]]
**150 [[Ethics]] &amp; morals
**160 [[Religion]]
**170 [[Shintoism]]
**180 [[Buddhism]]
**190 [[Christianity]]
*200 History
**210 [[History of Japan]]
**220 [[History of Asia]] and the Orient
**230 [[History of Europe]] and the West
**240 [[History of Africa]]
**250 [[History of North America]]
**260 [[History of South America]]
**270 [[History of Oceania]] &amp; [[Polar region]]s
**280 [[Biography]]
**290 [[Geography]], [[Topography]], [[Travel]]
*300 Social Sciences
**310 [[Politics]]
**320 [[Law]]
**330 [[Economics]]
**340 [[Finance]]
**350 [[Statistics]]
**360 [[Sociology]]
**370 [[Education]]
**380 [[Customs]], [[Folklore]], [[Ethnology]]
**390 National defence, [[Military science]]
*400 Natural Sciences
**410 [[Mathematics]]
**420 [[Physics]]
**430 [[Chemistry]]
**440 [[Astronomy]], [[Space science]]
**450 [[Earth science]]
**460 [[Biology]]
**470 [[Botany]]
**480 [[Zoology]]
**490 [[Medicine]], [[Pharmacology]]
*500 Technology &amp; Engineering
**510 [[Construction]], [[Civil engineering]]
**520 [[Architecture]]
**530 [[Mechanical engineering]], [[Nuclear engineering]]
**540 [[Electrical engineering|Electrical]] &amp; [[Electronic engineering]]
**550 Maritime &amp; [[Naval engineer]]ing
**560 Metal &amp; [[Mining engineering]]
**570 [[Chemical technology]]
**580 [[Manufacturing]]
**590 [[Domestic science|Domestic arts and sciences]]
*600 Industry and Commerce
**610 [[Agriculture]]
**620 [[Horticulture]]
**630 [[Sericulture|Silk industry]]
**640 [[Animal husbandry]]
**650 [[Forestry]]
**660 [[Fishing]]
**670 [[Commerce]]
**680 [[Transportation]] &amp; [[Traffic]]
**690 [[Communications]]
*700 Arts
**710 [[Plastic arts]] (sculpture)
**720 [[Painting]] &amp; [[Calligraphy]]
**730 [[Engraving]]
**740 [[Photography]] &amp; [[Printing]]
**750 [[Craft]]
**760 [[Music]] &amp; [[Dance]]
**770 [[Theatre]], [[Motion Pictures]]
**780 [[Sports]], [[Physical Education]]
**790 [[Recreation]], Amusements
*800 Language
**810 [[Japanese language|Japanese]]
**820 [[Chinese language|Chinese]], other [[oriental languages]]
**830 [[English language|English]]
**840 [[German language|German]]
**850 [[French language|French]]
**860 [[Spanish language|Spanish]]
**870 [[Italian language|Italian]]
**880 [[Russian language|Russian]]
**890 Other languages
*900 Literature
**910 [[Japanese literature]]
**920 [[Chinese literature]], Other [[Oriental literature]]
**930 [[English literature|English]] &amp; [[American literature]]
**940 [[German literature]]
**950 [[French literature]]
**960 [[Spanish literature]]
**970 [[Italian literature]]
**980 [[Russian literature|Russian]] &amp; [[Soviet literature]]
**990 Other language literature

== External links ==
*[http://www.jla.or.jp/index-e.html Japan Library Association]
*[http://www.asahi-net.or.jp/~ax2s-kmtn/ref/ndc/e_ndc.html CyberLibrarian]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]</text>
      <sha1>bbjv0pc5tweq5xv3fhjfh6q8od1148o</sha1>
    </revision>
  </page>
  <page>
    <title>Philosophy of information</title>
    <ns>0</ns>
    <id>4522868</id>
    <revision>
      <id>758240131</id>
      <parentid>724075297</parentid>
      <timestamp>2017-01-04T07:52:07Z</timestamp>
      <contributor>
        <username>Libcub</username>
        <id>6307086</id>
      </contributor>
      <minor />
      <comment>/* See also */ deleted redlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11341" xml:space="preserve">{{Information science}}
The '''philosophy of information''' ('''PI''') is the area of research that studies conceptual issues arising at the intersection of [[computer science]], [[information science]], [[information technology]], and [[philosophy]].

It includes:

# the critical investigation of the conceptual nature and basic principles of [[information]], including its dynamics, utilisation and sciences
# the elaboration and application of information-theoretic and computational methodologies to philosophical problems.&lt;ref&gt;Luciano Floridi, [http://www.blackwellpublishing.com/pci/downloads/introduction.pdf "What is the Philosophy of Information?"], ''Metaphilosophy'', 2002, (33), 1/2.&lt;/ref&gt;

==History==
The philosophy of information (PI) has evolved from the [[philosophy of artificial intelligence]], [[logic of information]], [[cybernetics]], [[social theory]], [[ethics]] and the study of language and information.

===Logic of information===
The [[logic of information]], also known as the ''logical theory of information'', considers the information content of logical [[sign (semiotics)|sign]]s and expressions along the lines initially developed by [[Charles Sanders Peirce]].

===Cybernetics===
One source for the philosophy of information can be found in the technical work of [[Norbert Wiener]], [[Alan Turing]] (though his work has a wholly different origin and theoretical framework), [[William Ross Ashby]], [[Claude Shannon]], [[Warren Weaver]], and many other scientists working on computing and information theory back in the early 1950s. See the main article on [[Cybernetics]].

Some important work on information and communication was done by [[Gregory Bateson]] and his colleagues.

===Study of language and information===
Later contributions to the field were made by [[Fred Dretske]], [[Jon Barwise]], [[Brian Cantwell Smith]], and others.

The [[Center for the Study of Language and Information|Center for the Study of Language and Information (CSLI)]] was founded at Stanford University in 1983 by philosophers, computer scientists, linguists, and psychologists, under the direction of [[John Perry (philosopher)|John Perry]] and [[Jon Barwise]].

===P.I.===
More recently this field has become known as the philosophy of information. The expression was coined in the 1990s by [[Luciano Floridi]], who has published prolifically in this area with the intention of elaborating a unified and coherent, conceptual frame for the whole subject.{{citation needed|date=April 2015}}

==Definitions of "information"==

The concept ''information'' has been defined by several theorists.

===Peirce===
[[Charles S. Peirce]]'s theory of information was embedded in his wider theory of symbolic communication he called the ''semeiotic'', now a major part of [[semiotics]]. For Peirce, information integrates the aspects of [[sign]]s and [[Expression (mathematics)|expressions]] separately covered by the concepts of [[denotation]] and [[extension (semantics)|extension]], on the one hand, and by [[connotation]] and [[comprehension (logic)|comprehension]] on the other.

=== Shannon and Weaver ===
Claude E. Shannon, for his part, was very cautious: "The word 'information' has been given different meanings by various writers in the general field of information theory. It is likely that at least a number of these will prove sufficiently useful in certain applications to deserve further study and permanent recognition. It is hardly to be expected that a single concept of information would satisfactorily account for the numerous possible applications of this general field." (Shannon 1993, p.&amp;nbsp;180){{full citation needed|date=April 2015}}. Thus, following Shannon, Weaver supported a tripartite analysis of information in terms of (1) technical problems concerning the quantification of information and dealt with by Shannon's theory; (2) semantic problems relating to meaning and truth; and (3) what he called "influential" problems concerning the impact and effectiveness of information on human behaviour, which he thought had to play an equally important role. And these are only two early examples of the problems raised by any analysis of information.

A map of the main senses in which one may speak of information is provided by  [http://plato.stanford.edu/entries/information-semantic/ the Stanford Encyclopedia of Philosophy article]. The previous paragraphs are based on it.

===Bateson===
[[Gregory Bateson]] defined information as "a difference that makes a difference".&lt;ref&gt;[http://plato.acadiau.ca/courses/educ/reid/papers/PME25-WS4/SEM.html Extract from "Steps to an Ecology of Mind"]&lt;/ref&gt; which is based on [[Donald M. MacKay]]: information is a distinction that makes a difference.&lt;ref&gt;The Philosophy of Information.
Luciano Floridi. Chapter 4. Oxford University Press, USA (March 8, 2011) ASIN: 0199232385 [http://www.amazon.com/Philosophy-Information-Luciano-Floridi/dp/0199232385]&lt;/ref&gt;

===Floridi===
According to Luciano Floridi{{citation needed|date=April 2015}}, four kinds of mutually compatible phenomena are commonly referred to as "information": 
*  Information about something (e.g. a train timetable)
*  Information as something (e.g. DNA, or fingerprints)
*  Information for something (e.g. algorithms or instructions)
*  Information in something (e.g. a pattern or a constraint).

The word "information" is commonly used so metaphorically or so abstractly that the meaning is unclear.

==Philosophical directions==

===Computing and philosophy===
Recent creative advances and efforts in [[computing]], such as [[semantic web]], [[ontology engineering]], [[knowledge engineering]], and modern [[artificial intelligence]] provide [[philosophy]] with fertile notions, new and evolving subject matters, methodologies, and models for philosophical inquiry.  While [[computer science]] brings new opportunities and challenges to traditional philosophical studies, and changes the ways philosophers understand foundational concepts in philosophy, further major progress in [[computer science]] would only be feasible when philosophy provides sound foundations for areas such as bioinformatics, software engineering, knowledge engineering, and ontologies.

Classical topics in philosophy, namely, [[mind]], [[consciousness]], [[experience]], [[reasoning]], [[knowledge]], [[truth]], [[morality]] and [[creativity]] are rapidly becoming common concerns and foci of investigation in [[computer science]], e.g., in areas such as agent computing, [[software agents]], and intelligent mobile agent technologies.{{citation needed|date=December 2012}}

According to Luciano Floridi "&lt;ref&gt;Luciano Floridi, [http://www.philosophyofinformation.net/publications/pdf/oppi.pdf ''Open Problems in the Philosophy of Information''] ''Metaphilosophy'' 35.4, 554-582. Revised version of ''The Herbert A. Simon Lecture on Computing and Philosophy'' given at Carnegie Mellon University in 2001, with [http://ethics.sandiego.edu/video/CAP/CMU2001/Floridi/index.html RealVideo]&lt;/ref&gt; one can think of several ways for applying computational methods towards philosophical matters:
# Conceptual experiments in silico: As an innovative extension of an ancient tradition of [[thought experiment]], a trend has begun in philosophy to apply computational [[Computer model|modeling]] schemes to questions in [[logic]], [[epistemology]], [[philosophy of science]], [[philosophy of biology]], [[philosophy of mind]], and so on.
# [[Digital physics#Pancomputationalism or the computational universe theory|Pancomputationalism]]: By this view, computational and informational concepts are considered to be so powerful that given the right level of [[abstraction]], anything in the world could be modeled and represented as a computational system, and any process could be simulated computationally. Then, however, pancomputationalists have the hard task of providing credible answers to the following two questions:
## how can one avoid blurring all differences among systems?
## what would it mean for the system under investigation not to be an [[Information system|informational system]] (or a computational system, if computation is the same as information processing)?

===Information and society===
Numerous philosophers and other thinkers have carried out philosophical studies of the social and cultural aspects of electronically mediated information.

* [[Albert Borgmann]], ''Holding onto Reality: The Nature of Information at the Turn of the Millennium'' (Chicago University Press, 1999)
* [[Mark Poster]], ''The Mode of Information'' (Chicago Press, 1990)
* [[Luciano Floridi]], "The Informational Nature of Reality", ''Fourth International European Conference on Computing and Philosophy'' 2006 (Dragvoll Campus, NTNU Norwegian University for Science and Technology, Trondheim, Norway, 22&#8211;24 June 2006).

==See also==
{{col-begin}}
{{col-break}}
* [[Barwise prize]]
* [[Complex system]]
* [[Digital divide]]
* [[Digital philosophy]]
* [[Digital physics]]
* [[Game theory]]
* [[Freedom of information]]
* [[Informatics (academic field)|Informatics]]
{{col-break}}
* [[Information]]
* [[Information art]]
* [[Information ethics]]
* [[Information theory]]
* [[International Association for Computing and Philosophy]]
* [[Logic of information]]
{{col-break}}
* [[Philosophy of artificial intelligence]]
* [[Philosophy of computer science]]
* [[Philosophy of technology]]
* [[Philosophy of thermal and statistical physics]]
* [[Physical information]]
* [[Relational quantum mechanics]]
* [[Social informatics]]
* [[Statistical mechanics]]
{{col-end}}

==Notes==
{{reflist}}

==Further reading==
*[[Luciano Floridi]], "[http://www.blackwellpublishing.com/pci/downloads/introduction.pdf What is the Philosophy of Information?]" ''Metaphilosophy'', 33.1/2: 123-145. Reprinted in T.W. Bynum and J.H. Moor (eds.), 2003. ''CyberPhilosophy: The Intersection of Philosophy and Computing''. Oxford &#8211; New York: Blackwell.
*-------- (ed.), 2004. ''[http://www.blackwellpublishing.com/pci/default.htm The Blackwell Guide to the Philosophy of Computing and Information.]'' Oxford - New York: Blackwell.
*Greco, G.M., Paronitti G., Turilli M., and Floridi L., 2005. ''[http://www.wolfson.ox.ac.uk/~floridi/pdf/htdpi.pdf How to Do Philosophy Informationally.]'' ''Lecture Notes on Artificial Intelligence'' 3782, pp.&amp;nbsp;623&#8211;634.

== External links ==
{{Library resources box}}
*{{cite SEP |url-id=information |title=Information |last=Adriaans |first=Peter |editor-last=Zalta |editor-first=Edward N. ||date=Autumn  2013}}
*{{cite SEP |url-id=information-semantic |title=Semantic Conceptions of Information |last=Floridi |first=Luciano |editor-last=Zalta |editor-first=Edward N. |date=Spring 2015}}
*[http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/ IEG site], the Oxford University research group on the philosophy of information.
*[[Luciano Floridi]], "[https://web.archive.org/web/20060820223325/http://academicfeeds.friwebteknologi.org/index.php?id=28 Where are we in the philosophy of information?]" [[University of Bergen]], [[Norway]]. Podcast dated 21.06.06.

{{Navboxes
|list=
{{Philosophy topics}}
{{philosophy of language}}
{{philosophy of mind}}
{{philosophy of science}}
}}

[[Category:Philosophy by topic|Inf]]
[[Category:Philosophy of artificial intelligence]]
[[Category:Knowledge representation]]</text>
      <sha1>i952x4zlmiqjgje6zy1ov0v7e39uya8</sha1>
    </revision>
  </page>
  <page>
    <title>National Library of Medicine classification</title>
    <ns>0</ns>
    <id>4165078</id>
    <revision>
      <id>714125777</id>
      <parentid>662532684</parentid>
      <timestamp>2016-04-07T20:08:35Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* top */clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4044" xml:space="preserve">The '''National Library of Medicine (NLM) classification system''' is a [[Library classification|library indexing system]] covering the fields of [[medicine]] and preclinical basic sciences. The [[National Library of Medicine|NLM]] classification is patterned after the [[Library of Congress Classification|Library of Congress (LC) Classification system]]: [[Alphabet|alphabetical letters ]]  denote broad subject categories which are subdivided by numbers.&lt;ref name=NLM_Factsheet&gt;{{cite web | title =Fact Sheet: NLM Classification | work = | url = https://www.nlm.nih.gov/pubs/factsheets/nlmclassif.html | date = 2005-07-15 | accessdate = 2007-05-12}}&lt;/ref&gt; For example, ''QW 279'' would indicate a book on an aspect of [[microbiology]] or [[immunology]].

The one- or two-letter alphabetical codes in the NLM classification use a limited range of letters: only QS&#8211;QZ and W&#8211;WZ. This allows the NLM system to co-exist with the larger LC coding scheme as neither of these ranges are used in the LC system. There are, however, three pre-existing codes in the LC system which overlap with the NLM: ''Human Anatomy'' (QM), ''Microbiology'' (QR), and ''Medicine'' (R). To avoid further confusion, these three codes are not used in the NLM.

The headings for the individual ''schedules'' (letters or letter pairs) are given in brief form (e.g., QW - ''Microbiology and Immunology''; WG - ''Cardiovascular System'') and together they provide an outline of the subjects covered by the NLM classification. Headings are interpreted broadly and include the [[Physiology|physiological]] system, the specialties connected with them, the regions of the body chiefly concerned and subordinate related fields. The NLM system is [[hierarchical]], and within each schedule, division by [[Organ (anatomy)|organ]] usually has priority. Each main schedule, as well as some sub-sections, begins with a group of form numbers ranging generally from 1&#8211;49 which  classify materials by publication type, e.g., [[Dictionary|dictionaries]], [[atlas]]es, laboratory manuals, etc.

The main schedules QS-QZ, W-WY, and WZ (excluding the range WZ 220&#8211;270)  classify works published after 1913; the 19th century schedule is used for works published 1801-1913; and WZ 220-270 is used to provide century groupings for works published before 1801.

==Overview of the NLM Classification categories==

'''Preclinical Sciences'''
* QS Human Anatomy
* QT Physiology
* QU Biochemistry
* QV Pharmacology
* QW Microbiology &amp; Immunology
* QX Parasitology
* QY Clinical Pathology
* QZ Pathology

'''Medicine and Related Subjects'''

* W  Health Professions
* WA Public Health
* WB Practice of Medicine
* WC Communicable Diseases
* WD Disorders of Systemic, Metabolic, or Environmental Origin, etc.
* WE Musculoskeletal System
* WF Respiratory System
* WG Cardiovascular System
* WH Hemic and Lymphatic Systems
* WI Digestive System
* WJ Urogenital System
* WK Endocrine System
* WL Nervous System
* WM Psychiatry
* WN Radiology. Diagnostic Imaging
* WO Surgery
* WP Gynecology
* WQ Obstetrics
* WR Dermatology
* WS Pediatrics
* WT Geriatrics. Chronic Disease
* WU Dentistry. Oral Surgery
* WV Otolaryngology
* WW Ophthalmology
* WX Hospitals &amp; Other Health Facilities
* WY Nursing
* WZ History of Medicine
* 19th Century Schedule

==See also==
*[[Dewey Decimal Classification]]
*[[Colon Classification]]
*[[Library of Congress Classification]]
*[[Universal Decimal Classification]]

==References==
&lt;!-- ---------------------------------------------------------------
See http://en.wikipedia.org/wiki/Wikipedia:Footnotes for a
discussion of different citation methods and how to generate
footnotes using the&lt;ref&gt; &amp; &lt;/ref&gt;  tags and the {{Reflist}} template
-------------------------------------------------------------------- --&gt;
{{Reflist}}

{{refbegin}}
* {{USGovernment|sourceURL=[http://wwwcf.nlm.nih.gov/class/ The NLM Classification 2005]}}
{{refend}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>a5s88zmkq2s6ytwy4ye08omh4xam1lz</sha1>
    </revision>
  </page>
  <page>
    <title>Korean decimal classification</title>
    <ns>0</ns>
    <id>6978390</id>
    <revision>
      <id>666864078</id>
      <parentid>581227952</parentid>
      <timestamp>2015-06-14T06:20:45Z</timestamp>
      <contributor>
        <username>Antunesi</username>
        <id>17428904</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="796" xml:space="preserve">{{Unreferenced stub|date=December 2009}}
The '''Korean decimal classification''' ('''KDC''') is a system of [[library classification]] used in [[South Korea]]. The main classes are the same as in the Dewey Decimal Classification but these are in a different order: Natural sciences 400; Technology and engineering 500; Arts 600; Language 700.

==Main classes==
* 000 General
* 100 Philosophy
* 200 Religion
* 300 Social sciences
* 400 Natural sciences
* 500 Technology and engineering
* 600 Arts
* 700 Language
* 800 Literature
* 900 History

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]
[[Category:Libraries in North Korea]]
[[Category:Libraries in South Korea]]

{{Library classification systems}}
{{Library-stub}}</text>
      <sha1>rb8xn0z25au6uqw0enkm8859zd2j3y4</sha1>
    </revision>
  </page>
  <page>
    <title>Is-a</title>
    <ns>0</ns>
    <id>294441</id>
    <revision>
      <id>757843230</id>
      <parentid>747485106</parentid>
      <timestamp>2017-01-02T01:44:22Z</timestamp>
      <contributor>
        <username>Mx. Granger</username>
        <id>4871659</id>
      </contributor>
      <comment>removing/replacing citations to Wiktionary, which is not a [[WP:Reliable sources|reliable source]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10070" xml:space="preserve">In [[knowledge representation]], [[object-oriented programming]] and [[Object-oriented design|design]] (see [[object oriented]] [[program architecture]]), '''is-a''' ('''is_a''' or '''is a''') is a [[wikt:subsume|subsumption]]&lt;ref&gt;See [[Liskov substitution principle]].&lt;/ref&gt; relationship between [[abstractions]] (e.g. [[type (disambiguation)|types]], [[class (disambiguation)|classes]]), where one [[Class (computer programming)|class]] ''A'' is a [[subclass (disambiguation)|subclass]] &lt;!-- This deliberately links to the disambiguation page --&gt; of another class ''B'' (and so ''B'' is a [[superclass (disambiguation)|superclass]] &lt;!--This deliberately links to the disambiguation page--&gt; of ''A'').
In other words, type A is a subtype of type B when A&#8217;s [[Formal specification|speci&#64257;cation]] implies B&#8217;s speci&#64257;cation. That is, any object (or class) that satis&#64257;es A&#8217;s speci&#64257;cation also satis&#64257;es B&#8217;s speci&#64257;cation, because B&#8217;s speci&#64257;cation is weaker.&lt;ref&gt;{{cite web|title=Subtypes and Subclasses|url=http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-170-laboratory-in-software-engineering-fall-2005/lecture-notes/lec14.pdf|publisher=MIT OCW|accessdate=2 October 2012}}&lt;/ref&gt;

The ''is-a'' relationship is to be contrasted with the ''[[has-a]]'' (''has_a'' or ''has a'') relationship between types (classes); confusing the relations ''has-a'' and ''is-a'' is a common error when designing a model (e.g., a [[computer program]]) of the real-world relationship between an object and its subordinate. The ''is-a'' relationship may also be contrasted with the ''[[Typeof|instance-of]]'' relationship between objects (instances) and types (classes): see "[[type-token distinction]]" and "[[type-token relations]]."&lt;ref&gt;[[Type&#8211;token relations]]&lt;/ref&gt; 

To summarize the relations, we have

* [[hyperonym]]-[[hyponym]] (supertype-subtype) relations between types (classes) defining a taxonomic hierarchy, where
** for a [[Inheritance (object-oriented programming)|subsumption]] relation: a hyponym (subtype, subclass) has a ''type-of'' (''is-a'') relationship with its hypernym (supertype, superclass);
* [[holonym]]-[[meronym]] (whole/entity/container-part/constituent/member) relations between types (classes) defining a possessive hierarchy, where 
** for an [[Aggregation (object-oriented programming)|aggregation]] (i.e. without ownership) relation: 
*** a holonym (whole) has a ''has-a'' relationship with its meronym (part),
** for a [[Composition (object-oriented programming)|composition]] (i.e. with ownership) relation: 
*** a meronym (constituent) has a ''[[part-of]]'' relationship with its holonym (entity),
** for a [[Object composition#Containment|containment]]&lt;ref&gt;See also [[Containment (computer programming)]].&lt;/ref&gt; relation:
*** a meronym (member) has a ''[[member-of]]'' relationship with its holonym ([[Container (abstract data type)|container]]);
* concept-object (type-token) relations between types (classes) and objects (instances), where
** a token (object) has an ''[[Instance (computer science)|instance-of]]'' relationship with its type (class).

==Examples of subtyping==

[[Subtype polymorphism|Subtyping]] enables a given type to be substituted for another type or abstraction. Subtyping is said to establish an '''is-a''' relationship between the subtype and some existing abstraction, either implicitly or explicitly, depending on language support. The relationship can be expressed explicitly via inheritance in languages that support inheritance as a subtyping mechanism.

===C++===
The following C++ code establishes an explicit inheritance relationship between classes '''B''' and '''A''', where '''B''' is both a subclass and a subtype of '''A''', and can be used as an '''A''' wherever a '''B''' is specified (via a reference, a pointer or the object itself).

&lt;source lang=cpp&gt;class A 
{ public:
   void DoSomethingALike() const {}
};

class B : public A 
{ public:
   void DoSomethingBLike() const {}
};

void UseAnA(A const&amp; some_A)
{
   some_A.DoSomethingALike();
}

void SomeFunc()
{
   B b;
   UseAnA(b); // b can be substituted for an A.
}
&lt;/source&gt;&lt;ref name="Mitchell2002"&gt;
{{cite book
 | last=Mitchell
 | first=John
 | authorlink=John C. Mitchell
 | title=Concepts in programming language
 | year=2002
 | publisher=Cambridge University Press
 | location=Cambridge, UK
 | isbn=0-521-78098-5
 | page=287
 | chapter=10 "Concepts in object-oriented languages"}}
&lt;/ref&gt;

===Python===
The following python code establishes an explicit inheritance relationship between classes '''B''' and '''A''', where '''B''' is both a subclass and a subtype of '''A''', and can be used as an '''A''' wherever a '''B''' is required.

&lt;source lang=python&gt;class A:
    def doSomethingALike(self):
        pass

class B(A):
    def doSomethingBLike(self):
        pass

def useAnA(some_A):
    some_A.doSomethingALike()

def someFunc():
    b = B();
    useAnA(b)  # b can be substituted for an A.
&lt;/source&gt;

The following example, type(a) is a "regular" type, and type(type(a)) is a metatype. While as distributed all types have the same metatype (PyType_Type, which is also its own metatype), this is not a requirement. The type of classic classes, known as types.ClassType, can also be considered a distinct metatype.&lt;ref&gt;{{cite web|last=Guido van Rossum|title=Subtyping Built-in Types|url=https://www.python.org/dev/peps/pep-0253/|accessdate=2 October 2012}}&lt;/ref&gt;

&lt;source lang=python&gt;
&gt;&gt;&gt; a = 0
&gt;&gt;&gt; type(a)
&lt;type 'int'&gt;
&gt;&gt;&gt; type(type(a))
&lt;type 'type'&gt;
&gt;&gt;&gt; type(type(type(a)))
&lt;type 'type'&gt;
&gt;&gt;&gt; type(type(type(type(a))))
&lt;type 'type'&gt;
&lt;/source&gt;

===Java===

In Java, '''is-a''' relation between the type parameters of one class or interface and the type parameters of another are determined by the extends and [[Interface (Java)|implements]] clauses.

Using the Collections classes, ArrayList&lt;E&gt; implements List&lt;E&gt;, and List&lt;E&gt; extends Collection&lt;E&gt;. So ArrayList&lt;String&gt; is a subtype of List&lt;String&gt;, which is a subtype of Collection&lt;String&gt;. The subtyping relationship is preserved between the types automatically. When we define an interface, PayloadList, that associates an optional value of generic type P with each element. Its declaration might look like:

&lt;source lang=java&gt;
interface PayloadList&lt;E, P&gt; extends List&lt;E&gt; {
    void setPayload(int index, P val);
    ...
}
&lt;/source&gt;

The following parameterizations of PayloadList are subtypes of List&lt;String&gt;:

&lt;source lang=java&gt;
PayloadList&lt;String, String&gt;
PayloadList&lt;String, Integer&gt;
PayloadList&lt;String, Exception&gt;
&lt;/source&gt;

==Liskov substitution principle==
{{main|Liskov substitution principle}}
Liskov substitution principle explains a property, ''"If for each object o1 of type S there is an object o2 of type T such that for all programs P de&#64257;ned in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T,"''.&lt;ref&gt;{{cite book|last=Liskov|first=Barbara|title=Data Abstraction and Hierarchy|date=May 1988|publisher=SIGPLAN Notices}}&lt;/ref&gt; Following example shows a violation of LSP.

&lt;source lang=cpp&gt;void DrawShape(const Shape&amp; s)
{
  if (typeid(s) == typeid(Square))
    DrawSquare(static_cast&lt;Square&amp;&gt;(s));
  else if (typeid(s) == typeid(Circle))
    DrawCircle(static_cast&lt;Circle&amp;&gt;(s));
}&lt;/source&gt;
Obviously, the DrawShape function is badly formatted. It has to know about every derivative classes of Shape class. Also, it should be changed whenever new subclass of Shape are created. In [[Object-oriented design|Object Oriented Design]], many view the structure of this as anathema.

Here is a more subtle example of violation of LSP

&lt;source lang=cpp&gt;
class Rectangle
{
  public:
    void   SetWidth(double w)  { itsWidth = w; }
    void   SetHeight(double h) { itsHeight = h; }
    double GetHeight() const   { return itsHeight; }
    double GetWidth() const    { return itsWidth; }
  private:
    double itsWidth;
    double itsHeight;
};
&lt;/source&gt;
This works well but when it comes to Square class, which inherits Rectangle class, it violates LSP even though the '''is-a''' relationship holds between Rectangle and Square. Because square is rectangular. The following example overrides two functions, Setwidth and SetHeight, to fix the problem. But fixing the code implies that the design is faulty.

&lt;source lang=cpp&gt;
public class Square : Rectangle
{
  public:
    virtual void SetWidth(double w);
    virtual void SetHeight(double h);
};
void Square::SetWidth(double w)
{
    Rectangle::SetWidth(w);
    Rectangle::SetHeight(w);
}
void Square::SetHeight(double h)
{
    Rectangle::SetHeight(h);
    Rectangle::SetWidth(h);
}
&lt;/source&gt;

The following example, function g just works for Rectangle class but not for Square, and so the open-closed principle has been violated.

&lt;source lang=cpp&gt;
void g(Rectangle&amp; r)
{
  r.SetWidth(5);
  r.SetHeight(4);
  assert(r.GetWidth() * r.GetHeight()) == 20);
}
&lt;/source&gt;
&lt;ref&gt;{{cite web|title=The Liskov Substitution Principle|url=http://www.objectmentor.com/resources/articles/lsp.pdf|publisher=Robert C. Martin, 1996|accessdate=2 October 2012}}&lt;/ref&gt;

== See also ==

* [[Inheritance (object-oriented programming)]]
* [[Liskov substitution principle]] (in [[object-oriented programming]])
* [[Subsumption (disambiguation)|Subsumption]]&lt;!--This deliberately links to the disambiguation page--&gt;
* Is-a
** [[Hypernymy]] (and [[supertype]])
** [[Hyponymy]] (and [[subtype]])
* [[Has-a]]
** [[Holonymy]]
** [[Meronymy]]

==Notes==
{{reflist|30em}}

==References==
* [[Ronald J. Brachman]]; [http://dblp.uni-trier.de/rec/bibtex/journals/computer/Brachman83 What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks]. IEEE Computer, 16 (10); October 1983
* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57

[[Category:Object-oriented programming]]
[[Category:Knowledge representation]]
[[Category:Abstraction]]
[[Category:Articles with example Java code]]</text>
      <sha1>idyhmbc1oi9n4yan61btcud1ojytwg7</sha1>
    </revision>
  </page>
  <page>
    <title>Yale shooting problem</title>
    <ns>0</ns>
    <id>2778728</id>
    <revision>
      <id>697559148</id>
      <parentid>697559115</parentid>
      <timestamp>2015-12-31T07:43:14Z</timestamp>
      <contributor>
        <username>Koavf</username>
        <id>205121</id>
      </contributor>
      <minor />
      <comment>fixed [[MOS:DASH|dashes]] using a [[User:GregU/dashes.js|script]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7552" xml:space="preserve">The '''Yale shooting problem''' is a conundrum or scenario in formal situational [[logic]] on which early logical solutions to the [[frame problem]] fail. The name of this problem derives from its inventors, [[Steve Hanks]] and [[Drew McDermott]], working at [[Yale University]] when they proposed it. In this scenario, Fred (later identified as a [[turkey (bird)|turkey]]) is initially alive and a gun is initially unloaded. Loading the gun, waiting for a moment, and then shooting the gun at Fred is expected to kill Fred. However, if [[inertia]] is formalized in logic by minimizing the changes in this situation, then it cannot be uniquely proved that Fred is dead after loading, waiting, and shooting. In one solution, Fred indeed dies; in another (also logically correct) solution, the gun becomes mysteriously unloaded and Fred survives.

Technically, this scenario is described by two [[fluent (artificial intelligence)|fluents]] (a fluent is a condition that can change [[truth value]] over time): &lt;math&gt;alive&lt;/math&gt; and &lt;math&gt;loaded&lt;/math&gt;. Initially, the first condition is true and the second is false. Then, the gun is loaded, some time passes, and the gun is fired. Such problems can be formalized in logic by considering four time points &lt;math&gt;0&lt;/math&gt;, &lt;math&gt;1&lt;/math&gt;, &lt;math&gt;2&lt;/math&gt;, and &lt;math&gt;3&lt;/math&gt;, and turning every fluent such as &lt;math&gt;alive&lt;/math&gt; into a predicate &lt;math&gt;alive(t)&lt;/math&gt; depending on time. A direct formalization of the statement of the Yale shooting problem in logic is the following one:

: &lt;math&gt;alive(0)&lt;/math&gt;
: &lt;math&gt;\neg loaded(0)&lt;/math&gt;
: &lt;math&gt;true \rightarrow loaded(1)&lt;/math&gt;
: &lt;math&gt;loaded(2) \rightarrow \neg alive(3)&lt;/math&gt;

The first two formulae represent the initial state. The third formula formalizes the effect of loading the gun at time &lt;math&gt;0&lt;/math&gt;. The fourth formula formalizes the effect of shooting at Fred at time &lt;math&gt;2&lt;/math&gt;. This is a simplified formalization in which action names are neglected and the effects of actions are directly specified for the time points in which the actions are executed. See [[situation calculus]] for details.

The formulae above, while being direct formalizations of the known facts, do not suffice to correctly characterize the domain. Indeed, &lt;math&gt;\neg alive(1)&lt;/math&gt; is consistent with all these formulae, although there is no reason to believe that Fred dies before the gun has been shot. The problem is that the formulae above only include the effects of actions, but do not specify that all fluents not changed by the actions remain the same. In other words, a formula &lt;math&gt;alive(0) \equiv alive(1)&lt;/math&gt; must be added to formalize the implicit assumption that loading the gun ''only'' changes the value of &lt;math&gt;loaded&lt;/math&gt; and not the value of &lt;math&gt;alive&lt;/math&gt;. The necessity of a large number of formulae stating the obvious fact that conditions do not change unless an action changes them is known as the [[frame problem]].

An early solution to the frame problem was based on minimizing the changes. In other words, the scenario is formalized by the formulae above (that specify only the effects of actions) and by the assumption that the changes in the fluents over time are as minimal as possible. The rationale is that the formulae above enforce all effect of actions to take place, while minimization should restrict the changes to exactly those due to the actions.

In the Yale shooting scenario, one possible evaluation of the fluents in which the changes are minimized is the following one.

{| cellpadding="5"
| &lt;math&gt;alive(0)&lt;/math&gt;
| &lt;math&gt;alive(1)&lt;/math&gt; 
| &lt;math&gt;alive(2)&lt;/math&gt; 
| &lt;math&gt;\neg alive(3)&lt;/math&gt;
|-
| &lt;math&gt;\neg loaded(0)&lt;/math&gt;
| &lt;math&gt;loaded(1)&lt;/math&gt;
| &lt;math&gt;loaded(2)&lt;/math&gt;
| &lt;math&gt;loaded(3)&lt;/math&gt;
|}

This is the expected solution. It contains two fluent changes: &lt;math&gt;loaded&lt;/math&gt; becomes true at time 1 and &lt;math&gt;alive&lt;/math&gt; becomes false at time 3. The following evaluation also satisfies all formulae above.

{| cellpadding="5"
| &lt;math&gt;alive(0)&lt;/math&gt;
| &lt;math&gt;alive(1)&lt;/math&gt; 
| &lt;math&gt;alive(2)&lt;/math&gt; 
| &lt;math&gt;alive(3)&lt;/math&gt;
|-
| &lt;math&gt;\neg loaded(0)&lt;/math&gt;
| &lt;math&gt;loaded(1)&lt;/math&gt;
| &lt;math&gt;\neg loaded(2)&lt;/math&gt;
| &lt;math&gt;\neg loaded(3)&lt;/math&gt;
|}

In this evaluation, there are still two changes only: &lt;math&gt;loaded&lt;/math&gt; becomes true at time 1 and false at time 2. As a result, this evaluation is considered a valid description of the evolution of the state, although there is no valid reason to explain &lt;math&gt;loaded&lt;/math&gt; being false at time 2. The fact that minimization of changes leads to wrong solution is the motivation for the introduction of the Yale shooting problem.

While the Yale shooting problem has been considered a severe obstacle to the use of logic for formalizing dynamical scenarios, solutions to it are known since the late 1980s. One solution involves the use of [[predicate completion]] in the specification of actions: according to this solution, the fact that shooting causes Fred to die is formalized by the preconditions: ''alive'' and ''loaded'', and the effect is that ''alive'' changes value (since ''alive'' was true before, this corresponds to ''alive'' becoming false). By turning this implication into an ''if and only if'' statement, the effects of shooting are correctly formalized. (Predicate completion is more complicated when there is more than one implication involved.)

A solution proposed by [[Erik Sandewall]] was to include a new condition of occlusion, which formalizes the &#8220;permission to change&#8221; for a fluent. The effect of an action that might change a fluent is therefore that the fluent has the new value, and that the occlusion is made (temporarily) true. What is minimized is not the set of changes, but the set of occlusions being true. Another constraint specifying that no fluent changes unless occlusion is true completes this solution.

The Yale shooting scenario is also correctly formalized by the [[Ray Reiter|Reiter]] version of the [[situation calculus]], the [[fluent calculus]], and the [[action description language]]s.

In 2005, the 1985 paper in which the Yale shooting scenario was first described received the [[AAAI Classic Paper award]]. In spite of being a solved problem, that example is still sometimes mentioned in recent research papers, where it is used as an illustrative example (e.g., for explaining the syntax of a new logic for reasoning about actions), rather than being presented as a problem.

==See also==

* [[Circumscription (logic)]]
* [[Frame problem]]
* [[Situation calculus]]

==References==

* M. Gelfond and V. Lifschitz (1993). Representing action and change by logic programs. ''Journal of Logic Programming'', 17:301&#8211;322.
* S. Hanks and D. McDermott (1987). Nonmonotonic logic and temporal projection. ''Artificial Intelligence'', 33(3):379&#8211;412.
* J. McCarthy (1986). Applications of circumscription to formalizing common-sense knowledge. ''Artificial Intelligence'', 28:89&#8211;116.
* T. Mitchell and H. Levesque (2006). The 2005 AAAI Classic Paper awards. "AI Magazine", 26(4):98&#8211;99.
* R. Reiter (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Vladimir Lifschitz, editor, ''Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy'', pages 359&#8211;380. Academic Press, New York.
* E. Sandewall (1994). ''Features and Fluents''. Oxford University Press.

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:1987 introductions]]</text>
      <sha1>8gx9o9sexs4v9cv997s926nq1pdgfx4</sha1>
    </revision>
  </page>
  <page>
    <title>Defeasible reasoning</title>
    <ns>0</ns>
    <id>2628057</id>
    <revision>
      <id>749302254</id>
      <parentid>741418890</parentid>
      <timestamp>2016-11-13T17:08:02Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* References */clean up; http&amp;rarr;https for [[Google Books]] and other Google services using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18785" xml:space="preserve">{{No footnotes|date=April 2010}}
In [[logic]], '''defeasible reasoning''' is a kind of [[reasoning]] that is rationally compelling though not [[deductive reasoning|deductively valid]].&lt;ref&gt;{{cite web | url=http://plato.stanford.edu/entries/reasoning-defeasible | title="Defeasbile Reasoning," ''Stanford Encyclopedia of Philosophy | accessdate=1 July 2016 }}&lt;/ref&gt; The distinction between defeasibility and indefeasibility may be seen in the context of this joke:

:During a train trip through the countryside, an engineer, a physicist, and a mathematician observe a flock of sheep. The engineer remarks, "I see that the sheep in this region are white." The physicist offers a correction, "''Some'' sheep in this region are white." And the mathematician responds, "In this region there exist sheep that are white on at least one side."

The engineer in this story has reasoned defeasibly; since [[engineering]] is a highly practical discipline, it is receptive to generalizations. In particular, engineers cannot and need not defer decisions until they have acquired perfect and complete knowledge. But [[mathematics|mathematical reasoning]], having different goals, inclines one to account for even the rare and special cases, and thus typically leads to a stance that is indefeasible.

Defeasible reasoning is a particular kind of non-demonstrative reasoning, where the reasoning does not produce a full, complete, or final demonstration of a claim, i.e., where fallibility and corrigibility of a conclusion are acknowledged. In other words defeasible reasoning produces a [[Wiktionary:contingent|contingent]] statement or claim.  Other kinds of non-demonstrative reasoning are [[probabilistic reasoning]], [[inductive reasoning]], [[statistical]] reasoning, [[abductive reasoning]], and [[paraconsistent]] reasoning.  Defeasible reasoning is also a kind of [[ampliative]] reasoning because its conclusions reach beyond the pure meanings of the premises.

The differences between these kinds of reasoning correspond to differences about the conditional that each kind of reasoning uses, and on what premise (or on what authority) the conditional is adopted:
* ''[[deductive reasoning|Deductive]]'' (from meaning postulate, axiom, or contingent assertion): if ''p'' then ''q'' (i.e., ''q'' or ''not-p'')
* ''Defeasible'' (from authority): if ''p'' then (defeasibly) ''q''
* ''[[Probabilistic logic|Probabilistic]]'' (from combinatorics and indifference): if ''p'' then (probably) ''q''
* ''[[Statistics|Statistical]]'' (from data and presumption):  the frequency of ''q''s among ''p''s is high (or inference from a model fit to data); hence, (in the right context) if ''p'' then (probably) ''q''
* ''[[inductive reasoning|Inductive]]'' (theory formation; from data, coherence, simplicity, and confirmation): (inducibly) "if ''p'' then ''q''"; hence, if ''p'' then (deducibly-but-revisably) ''q''
* ''[[abductive reasoning|Abductive]]'' (from data and theory):  ''p'' and ''q'' are correlated, and ''q'' is sufficient for ''p''; hence, if ''p'' then (abducibly) ''q'' as cause

Defeasible reasoning finds its fullest expression in [[jurisprudence]], [[ethics]] and [[moral philosophy]], [[epistemology]], [[pragmatics]] and conversational [[Convention (norm)|conventions]] in [[linguistics]], [[Constructivist epistemology|constructivist]] [[Decision theory|decision theories]], and in [[knowledge representation]] and [[planning]] in [[artificial intelligence]].  It is also closely identified with [[prima facie]] (presumptive) reasoning (i.e., reasoning on the "face" of evidence), and [[ceteris paribus]] (default) reasoning (i.e., reasoning, all things "being equal").

== History ==

Though [[Aristotle]] differentiated the forms of reasoning that are valid for [[logic]] and [[philosophy]] from the more general ones that are used in everyday life (see [[dialectics]] and [[rhetoric]]), 20th century philosophers mainly concentrated on deductive reasoning. At the end of the 19th century, logic texts would typically survey both demonstrative and non-demonstrative reasoning, often giving more space to the latter. However, after the blossoming of [[mathematical logic]] at the hands of [[Bertrand Russell]], [[Alfred North Whitehead]] and [[Willard van Orman Quine]], latter-20th century logic texts paid little attention to the non-deductive modes of inference.

There are several notable exceptions. [[John Maynard Keynes]] wrote his dissertation on non-demonstrative reasoning, and influenced the thinking of [[Ludwig Wittgenstein]] on this subject. Wittgenstein, in turn, had many admirers, including the [[positivist]] legal scholar [[H.L.A. Hart]] and the [[speech act]] linguist [[John L. Austin]], [[Stephen Toulmin]] in rhetoric ([[Chaim Perelman]] too), the moral theorists [[W.D. Ross]] and [[C.L. Stevenson]], and the [[vagueness]] epistemologist/ontologist [[Friedrich Waismann]].

The etymology of ''defeasible'' usually refers to Middle English law of contracts, where a condition of defeasance is a clause that can invalidate or annul a contract or deed. Though ''defeat'', ''dominate'', ''defer'', ''defy'', ''deprecate'' and ''derogate'' are often used in the same contexts as ''defeasible,'' the verbs ''annul'' and ''invalidate'' (and ''nullify,'' ''overturn,'' ''rescind,'' ''vacate,'' ''repeal,'' ''debar'', ''void'', ''cancel'', ''countermand'', ''preempt'', etc.) are more properly correlated with the concept of defeasibility than those words beginning with the letter ''d''. Many dictionaries do contain the verb, ''to defease'' with past participle, ''defeased.''

Philosophers in moral theory and rhetoric had taken defeasibility largely for granted when American epistemologists rediscovered Wittgenstein's thinking on the subject: John Ladd, [[Roderick Chisholm]], [[Roderick Firth]], [[Ernest Sosa]], [[Robert Nozick]], and [[John L. Pollock]] all began writing with new conviction about how ''appearance as red'' was only a defeasible reason for believing something to be red.  More importantly Wittgenstein's orientation toward [[language-games]] (and away from [[semantics]]) emboldened these epistemologists to manage rather than to expurgate ''prima facie'' logical inconsistency.

At the same time (in the mid-1960s), two more students of Hart and Austin at Oxford, [[Brian Barry]] and [[David Gauthier]], were applying defeasible reasoning to political argument and practical reasoning (of action), respectively. [[Joel Feinberg]] and [[Joseph Raz]] were beginning to produce equally mature works in ethics and jurisprudence informed by defeasibility.  

By far the most significant works on defeasibility by the mid-1970s were in epistemology, where [[John L. Pollock|John Pollock]]'s 1974 ''Knowledge and Justification'' popularized his terminology of ''undercutting'' and ''rebutting'' (which mirrored the analysis of Toulmin). Pollock's work was significant precisely because it brought defeasibility so close to philosophical logicians. The failure of logicians to dismiss defeasibility in epistemology (as Cambridge's logicians had done to Hart decades earlier) landed defeasible reasoning in the philosophical mainstream.  

Defeasibility had always been closely related to argument, rhetoric, and law, except in epistemology, where the chains of reasons, and the origin of reasons, were not often discussed. [[Nicholas Rescher]]'s ''Dialectics'' is an example of how difficult it was for philosophers to contemplate more complex systems of defeasible reasoning. This was in part because proponents of [[informal logic]] became the keepers of argument and rhetoric while insisting that formalism was anathema to argument.

About this time, researchers in [[artificial intelligence]] became interested in [[non-monotonic reasoning]] and its [[semantics]]. With philosophers such as Pollock and Donald Nute (e.g., [[defeasible logic]]), dozens of computer scientists and logicians produced complex systems of defeasible reasoning between 1980 and 2000. No single system of defeasible reasoning would emerge in the same way that Quine's system of logic became a de facto standard. Nevertheless, the 100-year headstart on non-demonstrative logical calculi, due to [[George Boole]], [[Charles Sanders Peirce]], and [[Gottlob Frege]] was being closed: both demonstrative and non-demonstrative reasoning now have formal calculi.

There are related (and slightly competing) systems of reasoning that are newer than systems of defeasible reasoning, e.g., [[belief revision]] and [[dynamic logic (modal logic)|dynamic logic]]. The dialogue logics of [[Charles Leonard Hamblin|Charles Hamblin]] and Jim Mackenzie, and their colleagues, can also be tied closely to defeasible reasoning. Belief revision is a non-constructive specification of the desiderata with which, or constraints according to which, epistemic change takes place. Dynamic logic is related mainly because, like paraconsistent logic, the reordering of premises can change the set of justified conclusions. Dialogue logics introduce an adversary, but are like belief revision theories in their adherence to deductively consistent states of belief.

==Political and judicial use==
Many political philosophers have been fond of the word ''indefeasible'' when referring to rights, e.g., that were ''inalienable,'' ''divine,'' or ''indubitable.''  For example, in the 1776 [[Virginia Declaration of Rights]], "community hath an indubitable, inalienable, and indefeasible right to reform, alter or abolish government..." (also attributed to [[James Madison]]); and [[John Adams]], "The people have a right, an indisputable, unalienable, indefeasible, divine right to that most dreaded and envied kind of knowledge &#8211; I mean of the character and conduct of their rulers."
Also, [[Lord Aberdeen]]:  "indefeasible right inherent in the British Crown" and [[Gouverneur Morris]]:  "the Basis of our own Constitution is the indefeasible Right of the People."  Scholarship about [[Abraham Lincoln]] often cites these passages in the justification of secession.  Philosophers who use the word ''defeasible'' have historically had different world views from those who use the word ''indefeasible'' (and this distinction has often been mirrored by Oxford and Cambridge zeitgeist); hence it is rare to find authors who use both words.

In judicial opinions, the use of ''defeasible'' is commonplace.  There is however disagreement among legal logicians whether ''defeasible reasoning'' is central, e.g., in the consideration of ''open texture'', [[precedent]], [[wikt:exception|exceptions]], and ''rationales'', or whether it applies only to explicit defeasance clauses.  [[H.L.A. Hart]] in ''[[The Concept of Law]]'' gives two famous examples of defeasibility:  "No vehicles in the park" (except during parades); and "Offer, acceptance, and memorandum produce a contract" (except when the contract is illegal, the parties are minors, inebriated, or incapacitated, etc.).

== Specificity ==

One of the main disputes among those who produce systems of defeasible reasoning is the status of a ''rule of specificity.''  In its simplest form, it is the same rule as subclass [[inheritance (computer science)|inheritance]] preempting class inheritance:  

  (R1) if ''r'' then (defeasibly) ''q''                  e.g., if bird, then can fly
  (R2) if ''p'' then (defeasibly) ''not-q''              e.g., if penguin, then cannot fly
  (O1) if ''p'' then (deductively) ''r''                 e.g., if penguin, then bird
  (M1) arguably, p                               e.g., arguably, penguin
  (M2) R2 is a more specific reason than R1      e.g., R2 is better than R1
  (M3) therefore, arguably, not-q                e.g., therefore, arguably, not-flies

Approximately half of the systems of defeasible reasoning discussed today adopt a rule of specificity, while half expect that such ''preference'' rules be written explicitly by whoever provides the defeasible reasons.  For example, Rescher's dialectical system uses specificity, as do early systems of multiple inheritance (e.g., [[David Touretzky]]) and the early argument systems of Donald Nute and of [[Guillermo Simari]] and [[Ronald Loui]].  Defeasible reasoning accounts of  precedent ([[stare decisis]] and [[case-based reasoning]]) also make use of specificity (e.g., [[Joseph Raz]] and the work of Kevin D. Ashley and Edwina Rissland).  Meanwhile, the argument systems of Henry Prakken and Giovanni Sartor, of Bart Verheij and Jaap Hage, and the system of Phan Minh Dung do not adopt such a rule.

== Nature of defeasibility ==

There is a distinct difference between those who theorize about defeasible reasoning as if it were a system of confirmational revision (with affinities to [[belief revision]]), and those who theorize about defeasibility as if it were the result of further (non-empirical) investigation.  There are at least three kinds of further non-empirical investigation:  progress in a lexical/syntactic process, progress in a computational process, and progress in an adversary or legal proceeding.  

'''''Defeasibility as corrigibility:'''''  Here, a person learns something new that annuls a prior inference.  In this case, defeasible reasoning provides a constructive mechanism for belief revision, like a [[truth maintenance system]] as envisioned by Jon Doyle.

'''''Defeasibility as shorthand for preconditions:'''''  Here, the author of a set of rules or legislative code is writing rules with exceptions.  Sometimes a set of defeasible rules can be rewritten, with more cogency, with explicit (local) pre-conditions instead of (non-local) competing rules.  Many non-monotonic systems with [[fixed point (mathematics)|fixed-point]] or [[preferential]] semantics fit this view.  However, sometimes the rules govern a process of argument (the last view on this list), so that they cannot be re-compiled into a set of deductive rules lest they lose their force in situations with incomplete knowledge or incomplete derivation of preconditions.  

'''''Defeasibility as an [[anytime algorithm]]:'''''  Here, it is assumed that calculating arguments takes time, and at any given time, based on a subset of the potentially constructible arguments, a conclusion is defeasibly justified.  [[Isaac Levi]] has protested against this kind of defeasibility, but it is well-suited to the heuristic projects of, for example, [[Herbert A. Simon]].  On this view, the ''best move so far'' in a chess-playing program's analysis at a particular depth is a defeasibly justified conclusion.  This interpretation works with either the prior or the next semantical view.

'''''Defeasibility as a means of controlling an investigative or social process:'''''  Here, justification is the result of the right kind of procedure (e.g., a fair and efficient hearing), and defeasible reasoning provides impetus for pro and con responses to each other.  Defeasibility has to do with the alternation of verdict as locutions are made and cases presented, not the changing of a mind with respect to new (empirical) discovery.  Under this view, defeasible reasoning and defeasible argumentation refer to the same phenomenon.

==See also==
* [[Defeasible estate]]
* [[Indefeasible rights of use]]
* [[Argument (logic)]]
* [[Prima facie]]
* [[Practical reasoning]]
* [[Pragmatics]]
* [[Non-monotonic reasoning]]

== References ==
{{Reflist}}
* [http://www.springerlink.com/index/UQ708JX7XG823H5F.pdf Defeasible logic], Donald Nute, Lecture Notes in Computer Science, Springer, 2003.
* [http://portal.acm.org/citation.cfm?id=371581 Logical models of argument], Carlos Chesnevar, et al., ACM Computing Surveys 32:4, 2000.
* [https://books.google.com/books?hl=en&amp;lr=&amp;id=bQHce6eNhDIC&amp;oi=fnd&amp;pg=PA219&amp;dq=prakken&amp;ots=h7xemV-dM1&amp;sig=E6Ar7mAiBU0raO-rlNmzq8-8HG4 Logics for defeasible argumentation], Henry Prakken and Gerard Vreeswijk, in Handbook of Philosophical Logic, [[Dov M. Gabbay]], [[Franz Guenthner]], eds., Kluwer, 2002.
* [https://books.google.com/books?hl=en&amp;lr=&amp;id=DN5ERAAxUSYC&amp;oi=fnd&amp;pg=PR9&amp;dq=rescher&amp;ots=vRu4s0Ely-&amp;sig=__Dvw746CkFNdCaAdNLCGImTbFU Dialectics], [[Nicholas Rescher]], SUNY Press, 1977.
* [http://linkinghub.elsevier.com/retrieve/pii/S0364021387800174 Defeasible reasoning], John Pollock, Cognitive Science, 1987.
* [https://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=7198700474843277547 Knowledge and Justification], John Pollock, Princeton University Press, 1974.
* [http://webdigg.net/Defeasible/Defeasible-reasoning/ Abstract argumentation systems], Gerard Vreeswijk, Artificial Intelligence, 1997.
* [http://portal.acm.org/citation.cfm?id=222099 Hart's critics on defeasible concepts and ascriptivism], [[Ronald Loui]], Proc. 5th Intl. Conf. on AI and Law, 1995.
* [https://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=7525164436422571935 Political argument], [[Brian Barry]], Routledge &amp; Kegan Paul, 1970.
* [https://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=8944770465668267468 The uses of argument], [[Stephen Toulmin]], Cambridge University Press, 1958.
* [http://portal.acm.org/citation.cfm?id=981352&amp;dl= Discourse relations and defeasible knowledge], Alex Lascarides and Nicholas Asher, Proc. of the 29th Meeting of the Assn. for Comp. Ling., 1991.
* [http://journals.cambridge.org/action/displayAbstract;jsessionid=E68F5CAC6B0001D1ABEFD7C8C24F919F.tomcat1?fromPage=online&amp;aid=191503 Defeasible logic programming: an argumentative approach], Alejandro Garcia and [[Guillermo Simari]], Theory and Practice of Logic Programming 4:95&#8211;138, 2004. 
* [http://portal.acm.org/citation.cfm?id=180954.180957 Philosophical foundations of deontic logic and the logic of defeasible conditionals], Carlos Alchourron, in Deontic logic in computer science: normative system specification, J. Meyer, R. Wieringa, eds., Wiley, 1994.
* [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6TYF-47YRKSD-7C&amp;_user=10&amp;_coverDate=02%2F29%2F1992&amp;_rdoc=1&amp;_fmt=high&amp;_orig=browse&amp;_origin=browse&amp;_zone=rslt_list_item&amp;_srch=doc-info%28%23toc%235617%231992%23999469997%23391734%23FLP%23display%23Volume%29&amp;_cdi=5617&amp;_sort=d&amp;_docanchor=&amp;_ct=16&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=0735cebb41ce81bdfe8e260dbef2c71d&amp;searchtype=a A Mathematical Treatment of Defeasible Reasoning and its Implementation.] [[Guillermo Simari]], [[Ronald Loui]], Artificial Intelligence Journal, 53(2&#8211;3): 125&#8211;157 (1992).

== External links ==
* [http://plato.stanford.edu/entries/reasoning-defeasible/ Article on Defeasible Reasoning] in the [[Stanford Encyclopedia of Philosophy]]
* [http://william-king.www.drexel.edu/top/prin/txt/Intro/Eco112c.html An example of defeasible reasoning in action]

[[Category:Epistemology]]
[[Category:Logic]]
[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Reasoning]]</text>
      <sha1>4tt9whmfinqaiusrdu0p14itaydgjd3</sha1>
    </revision>
  </page>
  <page>
    <title>Preferential entailment</title>
    <ns>0</ns>
    <id>3011353</id>
    <revision>
      <id>743599817</id>
      <parentid>631506043</parentid>
      <timestamp>2016-10-10T10:15:08Z</timestamp>
      <contributor>
        <username>Finlay McWalter</username>
        <id>22619</id>
      </contributor>
      <comment>fix misplaced newlines which broke templates</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1532" xml:space="preserve">'''Preferential entailment''' is a [[non-monotonic logic]] based on selecting only [[model (logic)|model]]s that are considered the most plausible. The plausibility of models is expressed by an ordering among models called a preference relation, hence the name preference entailment.

Formally, given a [[propositional formula]] &lt;math&gt;F&lt;/math&gt; and an ordering over propositional models &lt;math&gt;\leq&lt;/math&gt;, preferential [[entailment]] selects only the models of &lt;math&gt;F&lt;/math&gt; that are minimal according to &lt;math&gt;\leq&lt;/math&gt;. This selection leads to a non-monotonic inference relation: &lt;math&gt;F \models_\text{pref} G&lt;/math&gt; holds if and only if all minimal models of &lt;math&gt;F&lt;/math&gt; according to &lt;math&gt;\leq&lt;/math&gt; are also models of &lt;math&gt;G&lt;/math&gt;.&lt;ref name="s87"&gt;{{citation|last=Shoham|first=Y.|year=1987|contribution=Nonmonotonic logics: Meaning and utility|title=Proc. of the 10th Int. Joint Conf. on Artificial Intelligence (IJCAI&#8217;87)|pages=388&#8211;392|url=http://ijcai.org/Past%20Proceedings/IJCAI-87-VOL1/PDF/079.pdf}}.&lt;/ref&gt;

[[Circumscription (logic)|Circumscription]] can be seen as the particular case of preferential entailment when the ordering is based on containment of the sets of variables assigned to true (in the propositional case) or containment of the extensions of predicates (in the first-order logic case).&lt;ref name="s87"/&gt;

==See also==
* [[Rational consequence relation]]

==References==
{{reflist}}

[[Category:Logic in computer science]]
[[Category:Knowledge representation]]
[[Category:Non-classical logic]]</text>
      <sha1>9oif7oa68p6h2rwi6wti4glfsysefek</sha1>
    </revision>
  </page>
  <page>
    <title>Keyword AAA</title>
    <ns>0</ns>
    <id>13302227</id>
    <revision>
      <id>737088935</id>
      <parentid>714834082</parentid>
      <timestamp>2016-08-31T18:04:05Z</timestamp>
      <contributor>
        <username>Staszek Lem</username>
        <id>12536756</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="764" xml:space="preserve">{{notability|date=August 2016}}
{{Multiple issues|
{{Orphan|date=April 2016}}
{{no footnotes|date=December 2011}}
}}

'''Keyword AAA''' is a [[thesaurus]] created by the [[State Records Authority of New South Wales]]. It is often used to [[categorize|categorise]] documents in a [[document management system]]. The thesaurus is often implemented in terms of [[ISO 2788]].

==External links==
* [http://www.records.nsw.gov.au/recordkeeping/resources/keyword-products/keyword-aaa Keyword AAA Overview]
* [https://www.records.nsw.gov.au/recordkeeping/advice/records-classification/developing-and-implementing-a-keyword-thesaurus Developing and implementing a keyword thesaurus]

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>89d6bf99t8parkgwg80s1460ct82d25</sha1>
    </revision>
  </page>
  <page>
    <title>Open Knowledge Base Connectivity</title>
    <ns>0</ns>
    <id>4720390</id>
    <revision>
      <id>559794202</id>
      <parentid>544344815</parentid>
      <timestamp>2013-06-13T23:17:53Z</timestamp>
      <contributor>
        <username>Disavian</username>
        <id>784547</id>
      </contributor>
      <comment>clarify source and sponsor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="829" xml:space="preserve">{{Unreferenced stub|auto=yes|date=December 2009}}
'''Open Knowledge Base Connectivity''' ('''OKBC''')  is a [[protocol (computer science)|protocol]] and an [[application programming interface|API]] for accessing knowledge in [[knowledge representation]] systems such as [[ontology (computer science)|ontology]] repositories and [[object-relational database]]s. It is somewhat complementary to the [[Knowledge Interchange Format]] that serves as a general representation language for knowledge. It is developed by [[SRI International]]'s [[Artificial Intelligence Center]] for [[DARPA]]'s High Performance Knowledge Base program (HPKB).

==External links==
* [http://www.ai.sri.com/~okbc/ Open Knowledge Base Connectivity Home Page]

[[Category:SRI International software]]
[[Category:Knowledge representation]]

{{Comp-sci-stub}}</text>
      <sha1>4608oy95hr1l0stdxuuqia3mffqkktj</sha1>
    </revision>
  </page>
  <page>
    <title>FrameNet</title>
    <ns>0</ns>
    <id>2112884</id>
    <revision>
      <id>758680804</id>
      <parentid>758679587</parentid>
      <timestamp>2017-01-06T22:27:00Z</timestamp>
      <contributor>
        <username>Kingzoko</username>
        <id>30042487</id>
      </contributor>
      <comment>Made formatting of upper/lowercase and italicization of 'frames' and 'frame elements' more consistent.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10451" xml:space="preserve">{{refimprove|date=March 2012}}
In [[computational linguistics]], '''FrameNet''' is a project housed at the [[International Computer Science Institute]] in [[Berkeley, California]] which produces an electronic resource based on a theory of meaning called
[[Frame semantics (linguistics)|frame semantics]]. FrameNet reveals for example that the sentence "John sold a car to Mary" essentially describes the same basic situation (semantic frame) as "Mary bought a car from John", just from a different perspective. A semantic frame can be thought of as a conceptual structure describing an event, relation, or object and the participants in it. The FrameNet [[lexical database]] contains over 1,200 semantic ''frames'', 13,000 ''lexical units'' (a pairing of a [[word]] with a [[Meaning (linguistics)|meaning]]; [[Polysemy|polysemous]] words are represented by several ''lexical units'') and 202,000 example sentences. FrameNet is largely the creation of [[Charles J. Fillmore]], who developed the theory of frame semantics that the project is based on, and was initially the project leader when the project began in 1997.&lt;ref name="Goddard2011"&gt;{{cite book|author=Cliff Goddard|title=Semantic Analysis: A Practical Introduction|url=https://books.google.com/books?id=qfar1cmATvUC&amp;pg=PA78|accessdate=21 March 2012|date=25 September 2011|publisher=Oxford University Press|isbn=978-0-19-956028-8|pages=78&#8211;81}}&lt;/ref&gt; Collin Baker became the project manager in 2000.&lt;ref name="Linguistic Analysis"&gt;{{cite book|title=The Oxford Handbook of Linguistic Analysis|url=https://books.google.com/books?id=7plqH2gSq1wC&amp;pg=PP20|accessdate=21 March 2012|editor1-last=Heine|editor1-first=Bernd|editor2-last=Narrog|editor2-first=Heiko|publisher=Oxford University Press|isbn=978-0-19-160925-1|page=20}}&lt;/ref&gt; The FrameNet project has been influential in both linguistics and natural language processing, where it led to the task of automatic [[Semantic Role Labeling]].

==Concepts==

===Frames===
A frame is a schematic representation of a situation involving various participants, props, and other conceptual roles. Examples of frame names are &lt;tt&gt;Being_born&lt;/tt&gt; and &lt;tt&gt;Locative_relation&lt;/tt&gt;. A frame in FrameNet contains a textual description of what it represents (a frame definition), associated frame elements, lexical units, example sentences, and frame-to-frame relations.

===Frame elements===
Frame elements (FE) provide additional information to the semantic structure of a sentence. Each frame has a number of core and non-core FEs which can be thought of as semantic roles. Core FEs are essential to the meaning of the frame while non-core FEs are generally descriptive (such as time, place, manner, etc.).&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/glossary#core&lt;/ref&gt;

Some examples include:

* The only core FE of the &lt;tt&gt;Being_born&lt;/tt&gt; frame is called &lt;tt&gt;Child&lt;/tt&gt;; non-core FEs being &lt;tt&gt;Time&lt;/tt&gt;, &lt;tt&gt;Place&lt;/tt&gt;, &lt;tt&gt;Relatives&lt;/tt&gt;, etc.&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&amp;name=Being_born&lt;/ref&gt;
* Core FEs of the &lt;tt&gt;Commerce_goods-transfer&lt;/tt&gt; include the &lt;tt&gt;Seller&lt;/tt&gt;, &lt;tt&gt;Buyer&lt;/tt&gt;, &lt;tt&gt;Goods&lt;/tt&gt;, among other things, while non-core FEs include a &lt;tt&gt;Place&lt;/tt&gt;, &lt;tt&gt;Purpose&lt;/tt&gt;, etc.&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&amp;name=Commerce_goods-transfer&lt;/ref&gt;

FrameNet includes shallow data on syntactic roles that frame elements play in the example sentences. For an example sentence like "She was born about AD 460", FrameNet would mark "She" as a [[noun phrase]] referring to the &lt;tt&gt;Child&lt;/tt&gt; FE, and "about AD 460" as a [[noun phrase]] corresponding to the &lt;tt&gt;Time&lt;/tt&gt; frame element. Details of how frame elements can be realized in a sentence are important because this reveals important information about the [[subcategorization frame]]s as well as possible [[diathesis alternation]]s (e.g. "John broke the window" vs. "The window broke")
of a verb.

===Lexical units===
Lexical units (LU) are lemmas, with their part of speech, that evoke a specific frame. In other words, when a LU is identified in a sentence, that specific LU can be associated with its specific frame(s). For each frame, there are many LUs associated to one frame and many frames that share multiple LUs, this is typically the case with LUs that have multiple word senses.&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/glossary&lt;/ref&gt; Alongside the frame, each lexical unit is associated with specific frame elements by means of the annotated example sentences.

Example:

Lexical units that evoke the &lt;tt&gt;Complaining&lt;/tt&gt; frame (or more specific perspectivized versions of it, to be precise), include the verbs "complain", "grouse", "lament", and others.&lt;ref&gt;https://framenet2.icsi.berkeley.edu/fnReports/data/frameIndex.xml?frame=Complaining&lt;/ref&gt;

===Example sentences===
Frames are associated with example sentences and frame elements are marked within the sentences. Thus the sentence
:''She was '''born''' about AD 460''
is associated with the frame &lt;tt&gt;Being_born&lt;/tt&gt;, while "She" is marked as the frame element &lt;tt&gt;Child&lt;/tt&gt; and "about AD 460" is marked as &lt;tt&gt;Time&lt;/tt&gt;.
(See the [http://framenet.icsi.berkeley.edu/fnReports/displayReport.php?anno=9791 FrameNet Annotation Report] for &lt;tt&gt;born.v&lt;/tt&gt;.)
From the start, the FrameNet project has been committed to looking at evidence from actual language use as found in text collections like the [[British National Corpus]]. 
Based on such example sentences, automatic [[semantic role labeling]] tools are able to determine frames and mark frame elements in new sentences.

===Valences===
FrameNet also exposes the statistics on the ''valences'' of the ''frames'', that is the number and the position of the ''frame elements'' within example sentences. The sentence
:''She was '''born''' about AD 460''
falls in the valence pattern
:'''NP Ext, INI --, NP Dep'''
which occurs two times in the [https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu9791.xml example sentences] in FrameNet,
namely in:
:She'' was '''born''' ''about AD 460'', daughter and granddaughter of Roman and Byzantine emperors, whose family had been prominent in Roman politics for over 700 years.''
:''He was soon posted to north Africa, and never met their only child, ''a daughter'' '''born''' ''8 June 1941''.''

===Frame Relations===

FrameNet additionally captures relationships between different frames using relations. These include the following.

* Inheritance: When one frame is a more specific version of another, more abstract parent frame. Anything that is true about the parent frame must also be true about the child frame, and a mapping is specified between the frame elements of the parent and the frame elements of the child.
* Perspectivized_in: A neutral frame (like &lt;tt&gt;Commerce_transfer-goods&lt;/tt&gt;) is connected to a frame with a specific perspective of the same scenario (e.g. the &lt;tt&gt;Commerce_sell&lt;/tt&gt; frame, which assumes the perspective of the seller or the &lt;tt&gt;Commerce_buy&lt;/tt&gt; frame, which assumes the perspective of the buyer)
* Subframe: Some frames like the &lt;tt&gt;Criminal_process&lt;/tt&gt; frame refer to complex scenarios that consist of several individual states or events that can be described by separate frames like &lt;tt&gt;Arrest&lt;/tt&gt;, &lt;tt&gt;Trial&lt;/tt&gt;, and so on.
* Precedes: The Precedes relation captures a temporal order that holds between subframes of a complex scenario.
* Causative_of and Inchoative_of: There is a fairly systematic relationship between stative descriptions (e.g. the &lt;tt&gt;Position_on_a_scale&lt;/tt&gt; frame, "She had a high salary") and causative descriptions (&lt;tt&gt;Cause_change_of_scalar_position&lt;/tt&gt;, "She raised his salary") or inchoative descriptions (&lt;tt&gt;Change_position_on_a_scale&lt;/tt&gt;, e.g. "Her salary increased").
* Using: A relationship that holds between a frame that in some way involves another frame. For instance, the &lt;tt&gt;Judgment_communication&lt;/tt&gt; frame uses both the &lt;tt&gt;Judgment&lt;/tt&gt; frame and the &lt;tt&gt;Statement&lt;/tt&gt; frame, but does not inherit from either of them because there is no clear correspondence of the frame elements.
* See_also: Connects frames that bear some resemblance but need to be distinguished carefully.

==Applications==

FrameNet has proven useful in a number of computational applications, because computers need additional knowledge in order to recognize that "John sold a car to Mary" and "Mary bought a car from John" describe essentially the same situation, despite using two very different verbs, different prepositions and a different word order. FrameNet has been used in applications like [[question answering]], paraphrasing, recognizing textual entailment, and information extraction, either directly or by means of [[Semantic Role Labeling]] tools. The first automatic system for [[Semantic Role Labeling]] (SRL, sometimes also referred to as "shallow semantic parsing") was developed by Daniel Gildea and Daniel Jurafsky based on FrameNet in 2002, and Semantic Role Labelling has since become one of the standard tasks in natural language processing.

Since frames are essentially semantic descriptions, they are similar across languages, and several projects have arisen over the years that have relied on the original FrameNet as the basis for additional non-English FrameNets, for Spanish, Japanese, German, and Polish, among others.

==See also==
*[[BabelNet]]: a multilingual semantic network integrating FrameNet
*[[PropBank]]
*[[Null instantiation]]
*[[Frame language]]
*[[UBY]]: a database of 10 resources including FrameNet

==References==
{{Reflist}}

===Further reading===
*[https://framenet2.icsi.berkeley.edu/docs/r1.5/book.pdf FrameNet II: Extended Theory and Practice] (e-book)

==External links==
*[http://framenet.icsi.berkeley.edu/ FrameNet home page]
*[http://sccfn.sxu.edu.cn/ Chinese FrameNet]
*[http://framenet.dk/ Danish FrameNet]
*[http://gframenet.gmc.utexas.edu/ German FrameNet]
*[http://jfn.st.hc.keio.ac.jp/ Japanese FrameNet]
*[http://www.ramki.uw.edu.pl/en/index.html Polish FrameNet]
*[http://www.ufjf.br/framenetbr/ Portuguese FrameNet (Brazil)]
*[http://gemini.uab.es/SFN/ Spanish FrameNet]
*[http://spraakbanken.gu.se/eng/swefn/ Swedish FrameNet]

[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Corpus linguistics]]
[[Category:History of the Internet]]
[[Category:Hypertext]]
[[Category:Online dictionaries]]
[[Category:Science and technology in the San Francisco Bay Area]]</text>
      <sha1>kyoapzvr042m6d81tfmlugxxcv8ggd3</sha1>
    </revision>
  </page>
  <page>
    <title>Chinese Library Classification</title>
    <ns>0</ns>
    <id>5625552</id>
    <revision>
      <id>753476984</id>
      <parentid>753457845</parentid>
      <timestamp>2016-12-07T11:43:00Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{FACT}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="19952" xml:space="preserve">The '''Chinese Library Classification''' ({{Zh|c = &#20013;&#22269;&#22270;&#20070;&#39302;&#20998;&#31867;&#27861;|s = |t = }}; CLC), also known as '''Classification for Chinese Libraries''' (CCL){{FACT|date=December 2016}}, is effectively the national [[library classification]] scheme in [[China]]. It is used in almost all primary and secondary schools, universities, academic institutions, as well as public [[libraries]]. It is also used by publishers to classify all books published in China.

The '''Book Classification of Chinese Libraries''' (BCCL) was first published in 1975, under the auspices of China's Administrative Bureau of Cultural Affairs. Its fourth edition (1999) was renamed CLC. In September 2010, the fifth edition was published by National Library of China Publishing House.
CLC has twenty-two top-level categories, and inherits a [[Marxist]] orientation from its earlier editions.&lt;ref&gt;[http://research.dils.tku.edu.tw/joemls/41/41-1/1-22.pdf Zhang, Wenxian (2003). ''Classification for Chinese Libraries (CCL): Histories, accomplishments, problems and its comparisons''. Journal of Educational Media &amp; Library Sciences, vol. 41, nr. 1, p. 1-22.] (PDF)&lt;/ref&gt; (For instance, category A is [[Marxism]], [[Leninism]], [[Maoism]] &amp; [[Deng Xiaoping Theory]].) It contains a total of 43600 categories, many of which are recent additions, meeting the needs of a rapidly changing nation.&lt;ref&gt;''The Standardization of Chinese Library Classification'', Xiaochun Liu, [[Cataloging &amp; Classification Quarterly]], Volume 16, Issue 2, ISSN 0163-9374, Pub Date: 8/13/1993
&lt;/ref&gt;

== The CLC System ==
The 22 top categories and selected sub-categories of CLC (5th Edition) are as follows:

=== A.  [[Marxism]], [[Leninism]], [[Maoism]] &amp; [[Deng Xiaoping Theory]] ===
* A1 The Works of [[Karl Marx]] and [[Friedrich Engels]]
* A2 The Works of [[Vladimir Lenin]]
* A3 The Works of [[Joseph Stalin]]
* A4 The Works of [[Mao Zedong]]
**   A49 The works of [[Deng Xiaoping]]
* A5 The Symposium/Collection of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping
* A7 The biobibliography and [[biography]] of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping
* A8 Study and Research of Marxism, Leninism, Maoism &amp; Deng Xiaoping Theory

=== B.  [[Philosophy]] and [[Religions]] ===
* B-4 Education and dissemination of philosophy
** B-49 Learners' book und popular literature of philosophy
* B0 theory of philosophy
** B0-0 Marxist philosophy
** B01 Basic problems of philosophy
*** B014 Object, purpose and method of philosophy
*** B015 [[Materialism]] and [[idealism]]
*** B016 [[Ontology]]
**** B016.8 [[Cosmology]]
**** B016.9 Time-space-theory
*** B017 [[Epistemology]]
**** B017.8 [[Determinism]] and [[Indeterminism]]
**** B017.9 Self Theory
*** B018 [[Axiology]]
**** B019.1 Materialism
***** B019.11 Naive materialism
***** B019.12 Metaphysical materialism
***** B019.13 [[Dialectical materialism]]
**** B019.2 [[Idealism]]
** B02 [[Dialectical materialism]]
*** B024 [[Materialist dialectics]]
*** B025 Categories of materialist dialectics
*** B026 [[Methodology]]
*** B027 Application of dialectical materialism
*** B028 [[Natural philosophy]]
*** B029 [[Dialectics of nature]]
** B03 [[Historical materialism]]
*** B031 Social material requirements of life
*** B032 Basic social conflict
**** B032.1 [[Productive forces]] und [[relations of production]]
**** B032.2 [[Base and superstructure]]
*** B033 [[Marxian Class Theory|Class Theory]]
*** B034 [[Marxism#Revolution|Theory of Revolution]]
*** B035 Theory of country
*** B036 [[Social being]] and [[social consciousness]]
*** B037 [[On Contradiction#Basics of Contradiction and its History|Contradictions among the People]]
*** B038 Role of the people in historical development
** B08 [[Philosophical schools]] and research
*** B081 [[Idealism]]
**** B081.1 [[Metaphysics]]
**** B081.2 Epistemology of idealism, apriorism
*** B082 [[Positivism]], [[Machism]]
*** B083 [[Voluntarism (metaphysics)|Voluntarism]] and [[philosophy of life]]
*** B084 [[Neo-Kantianism]] and [[Neohegelianism]]
*** B085 [[New realism (philosophy)|Neorealism]], [[logical positivism]] (new positivism, logical empiricism)
*** B086 [[Existentialism]] ([[survivalism]])
*** B087 [[Pragmatism]]
*** B088 [[Neo-Thomism]] (new scholasticism)
*** B089 Other philosophical schools
**** B089.1 [[Western Marxism]]
**** B089.2 [[Philosophical hermeneutics]]
**** B089.3 [[Philosophical anthropology]]
*  B1 Philosophy (Worldwide)
*  B2 Philosophy in China
**    B22 Pre-[[Qin Dynasty]] Philosophy (~before 220 BC)
***       B222 The Confucian School
****           B222.2 [[Confucius]] (K&#466;ng Qi&#363;, 551-479 BC)
*  B3 Philosophy in Asia
*  B4 [[African philosophy|Philosophy in Africa]]
*  B5 Philosophy in Europe
*  B6 Philosophy in Australasia
*  B7 [[Philosophy in America]]
*  B8 [[Cognitive science]]
*  B9 Religions
**    B91 [[Sociology of Religion]], [[Religion]] and [[Science]]
**    B92 [[Philosophy of religion|Philosophy]] and [[History of Religion]]
**    B93 [[Mythology]] and [[Animism|Primitive religion]]
**    B94 [[Buddhism]]
**    B95 [[Taoism]]
**    B96 [[Islam]]
**    B97 [[Christianity]]
***       B971 [[Bible]]
****           B971.1 [[Old Testament]]
****           B971.2 [[New Testament]]
***       B972 [[Doctrine]], [[Theology]]
***       B975 [[Evangelism]], [[Sermon]]
***       B976 [[Christian Denomination]]
****           B976.1 [[Roman Catholic Church]]
****           B976.2 Orthodox Christianity ([[Eastern Orthodox Church|Eastern Orthodoxy]], [[Oriental Orthodoxy]])
****           B976.3 [[Protestantism]] ([[Protestant Reformation]])
***       B977 [[Ecclesiastical polity]]
***       B978 Research on Christianity
***       B979 [[History of Christianity]]
****           B979.9 Biography
**    B98 Other Religions
**    B99 [[Augury]], [[Superstition]]

=== C.  [[Social Sciences]] ===
*  C0 Social Scientific Theory and Methodology
*  C1 Present and Future of Social Sciences
*  C2 Organisations, Groups, Conferences
*  C3 Method of Research in Social Sciences
*  C4 Education and Popularization of Social Sciences
*  C5 Serials, [[Anthology|Anthologies]], Periodicals in Social Sciences
*  C6 Reference Materials in Social Sciences
*  C7 (no longer used)
*  C8 Statistics in Social Sciences
*  C9 [[Sociology]]

=== D.  [[Politics]] and [[Law]] ===
*  D0 [[Political theory]]
*  D1 International Campaign of [[Communism]]
*  D2 [[Communist Party of China]]
*  D3 [[Communist Parties]] of other Countries
*  D4 Labor, Peasant, Youth, Female Organizations and Movements
*  D5 Politics (worldwide)
*  D6 Politics in China
*  D7 Politics in individual [[Countries]]
*  D8 [[Diplomacy]], [[International relations]]
*  D9 [[Law]]

=== E.  [[Military Science]] ===
*  E0 Military Theory
*  E1 Military (worldwide)
*  E2 [[Military in China]]
*  E3 Military in Asia
*  E4 Military in Africa
*  E5 Military in Europe
*  E6 Military in Australasia
*  E7 Military in America
*  E8 Strategies, Tactics, and Battles
*  E9 [[Military technology|Military Technology]]

=== F.  [[Economics]] ===
*  F0 [[Economics]]
*  F1 [[Economics]], [[Economic history]] and [[Economic geography]] of individual countries
*  F2 Economic Planning and Management
*  F3 [[Agricultural Economics]]
*  F4 Industrial Economics
*  F5 Economics of [[Transport]]
*  F6 Economics of Postal and Cable Services
*  F7 Economics of [[Commerce]]
*  F8 [[Finance]], [[Banking]]

=== G.  [[Culture]], [[Science]], [[Education]] and [[Sports]] ===
*  G0 Philosophy of Culture
*  G1 Culture
*  G2 Knowledge transmission
*  G3 [[Science]], [[Scientific Research]]
*  G4 [[Education]]
*  G5 Education in individual [[Countries]]
*  G6 Education (Primary, Secondary, Tertiary)
*  G7 Education (specialized)
*  G8 Sports

=== H.  [[Languages]] and [[Linguistics]] ===
*  H0 Linguistics
**    H01 [[Phonetics]]
***      H109 Method of Recitation, Oratory of Speech
**    H02 [[Grammatology]]
**    H03 [[Semantics]], [[Lexicology]] and Meaning of words
***      H033 [[Idiom]]
***      H034 [[Adage]]
**    H04 [[Syntax]]
**    H05 Study of writing, [[Rhetoric]]
***      H059 Study of translation
**    H06 [[Lexicography]]
***      H061 [[Dictionary]]
*  H1 [[Chinese language]]
**    H10&lt;!--to be filled--&gt;
***      H102 Regulation, Standardisation of Chinese language, Promotion of [[Putonghua]]
***      H109&lt;!--to be filled--&gt;
****        H109.2 Ancient Chinese language
****        H109.4 Modern Chinese language
**    H11 [[Phone (phonetics)|Phone]] ([[Historical Chinese phonology]])
**    H12 Grammatology
*  H2  [[Languages of China|Languages of China's ethnic minorities]]
*  H3 Commonly Used Foreign Languages
**    H31 [[English language]]
**    H32 [[French language]]
**    H33 [[German language]]
**    H34 [[Spanish language]]
**    H35 [[Russian language]]
**    H36 [[Japanese language]]
**    H37 [[Arabic language]]
*  H4 Family of [[Sino-Tibetan languages]] ([[China]], [[Tibet]] and [[Burma]])
*  H5 Family of [[Altaic languages]] ([[Turkic languages|Turkic]], [[Mongolian language|Mongolian]] and [[Tungusic languages|Tungusic]])
*  H6 [[Language family|Language families]] in other areas of the World
**    H61 [[Austroasiatic languages]] and [[Tai languages]] ([[Southeast Asia|Mainland Southeast Asia]]))
**    H62 [[Dravidian languages]] ([[South India]])
**    H63 [[Austronesian languages]] ([[Malayo-Polynesian]])
**    H64 [[Paleosiberian languages]] ([[Siberia]])
**    H65 [[Ibero-Caucasian languages]] ([[Caucasus Mountains]])
**    H66 [[Uralic languages]]
**    H67 [[Afroasiatic languages]] ([[Southwest Asia]], [[Arabian Peninsula]], [[North Africa]])
*  H7 [[Indo-European languages]]
*  H8 [[Language family|Language families]] on other Continents
**    H81 [[African languages]]
**    H83 [[Indigenous languages of the Americas|American languages]]
**    H84 [[Papuan languages]]
*  H9 International Auxiliary Languages ([[Interlingua]], [[Ido (language)|Ido]], [[Esperanto]], etc.)

=== I.  [[Literature]] ===
*  I0 [[Literary Theory]]
*  I1 Literature (worldwide)
*  I2 Literature in China
*  I3 Literature in Asia
*  I4 Literature in Africa
*  I5 Literature in Europe
*  I6 Literature in Australasia
*  I7 Literature in America

=== J.  Art ===
*  J0 Theory of [[Fine Art]]
*  J1 Fine Art of the World
*  J2 [[Painting]]
*  J3 [[Sculpture]]
*  J4 [[Photography]]
*  J5 [[Applied arts]]
*  J6 [[Music]]
*  J7 [[Dance]]
*  J8 [[Drama]]
*  J9 [[Cinematography]], [[Television]]

=== K.  [[History]] and [[Geography]] ===
* K0 Historical Theory
* K1 [[History of the World]]
* K2 [[History of China]]
* K3 [[History of Asia]]
* K4 [[History of Africa]]
* K5 [[History of Europe]]
* K6 [[History of Australasia]]
* K7 [[History of the Americas|History of America]]
* K8 Biography, [[Archaeology]]
* K9 Geography

=== N.  [[Natural Science]] ===
*  N0 Theory and Methodology
*  N1 Present state
*  N2 Organisations, Groups, Conferences
*  N3 Research Methodology
*  N4 Education and Popularization
*  N5 Serials, Anthologies, Periodicals
*  N6 Reference Materials
*  N8 Field Surveys
*  N9 Minor Sciences

=== O.  Mathematics, Physics and Chemistry ===
* O1 [[Mathematics]]
* O2 [[Applied Mathematics]]
* O3 [[Mechanics]]
* O4 [[Physics]]
* O6 [[Chemistry]]
* O7 [[Crystallography]]

=== P.  [[Astronomy]] and [[Geoscience]] ===
*  P1 [[Astronomy]]
*  P2 [[Geodesy]]
*  P3 [[Geophysics]]
*  P4 [[Meteorology]]
*  P5 [[Geology]]
*  P6 [[Mineralogy]]
*  P7 [[Oceanography]]
*  P9 [[Physiography]]

=== Q.  [[Life Sciences]] ===
*  Q1 [[Biology|General Biology]]
*  Q2 [[Cell biology|Cytology]]
*  Q3 [[Genetics]]
*  Q4 [[Physiology]]
*  Q5 [[Biochemistry]]
*  Q6 [[Biophysics]]
*  Q7 [[Molecular Biology]]
*  Q8 [[Bioengineering]]
*  Q9 [[Zoology]] and [[Botany]]

=== R.  [[Medicine]] and [[Health Sciences]] ===
*  R1 [[Preventive Medicine]], [[Public health]]
*  R2 [[Traditional Chinese Medicine]]
*  R3 [[Human anatomy]], [[Physiology]], [[Pathology]], [[Microbiology]], [[Parasitology]]
*  R4 [[Clinical Medicine]]
*  R5 [[Internal medicine]]
*  R6 [[Surgery]]
*  R7 [[:Category:Medical specialties|Medical Specialties]]
** R71 [[Obstetrics]], [[Gynecology]]
** R72 [[Pediatrics]]
** R73 [[Oncology]]
** R74 [[Neurology]], [[Psychiatry]]
** R75 [[Dermatology]], [[Venereology]]
** R76 [[Otolaryngology]]
** R77 [[Ophthalmology]]
** R78 [[Dentistry]]
** R79 Non-Chinese [[Traditional medicine|Traditional Medicine]]
*  R8 [[Radiology]], [[Sport medicine]], [[Diving medicine]], [[Aerospace medicine]]
*  R9 [[Pharmacology]], [[Pharmacy]]

=== S.  [[Agricultural Science]] ===
*  S1 Fundamental Agricultural Science
*  S2 [[Agricultural Engineering]]
*  S3 [[Agronomy]]
*  S4 [[Phytopathology]]
*  S5 [[Crop|Individual Crops]]
*  S6 [[Horticulture]]
*  S7 [[Forestry]]
*  S8 [[Animal Husbandry]], [[Veterinary medicine]], [[Hunting]], [[Sericulture]], [[Apiculture]]
*  S9 [[Aquaculture]], [[Fishery]]

=== T.  Industrial Technology ===
*  TB General Industrial Technology
*  TD [[Mining Engineering]]
*  TE [[Petroleum]], [[Natural Gas]]
*  TF [[Extractive metallurgy]], [[Smelting]]
*  TG [[Metallurgy]], [[Metalworking]]
*  TH [[Machinery]], [[Instrumentation]]
*  TJ [[Military technology and equipment|Military Technology]]
*  TK [[Power Plant]]
*  TL [[Nuclear technology]]
*  TM [[Electrical Engineering]]
*  TN [[Electronic Engineering]], [[Telecommunication|Telecommunication Engineering]]
*  TP [[Automation]], [[Computer Engineering]]
*  TQ [[Chemical Engineering]]
*  TS [[Light industry|Light Industry]], [[Handicraft]]
*  TU [[Construction Engineering]]
*  TV [[Water Resources]], [[Hydraulics|Hydraulic Engineering]]

=== U.  [[Transportation]] ===
*  U1 General [[Transport]]
*  U2 [[Railway]] Transport
*  U4 [[Highway]] Transport
*  U6 [[Ship transport|Marine Transport]]

=== V.  [[Aviation]] and Aerospace ===
*  V1 Research and Exploration of Aviation and Aerospace Technology
*  V2 Aviation
*  V4 [[Aerospace]] ([[Spaceflight]])

=== X.  [[Environmental Science]] ===
*  X1 Fundamental Environmental Science
*  X2 Environmental Research
*  X3 [[Environmental Protection]] and Management
*  X4 Disaster Protection
*  X5 [[Pollution|Pollution Control]]
*  X7 [[Waste Management]] and [[Recycling]]
*  X8 Environmental Quality Monitoring
*  X9 [[Occupational safety and health]]

=== Z.  General Works ===
*  Z1 Collectanea/Generalia ([[Book series]])
** Z12 Collectanea of China
*** Z121 General Collectanea
**** Z121.2 Song Dynasty
**** Z121.3 Yuan Dynasty
**** Z121.4 Ming Dynasty
**** Z121.5 Qing Dynasty
**** Z121.6 Republic period
**** Z121.7 Modern
*** Z122 Collectanea of a particular locality
*** Z123 Collectanea by members of a particular family
*** Z124 Collectanea by individual writers
*** Z125 Collectanea of lost books
*** Z126 Collectanea of [[Chinese Classics]]
**** Z126.1 Collection of [[Confucian Classics]]
**** Z126.2 Collection of treatises
***** Z126.21 General Collection
***** Z126.22 Remake of lost books
***** Z126.23 Collection of a particular theme
***** Z126.24 [[Timeline of Chinese history|Chronological tables]], tablets, illustrated works
***** Z126.25 Works on [[phonetics]], [[semantics]] and [[Verisimilitude|authenticity]]
***** Z126.27 Research, critics and proves
** Z13 Collectanea and Book series of Asia
** Z14 Book series of Africa
** Z15 Book series of Europe
** Z16 Book series of Oceania
** Z17 Book series of America
*  Z2 [[Encyclopedia]] and Chinese Encyclopedia (Leishu)
** Z22 Chinese Encyclopedia
*** Z221 Tang Dynasty
*** Z222 Song Dynasty
*** Z223 Yuan Dynasty
*** Z224 Ming Dynasty
*** Z225 Qing Dynasty
*** Z226 Republic
*** Z227 Modern
*** Z228 General popular Literature 
**** Z228.1 Children's book
**** Z228.2 Popular Youth Book
**** Z228.3 Elders' book
**** Z228.4 Women's reader
**** Z228.5 Men's reader
** Z23 Encyclopedia of Asia
** Z24 Encyclopedia of Africa
** Z25 Encyclopedia of Europe
** Z26 Encyclopedia of Oceania
** Z27 Encyclopedia of America
** Z28 Encyclopedia of a particular field
*  Z3 [[Dictionary]]
*  Z4 [[Symposium]], [[Anthologies]], Selected Works, [[Essay]]
*  Z5 [[Almanac]]
*  Z6 [[Serial (literature)|Serial]], [[Periodicals]]
*  Z8 Catalogue, Abstract, Index

== Other classifications ==
The other library classifications in China are:

* [[Library Classification of the People&#8217;s University of China]] (LCPUC)
* [[Library Classification of the Chinese Academy of Sciences]] (LCCAS)
* [[Library Classification for Medium and Small Libraries]] (MSL)
* [[Library Classification of Wuhan University]] (LCWU)

The other library classifications for Chinese materials outside mainland China are:
* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books devised by Profs. Haloun and P. van der Loon for Cambridge University, UK.
* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])
* [[Harvard-Yenching Classification]] System
* [[New Classification Scheme for Chinese Libraries]] (commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]].)

== See also ==
* [[Libraries in the People's Republic of China]]

==References==
{{reflist}}

== External links ==
* [http://clc.nlc.gov.cn/ Official website ]
* [http://www.ifla.org/IV/ifla62/62-qiyz.htm Contemporary Classification Systems and Thesauri in China, Zhang Qiyu, Liu Xiangsheng, Wang Dongbo, 62nd IFLA General Conference - Conference Proceedings - August 25-31, 1996]
* [http://www.nlc.gov.cn/old/old/newpages/english/org/clce.htm Chinese Library Classification Editorial Board]
* [http://www.zju.edu.cn/jzus/download/clc.pdf Abridged third (obsolete) edition of CLC ]{{Zh icon}}
* [http://www.33tt.com/tools/ztf/ CLC Online ]{{Zh icon}}
* [http://journals.sfu.ca/dcpapers/2004/Paper_12.pdf Research on Interoperability of Metadata in Classification Schemes-construction of automatic mapping system between CLC and DDC, Jianbo Dai, Hanqing Hou, Ling Cao, Dept. of Libr. &amp; Inform. Sci., Nanjing Agri. Univ., Nanjing, China 210095]
* [ftp://ext-ftp.fao.org/GI/Agris/aims/publications/workshops/AOS_5/ppt/3-3.pdf Construction of Knowledge Base for Automatic Indexing and Classification based on CLC, Hanqing Hou, Chunxiang Xue, Nanjing Agri. Univ., Nanjing, China 210095]
* [http://www.fao.org/Agris/AOS/ConferencesW/FifthAOS_China04/AOS_Proceedings/docs/4-1.pdf#search='chinese%20library%20classification' An Intelligent Retrieval System for Chinese Agricultural Literature indexed by Chinese Classification System, Ping Qian, Xiaolu Su, Chinese Academy of Agricultural Sciences, China]
* [http://www.freewebs.com/yahnkim/East%20Asian%20Library%20Classification%20Systems%5B1%5D.doc East Asian Library Classification Systems], [http://archive.is/20121209181441/http://webcache.googleusercontent.com/search?q=cache:z3hxqowOTFoJ:www.freewebs.com/yahnkim/East%2520Asian%2520Library%2520Classification%2520Systems%255B1%255D.doc+chinese+library+classification&amp;hl=en&amp;gl=hk&amp;ct=clnk&amp;cd=410 archived]
* [http://www.nii.ac.jp/publications/CJK-WS3/cjk3-04a.pdf The Development of Authority Database in National Library of China (NLC), March 2002, Beixin Sun of NLC] NLC's classification subject thesaurus database based on CLC.
* [http://www.ifla.org/IV/ifla72/papers/109-Gu-en.pdf National Bibliographies: the Chinese Experience, 72nd IFLA Conference at Seoul in Korea, August 2006, Ben Gu of NLC] An overview of the current situation of the National Bibliography and classification systems in China.
* [http://pubs.nrc-cnrc.gc.ca/jchla/jchla26/c05-018.pdf A month at the Shanghai Library, November 2004, Helen Michael, University of Toronto] A librarian from Canada shared her experience of working in a library of China.

{{Library classification systems}}

[[Category:1975 introductions]]
[[Category:1975 establishments in China]]
[[Category:Library cataloging and classification]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Chinese culture]]</text>
      <sha1>6vwper4rwv8j81mblpmdx7kzluyo3x3</sha1>
    </revision>
  </page>
  <page>
    <title>Reification (knowledge representation)</title>
    <ns>0</ns>
    <id>5662676</id>
    <revision>
      <id>746411798</id>
      <parentid>746411731</parentid>
      <timestamp>2016-10-27T07:10:10Z</timestamp>
      <contributor>
        <ip>88.79.92.56</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3104" xml:space="preserve">{{multiple issues|{{refimprove|date=December 2009}}
{{Expert-subject|Information Science|ex2=Cyc}}}}


'''Reification''' in [[knowledge representation]] is the process of turning a predicate into an object.&lt;ref&gt;{{cite web |last=Hunt |first=Matthew |title=Notes on Semantic Nets and Frames |url=http://www.eecs.qmul.ac.uk/~mmh/AINotes/AINotes4.pdf |date=1996 |access-date=15 June 2016}}&lt;/ref&gt; Reification involves the representation of factual assertions that are referred to by ''other'' assertions, which might then be manipulated in some way; e.g., comparing [[logical assertion]]s from different [[witness]]es in order to determine their [[credibility]].

The message "John is six feet tall" is an assertion involving truth that commits the speaker to its factuality, whereas the reified statement "Mary reports that John is six feet tall" defers such commitment to Mary. In this way, the statements can be incompatible without creating contradictions in [[reasoning]]. For example, the statements "John is six feet tall" and "John is five feet tall" are mutually exclusive (and thus incompatible), but the statements "Mary reports that John is six feet tall" and "Paul reports that John is five feet tall" are not incompatible, as they are both governed by a conclusive rationale that either Mary or Paul is (or both are), in fact, incorrect.

In Linguistics, reporting, telling, and saying are recognised as ''verbal processes that project a wording (or locution)''. If a person says that "Paul told x" and "Mary told y", this person stated only that the telling took place. In this case, the person who made these two statements did not represent a person inconsistently. In addition, if two people are talking to each other, let's say Paul and Mary, and Paul tells Mary "John is five feet tall" and Mary rejects Paul's statement by saying "No, he is actually six feet tall", the socially constructed model of John does not become inconsistent. The reason for that is that statements are to be understood as an attempt to convince the addressee of something (Austin's How to do things with words), alternatively as a request to add some attribute to the model of Paul. The response to a statement can be an acknowledgement, in which case the model is changed, or it can be a statement rejection, in which case the model does not get changed. Finally, the example above for which John is said to be "five feet tall" or "six feet tall" is only incompatible because John can only be a single number of feet tall. If the attribute were a possession as in "he has a dog" or "he also has a cat", a model inconsistency would not happen. In other words, the issue of model inconsistency has to do with our model of the domain element (John) and not with the ascription of different range elements (measurements such as "five feet tall" or "six feet tall") nor with statements.

==See also==
*[[Reification (computer science)]]
*[[Reification (fallacy)]]
*[[Reification (linguistics)]]

==References==
{{Reflist}}

{{DEFAULTSORT:Reification (Knowledge Representation)}}
[[Category:Knowledge representation]]</text>
      <sha1>20dwxx0gdkshltv3s60yprzuv0g2lo2</sha1>
    </revision>
  </page>
  <page>
    <title>Cognitive map</title>
    <ns>0</ns>
    <id>1385766</id>
    <revision>
      <id>759296219</id>
      <parentid>748493035</parentid>
      <timestamp>2017-01-10T09:53:01Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>/* Parallel map theory */ lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16158" xml:space="preserve">A '''cognitive map''' (sometimes called a [[mental map]] or [[mental model]]) is a type of [[mental representation]] which serves an individual to acquire, code, store, recall, and decode information about the relative locations and attributes of phenomena in their everyday or metaphorical spatial environment. The concept was introduced by [[Edward C. Tolman|Edward Tolman]] in 1948.&lt;ref name="pmid18870876"&gt;{{cite journal |last=Tolman |first=Edward C. |authorlink=Edward C. Tolman |title=Cognitive maps in rats and men |journal=[[Psychological Review]] |volume=55 |issue=4 |pages=189&#8211;208 |date=July 1948 |pmid=18870876| doi=10.1037/h0061626}}&lt;/ref&gt; The term was later generalized by some researchers, especially in the field of [[operations research]], to refer to a kind of [[semantic network]] representing an individual's personal knowledge or [[Schema (psychology)|schemas]].&lt;ref&gt;{{cite journal |last=Eden |first=Colin |date=July 1988 |title=Cognitive mapping |journal=[[European Journal of Operational Research]] |volume=36 |issue=1 |pages=1&#8211;13 |doi=10.1016/0377-2217(88)90002-1 |quote=In the practical setting of work in with a team of busy managers cognitive mapping is a tool for building interest from all team members in the problem solving activity. [...] The cycle of ''problem construction'', ''making sense'', ''defining the problem'', and declaring a ''portfolio of solutions'', which I have discussed elsewhere (Eden, 1982) is the framework that guides the process of working with teams. Thus building and working with the cognitive maps of each individual is primarily aimed at helping each team member reflectively 'construct' and 'make sense' of the situation they believe the team is facing. (pp. 7&#8211;8)}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |last1=Fiol |first1=C. Marlene |last2=Huff |first2=Anne Sigismund |date=May 1992 |title=Maps for managers: Where are we? Where do we go from here? |journal=[[Journal of Management Studies]] |volume=29 |issue=3 |pages=267&#8211;285 |doi=10.1111/j.1467-6486.1992.tb00665.x |quote=For geographers, a map is a means of depicting the world so that people understand where they are and where they can go. For cognitive researchers, who often use the idea of a 'map' as an analogy, the basic idea is the same. Cognitive maps are graphic representations that locate people in relation to their information environments. Maps provide a frame of reference for what is known and believed. They highlight some information and fail to include other information, either because it is deemed less important, or because it is not known. (p. 267)}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last1=Ambrosini |first1=V&#233;ronique |last2=Bowman |first2=Cliff |date=2002 |chapter=Mapping successful organizational routines |editor1-last=Huff |editor1-first=Anne Sigismund |editor2-last=Jenkins |editor2-first=Mark |title=Mapping strategic knowledge |location=London; Thousand Oaks, CA |publisher=[[Sage Publications]] |pages=19&#8211;45 |isbn=0761969497 |oclc=47900801 |quote=We shall not explain here what cognitive maps are about as this has been done extensively elsewhere (Huff, 1990). Let us just say that cognitive maps are the representation of an individual's personal knowledge, of an individual's own experience (Weick and Bougon, 1986), and they are ways of representing individuals' views of reality (Eden et al., 1981). There are various types of cognitive maps (Huff, 1990). (pp. [//books.google.com/books?id=LE95fcRz_IcC&amp;pg=PA21 21&#8211;22])}}&lt;/ref&gt;

== Overview ==

Cognitive maps have been studied in various fields, such as psychology, education, archaeology, planning, geography, cartography, architecture, landscape architecture, urban planning, management and history.&lt;ref&gt;{{cite book |title=Conspiracy nation: the politics of paranoia in Postwar America |last=Knight |first=Peter |year=2002 |publisher=[[New York University Press]] |location=New York and London |isbn=0814747353}}&lt;/ref&gt;{{Page needed|date=August 2016}} As a consequence, these mental models are often referred to, variously, as cognitive  maps, [[mental map]]s, [[Behavioral script|script]]s, [[Schema (psychology)|schemata]], and [[Frame of reference|frames of reference]].

Cognitive maps serve the construction and accumulation of spatial knowledge, allowing the "[[mind's eye]]" to visualize images in order to reduce [[cognitive load]], enhance [[recollection|recall]] and [[learning]] of information. This type of spatial thinking can also be used as a metaphor for non-spatial tasks, where people performing non-spatial tasks involving [[memory]] and imaging use spatial knowledge to aid in processing the task.&lt;ref&gt;{{cite journal |last=Kitchin |first=Robert M. |title=Cognitive maps: what are they and why study them? |journal=[[Journal of Environmental Psychology]] |year=1994 |volume=14 |issue=1 |pages=1&#8211;19 |doi=10.1016/S0272-4944(05)80194-X}}&lt;/ref&gt;

The [[neural correlate]]s of a cognitive map have been speculated to be the [[place cell]] system in the [[hippocampus]]&lt;ref name="O'Keefe"&gt;{{cite book |last1=O'Keefe |first1=John |authorlink1=John O'Keefe (neuroscientist) |last2=Nadel |first2=Lynn |authorlink2=Lynn Nadel |date=1978 |title=The hippocampus as a cognitive map |location=Oxford; New York |publisher=[[Clarendon Press]]; [[Oxford University Press]] |isbn=0198572069 |oclc=4430731 |url=http://www.cognitivemap.net/}}&lt;/ref&gt; and the recently discovered [[grid cells]] in the [[entorhinal cortex]].&lt;ref name="pmid16675704"&gt;{{cite journal |last1=Sargolini |first1=Francesca |last2=Fyhn |first2=Marianne |last3=Hafting |first3=Torkel |last4=McNaughton |first4=Bruce L. |last5=Witter |first5=Menno P. |last6=Moser |first6=May-Britt |last7=Moser |first7=Edvard I. |title=Conjunctive representation of position, direction, and velocity in entorhinal cortex |journal=[[Science (journal)|Science]] |volume=312 |issue=5774 |pages=758&#8211;762 |date=May 2006 |pmid=16675704 |doi=10.1126/science.1125572 |bibcode=2006Sci...312..758S }}&lt;/ref&gt;

== Neurological basis ==

Cognitive mapping is believed to largely be a function of the hippocampus. The hippocampus is connected to the rest of the brain in such a way that it is ideal for integrating both spatial and nonspatial information. Connections from the [[postrhinal cortex]] and the medial entorhinal cortex provide spatial information to the hippocampus. Connections from the [[perirhinal cortex]] and lateral entorhinal cortex provide nonspatial information. The integration of this information in the hippocampus makes the hippocampus a practical location for cognitive mapping, which necessarily involves combining information about an object's location and its other features.&lt;ref name="Manns"&gt;{{cite journal |last1=Manns |first1=Joseph R. |last2=Eichenbaum |first2=Howard |authorlink2=Howard Eichenbaum |date=October 2009 |title=A cognitive map for object memory in the hippocampus |journal=[[Learning &amp; Memory]] |volume=16 |issue=10 |pages=616&#8211;624 |doi=10.1101/lm.1484509 |pmc=2769165 |pmid=19794187 }}&lt;/ref&gt;

O'Keefe and Nadel were the first to outline a relationship between the hippocampus and cognitive mapping.&lt;ref name="O'Keefe" /&gt; Many additional studies have shown additional evidence that supports this conclusion.&lt;ref name=Moser&gt;{{cite journal |last1=Moser |first1=Edvard I. |authorlink1=Edvard Moser |last2=Kropff |first2=Emilio |last3=Moser |first3=May-Britt |authorlink3=May-Britt Moser |date=2008 |title=Place cells, grid cells, and the brain's spatial representation system |journal=[[Annual Review of Neuroscience]] |volume=31 |pages=69&#8211;89 |doi=10.1146/annurev.neuro.31.061307.090723 |pmid=18284371 |url=http://www.annualreviews.org/eprint/7t2VcSrTYa8V8yACMweG/full/10.1146/annurev.neuro.31.061307.090723}}&lt;/ref&gt; Specifically, [[pyramidal cells]] ([[place cells]], [[boundary cell]]s, and [[grid cells]]) have been implicated as the neuronal basis for cognitive maps within the hippocampal system.

Numerous studies by O'Keefe have implicated the involvement of place cells. Individual place cells within the hippocampus correspond to separate locations in the environment with the sum of all cells contributing to a single map of an entire environment. The strength of the connections between the cells represents the distances between them in the actual environment. The same cells can be used for constructing several environments, though individual cells' relationships to each other may differ on a map by map basis.&lt;ref name="O'Keefe" /&gt; The possible involvement of place cells in cognitive mapping has been seen in a number of mammalian species, including rats and macaque monkeys.&lt;ref name=Moser /&gt; Additionally, in a study of rats by Manns and Eichenbaum, pyramidal cells from within the hippocampus were also involved in representing object location and object identity, indicating their involvement in the creation of cognitive maps.&lt;ref name=Manns /&gt; However, there has been some dispute as to whether such studies of mammalian species indicate the presence of a cognitive map and not another, simpler method of determining one's environment.&lt;ref name=Bennet /&gt;

While not located in the hippocampus, grid cells from within the medial entorhinal cortex have also been implicated in the process of [[path integration]], actually playing the role of the path integrator while place cells display the output of the information gained through path integration.&lt;ref name=McNaughton&gt;{{cite journal |last1=McNaughton |first1=Bruce L. |last2=Battaglia |first2=Francesco P. |last3=Jensen |first3=Ole |last4=Moser |first4=Edvard I. |authorlink4=Edvard Moser |last5=Moser |first5=May-Britt |authorlink5=May-Britt Moser |date=August 2006 |title=Path integration and the neural basis of the 'cognitive map' |journal=[[Nature Reviews Neuroscience]] |volume=7 |issue=8 |pages=663&#8211;678 |doi=10.1038/nrn1932 |pmid=16858394 }}&lt;/ref&gt; The results of path integration are then later used by the hippocampus to generate the cognitive map.&lt;ref name=Jacobs /&gt; The cognitive map likely exists on a circuit involving much more than just the hippocampus, even if it is primarily based there. Other than the medial entorhinal cortex, the presubiculum and parietal cortex have also been implicated in the generation of cognitive maps.&lt;ref name=Moser /&gt;

=== Parallel map theory ===

There has been some evidence for the idea that the cognitive map is represented in the [[hippocampus]] by two separate maps. The first is the bearing map, which represents the environment through self-movement cues and [[gradient]] cues. The use of these [[vector (mathematics)|vector]]-based cues creates a rough, 2D map of the environment. The second map would be the sketch map that works off of positional cues. The second map integrates specific objects, or [[landmark]]s, and their relative locations to create a 2D map of the environment. The cognitive map is thus obtained by the integration of these two separate maps.&lt;ref name=Jacobs /&gt;

==Generation==

The cognitive map is generated from a number of sources, both from the [[visual system]] and elsewhere. Much of the cognitive map is created through self-generated movement [[sensory cue|cues]]. Inputs from senses like vision, [[proprioception]], olfaction, and hearing are all used to deduce a person's location within their environment as they move through it. This allows for path integration, the creation of a vector that represents one's position and direction within one's environment, specifically in comparison to an earlier reference point. This resulting vector can be passed along to the hippocampal place cells where it is interpreted to provide more information about the environment and one's location within the context of the cognitive map.&lt;ref name=Jacobs&gt;{{cite journal |last1=Jacobs |first1=Lucia F. |last2=Schenk |first2=Fran&#231;oise |date=April 2003 |title=Unpacking the cognitive map: the parallel map theory of hippocampal function |journal=[[Psychological Review]] |volume=110 |issue=2 |pages=285&#8211;315 |doi=10.1037/0033-295X.110.2.285 |pmid=12747525}}&lt;/ref&gt;

Directional cues and positional landmarks are also used to create the cognitive map. Within directional cues, both explicit cues, like markings on a compass, as well as gradients, like shading or magnetic fields, are used as inputs to create the cognitive map. Directional cues can be used both statically, when a person does not move within his environment while interpreting it, and dynamically, when movement through a gradient is used to provide information about the nature of the surrounding environment. Positional landmarks provide information about the environment by comparing the relative position of specific objects, whereas directional cues give information about the shape of the environment itself. These landmarks are processed by the hippocampus together to provide a graph of the environment through relative locations.&lt;ref name=Jacobs /&gt;

== History ==

The idea of a cognitive map was first developed by [[Edward C. Tolman]]. Tolman, one of the early cognitive psychologists, introduced this idea when doing an experiment involving rats and mazes. In Tolman's experiment, a rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial cognitive map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.&lt;ref&gt;{{cite book |last=Goldstein |first=E. Bruce |date=2011 |title=Cognitive psychology: connecting mind, research, and everyday experience |edition=3rd |location=Belmont, CA |publisher=[[Wadsworth Cengage Learning]] |isbn=9780840033550 |oclc=658234658 |pages=11&#8211;12}}&lt;/ref&gt;

== Criticism ==

In a review, Andrew T.D. Bennett argued that there are no clear evidence for cognitive maps in non-human animals (i.e. cognitive map according to Tolman's definition).&lt;ref name=Bennet&gt;{{cite journal |last=Bennett |first=Andrew T. D. |date=January 1996 |title=Do animals have cognitive maps? |journal=[[The Journal of Experimental Biology]] |volume=199 |issue=Pt 1 |pages=219&#8211;224 |pmid=8576693}}&lt;/ref&gt; This argument is based on analyses of studies where it has been found that simpler explanations can account for experimental results. Bennett highlights three simpler alternatives that cannot be ruled out in tests of cognitive maps in non-human animals "These alternatives are (1) that the apparently novel short-cut is not truly novel; (2) that path integration is being used; and (3) that familiar landmarks are being recognised from a new angle, followed by movement towards them."

== Related term ==
{{Refimprove section|date=August 2016}}

A cognitive map is a spatial representation of the outside world that is kept within the mind, until an actual manifestation (usually, a drawing) of this perceived knowledge is generated, a mental map. Cognitive mapping is the implicit, mental mapping the explicit part of the same process. In most cases, a cognitive map exists independently of a mental map, an article covering just cognitive maps would remain limited to theoretical considerations.

In some uses, mental map refers to a practice done by urban theorists by having city dwellers draw a map, from memory, of their city or the place they live. This allows the theorist to get a sense of which parts of the city or dwelling are more substantial or imaginable. This, in turn, lends itself to a decisive idea of how well urban planning has been conducted.

==See also==
* [[Cognitive geography]]
* [[Fuzzy cognitive map]]
* [[Motion perception]]
* [[Repertory grid]]

==References==
{{Reflist|30em}}

== External links ==
* {{commonscat-inline|Cognitive maps}}

{{DEFAULTSORT:Cognitive Map}}
[[Category:Cognitive science]]
[[Category:Mnemonics]]
[[Category:Knowledge representation]]</text>
      <sha1>ew9tbaai6o5ar6dgd0scyvq7e4nehl3</sha1>
    </revision>
  </page>
  <page>
    <title>Vivid knowledge</title>
    <ns>0</ns>
    <id>25154733</id>
    <revision>
      <id>646953751</id>
      <parentid>496915099</parentid>
      <timestamp>2015-02-13T13:48:34Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo fixing, replaced: Kownledge &#8594; Knowledge (3) per the book cover image at Amazon using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2010" xml:space="preserve">'''Vivid knowledge''' refers to a specific kind of [[knowledge representation]].

The idea of a '''vivid knowledge base''' is to get an interpretation mostly straightforward out of it &amp;ndash; it implies the interpretation. Thus, any query to such a [[knowledge base]] can be reduced to a [[database]]-like query.

== Propositional knowledge base ==

A [[Propositional logic|propositional]] [[knowledge base]] KB is '''vivid''' ''iff'' KB is a [[Completeness (knowledge bases)|complete]] and [[consistency (knowledge bases)|consistent]] set of [[Literal (mathematical logic)|literals]] (over some vocabulary).&lt;ref&gt;Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337&lt;/ref&gt;

Such a knowledge base has the property that it as exactly one interpretation, i.e. the interpretation is unique. A check for entailment of a sentence can simply be broken down into its literals and those can be answered by a simple database-like check of KB.

== First-order knowledge base ==

A [[First-order logic|first-order]] knowledge base KB is '''vivid''' ''iff'' for some finite set of positive function-free ground literals KB&lt;sup&gt;+&lt;/sup&gt;,

: KB = KB&lt;sup&gt;+&lt;/sup&gt; &#8746; Negations &#8746; DomainClosure &#8746; UniqueNames,

whereby

: Negations &#8788; { &#172;p | p is atomic and KB &#8877; p },
: DomainClosure &#8788; { (c&lt;sub&gt;i&lt;/sub&gt; &#8800; c&lt;sub&gt;j&lt;/sub&gt;) | c&lt;sub&gt;i&lt;/sub&gt;, c&lt;sub&gt;j&lt;/sub&gt; are distinct constants },
: UniqueNames &#8788; { &#8704;x: (x = c&lt;sub&gt;1&lt;/sub&gt;) &#8744; (x = c&lt;sub&gt;2&lt;/sub&gt;) &#8744; ..., where the c&lt;sub&gt;i&lt;/sub&gt; are all the constants in KB&lt;sup&gt;+&lt;/sup&gt; }.

&lt;ref&gt;Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337&lt;/ref&gt;

All interpretations of a vivid first-order knowledge base are isomorphic.&lt;ref&gt;Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 339&lt;/ref&gt;

== See also ==
* [[Closed world assumption]]

{{computable knowledge}}

== References ==

&lt;references/&gt;

[[Category:Knowledge representation]]


{{logic-stub}}
{{database-stub}}</text>
      <sha1>1n6bg1fqv1w2jy99eh3y494ffnh0tq1</sha1>
    </revision>
  </page>
  <page>
    <title>IDIS (software)</title>
    <ns>0</ns>
    <id>2889751</id>
    <revision>
      <id>683750522</id>
      <parentid>499866604</parentid>
      <timestamp>2015-10-02T07:48:33Z</timestamp>
      <contributor>
        <username>Ymblanter</username>
        <id>14596827</id>
      </contributor>
      <comment>/* See also */  rm redlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="342" xml:space="preserve">{{Unreferenced|date=December 2009}}

'''IDIS''' is a [[software]] tool for direct [[data exchange]] between [[CDS/ISIS]] and [[IDAMS]]. It is developed, maintained and disseminated by [[UNESCO]].

==See also==
*[[CDS/ISIS]] - database software

{{DEFAULTSORT:Idis (Software)}}
[[Category:Knowledge representation]]


{{network-software-stub}}</text>
      <sha1>pofplfqvfla3agmv5uqailphkleyg60</sha1>
    </revision>
  </page>
  <page>
    <title>AGROVOC</title>
    <ns>0</ns>
    <id>5465574</id>
    <revision>
      <id>697217849</id>
      <parentid>656596890</parentid>
      <timestamp>2015-12-29T01:40:04Z</timestamp>
      <contributor>
        <username>CaliViking</username>
        <id>11006619</id>
      </contributor>
      <comment>/* External links */  - Fixed broken link to VoCBench</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9442" xml:space="preserve">'''AGROVOC''' (a portmanteau of agriculture and vocabulary) is a multilingual controlled vocabulary covering all areas of interest to the [[Food and Agriculture Organization of the United Nations]] (FAO), including food, nutrition, agriculture, fisheries, forestry and the environment. The vocabulary consists of over 32,000 concepts with up to 40,000 terms in 23 languages: Arabic, Chinese, Czech, English, French, German, Hindi, Hungarian, Italian, Japanese, Korean, Lao, Malay, Persian, Polish, Portuguese, Russian, Slovak, Spanish, Telugu, Thai, Turkish and Ukrainian. It is a collaborative effort, edited by a community of experts and coordinated by FAO.

AGROVOC is made available by FAO as an [[Resource Description Framework|RDF]]/[[SKOS]]-XL concept scheme and published as a [[linked data]] set aligned to 16 other vocabularies.

==History==
AGROVOC was first published at the beginning of the 1980s by FAO in English, Spanish and French to serve as a controlled vocabulary to index publications in agricultural science and technology, especially for [[AGRIS]].

In the 1990s, AGROVOC abandoned paper printing and went digital with data storage handled by a relational database. In 2004, preliminary experiments with expressing AGROVOC into the [[Web Ontology Language]](OWL) took place. At the same time a web based editing tool was developed, then called WorkBench, nowadays VocBench. In 2009 AGROVOC became an SKOS resource.

Today, AGROVOC is available in 23 languages as an SKOS-XL concept scheme and published as a Linked Open Data (LOD) set aligned to 16 other data sets related to agriculture.

==Users==
AGROVOC is used by researchers, librarians and information managers for indexing, retrieving and organizing data in agricultural information systems and web pages.&lt;ref&gt;[http://aims.fao.org/standards/agrovoc/uses-agrovoc AGROVOC Uses]&lt;/ref&gt; Within the context of the [[Semantic Web]] also new users are emerging, like software developers and ontology builders.

==Access==
AGROVOC is accessible in various ways:

* Online: Search &lt;ref&gt;[http://aims.fao.org/standards/agrovoc/functionalities/search AGROVOC search]&lt;/ref&gt; and browse AGROVOC on the [[Agricultural Information Management Standards]] (AIMS) website. 
* Download: RDF-SKOS (AGROVOC only or AGROVOC LOD).&lt;ref&gt;[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC Releases]&lt;/ref&gt; 
* Live: SPARQL endpoint &lt;ref&gt;[http://202.45.139.84:10035/catalogs/fao/repositories/agrovoc AGROVOC SPARQL endpoint]&lt;/ref&gt; and the AGROVOC Web services.&lt;ref&gt;[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC releases]&lt;/ref&gt;

==Maintenance==
The AGROVOC team, located at FAO Headquarter, coordinates the editorial activities related to the maintenance of AGROVOC. The actual maintenance is carried out by a community of editors and institutions&lt;ref&gt;[http://aims.fao.org/standards/agrovoc/community AGROVOC people]&lt;/ref&gt; for each of the 23 language versions.

The tool used by the community to edit and maintain AGROVOC is Vocbench, which was designed to meet the needs of the Semantic Web and linked data environments. VocBench provides tools and functionalities that facilitate both collaborative editing and multilingual terminology. It also includes administration and group management features that permit flexible roles for maintenance, validation and quality assurance.

FAO also facilitates the technical maintenance of AGROVOC, including its publication as a LOD resource. Technical support is provided by the University of Tor Vergata&lt;ref&gt;[http://art.uniroma2.it/ Tor Vergata University]&lt;/ref&gt; (Rome, Italy) which leads the technical development of VocBench. The technical infrastructure for the online publication of AGROVOC is hosted by MIMOS Berhad&lt;ref&gt;[http://www.mimos.my/ MIMOS Berhad]&lt;/ref&gt; (Kuala Lumpur, Malaysia).

==Structure==
All 32,000+ concepts of the AGROVOC thesaurus are hierarchically organized under 25 top concepts. AGROVOC top concepts are very general and high level concepts, like &#8220;activities&#8221;, &#8220;organisms&#8221;, &#8220;location&#8221;, &#8220;products&#8221; etc. More than half of the total number of concepts (20,000+) fall under the top concept &#8220;organism&#8221;, which confirms how AGROVOC is largely oriented towards the agricultural sector.

AGROVOC is an RDF/SKOS-XL concept scheme, meaning the conceptual and terminological level are separated. The basic notions for such a concept scheme are: concepts, their labels and relations.

*'''Concepts''' 
Concepts are anything we want to represent or &#8220;talk about&#8221; in our domain. Concepts are represented by terms. A concept could also be considered as the set of all terms used to express it in various languages.
In SKOS, concepts are formalized as skos:Concept, identified by dereferenceable URIs (= URL). For example, the AGROVOC concept with URI http://aims.fao.org/aos/agrovoc/c_12332 is for ''maize''.

*'''Terms''' 
Terms are the actual terms used to name a concept. For example ''maize'', ''ma&#239;s'', ''&#29577;&#31859;'', ''&#3586;&#3657;&#3634;&#3623;&#3650;&#3614;&#3604;'' are all terms used to refer to the same concept in English, French, Chinese and Hindi respectively.

AGROVOC terms are expressed by means of the SKOS extension for labels, SKOS-XL. The predicates used are:
skosxl:prefLabel, used for preferred terms (&#8220;descriptors&#8221; in thesaurus terminology), and 
skosxl:altLabel, used for non- preferred terms.

*'''Relations'''
In SKOS, hierarchical relations between concepts are expressed by the predicates skos:broader, skos:narrower. They correspond to the classical thesaurus relations broader/narrower (BT/NT).

Non-hierarchical relations express a notion of &#8220;relatedness&#8221; between concepts. AGROVOC uses the SKOS relation skos:related (corresponding to the classical thesaurus RT), and a specific vocabulary of relations called Agrontology.&lt;ref&gt;[https://aims-fao.atlassian.net/wiki/display/AGV/Agrontology Agrontology]&lt;/ref&gt;

AGROVOC also allows for relations between labels (i.e. terms), thanks to the SKOS-XL extension to SKOS.

==Linked data==
AGROVOC is available as a linked data set and is aligned (linked) with 16 vocabularies related to agriculture (see table down below). The linked data version of AGROVOC is exposed as RDF and HTML, through a content-negotiation mechanism. It is also exposed through a SPARQL endpoint.

The advantage of having a thesaurus like AGROVOC published as LOD is that once thesauri are linked, the resources they index are linked as well. A good example is AGRIS, a mash-up web application that links the AGRIS bibliographic repository (indexed with AGROVOC) to related web resources (indexed with vocabularies linked to AGROVOC).

{| class="wikitable"
|-
! Resource !! Topics !! Linked concpets !! Languages !! Linked data !! Type of link
|-
| ASFA || Fisheries|| 1784|| || || skos:closeMatch
|-
| FAO Biotechnology Glossary || Biotechnologies|| 810|| EN, ES, FR, +3 more|| Yes|| skos:closeMatch
|-
| Chinese Agriculture Thesaurus (CAT)|| Agriculture|| || || Yes|| skos:closeMatch
|-
| EARTh|| Environment|| 1363 || EN+|| Yes|| skos:closeMatch
|-
| EUROVOC|| General EU || 1,297 || EN, ES, FR + 21 more || Yes || skos:exactMatch
|-
| GEMET || Environment || 1,191 || EN, ES, FR + 30 more || Yes|| skos:exactMatch
|-
| Library of Congress Subject Headings (LCSH)|| General || 1,093 || EN || Yes || skos:exactMatch
|-
| NAL Thesaurus || Agriculture || 13,390 || EN,ES || Yes || skos:exactMatch
|-
| RAMEAU R&#233;pertoire d'autorit&#233;-mati&#232;re encyclopedique et alphabetique unifie  || General || 686 || FR || Yes || skos:exactMatch
|-
| STW - Thesaurus for Economics || Economy || 1,136 || EN, DE || Yes || skos:exactMatch
|-
| TheSoz - Thesaurus for the Social Sciences || Social sciences || 846 || EN,DE || Yes || skos:exactMatch
|-
| Geopolical Ontology || Geopolitical entities || 253 || AR, CH, EN, ES, FR, RU || Yes || skos:exactMatch
|-
| Dewey Decimal Classification (DDC) || General || 409 || EN, ES, FR + 8 more || Yes || skos:exactMatch
|-
| DBpedia || General || 10,989 || EN, ES, FR + 8 more || Yes || skos:exactMatch
skos:closeMatch
|-
| SWD (Schlagwortnormdatei)|| General || 6,245 || DE || Yes || skos:exactMatch
skos:closeMatch
skos:broadMatch
skos:narrowMatch
|-
| GeoNames || Geographical entities || 212 || EN, ES, FR + 63 more || Yes || skos:exactMatch
|}

==Copyright and license==
The copyright for the AGROVOC thesaurus content in English, French, Russian and Spanish stays with FAO and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.&lt;ref&gt;[http://creativecommons.org/licenses/by-sa/3.0/ Creative Commons Attribution ShareAlike 3.0 Unported]&lt;/ref&gt; For any other language, the copyright rests with the institution responsible for its production.

==Related links==
* [[Agricultural Information Management Standards]]
* [[AGRIS]]
* [[Food and Agriculture Organization]]
* [[Geopolitical ontology]]

==External links==
* [http://agris.fao.org/ AGRIS]
* [http://aims.fao.org/standards/agrovoc AGROVOC]
* [http://aims.fao.org/ AIMS]
* [http://www.fao.org/home/en/ FAO]
* [https://www.w3.org/2001/sw/wiki/VocBench VocBench/Agricultural Ontology Server]

==Further reading==
* [http://aims.fao.org/standards/agrovoc/publications AGROVOC Publications]

==References==
{{reflist}}

{{DEFAULTSORT:Agrovoc}}
[[Category:Agricultural databases]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Food and Agriculture Organization]]
[[Category:Thesauri]]</text>
      <sha1>4ptfmztxv5ejhnkh4rrnc6ykmwvvcxt</sha1>
    </revision>
  </page>
  <page>
    <title>Darwin Core Archive</title>
    <ns>0</ns>
    <id>29824007</id>
    <revision>
      <id>689756392</id>
      <parentid>620216008</parentid>
      <timestamp>2015-11-09T05:45:01Z</timestamp>
      <contributor>
        <ip>2602:306:3651:8E50:2527:DDD9:A609:BBB</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3819" xml:space="preserve">{{Orphan|date=January 2011}}

'''Darwin Core Archive''' (DwC-A) is a [[biodiversity informatics]] data standard that makes use of the [[Darwin Core]] terms to produce a single, self-contained dataset for species occurrence or checklist data. Essentially it is a set of text (CSV) files with a simple descriptor (meta.xml) to inform others how your files are organized. The format is defined in the Darwin Core Text Guidelines.&lt;ref name="dwc-text"&gt;[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guidelines]&lt;/ref&gt; It is the preferred format for publishing data to the [[GBIF]] network. 

__TOC__

==Darwin Core==
The Darwin Core standard has been used to mobilise the vast majority of specimen occurrence and observational records within the GBIF network.&lt;ref name="gbif-dwca"&gt;[http://www.gbif.org/resources/2552 GBIF Darwin Core Archive, How-to Guide]&lt;/ref&gt; The [[Darwin Core]] standard was originally conceived to facilitate the discovery, retrieval, and integration of information about modern biological specimens, their spatio-temporal occurrence, and their supporting evidence housed in collections (physical or digital).

The Darwin Core today is broader in scope. It aims to provide a stable, standard reference for sharing information on biological diversity. As a glossary of terms, the Darwin Core provides stable semantic definitions with the goal of being maximally reusable in a variety of contexts. This means that Darwin Core may still be used in the same way it has historically been used, but may also serve as the basis for building more complex exchange formats, while still ensuring interoperability through a common set of terms.

==Archive Format==
{{unreferenced section|date=December 2010}}
The central idea of an archive is that its data files are logically arranged in a star-like manner, with one core data file surrounded by any number of &#8217;extensions&#8217;. Each extension record (or &#8216;extension file row&#8217;) points to a record in the core file; in this way, many extension records can exist for each single core record.

Details about recommended extensions can be found in their respective subsections and will be extensively documented in the GBIF registry, which will catalogue all available extensions.

Sharing entire datasets instead of using pageable web services like DiGIR and TAPIR allows much simpler and more efficient data transfer. For example, retrieving 260,000 records via TAPIR takes about nine hours, issuing 1,300 http requests to transfer 500 MB of XML-formatted data. The exact same dataset, encoded as DwC-A and zipped, becomes a 3 MB file. Therefore, GBIF highly recommends compressing an archive using ZIP or GZIP when generating a DwC-A. 

An archive requires stable identifiers for core records, but not for extensions. For any kind of shared data it is therefore necessary to have some sort of local record identifiers. It&#8217;s good practice to maintain &#8211; with the original data &#8211; identifiers that are stable over time and are not being reused after the record is deleted. If you can, please provide globally unique identifiers instead of local ones.

===Archive Descriptor===
To be completed.

&lt;!--
===Data Files===
To be completed.
--&gt;

===Dataset Metadata===
A Darwin Core Archive should contain a file containing metadata describing the whole dataset. The [[Ecological Metadata Language]] (EML) is the most common format for this, but simple Dublin Core files are being used too.

==References==
{{reflist}}

==External links==
* [http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]
* [[Biodiversity Information Standards]] (TDWG)
* [[Global Biodiversity Information Facility]] (GBIF)
* [[Biodiversity informatics]]

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]</text>
      <sha1>fb4ho39he1qckw8kou2ph0vq8qicyxz</sha1>
    </revision>
  </page>
  <page>
    <title>General Architecture for Text Engineering</title>
    <ns>0</ns>
    <id>11270152</id>
    <revision>
      <id>755401228</id>
      <parentid>748380685</parentid>
      <timestamp>2016-12-17T20:31:33Z</timestamp>
      <contributor>
        <username>Catlemur</username>
        <id>13510414</id>
      </contributor>
      <comment>Filled in 8 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8648" xml:space="preserve">{{Infobox software
| name = GATE
| screenshot = [[Image:GATE5 main window.png|250px]]
| caption = GATE Developer v5 main window
| developer = [http://gate.ac.uk/people GATE research team], [http://www.dcs.shef.ac.uk/ Dept. Computer Science, University of Sheffield]
| released = {{start date and age |1995}}
| frequently_updated = yes&lt;!-- Release version update? Don't edit this page, just click on the version number! --&gt;
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| language = English
| genre = [[Text mining]] [[Information Extraction]]
| license = [[LGPL]]
| website = {{url|http://gate.ac.uk}}
}}
'''General Architecture for Text Engineering''' or '''GATE''' is a [[Java (programming language)|Java]] suite of tools originally developed at the [[University of Sheffield]] beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many [[natural language processing]] tasks, including [[information extraction]] in many languages.&lt;ref&gt;Languages mentioned on http://gate.ac.uk/gate/plugins/ include Arabic, Bulgarian, Cebuano, Chinese, French, German, Hindi, Italian, Romanian and Russian.&lt;/ref&gt;

GATE has been compared to [[NLTK]], [[R (programming language)|R]] and [[RapidMiner]].&lt;ref&gt;{{cite web|url=http://www.b-eye-network.com/view/9516|title=Open Source Text Analytics by Seth Grimes - BeyeNETWORK|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; As well as being widely used in its own right, it forms the basis of the KIM semantic platform.&lt;ref&gt;{{cite journal|url=https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitlekim-a-semantic-platform-for-information-extraction-and-retrievaldiv/7249CC61F5AB25CBC7AAE182509DFEDE|title=KIM &#8211; a semantic platform for information extraction and retrieval|first1=Borislav|last1=Popov|first2=Atanas|last2=Kiryakov|first3=Damyan|last3=Ognyanoff|first4=Dimitar|last4=Manov|first5=Angel|last5=Kirilov|date=1 September 2004|publisher=|volume=10|issue=3-4|pages=375&#8211;392|accessdate=17 December 2016|via=Cambridge Core|doi=10.1017/S135132490400347X}}&lt;/ref&gt;

GATE community and research has been involved in several European research projects including [[Transitioning Applications to Ontologies|TAO]], [[SEKT]], NeOn, Media-Campaign, Musing, [[Service-Finder]], LIRICS and [[KnowledgeWeb Project|KnowledgeWeb]], as well as many other projects.

As of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from [[SourceForge]] are recorded since the project moved to SourceForge in 2005.&lt;ref&gt;{{cite web|url=http://sourceforge.net/projects/gate/|title=GATE|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; The paper "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications"&lt;ref&gt;[http://gate.ac.uk/sale/acl02/acl-main.pdf "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications", by Cunningham H., Maynard D., Bontcheva K. and Tablan V. (In proc. of the 40th Anniversary Meeting of the Association for Computational Linguistics, 2002)]&lt;/ref&gt; has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide,&lt;ref&gt;{{cite web|url=http://gate.ac.uk/userguide/|title=GATE.ac.uk  - sale/tao/split.html|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; include "Building Search Applications: Lucene, LingPipe, and Gate", by Manu Konchady,&lt;ref&gt;Konchady, Manu. [https://books.google.com/books?id=mcM-OAAACAAJ&amp;dq=Building+Search+Applications:+Lucene,+LingPipe,+and+Gate&amp;hl=en&amp;ei=avbDTczPJITqrQfk1IXQBA&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CDEQ6AEwAA Building Search Applications: Lucene, LingPipe, and Gate]. Mustru Publishing. 2008.&lt;/ref&gt; and "Introduction to Linguistic Annotation and Text Analytics", by Graham Wilcock.&lt;ref&gt;{{cite web|url=https://books.google.com/books?id=TDQJb1UgVywC&amp;dq=Introduction%20to%20Linguistic%20Annotation%20and%20Text%20Analytics&amp;printsec=frontcover&amp;source=bl&amp;ots=bAF26ZQSTx&amp;sig=TbxZ_-3tRy3IeDBKFofeVN6bAIc&amp;hl=en&amp;ei=vc0gS7PlLo-64QaSgqnfCQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=2&amp;ved=0CBcQ6AEwAQ#v=onepage&amp;q=&amp;f=false|title=Introduction to Linguistic Annotation and Text Analytics|first=Graham|last=Wilcock|date=1 January 2009|publisher=Morgan &amp; Claypool Publishers|accessdate=17 December 2016|via=Google Books}}&lt;/ref&gt;

== Features ==

GATE includes an [[information extraction]] system called '''ANNIE''' ('''A Nearly-New Information Extraction System''') which is a set of modules comprising a [[Lexical analysis|tokenizer]], a [[gazetteer]], a [[Sentence boundary disambiguation|sentence splitter]], a [[Part-of-speech tagging|part of speech tagger]], a [[Named entity recognition|named entities]] transducer and a [[coreference]] tagger. ANNIE can be used as-is to provide basic [[information extraction]] functionality, or provide a starting point for more specific tasks.

Languages currently handled in GATE include [[English language|English]], [[Mandarin Chinese|Chinese]], [[Arabic]], [[Bulgarian language|Bulgarian]], [[French language|French]], [[German language|German]], [[Hindi]], [[Italian language|Italian]], [[Cebuano language|Cebuano]], [[Romanian language|Romanian]], [[Russian language|Russian]], [[Danish language|Danish]].

Plugins are included for [[machine learning]] with [[Weka (machine learning)|Weka]], RASP, MAXENT, SVM Light, as well as a [[LIBSVM]] integration and an in-house [[perceptron]] implementation, for managing [[Ontology (information science)|ontologies]] like [[WordNet]], for querying [[search engines]] like [[Google]] or [[Yahoo]], for [[part of speech tagging]] with [[Brill tagger|Brill]] or TreeTagger, and many more. Many external plugins are also available, for handling e.g. [[Twitter|tweets]].&lt;ref&gt;{{cite web|url=https://gate.ac.uk/wiki/twitie.html|title=GATE.ac.uk  - wiki/twitie.html|publisher=|accessdate=17 December 2016}}&lt;/ref&gt;

GATE accepts input in various formats, such as [[Text file|TXT]], [[HTML]], [[XML]], [[DOC (computing)|Doc]], [[PDF]] documents, and [[Serialization|Java Serial]], [[PostgreSQL]], [[Lucene]], [[Oracle database|Oracle]] Databases with help of [[RDBMS]] storage over [[JDBC]].

[[JAPE (linguistics)|JAPE]] transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide.&lt;ref&gt;{{cite web|url=http://gate.ac.uk/userguide/chap:jape|title=GATE.ac.uk  - sale/tao/splitch8.html|publisher=|accessdate=17 December 2016}}&lt;/ref&gt; A tutorial has also been written by Press Association Images.&lt;ref&gt;{{cite web|url=http://realizingsemanticweb.blogspot.com/2009/07/jape-grammar-tutorial.html|title=Realizing Semantic Web: JAPE grammar tutorial|first=Dhavalkumar|last=Thakker|date=17 July 2009|publisher=|accessdate=17 December 2016}}&lt;/ref&gt;

== GATE Developer ==

[[Image:GATE5 main window.png|thumb|400px|GATE 5 main window.]]

The screenshot shows the document viewer used to display a document and its annotations. In pink are &lt;A&gt; hyperlink annotations from an [[Hypertext Markup Language|HTML]] file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.

== GATE M&#237;mir ==
&lt;!-- re-written to remove any lingering copyright worries --&gt;
 Generate vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and [[SPARQL]].

==See also==
{{Portal|Free software}}
* [[Unstructured Information Management Architecture]] (UIMA)
* [[OpenNLP]]
* [[List of natural language processing toolkits]]
* [[Pheme (project)|Pheme]], a major EU project managed by the GATE group on early detection of false information in social media

==References==
&lt;references/&gt;

{{DEFAULTSORT:General Architecture For Text Engineering}}
[[Category:Data mining and machine learning software]]
[[Category:Free computer libraries]]
[[Category:Free science software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free integrated development environments]]
[[Category:Knowledge representation]]
[[Category:Natural language processing toolkits]]
[[Category:Ontology editors]]</text>
      <sha1>lk8mr4h5qtnk6d56pbf8bls6qjy2drx</sha1>
    </revision>
  </page>
  <page>
    <title>Visual hierarchy</title>
    <ns>0</ns>
    <id>18587056</id>
    <revision>
      <id>748353169</id>
      <parentid>742261112</parentid>
      <timestamp>2016-11-07T20:15:24Z</timestamp>
      <contributor>
        <username>DearPrudence</username>
        <id>410416</id>
      </contributor>
      <comment>Reverting erroneous wording</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3870" xml:space="preserve">'''Visual hierarchy''' refers to the arrangement or presentation of elements in a way that implies importance.&lt;ref&gt;{{cite web|url = http://support.esri.com/en/knowledgebase/GISDictionary/term/visual%20hierarchy|title = GIS Dictionary|accessdate = 2014-08-13|publisher=ESRI}}&lt;/ref&gt; In other words, visual hierarchy influences the order in which the human eye perceives what it sees. This order is created by the visual [[Contrast (vision)|contrast]] between forms in a field of perception. Objects with highest contrast to their surroundings are recognized first by the human mind. The term visual hierarchy is used most frequently in the discourse of the visual arts fields, notably so within the field of [[graphic design]].

==Theory==
The concept of visual hierarchy is based in [[Gestalt psychology|Gestalt psychological theory]], an early 20th-century German theory that proposes that the human brain has innate organizing tendencies that &#8220;structure individual elements, shapes or forms into a coherent, organized whole.&#8221; &lt;ref&gt;Jackson, Ian. &#8220;Gestalt&#8212;A  Learning Theory for Graphic Design Education.&#8221; ''International Journal of Art and Design Education''. Volume 27. Issue 1 (2008): 63-69. Digital.&lt;/ref&gt; The German word Gestalt translates into &#8220;form,&#8221; &#8220;pattern,&#8221; or &#8220;shape&#8221; in English.&lt;ref&gt;Pettersson, Rune. &#8220;Information Design&#8212;Principles and Guidelines.&#8221; ''Journal of Visual Literacy''. Volume 29. Issue 2 (2010): 167-182. Digital.&lt;/ref&gt; When an element in a visual field disconnects from the &#8216;whole&#8217; created by the brain&#8217;s perceptual organization, it &#8220;stands out&#8221; to the viewer. The shapes that disconnect most severely from their surroundings stand out the most.

==Physical characteristics==
The brain disassociates objects from one another based upon the differences between their physical characteristics. These characteristics fall into four categories: color, size, alignment, and character. The category of color encompasses the [[hue]], [[Colorfulness|saturation]], [[Lightness (color)|value]], and perceived [[Texture (visual arts)|texture]] of forms. Size describes the surface area of a form. Alignment is the arrangement of forms with respect to their direction, orientation, or pattern.&lt;ref&gt;Feldsted, CJ. ''Design Fundamentals''. New York: Pittman Publishing Corporation, 1950.&lt;/ref&gt; Character is the [[Rectilinear polygon|rectilinearity]] and [[Curvilinear coordinates|curvilinearity]] of forms. Forms that have differences in these characteristics contrast each other.

==Application==
{{Unreferenced section|date=February 2015}}
Visual hierarchy is an important concept in the field of [[graphic design]], a field that specializes in visual organization. Designers attempt to control visual hierarchy to guide the eye to information in a specific order for a specific purpose. One could compare visual hierarchy in graphic design to grammatical structure in writing in terms of the importance of each principle to these fields.

===Examples===
[[Fluorescence|Fluorescent]] color contrasts highly against most naturally occurring colors. Fluorescent substances achieve this contrast by emitting light. Forms of this type of color are almost always high in visual hierarchy. [[Tennis ball]]s are fluorescent green for the perceptual ease of players, match officials, and spectators.

[[Camouflage]] patterns diminish the contrast between themselves and their surroundings. Camouflage describes a form that mimics the physical characteristics of its environment. These patterns are difficult and sometimes impossible to perceive. Certain animals and military forces have both developed their own camouflaged patterns as mechanisms of defense.

==See also==
*[[Bauhaus]]
*[[Cognitive psychology]]
*[[Pattern recognition]]

==References==
{{Reflist}}

[[Category:Page layout]]
[[Category:Knowledge representation]]</text>
      <sha1>ekxy3isiepp2omdcc2sm9v0vg0sdy73</sha1>
    </revision>
  </page>
  <page>
    <title>OntoWiki</title>
    <ns>0</ns>
    <id>12105194</id>
    <revision>
      <id>699952064</id>
      <parentid>626614461</parentid>
      <timestamp>2016-01-15T12:54:50Z</timestamp>
      <contributor>
        <username>White gecko</username>
        <id>7656203</id>
      </contributor>
      <comment>Add current version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2548" xml:space="preserve">{{Infobox Software | name = OntoWiki
| logo = 
| screenshot =&lt;!-- Deleted image removed:  [[Image:OntoWiki-screenshot.jpg|180px]] --&gt;
| caption = 
| founder = S&#246;ren Auer
| current maintainer = Sebastian Tramp
| latest_release_version = 0.9.11
| latest_release_date = January 31, 2014
| operating_system = [[Cross-platform]]
| programming language=[[PHP]]
| database=[[MySQL]]
| genre = [[knowledge management system]]
| license = [[GNU General Public License|GPL]]
| website = http://ontowiki.net
}}

'''OntoWiki''' is a free, [[open source software|open-source]] [[semantic wiki]] application, meant to serve as an [[ontology (computer science)|ontology]] editor and a [[knowledge management|knowledge acquisition]] system. It is a web-based application written in [[PHP]] and using either a [[MySQL]] database or a [[Virtuoso Universal Server|Virtuoso triple store]]. OntoWiki is form-based rather than syntax-based, and thus tries to hide as much of the complexity of knowledge representation formalisms from users as possible. OntoWiki is mainly being developed by the [http://aksw.org Agile Knowledge Engineering and Semantic Web (AKSW) research group] at the [[University of Leipzig]], a group also known for the [[DBpedia]] project among others, in collaboration with volunteers around the world.

In 2009 the AKSW research group got a budget of &#8364;425,000 from the [[Federal Ministry of Education and Research (Germany)|Federal Ministry of Education and Research of Germany]] for the development of the OntoWiki.&lt;ref&gt;[http://idw-online.de/pages/de/news300375 "OntoWiki" hilft Daten im Web zu verkn&#252;pfen] (German)&lt;/ref&gt;

In 2010 OntoWiki became part of the technology stack supporting the [[Framework Programmes for Research and Technological Development#LOD2|LOD2]] (Linked Open Data) project. Leipzig University is one of the consortium members of the project, which is funded by a &#8364;6.5m EU grant.&lt;ref&gt;{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&amp;ACTION=D&amp;CAT=PROJ&amp;RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects - LOD2 |date=2010-04-20}}&lt;/ref&gt;

==See also==
* [[Semantic MediaWiki]]
* [[DBpedia]]

== External links ==
* {{official website|http://OntoWiki.net}}
* [https://github.com/AKSW/OntoWiki#ontowiki About page on GitHub]
* [http://blog.aksw.org AKSW blog]

== References ==
&lt;references/&gt;

{{DEFAULTSORT:Ontowiki}}
[[Category:Semantic wiki software]]
[[Category:Free integrated development environments]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]</text>
      <sha1>5ce0sosxwickptm2gu8vxph3jqgeept</sha1>
    </revision>
  </page>
  <page>
    <title>Agricultural Information Management Standards</title>
    <ns>0</ns>
    <id>5465644</id>
    <revision>
      <id>727374332</id>
      <parentid>711272018</parentid>
      <timestamp>2016-06-28T15:31:38Z</timestamp>
      <contributor>
        <username>Yaron K.</username>
        <id>2276977</id>
      </contributor>
      <comment>/* See also */ Simplified</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12595" xml:space="preserve">{{Infobox website
|name=Agricultural Information Management Standards (AIMS)
|logo =[[File:Agricultural Information Management Standards (AIMS) logo.jpg|100px|AIMS logo]]
|slogan = ''Standards, Tools, Services &amp; Advice''
|url ={{URL|http://aims.fao.org}}
|type = [[Community of Practice]]
|commercial      = No
|registration    = Optional
|language        = English
|launch date = 2006
|current status = Online
|screenshot     = 
}}

[http://aims.fao.org/ '''Agricultural Information Management Standards'''], abbreviated to '''AIMS''' is a space for accessing and discussing agricultural information management standards, tools and methodologies connecting information workers worldwide to build a global community of practice. Information management standards, tools and good practices can be found on AIMS:

* to support the implementation of structured and linked information and knowledge to enable institutions and individuals from different technical backgrounds to build open and interoperable information systems;
* to provide advice on how to best manage, disseminate, share and exchange agricultural scientific information;
* to promote good practices widely applicable and easy to implement, and;
* to foster communities of practices centered on interoperability, reusability and cooperation.

== Users ==

AIMS is primarily intended for information workers&#8212;librarians, information managers, software developers&#8212;but is also of interest to those who are simply passionate about knowledge and information sharing. The success of AIMS depends upon its communities reaching a critical mass to show that the investment in interoperability standards has a return.

== Community ==

AIMS holds [http://aims.fao.org/communities-aims 9 communities of practice]. They are intended to discuss and share information about the different ongoing initiatives under the AIMS umbrella. AIMS supports collaboration through forums and blogs amongst institutions and individuals that wish to share expertise on how to use tools, standards and methodologies. Moreover, news and events are published on AIMS as part of its &#8216;one-stop&#8221; access to interoperability and reusability of information resources. The AIMS communities are aimed at the global agricultural community, including information providers, from research institutes, academic institutions, educational and extension institutions and also the private sector.

== Content ==

=== Vocabularies ===

* [[AGROVOC]] is a comprehensive multilingual vocabulary that contains close to 40,000 concepts in over 20 languages covering subject fields in agriculture, forestry and fisheries together with cross-cutting themes such as land use, rural livelihoods and food security.&lt;ref&gt;{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai144e/ai144e00.pdf |title=Basic Guidelines for Managing AGROVOC|year=2008 |accessdate=2011-08-01}}&lt;/ref&gt; It standardizes data description to enable a set of core integration goals: interoperability, reusability and cooperation.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/008/af238e/af238e04.htm |title=Agricultural Information Systems and Common Exchange Standards|year=2005 |accessdate=2011-08-01}}&lt;/ref&gt; In this spirit of collaboration, [[AGROVOC]] also works with other organizations that are using [[Linked Open Data]] techniques to connect vocabularies and build the backbone of the next generation of internet data; data that is marked up not just for style but for meaning. It is maintained by a global community of librarians, terminologists, information managers and software developers&lt;ref&gt;{{cite web|url=http://aims.fao.org/standards/agrovoc/community |title=AGROVOC Community |accessdate=2011-08-01}}&lt;/ref&gt; using [http://aims.fao.org/tools/vocbench-2 VocBench], a multilingual, web-based vocabulary editor and workflow management tool that allows for simultaneous, distributed editing.&lt;ref&gt;{{cite web|url=http://aims.fao.org/tools/vocbench-2 |title=VocBench Homepage |accessdate=2011-08-01}}&lt;/ref&gt;
* In addition to AGROVOC, AIMS provides access to other vocabularies like the [[Geopolitical ontology]] and [http://aims.fao.org/standards/agvocabularies/fisheries-ontology Fisheries Ontologies]. The [[Geopolitical ontology]] is used to facilitate data exchange and sharing in a standardized manner among systems managing information about countries and/or regions. The network of fisheries ontologies was created as a part of the [http://www.neon-project.org/nw/Welcome_to_the_NeOn_Project NeOn Project] and it covers the following areas: Water areas: for statistical reporting, jurisdictional ([[EEZ]]), environmental (LME), Species: taxonomic classification, ISSCAAP commercial classification, Aquatic resources, Land areas, Fisheries commodities, Vessel types and size, Gear types, [[AGROVOC]], ASFA.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/field/009/ai254e/ai254e00.htm |title=Revised and enhanced fisheries ontologies |accessdate=2011-08-01}}&lt;/ref&gt;
* [[AgMES]] is as a namespace designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]] or [[Australian Government Locator Service|AGLS]], used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc.&lt;ref&gt;{{cite web|url=ftp://193.43.36.44/gi/gil/gilws/aims/publications/workshops/coherence0/ppt/agmes.pdf |title=Agricultural Metadata Element Set: Standardization and Information Dissemination|accessdate=2011-08-01}}&lt;/ref&gt;
* Linked Open Data (LOD) - Enabled Bibliographic Data [http://aims.fao.org/lode/bd (LODE-BD) Recommendations 2.0] are a reference tool that assists bibliographic data providers in selecting appropriate encoding strategies according to their needs in order to facilitate metadata exchange by, for example, constructing crosswalks between their local data formats and widely used formats or even with a [[Linked Data]] representation

=== Tools ===
* [http://aims.fao.org/tools/agridrupal AgriDrupal] is both a suite of solutions for agricultural information management and a community of practice around these solutions.  The AgriDrupal community is made up of people who work in the community of agricultural information management specialists and have been experimenting with IM solutions in [[Drupal]].&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/article/am642e.pdf |title=AgriDrupal: repository management integrated into a content management system |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://aims.fao.org/agriocean-dspace AgriOcean DSpace] is a joint initiative of the [[United Nations]] agencies of [[FAO]] and [[UNESCO]]-IOC/IODE to provide a customized version of [[DSpace]]. It uses standards for [[metadata]], [[thesauri]] and other [[controlled vocabularies]] for [[oceanography]], [[marine science]], food, agriculture, development, [[fisheries]], [[forestry]], [[natural resources]] and other related sciences.&lt;ref&gt;{{cite web|url=http://eprints.rclis.org/handle/10760/15812 |title=AgriOcean DSpace : FAO and UNESCO/IOC-IODE Combine Efforts in their Support of Open Access |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://aims.fao.org/tools/vocbench-2 VocBench] is a web-based multilingual vocabulary management tool developed by [[FAO]] and hosted by [[MIMOS Berhad]]. It transforms thesauri, authority lists and glossaries into [[SKOS]]/[[Resource Description Framework|RDF]] concept schemes for use in a linked data environment. VocBench also manages the workflow and editorial processes implied by vocabulary evolution such as user rights/roles, validation and versioning. VocBench  supports a growing set of user communities, including the global, distributed group of terminologists who manage [[AGROVOC]].&lt;ref&gt;{{cite web|url=http://semtech2011.semanticweb.com/uploads/handouts/MON_600_Jaques_3910.pdf |title=VocBench: vocabulary editing and workflow management |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://aims.fao.org/tools/webagris-2 WebAGRIS] is a multilingual Web-based system for distributed data input, processing and dissemination (through the Internet or on CD-Rom), of agricultural bibliographic information. It is based on common standards of data input and dissemination formats ([[XML]], [[Html|HTML]], ISO2709), as well as subject categorization schema and [[AGROVOC]].&lt;ref&gt;{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai161e/ai161e00.pdf |title=FAO&#8217;s experience in metadata exchange from CDS/ISIS bibliographic databases using XML format, compliant to Dublin Core standard |accessdate=2011-08-01}}&lt;/ref&gt;

=== Services ===
* [http://www.agrifeeds.org/ AgriFeeds] is a service that allows users to search and filter news and events from several agricultural information sources and to create custom feeds based on the filters applied.&lt;ref&gt;{{cite web|url=ftp://ftp.fao.org/docrep/fao/011/ak182e/ak182e00.pdf |title=AgriFeeds: The Agricultural News and Events Aggregator |accessdate=2011-08-01}}&lt;/ref&gt; AgriFeeds was designed in the context on [http://www.ciard.net/ CIARD] (Coherence in Information for Agricultural Research for Development). Within CIARD, the partners who designed and implemented AgriFeeds are [[FAO]] and [[Global Forum on Agricultural Research|GFAR]]. AgriFeeds is currently maintained by [[FAO]].
* [[AGRIS]] is a global public domain database with nearly 3 million structured bibliographical records on agricultural science and technology. The database is maintained by [[FAO]], with the content provided by more than 100 participating institutions from 65 countries.&lt;ref&gt;{{cite web|url=http://agris.fao.org/knowledge-and-information-sharing-through-agris-network |title=Knowledge and information sharing through the AGRIS Network |accessdate=2011-08-01}}&lt;/ref&gt;
* [http://ring.ciard.net/ CIARD Routemap to Information Nodes and Gateways (RING)] is a project implemented within CIARD and is led by [[Global Forum on Agricultural Research|GFAR]]. The RING is a global registry of web-based services that give access to any kind of information pertaining to agricultural research for development (ARD). It allows information providers to register their services in various categories and so facilitate the discovery of sources of agriculture-related information across the world.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/012/al207e/al207e00.pdf |title=The CIARD RING, an infrastructure for interoperability of agricultural research information services |year=2010 |accessdate=2011-08-01}}&lt;/ref&gt;
* Since January 2011, AIMS supports [[E-lis|E-LIS]], the international electronic [[Open Archives Initiative|archive]] for [[Library science|library and information science]] (LIS). E-LIS is established, managed and maintained by an international team of 73 librarians and information scientists from 47 countries and support for 22 languages. It is freely accessible, aligned with the [[Open access (publishing)|Open Access]] (OA) movement and is a voluntary enterprise. Currently it is the largest international repository in the LIS field. Searching or browsing E-LIS is a kind of multilingual, multicultural experience, an example of what could be accomplished through open access archives to bring the people of the world together.&lt;ref&gt;{{cite web|url=http://eprints.rclis.org/handle/10760/6634 |title=E-LIS: an international open archive towards building open digital libraries |year=2005 |accessdate=2011-08-02}}&lt;/ref&gt;
* [http://aims.fao.org/vest-registry VEST Registry] is a catalog of controlled vocabularies (such as authority files, classification systems, [[concept maps]], controlled lists, dictionaries, [[ontologies]] or subject headings); [[metadata]] sets ([[metadata]] element sets, namespaces and application profiles); and tools (such as library management software, content management systems or document repository software). It is concerned primarily with collecting and maintaining a consistent set of [[metadata]] for each resource. The scope of the VEST Registry is to provide a clearing house for tools, [[metadata]] sets and vocabularies used in food, [[agriculture]], development, [[fisheries]], [[forestry]] and [[natural resources]] [[information management]] context.

==See also==
* [[AGRIS]]
* [[AGROVOC]]
* [[E-LIS]]
* [[IMARK]]
* [[Geopolitical ontology]]

== References ==

{{Reflist|2}}

== External links ==
* [http://aims.fao.org/home Agricultural Information Management Standards]

[[Category:Agriculture]]
[[Category:Food and Agriculture Organization]]
[[Category:Standards]]
[[Category:Information science]]
[[Category:Knowledge]]
[[Category:Knowledge representation]]
[[Category:Library science]]</text>
      <sha1>bycpu2156tett01plagopzshj2vofiw</sha1>
    </revision>
  </page>
  <page>
    <title>Personal knowledge base</title>
    <ns>0</ns>
    <id>33562977</id>
    <revision>
      <id>755569068</id>
      <parentid>749153229</parentid>
      <timestamp>2016-12-18T22:01:30Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Spatial */ rm spaces</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="82612" xml:space="preserve">{{about|knowledge management software|the general concept|Personal knowledge management}}
{{Copypaste|url=http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf|date=June 2016}}
A '''personal knowledge base''' ('''PKB''') is an electronic tool used to express, capture, and later retrieve the personal knowledge of an individual. It differs from a traditional [[database]] in that it contains subjective material particular to the owner, that others may not agree with nor care about. Importantly, a PKB consists primarily of knowledge, rather than [[information]]; in other words, it is not a collection of documents or other sources an individual has encountered, but rather an expression of the distilled knowledge the owner has extracted from those sources.&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

== Definition ==

The term ''personal knowledge base'' was mentioned as early as the 1980s,&lt;ref name = "Brooks 1985"/&gt;&lt;ref name = "Kruger 1986"/&gt;&lt;ref name = "Forman 1988"/&gt;&lt;ref name = "Smith 1991"/&gt; but the term came to prominence when it was described at length in publications by computer scientist Stephen Davies and colleagues,&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt; who defined the term as follows:{{efn|An earlier version of this article incorrectly stated that the term ''personal knowledge base'' was coined in 2011; in fact, Stephen Davies and colleagues wrote a paper on the subject in 2005,&lt;ref name = "Davies 2005"/&gt; and publications by other authors had mentioned the term as early as the 1980s.&lt;ref name = "Brooks 1985"/&gt;&lt;ref name = "Kruger 1986"/&gt;&lt;ref name = "Forman 1988"/&gt;&lt;ref name = "Smith 1991"/&gt; Much of the present article closely follows the publications by Davies and colleagues.}}

* '''personal''': a PKB is intended for private use, and its contents are custom-tailored to the individual. It contains trends, relationships, categories, and personal observations that its owner perceives but which no one else may agree with. It can be shared, just as one can explain one's own opinion to a hearer, but it is not jointly ''owned'' by anyone else any more than explaining one's opinion to a friend causes the friend to own one's mind.
* '''knowledge''': a PKB contains knowledge, not merely information. Its purpose is not simply to aggregate all the information sources one has seen, but to preserve the knowledge that one has ''learned'' from those sources. When a user returns to a PKB to retrieve knowledge she has stored, she is not merely pointed back to the original documents, where she must relocate, reread, reparse, and relearn the relevant passages. Instead, she is returned to the distilled version of the particular truth she is seeking, so that the mental model she originally had in mind can be easily reformed.
* '''base''': a PKB is a consolidated, integrated knowledge store. It is a reflection of its owner's memory, which, as Bush and many others have observed, can freely associate any two thoughts together, without restriction. Hence a PKB does not attempt to partition a user's field of knowledge into multiple segments that cannot reference one another. Rather, it can connect any two concepts without regard for artificial boundaries, and acts as a single, unified whole.

=== Contrast with other classes of systems ===

The following classes of systems ''cannot'' be classified as PKBs:&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

* collaborative efforts to build a universal objective space (as opposed to an individual's personal knowledge.) The World Wide Web itself is in this category, as were its predecessors HyperTIES&lt;ref name = "Schneiderman 1987"/&gt; and Xanadu,&lt;ref name = "Nelson 1987"/&gt; Web categorization systems like the [[Open Directory Project]], and collaborative information collections like [[Wikipedia]].
* search systems like Enfish and the Stuff I've Seen project&lt;ref name="Dumais et al 2003" /&gt; that index and search one's information sources on demand, but do not give the user the ability to craft and express personal knowledge.
* tools whose goal is to produce a design artifact rather than to maintain knowledge for its own sake. Systems like ART&lt;ref name="Nakakoji et al 2000" /&gt; and Writing Environment&lt;ref name="Smith et al 1987" /&gt; use intermediate knowledge representations as a means to an end, abandoning them once a final artifact has been produced, and hence are not suitable as PKBs.
* systems that focus on capturing transient information, rather than archiving knowledge that has long-term value. Examples would be Web logs&lt;ref name="Godwin-Jones 2003"/&gt; and e-diaries.&lt;ref name="Kovalainen et al 1998" /&gt;  Tools whose information domain is mostly limited to [[time management]] tasks (calendars, action items, contacts, etc.) rather than "general knowledge". Blandford and [[Thomas R.G. Green|Green]]&lt;ref name="Blandford and Green 2001" /&gt; and Palen&lt;ref name="Palen 1999" /&gt; give excellent surveys; common commercial examples would be [[Microsoft Outlook]], [[IBM Lotus Notes|Lotus Notes]], and [[Evolution (software)|Novell Evolution]].
* similarly, tools developed for a specific domain, such as bibliographic research rather than for "general knowledge".

==== Personal information management ====

PKM is similar to [[personal information management]], but is a distinct topic based on the "information" vs. "knowledge" difference. PKBs are about recording and managing the knowledge one derives from documents, whereas PIM is more about managing and retrieving the documents themselves.&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

== Historical influences ==

Non-electronic personal knowledge bases have probably existed in some form since the dawn of written language: [[Leonardo da vinci#Journals and notes|Da Vinci's notebooks]] are a famous example. More commonly, card files and personal annotated libraries have served this function in the pre-electronic age.

=== Bush's Memex ===

Undoubtedly the most famous early formulation of an electronic PKB was [[Vannevar Bush]]'s description of the "[[Memex]]" in 1945.&lt;ref name="Bush 1945" /&gt; Bush surveyed the post-World-War-II landscape and laid out what he viewed as the most important forthcoming challenges to humankind in ''[[The Atlantic Monthly]]''. The Memex was a theoretical (never implemented) design or a system to help tackle the [[information overload]] problem, already formidable in 1945. In Bush's own words:

&lt;blockquote&gt;Consider a future device for individual use, which is a sort of mechanized private file and library. ... [A] device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.  &lt;/blockquote&gt;

Bush envisioned collaborative aspects as well, and even a world-wide system that scientists could freely consult.{{Citation needed|date=November 2012}} But an important emphasis throughout the article was on expanding our own powers of recollection: "Man needs to mechanize his record more fully," he says, if he is not to "become bogged down...by overtaxing his limited memory". With the Memex, the user could "add marginal notes and comments," and "build a trail of his interest" through the larger information space. She could share trails with friends, identify related works, and create personal annotations. Bush's Memex would give each individual the ability to create, categorize, classify, and relate his own set of information corresponding to his unique personal viewpoint.  Much of that information would in fact consist of bits and pieces from public documents, just as the majority of the knowledge inside our own heads has been imbibed from what we read and hear. But the Memex also allowed for the specialized recording of information that each individual perceived and needed to retain. The idea of supplementing our memory" was not a one-size-fits-all proposition, since no two people have the same interests, opinions, or memories.  Instead, it demanded a subjective expression of knowledge, unique to each individual.

=== Graphical knowledge capture tools ===

Great emphasis is placed on the pictorial nature of diagrams to represent abstract knowledge; the use of spatial layout, color, and images is said to strengthen understanding and promote creativity. Each of the three primary schools&#8212;[[mind map]]ping, [[concept map]]ping, and [[cognitive map]]ping&#8212;prescribes its own data model and procedures, and each boasts a number of software applications designed specifically to create compatible diagrams.

==== Mind mapping ====
[[Mind map]]ping was promoted by pop psychologist [[Tony Buzan]] in the 1960s, and commands the allegiance of an impressive number of adherents worldwide.  A mind map is essentially nothing more than a visual outline, in which a main idea or topic is written in the center of the diagram, and subtopics radiate outwards in increasing levels of specificity. The primary value is in the freeform, spatial layout (rather than a sequential, numbered outline), the ability for a software application to hide or reveal select levels of detail, and as mentioned above, graphical adornments. The basic data model is a [[Tree (graph theory)|tree]], rather than a [[Graph (discrete mathematics)|graph]], with all edges implicitly labeled "supertopic/subtopic". Numerous tools are available for constructing mind maps.

==== Concept mapping ====

[[Concept map]]s were developed by Cornell Professor [[Joseph D. Novak|Joseph Novak]],&lt;ref name="Novak 2003" /&gt; and based on [[David Ausubel]]'s assimilation theory of learning.&lt;ref name="Ausubel 1968" /&gt; An essential tenet is that newly encountered knowledge must be related to one's prior knowledge in order to be properly understood. Concept maps help depict such connections graphically. Like mind maps, they feature evocative words or phrases in boxes connected by lines. There are two principal differences, however: first, a concept map is properly a graph, not a tree, permitting arbitrary links between nodes rather than only parent/child relationships; and second, the links are labeled to identify the nature of the inter-concept relationship, typically with a verb phrase. In this way, the links on a diagram can be read as English sentences, with the upstream node as the subject and the downstream node as the direct object of the sentence.

There are many applications available that could be used for drawing these diagrams, not all of which directly acknowledge their support for concept maps in particular.&lt;ref name="Canas et al 2005"/&gt;&lt;ref name="Gaines and Shaw 1995" /&gt;

A concept map is virtually identical to the notion of a "[[semantic network]]",&lt;ref name="Woods 1985" /&gt; which has served as a cornerstone for much artificial intelligence work since its inception. Semantic networks, too, are directed graphs in which the nodes represent concepts and labeled edges the relationships between them. Much psychology research has strengthened the idea that the human mind internalizes knowledge in something very like this sort of framework. This likely explains the ease with which concept mapping techniques have been adopted by the uninitiated, since concept maps and semantic networks can be considered equivalent.

==== Cognitive mapping ====

[[Cognitive mapping]], developed by Fran Ackermann and Colin Eden at the University of Strathclyde, uses the same data model as does concept mapping, but with a new set of techniques. In cognitive maps, element names have two parts, separated by an ellipsis that is read "as opposed to" in order to further clarify the semantics of the node. ("Cold...hot" is different from "cold...freezing," for example.) Links are of three types&#8212;causal, temporal, connotative&#8212;the first of which is the most common and is read as "may lead to". Generally cognitive mapping is best suited to domains involving arguments and [[decision making]]. Cognitive mapping is not nearly as widespread as the other two paradigms. Together, these and related methods have brought into the mainstream the idea of breaking down knowledge into its fundamental elements, and representing them graphically. Students and workers from widely diverse backgrounds have experienced success in better articulating and examining their own knowledge, and in discovering how it relates to what else they know. Although architectural considerations prevent any of these tools from functioning as bona fide PKBs, the ideas they have contributed to a front-end interface mechanism cannot be overestimated.

=== Hypertext systems ===

Many in the hypertext community [[Hypertext#History|reference]] Vannevar Bush's article as the cornerstone of their heritage. Hence the development of hypertext techniques, while seldom applied specifically towards PKB solutions, is important.  There have basically been three types of hypertext systems: those that exploit features of non-linear text to create a dynamic, but coherent "hyperdocument";&lt;ref name="Schneiderman 1987" /&gt;&lt;ref name="Goodman 1988" /&gt; those
that prescribe ways of linking existing documents together for navigation and expression of affinities;&lt;ref name="Davis et al 1993" /&gt;&lt;ref name="Garrett et al 1986" /&gt;&lt;ref name="Pearl 1989" /&gt; and those that use the hypertext model specifically to model abstract knowledge. Though the first and especially the second category have dominated research efforts (and public enthusiasm) over the past several decades, it is this third class that is closest in spirit to the original vision of hypertext by its founders.

In a similar vein to [[Vannevar Bush|Bush]], [[Doug Engelbart]]'s focus was to develop computer systems to "help people think better".  He sought data models that more closely paralleled the human thought process, and settled on using hypertext as a way to represent and store abstract human knowledge. Although his "[[NLS (computer system)|Augment]]" system underwent many changes, the original purpose closely aligned with that of PKBs.&lt;ref name="Engelbart 1953"/&gt;

More recently, Randall Trigg's TextNet&lt;ref name="Trigg and Weiser 1986" /&gt; and [[NoteCards]]&lt;ref name="Halasz et al 1987" /&gt; systems further explored this idea. TextNet revolved around "primitive pieces of text connected with typed links to form a network similar in many ways to a semantic network".&lt;ref name="Conklin and Begeman 1988"/&gt; Though text-centric, it was clear that Trigg's goal was to model the associations between primitive ideas and hence to reflect the mind's understanding. "By using...structure, meaning can be extracted from the relationships between chunks (small pieces of text) rather than from the words making them up."&lt;ref name="Trigg and Weiser 1986" /&gt; The subsequent [[NoteCards]] effort was similarly designed to "formulate, structure, compare, and manage ideas". It was useful for "analyzing information, constructing models, formulating arguments, designing artifacts, and generally processing ideas".

Conklin and Begeman's [[gIBIS]] system was another early effort into true knowledge representation, specifically for the field of design deliberations and arguments.&lt;ref name="Conklin and Begeman 1988"/&gt; The project lived on in the later project QuestMap&lt;ref name="Selvin 1999" /&gt; and the more modern [[Compendium (software)|Compendium]], which has been primarily used for capturing group knowledge expressed in face-to-face meetings. In all these cases, systems use semantic hypertext in an attempt to capture shared knowledge in its most basic form. Other examples of knowledge-based hypertext tools include Mental Link,&lt;ref name="Dede and Jayaram 1990" /&gt; Aquanet,&lt;ref name="Marshall et al 1991" /&gt; and SPRINT,&lt;ref name="Carlson and Ram 1990" /&gt; as well
as a few current commercial tools such as [[PersonalBrain]] and [[Tinderbox (application software)|Tinderbox]]&lt;ref name="Bernstein 2003" /&gt; and open source tools such as [[TiddlyWiki]].

=== Note-taking applications ===

[[Electronic Notetaking|Note-taking applications]] allow a user to create snippets of text and then organize or categorize them in some way. These tools can be used to form PKBs that are composed of such text snippets.

Most of these tools are based on a [[Tree (graph theory)|tree]] hierarchy, in which the user can write pages of notes and then organize them into sections and subsections. The higher level sections or chapters often receive a colored tab exactly as a physical three-ring notebook might. Other designers eschew the tree model for a more flexible category-based approach (see section [[#Data models|data models]]). The primary purpose of all these tools is to offer the benefits of freeform note-taking with none of the deficiencies: users are free to brainstorm and jot down anything from bullet points to polished text, while still being able to search, rearrange, and restructure the entire notebook easily.

An important subcategory of note-taking tools is outliners (e.g., [[OmniOutliner]]), or applications specifically designed to organize ideas in a hierarchy. These tools typically show a two-pane display with a tree-like navigation widget in the left-pane and a list of items in the right-pane. Topics and subtopics can be rearranged, and each outline stored in its own file. Modern outliners feature the ability to add graphics and other formatting to an item, and even hyper links to external websites or documents. The once abandoned (but now resurrected) Ecco system was among the first to allow items to have typed attributes, displayed in columns. This gives the effect of a custom spreadsheet per topic, with the topic's items as rows and the columns as attributes. It allows the user to gracefully introduce structure to their information as it is identified.

Of particular interest are applications optimized for subsuming portions of an information space realm into a PKB, where they can be clustered and arranged according to the user's own perceptions. The Virtual Notebook System (VNS)&lt;ref name="Burger et al 1991" /&gt; was one of the first to emphasize this. VNS was designed for sharing information among scientists at the Baylor College of Medicine; a user's "personal notebook" could make references to specific sections of a "community notebook," and even include arbitrary segments of other documents through a cut-and-paste mechanism.

=== Document management systems ===
{{main|Document management system}}

Another influence on PKBs are systems whose primary purpose is to help users organize documents, rather than personal knowledge derived from those documents. Such systems do not encode subjective knowledge per se, but they do create a personal knowledge base of sorts by allowing users to organize and cross-reference their information artifacts.

These efforts provide alternative indexing mechanisms to the limited "directory path and file name" approach. Presto&lt;ref name="Dourish et al 1999" /&gt; replaces the directory hierarchy entirely with attributes that users assign to files. These key-value pairs represent user-perceived properties of the documents, and are used as a flexible means for retrieval and organization. William Jones' Memory Extender&lt;ref name="Jones 1986" /&gt; was similar in spirit, but it dynamically varied the "weight" of a file's keywords according to the user's context and perceived access patterns. In [[Haystack (MIT project)|Haystack]],&lt;ref name="Adar et al 1999" /&gt; users&#8212;in conjunction with automated software agents&#8212;build a graph-based network of associative links through which documents can be retrieved.

Metadata and multiple
categorization can also be applied to provide multiple retrieval paths customized to the way the individual thinks and works with their information sources. WebTop&lt;ref name="Wolber et al 2002" /&gt; allowed the user to create explicit links between documents, but then also merged these user-defined relationships with other types of associations. These included the hyperlinks contained in the documents, associations implied by structural relationships, and content similarities discovered by text analysis. The idea was that any way in which items can be considered "related" should be made available to the user for help with retrieval.

A subclass of these systems integrate the user's personal workspace with a search facility, blurring the distinction between information retrieval and information organization.  SketchTrieve,&lt;ref name="Hendry and Harper 1997" /&gt; DLITE,&lt;ref name="Cousins et al 1997" /&gt; and Garnet&lt;ref name="Buchanan et al 2004" /&gt; each materialized elements from the retrieval domain (repositories, queries, search results) into tangible, manipulatable screen objects. These could be introduced directly into a spatial layout that also included the information sources themselves. These systems can be seen as combining a spatial hypertext interface as in VIKI&lt;ref name="Marshall and Shipman 1995" /&gt; with direct access to digital library search facilities.  NaviQue&lt;ref name="Furnas and Rauch 1998" /&gt; was largely in the same vein, though it incorporated a powerful similarity engine to proactively aid the user in organization. CYCLADES&lt;ref name="Renda and Straccia 2005" /&gt; let users organize Web pages into folders, and then attempted to infer what each folder "means" to that user, based on a statistical textual analysis of its contents. This helps users locate other items similar to what's already in a folder, learn what other users have found interesting and have grouped together, etc.

All of these document management systems are principally concerned with organizing objective information sources rather than the expression of subjective knowledge. Yet their methods are useful to consider with respect to PKB systems, because such a large part of our knowledge comprises things we remember, assimilate, and repurpose from objective sources. Search environments like SketchTrieve, as well as snippet gatherers like YellowPen, address an important need in [[knowledge management]]: bridging the divide between the subjective and objective realms, so that the former can make reference to and bring structure to the latter.

== Claims and benefits ==

PKB systems make various claims about the advantages of using them. These can be classified as follows:&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

* '''Knowledge generation and formulation.''' Here the emphasis is on procedure, not persistence; it is the act of simply using the tool to express one's knowledge that helps, rather than the ability to retrieve it later.
* '''[[Knowledge capture]].''' PKBs do not merely allow one to express knowledge, but also to capture it before it elusively disappears. Often the emphasis is on a streamlined user interface, with few distractions and little encumbrance.  The point is to lower the burden of jotting down one's thoughts so that neither task nor thought process is interrupted.
* '''Knowledge organization.''' A 2003 study on note-taking habits found that "better organization" was the most commonly desired improvement in people's own information recording practices.&lt;ref name="Hayes et al 2003" /&gt;
* '''Knowledge management and [[knowledge retrieval|retrieval]].''' Perhaps the most critical aspect of a PKB is that the knowledge it stores is permanent and accessible, ready to be retrieved at any later time.
* '''[[Knowledge integration|Integrating]] heterogeneous sources.''' Recognizing that the knowledge people form comes from a variety of different places, many PKB systems emphasize that the information from diverse sources and of different types can be integrated into a single database and interface.

== Data models ==
{{see|Data model}}
PKB systems can be compared along a number of different axes, the most important of which is the underlying data model they support. This is what prescribes and constrains the nature of the knowledge they can contain: what types of knowledge elements are allowed, how they can be structured, and how the user perceives them and can interact with them.&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

Three aspects of data models can be identified: the ''structural framework'', which prescribes rules about how knowledge elements can be structured and interrelated; the ''knowledge elements'' themselves, or basic building blocks of information that a user creates and works with; and ''schema'', which involves the level of formal semantics introduced into the data model.

=== Structural frameworks ===

The following structural frameworks have been featured in one or more prominent PKB systems.

==== Tree ====

Systems that support a [[Tree (data structure)|tree]] model allow knowledge elements to be organized into a containment hierarchy, in which each element has one and only one "parent". This takes advantage of the mind's natural tendency to classify objects into groups, and to further break up each classification into subclassifications. It also mimics the way that a document can be broken up into chapters, sections, and subsections.  It tends to be natural for users to understand.

All of the applications for creating Buzan [[Mind Map|mind maps]] are based on a tree model, because a mind map ''is'' a tree. Each mind map has a "root" element in the center of the diagram (often called a "main topic") from which all other elements emanate as descendents.  Every knowledge element has one and only one place in this structure. Some tools, such as [[MindManager]], extend this paradigm by introducing "floating topics", which are not anchored to the hierarchy, and permitting "crosslinks" to arbitrary topics, similar to those in concept maps.

Other examples of tree-based systems are most personalized search interfaces,&lt;ref name="Renda and Straccia 2005" /&gt;&lt;ref name="Di Giacomo et al 2001" /&gt;&lt;ref name="Reyes-Farfan and Sanchez 2003"/&gt; outliners, and most of the "notebook-based" note-taking systems. By allowing them to partition their notes into sections and subsections, note-taking tools channel users into a tree hierarchy. In recognition of this confining limitation, many of these tools also permit a kind of "crosslink" between items, or employ some form of transclusion (see below) to allow items to co-exist in several places. The dominant paradigm in such tools, however, remains the simple parent-child hierarchy.

==== Graph ====

Graph-based systems allow users to create knowledge elements and then to interconnect them in arbitrary ways. The elements of a [[Graph (discrete mathematics)|graph]] are traditionally called "vertices," and connected by "arcs," though the terminology used by graph-based systems varies widely (see Table 1) and the hypertext community normally uses the terms "nodes" and "links". There are no restrictions on how many arcs one vertex can have with others, no notion of a "parent/child" relationship between vertices (unless the user chooses to label an arc with those semantics), and normally no "root" vertex. In many systems, arcs can optionally be labeled with a word or phrase indicating the nature of the relationship, and adorned with arrowheads on one or both ends to indicate navigability. (Neither of these adornments is necessary with a tree, since all relationships are implicitly labeled "parent/child" and are navigable from parent to child.) A graph is a more general form of a tree, and hence a strictly more powerful form of expression.

{| class="wikitable"
|+Terminology employed by a sampling of graph-based knowledge tools.
! System !! Vertex !! Arc !! Graph
|-
|[[Axon Idea Processor]]||object||link||diagram
|-
|Banxia Decision Explorer||concept||link||view
|-
|[[Compendium (software)|Compendium]]||node||link||view
|-
|[[Haystack (MIT project)|Haystack]]||needle||tie||bale
|-
|Idea Graph||idea||connection||ideagraph
|-
|Knowledge Manager||concept||relation||map
|-
|[[MyLifeBits]]||resource||link/annotation||story
|-
|[[NoteCards]]||note card||link||browser
|-
|[[PersonalBrain]]||thought||link||brain
|-
|RecallPlus||idea||association||diagram
|-
|SMART Ideas||symbol||connector||level
|}

This model is the defining characteristic of hypertext systems&lt;ref name="Halasz and Schwartz 1994" /&gt; including many of those used for document management.&lt;ref name="Wolber et al 2002" /&gt;&lt;ref name="Adar et al  1999" /&gt; It is also the underpinning of all concept-mapping tools, whether they actually acknowledge the name "concept maps"&lt;ref name="Canas et al 2005" /&gt;&lt;ref name="Gaines and Shaw 1995" /&gt; or advertise themselves simply as tools to draw knowledge diagrams. As mentioned previously, graphs draw their power from the fact that humans are thought to model knowledge as graphs (or equivalently, semantic networks) internally. In fact, it could be argued that all human knowledge can be ultimately reduced to a graph of some kind, which argues strongly for its sufficiency as a structural framework.&lt;ref name="Quillian 1968" /&gt;&lt;ref name="Nosek and Roth 1990" /&gt;

An interesting aspect of graph-based systems is whether or not they require a ''[[Connectivity (graph theory)|fully connected]]'' graph.  A fully connected graph is one in which every vertex can be reached from any other by simply performing enough arc traversals. There are no "islands" of vertices that are severed from each other. Most graph-based tools allow non-fully-connected graphs: knowledge elements are added to the system, and connected arbitrarily to each other, without constraint.  But a few tools, such as [[PersonalBrain]] and [[Compendium (software)|Compendium]], actually require a single network of information in which every knowledge element must be indirectly connected to every other. If one attempts to remove the last link that connects a body of nodes to the original root, the severed elements are either "forgotten" or else moved to a deleted objects heap where they can only be accessed by restoring a connection to the rest of the graph.

Some hypertext systems&lt;ref name="Garrett et al 1986" /&gt;&lt;ref name="Delisle and Schwartz 1986" /&gt; add precision to the basic linking mechanism by allowing nodes to reference not only other nodes, but sections within nodes.&lt;ref name="Halasz and Schwartz 1994" /&gt; This ability is especially useful if the nodes themselves contain sizeable content, and also for PKB elements making reference to fragments of objective sources.

==== Tree plus graph ====

Although graphs are a strict superset of trees, trees offer some important advantages in their own right: simplicity, familiarity, ease of navigation, and the ability to conceal details at any level of abstraction. Indeed, the problem of "disorientation" in hypertext navigation&lt;ref name="Conklin and Begeman 1988"/&gt;&lt;ref name="Mantei 1982" /&gt; largely disappears with the tree model; one is never confused about "where one is" in the larger structure, because traversing the parent hierarchy gives the context of the larger surroundings. For this reason, several graph-based systems have incorporated special support for trees as well, to combine the advantages of both approaches.  For instance, in concept mapping techniques, a generally hierarchical paradigm is prescribed, after which users are encouraged to identify "crosslinks" between distant concepts. Similarly, some systems using the mind mapping paradigm permit arbitrary relationships between nodes.

One of the earliest systems to combine tree and graph primitives was TEXTNET,&lt;ref name="Trigg and Weiser 1986" /&gt; which featured two types of nodes: "chunks" (which contained content to be browsed and organized) and "table of contents" nodes (or "tocs".) Any node could freely link to any other, permitting an unrestricted graph. But a group of tocs could be combined to form a tree-like hierarchy that bottomed out in various chunk nodes. In this way, any number of trees could be superimposed upon an arbitrary graph, allowing it to be viewed and browsed as a tree, with all the requisite advantages. Strictly speaking, a network of tocs formed a [[Directed acyclic graph|DAG]] rather than a tree. This means that a "chunk" could be represented in multiple places in the tree, if two different traversal paths ended up referring to the same chunk. A DAG is essentially the result of applying transclusion to the tree model. This is also true of NoteCards. NoteCards&lt;ref name="Halasz et al 1987" /&gt; offered a similar mechanism, using "FileBoxes" as the tree component that was overlaid upon the semantic network of notecards.

Brown University's IGD project explored various ways to combine and display unrestricted graphs with hierarchy, and used a visual metaphor of spatial containment to convey both graph and tree structure.&lt;ref name="Feiner 1988" /&gt; Their notion of "link inheritance" simplifies the way in which complex dual structures are displayed while still faithfully depicting their overall trends. Commercially, both [[PersonalBrain]] and Multicentrix&lt;ref name="Koy 1997" /&gt; provide explicit support for parent/child relationships in addition to arbitrary connections between elements, allowing tree and graph notions to coexist.  Some note-taking tools, while essentially tree-based, also permit crosslinks between notes.

==== Spatial ====
Some designers have shunned links between elements altogether, favoring instead spatial positioning as the sole organizational paradigm. Capitalizing on the human's tendency to implicitly organize through clustering, making piles, and spatially arranging, some tools offer a 2D workspace for placing and grouping items. This provides a less formal (and perhaps less intimidating) way for a user to gradually introduce structure into a set of items as it is discovered.

This approach originated from the spatial hypertext community, demonstrated in various projects,&lt;ref name="diSessa and Abelson 1986" /&gt; and VIKI/VKB&lt;ref name="Marshall and Shipman 1995" /&gt;&lt;ref name="Shipman et al 2000" /&gt; With these programs, users place information items on a canvas and can manipulate them to convey organization imprecisely. Some project&lt;ref name="Marshall and Shipman 1995" /&gt; could infer the structure from a user's freeform layout: a spatial parser examines which items have been clustered together, colored or otherwise adorned similarly, etc., and makes judgments about how to turn these observations into machine-processible assertions. While others (Pad&lt;ref name="Perlin and Fox 1993" /&gt;) allowed users to view different objects in varying levels of detail as they panned around the workspace.

Certain note-taking tools&lt;ref name="Burger et al 1991" /&gt;&lt;ref name="Akscyn et al 1987" /&gt; combine an overarching tree structure with spatial freedom on each "frame" or "page". Users can access a particular page of the notebook with basic search or tree navigation facilities, and then lay out notes and images on the page as desired. Many graph-based approaches (such as concept mapping tools) also allow for arbitrary spatial positioning of elements. This allows both kinds of relationships to be expressed: explicit links and less formal expression through creative use of the screen.

==== Categories ====

In category-based structural frameworks, rather than being described in terms of their relationships to other elements (as with a tree or graph), items are simply grouped together in one or more categories, indicating that they have something in common. This scheme is based on the branch of pure mathematics called [[set theory]], in which each of a body of objects either has, or does not have, membership in each of some number of sets. There is normally no restriction as to how many different categories a given item can belong to, as is the case with mathematical sets.

Users may think of categories as collections, in which the category somehow encloses or "owns" the items within it. Indeed, some systems depict categories in this fashion, such as the Vista interface&lt;ref name="Dourish et al 1999" /&gt; where icons standing for documents are enclosed within ovals that represent categories. This is merely a convention of display, however, and fundamentally, categories are the same as simple keywords.

The most popular application to embrace the category approach was the original [[Lotus Agenda|Agenda]].&lt;ref name="Kaplan et al 1990" /&gt; All information retrieval in Agenda was performed in terms of category membership. Users specified queries that were lists of categories to include (or exclude), and only items that satisfied those criteria were displayed. Agenda was particularly sophisticated in that the categories themselves formed a tree hierarchy, rather than a flat namespace. Assigning an item to a category also implicitly assigned it to all ancestors in the hierarchy.

[[Personal Knowbase]] is a more modern commercial product based solely on a keyword (category) paradigm, though it uses a simple flat keyword structure rather than an inheritance hierarchy like Agenda. [[Haystack (MIT project)|Haystack]]&lt;ref name="Adar et al 1999" /&gt; and [[Open Source Applications Foundation|Chandler]] are other information management tools which use categorization in important ways. William Jones' Memory Extender&lt;ref name="Jones 1986" /&gt; took an artificial intelligence twist on the whole notion of keywords/categories, by allowing an item's keywords to be weighted, and adjusted over time by both the user and the system. This allowed the strength of category membership to vary dynamically for each of an item's assignments, in an attempt to yield more precise retrieval.

==== Chronological ====

Yale University's Lifestreams project&lt;ref name="Fertig et al 1996" /&gt; used timestamps as the principal means of organization and retrieval of personal documents. In Fertig et al.'s own words:

&lt;blockquote&gt;A [[lifestreaming|lifestream]] is a time-ordered stream of documents that functions as a diary of your electronic life; every document you create is stored in your lifestream, as are the documents other people send you. The tail of your stream contains documents from the past, perhaps starting with your electronic birth certificate.  Moving away from the tail and toward the present, your stream contains more recent documents such as papers in progress or the latest electronic mail you've received...&lt;/blockquote&gt;

Documents are thus always ordered and accessed chronologically. Metadata-based queries on the collection produce "substreams," or chronologically ordered subsets of the original documents. The rationale for time-based ordering is that "time is a natural guide to experience; it is the attribute that comes closest to a universal skeleton-key for stored experience".&lt;ref name="Freeman and Gelernter 1996" /&gt; Whether chronology is our principal or even a common natural coding mechanism psychologically can be debated. But since any PKB system can easily create such an index, it seems worthwhile to follow Lifestreams' lead and allow the user to sort and retrieve based on time, as many systems have done. If nothing else, it relieves the user from having to create names for knowledge elements, since the timestamp is always an implicit identifying mark. PlanPlus, based on the Franklin-Covey planner system, is also chronologically modeled, and a number of products based on other data models&lt;ref name="Kaplan et al 1990" /&gt; offer chronological indexing in addition to their core paradigm.

==== Aquanet's framework ====

Though advertised as a hypertext system, Marshall ''et al.'''s Aquanet&lt;ref name="Marshall et al 1991" /&gt; went far beyond the traditional node-link graph model.  Knowledge expressed in Aquanet is centered around "relations," or n-ary links between objects in which the semantics of each participant in the relation is specified by the relation type. Each type of relation specifies a physical display (i.e., how it will be drawn on the screen, and the spatial positioning of each of its participants), and a number of "slots" into which participants can be plugged. Each participant in a relation can be either a base object, or another relation. Users can thus define a schema of relation types, and then build a complex semantic model out of relations and objects. Since relation types can be specified to associate any number of nodes (instead of just two, as in the graph model), this potentially allows more complex relationships to be expressed.

It should be noted, however, that the same effect can be achieved in the basic graph model by simply taking the n-ary relations and "reifying" them (i.e., turning them into nodes in their own right.) For instance, suppose we define a relation type "assassination," with slot types of "assassin," "victim," "location," and "weapon". We could then create a relation based on this type where the participants are "John Wilkes Booth," "Abraham Lincoln," "Ford's Theatre," and "derringer". This allows us to express a complex relationship between multiple objects in Aquanet. But we can express the same knowledge with the basic graph model by simply creating a node called "Lincoln's assassination" and then creating typed links between that node and the other four labeled "assassin," "victim," etc. Aquanet's biggest achievement in this area is the ability to express the schema of relation types, so that the types of objects an "assassination" relation can connect are consistent and enforced.

=== Knowledge elements ===

There are several options for specifying what knowledge elements consist of, and what kind of internal structure, if any, they possess:

# '''Word/phrase/concept'''.  Most systems engineered for knowledge representation encourage structures to be composed of very simple elements, usually words or phrases.  This is in the spirit of both mind mapping and concept mapping, where users are encouraged to use simple phrases to stand for mental concepts.
# '''Free text notes'''. Nearly all systems permit large amounts of free text to exist in the PKB, either as the contents of the elements themselves (NoteCards,&lt;ref name="Halasz et al 1987" /&gt; Hypercard,&lt;ref name="Goodman 1988" /&gt; TreePad) or attached to elements as separate, supplementary pages (Agenda,&lt;ref name="Kaplan et al 1990" /&gt; Zoot, HogBay).
# '''Links to an information space'''.  Since a user's knowledge base is to correspond to her mental perceptions, it seems profitable for the PKB to point to entities in the information space from which she formed those perceptions.  Many systems do in fact allow their knowledge elements to point to the original sources in some way. There are three common techniques:
##The knowledge element actually ''represents'' an original source.  This is the case for document management systems (WebTop,&lt;ref name="Wolber et al 2002" /&gt; MyLifeBits,&lt;ref name="Gemmell et al 2002" /&gt; Haystack&lt;ref name="Adar et al 1999" /&gt;), integrated search facilities (NaviQue,&lt;ref name="Furnas and Rauch 1998" /&gt; CYCLADES&lt;ref name="Renda and Straccia 2005" /&gt;), VIKI/VKB.&lt;ref name="Marshall and Shipman 1995" /&gt;&lt;ref name="Shipman et al 2000" /&gt;  Tinderbox&lt;ref name="Bernstein 2003" /&gt; will also allow one of its notes to be a URL, and the user can control whether its contents should be captured once, or "auto-fetched" as to receive constant web updates.  Many systems, in addition to storing a page of free text for each knowledge element, also permit any number of hyperlinks to be attached to a knowledge element (e.g., [[FreeMind|Freemind]], [[PersonalBrain]], Inspiration). VNS,&lt;ref name="Burger et al  1991" /&gt; which allows users to point to a community notebook page from within their personal notebook, gives similar functionality.
## The knowledge element is a repurposed snippet from an original source. This is potentially the most powerful form, but is rare among fully featured PKB systems.  Cartagio, Hunter-Gatherer,&lt;ref name="Schraefel et al 2002" /&gt; and YellowPen all allow Web page excerpts to be assimilated and organized, although they primarily only do that, without allowing them to easily be combined with other subjective knowledge. DEVONThink and MyBase's WebCollect plug-in add similar functionality to their more general-purpose, tree-based information managers. Both of these systems, when a snippet is captured, archive the entire Web page locally so it can be returned to later. The user interfaces of CircusPonies and StickyBrain have been heavily optimized towards grabbing information from other applications and bringing them into the PKB without disturbing the user's workflow.
# '''Composites''' Some programs allow a user to embed knowledge elements (and perhaps other information as well) inside a knowledge element to form an implicit hierarchy. Trees by themselves fall into this category, of course, since each node in the tree can be considered a "composite" of its content and children. But a few graph-based tools offer composite functionality as well. In Aquanet,&lt;ref name="Marshall et al  1991" /&gt; "relations" form the fundamental means of connection, and the units that are plugged into a relation can be not only objects, but other relations as well. This lends a recursive quality to a user's modeling. VIKI/VKB's spatial environment offers "subspaces" which let a user partition their visual workspace into subregions, whose internal contents can be viewed at a glance from the parent. Boxer's&lt;ref name="diSessa and Abelson 1986" /&gt; paradigm is similar. Tinderbox is a graph-based tool that supports hierarchical composite structures, and [[Compendium (software)|Compendium]] extends this even further by allowing transclusion of "views" as well as of nodes. Unlike the other tools, in Compendium the composite hierarchy does not form a [[Directed acyclic graph|DAG]], but rather an arbitrary graph: view A can appear on view B, and B can in turn appear on A. The user's intuitive notion of "inside" must be adapted somewhat in this case.

=== Schema ===

In the context of PKBs, "schema" means the ability for a user to specify types and introduce structure to aspects of the data model. It is a form of metadata whereby more precise semantics can be applied to various elements of the system. This facilitates more formal knowledge expression, ensures consistency across items of the same kind, and can better allows automated agents to process the information.

Both knowledge elements, and links, can contain various aspects of schema.

==== Schema for knowledge elements ====

===== Types, and related schema =====

In a PKB, a "[[type system]]" allows users to specify that a knowledge element is a member of a specific class or category or items, to provide a built-in method of organization and retrieval. Generally speaking, systems can make knowledge elements untyped, rigidly typed, or flexibly typed. In addition, they can incorporate some notion of inheritance among elements and their types.  There is a distinction between types and categories here. A category-based scheme, typically allows any number of categories/keywords to be assigned to an item. There are two differences between this and the notion of type. First, items are normally restricted to being of a single type, and this usually indicates a more intrinsic, permanent property of an item than simply its presence in a category collection.  (For example, one could imagine an item called "XYZ Corporation" shifting into and out of categories like "competitors", "overseas distributors," or "delinquent debtors" over time, but its core type of "company" would probably be static for all time.) Second, types often carry structural specifications with them: if an item is of a given type, this means it will have values for certain attributes appropriate to that type.  Some systems that do not allow typing offer the ability to approximate this function through categories.

Untyped elements are typical among informal knowledge capture tools, since they are designed to stimulate brainstorming and help users discover their nascent mental models.  These tools normally want to avoid forcing the user to commit to structure prematurely.  Most mind mapping and many concept mapping tools are in this category: a concept is simply a word or phrase, with no other semantic information (e.g., [[Visual Mind]]). Note-taking tools also usually take this approach, with all units of information being of the same type "note".

At the other extreme are tools which, like older relational database technology, require all items to be declared as of a specific type when they are created. Often this type dictates the internal structure of the element. These tools are better suited to domains in which the structure of knowledge to be captured is predictable, well-understood, and known in advance. For PKB systems, they are probably overly restrictive. KMap&lt;ref name="Gaines and Shaw 1995" /&gt; and Compendium are examples of tools that allow (and require) each item to be typed; in their case, the type controls the visual appearance of the item, rather than any internal structure.

In between these two poles are systems that permit typed and untyped elements to co-exist. NoteTaker is such a product; it holds simple free-text pages of notes, without any structure, but also lets the user define "templates" with predefined fields that can be used to instantiate uniformly structured forms. TreePad has a similar feature. Some other systems blur the distinction between typed and untyped, allowing the graceful introduction of structure as it is discovered. VKB,&lt;ref name="Shipman et al 2000" /&gt; for example, supports an elegant, flexible typing scheme, well suited to PKBs.  Items in general consist of an arbitrary number of [[attribute&#8211;value pair]]s. But when consistent patterns emerge across a set of objects, the user can create a type for that group, and with it a list of expected attributes and default values. This structure can be selectively overridden by individual objects, however, which means that even objects assigned to a particular type have flexible customization available to them. Tinderbox offers an alternate way of achieving this flexibility, as described below.

Finally, the [[object-oriented]] notion of [[Inheritance (computer science)|type inheritance]] is available in a few solutions. The different card types in NoteCards are arranged into an inheritance hierarchy, so that new types can be created as extensions of old. Aquanet extends this to multiple inheritance among types; the "slots" that an object contains are those of its type, plus those of all supertypes. SPRINT and Tinderbox also use a frame-based approach, and allow default values for attributes to be inherited from supertypes. This way, an item need not define values for all its attributes explicitly: unless overridden, an item's slot will have the shared, default value for all items of that type.

===== Other forms of schema =====

In addition to the structure that is controlled by an item's type, other forms of metadata and schema can be applied to knowledge elements.

* '''Keywords'''. Many systems let users annotate items with user-defined keywords. Here the distinction between an item's contents and the overall knowledge structure becomes blurred, since an item keyword could be considered either a property of the item, or an organizational mechanism that groups it into a category with like items. Systems using the category data model (e.g., Agenda) can employ keywords for the latter purpose. Some systems based on other data models also use keywords to achieve category-like functionality.
* '''Attribute/value pairs'''. Arbitrary attribute/value pairs can also be attached to elements in many systems, which gives a PKB the ability to define semantic structure that can be queried.  Frame-based systems like SPRINT and Aquanet are examples, as well as NoteTaker, VKB, and Tinderbox. MindPad[AKS-Labs 2005] is notable for taking the basic concept mapping paradigm and introducing schema to it via its "model editor". As mentioned earlier, adding user-defined attribute/value pairs to the items in an outliner yields spreadsheet-like functionality, as in Ecco and [[OmniOutliner]]. Some systems feature attribute/value pairs, but only in the form of system-defined attributes, not user-defined ones.
* '''Knowledge element appearance'''. Some tools modify a knowledge element's visual appearance on the screen in order to convey meaning to the user. SMART Ideas and [[Visual Mind]] let the user freely choose each element's icon from a variety of graphics, while KMap&lt;ref name="Gaines and Shaw 1995" /&gt; ties the icon directly to its underlying type. Other graphical aspects that can be modified include color (VIKI&lt;ref name="Marshall and Shipman 1995" /&gt;), the set of attributes shown in a particular context (VKB&lt;ref name="Shipman et al 2000" /&gt;), and the spatial positioning of objects in a relation (Aquanet&lt;ref name="Marshall et al 1991" /&gt;).

==== Schema for links ====

In addition to prescribing schema for knowledge elements, many systems allow some form of information to be attached to the links that connect them.

In most of the early hypertext systems, links were unnamed and untyped, their function being merely to associate two items in an unspecified manner. The mind mapping paradigm also does not name links, but for a different reason: the implicit type of every link is one of generalization/specialization, associating a topic with a subtopic. Hence specifying types for the links would be redundant, and labeling them would clutter the diagram.

Concept mapping prescribes the naming of links, such that the precise nature of the relationship between two concepts is made clear. As mentioned above, portions of a concept map are meant to be read as English sentences, with the name of the link serving as a verb phrase connecting the two concepts. Numerous systems thus allow a word or phrase to decorate the links connecting elements.

Named links can be distinguished from ''typed'' links, however. If the text attached to a link is an arbitrary string of characters, unrelated to that of any other link, it can be considered the link name. Some systems, however, encourage the re-use of link names that the user has defined previously. In [[PersonalBrain]], for instance, before specifying the nature of a link, the user must create an appropriate "link type" (associated with a color to be used in presentation) in the system-wide database, and then assign that type to the link in question. This promotes consistency among the names chosen for links, so that the same logical relationship types will hopefully have the same tags throughout the knowledge base. This feature also facilitates searches based on link type, among other things. Other systems, especially those suited for specific domains such as decision modeling ([[gIBIS]]&lt;ref name="Conklin and Begeman 1988" /&gt; and Banxia Decision Explorer), predefine a set of link types that can be assigned (but not altered) by the user.

Some more advanced systems allow links to bear attribute/value pairs themselves, and even embedded structure, similar to those of the items they connect. In Haystack&lt;ref name="Adar et al 1999" /&gt; this is the case, since links ("ties") and nodes ("needles") are actually defined as subtypes of a common type ("straw").

KMap similarly defines a link as a subclass of node, which allows links to represent n-ary relationships between nodes, and enables recursive structure within a link itself. It is unclear how much value this adds in knowledge modeling, or how often users take advantage of such a feature. Neptune&lt;ref name="Delisle and Schwartz 1986" /&gt; and Intermedia&lt;ref name="Garrett et
al 1986" /&gt; are two older systems that also support attributes for links, albeit in a simpler manner.

Another aspect of links that generated much fervor in the early hypertext systems was that of link ''precision'': rather than merely connecting one element to another, systems like Intermedia defined anchors within documents, so that a particular snippet within a larger element could be linked to another snippet. The Dexter model&lt;ref name="Halasz and Schwartz 1994" /&gt; covers this issue in detail. For PKB purposes, this seems to be most relevant as regards links to the objective space, as discussed previously. If the PKB truly contains knowledge, expressed in appropriately fine-grained parts, then link precision between elements in the knowledge base is much less of a consideration.

This discussion on links has only considered connections between knowledge elements in the system, where the system has total control over both ends of the connection. As described in the previous section, numerous systems provide the ability to "link" from a knowledge element inside the system to some external resource: a file or a URL, say. These external links typically cannot be enhanced with any additional information, and serve only as convenient retrieval paths, rather than as aspects of knowledge representation.

== Architecture ==

The idea of a PKB gives rise to some important architectural considerations. While not constraining the nature of what knowledge can be expressed, the architecture nevertheless affects more mundane matters such as availability and workflow. But even more importantly, the system's architecture determines whether it can truly function as a lifelong, integrated knowledge store&#8212;the "base" aspect of the personal knowledge base defined above.

=== File-based ===

Traditionally, most electronic PKB systems have employed a simple storage mechanism based on flat files in a filesystem. This is true of virtually all of the mind mapping tools ([[MindManager]]), concept mapping tools, and even a number of hypertext tools (NoteCards,&lt;ref name="Halasz et al 1987" /&gt; Hypercard,&lt;ref name="Goodman 1988" /&gt; Tinderbox&lt;ref name="Bernstein 2003" /&gt;). Typically, the main "unit" of a user's knowledge design&#8212;whether that be a mind map, a concept map, an outline, or a "notebook"&#8212;is stored in its own file somewhere in the filesystem. The application can find and load such files via the familiar "File | Open..." paradigm, at which point it typically maintains the entire knowledge structure in memory.

The advantage of such a paradigm is familiarity and ease of use; the disadvantage is a possibly negative influence on knowledge formulation. Users must choose one of two basic strategies: either store all of their knowledge in a single file; or else break up their knowledge and store it across a number of different files, presumably according to subject matter and/or time period. The first choice can result in scalability problems&#8212;consider how much knowledge a user might collect over a decade, if they stored things related to their personal life, hobbies, relationships, reading materials, vacations, academic course notes, multiple work-related projects, future planning, etc. It seems unrealistic to keep adding this kind of volume to a single, ever-growing multi-gigabyte file. The other option, however, is also constraining: each bit of knowledge can be stored in only one of the files (or else redundantly, which leads to synchronization problems), and the user is forced to choose this at knowledge capture time.

=== Database-based ===

If a PKB's data is stored in a database system, then knowledge elements reside in a global space, which allows any idea to relate to any other: now a user can relate a book he read on productivity not only to other books on productivity, but also to "that hotel in Orlando that our family stayed in last spring," because that is where he remembers having read the book. Though such a relationship may seem "out of bounds" in traditional knowledge organization, it is exactly the kind of retrieval path that humans often employ in retrieving memories.&lt;ref name="Lorayne and Lucas 1974" /&gt;&lt;ref name="Anderson 1990" /&gt;&lt;ref name="Conway et al 1991" /&gt; The database architecture enables a PKB to truly form an integrated knowledge base, and contain the full range of relationships.

Agenda&lt;ref name="Kaplan et al 1990" /&gt; and [[gIBIS]]&lt;ref name="Conklin and Begeman 1988"/&gt; were two early tools that subsumed a database backend in their architecture. More recently, the MyLifeBits project&lt;ref name="Gemmell et al 2002" /&gt; uses Microsoft SQL Server as its storage layer, and [[Compendium (software)|Compendium]] interfaces with the open source MySQL database.  A few note-taking applications also store information in an integrated database rather than in user-named files. The only significant drawback to this architectural choice (other than the modest footprint of the database management system) is that data is more difficult to copy and share across systems.  This is one true advantage of files: it is a simple matter to copy them across a network, or include them as an e-mail attachment, where they can be read by the same application on a different machine. This problem is solved by some of the following architectural choices.

=== Client&#8211;server ===

Decoupling the actual knowledge store from the PKB user interface can achieve architectural flexibility. As with all client-server architectures, the benefits include load distribution, platform interoperability, data sharing, and ubiquitous availability.  Increased complexity and latency are among the liabilities, which can indeed be considerable factors in PKB design.

One of the earliest and best examples of a client-server knowledge base was the Neptune hypertext system.&lt;ref name="Delisle and Schwartz 1986" /&gt; Neptune was tailored to the task of maintaining shared information within software engineering teams, rather than to personal knowledge storage, but the elegant implementation of its "Hypertext Abstract Machine" (HAM) was a significant and relevant achievement. The HAM was a generic hypertext storage layer that provided node and link storage and maintained version history of all changes. Application layers and user interfaces were to be built on top of the HAM. Architecturally, the HAM provided distributed network access so that client applications could run from remote locations and still access the central store. Another, more recent example, is the Scholarly Ontologies Project&lt;ref name="Uren et al  2004" /&gt;&lt;ref name="Sereno et al 2005" /&gt; whose ClaiMapper and ClaiMaker components form a similar distributed solution in order to support collaboration.

These systems implemented a distributed architecture primarily in order to share data among colleagues. For PKBs, the prime motive is rather user mobility. This is a key consideration, since if a user is to store all of their knowledge into a single integrated store, they will certainly need access to it in a variety of settings.  MyBase Networking Edition is one example of how this might be achieved. A central server hosts the user's data, and allows network access from any client machine. Clients can view the knowledge base from within the MyBase application, or through a Web browser (with limited functionality.)

The Haystack project&lt;ref name="Adar et al 1999" /&gt; outlines a three-tiered architecture, which allows the persistent store, the Haystack data model itself, and the clients that access it to reside on separate machines. The interface to the middle tier is flexible enough that a number of different persistent storage models can be used, including relational databases, semistructured databases, and object-oriented databases. Presto's architecture&lt;ref name="Dourish et al 1999" /&gt; exhibits similar features.

==== Web-based ====

A variation of the client-server approach is Web-based systems, in which the client system consists of nothing but a (possibly enhanced) browser. This gives the same ubiquitous availability that client-server approaches do, while minimizing (or eliminating) the setup and installation required on each client machine.

KMap&lt;ref name="Gaines and Shaw 1995" /&gt; was one of the first knowledge systems to integrate with the World Wide Web. It allowed concept maps to be shared, edited, and remotely stored using the HTTP protocol. Concept maps were still created using a standalone client application for the Macintosh, but they could be uploaded to a central server, and then rendered in browsers as "clickable GIFs". Clicking on a concept within the map image in the browser window would have the same navigation effect as clicking on it locally inside the client application.  The user's knowledge expressions are stored on a central server in nearly all cases, rather than locally on the browser's machine.

=== Handheld devices ===

Lastly, mobile devices are a possible PKB architecture. Storing all of one's personal knowledge on a PDA would solve the availability problem, of course, and even more completely than would a client-server or web-based architecture.  The safety of the information is an issue, since if the device were to be lost or destroyed, the user could face irrevocable data loss; this is easily remedied, however, by periodically synchronizing the device's contents with a host computer.

Most handheld applications are simple note-taking software, with far fewer features than their desktop counterparts. BugMe! is an immensely popular note-taking tool that simply lets users enter text or scribble onto "notes" (screenfulls of space) and then organize them in primitive ways. Screen shots can be captured and included as graphics, and the tool features an array of drawing tools, clip art libraries, etc. The value add for this and similar tools is purely the size and convenience of the handheld device, not the ability to manage large amounts of information.

Perhaps the most effective use of a handheld architecture would be as a satellite data capture and retrieval utility. A user would normally employ a fully functional desktop application for personal knowledge management, but when "on the go," they could capture knowledge into a compatible handheld application and upload it to their PKB at a later convenient time. To enable mobile knowledge retrieval, either select information would need to be downloaded to the device before the user needed it, or else a wireless client-server solution could deliver any part of the PKB on demand. This is essentially the approach taken by software like KeySuite, which supplements a feature-rich desktop information management tool (e.g. [[Microsoft outlook|Microsoft Outlook]]) by providing access to that information on the mobile device.

== See also ==
* [[Commonplace book]]
* [[Lifelog]]
* [[Notetaking]]
** [[Comparison of notetaking software]]
* [[Outliner]]
* [[Personal knowledge management]]
* [[Personal wiki]]
** {{section link|List of wiki software|Personal wiki software}}
* {{section link|Tag (metadata)|Knowledge tags}}

== Notes ==
{{notelist}}

== References ==
{{reflist|30em|
refs=
&lt;!-- Converted to LDR format
     using [[User:PleaseStand/References segregator]] --&gt;

&lt;ref name = "Davies 2005"&gt;Davies, S., Velez-Morales, J. and King, R. [http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf Building the Memex sixty years later: trends and directions in personal knowledge bases]. Technical Report CU-CS-997-05. Boulder, Colorado: Department of Computer Science, University of Colorado at Boulder, August 2005.&lt;/ref&gt;

&lt;ref name = "Davies 2011"&gt;Davies, S. Still Building the Memex. ''Communications of the ACM'', vol. 53, issue 2, February 2011, 80-88.&lt;/ref&gt;

&lt;ref name = "Brooks 1985"&gt;Brooks, T. New technologies and their implications for local area networks. ''Computer Communications'', vol. 8, no. 2, 1985, 82-87.&lt;/ref&gt;

&lt;ref name = "Kruger 1986"&gt;Kr&#252;ger, G. Future information technology&#8212;motor of the "information society". in ''Employment and the Transfer of Technology''. Berlin: Springer, 1986, 39-52.&lt;/ref&gt;

&lt;ref name = "Forman 1988"&gt;Forman, G. Making intuitive knowledge explicit through future technology. in ''Constructivism in the Computer Age''. Hillsdale, New Jersey: L. Erlbaum, 1988, 83-101.&lt;/ref&gt;

&lt;ref name = "Smith 1991"&gt;Smith, C.F. Reconceiving hypertext. in ''Evolving Perspectives on Computers and Composition Studies: Questions for the 1990s''. Urbana, Illinois: National Council of Teachers of English, 1991, 224-260.&lt;/ref&gt;

&lt;ref name="Schneiderman 1987"&gt;Schneiderman, B., User interface design for the Hyperties electronic encyclopedia. in ''Proceedings of the ACM Conference on Hypertext''. Chapel Hill, North Carolina, 1987, 189-194.&lt;/ref&gt;

&lt;ref name = "Nelson 1987"&gt;Nelson, T.H. ''Literary machines: the report on, and of, Project Xanadu concerning word processing, electronic publishing, hypertext, thinkertoys, tomorrow's intellectual revolution, and certain other topics including knowledge, education and freedom''. Swarthmore, Pennsylvania: Theodor H. Nelson, 1987.&lt;/ref&gt;

&lt;ref name="Dumais et al 2003"&gt;Dumais, S.T., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R. and Robbins, D.C. Stuff I've Seen: a system for personal information retrieval and re-use. in ''Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval''. Toronto, Canada, 2003, 72-79.&lt;/ref&gt;

&lt;ref name="Nakakoji et al 2000"&gt;Nakakoji, K., Yamamoto, Y., Takada, S. and Reeves, B.N. Two-dimensional spatial positioning as a means for reflection in design. in ''Proceedings of the Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques''. New York: ACM, 2000, 145-154.&lt;/ref&gt;

&lt;ref name="Smith et al 1987"&gt;Smith, J.B., Weiss, S.F. and Ferguson, G.J. A hypertext writing environment and its cognitive basis. in ''Proceedings of the ACM Conference on Hypertext''. Chapel Hill, North Carolina, 1987, 195-214.&lt;/ref&gt;

&lt;ref name="Godwin-Jones 2003"&gt;Godwin-Jones, B. Blogs and wikis: environments for on-line collaboration. ''Language Learning and Technology'', vol. 7, no. 2 (May 2003), 12-16.&lt;/ref&gt;

&lt;ref name="Kovalainen et al 1998"&gt;Kovalainen, M., Robinson, M. and Auramaki, E. Diaries at work. in ''Proceedings of the 1998 ACM Conference on Computer Supported Collaborative Work'', Seattle, Washington, 1998, 49-58.&lt;/ref&gt;

&lt;ref name="Blandford and Green 2001"&gt;Blandford, A.E. and [[Thomas R.G. Green|Green, T.R.G.]]. Group and individual time management tools: what you get is not what you need. ''Personal and Ubiquitous Computing'', vol. 5, no. 4 (December 2001), 213-230.&lt;/ref&gt;

&lt;ref name="Palen 1999"&gt;Palen, L. Social, individual and technological issues for groupware calendar systems. In ''Proceedings of the SIGCHI conference on Human Factors in Computing Systems'', pp. 17-24. ACM, 1999.&lt;/ref&gt;

&lt;ref name="Bush 1945"&gt;Bush, V. As we may think. ''The Atlantic Monthly'', July 1945, 101-108.&lt;/ref&gt;

&lt;ref name="Novak 2003"&gt;Novak, J.D. The theory underlying concept maps and how to construct them. Institute for Human and Machine Cognition, University of West Florida, 2003.&lt;/ref&gt;

&lt;ref name="Ausubel 1968"&gt;Ausubel, D.P. ''Educational Psychology: A Cognitive View''. New York: Holt, Rinehart, and Winston, 1968&lt;/ref&gt;

&lt;ref name="Canas et al 2005"&gt;Ca&#241;as, A.J., Hill, G., Carff, R., Suri, N., Lott, J., Gomez, G., Eskridge, T.C., Arroyo, M. and Carvajal, R. CmapTools: a knowledge modeling and sharing environment. in ''Proceedings of the First International Conference on Concept Mapping'', Pamplona, Spain, 2005, 125-133&lt;/ref&gt;

&lt;ref name="Woods 1985"&gt;Woods, W.A. What's in a Link: Foundations for Semantic Networks. in Brachman, R.J. and Levesque, J. eds. ''Readings in Knowledge Representation'', Morgan Kaufmann, 1985.&lt;/ref&gt;

&lt;ref name="Goodman 1988"&gt;Goodman, D. ''The Complete Hypercard Handbook''. New York: Bantam Books, 1988.&lt;/ref&gt;

&lt;ref name="Garrett et al 1986"&gt;Garrett, L.N., Smith, K.E. and Meyrowitz, N. Intermedia: Issues, strategies, and tactics in the design of a hypermedia document system. in ''Proceedings of the Conference on Computer-Supported Cooperative Work'', 1986, 163-174.&lt;/ref&gt;

&lt;ref name="Davis et al 1993"&gt;Davis, H., Hall, W., Heath, I., Hill, G. and Wilkins, R. MICROCOSM: an open hypermedia environment for information integration. in ''Proceedings of the INTERCHI Conference on Human Factors in Computing Systems''. ACM Press, 1993.&lt;/ref&gt;

&lt;ref name="Pearl 1989"&gt;Pearl, A. Sun's Link Service: a protocol for open linking. in ''Proceedings of the Second Annual ACM Conference on Hypertext'', Pittsburgh, Pennsylvania, 1989, 137-146.&lt;/ref&gt;

&lt;ref name="Engelbart 1953"&gt;Engelbart, D.C. A conceptual framework for the augmentation of man's intellect. in Howerton, P.W. ed. ''Vistas in Information Handling'', Spartan Books, Washington, D.C., 1963, 1-29.&lt;/ref&gt;

&lt;ref name="Trigg and Weiser 1986"&gt;Trigg, R.H. and Weiser, M. TEXTNET: a network-based approach to text handling. ''ACM Transactions on Information Systems'', vol. 4, no. 1, 1986, 1-23.&lt;/ref&gt;

&lt;ref name="Halasz et al 1987"&gt;Halasz, F.G., Moran, T.P. and Trigg, R.H. NoteCards in a Nutshell. ''ACM SIGCHI Bulletin'', 17, 1986, 45-52.&lt;/ref&gt;

&lt;ref name="Conklin and Begeman 1988"&gt;Conklin, J. and Begeman, M.L. gIBIS: a hypertext tool for exploratory policy discussion. in ''Proceedings of the 1988 ACM Conference on Computer-supported Cooperative Work'', Portland, Oregon, 1988, 140-152.&lt;/ref&gt;

&lt;ref name="Dede and Jayaram 1990"&gt;Dede, C.J. and Jayaram, G. Designing a training tool for imaging mental models. Air Force Human Resources Laboratory, Brooks Air Force Base, Texas, 1990.&lt;/ref&gt;

&lt;ref name="Marshall et al 1991"&gt;Marshall, C., Halasz, F.G., Rogers, R.A. and Janssen, W.C. Aquanet: a hypertext took to hold your knowledge in place. in ''Proceedings of the Third Annual ACM Conference on Hypertext'', San Antonio, Texas, 1991, 261-275.&lt;/ref&gt;

&lt;ref name="Carlson and Ram 1990"&gt;Carlson, D.A. and Ram, S. HyperIntelligence: the next frontier. ''Communications of the ACM'', vol. 33, no. 3, 1990, 311-321.&lt;/ref&gt;

&lt;ref name="Bernstein 2003"&gt;Bernstein, M. Collages, composites, construction. in ''Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia'', Nottingham, UK, August 2003, 121-123.&lt;/ref&gt;

&lt;ref name="Kaplan et al 1990"&gt;Kaplan, S.J., Kapor, M.D., Belove, E.J., Landsman, R.A. and Drake, T.R. Agenda: a personal information manager. ''Communications of the ACM'', vol. 33, no. 7, 1990, 105-116.&lt;/ref&gt;

&lt;ref name="Burger et al 1991"&gt;Burger, A.M., Meyer, B.D., Jung, C.P. and Long, K.B. The virtual notebook system. in ''Proceedings of the Third Annual ACM Conference on Hypertext'', San Antonio, Texas, 1991, 395-401.&lt;/ref&gt;

&lt;ref name="Schraefel et al 2002"&gt;Schraefel, M.C., Zhu, Y., Modjeska, D., Widgdor, D. and Zhao, S. Hunter Gatherer: interaction support for the creation and management of within-Webpage collections. in ''Proceedings of the Eleventh International Conference on the World Wide Web'', 2002, 172-181.&lt;/ref&gt;

&lt;ref name="Dourish et al 1999"&gt;Dourish, P., Edwards, W.K., LaMarca, A. and Salisbury, M. Presto: an experimental architecture for fluid interactive document spaces. ''ACM Transactions on Computer-Human Interaction'', 6, 2, 133-161.&lt;/ref&gt;

&lt;ref name="Jones 1986"&gt;Jones, W.P. The Memory Extender personal filing system. in ''Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'', Boston, Massachusetts, 1986, 298-305.&lt;/ref&gt;

&lt;ref name="Adar et al 1999"&gt;Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in ''Proceedings of the Eighth International Conference on Information Knowledge Management'', Kansas City, Missouri, 1999, 413-422.&lt;/ref&gt;

&lt;ref name="Wolber et al 2002"&gt;Wolber, D., Kepe, M. and Ranitovic, I. Exposing document context in the personal web. in Proceedings of the 7th International Conference on Intelligent User Interfaces, San Francisco, California, 2002, 151-158.&lt;/ref&gt;

&lt;ref name="Hendry and Harper 1997"&gt;Hendry, D.G. and Harper, D.J. An informal information-seeking environment. ''Journal of the American Society for Information Science'', vol. 48, no. 11, 1997, 1036-1048.&lt;/ref&gt;

&lt;ref name="Cousins et al 1997"&gt;Cousins, S.B., Paepcke, A., Winograd, T., Bier, E.A. and Pier, K. The digital library integrated task environment (DLITE). in ''Proceedings of the Second ACM International Conference on Digital Libraries'', Philadelphia, Pennsylvania, 1997, 142-151.&lt;/ref&gt;

&lt;ref name="Buchanan et al 2004"&gt;Buchanan, G., Blandford, A.E., Thimbleby, H. and Jones, M. Integrating information seeking and structuring: exploring the role of spatial hypertext in a digital library. in ''Proceedings of the Fifteenth ACM Conference on Hypertext and Hypermedia'', Santa Cruz, California, 2004, 225-234.&lt;/ref&gt;

&lt;ref name="Marshall and Shipman 1995"&gt;Marshall, C. and Shipman, F. Spatial hypertext: designing for change. ''Communications of the ACM'', vol. 38, no. 8, 1995, 88-97.&lt;/ref&gt;

&lt;ref name="Furnas and Rauch 1998"&gt;Furnas, G.W. and Rauch, S.J. Considerations for information environments and the NaviQue workspace. in ''Proceedings of the ACM Conference on Digital Libraries'', 1998, 79-88.&lt;/ref&gt;

&lt;ref name="Renda and Straccia 2005"&gt;Renda, M.E. and Straccia, U. A personalized collaborative digital library environment: a model and an application. ''Information Processing and Management: an International Journal'', vol. 41, no. 1, 2005, 5-21.&lt;/ref&gt;

&lt;ref name="Hayes et al 2003"&gt;Hayes, G., Pierce, J.S. and Abowd, G.D. Practices for capturing short important thoughts. in ''CHI '03 Extended Abstracts on Human Factors in Computing Systems'', Ft. Lauderdale, Florida, 2003, 904-905.&lt;/ref&gt;

&lt;ref name="Reyes-Farfan and Sanchez 2003"&gt;Reyes-Farfan, N. and Sanchez, J.A. Personal spaces in the context of OAI. in ''Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries'', 2003, 182-183.&lt;/ref&gt;

&lt;ref name="Halasz and Schwartz 1994"&gt;Halasz, F.G. and Schwartz, M. The Dexter hypertext reference model. ''Communications of the ACM'', vol. 37, no. 2, February 1994, 30-39.&lt;/ref&gt;

&lt;ref name="Adar et al 1999"&gt;Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in ''Proceedings of the Eighth International Conference on Information Knowledge Management'', Kansas City, Missouri, 1999, 413-422.&lt;/ref&gt;

&lt;ref name="Gaines and Shaw 1995"&gt;Gaines, B.R. and Shaw, M.L.G. Concept maps as hypermedia components. ''International Journal of Human Computer Studies'', vol. 43, no. 3, 1995, 323-361.&lt;/ref&gt;

&lt;ref name="Quillian 1968"&gt;Quillian, M.R. Semantic memory. in ''Semantic Information Processing'', Cambridge, Massachusetts: MIT Press, 1968, 227-270.&lt;/ref&gt;

&lt;ref name="Nosek and Roth 1990"&gt;Nosek, J.T. and Roth, I. A comparison of formal knowledge representationschemes as communication tools: predicate logic vs semantic network. ''International Journal of Man-Machine Studies'', vol. 33, no. 2, 1990, 227-239.&lt;/ref&gt;

&lt;ref name="Delisle and Schwartz 1986"&gt;Delisle, N. and Schwartz, M. Neptune: a hypertext system for CAD applications. in ''Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data'', Washington, D.C., 1986, 132-143.&lt;/ref&gt;

&lt;ref name="Mantei 1982"&gt;Mantei, M.M. ''Disorientation behavior in person&#8211;computer interaction''. Ph.D. thesis. Communications Department, University of Southern California, 1982.&lt;/ref&gt;

&lt;ref name="Feiner 1988"&gt;Feiner, S. Seeing the forest for the trees: hierarchical display of hypertext structure. in ''Proceedings of the ACM SIGOIS and IEEECS TC-OA 1988 Conference on Office Information Systems'', Palo Alto, California, 1988, 205-212.&lt;/ref&gt;

&lt;ref name="Koy 1997"&gt;Koy, A.K. Computer aided thinking. in ''Proceedings of the 7th International Conference on Thinking'', Singapore, 1997.&lt;/ref&gt;

&lt;ref name="Selvin 1999"&gt;Selvin, A.M. Supporting collaborative analysis and design with hypertext functionality. ''Journal of Digital Information'', vol. 1, no. 4, 1999.&lt;/ref&gt;

&lt;ref name="Di Giacomo et al 2001"&gt;Di Giacomo, M., Mahoney, D., Bollen, J., Monroy-Hernandez, A. and Meraz, C.M.R. MyLibrary, a personalization service for digital library environments. In ''DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries'', June 2001.&lt;/ref&gt;

&lt;ref name="Akscyn et al 1987"&gt;Akscyn, R., McCracken, D. and Yoder, E. KMS: a distributed hypermedia system for managing knowledge in organizations. in ''Proceedings of the ACM Conference on Hypertext'', Chapel Hill, North Carolina, 1987.&lt;/ref&gt;

&lt;ref name="Fertig et al 1996"&gt;Fertig, S., Freeman, E. and Gelernter, D. Lifestreams: An alternative to the desktop metaphor. in ''Proceedings of the Conference on Human Factors in Computing Systems (CHI96)'', Vancouver, British Columbia, 1996, 410-411.&lt;/ref&gt;

&lt;ref name="Freeman and Gelernter 1996"&gt;Freeman, E. and Gelernter, D. Lifestreams: a storage model for personal data. ACM SIGMOD Record, 25, 1.(March 1996), 80-86.&lt;/ref&gt;

&lt;ref name="Gemmell et al 2002"&gt;Gemmell, J., Bell, G., Lueder, R., Drucker, S. and Wong, C. MyLifebits: Fulfilling the Memex vision. in ''Proceedings of the 2002 ACM Workshops on Multimedia'', 2002, 235-238.&lt;/ref&gt;

&lt;ref name="Shipman et al 2000"&gt;Shipman, F., Hsieh, H. and Airhart, R. Analytic workspaces: supporting the emergence of interpretation in the Visual Knowledge Builder. Department of Computer Science and Center for the Study of Digital Libraries, Texas A&amp;M University, 2000.&lt;/ref&gt;

&lt;ref name="diSessa and Abelson 1986"&gt;diSessa, A.A. and Abelson, H. Boxer: a reconstructible computational medium. ''Communications of the ACM'', vol. 29, no. 9, 1986, 859-868.&lt;/ref&gt;

&lt;ref name="Anderson 1990"&gt;Anderson, J.R. ''Cognitive Psychology and Its Implications'', 3rd Ed. New York: W.H. Freeman, 1990.&lt;/ref&gt;

&lt;ref name="Lorayne and Lucas 1974"&gt;Lorayne, H. and Lucas, J. ''The Memory Book''. New York: Stein and Day, 1974.&lt;/ref&gt;

&lt;ref name="Conway et al 1991"&gt;Conway, M.A., Kahney, H., Bruce, K. and Duce, H. Imaging objects, routines, and locations. in Logie, R.H. and Denis, M. eds. ''Mental Images in Human Cognition'', New York: Elsevier Science Publishing, 1991, 171-182.&lt;/ref&gt;

&lt;ref name="Uren et al 2004"&gt;Uren, V., Buckingham Shum, S., Li, G. and Bachler, M. Sensemaking tools for understanding research literatures: design, implementation, and user evaluation. Knowledge Media Institute, The Open University, 2004, 1-42.&lt;/ref&gt;

&lt;ref name="Sereno et al 2005"&gt;Sereno, B., Buckingham Shum, S. and Motta, E. ClaimSpotter: an environment to support sensemaking with knowledge triples. in ''Proceedings of the International Conference on Intelligent User Interfaces'', San Diego, California, 2005, 1999-1206.&lt;/ref&gt;

&lt;ref name="Perlin and Fox 1993"&gt;Perlin, K. and Fox, D. Pad: an alternative approach to the computer interface. in ''Proceedings of the 20th annual conference on computer graphics and interactive techniques'', 1993, 57-64.&lt;/ref&gt;
}}

{{Computable knowledge}}

[[Category:Knowledge representation]]</text>
      <sha1>8mch4u581kf0ces0ztyrx9a8t7xxu8l</sha1>
    </revision>
  </page>
  <page>
    <title>GermaNet</title>
    <ns>0</ns>
    <id>33768132</id>
    <revision>
      <id>745608935</id>
      <parentid>663113959</parentid>
      <timestamp>2016-10-22T05:10:40Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4768" xml:space="preserve">{{primary sources|date=November 2011}}
'''GermaNet''' is a lexical-semantic net for the [[German language]] that relates [[noun]]s, [[verb]]s, and [[adjective]]s semantically by grouping lexical units that express the same concept into ''[[synset]]s'' and by defining [[semantic]] relations between these synsets.&lt;ref name="Storjohann2010"&gt;{{cite book|author=Petra Storjohann|title=Lexical-semantic relations: theoretical and practical perspectives|url=https://books.google.com/books?id=OYBWObJ547AC&amp;pg=PA165|accessdate=16 November 2011|date=23 June 2010|publisher=John Benjamins Publishing Company|isbn=978-90-272-3138-3|pages=165&#8211;}}&lt;/ref&gt; GermaNet has much in common with the English [[WordNet]] and can be viewed as an on-line [[thesaurus]] or a light-weight [[ontology (information science)|ontology]]. GermaNet has been developed and maintained within various projects at the research group for General and Computational Linguistics, [[University of T&#252;bingen]] since 1997. It has been integrated into the [[EuroWordNet]], a multilingual lexical-semantic database.&lt;ref name="homepage"&gt;[http://www.sfs.uni-tuebingen.de/lsd/index.shtml GermaNet homepage]&lt;/ref&gt;

==Database==

===Contents===
GermaNet  partitions the lexical space into a set of concepts that are interlinked by semantic relations. A semantic concept is modeled by a ''[[synset]]''. A synset is a set of words (called lexical units) where all the words are taken to have (almost) the same meaning. Thus a synset is a set-representation of the semantic relation of synonymy, which means that it consists of a list of lexical units and a definition (paraphrase). The lexical units in turn have frames (which specify syntactic valence) and examples of their use.&lt;ref name="GernEdiT"&gt;V. Henrich, E. Hinrichs. 2010. [http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf GernEdiT - The GermaNet Editing Tool]. In: ''Proceedings of the Seventh Conference on International Language Resources and Evaluation''.&lt;/ref&gt;
Just as in WordNet, for each word category the semantic space is divided into a number of [[semantic field]]s closely related to major nodes in the semantic network: ''Ort'', or "location", ''K&#246;rper'', or "body", etc.&lt;ref name="homepage"/&gt;

The following is an up-to-date statistics of GermaNet's version 6.0 contents (release April 2011):
 
*Number of synsets: 69594
**Of which adjectives: 5991
**Of which nouns: 53753
**Of which verbs: 9850
*Number of lexical units: 93407
**Of which adjectives: 8582
**Of which nouns: 71844
**Of which verbs: 12981 &lt;ref name="homepage"/&gt;

===Format===
All GermaNet data is stored in a relational [[PostgreSQL]] 5 database. The database model follows the internal structure of GermaNet: there are tables to store synsets, lexical units, conceptual and lexical relations, etc.&lt;ref name="GernEdiT"/&gt; The distribution format of all GermaNet data is [[XML]]. The two types of files, one for synsets and the other for relations, represent all data that is available in the GermaNet database.

==Interfaces==
There are several [[Application Programming Interface]]s (API) available for [[Java (programming language)|Java]]&lt;ref name="api"&gt;[http://www.sfs.uni-tuebingen.de/lsd/tools.shtml GermaNet APIs in Java]&lt;/ref&gt; and for [[Perl]]. These APIs are distributed freely and provide easy access to all information in various versions of GermaNet.

==Licenses==
GermaNet 6.0 (released April 2011) can be distributed under one of the following types of [[software license agreement|license agreements]]: ''Academic Research Agreement'', ''Research and Development Agreement'', or ''Commercial Agreement''. GermaNet is free for academic use.

==Applications==
GermaNet has been used for a variety of applications, including semantic analysis, shallow recognition of implicit document structure, compound analysis;&lt;ref&gt;Manuela Kunze and Dietmar R&#246;sner. 2004. Issues in Exploiting GermaNet as a Resource in Real Applications.&lt;/ref&gt; for analyzing selectional preferences,&lt;ref&gt;Sabine Schulte im Walde, 2004. GermaNet Synsets as Selectional Preferences in Semantic Verb Clustering.&lt;/ref&gt; for word sense disambiguation,&lt;ref&gt;Saito et al., 2002. Evaluation of GermanNet: Problems Using GermaNet for Automatic Word Sense Disambiguation.&lt;/ref&gt; etc.
== See also==
* [[Hyponym]]
* [[Is-a]]
* [[Machine-readable dictionary]]
* [[Ontology (information science)]]
* [[Semantic network]]
* [[Semantic Web]]
* [[Synonym Ring]]
* [[Taxonomy (general)|Taxonomy]]
* [[ThoughtTreasure]]
* [[UBY-LMF]]
* [[Word sense disambiguation]]

==References==
{{Reflist}}

{{Authority control}}
[[Category:German language]]
[[Category:Thesauri]]
[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Online dictionaries]]</text>
      <sha1>hwob5wucbvcfg0nomqpdp24gzznin6c</sha1>
    </revision>
  </page>
  <page>
    <title>Template:InfoMaps</title>
    <ns>10</ns>
    <id>36485231</id>
    <revision>
      <id>761261938</id>
      <parentid>761261767</parentid>
      <timestamp>2017-01-21T23:54:29Z</timestamp>
      <contributor>
        <username>Merbst</username>
        <id>296729</id>
      </contributor>
      <minor />
      <comment>I realised this is case sensitive.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1748" xml:space="preserve">{{Sidebar
|name     = InfoMaps
|topimage = [[Image:Screen_Shot_2012-07-19_at_5.56.57_PM.png |165px|Part of "School of Athens" by Raphael (Raffaelo Sanzio, 1483-1520)]]
|title    = [[Information mapping]]
|bodyclass = hlist
|titleclass= navbox-title
|headingstyle = background:transparent;

|heading1 = Topics &amp; fields
|content1style = padding-bottom:0.9em;
|content1 = 
* [[Business decision mapping]]  
* [[Cognitive map]]  
* [[Data visualization]]  
* [[Decision tree]] 
* [[Educational psychology]] 
* [[Educational technology]] 
* [[Graphic communication]] 
* [[Information design]]  
* [[Information graphics]]  
* [[Interactive visualization]]  
* [[Knowledge visualization]] 
* [[Mental model]]  
* [[Morphological analysis (problem-solving)|Morphological analysis]]  
* [[Visual analytics]]  
* [[Visual language]]

|heading2 = Tree-like approaches
|content2style = padding-bottom:0.9em;
|content2 = 
* [[Cladistics]]  
* [[Argument map]] 
* [[Cognitive map]]
* [[Concept lattice]] 
* [[Concept map]]ping 
* [[Conceptual graph]] 
* [[Dendrogram]]  
* [[Graph drawing]]  
* [[Hyperbolic tree]]  
* [[Layered graph drawing]]  
* [[Mental model]]  
* [[Mind map]]ping 
* [[Object-role modeling]] 
* [[Organizational chart]]  
* [[Radial tree]] 
* [[Semantic network]] 
* [[Sociogram]]  
* [[Timeline]]   
* [[Topic Maps]]  
* [[Tree structure]]   

|heading3 =See also
|content3style = padding-bottom:0.9em;
|content3 =
* [[Diagrammatic reasoning]]
* [[Entity-relationship model]]
* [[Geovisualization]]  
* [[List of concept- and mind-mapping software]]  
* [[Olog]]  
* [[Semantic web]]  
* [[Treemapping]]  
* [[Wicked problem]]  

|tnavbarstyle = border-top:1px solid #aaa;
}}&lt;noinclude&gt;
[[Category:Knowledge representation]]
&lt;/noinclude&gt;</text>
      <sha1>4efzfk10r4ehu9474xv55wxddvanp56</sha1>
    </revision>
  </page>
  <page>
    <title>Prezi</title>
    <ns>0</ns>
    <id>23948922</id>
    <revision>
      <id>759572827</id>
      <parentid>758173897</parentid>
      <timestamp>2017-01-12T00:04:13Z</timestamp>
      <contributor>
        <ip>128.177.170.230</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13712" xml:space="preserve">{{Infobox Website
| name           = Prezi
| logo           = [[File:Prezi logo transparent 2012.svg|frameless|150px]]
| url            = {{url|https://www.prezi.com/}}
| origin         = [[Hungary]]
| founder        = [[Adam Somlai-Fischer]]&lt;br&gt;Peter Halacsy&lt;br&gt;[[Peter Arvai]]
| language       = [[English language|English]], [[Portuguese language|Portuguese]], [[Spanish language|Spanish]], [[Korean language|Korean]], [[Japanese language|Japanese]], [[German language|German]], [[Italian language|Italian]], [[French language|French]], [[Hungarian language|Hungarian]]
| type           = [[Presentation Software|Presentation]] [[Collaboration tool|Collaboration]]
| launch date    = {{start date and age|2009|4|5}}
| current status = Active
}}

'''Prezi''' is a visual storytelling software alternative to traditional slide-based presentation formats. Prezi presentations feature a map-like, schematic overview that lets users pan between topics at will, zoom in on desired details, and pull back to reveal context.

This freedom of movement enables &#8220;conversational presenting,&#8221; a new presentation style in which presentations follow the flow of dialogue, instead of vice-versa.

Founded in 2009, and with offices in San Francisco, Budapest, and Mexico City, Prezi now fosters a community of over 75 million users with more than 260 million prezis around the world.

The company launched Prezi Business in 2016, with a suite of creation, collaboration, and analytics tools for teams. Prezi Business is an HTML5 application that runs on JavaScript.

The word ''Prezi'' is the short form of &#8220;presentation&#8221; in Hungarian.

== History ==

Prezi was founded in 2008 in Budapest, Hungary by Adam Somlai-Fischer, Peter Halacsy, and Peter Arvai. 

The earliest zooming presentation prototype had been previously developed by Somlai-Fischer to showcase his media-art pieces. Halacsy, an engineer, saw one of these presentations and proposed to improve the software. They were joined by entrepreneur and future CEO Arvai with the goal of making Prezi a globally recognized SaaS company.

The company established incorporation on May 20, 2009 and received its first major investment from TED two months later. A San Francisco office was opened that December. 

Early 2011 saw the launch of Prezi&#8217;s first iPad application, followed by $14M in Series B funding led by Accel Partners. A Prezi iPhone app was launched in late 2012. 

In March of 2014, Prezi pledged $100M in free licenses to Title 1 schools as part of the Obama administration&#8217;s ConnectED program. November of that year saw the announcement of $57M in new funding from Spectrum Equity and Accel Partners. 

Prezi for Android was launched in 2015, and in June of 2016, the company launched Prezi Business. As of June 2, 2016, Prezi reports 75 million registered users and 1 billion &#8216;prezi&#8217; presentation views worldwide.

== Products and features ==
[[File:Path Tool.png|thumb|Prezi Path Tool]]

=== Prezi ZUI ===
The Prezi online and offline ZUI editors employ a common tool palette, allowing users to pan and zoom, and to size, rotate, or edit an object. The user places objects on a canvas and navigates between videos, images, texts and other presentation media. Frames allow grouping of presentation media together as a single presentation object. Paths are navigational sequences that connect presentation objects for the purposes of structuring a linear presentation.

=== Prezi Desktop ===
Prezi Desktop&lt;ref&gt;{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-06-05}}&lt;/ref&gt; allows Prezi Pro or Edu Pro subscribers to work off-line and create and save their presentations on their own [[Microsoft Windows|Windows]] or [[Mac OS X|Mac]] systems. Prezi Desktop Editor allows users to work on the presentation off-line in a .pez file format. Users can have files up to 500 MB in size when signing up with a school-affiliated e-mail address. This storage capability doesn't affect when users use an appropriate third-party conversion software with [[FLV]] or [[SWF]] format.&lt;ref&gt;{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-07-23}}&lt;/ref&gt;

=== Prezi Collaborate ===
Prezi Collaborate is an online collaboration feature that allows up to ten people (co-located or geographically separated) to co-edit and show their presentations in real time. Users participate in a prezi simultaneously, and each is visually represented in the presentation window by a small avatar. Although Prezi Meetings can be done simultaneously, that is not the only option. Participants can be invited to edit the Prezi presentation at a later time if they wish. A link will be sent and the participant has up to ten days to edit the presentation. Prezi Meeting is included in all license types.

===Prezi Viewer for iPad===
Prezi Viewer&lt;ref&gt;{{cite web|url=http://prezi.com/ipad/ |title=Viewer for iPad |publisher=Prezi |date= |accessdate=2012-06-05}}&lt;/ref&gt; is an app developed for the [[iPad]] for viewing prezis created on one's Prezi online account. The iPad [[touchscreen]] and [[multi-touch]] [[user interface]] enables users to pan, and pinch to zoom in or out of their media.

Prezzip also offers templates for PreziU, with tool kits and visuals for file presentations.&lt;ref&gt;{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/prezi-ipad-viewer/|title=Prezi iPad viewer |publisher=Prezzip |accessdate=25 July 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/what-we-do/ |title=what we do |publisher=Prezzip |date= |accessdate=2012-07-25}}&lt;/ref&gt;

== Revenue model ==
Prezi uses the [[freemium]] model. Customers who use the product's Public license must publish their work on the Prezi.com website, which is publicly viewable. Customers who pay for a Prezi Enjoy or Prezi Pro can make their presentations private. Only Pro license users have access to Prezi Desktop, which enables offline editing. Prezi also offers an educational license for students and educators.

== Uses ==

=== Business and conferences ===
Some users at the [[World Economic Forum]] are currently using Prezi for their presentations.&lt;ref&gt;{{cite web|url=http://blog.prezi.com/2009/11/03/how-to-create-a-good-prezi-for-the-world-economic-forum/ |title=zoomintoprezi - Latest - How to create a good prezi - World Economic Forum |publisher=Blog.prezi.com |date=2009-11-03 |accessdate=2012-06-05}}&lt;/ref&gt; Many [[TED (conference)|TED Conference]] speakers have used Prezi, including TED curator [[Chris Anderson (entrepreneur)|Chris Anderson]], who used a Prezi for his TEDGlobal 2010 presentation: How Web Video Powers Global Innovation.&lt;ref&gt;{{cite web|author= |url=https://www.youtube.com/watch?annotation_id=annotation_962757&amp;feature=iv&amp;src_vid=X6Zo53M0lcY&amp;v=LnQcCgS7aPQ |title=Chris Anderson: How YouTube is driving innovation |publisher=YouTube |date=2010-09-14 |accessdate=2015-05-06}}&lt;/ref&gt; Michael Chasen, President/CEO of [[Blackboard, Inc.]], used Prezi to deliver the keynote at their BbWorld 2011 annual users' conference.&lt;ref&gt;{{cite web|author= |url=https://www.youtube.com/watch?v=rlGA9_p_--c |title=BbWorld 2011 Corporate Keynote |publisher=YouTube |date=2011-07-26 |accessdate=2012-06-05}}&lt;/ref&gt; [[FBLA]] members have recently started using this software.{{citation needed|reason=|date=September 2013}}

=== Education ===
Prezi is used at [[Oregon State University]],&lt;ref&gt;{{cite web|author= not fuly true. should expand further|url=http://calendar.oregonstate.edu/event/63614 |title=Prezi in the  classroom |publisher=Oregon University State University calendar |date= |accessdate=2012-07-24}}&lt;/ref&gt; as well as at the [[Dwight School]]&lt;ref&gt;{{cite news|last=Anderson |first=Jenny |url=http://cityroom.blogs.nytimes.com/2011/06/21/at-a-private-school-virtual-learning-and-the-rock/ |title=At Dwight School, Virtual Learning and the Rock - NYTimes.com |location=Manhattan (NYC) |publisher=Cityroom.blogs.nytimes.com |date=2011-06-21 |accessdate=2012-06-05}}&lt;/ref&gt; and elsewhere in primary education and higher education.&lt;ref&gt;{{cite web|author=Zoltan Radnai|url=http://edu.prezi.com/article/27827/-Prezi-makes-you-stop-and-think/|title=Prezi makes you stop and think|publisher=Prezi|accessdate=23 July 2012}}&lt;/ref&gt; It can be used by teachers and students to collaborate on presentations with multiple users able to access and edit the same presentation,&lt;ref&gt;{{cite web|author=Tilt |url=https://www.youtube.com/watch?v=lZyv6MTVsjc |title=Student Web 2.0 |publisher=YouTube |date= |accessdate=2012-07-18}}&lt;/ref&gt; and to allow students to construct and present their knowledge in different learning styles.&lt;ref&gt;{{cite web|url=http://www.nactateachers.org/attachments/article/1060/NACTA%20Journal%20Vol%2055%20Sup%201.pdf/|title=Thinking outside of slide |publisher=NACTA |accessdate=24 July 2012}}&lt;/ref&gt; The product is also being used in [[e-learning]] and [[edutainment]].&lt;ref&gt;{{cite web|author= |url=https://www.youtube.com/watch?v=s7nDT_KgPpk |title=Daniel Gallichan - 1. Platz beim 1. Freiburger Science Slam |publisher=YouTube |date= |accessdate=2012-06-05}}&lt;/ref&gt; However note that Prezi is considered by Web2Access to be an 'inaccessible service'.&lt;ref&gt;{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}&lt;/ref&gt; Educators have been advised that Prezi is not ADA/508 compliant and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.&lt;ref&gt;{{cite web|url=http://webaccessibility.gmu.edu/prezi.html |title=Prezi Known Accessibility Issues|publisher=George Mason University |date= |accessdate=2014-03-01}}&lt;/ref&gt;

=== Information visualization ===
In July 2011, ''[[The Guardian]]'' used Prezi to publish a new world map graphic on their website, for an article about the newly independent South Sudan.&lt;ref&gt;{{cite news|author=Simon Rogers, Jenny Ridley |url=https://www.theguardian.com/news/datablog/interactive/2011/jul/08/world-map-new-south-sudan |title=The new world map: download it for yourself &amp;#124; World news &amp;#124; guardian.co.uk |publisher=Guardian |date= 2011-07-08|accessdate=2012-06-05 |location=London}}&lt;/ref&gt;

== Platform compatibility ==
Prezi is developed in Adobe Flash, Adobe AIR and built on top of Django. It is compatible with most modern computers and web browsers. 

Prezi Business is an HTML5 application which runs on JavaScript. It also is compatible with most modern systems.

== Criticism ==
The company has acknowledged that the &#8220;[[zooming user interface]] (ZUI)&#8221; has the potential to induce nausea, and offers tutorials with recommendations for use of layout to avoid excessive visual stimulation.&lt;ref&gt;{{cite web|url=http://prezi.com/learn/grouping-and-layering/ |title=Why the Best Prezis use Grouping &amp; Layering &amp;#124; Prezi Learn Center |publisher=Prezi.com |date= |accessdate=2012-06-05}}&lt;/ref&gt;
There has also been criticism of Prezi&#8217;s lack of font and color options. Notably, Presentation Zen author Garr Reynolds once stated that he had never seen a good presentation using Prezi and was looking for one;&lt;ref&gt;{{cite web|url=http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |title=Have you ever seen a great talk given with the help of Prezi? Do you have a link? - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-10 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080355/http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |archivedate=April 2, 2012 }}
&lt;/ref&gt; in a later post, he refers to Chris Anderson&#8217;s talk at TED Global 2010 as one of the best TED talks ever, commenting that it was a good use of Prezi.&lt;ref&gt;
{{cite web|url=http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |title=On train to Tokyo watching one of the best TED talks ever - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-14 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080641/http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |archivedate=April 2, 2012 }}
&lt;/ref&gt;

As Prezi is a Flash-based online zooming tool, most elements of the presentation cannot be read aloud by users with disabilities by means of a screen reader (e.g. it is not possible to add [[alt attribute]]s to images and [[iframe]]s used for the page design, and templates have been built to work without [[accessibility]] options). Prezi is considered by Web2Access to be an 'inaccessible service'.&lt;ref&gt;{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}&lt;/ref&gt; American educators have been advised that Prezi is not compliant with the Americans With Disabilities Act (ADA/508) and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.&lt;ref&gt;{{cite web|url=http://barrydahl.com/2015/01/08/accessibility-concerns-of-using-prezi-in-education/|title=Accessibility Concerns of Using Prezi in Education |publisher=Barry Dahl |date= |accessdate=2015-10-07}}&lt;/ref&gt;

==See also==
* [[Scientific visualization]]
* [[Data Presentation Architecture]]

==References==
{{reflist|30em}}

==External links==
{{Commons category|Mind maps}}
* {{Official website|https://www.prezi.com}}

{{Mindmaps}}
{{Presentation software}}
{{Notetaking softwares}}

[[Category:Zoomable user interfaces]]
[[Category:Panorama viewers]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Note-taking software]]</text>
      <sha1>fgmapfwmy8vvvdokww6xvpdml1vgj8p</sha1>
    </revision>
  </page>
  <page>
    <title>Harvard&#8211;Yenching Classification</title>
    <ns>0</ns>
    <id>8311927</id>
    <revision>
      <id>730571750</id>
      <parentid>708107281</parentid>
      <timestamp>2016-07-19T21:39:47Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12315" xml:space="preserve">[[Alfred Kaiming Chiu]] &lt;ref&gt;{{zh|t=&#35032;&#38283;&#26126; | s=&#35032;&#24320;&#26126; |p=Qi&#250; K&#257;im&#237;ng|w=Ch'iu&lt;sup&gt;2&lt;/sup&gt; K'ai&lt;sup&gt;1&lt;/sup&gt;-Ming&lt;sup&gt;2&lt;/sup&gt;}}.&lt;/ref&gt; (1898&#8211;1977) was a pioneer of establishing a library classification system for Chinese language materials in the United States of America.  The system devised by him was known as '''Harvard&#8211;Yenching Classification System'''.  The system was primarily created for the classification of Chinese language materials in the [[Harvard-Yenching Library]] which was founded in 1927 at the [[Harvard-Yenching Institute]].&lt;ref&gt;Eugene W. Wu, "The Founding of the Harvard-Yenching Library," ''Journal of East Asian Libraries'' 101.1  (1993):  65-69.     [http://scholarsarchive.byu.edu/jeal/vol1993/iss101/16/]&lt;/ref&gt;

During that early period other systems, such as the early edition of the [[Library of Congress Classification]], did not consist of appropriate subject headings to classify the Chinese language materials, particularly the ancient published materials.  As many American libraries started to collect the ancient and contemporary published materials from China, a number of American libraries subsequently followed Harvard University to adopt Harvard&#8211;Yenching classification system, such as the East Asian Library of the University of California in Berkeley, Columbia University, The University of Chicago, Washington University in St. Louis etc.

In addition to American libraries, the libraries of other universities in the world including England, Australia, New Zealand, Hong Kong, Singapore etc. also followed Harvard University to adopt the system.  During the period from the 1930s to the 1970s, the use of the system became popular for classifying not only Chinese language materials but also other East Asian materials including Korean and Japanese language materials.

During the period from the 1970s to the 1980s, a comprehensive subset of subject headings for Chinese language materials was gradually established in the Library of Congress Classification System so that almost a full spectrum of ancient and contemporary Chinese topics can be widely covered. As a result of this, the Library of Congress Classification System eventually replaced the Harvard&#8211;Yenching Classification System for all Chinese language materials acquired after the 1970s in many American Libraries.

Though the system has largely been phased out, the system is still being used in some libraries for Chinese language materials acquired prior to the Library of Congress update. Such previously acquired books are normally stored in separate stacks in libraries. However, some of the university libraries in the Commonwealth countries of the United Kingdom such as England, Australia and New Zealand still continue to use the Harvard-Yenching system; for example, the Institute for Chinese Studies Library of the University of Oxford, University of Sydney, University of Melbourne, and University of Auckland.

== The Harvard&#8211;Yenching classification system ==
The key classes of the system are listed as follows:

===Key classes===
** 0100&#8211;0999 Chinese Classics
** 1000&#8211;1999 Philosophy and Religion
** 2000&#8211;3999 Historical Sciences
** 4000&#8211;4999 Social Sciences
** 5000&#8211;5999 Language and Literature
** 6000&#8211;6999 Fine and Recreative Arts
** 7000&#8211;7999 Natural Sciences
** 8000&#8211;8999 Agriculture and Technology
** 9000&#8211;9999 Generalia and Bibliography

===Subjects of sub-classes===
{{Empty section|date=January 2011}}

===0100 to 0999 Chinese Classics===
** 0100&#8211;0199 Chinese classics in general
** 0200&#8211;0299 I Ching
** 0300&#8211;0399 Shu Ching
** 0400&#8211;0499 Shih Ching
** 0500&#8211;0669 San Li
** 0680&#8211;0799 Ch&#8217;un Ch&#8217;iu
** 0800&#8211;0849 Hsiao Ching
** 0850&#8211;0999 Ssu Shu

===1000 to 1999 Philosophy and Religion===
** 1000&#8211;1008 Philosophy &amp; religion in general
** 1010&#8211;1429 Chinese philosophy
** 1470&#8211;1499 Hindu philosophy
** 1500&#8211;1539 Occident philosophy
** 1540&#8211;1569 Philosophical problems and systems
** 1570&#8211;1609 Logic
** 1610&#8211;1649 Metaphysics
** 1650&#8211;1699 Ethics
** 1700&#8211;1729 Religion in general
** 1730&#8211;1738 Mythology
** 1739&#8211;1749 Occultism numerology
** 1750&#8211;1779 History of religions
** 1780&#8211;1799 Chinese state cults
** 1800&#8211;1919 Buddhism
** 1920&#8211;1939 Taoism
** 1975&#8211;1987 Christianity
** 1988&#8211;1999 Other religions

===2000 to 3999 Historical Sciences===
** 2000&#8211;2049 Archaeology, Antiquities in general
** 2060&#8211;2159 China archaeology
** 2200&#8211;2249 Ethnology, ethnography
** 2250&#8211;2299 Genealogy and biography
** 2300&#8211;2349 World history
** 2350&#8211;2399 World geography
** 2400&#8211;2440 Asian history and geography
** 2450&#8211;2459 History of China in general
** 2461&#8211;2469 Chinese historiography
** 2470&#8211;2479 History of Chinese civilisation
** 2480&#8211;2509 Diplomatic history of China
** 2510&#8211;2519 General China history
** 2520&#8211;2533 Ancient history of China in general
** 2535 Ch&#8217;in, Han and 3 Kingdom in general
** 2536&#8211;2543 Ch&#8217;in Dynasty
** 2545&#8211;2559 Han Dynasty
** 2560&#8211;2567 The Three Kingdom
** 2570 Chin Dynasty and the Southern / Northern Dynasties
** 2571&#8211;2578 Chin Dynasty (265&#8211;420)
** 2581&#8211;2588 The Southern Dynasties
** 2590&#8211;2599 The Northern Dynasties
** 2605&#8211;2618 Sui, T&#8217;ang &amp; the Five Dynasties in general
** 2605&#8211;2619 Sui Dynasty
** 2620&#8211;2639 T&#8217;ang Dynasty
** 2640&#8211;2649 Epoch of the Five Dynasties (North)
** 2650&#8211;2660 The Ten Kingdoms (South)
** 2662 Sung, Liao, Chin and Yuan Dynasties in general
** 2665&#8211;2684 Sung Dynasty (960&#8211;1279)
** 2685&#8211;2688 The Liao Kingdom (916&#8211;1201)
** 2690 The Chin Kingdom (1115&#8211;1234)
** 2695 The [[Hsi Hsia Kingdom]] (982&#8211;1227)
** 2700&#8211;2713 Yuan Dynasty (1280&#8211;1268)
** 2718 Ming and Ching Dynasties in general
** 2720&#8211;2739 Ming Dynasty
** 2740&#8211;2969 Ch&#8217;ing Dynasty
** 2970 Period of Republic, 1912
** 3000&#8211;3019 China: geography &amp; history in general
** 3020&#8211;3031 General system treatises
** 3032&#8211;3049 Special works of geography: China
** 3507&#8211;3079 China: local description and travel
** 3080&#8211;3109 Maps, Atlas of China
** 3110&#8211;3299 Gazetteers of China
** 3300&#8211;3479 [[Japanese history]]
** 3400&#8211;3479 [[Geography of Japan|Japanese geography]]
** 3480&#8211;3489 [[Korean history]]
** 3490&#8211;3499 [[Hong Kong]], [[Macau]] history and geography
** 3500&#8211;3599 Other counties in Asia: history and geography
** 3600&#8211;3799 Europe: history and geography
** 3800&#8211;3899 America: history and geography
** 3900&#8211;3999 Africa, Oceania: history and geography

===4000 to 4999 Social Sciences===
** 4000&#8211;4019 Social sciences in general
** 4020&#8211;4099 Statistics
** 4100&#8211;4299 Sociology
** 4300&#8211;4599 Economics
** 4600&#8211;4899 Politics and Law
** 4900&#8211;4999 Education

===5000 to 5999 Language and Literature===
** 5000&#8211;5039 Linguistics in general
** 5040&#8211;5059 Literature in general
** 5060&#8211;5069 Chinese language in general
** 5070&#8211;5089 Semantic studies
** 5090&#8211;5119 Graphic studies
** 5120&#8211;5139 Phonological Studies
** 5140&#8211;5149 Grammar
** 5150&#8211;5159 Dialects
** 5160&#8211;5169 Texts: learning the language
** 5170&#8211;5199 Lexicography dictionaries
** 5200&#8211;5209 Chinese literature in general
** 5210&#8211;5217 Chinese literature: literary criticism
** 5218&#8211;5229 Chinese literature: history &amp; biography
** 5230&#8211;5235 Chinese literature: collection of individual complete works
** 5236&#8211;5241 Chinese literature: general anothlogies
** 5242&#8211;5569 Collected Chinese literart works of individual authors
** 5570&#8211;5649 Tz&#8217;u
** 5650&#8211;5730 Lyrical works and drama
** 5731&#8211;5769 Chinese Fiction
** 5770&#8211;5779 Letters
** 5780&#8211;5799 Miscellany: proverbs, fables, juv. lit.
** 5800&#8211;5809 Minor languages in China
** 5810&#8211;5859 Japanese language
** 5860&#8211;5959 Japanese literature
** 5973 Korean language and literature
** 5975&#8211;5993 Indo-European language and literature
** 5994&#8211;5999 Other language and literature

===6000 to 6999 Fine and Recreative Arts===
** 6000&#8211;6019 Fine and recreative arts in general
** 6020&#8211;6029 Aesthetics
** 6030&#8211;6069 History of arts
** 6070&#8211;6289 Chinese &amp; Japanese Calligraphy and painting
** 6290&#8211;6299 Materials &amp; instruments
** 6300&#8211;6349 Western painting
** 6350&#8211;6359 Engraving Prints
** 6360&#8211;6399 Photography
** 6400&#8211;6499 Sculpture
** 6500&#8211;6599 Architecture
** 6600&#8211;6699 Industrial arts
** 6700&#8211;6799 Music
** 6800&#8211;6899 Amusements &amp; games
** 6900&#8211;6999 Physical training &amp; sports

===7000 to 7999 Natural Sciences===
** 7000&#8211;7019 Natural science in general
** 7020&#8211;7099 [[Mathematics]]
** 7100&#8211;7199 [[Astronomy]]
** 7200&#8211;7299 [[Physics]]
** 7300&#8211;7399 [[Chemistry]]
** 7400&#8211;7499 [[Geology|Geological science]]
** 7500&#8211;7599 [[Natural history]]
** 7600&#8211;7699 [[Botany]]
** 7700&#8211;7799 [[Zoology]]
** 7800&#8211;7869 Anthropology (Physical)
** 7870&#8211;7899 Psychology
** 7900&#8211;7999 Medical science

===8000 to 8999 Agriculture and Technology===
** 8000&#8211;8009 Agriculture &amp; technology in general
** 8020&#8211;8239 Agriculture
** 8240&#8211;8289 Home economics (Domestic)
** 8290&#8211;8299 Technology in general
** 8300&#8211;8349 Handicrafts &amp; artisan trades
** 8400&#8211;8499 Manufactures
** 8500&#8211;8599 Chemical technology
** 8600&#8211;8699 Mining &amp; Metallurgy
** 8700&#8211;8899 Engineering
** 8900&#8211;8999 Military &amp; Naval science

===9000 to 9999 Generalia and Bibliography===
** 9000&#8211;9007 Generalia and bibliography in general
** 9100 Chinese general series of composite nature
** 9101&#8211;9109 Chinese general series of a special type
** 9110 Chinese series of particular locality
** 9111&#8211;9120 Chinese family &amp; individual author
** 9130&#8211;9163 Sinology
** 9164&#8211;9179 Japanese general series
** 9180&#8211;9199 Japanese individual polygraphic books
** 9200&#8211;9229 General periodicals &amp; society publications
** 9230&#8211;9289 General congresses &amp; museums
** 9290&#8211;9339 General encyclopedias and reference books
** 9401&#8211;9409 Bibliography in general
** 9410&#8211;9510 Bibliography
** 9511&#8211;9519 Subject bibliographies
** 9520&#8211;9539 Chinese collective bibliographies
** 9540&#8211;9549 Other general bibliographies of various countries
** 9550&#8211;9559 Reading lists &amp; best books, periodical index
** 9562&#8211;9569 Special bibliographies
** 9570&#8211;9579 Bibliographies of critical reviews
** 9600&#8211;9629 Library catalogues
** 9630&#8211;9639 Dealers&#8217; &amp; publishers&#8217; catalogues
** 9640&#8211;9684 Japanese bibliographies
** 9696&#8211;9699 Bibliographies of Western countries
** 9700&#8211;9929 Librarianship
** 9930&#8211;9999 Journalism, newspapers

== See also ==
The official library classification in China is:

* [[Chinese Library Classification]] (CLC)

The other library classifications for Chinese materials outside China are:
* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books drawn up by Profs. Haloun and P. van der Loon for Cambridge University, UK.
* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])

== Notes==

&lt;references /&gt;

==References==
* [http://www.lib.unimelb.edu.au/collections/asian/Harvard-Yenching.html Harvard-Yenching Classification in the University of Melbourne] - Subject headings in both Chinese and English.
* [http://research.dils.tku.edu.tw/joemls/41/41-2/139-162.pdf PDF] - Brief history of the Harvard&#8211;Yenching Classification System, and an overview of the collections of Chinese language materials in Columbia University, Cornell University, Harvard University, the Library of Congress, Princeton University and Yale University.
* [http://www.news.harvard.edu/gazette/2003/10.30/19-yenching.html Yenching, The singular history of a singular library, Ken Gewertz, Harvard News Office] - A brief history of the Harvard-Yenching Library and the Harvard&#8211;Yenching Classification System.

== External links ==
* [https://www.lib.uchicago.edu/e/easia/shelf.html Examples of the Harvard-Yenching classification system]
* [http://www.library.ucla.edu/libraries/eastasian/collect.htm East Asian Library of the University of California in Los Angeles]

{{Library classification systems}}

{{DEFAULTSORT:Harvard-Yenching Classification}}
[[Category:Library cataloging and classification]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Chinese culture]]
[[Category:Harvard University]]
[[Category:Yenching University]]</text>
      <sha1>eot4p2xptbhzvnkoqwxpbch6cs3tu03</sha1>
    </revision>
  </page>
  <page>
    <title>Designated Community</title>
    <ns>0</ns>
    <id>39348172</id>
    <revision>
      <id>572881136</id>
      <parentid>554361624</parentid>
      <timestamp>2013-09-14T12:49:16Z</timestamp>
      <contributor>
        <username>Trivialist</username>
        <id>5360838</id>
      </contributor>
      <comment>moving {{library-stub}} to bottom</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="600" xml:space="preserve">In information and archival communities, a '''Designated Community''' is an identified group of potential consumers who should be able to understand a particular set of information. These consumers may consist of multiple communities, are designated by the archive, and may change over time.&lt;ref&gt;{{cite web|title=Reference Model for an Open Archival Information System (ISO 14721:2012)|url=http://public.ccsds.org/publications/archive/650x0m2.pdf}}&lt;/ref&gt;


==References==
{{reflist}}
{{library-stub}}

[[Category:Archival science]]
[[Category:Knowledge representation]]
[[Category:Digital libraries]]</text>
      <sha1>ny3k6isb4zgs5mybbw7c8h2l9aedaf0</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise Interoperability Framework</title>
    <ns>0</ns>
    <id>43071232</id>
    <revision>
      <id>756512284</id>
      <parentid>742707670</parentid>
      <timestamp>2016-12-24T20:18:54Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6952" xml:space="preserve">{{Underlinked|date=June 2014}}
The '''Enterprise Interoperability Framework''' is used as a guideline for collecting and structuring knowledge/solution for [[enterprise interoperability]]. The Enterprise Interoperability Framework defines the domain and sub-domains for [[interoperability]] research and development in order to identify a set of pieces of knowledge for solving enterprise interoperability problems by removing barriers to interoperability.

==Existing Interoperability Frameworks==

Some existing works on interoperability have been carried out to define interoperability framework or reference models, in particular, the LISI &lt;ref name="C4ISR1998"/&gt; reference model, [[European Interoperability Framework]] (EIF),&lt;ref name="EIF2004"/&gt; IDEAS interoperability framework,&lt;ref name=" IDEAS2003"/&gt; [[Model Driven Interoperability|ATHENA]] interoperability framework,&lt;ref name="ATHENA2003"/&gt; and E-Health Interoperability Framework.&lt;ref name="NEHTA2006"/&gt; These existing approaches constitute the basis for the Enterprise Interoperability Framework.

The necessity to elaborate the Enterprise Interoperability Framework has been discussed in.&lt;ref name="INTEROP"/&gt; Existing interoperability frameworks do not explicitly address barriers to interoperability, which is a basic assumption of this research; they are not aimed at structuring interoperability knowledge with respect to their ability to remove various barriers.

The Enterprise Interoperability framework has three basic [[dimensions]]:

# Interoperability concerns defined the content (or aspect) of interoperation that may take place at various levels of the [[Business|enterprise]]. In the domain of Enterprise Interoperability, the following four interoperability concerns are identified : Data, Service, Process, and Business.&lt;ref name=" Guglielmina2005"/&gt; [[File:Interoperability Concerns Data, Service, Process, and Business.jpg|thumb|Interoperability Concerns:  Data, Service, Process, and Business]]
# Interoperability barriers: Interoperability barrier is a fundamental concept in defining the interoperability domain. Many interoperability issues are specific to particular application domains. These can be things like support for particular attributes, or particular access control regimes. Nevertheless, general barriers and problems of interoperability can be identified; and most of them being already addressed,&lt;ref name="EIF2004"/&gt;&lt;ref name=" Kasunic2004"/&gt;&lt;ref name="ERISA2004"/&gt; Consequently, the objective is to identify common barriers to interoperability. By the term &#8216;barrier&#8217; we mean an &#8216;incompatibility&#8217; or &#8216;mismatch&#8217; which obstructs the sharing and exchanging of information. Three categories of barriers are identified: conceptual, technological and organisational.
# Interoperability approaches represents the different ways in which barriers can be removed (integrated, unified, and federated) 
 [[File:Basic Approaches to Develop Interoperability.jpg|thumb|Basic Approaches to Develop Interoperability]]

The Enterprise Interoperability Framework with its three basic dimensions is shown.
[[File:Enterprise Interoperability Framework.jpg|thumb|Enterprise Interoperability Framework]]

== Enterprise Interoperability Framework Use ==

The Enterprise Interoperability Framework allows to:

* Capture and structure interoperability knowledge/solutions in the framework through a barrier-driven approach 
* Provide support to enterprise interoperability engineers and industry end users to carry out their interoperability projects.

The Enterprise Interoperability Framework not only aims at structuring concepts, defining research domain and capturing knowledge, but also at helping [[industry|industries]] to solve their interoperability problems. When carrying out an interoperability project involving two particular enterprises, interoperability concerns and interoperability barriers between the two enterprises will be identified first and mapped to this Enterprise Interoperability Framework. Using the [[Enterprise architecture framework|framework]], existing interoperability degree can be characterised and targeted interoperability degree can be defined as the [[Goal|objective]] to meet. Then knowledge/solutions associated to the barriers and concerns can be searched in the framework, and solutions found will be proposed to users for possible adaptation and/or combination with other solutions to remove the identified barriers so that the required interoperability can be established.

== References ==
{{reflist|
refs=
&lt;ref name= C4ISR1998&gt;C4ISR (1998), Architecture Working Group (AWG), Levels of Information Systems Interoperability (LISI), 30 March 1998.&lt;/ref&gt;
&lt;ref name= EIF2004&gt;EIF (2004), European Interoperability Framework for PAN-European EGovernment services, IDA working document - Version 4.2 &#8211; January 2004.&lt;/ref&gt;
&lt;ref name= IDEAS2003&gt;IDEAS (2003), IDEAS Project Deliverables (WP1-WP7), Public reports, www.ideas-road map.net.&lt;/ref&gt;
&lt;ref name= ATHENA2003 &gt;ATHENA (2003): Advanced Technologies for Interoperability of Heterogeneous Enterprise Networks and their Applications, FP6-2002-IST-1, Integrated Project Proposal, April 2003.  Deriverable.&lt;/ref&gt;
&lt;ref name=NEHTA2006&gt;NEHTA (2006), Towards a Health Interop Framework, ({{cite web|url=http://www |title=Archived copy |accessdate=2011-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20060326070849/http://www |archivedate=2006-03-26 |df= }}. providersedge.com/ehdocs/.../Towards_an_Interoperability_Framework.pdf)&lt;/ref&gt;
&lt;ref name= INTEROP &gt;INTEROP D1.1, Knowledge map of research in interoperability in the INTEROP NoE, WP1, Version 1, August 11th 2004.&lt;/ref&gt;
&lt;ref name= Guglielmina2005&gt;C. Guglielmina and A. Berre, Project A4 (Slide presentation), ATHENA Intermediate Audit 29.-30. September 2005, Athens, Greece.&lt;/ref&gt;
&lt;ref name= Kasunic2004&gt;Kasunic, M., Anderson, W.,: Measuring systems interoperability: challenges and opportunities, Software engineering measurement and analysis initiative, April 2004&lt;/ref&gt;
&lt;ref name= EIF2004&gt;EIF: European Interoperability Framework, Write Paper, Brussels, 18, Feb. 2004, http://www.comptia.org&lt;/ref&gt;
&lt;ref name= ERISA2004&gt;ERISA (The European Regional Information Society Association), A guide to Interoperability for Regional Initiatives, Brussels, September 2004.&lt;/ref&gt;
}}

== External links ==
* [http://www.interop-vlab.eu INTEROP-VLab]
* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-2-enterprise-interoperability-framework-and-knowledge-corpus/ DI.2.Enterprise Interoperability Framework and knowledge corpus]
* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-3_enterprise-interoperability-framework-and-knowledge-corpus/ DI.3.Enterprise Interoperability Framework and knowledge corpus]

[[Category:Interoperability]]
[[Category:Enterprise modelling]]
[[Category:Knowledge representation]]</text>
      <sha1>0wdfrz57tnbqvshxb3lxqc55hyhihln</sha1>
    </revision>
  </page>
  <page>
    <title>Logic Programming Associates</title>
    <ns>0</ns>
    <id>1899829</id>
    <revision>
      <id>761388029</id>
      <parentid>736093565</parentid>
      <timestamp>2017-01-22T18:42:33Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor />
      <comment>Fixed a spelling error</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4889" xml:space="preserve">{{multiple issues|
{{notability|Companies|date=January 2012}}
{{more footnotes|date=April 2013}}
}}

{{Infobox company
|name   = Logic Programming Associates Ltd
|type   = [[Private company|Private]]
|foundation = 1981
|directors= Diane Reeve &lt;br /&gt;Clive Spenser &lt;br /&gt;Brian Steel 
|location = London SW18 3SX
|area_served = UK, United States, [[Europe, the Middle East and Africa|EMEA]]
|industry             = [[Computer software]] 
|products = [[VisiRule]], [[Flex expert system|Flex expert system toolkit]], [[Flint toolkit]], LPA Prolog for Windows
|website = [http://www.lpa.co.uk www.lpa.co.uk]}}

'''Logic Programming Associates''' ('''LPA''') is a company specializing in [[logic programming]] and [[artificial intelligence]] software. LPA was founded in 1980 and is widely known for its range of [[Prolog]] compilers and more recently for [[VisiRule]].

LPA was established to exploit research at Imperial College, London into [[logic programming]] carried out under the supervision of [[Robert Kowalski|Prof Robert Kowalski]]. One of the first implementations made available by LPA was micro-PROLOG&lt;ref name = "Prolog implementations"&gt;{{citation |url=http://www.berghel.com/publications/micropro/micropro_ncc87.pdf | title= Microcomputer PROLOG implementations | accessdate=2013-04-29}}&lt;/ref&gt; which ran on popular 8-bit home computers such as the [[Sinclair Spectrum]]&lt;ref name = "micro-PROLOG for Sinclair Spectrum"&gt;{{citation |url=http://www.worldofspectrum.org/infoseekid.cgi?id=0008429 | title= micro-PROLOG for Sinclair Spectrum | accessdate=2013-04-29}}&lt;/ref&gt; and [[Apple II]]. This was followed by micro-PROLOG Professional one of the first Prolog implementations for MS-DOS.

As well as continue with Prolog compiler technology development, LPA has a track record of creating innovative associated tools and products to address specific challenges and opportunities.

In 1989, LPA developed the [[Flex expert system|Flex expert system toolkit]], which incorporated [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures. Flex has its own English-like Knowledge Specification Language (KSL) which means that knowledge and rules are defined in an easy-to-read and understand way.

In 1992, LPA helped set up the Prolog Vendors Group,&lt;ref name = "PVG launched"&gt;{{citation |url=http://iospress.metapress.com/content/c1p1351212770518/fulltext.pdf | title=Prolog Vendors Group Launched | accessdate=2013-04-29}}&lt;/ref&gt; a not-for-profit organization whose aim was to help promote Prolog by making people aware of its usage in industry.

In 2000, LPA helped set up [[Business Integrity]], now a leading supplier of document assembly and contract creation software solutions for the legal market.

LPA's core product is LPA Prolog for Windows,&lt;ref name = "WIN-PROLOG"&gt;{{citation |url=http://www.lpa.co.uk/win.htm | title= LPA Prolog for Windows | accessdate=2013-04-29}}&lt;/ref&gt; a compiler and development system for the Microsoft Windows platform. The current LPA software range comprises an integrated AI toolset which covers various aspects of [[Artificial Intelligence]] including Logic Programming, [[Expert Systems]], [[Knowledge-based Systems]], Data Mining, Agents and [[Case-based reasoning]] etc.

In 2004, LPA launched [[VisiRule]] &lt;ref name = "VisiRule"&gt;{{citation |url=http://www.lpa.co.uk/vsr.htm | title= LPA VisiRule | accessdate=2013-04-29}}&lt;/ref&gt; a graphical tool for developing knowledge-based and decision support systems. VisiRule has been used in various sectors, to build [[legal expert systems]], machine diagnostic programs, medical and financial advice systems, etc.

==Customers==
For many years, LPA has worked closely with [[Valdis Krebs]], an American-Latvian researcher, author, and consultant in the field of social and organizational network analysis. Valdis is the founder and chief scientist of Orgnet, and the creator of the popular Inflow &lt;ref name = "InFlow"&gt;[http://www.orgnet.com/inflow3.html InFlow]&lt;/ref&gt; software package.

==External links==
*[http://www.lpa.co.uk/ind_pro.htm LPA home page]
*[http://www.lpa.co.uk/abo_lpa.htm About LPA]
*[[:es:Micro-PROLOG|Micro-PROLOG (in Spanish)]]
*[http://www.teamethno-online.org.uk/Issue2/Rouchy.pdf Aspects of PROLOG History]
*[http://www.lpa.co.uk/vrs_dem.htm VisiRule demos]
*[http://dssresources.com/news/83.php VisiRule: a new graphical business rules tool from LPA]
*[http://dl.acm.org/citation.cfm?id=297981 A flex-based expert system for sewage treatment works support]
*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]

== References ==
{{Reflist}}

[[Category:Information technology organisations]]
[[Category:Software companies of the United Kingdom]]
[[Category:Expert systems]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}</text>
      <sha1>9rfel3urx7rc9ijnc7p3ucx591ttpfs</sha1>
    </revision>
  </page>
  <page>
    <title>Class (knowledge representation)</title>
    <ns>0</ns>
    <id>46926920</id>
    <revision>
      <id>666234941</id>
      <parentid>666144276</parentid>
      <timestamp>2015-06-09T20:11:30Z</timestamp>
      <contributor>
        <username>TomT0m</username>
        <id>11109968</id>
      </contributor>
      <minor />
      <comment>TomT0m moved page [[Class (Knowledge representation)]] to [[Class (knowledge representation)]]: typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2473" xml:space="preserve">{{Main|Ontology components#Classes|l1 = Classes on the ontology component page}}

In  [[Knowledge representation and reasoning|knowledge representation]], a '''class''' is a collection of individuals or objects.&lt;ref name="DLs"&gt;{{cite proceedings|url=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=110B072B7265573684AB8D4F0D6B2306?doi=10.1.1.177.2787&amp;rep=rep1&amp;type=pdf|title=Description Logics: Foundations for Class-based Knowledge Representation|author1=Diego Calvanese|author2=Giuseppe De Giacomo|author3=Maurizio Lenzerini|conference=[[Logic in Computer Science]]|year=2002}}&lt;/ref&gt; A class can be defined either by [[Extensional definition|extension]], or by [[Intensional definition|intension]], using what is called in some ontology languages like [[Web Ontology Language|OWL]]. If we follow the [[Type&#8211;token distinction]], the ontology is divided into individuals, who are real worlds objects, or events, and types, or classes, who are sets of real world objects. Class expressions or definitions gives the properties that the individuals must fulfill to be members of the class. Individuals that fulfill the property are called [[Instance (computer science)|Instances]].

== Relationships ==

=== Instantiation ===

The instantiation [[relation (mathematics)|relationship]] is a relation between objects and classes. We say that an object O, say ''Harry the eagle'' is an instance of a class, say ''Eagle''. ''Harry the eagle'' has all the properties that we can attribute to an eagle, for example his parents were eagles, he's a bird, he's a meat eater and so on. It's a special kind of [[is a]] relationship. It's noted [[Concept assertion]] (&lt;math&gt; : &lt;/math&gt;) in [[Description logic]]s, a family of logic based on classes, class assertion &lt;ref name="owlclass"&gt;{{cite web|url=http://www.w3.org/TR/owl2-syntax/#Class_Assertions|title=owl2 syntax}}&lt;/ref&gt;

=== Subsumption ===
Classes can [[is a|subsume]] each other. We say usually that if &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are classes, and all &lt;code&gt;A&lt;/code&gt; instances are also &lt;code&gt;B&lt;/code&gt; instances, then B subsumes A, or A is a subclass of B, for example in the OWL Language it's called subclassof.&lt;ref name="owlclass"/&gt;

==References==
&lt;references/&gt;

== See also ==
* [[Metaclass (Semantic Web)]]
* [[Ontology (information science)|Ontology]]
* [[Ontology components]]
* [[Description logic]]

[[Category:Knowledge representation]]
[[Category:Semantic Web Ontology]]


{{Information-science-stub}}</text>
      <sha1>fx325ir9o10aeasvogjnqra6vjcan5u</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted metadata</title>
    <ns>0</ns>
    <id>47571988</id>
    <redirect title="Faceted classification" />
    <revision>
      <id>676887756</id>
      <parentid>676887715</parentid>
      <timestamp>2015-08-19T19:51:32Z</timestamp>
      <contributor>
        <username>Andy Dingley</username>
        <id>3606755</id>
      </contributor>
      <comment>added [[Category:Knowledge representation]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="131" xml:space="preserve">#REDIRECT [[Faceted classification]]

{{R with possibilities}}

[[Category:Metadata|Faceted]]
[[Category:Knowledge representation]]</text>
      <sha1>rwa5w75ouq5qeewb4u4340a02wcmnim</sha1>
    </revision>
  </page>
  <page>
    <title>Script theory</title>
    <ns>0</ns>
    <id>18211613</id>
    <revision>
      <id>704727017</id>
      <parentid>690255203</parentid>
      <timestamp>2016-02-13T06:03:01Z</timestamp>
      <contributor>
        <username>Nyttend</username>
        <id>1960810</id>
      </contributor>
      <comment>Stray header</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3786" xml:space="preserve">'''Script theory''' is a [[Psychology|psychological]] theory which posits that [[human behaviour]] largely falls into patterns called "scripts" because they function analogously to the way a written script does, by providing a program for action. [[Silvan Tomkins]] created script theory as a further development of his [[affect theory]], which regards human beings' emotional responses to stimuli as falling into categories called "[[Affect (psychology)|affects]]": he noticed that the purely biological response of affect may be followed by awareness and by what we [[Cognition|cognitively]] do in terms of acting on that affect so that more was needed to produce a complete explanation of what he called "human being theory".

In script theory, the basic unit of analysis is called a "scene", defined as a sequence of events linked by the affects triggered during the experience of those events. Tomkins recognized that our affective experiences fall into patterns that we may group together according to criteria such as the types of persons and places involved and the degree of intensity of the effect experienced, the patterns of which constitute scripts that inform our behavior in an effort to maximize positive affect and to minimize negative affect.

== In artificial intelligence ==
[[Roger Schank]], [[Robert P. Abelson]] and their research group, extended Tomkins' scripts and used them in early artificial intelligence work as a method of representing [[procedural knowledge]]. In their work, scripts are very much like [[frame (artificial intelligence)|frames]], except the values that fill the slots must be ordered. A script is a structured representation describing a stereotyped sequence of events in a particular context. Scripts are used in natural language understanding systems to organize a knowledge base in terms of the situations that the system should understand.

The classic example of a script involves the typical sequence of events that occur when a person drinks in a restaurant:  ''finding a seat, reading the menu, ordering drinks from the waitstaff...''  In the script form, these would be decomposed into [[conceptual dependency theory|conceptual transitions]], such as '''MTRANS''' and '''PTRANS''', which refer to ''mental transitions [of information]'' and ''physical transitions [of things]''.

Schank, Abelson and their colleagues tackled some of the most difficult problems in [[artificial intelligence]] (i.e., [[story understanding]]), but ultimately their line of work ended without tangible success. This type of work received little attention after the 1980s, but it is very influential in later [[knowledge representation]] techniques, such as [[case-based reasoning]].

Scripts can be inflexible. To deal with inflexibility, smaller modules called [[memory organization packet]]s (MOP) can be combined in a way that is appropriate for the situation.{{citation needed|date=February 2012}}

== References ==
{{More footnotes|date=November 2014}}
* Nathanson, Donald L. ''Shame and Pride: Affect, Sex, and the Birth of the Self''. London: W.W. Norton, 1992 
* [[Eve Kosofsky Sedgwick|Sedgwick, Eve Kosofsky]] and Adam Frank, eds. 1995. ''Shame and Its Sisters: A Silvan Tomkins Reader''. Durham and London: Duke University Press.
* Tomkins, Silvan. "Script Theory". ''The Emergence of Personality''. Eds. Joel Arnoff, A. I. Rabin, and Robert A. Zucker. New York: Springer Publishing Company, 1987. 147&#8211;216. 
* Tomkins, Silvan. "Script Theory: Differential Magnification of Affects". Nebraska Symposium On Motivation 1978. Ed. Richard A. Deinstbier. Lincoln, NE: [[University of Nebraska Press]], 1979. 201&#8211;236.

[[Category:History of artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Psychological theories]]</text>
      <sha1>i3bbcrdk0rogc4rr9hipamp72ykd1o1</sha1>
    </revision>
  </page>
  <page>
    <title>Tree (data structure)</title>
    <ns>0</ns>
    <id>30806</id>
    <revision>
      <id>761693076</id>
      <parentid>761689495</parentid>
      <timestamp>2017-01-24T07:47:48Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>Undid revision 761689495 by [[Special:Contributions/45.127.106.179|45.127.106.179]] ([[User talk:45.127.106.179|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21632" xml:space="preserve">{{hatnote|Not to be confused with [[trie]], a specific type of tree data structure.}}
{{Refimprove|date=August 2010}}
[[File:binary tree.svg|right|192|thumb|A simple unordered tree; in this diagram, the node labeled 7 has two children, labeled 2 and 6, and one parent, labeled 2. The root node, at the top, has no parent.]]
In [[computer science]], a '''tree''' is a widely used [[abstract data type]] (ADT)&#8212;or [[data structure]] implementing this ADT&#8212;that simulates a hierarchical [[tree structure]], with a root value and [[subtrees]] of children with a parent node, represented as a set of linked [[Vertex (graph theory)|nodes]].

A tree data structure can be defined [[Recursion|recursively]] (locally) as a collection of [[node (computer science)|nodes]] (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the "children"), with the constraints that no reference is duplicated, and none points to the root.

Alternatively, a tree can be defined abstractly as a whole (globally) as an [[ordered tree]], with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a list of nodes and an [[adjacency list]] of edges between nodes, as one may represent a [[#Digraphs|digraph]], for instance). For example, looking at a tree as a whole, one can talk about "the parent node" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).

==Definition==
{| style="float:right"
| [[File:Directed graph, disjoint.svg|thumb|x100px|{{color|#800000|Not a tree}}: two non-[[Connectivity (graph theory)#Definitions of components, cuts and connectivity|connected]] parts, A&#8594;B and C&#8594;D&#8594;E. There is more than one root.]]
|}
{| style="float:right"
| [[File:Directed graph with branching SVG.svg|thumb|x100px|{{color|#800000|Not a tree}}: undirected cycle 1-2-4-3. 4 has more than one parent (inbound edge).]]
|}
{| style="float:right"
| [[File:Directed graph, cyclic.svg|thumb|x100px|{{color|#800000|Not a tree}}: cycle B&#8594;C&#8594;E&#8594;D&#8594;B. B has more than one parent (inbound edge).]]
|}
{| style="float:right"
| [[File:Graph single node.svg|thumb|x50px|{{color|#800000|Not a tree}}: cycle A&#8594;A. A is the root but it also has a parent.]]
|}
{| style="float:right"
| [[File:Directed Graph Edge.svg|thumb|x50px|Each linear list is trivially {{color|#008000|a tree}}]]
|}

A tree is a (possibly non-linear) data structure made up of nodes or vertices and edges without having any cycle. The tree with no nodes is called the '''null''' or '''empty''' tree. A tree that is not empty consists of a root node and potentially many levels of additional nodes that form a hierarchy.

==Terminology used in Trees==
{{term|Root}} {{defn|The top node in a tree.}}
{{term|Child}} {{defn|A node directly connected to another node when moving away from the Root.}}
{{term|Parent}} {{defn|The converse notion of a ''child''.}}
{{term|Siblings}} {{defn| A group of nodes with the same parent.}}
{{term|Descendant}} {{defn|A node reachable by repeated proceeding from parent to child.}}
{{term|Ancestor}} {{defn|A node reachable by repeated proceeding from child to parent.}}
{{term|Leaf}} {{term|(less commonly called External node)|multi=y}} {{defn|A node with no children.}}
{{term|Branch}} {{term|Internal node|multi=y}} {{defn|A node with at least one child.}}
{{term|Degree}} {{defn|The number of sub trees of a node.}}
{{term|Edge}} {{defn|The connection between one node and another.}}
{{term|Path}} {{defn|A sequence of nodes and edges connecting a node with a descendant.}}
{{term|Level}} {{defn|The level of a node is defined by 1 + (the number of connections between the node and the root).}}
{{term|Height of node}} {{defn|The height of a node is the number of edges on the longest path between that node and a leaf.}}
{{term|Height of tree}} {{defn|The height of a tree is the height of its root node.}}
{{term|Depth}} {{defn|The depth of a node is the number of edges from the tree's root node to the node.}}
{{term|Forest}} {{defn|A forest is a set of n &#8805; 0 disjoint trees.}}

===Data type vs. data structure===
There is a distinction between a tree as an abstract data type and as a concrete data structure, analogous to the distinction between a [[List (abstract data type)|list]] and a [[linked list]].
As a data type, a tree has a value and children, and the children are themselves trees; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node. To allow finite trees, one must either allow the list of children to be empty (in which case trees can be required to be non-empty, an "empty tree" instead being represented by a forest of zero trees), or allow trees to be empty, in which case the list of children can be of fixed size ([[branching factor]], especially 2 or "binary"), if desired.

As a data structure, a linked tree is a group of [[Node (computer science)|nodes]], where each node has a value and a list of [[Reference (computer science)|references]] to other nodes (its children). This data structure actually defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} because it may have loops or several references to the same node, just as a linked list may have a loop. Thus there is also the requirement that no two references point to the same node (that each node has at most a single parent, and in fact exactly one parent, except for the root), and a tree that violates this is "corrupt".

Due to the use of ''references'' to trees in the linked tree data structure, trees are often discussed implicitly assuming that they are being represented by references to the root node, as this is often how they are actually implemented. For example, rather than an empty tree, one may have a null reference: a tree is always non-empty, but a reference to a tree may be null.

===Recursive===

Recursively, as a data type a tree is defined as a value (of some data type, possibly empty), together with a list of trees (possibly an empty list), the subtrees of its children; symbolically:
 t: v &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
(A tree ''t'' consists of a value ''v'' and a list of other trees.)

More elegantly, via [[mutual recursion]], of which a tree is one of the most basic examples, a tree can be defined in terms of a forest (a list of trees), where a tree consists of a value and a forest (the subtrees of its children):
 f: &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
 t: v f

Note that this definition is in terms of values, and is appropriate in [[functional language]]s (it assumes [[Referential transparency (computer science)|referential transparency]]); different trees have no connections, as they are simply lists of values.

As a data structure, a tree is defined as a node (the root), which itself consists of a value (of some data type, possibly empty), together with a list of references to other nodes (list possibly empty, references possibly null); symbolically:
 n: v &lt;nowiki&gt;[&amp;amp;n[1], ..., &amp;amp;n[k]]&lt;/nowiki&gt;
(A node ''n'' consists of a value ''v'' and a list of references to other nodes.)

This data structure defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} and for it to be a tree one must add a condition on its global structure (its topology), namely that at most one reference can point to any given node (a node has at most a single parent), and no node in the tree point to the root. In fact, every node (other than the root) must have exactly one parent, and the root must have no parents.

Indeed, given a list of nodes, and for each node a list of references to its children, one cannot tell if this structure is a tree or not without analyzing its global structure and  that it is in fact topologically a tree, as defined below.

===Type theory===
As an [[Abstract data type|ADT]], the abstract tree type ''T'' with values of some type ''E'' is defined, using the  abstract forest type ''F'' (list of trees), by the functions:
:value: ''T'' &#8594; ''E''
:children: ''T'' &#8594; ''F''
:nil: () &#8594; ''F''
:node: ''E'' &#215; ''F'' &#8594; ''T''
with the axioms:
:value(node(''e'', ''f'')) = ''e''
:children(node(''e'', ''f'')) = ''f''
In terms of [[type theory]], a tree is an [[Recursive data type|inductive type]] defined by the constructors ''nil'' (empty forest) and ''node'' (tree with root node with given value and children).

===Mathematical===
Viewed as a whole, a tree data structure is an [[ordered tree]], generally with values attached to each node. Concretely, it is (if required to be non-empty):
* A [[rooted tree]] with the "away from root" direction (a more narrow term is an "[[Arborescence (graph theory)|arborescence]]"), meaning:
** A [[directed graph]],
** whose underlying [[undirected graph]] is a [[tree (graph theory)|tree]] (any two vertices are connected by exactly one simple path),
** with a distinguished root (one vertex is designated as the root),
** which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the ''parent'' and the node that the edge points to is called the ''child''),
together with:
* an ordering on the child nodes of a given node, and
* a value (of some data type) at each node.
Often trees have a fixed (more properly, bounded) [[branching factor]] ([[outdegree]]), particularly always having two child nodes (possibly empty, hence ''at most'' two ''non-empty'' child nodes), hence a "binary tree".

Allowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes "an empty tree, or a rooted tree such that ...". On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty).The complete sets of operations on tree must include fork operation.

==Terminology==
A '''[[node (computer science)|node]]''' is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). Each node in a tree has zero or more '''child nodes''', which are below it in the tree (by convention, trees are drawn growing downwards). A node that has a child is called the child's '''parent node''' (or ''ancestor node'', or [[Superior (hierarchy)|superior]]). A node has at most one parent.

An '''internal node''' (also known as an '''inner node''', '''inode''' for short, or '''branch node''') is any node of a tree that has child nodes. Similarly, an '''external node''' (also known as an '''outer node''', '''leaf node''', or '''terminal node''') is any node that does not have child nodes.

The topmost node in a tree is called the '''root node'''. Depending on definition, a tree may be required to have a root node (in which case all trees are non-empty), or may be allowed to be empty, in which case it does not necessarily have a root node. Being the topmost node, the root node will not have a parent. It is the node at which algorithms on the tree begin, since as a data structure, one can only pass from parents to children. Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only visit the root last (i.e., they first access the children of the root, but only access the ''value'' of the root last). All other nodes can be reached from it by following '''edges''' or '''links'''. (In the formal definition, each such path is also unique.) In diagrams, the root node is conventionally drawn at the top. In some trees, such as [[heap (data structure)|heaps]], the root node has special properties. Every node in a tree can be seen as the root node of the subtree rooted at that node.

The '''height''' of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The '''depth''' of a node is the length of the path to its root (i.e., its ''root path''). This is commonly needed in the manipulation of the various self-balancing trees, [[AVL Trees]] in particular. The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has depth and height &#8722;1.

A '''subtree''' of a tree ''T'' is a tree consisting of a node in ''T'' and all of its descendants in ''T''.{{efn|This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree &#8211; it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).}}&lt;ref&gt;{{MathWorld|id=Subtree|title=Subtree}}&lt;/ref&gt; Nodes thus correspond to subtrees (each node corresponds to the subtree of itself and all its descendants) &#8211; the subtree corresponding to the root node is the entire tree, and each node is the root node of the subtree it determines; the subtree corresponding to any other node is called a '''proper subtree''' (by analogy to a [[proper subset]]).

==Drawing Trees==
Trees are often drawn in the plane. Ordered trees can be represented essentially uniquely in the plane, and are hence called ''plane trees,'' as follows: if one fixes a conventional order (say, counterclockwise), and arranges the child nodes in that order (first incoming parent edge, then first child edge, etc.), this yields an embedding of the tree in the plane, unique up to [[ambient isotopy]]. Conversely, such an embedding determines an ordering of the child nodes.

If one places the root at the top (parents above children, as in a [[family tree]]) and places all nodes that are a given distance from the root (in terms of number of edges: the "level" of a tree) on a given horizontal line, one obtains a standard drawing of the tree. Given a binary tree, the first child is on the left (the "left node"), and the second child is on the right (the "right node").

==Representations==
There are many different ways to represent trees; common representations represent the nodes as [[Dynamic memory allocation|dynamically allocated]] records with pointers to their children, their parents, or both, or as items in an [[Array data structure|array]], with relationships between them determined by their positions in the array (e.g., [[binary heap]]).

Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp [[S-expression]]s, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child.

In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a [[threaded binary tree]].

==Generalizations==

===Digraphs===
If edges (to child nodes) are thought of as references, then a tree is a special case of a digraph, and the tree data structure can be generalized to represent [[directed graph]]s by removing the constraints that a node may have at most one parent, and that no cycles are allowed. Edges are still abstractly considered as pairs of nodes, however, the terms ''parent'' and ''child'' are usually replaced by different terminology (for example, ''source'' and ''target''). Different [[graph (data structure)#Representations|implementation strategies]] exist: a digraph can be represented by the same local data structure as a tree (node with value and list of children), assuming that "list of children" is a list of references, or globally by such structures as [[adjacency list]]s.

In [[graph theory]], a [[tree (graph theory)|tree]] is a connected acyclic [[Graph (data structure)|graph]]; unless stated otherwise, in graph theory trees and graphs are assumed undirected. There is no one-to-one correspondence between such trees and trees as data structure. We can take an arbitrary undirected tree, arbitrarily pick one of its [[vertex (graph theory)|vertices]] as the ''root'', make all its edges directed by making them point away from the root node &#8211; producing an [[Arborescence (graph theory)|arborescence]] &#8211; and assign an order to all the nodes. The result corresponds to a tree data structure. Picking a different root or different ordering produces a different one.

Given a node in a tree, its children define an ordered forest (the union of subtrees given by all the children, or equivalently taking the subtree given by the node itself and erasing the root). Just as subtrees are natural for recursion (as in a depth-first search), forests are natural for [[corecursion]] (as in a breadth-first search).

Via [[mutual recursion]], a forest can be defined as a list of trees (represented by root nodes), where a node (of a tree) consists of a value and a forest (its children):
 f: &lt;nowiki&gt;[n[1], ..., n[k]]&lt;/nowiki&gt;
 n: v f

==Traversal methods==
{{Main article|Tree traversal}}
Stepping through the items of a tree, by means of the connections between parents and children, is called '''walking the tree''', and the action is a '''walk''' of the tree. Often, an operation might be performed when a pointer arrives at a particular node. A walk in which each parent node is traversed before its children is called a '''pre-order''' walk; a walk in which the children are traversed before their respective parents are traversed is called a '''post-order''' walk; a walk in which a node's left subtree, then the node itself, and finally its right subtree are traversed is called an '''in-order''' traversal. (This last scenario, referring to exactly two subtrees, a left subtree and a right subtree, assumes specifically a [[binary tree]].)
A '''level-order''' walk effectively performs a [[breadth-first search]] over the entirety of a tree; nodes are traversed level by level, where the root node is visited first, followed by its direct child nodes and their siblings, followed by its grandchild nodes and their siblings, etc., until all nodes in the tree have been traversed.

==Common operations==
* Enumerating all the items
* Enumerating a section of a tree
* Searching for an item
* Adding a new item at a certain position on the tree
* Deleting an item
* [[Pruning (algorithm)|Pruning]]: Removing a whole section of a tree
* [[Grafting (algorithm)|Grafting]]: Adding a whole section to a tree
* Finding the root for any node

==Common uses==
* Representing [[hierarchical]] data
* Storing data in a way that makes it efficiently [[search algorithm|searchable]] (see [[binary search tree]] and [[tree traversal]])
* Representing [[sorting algorithm|sorted lists]] of data
* As a workflow for [[Digital compositing|compositing]] digital images for [[visual effects]]
* [[Routing]] algorithms

==See also==
* [[Tree structure]]
* [[Tree (graph theory)]]
* [[Tree (set theory)]]
* [[Hierarchy (mathematics)]]
* [[Dialog tree]]
* [[Single inheritance]]
* [[Generative grammar]]
* [[Hierarchical clustering]]
* [[Binary space partition tree]]
* [[Recursion]]

===Other trees===
* [[Trie]]
* [[DSW algorithm]]
* [[Enfilade (Xanadu)|Enfilade]]
* [[Left child-right sibling binary tree]]
* [[Hierarchical temporal memory]]

==Notes==
{{notelist}}

==References==
{{Reflist}}
{{refbegin}}
* [[Donald Knuth]]. ''[[The Art of Computer Programming]]: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&amp;nbsp;308&#8211;423.
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&amp;nbsp;214&#8211;217. Chapters 12&#8211;14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&amp;nbsp;253&#8211;320.
{{refend}}

==External links==
{{Commons category|Tree structures}}
* [http://www.community-of-knowledge.de/beitrag/data-trees-as-a-means-of-presenting-complex-data-analysis/ Data Trees as a Means of Presenting Complex Data Analysis] by Sally Knipe
* [https://xlinux.nist.gov/dads/HTML/tree.html Description] from the [[Dictionary of Algorithms and Data Structures]]
* [http://www.ipub.com/data.tree data.tree] implementation of a tree data structure in the R programming language
* [http://wormweb.org/celllineage WormWeb.org: Interactive Visualization of the ''C. elegans'' Cell Tree] &#8211; Visualize the entire cell lineage tree of the nematode ''C. elegans'' (javascript)
* [http://www.allisons.org/ll/AlgDS/Tree/ ''Binary Trees'' by L. Allison]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Tree (Data Structure)}}
[[Category:Data types]]
[[Category:Trees (data structures)| ]]
[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]</text>
      <sha1>5mw8vhows2qc8x38az6c5u48rb9q1jv</sha1>
    </revision>
  </page>
  <page>
    <title>Library system</title>
    <ns>0</ns>
    <id>21140383</id>
    <revision>
      <id>750147391</id>
      <parentid>748453823</parentid>
      <timestamp>2016-11-18T02:10:07Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>[[User:AnomieBOT/docs/TemplateSubster|Substing templates]]: {{PDFlink}}. See [[User:AnomieBOT/docs/TemplateSubster]] for info.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3074" xml:space="preserve">[[File:Boston Public Library Reading Room.jpg|thumb|Reading Room at [[Boston Public Library, McKim Building|McKim Building]] in 2013]]Library system is a central organization created to manage and coordinate operations and services in or between different centers, buildings or [[Library branch|libraries branches]] and libraries patrons. It uses a [[Library classification]] to organize their volumes and nowadays also uses a [[Integrated library system]], an [[enterprise resource planning]] system for a [[library]], used to track items owned, orders made, bills paid, and patrons who have borrowed.&lt;ref&gt;Adamson, Veronica, ''et al.'' (2008). {{cite web|url= http://www.jisc.ac.uk/media/documents/programmes/resourcediscovery/lmsstudy.pdf |title=''JISC &amp; SCONUL Library Management Systems Study'' }}&amp;nbsp;{{small|(1&amp;nbsp;MB)}}. Sheffield, UK: Sero Consulting. p. 51. Retrieved on 21 January 2009. "... a Library Management System (LMS or ILS 'Integrated Library System' in US parlance)."
Some useful library automation software are: KOHA ,Grennstone .LIBsis, and granthlaya.&lt;/ref&gt; Many counties, states or Universities have developed their own libraries systems, among them can be named [[Los Angeles Public Library|Los Angeles Public Library System]],&lt;ref&gt;{{Cite web|url=http://www.lapl.org/about-lapl/press/2013-library-facts|title=Los Angeles Public Library Facts 2013 (for fiscal year 2012-13) {{!}} Los Angeles Public Library|website=www.lapl.org|access-date=2016-03-06}}&lt;/ref&gt; [[Harvard Library|Harvard Library System]],.&lt;ref name="AR2013"&gt;{{cite web|title=Harvard Library Annual Report FY 2013 |url=http://library.harvard.edu/annual-report-fy-2013 |date=2013 |website=Harvard Library |author=Harvard University |accessdate=17 March 2015}}&lt;/ref&gt;

Most of [[County|counties]] of every country have their own '''library system'''s that usually have between 10 to 30 libraries on every city of their counties, some of them are; [[London Public Library]] on [[Canada]] with 16 library branches, [[Helsinki Metropolitan Area Libraries]], in [[Finland]], with 63 libraries,&lt;ref&gt;{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}&lt;/ref&gt; and some countries, like Venezuela has only one library system for the whole country as is [[National Library of Venezuela]] with 685 branches.  In the United States can be named [[Boston Public Library|Boston Public Library System]], [[New York Public Library|New York Public Library System]], [[District of Columbia Public Library|District of Columbia Public Library System]], among others.

==See also==
* [[Integrated library system]]
* [[Library classification]]
* [[Library branch]]
* [[List of the largest libraries in the United States]]

==References==
{{reflist}}

[[Category:Public libraries]]
[[Category:Private libraries]]
[[Category:Libraries]]
[[Category:Culture]]
[[Category:Knowledge representation]]


{{Library-stub}}</text>
      <sha1>ne4w2mas5phml93uj35pyv1pt2sckb3</sha1>
    </revision>
  </page>
  </mediawiki>