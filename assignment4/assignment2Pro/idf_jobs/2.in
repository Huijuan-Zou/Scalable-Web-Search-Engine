<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Data management</title>
    <ns>14</ns>
    <id>762162</id>
    <revision>
      <id>761378461</id>
      <parentid>728513377</parentid>
      <timestamp>2017-01-22T17:37:48Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* top */Cleaning up... using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="399" xml:space="preserve">{{Commons cat|Data management}}
*'''[[Data management]]''' &#8212; all the disciplines related to managing '''{{C|Data|data}}''' as a valuable resource.

{{clr}}
:::::{{Catmain|Data management}}
{{catdiffuse}}
{{CategoryTOC}}

{{Database}}
{{Databases}}

[[Category:Data|Management]]
[[Category:Computer data|Management]]
[[Category:Information retrieval]]
[[Category:Information technology management]]</text>
      <sha1>mji0q95ez6jn8lhp588vshpg3l5y4i2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directories</title>
    <ns>14</ns>
    <id>3119166</id>
    <revision>
      <id>637022163</id>
      <parentid>604574833</parentid>
      <timestamp>2014-12-07T14:10:02Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <comment>added to Information Retrieval category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="288" xml:space="preserve">A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.
{{Cat main|Directories}}
{{Commons cat|Directories}}

[[Category:Telephony]]
[[Category:Reference works]]
[[Category:Data management]]
[[Category:Information retrieval]]</text>
      <sha1>oqc4h7er045u1iygtnm663lkgnfom32</sha1>
    </revision>
  </page>
  <page>
    <title>Comprehensive Model of Information Seeking</title>
    <ns>0</ns>
    <id>45206870</id>
    <revision>
      <id>753673225</id>
      <parentid>708778578</parentid>
      <timestamp>2016-12-08T16:01:46Z</timestamp>
      <contributor>
        <ip>153.1.30.106</ip>
      </contributor>
      <comment>/* Antecdents */ fixed spelling mistake</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7574" xml:space="preserve">The '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].

The CMIS has been empirically tested in health and organizational contexts&lt;ref&gt;Johnson, J. D., &amp; Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women &amp; Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., &amp; Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. 
Science Communication, 16, 274-303.&lt;/ref&gt; The CMIS has inherent strengths for studying how people react to health problems such as cancer.&lt;ref name="auto"&gt;Johnson, J. D., Andrews, J. E. &amp; Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.&lt;/ref&gt; The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.

==Design==

[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]
The CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).&lt;ref name="auto"/&gt; There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person&#8217;s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer&#8217;s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.&lt;ref name="auto"/&gt;

==Antecedents==
The CMIS antecedents&#8212;demographics, personal experience, salience, and beliefs&#8212;are factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.&lt;ref&gt;Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.&lt;/ref&gt; In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.&lt;ref&gt;Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.&lt;/ref&gt;

==Information Carrier Characteristics==

The information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.&lt;ref name="auto"/&gt;

==Information Seeking Actions==

There are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.&lt;ref name="auto"/&gt;

==Stages in the CMIS==

A key concept from the CMIS is the notion of &#8220;stages,&#8221; or &#8220;cancer involvement&#8221;.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.

The first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.
 
The second stage is ''Purposive-Placid''. This is characterized by the question, &#8220;What can I do to prevent cancer?&#8221; Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.

The third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.

The fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.&lt;ref name="auto"/&gt;

== References ==
{{Research help|Med}}
{{Reflist}}



[[Category:Communication]]
[[Category:Information retrieval]]
[[Category:Health sciences]]</text>
      <sha1>02njhivw5b28hg6j444eqa77lp4mwqz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge representation</title>
    <ns>14</ns>
    <id>796635</id>
    <revision>
      <id>742239637</id>
      <parentid>735894183</parentid>
      <timestamp>2016-10-02T14:39:38Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Information, knowledge, and uncertainty]], this is a subfield of microeconomics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="506" xml:space="preserve">&lt;!--''Main article : [[Knowledge representation and reasoning]]''--&gt;
{{Cat main|Knowledge representation}}

Significant articles:
* [[Library classification]]
* [[Ontology (computer science)]]
* [[Semantic network]]
{{Category TOC}}

{{Commons cat|Knowledge representation}}

[[Category:Artificial intelligence]]
[[Category:Information science]]
[[Category:Knowledge engineering]]
[[Category:Programming paradigms]]
[[Category:Reasoning]]
[[Category:Scientific modeling]]
[[Category:Information retrieval]]</text>
      <sha1>ixypgtjat1ssnen0u68ykbdl5w08eb1</sha1>
    </revision>
  </page>
  <page>
    <title>Wiener connector</title>
    <ns>0</ns>
    <id>45655492</id>
    <revision>
      <id>759706479</id>
      <parentid>725381093</parentid>
      <timestamp>2017-01-12T18:57:49Z</timestamp>
      <contributor>
        <ip>68.181.207.217</ip>
      </contributor>
      <comment>added link to references and added categories</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8842" xml:space="preserve">{{Orphan|date=February 2016}}

In mathematics applied to the study of networks, the '''Wiener connector''', named in honor of chemist [[Harry Wiener]] who first introduced the [[Wiener Index]], is a means of maximizing efficiency in connecting specified "query vertices" in a network. Given a [[connected graph|connected]], [[undirected graph]] and a set of query vertices in a graph, the '''minimum Wiener connector''' is an [[induced subgraph]] that connects the query vertices and minimizes the sum of [[shortest path]] distances among all pairs of vertices in the subgraph. In [[combinatorial optimization]], the '''minimum Wiener connector problem''' is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic [[Steiner tree problem]] (one of [[Karp's 21 NP-complete problems]]), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.&lt;ref name="steiner"&gt;{{cite journal|last1=Hwang|first1=Frank|last2=Richards|first2=Dana|last3=Winter|first3=Dana|last4=Winter|first4=Pawel|title=The Steiner Tree Problem|journal=Annals of Discrete Mathematics|date=1992|url=http://www.sciencedirect.com/science/bookseries/01675060/53}}&lt;/ref&gt;&lt;ref name="dimacs"&gt;[http://dimacs11.cs.princeton.edu/ DIMACS Steiner Tree Challenge]&lt;/ref&gt;

The minimum Wiener connector was first presented by Ruchansky, et al. in 2015.&lt;ref name="sigmod"&gt;{{cite journal|last2=Bonchi|first2=Francesco|last3=Garcia-Soriano|first3=David|last4=Gullo|first4=Francesco|last5=Kourtellis|first5=Nicolas|date=2015|year=|title=The Minimum Wiener Connector|url=https://arxiv.org/abs/1504.00513|journal=SIGMOD|volume=|pages=|via=|last1=Ruchansky|first1=Natali}}&lt;/ref&gt;

The minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?

==Problem definition==
The [[Wiener index]] is the sum of shortest path distances in a (sub)graph. Using &lt;math&gt;d(u,v)&lt;/math&gt; to denote the shortest path between &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt;, the Wiener index of a (sub)graph &lt;math&gt;S&lt;/math&gt;, denoted &lt;math&gt;W(S)&lt;/math&gt;, is defined as
: &lt;math&gt;W(S) = \sum_{(u, v) \in S} d(u,v)&lt;/math&gt;.

The minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set &lt;math&gt;V&lt;/math&gt; and edge set &lt;math&gt;E&lt;/math&gt; and a set of query vertices &lt;math&gt;Q\subseteq V&lt;/math&gt;, find a connector &lt;math&gt;H\subseteq V&lt;/math&gt; of minimum Wiener index. More formally, the problem is to compute
: &lt;math&gt;\operatorname*{arg\,min}_H W(H\cup Q)&lt;/math&gt;,
that is, find a connector &lt;math&gt;H&lt;/math&gt; that minimizes the sum of shortest paths in &lt;math&gt;H&lt;/math&gt;.

==Relationship to Steiner tree==
[[File:SteinerExample nicer.pdf|thumb|upright=2.0|The optimal solutions to the Steiner tree problem and the minimum Wiener connector can differ. Define the set of query vertices ''Q'' by ''Q'' = {''v''&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;, ''v''&lt;sub&gt;10&lt;/sub&gt;}. The unique optimal solution to the Steiner tree problem is ''Q'' itself, which has Wiener index 165, whereas the optimal solution for the minimum Wiener connector problem is ''Q'' &#8746; {''r''&lt;sub&gt;1&lt;/sub&gt;, ''r''&lt;sub&gt;2&lt;/sub&gt;}, which has Wiener index 142.]]
The minimum Wiener connector problem is related to the [[Steiner tree problem]]. In the former, the [[objective function]] in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.

== Computational complexity ==

===Hardness===
The problem is [[NP-hard]], and does not admit a [[polynomial-time approximation scheme]] unless [[P = NP|'''P''' = '''NP''']].&lt;ref name="sigmod"/&gt; This can be proven using the [[inapproximability]] of [[vertex cover]] in bounded degree graphs.&lt;ref name="dinursafra"&gt;{{cite journal|last1=Dinur|first1=Irit|last2=Safra|first2=Samuel|title=On the hardness of approximating minimum vertex cover|journal=Annals of Mathematics|date=2005}}&lt;/ref&gt; Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation&#8212;an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of [[complexity class]]es, the minimum Wiener connector problem is in '''[[APX]]''' but is not in '''PTAS''' unless '''P''' = '''NP'''.

=== Exact algorithms ===
An exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in &lt;math&gt;2^{O(n)}&lt;/math&gt; time (that is, [[exponential time]]) on graphs with ''n'' vertices. In the special case that there are exactly two query vertices, the optimum solution is the [[shortest path]] joining the two vertices, so the problem can be solved in [[polynomial time]] by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.

=== Approximation algorithms ===
There is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time &lt;math&gt;O(q (m \log n + n \log^2 n))&lt;/math&gt; on a graph with ''n'' vertices, ''m'' edges, and ''q'' query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph.&lt;ref name="sigmod"/&gt; The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.

==Behavior==

The minimum Wiener connector behaves like [[Centrality#Betweenness centrality|betweenness centrality]].

When the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community.  Such vertices are likely to be [[influential]] vertices playing leadership roles in the community. In a [[social network]], these influential vertices might be good users for spreading information or to target in a viral marketing campaign.&lt;ref name="viral"&gt;{{cite journal | first1=Oliver | last1=Hinz | first2=Bernd | last2=Skiera | first3=Christian | last3=Barrot | first4=Jan U. | last4=Becker | title=Seeding Strategies for Viral Marketing: An Empirical Comparison | journal=Journal of Marketing | volume=75 | number=6 | pages=55&#8211;71 | year = 2011 | doi=10.1509/jm.10.0088}}&lt;/ref&gt;

When the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a [[Social Network#Structural holes|structural hole]] in the graph and are important.&lt;ref name="structhole"&gt;{{cite conference|last1=Lou|first1=Tiancheng|last2=Tang|first2=Jie|title=Mining Structural Hole Spanners Through Information Diffusion in Social Networks|booktitle=Proceedings of the 22nd International Conference on World Wide Web|date=2013|isbn=9781450320351|location=Rio de Janeiro, Brazil|pages=825&#8211;836|url=http://dl.acm.org/citation.cfm?id=2488388.2488461|publisher=International World Wide Web Conferences Steering Committee}}&lt;/ref&gt;

==Applications==
The minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,
* in [[biology]], it provides insight into how a set of proteins in a [[protein&#8211;protein interaction]] network are related,
* in [[social network]]s (like [[Twitter]]), it demonstrates the communities to which a set of users belong and how these communities are related,
* in [[computer network]]s, it may be useful in identifying an efficient way to route a [[multicast]] message to a set of destinations.

==References==
{{reflist}}

[[Category:NP-complete problems]]
[[Category:Trees (graph theory)]]
[[Category:Computational problems in graph theory]]
[[Category:Geometric algorithms]]
[[Category:Geometric graphs]]
[[Category:Graph algorithms]]
[[Category:Data mining]]
[[Category:Social networks]]
[[Category:Computational biology]]
[[Category:Computer science]]
[[Category:Algorithms]]
[[Category:Information retrieval]]
__INDEX__</text>
      <sha1>bvbcr40ojals22bj58wcu20m3tvdne4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Query languages</title>
    <ns>14</ns>
    <id>911721</id>
    <revision>
      <id>546489366</id>
      <parentid>494452511</parentid>
      <timestamp>2013-03-23T07:13:36Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 21 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7142646]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="258" xml:space="preserve">This [[Wikipedia:Category|category]] lists those [[domain-specific programming language]]s targeted at performing [[database]] [[query language|queries]].

[[Category:Domain-specific programming languages]]
[[Category:Data management]]
[[Category:Databases]]</text>
      <sha1>1z0upln47suqadco8ygku8ac77orck1</sha1>
    </revision>
  </page>
  <page>
    <title>QuickPar</title>
    <ns>0</ns>
    <id>1756767</id>
    <revision>
      <id>762794371</id>
      <parentid>747441420</parentid>
      <timestamp>2017-01-30T19:55:39Z</timestamp>
      <contributor>
        <username>Bulat Ziganshin</username>
        <id>11345125</id>
      </contributor>
      <comment>/* External links */ Removed link to malicious multipar.eu site!</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4288" xml:space="preserve">{{Merge to |Parchive |date=March 2014}}

{{Infobox software
| logo                   = 
| screenshot             = [[File:QuickPar Screenshot.png|250px]]
| caption                = QuickPar 0.9 checking a series of [[RAR (file format)|RAR]] files for integrity.
| collapsible            = 
| author                 = 
| developer              = Peter Clements
| released               = 0.1, (February 5, 2003)&lt;ref&gt;{{cite web|url=http://www.quickpar.org.uk/ReleaseNotes2.htm|title=QuickPar - Old Release Notes|accessdate=2010-11-19}}&lt;/ref&gt;
| latest release version = 0.9.1
| latest release date    = {{Start date and age|2004|07|04}}&lt;ref&gt;{{cite web |url=http://www.quickpar.org.uk/ |title=QuickPar for Windows |accessdate=2009-09-27}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
| frequently updated     = 
| programming language   = 
| operating system       = [[Microsoft Windows]]
| platform               = [[x86]]
| size                   = 
| language               = 
| status                 = 
| genre                  = [[Data recovery]]
| license                = [[Proprietary software|Proprietary]], [[Freeware]]
| website                = {{URL|www.quickpar.org.uk}}
}}

'''QuickPar''' is a computer program that creates [[parchive]]s used as verification and recovery information for a file or group of files, and uses the recovery information, if available, to attempt to reconstruct the originals from the damaged files and the PAR volumes.

Designed for the [[Microsoft Windows]] [[operating system]], it is often used to recover damaged or missing files that have been downloaded through [[Usenet]].&lt;ref&gt;{{cite book
| last        = Wang
| first       = Wallace
| authorlink  = 
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&amp;pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 &#8211; 167
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}&lt;/ref&gt; QuickPar may also be used under [[Linux]] via [[Wine (software)|Wine]].&lt;ref&gt;{{cite book
| last        = Petersen
| first       = Richard
| title       = Ubuntu 9.04 Desktop Handbook
| url         = https://books.google.com/books?id=-XLrpiHDYoQC&amp;pg=PT224
| accessdate  = 2009-09-27
| date        = 2009-05-01
| publisher   = Surfing Turtle Press
| location    = [[Los Angeles, California]]
| isbn        = 0-9820998-4-3
| page        = 224
| chapter     = Internet Applications
}}&lt;/ref&gt;
[[Image:QuickPar Protect Screenshot.png|thumb|right|Par2 file creation screen]]

There are two main versions of [[Parchive|PAR files]]: PAR and PAR2. The PAR2 file format lifts many of its previous restrictions.&lt;ref&gt;{{cite web
| url         = http://www.quickpar.org.uk/AboutPAR2.htm
| title       = QuickPar - About PAR2
| accessdate  = 2009-09-27
}}&lt;/ref&gt; QuickPar is [[freeware]] but not [[open source]]. It uses the [[Reed-Solomon error correction]] algorithm internally to create the error correcting information.&lt;ref name="newsgroups"&gt;{{Cite journal | last1 = Fellows | first1 = G. | title = Newsgroups reborn &#8211; the binary posting renaissance | doi = 10.1016/j.diin.2006.04.006 | journal = Digital Investigation | volume = 3 | issue = 2 | pages = 73&#8211;78 | year = 2006 | pmid =  | pmc = }}&lt;/ref&gt;

==Abandonware==
Though QuickPar works well, it is currently considered [[abandonware]], since there have been no updates for it in {{age|2004|07|04}} years.  The software, [[Parchive#Windows| MultiPar]], is actively being developed by another author named Yutaka Sawada, who is adding support for the new PAR3 file format.

==See also==
* [[:nl:Data Archiving and Networked Services|DANS; has some similar software]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.quickpar.org.uk/}}
* [http://www.quickpar.org.uk/Tutorials.htm QuickPar tutorial referencing Usenet downloads]
* [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar], successor to QuickPar, supports PAR2, PAR3 and multicore cpu's

[[Category:Data management]]


{{storage-software-stub}}</text>
      <sha1>sky0fcczl0xx96h4lqpnymj56i8ewu9</sha1>
    </revision>
  </page>
  <page>
    <title>Universal Data Element Framework</title>
    <ns>0</ns>
    <id>2570284</id>
    <revision>
      <id>752566784</id>
      <parentid>733648601</parentid>
      <timestamp>2016-12-02T00:18:29Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http&amp;rarr;https for [[YouTube]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9231" xml:space="preserve">The '''Universal Data Element Framework''' ('''UDEF''') provides the foundation for building an enterprise-wide [[controlled vocabulary]]. It is a standard way of indexing enterprise information that can produce big cost savings. UDEF simplifies information management through consistent classification and assignment of a global standard identifier to the data names and then relating them to similar data element concepts defined by other organizations. Though this approach is a small part of the overall picture, it is potentially a crucial enabler of semantic [[interoperability]].

== How UDEF works ==
UDEF provides semantic links, through assigning an intelligent, derived ID as an attribute of the data element, essentially labeling the element as a specific data element concept. When this UDEF ID exists in both source and target formats, it can then be used as an easy analysis point via a [[match report]], and then as the primary pivot point for transformations between source and target.

UDEF takes a list of high-level root object classes and assigns an integer to each class plus alpha characters to each specialization modifier.  It then also assigns integers to property word plus integers to each specialization modifier.  These object class alpha-integers are concatenated together with the property integers to form a dewey-decimal like code for each data element concept.

===Examples===
For the following examples, go to http://www.opengroup.org/udefinfo/htm/en_defs.htm and expand the applicable UDEF object and property trees

Assuming an application used by a hospital needs to map the data element concepts to the UDEF, the last name and first name (within UDEF you will find Family Name and Given Name under the UDEF property Name) of several people that are likely to appear on a medical record that could include the following example data element concepts &#8211;

*Patient Person Family Name &#8211; find the word &#8220;Patient&#8221; under the UDEF object &#8220;Person&#8221; and find the word &#8220;Family&#8221; under the UDEF property &#8220;Name&#8221;
*Patient Person Given Name &#8211; find the word &#8220;Patient&#8221; under the UDEF object &#8220;Person&#8221; and find the word &#8220;Given&#8221; under the UDEF property &#8220;Name&#8221;
*Doctor Person Family Name &#8211; find the word &#8220;Doctor&#8221; under the UDEF object &#8220;Person&#8221; and find the word &#8220;Family&#8221; under the UDEF property &#8220;Name&#8221;
*Doctor Person Given Name &#8211; find the word &#8220;Doctor&#8221; under the UDEF object &#8220;Person&#8221; and find the word &#8220;Given&#8221; under the UDEF property &#8220;Name&#8221;

The associated UDEF IDs for the above are derived by walking up each tree respectively and using an underscore to separate the object from the property. For the examples above, the following data element concepts are available within the current UDEF &#8211; see http://www.opengroup.org/udefinfo/htm/en_ob5.htm and http://www.opengroup.org/udefinfo/htm/en_pr10.htm

*&#8220;Patient Person Family Name&#8221; the UDEF ID is &#8220;au.5_11.10&#8221;
*&#8220;Patient Person Given Name&#8221; the UDEF ID is &#8220;au.5_12.10&#8221;
*&#8220;Doctor Person Family Name&#8221; the UDEF ID is &#8220;aq.5_11.10&#8221;
*&#8220;Doctor Person Given Name&#8221; the UDEF ID is &#8220;aq.5_12.10&#8221;

== Six basic steps to map enterprise data to the UDEF ==

There are six basic steps to follow when mapping data element concepts to the UDEF.

1. Identify the applicable UDEF property word that characterizes the dominant attribute (property) of the data element concept. For example: Name, Identifier, Date, etc.

2. Identify the dominant UDEF object word that the dominant property (selected in step 1) is describing. For example, Person_Name, Product_Identifier, Document_Date, etc.

3. By reviewing the UDEF tree for the selected property identified in step 1, identify applicable qualifiers that are necessary to describe the property word term unambiguously. For example, Family Name.

4. By reviewing the UDEF tree for the selected object identified in step 2, identify applicable qualifiers that are necessary to describe the object word term unambiguously. For example, Customer Person.

5. Concatenate the object term and the property term to create a UDEF naming convention compliant name where it is recognized that the name may seem artificially long. For example, Customer Person_Family Name.

6. Derive a structured ID based on the UDEF taxonomy that carries the UDEF inherited indexing scheme. For example &lt;CustomerPersonFamilyName UDEFID=&#8221;as.5_11.10&#8221;&gt;.

== The Open Group UDEF Project objectives ==

The UDEF Project aims to establish the Universal Data Element Framework (UDEF) as the universally-used classification system for data element concepts. It focuses on developing and maintaining the UDEF as an open standard, advocating and promoting it, putting in place a technical infrastructure to support it, implementing a Registry for it, and setting up education programs to train information professionals in its use.

Organizations that implement UDEF will likely realize the greatest benefit by defining their controlled vocabulary based on the UDEF. To help an organization manage its UDEF based controlled vocabulary, it should seriously consider a metadata registry that is based on ISO/IEC 11179-5.

== History of UDEF ==

Ron Schuldt, Sr. Enterprise Data Architect, Lockheed Martin, originated the UDEF concept based on [[ISO/IEC 11179]] Metadata standards in the early 1990s. Currently, he is a Senior Partner with Femto-Data LLC

== Ownership of UDEF intellectual property ==

The Open Group assumed from the [[Association for Enterprise Information]] (AFEI) the right to grant public use licensing of the UDEF.

The Supplier Management Council Electronic Enterprise Working Group of the [[Aerospace Industry Association]] (AIA) supports the UDEF as the naming convention solution to [[XML]] interoperability between standards that include all functions throughout a product's life-cycle and is working through a well defined process to obtain approval of this position from AIA and its member companies.

== Criticism ==

Classification in UDEF is sometimes hampered by ad hoc decisions that might produce problems. 
Example: 
* b.be.5 is "United-Kingdom Citizen Person" and
* c.be.5 is "European Union Citizen Person"
As the United Kingdom is part of the European Union, the classification is not unique.
Response:
The UDEF is flexible and is designed to match the semantics and behaviour of existing systems. Therefore, if one system has a table for United Kingdom Citizens and a different system has a table for European Union Citizens, the UDEF can handle both situations.

Some of the concepts in UDEF are not as universal as it is claimed. They show a lot of bias to Anglo-American tradition and way of thinking and are not easily transferable to other languages.
Example: The following part of the hierarchy shows the concept of an officer.

* j.5        Officer.Person
* a.j.5      Contracting.Officer.Person
* a.a.j.5    Procuring.Contracting.Officer.Person
* a.a.a.j.5  Government.Procuring.Contracting.Officer.Person
* b.a.j.5    Administrative.Contracting.Officer.Person
* b.j.5      Police.Officer.Person
* c.j.5      Military.Officer.Person
In many cultures, the part of the tree below "a.j.5 Contracting Officer Person" would not be placed under j.5 (see [[Officer (disambiguation)|officer]]) as b.j.5 (see [[Law enforcement officer]]) or c.j.5 (see [[Officer (armed forces)]]).

==See also==
* [[Data integration]]
* [[ISO/IEC 11179]]
* [[National Information Exchange Model]]
* [[Metadata]]
* [[Naming conventions (programming)]]
* [[Semantics]]
* [[Semantic web]]
* [[Semantic equivalency]]
* [[Data element]]
* [[Representation term]]
* [[Controlled vocabulary]]

== External links ==
* [http://www.opengroup.org/udefinfo/ UDEF Project of The Open Group]
* [https://www.youtube.com/embed/y6hID5qzAzQ YouTube - UDEF Tutorial, Part 1]
* [https://www.youtube.com/embed/d6dH_U8TqhY YouTube - UDEF Tutorial, Part 2]
* [http://www.opengroup.org/udefinfo/faq.htm UDEF Frequently Asked Questions]
* [https://udef-it.com/UDEF_Tools.html - Obtain Enhanced UDEF Gap Analysis Tool in English, Dutch, or French]

== Further reading ==
* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt |publisher=CreateSpace |date=November 15, 2011 |isbn=978-1-4664-6762-0 |url=https://www.createspace.com/3711806}}
* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt and Roberta Shauger |publisher=Amazon Digital Services |date=January 16, 2012 |isbn=1-4664-6762-2 |url=http://www.amazon.com/dp/B006YK6YOQ}}
* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger |publisher=CreateSpace |date=December 20, 2011 |isbn=978-1-4681-1483-6 |url=https://www.createspace.com/3753707}}
* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger and Ronald Schuldt |publisher=Amazon Digital Services |date=January 14, 2012 |isbn=1-4681-1483-2 |url=http://www.amazon.com/dp/B006XXMLQE}}

[[Category:Data management]]
[[Category:Interoperability]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Open Group standards]]
[[Category:Software that uses Motif]]
[[Category:Technical communication]]</text>
      <sha1>4f9xyq8lyn2r8u7p34rz6wgw3fcaqno</sha1>
    </revision>
  </page>
  <page>
    <title>Data architecture</title>
    <ns>0</ns>
    <id>4071997</id>
    <revision>
      <id>747179067</id>
      <parentid>744295548</parentid>
      <timestamp>2016-10-31T21:56:24Z</timestamp>
      <contributor>
        <username>Rednblu</username>
        <id>15356</id>
      </contributor>
      <minor />
      <comment>/* Overview */  grammar</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10439" xml:space="preserve">{{Refimprove|date=November 2008}}
In [[information technology]], '''data architecture''' is composed of models, policies, rules or standards that govern which data is collected, and how it is stored, arranged, integrated, and put to use in data systems and in organizations.&lt;ref&gt;[http://www.businessdictionary.com/definition/data-architecture.html Business Dictionary - Data Architecture]&lt;/ref&gt;  Data is usually one of several [[architecture domain]]s that form the pillars of an [[enterprise architecture]] or [[solution architecture]].&lt;ref&gt;[http://www.learn.geekinterview.com/data-warehouse/data-architecture/what-is-data-architecture.html What is data architecture] GeekInterview, 2008-01-28, accessed 2011-04-28&lt;/ref&gt;

== Overview ==
A data architecture should{{POV statement|date=March 2013}} set data standards for all its data systems as a vision or a model of the eventual interactions between those data systems. [[Data integration]], for example, should be dependent upon data architecture standards since data integration requires data interactions between two or more data systems. A data architecture, in part, describes the [[data structure]]s used by a business and its computer [[applications software]]. Data architectures address data in storage and data in motion; descriptions of data stores, data groups and data items; and [[data mapping|mappings]] of those data artifacts to data qualities, applications, locations etc.

Essential to realizing the target state, Data Architecture describes how data is processed, stored, and utilized in an [[information system]]. It provides criteria for [[data processing]] operations so as to make it possible to design [[data flow]]s and also control the flow of data in the system.

The [[data architect]] is typically responsible for defining the target state, aligning during development and then following up to ensure enhancements are done in the spirit of the original blueprint.

During the definition of the target state, the Data Architecture breaks a subject down to the atomic level and then builds it back up to the desired form. The data architect breaks the subject down by going through 3 traditional architectural processes:
* Conceptual - represents all business entities.
* Logical - represents the logic of how entities are related.
* Physical - the realization of the data mechanisms for a specific type of functionality.

The "data" column of the [[Zachman Framework]] for enterprise architecture &amp;ndash;

{| border=1
|'''Layer''' || '''View''' || '''Data (What)''' || '''Stakeholder'''
|-
|1||'''Scope/Contextual''' || List of things and architectural standards&lt;ref&gt;[http://www.strins.com/data-architecture-standards.html Data Architecture Standards]&lt;/ref&gt; important to the business || Planner
|-
|2||'''Business Model/Conceptual'''  || Semantic model or [[Entity-relationship model|Conceptual]]/[http://tdan.com/the-enterprise-data-model/5205 Enterprise Data Model] || Owner
|-
|3||'''System Model/Logical''' || Enterprise/[[Logical data model|Logical Data Model]] || Designer
|-
|4||'''Technology Model/Physical''' || [[Physical data model|Physical Data Model]] || Builder
|-
|5||'''Detailed Representations'''  ||  Actual [[database]]s || Subcontractor
|}

In this second, broader sense, data architecture includes a complete analysis of the relationships among an organization's functions, available [[technologies]], and [[data type]]s.

Data architecture should be defined in the '''planning phase''' of the design of a new data processing and storage system. The major types and sources of data necessary to support an enterprise should be identified in a manner that is complete, consistent, and understandable. The primary requirement at this stage is to define all of the relevant '''data entities''', not to specify [[computer hardware]] items. A data entity is any real or abstracted thing about which an organization or individual wishes to store data.

== Physical data architecture ==
Physical data architecture of an information system is part of a [[Technology roadmapping|technology plan]]. As its name implies, the technology plan is focused on the actual tangible [[element (mathematics)|elements]] to be used in the implementation of the data architecture [[design]]. Physical data architecture encompasses database architecture. Database architecture is a [[Model (abstract)|schema]] of the actual database technology that will support the designed data architecture.

== Elements of data architecture ==
Certain elements must be defined during the design phase of the data architecture schema. For example, administrative structure that will be established in order to manage the data resources must be described. Also, the methodologies that will be employed to store the data must be defined. In addition, a description of the database technology to be employed must be generated, as well as a description of the processes that will manipulate the data. It is also important to design [[interface (computing)|interfaces]] to the data by other systems, as well as a design for the [[infrastructure]] that will support common data operations (i.e. emergency procedures, [[data import]]s, [[data backup]]s, external [[data transfer|transfers of data]]).

Without the guidance of a properly implemented data architecture design, common data operations might be implemented in different ways, rendering it difficult to understand and control the flow of data within such systems. This sort of fragmentation is highly undesirable due to the potential increased cost, and the data disconnects involved. These sorts of difficulties may be encountered with rapidly growing enterprises and also enterprises that service different lines of [[business]] (e.g. [[insurance]] [[Product (business)|products]]).

Properly executed, the data architecture phase of information system planning forces an organization to precisely specify and describe both internal and external information flows. These are patterns that the organization may not have previously taken the time to conceptualize. It is therefore possible at this stage to identify costly information shortfalls, disconnects between departments, and disconnects between organizational systems that may not have been evident before the data architecture analysis.&lt;ref&gt;{{cite book|last=Mittal|first=Prashant|title=Author|year=2009|publisher=Global India Publications|location=pg 256|isbn=978-93-8022-820-4|pages=314|url=https://books.google.com/books?id=BpkhYDj4tm0C&amp;dq=inauthor:%22PRASHANT+MITTAL%22&amp;source=gbs_navlinks_s}}&lt;/ref&gt;

== Constraints and influences ==
Various constraints and influences will have an effect on data architecture design. These include enterprise requirements, technology drivers, economics, business policies and data processing needs.

; Enterprise requirements: These will generally include such elements as economical and effective system expansion, acceptable performance levels (especially system access speed), [[Financial transaction|transaction]] reliability, and transparent [[data management]]. In addition, the [[Data conversion|conversion]] of raw data such as transaction [[Record (computer science)|records]] and [[image]] [[Computer file|files]] into more useful [[information]] forms through such features as [[data warehouse]]s is also a common organizational [[requirement]], since this enables managerial decision making and other organizational processes. One of the architecture techniques is the split between managing [[transaction data]] and (master) [[reference data]]. Another one is splitting [[Automatic identification and data capture|data capture systems]] from data retrieval systems (as done in a data warehouse).

; Technology drivers: These are usually suggested by the completed data architecture and database architecture designs. In addition, some technology drivers will derive from existing organizational integration frameworks and standards, organizational economics, and existing site resources (e.g. previously purchased [[software licensing]]).

; Economics: These are also important factors that must be considered during the data architecture phase. It is possible that some solutions, while optimal in principle, may not be potential candidates due to their cost. External factors such as the [[business cycle]], interest rates, market conditions, and legal considerations could all have an effect on decisions relevant to data architecture.

; Business policies: [[Business policies]] that also drive data architecture design include internal organizational policies, rules of [[regulatory agency|regulatory bodies]], professional standards, and applicable governmental [[laws]] that can vary by applicable [[government agency|agency]]. These policies and rules will help describe the manner in which enterprise wishes to process their data.

; Data processing needs: These include accurate and reproducible [[data transaction|transactions]] performed in high volumes, data warehousing for the support of management information systems (and potential [[data mining]]), repetitive periodic [[Data reporting|reporting]], ad hoc reporting, and support of various organizational initiatives as required (i.e. annual budgets, new [[Product (business)|product]] development).

== See also ==
* [[Enterprise Information Security Architecture]] - (EISA) positions data security in the enterprise information framework.
* [[FDIC Enterprise Architecture Framework]]
* [[Controlled vocabulary]]
* [[Information silo]]
* [[Disparate system]]
* [[Data Warehouse]]

== References ==
{{reflist}}

== Further reading ==
* Bass, L.; John, B.; &amp; Kates, J. (2001). ''Achieving Usability Through Software Architecture'', Carnegie Mellon University.
* Lewis, G.; Comella-Dorda, S.; Place, P.; Plakosh, D.; &amp; Seacord, R., (2001). ''Enterprise Information System Data Architecture Guide'' Carnegie Mellon University.
* Adleman, S.; Moss, L.; Abai, M. (2005). ''Data Strategy'' Addison-Wesley Professional.

== External links ==
{{commons category|Data architecture}}
* [http://www.sei.cmu.edu/library/abstracts/reports/01tr005.cfm Achieving Usability Through Software Architecture], sei.cmu.edu 2001
* [http://sunsite.uakom.sk/sunworldonline/swol-07-1998/swol-07-itarchitect.html The Logical Data Architecture], by Nirmal Baid

{{Data model}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:Enterprise architecture]]</text>
      <sha1>l50n91fv2xgkm9p26bn3y1g2nlco2c3</sha1>
    </revision>
  </page>
  <page>
    <title>Data archaeology</title>
    <ns>0</ns>
    <id>2588620</id>
    <revision>
      <id>757142692</id>
      <parentid>757142652</parentid>
      <timestamp>2016-12-29T01:23:00Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* See also */ alpha</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5447" xml:space="preserve">{{for|the computer-based analysis of archaeological data|Computational archaeology}}

'''Data archaeology''' refers to the art and science of recovering [[computer]] [[data]] encoded and/or encrypted in now obsolete [[Computer media|media]] or [[content format|formats]]. Data archaeology can also refer to recovering information from damaged electronic formats after natural or man made disasters.

The term originally appeared in 1993 as part of the [[Global Oceanographic Data Archaeology and Rescue Project]] (GODAR). The original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of [[climate change]]. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the [[Nimbus program|Nimbus 2]] satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.&lt;ref&gt;[http://nsidc.org/monthlyhighlights/january2010.html Techno-archaeology rescues climate data from early satellites] U.S. National Snow and Ice Data Center (NSIDC), January 2010 [http://www.webcitation.org/5xN1sNyDp Archived]&lt;/ref&gt;

[[NASA]] also utilizes the services of data archaeologists to recover information stored on 1960s era vintage computer tape, as exemplified by the [[Lunar Orbiter Image Recovery Project]] (LOIRP).&lt;ref&gt;[http://www.nasa.gov/topics/moonmars/features/LOIRP/ LOIRP Overview] NASA website November 14, 2008 [http://www.webcitation.org/5xN1DjLG4 Archived]&lt;/ref&gt;

==Recovery==
It is also important to make the distinction in data archaeology between data recovery, and data intelligibility. You may be able to recover the data, but not understand it. For data archaeology to be effective the data must be intelligible.&lt;ref name="www.ukoln.ac.uk"&gt; [http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf] Study on website October 23, 2011 &lt;/ref&gt;

===Disaster recovery===
Data archaeologists can also use [[data recovery]] after natural disasters such as fires, floods, earthquakes, or even hurricanes. For example, in 1995 during [[Hurricane Marilyn]] the National Media Lab assisted the [[National Archives and Records Administration]] in recovering data at risk due to damaged equipment. The hardware was damaged from rain, salt water, and sand, yet it was possible to clean some of the disks and refit them with new cases thus saving the data within.&lt;ref name="www.ukoln.ac.uk"/&gt;

===Recovery techniques===
When deciding whether or not to try and recover data, the cost must be taken into account. If there is enough time and money, most data will be able to be recovered. In the case of [[magnetic media]], which are the most common type used for data storage, there are various techniques that can be used to recover the data depending on the type of damage.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|17}}

Humidity can cause tapes to become unusable as they begin to deteriorate and become sticky.  In this case, a heat treatment can be applied to fix this problem, by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape.  However, this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|17&#8211;18}}

Lubrication loss is another source of damage to tapes. This is most commonly caused by heavy use, but can also be a result of improper storage or natural evaporation.  As a result of heavy use, some of the lubricant can remain on the read-write heads which then collect dust and particles.  This can cause damage to the tape.  Loss of lubrication can be addressed by re-lubricating the tapes.  This should be done cautiously, as excessive re-lubrication can cause tape slippage, which in turn can lead to media being misread and the loss of data.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|18}}

Water exposure will damage tapes over time.  This often occurs in a disaster situation.  If the media is in salty or dirty water, it should be rinsed in fresh water.  The process of cleaning, rinsing, and drying wet tapes should be done at room temperature in order to prevent heat damage.  Older tapes should be recovered prior to newer tapes, as they are more susceptible to water damage.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|18}}

==Prevention==
To prevent the need of data archaeology, creators and holders of digital documents should take care to employ [[digital preservation]].

==See also==
* [[Data degradation|Bit rot]]
* [[Digital dark age]]
* [[Knowledge discovery]]

==References==
&lt;references /&gt;
*[http://www.worldwidewords.org/turnsofphrase/tp-dat1.htm World Wide Words: Data Archaeology]
*O'Donnell, James Joseph.  ''Avatars of the Word:  From Papyrus to Cyperspace''  Harvard University Press, 1998.
* {{cite book | last1  = Ross | first1 = Seamus | last2  = Gow | first2 = Ann| lastauthoramp = yes| title = Digital archaeology : rescuing neglected and damaged data resources| publisher = British Library and Joint Information Systems Committee| place     = London &amp; Bristol| series = Electronic libraries programme studies
| year    = 1999| | language = EN| url = http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf| isbn = 1-90050-851-6}}

[[Category:Data management|Archaeology]]
[[Category:Digital preservation]]
[[Category:Archaeological sub-disciplines]]</text>
      <sha1>cwjcbbyq13ianhr8fweg3vygl6y5pbh</sha1>
    </revision>
  </page>
  <page>
    <title>Content management</title>
    <ns>0</ns>
    <id>223240</id>
    <revision>
      <id>744979862</id>
      <parentid>744971461</parentid>
      <timestamp>2016-10-18T16:20:32Z</timestamp>
      <contributor>
        <username>Biogeographist</username>
        <id>18201938</id>
      </contributor>
      <comment>/* See also */ [[Snippet management]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11862" xml:space="preserve">{{distinguish|Information management|Knowledge management}}
{{Refimprove|date=July 2007}}

{{Business administration}}
'''Content management''' ('''CM'''), is a set of processes and technologies that supports the collection, managing, and publishing of information in any form or medium.  When stored and accessed via computers, this information may be more specifically referred to as [[digital content]], or simply as [[Content (media and publishing)|content]].  Digital content may take the form of text (such as [[electronic document]]s), multimedia files (such as audio or video files), or any other file type that follows a content lifecycle requiring [[Product lifecycle management|management]]. The process is complex enough to manage that several large and small commercial software vendors such as [[Interwoven]] and [[Microsoft]] offer [[Content management system|content management software]] to control and automate significant aspects of the content lifecycle.

==The process of content management==
Content management practices and goals vary by mission and by organizational governance structure.
News organizations, [[e-commerce]] websites, and educational institutions all use content management, but in different ways. This leads to differences in terminology and in the names and number of steps in the process.

For example, some digital content is created by one or more authors. Over time that content may be edited. One or more individuals may provide some editorial oversight, approving the content for publication. Publishing may take many forms: it may be the act of "pushing" content out to others, or simply granting digital access rights to certain content to one or more individuals.  Later that content may be superseded by another version of the content and thus retired or removed from use (as when this wiki page is modified).

Content management is an inherently collaborative process. It often consists of the following basic roles and responsibilities:

* '''Creator''' &#8211; responsible for creating and editing content.
* '''Editor''' &#8211; responsible for tuning the content message and the style of delivery, including translation and localization.
* '''Publisher''' &#8211; responsible for releasing the content for use.
* '''Administrator''' &#8211; responsible for managing access permissions to folders and files, usually accomplished by assigning access rights to user groups or roles. Admins may also assist and support users in various ways.
* '''Consumer, viewer or guest''' &#8211; the person who reads or otherwise takes in content after it is published or shared.

A critical aspect of content management is the ability to manage versions of content as it evolves (''see also'' [[version control]]). Authors and editors often need to restore older versions of edited products due to a process failure or an undesirable series of edits.

Another equally important aspect of content management involves the creation, maintenance, and application of review standards. Each member of the content creation and review process has a unique role and set of responsibilities in the development or publication of the content. Each review team member requires clear and concise review standards. These must be maintained on an ongoing basis to ensure the long-term consistency and health of the [[knowledge base]].

A content management system is a set of automated processes that may support the following features:

* Import and creation of documents and multimedia material
* Identification of all key users and their roles
* The ability to assign roles and responsibilities to different instances of content categories or types
* Definition of workflow tasks often coupled with messaging so that content managers are alerted to changes in content
* The ability to track and manage multiple versions of a single instance of content
* The ability to publish the content to a repository to support access
* The ability to personalize content based on a set of rules
* 
Increasingly, the repository is an inherent part of the system, and incorporates [[enterprise search]] and retrieval. Content management systems take the following forms:

* [[Web content management system]]&#8212;[[software]] for [[web site]] management (often what ''content management'' implicitly means)
* Output of a [[newspaper]] editorial staff organization
* [[Workflow]] for [[essay|article]] publication
* [[Document management system]]
* [[Single source publishing|Single source]] content management system&#8212;content stored in chunks within a relational database
* Variant management system&#8212;where personnel tag source content (usually text and graphics) to represent variants stored as single source "master" content modules, resolved to the desired variant at publication (for example: automobile owners manual content for 12 model years stored as single master content files and "called" by model year as needed)&#8212;often used in concert with database chunk storage (see above) for large content objects

==Governance structures==
Content management expert Marc Feldman defines three primary content management governance structures: localized, centralized, and federated&#8212;each having its unique strengths and weaknesses.&lt;ref&gt;http://www.clickz.com/clickz/column/1715089/governance-issues-content-management&lt;/ref&gt;

===Localized governance===
By putting control in the hands of those closest to the content, the context experts, localized governance models empower and unleash creativity.  These benefits come, however, at the cost of a partial-to-total loss of managerial control and oversight.

===Centralized governance===
When the levers of control are strongly centralized, content management systems are capable of delivering an exceptionally clear and unified brand message.  Moreover, centralized content management governance structures allow for a large number of cost-savings opportunities in large enterprises, realized, for example, (1) the avoidance of duplicated efforts in creating, editing, formatting, repurposing and archiving content, (2) through process management and the streamlining of all content related labor, and/or (3) through an orderly deployment or updating of the content management system.

===Federated governance===
Federated governance models potentially realize the benefits of both localized and centralized control while avoiding the weaknesses of both. While content management software systems are inherently structured to enable federated governance models, realizing these benefits can be difficult because it requires, for example, negotiating the boundaries of control with local managers and content creators.  In the case of larger enterprises, in particular, the failure to fully implement or realize a federated governance structure equates to a failure to realize the full return on investment and cost savings that content management systems enable.

==Implementation==
Content management implementations must be able to manage content distributions and digital rights in content life cycle.&lt;ref&gt;{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture &amp; Television Engineers |volume=113 |issue=4 |pages=110&#8211;120 |doi= |access-date=July 1, 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution &amp; DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}&lt;/ref&gt; Content management systems are usually involved with [[digital rights management]] in order to control user access and digital rights. In this step, the read-only structures of [[digital rights management]] systems force some limitations on content management, as they do not allow authors to change protected content in their life cycle. Creating new content using managed (protected) content is also an issue that gets protected contents out of management controlling systems. A few content management implementations cover all these issues.&lt;ref&gt;{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture &amp; Television Engineers |volume=113 |issue=4 |pages=110&#8211;120 |doi= |access-date=July 1, 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution &amp; DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}&lt;/ref&gt;

==See also==
{{Div col|colwidth=22em}}
* [[Content delivery]]
* [[Content engineering]]
* [[Content Management Interoperability Services]]
* [[Content management system]]
* [[Digital asset management]]
* [[Enterprise content management]]
* [[Enterprise information management]]
* [[Information architecture]]
* [[List of content management systems]]
* [[Single source publishing]]
* [[Snippet management]]
* [[Web content lifecycle]]
* [[Web design]]
* [[Website architecture]]
* [[Website governance]]
{{Div col end}}

==References==
{{reflist|colwidth=30em}}

==External links==
* {{cite book | last = Boiko | first = Bob | authorlink = | title = Content Management Bible | publisher = Wiley | date = 2004-11-26 | location = | pages = 1176 | url = | doi = | id = | isbn = 0-7645-7371-3 }}
* {{cite book | last = Rockley | first = Ann | authorlink = | title = Managing Enterprise Content: A Unified Content Strategy | publisher = New Riders Press | date = 2002-10-27 | location = | pages = 592 | url = | doi = | id = | isbn = 0-7357-1306-5 }}
* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}
* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath| title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}
* {{cite book | last = Ferran | first = N&#250;ria | authorlink = | author2=Juli&#224; Minguill&#243;n | title = Content Management for E-Learning | publisher = Springer | date = 2011 | location = | pages = 215 | url = http://www.springer.com/us/book/9781441969583 | doi = | id = | isbn = 978-1-4419-6958-3 }}

{{Content management systems}}
{{WebManTools}}

[[Category:Technical communication]]
[[Category:Data management]]
[[Category:Content management systems]]

[[fr:Syst&#232;me de gestion de contenu]]</text>
      <sha1>6zewsi8cyhjvmvnv865uae3whsjljbb</sha1>
    </revision>
  </page>
  <page>
    <title>Long-running transaction</title>
    <ns>0</ns>
    <id>4984219</id>
    <revision>
      <id>719328322</id>
      <parentid>687016914</parentid>
      <timestamp>2016-05-09T01:06:34Z</timestamp>
      <contributor>
        <ip>2620:0:E50:100F:81EE:1CE1:86BD:52B8</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1346" xml:space="preserve">{{More sources|date=October 2015}}
'''Long-running transactions''' (also known as Saga transactions) are computer [[database transaction]]s that avoid [[lock (computer science)|locks]] on non-local resources, use compensation to handle failures, potentially aggregate smaller [[ACID]] transactions (also referred to as [[atomic transaction]]s), and typically use a coordinator to complete or abort the transaction. In contrast to [[rollback (data management)|rollback]] in ACID transactions, compensation restores the original state, or an equivalent, and is business-specific. For example, the compensating action for making a hotel reservation is canceling that reservation, possibly with a penalty.

A number of protocols have been specified for long-running transactions using Web services within business processes. OASIS Business Transaction Processing&lt;ref&gt;http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=business-transaction&lt;/ref&gt; and WS-CAF&lt;ref&gt;http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=ws-caf&lt;/ref&gt; are examples. These protocols use a coordinator to mediate the successful completion or use of compensation in a long-running transaction.

==See also==
*[[Optimistic concurrency control]]
*[[Long-lived transaction]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>mw56ofvdd8xr6hzqpl5n5x4rbzzf8az</sha1>
    </revision>
  </page>
  <page>
    <title>Versomatic</title>
    <ns>0</ns>
    <id>9372544</id>
    <revision>
      <id>655076536</id>
      <parentid>532264024</parentid>
      <timestamp>2015-04-05T18:48:11Z</timestamp>
      <contributor>
        <username>RadicalRedRaccoon</username>
        <id>15526134</id>
      </contributor>
      <minor />
      <comment>Removed excess spaces.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2556" xml:space="preserve">'''Versomatic''' installs as a file system service where it tracks file changes and preemptively archives a copy of a file before it is modified. Archiving copies pre-emptively obviates the need to archive a reference copy of the files beforehand, as would be the case if the files were archived after being edited.

Starting from the moment Versomatic is installed, the last version of a file remains where the user expects it to be, and prior versions, if any, reside in a separate archive. Without this capability, we would have to scan your entire hard drive beforehand and make a duplicate copy of every file in order to create a baseline for subsequent revisions.

Files can be moved, renamed and copied without losing connection to their revision histories. Depending on user preferences, files can be monitored on local, removable and network drives. File revision histories are stored in a central database. Thus, for example, a user may insert a USB drive into his computer and edit a file on the USB drive. The edited file remains on the USB drive but a copy of the original unedited version is copied to the archive.

When a file is deleted, the deleted file is added to the repository together with any previous versions of the deleted file. This provides excellent level of protection against inadvertent file deletion.

Access to prior versions of a file is easy. A user merely selects a file and clicks the right mouse button to display a contextual pop-up menu listing the X most recent previous versions of the file, if any.

Upon user selection of one of the entries the appropriate previous version is opened with read-only privileges using the same application that created the file. Versomatic can also retrieve a copy of a previous version and move it into the same directory where the current version resides. The previous version will have a date &amp; time stamp added to its file name in order to distinguish it from the current version. If a previous version is exported from Versomatic&#8217;s archive and changes are made thereto, a new revision history is created for the new file.

==History==

Versomatic is the first product from the long time team of [[Joaquin de Soto]], Jorge Miranda and Manny Menendez released under their new company [[Acertant]]. The team has a long list of hit products to their credit: MacLightning, [[ACD Canvas|Canvas]], Spelling Coach, BigThesaurus, Comment, UltraPaint, Artworks, [[ACDSee]], DenebaCAD, etc.

==External links==
*[http://www.acertant.com Company Web Page]

[[Category:Data management]]</text>
      <sha1>cdes6py2m55sbww1bmd7t5310c8mjzo</sha1>
    </revision>
  </page>
  <page>
    <title>Holos</title>
    <ns>0</ns>
    <id>1423557</id>
    <revision>
      <id>570293542</id>
      <parentid>570293506</parentid>
      <timestamp>2013-08-26T18:24:36Z</timestamp>
      <contributor>
        <username>My name is not dave</username>
        <id>19079409</id>
      </contributor>
      <minor />
      <comment>[[Help:Reverting|Reverted]] edits by [[Special:Contributions/39.42.254.201|39.42.254.201]] ([[User talk:39.42.254.201|talk]]) to last version by Mauls</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8914" xml:space="preserve">{{multiple issues|
{{notability|date=May 2010}}
{{Unreferenced|date=May 2010}}
{{Peacock|date=September 2012}}
}}

'''Holos''' is an influential [[OLAP]] (Online Analytical Processing) product of the 1990s. Developed by Holistic Systems in 1987, the product remained in use until around 2004.

==Conception==
The Holos product succeeded an older generation of mainframe products such as [[System-W]]. It was the first to use an industry standard [[SQL]] database (as opposed to a proprietary one), and also the first to use the new GUI PC for the user interface.{{citation needed|date=September 2012}} In physically separating the ''number crunching'' from the user interface, the product was immediately client/server, although the term didn't come into use until some time later. In fact the process was described as cooperative processing at the time as client/server was not a current term at that time. The client/server model used for Holos was initially for a very "light" client as it was not clear at that time (1986/7) that PCs were going to be so commonplace and most were still running MS-DOS.

In fact it was technically possible to run the system using "dumb" terminal with reduced functionality in early versions although save for in Holistic's test environment this was rarely if ever done. In time due to the increased popularly of PCs and their increased power and the available of a stable and more functional version of Microsoft Windows additional functionality was added to the client end mostly in the form of development aids. In addition to data services, the Holos Server supplied business logic and calculation services. It also provided complementary services to the Holos Client which meant the internal processing associated with the report writer, worksheet, etc., was distributed between the two components.

==Architecture==
The core of the Holos Server was a [[business intelligence]] (BI) [[virtual machine]]. The Holos Language (HL) was compiled into a soft instruction code, and executed in this virtual machine (similar in concept to Java in more modern systems). The virtual machine was fully fault-tolerant, using structured [[exception handling]] internally, and provided a debugger interface. The debugger was machine-level until quite late on, after which it also supported source-level access.

OLAP data was handled as a core data type of HL, with specific syntax to accommodate multidimensional data concepts, and complete programmatic freedom to explore and utilise the data. This made it very different from the industry trend of query-based OLAP and SQL engines. On the upside, it allowed amazing flexibility in the applications to which it could be applied. On the downside, it mean that 3-tier configurations were never successfully implemented since the processing had to be close to the data itself. This hindered large-scale deployment to many clients, and the use of OLAP data from other vendors. In reality, its own data access times were probably some of the fastest around&#8212;at the individual cell level; they had to be in order to be practical. However, when fetching back bulk data for non-cooperating clients, or data from other vendors, the queries could not be optimised as a whole. Its own data access used a machine-wide shared memory cache.

==Language==
The Holos Language was a very broad language in that it covered a wide range of statements and concepts, including the reporting system, business rules, OLAP data, SQL data (using the Embedded SQL syntax within the hosting HL), device properties, analysis, forecasting, and data mining. It even supported elements to enable self-documentation and self-verification. Placing all these areas on a common footing, and allowing them to co-operate by sharing data, events, etc., was key to the number of possibilities that resulted. For instance, the report writer supported input as well as output, plus interactive graphics, and a comprehensive event mechanism to pass back information about the viewed data to event handlers. Also, reports and data were separate entities, thus allowing the same report to be applied to different data as long as it was described by similar meta-data. This meant that when terms like [[Enterprise Information System|EIS]] and [[Management information systems|MIS]] were first coined, the industry norm was "slideshows", i.e. pre-programmed transitions between views, whereas Holos provided data-driven drill-down, i.e. no pre-programmed views or links. The transitions could be made dependent upon the data values and trends, in conjunction with the available business logic.

==OLAP Storage==
Holos Server provided an array of different, but compatible, storage mechanisms for its multi-cube architecture: memory, disk, SQL. It was therefore the first product to provide "hybrid OLAP" ([[HOLAP]]). It provided a very versatile mechanism for joining cubes, irrespective of their storage technology, dimensionality, or meta-data, and this was eventually given a [[US patent]] (called COA&#8212;Compound OLAP Architecture {{US patent|6289352}}{{US patent|6490593}}). One novel aspect of this was a 'stack' feature that allowed read/write cubes to be stacked over read-only cubes. Read operations to the overall virtual cube then visited both 'racks' (top first, and then the bottom), whereas write operations only affected the top. The resulting valve-like mechanism found many applications in data sharing, what-if forecasting, and aggregation of slow SQL-based data. Since the overhead of the joining was small, it was not uncommon to have stacks 7 levels deep, and joining terabytes of real OLAP data. Around about V8.5, Holos Server implemented a hierarchical lock manager, allowing nesting of fine and coarse-grain OLAP locks, and full transaction control.

==Business Rules==
The business logic supported full cross-dimensional calculations, automatic ordering of rules using static data-flow analysis, and the identification and solution of simultaneous equations. The rules treated all dimensions in an orthogonal fashion. The aggregation process did not distinguish between simple summation or average calculations, and more complex non-commutative calculations. Both could be applied to any dimension member. The process allowed aggregation levels (i.e. those calculation levels starting with base data (level 0) and proceeding up to the overall grand total) to be individually pre-stored or left to be calculated on demand.

==Holos Client==
The Holos Client was both a design and delivery vehicle, and this made it quite large. Around about 2000, the Holos Language was made object-oriented (HL++) with a view to allowing the replacement of the Holos Client with a custom Java or VB product. However, the company were never sold on this, and so the project was abandoned.

One of the biggest failures was not to provide a thin-client interface to the Holos Server, and this must have contributed to the product's demise. Although an [[HTML]] toolkit was sold, it was clumsy and restricted. By the time a real thin-client mechanism was developed, it was far too late and it never got to market.

==Deployment==
Before its demise, the Holos Server product ran under Windows NT (Intel and Alpha), VMS (VAX and Alpha), plus about 10 flavours of UNIX, and accessed over half-a-dozen different SQL databases. It was also ported to several different locales, including Japanese.

==Company==
{{main|Crystal Decisions}}
Holistic Systems was purchased by the hardware company [[Seagate Technology]] in 1996. Along with other companies such as [[Crystal Services]], it was used to create a new subsidiary company called [[Seagate Software]]. Only Holistic and Crystal remained, and Seagate Software was renamed to [[Crystal Decisions]]. Holistic and Crystal had very different sales models. The average sale for the Holos Product in the United States was in excess of $250,000 and was sold primarily to Fortune 500 companies by a direct sales force. The Crystal sales model was based upon a "shrink wrapped" product Crystal Reports sold primarily through resellers. As Crystal was acquired prior to Holistic the senior management in the sales and marketing arena were mostly drawn from that organisation. They felt that all the product range should be sold through third parties and over a period of time dismantled the direct sales force culmination in a significant drop in sales for the Holos Product. Subsequently after some in-fighting and argument over product strategy, the main Holos development team finally started to leave around 2000, and Crystal Decisions was finally taken over by [[Business Objects (company)|Business Objects]] in 2004. Following the takeover, support for Holos was outsourced to [[Raspberry Software]], which was set up by former employees of Crystal Decisions.

[[Category:Data management]]
[[Category:Online analytical processing]]</text>
      <sha1>ayngzf4xllfvfznxeu96z5vrd0028l1</sha1>
    </revision>
  </page>
  <page>
    <title>Data classification (data management)</title>
    <ns>0</ns>
    <id>3743270</id>
    <revision>
      <id>755676697</id>
      <parentid>651075521</parentid>
      <timestamp>2016-12-19T14:55:23Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[data migration]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5581" xml:space="preserve">In the field of [[data management]], '''data classification''' as a part of [[Information Lifecycle Management]] (ILM) process can be defined as a tool for categorization of data to enable/help organization to effectively answer following questions:
*What [[data type]]s are available?
*Where are certain data located?
*What [[access level]]s are implemented?
*What protection level is implemented and does it adhere to [[compliance (regulation)|compliance]] regulations?
When implemented it provides a bridge between IT professionals and process or application owners. IT staff is informed about the data value and on the other hand management (usually application owners) understands better to what segment of data centre has to be invested to keep operations running effectively.  This can be of particular importance in risk management, legal discovery, and compliance with government regulations.
Data classification is typically a manual process; however, there are many tools from different vendors that can help gather information about the data.

==How to start process of data classification==
''Note that this classification structure is written from a Data Management perspective and therefore has a focus for text and text convertible binary data sources. Images, videos, and audio files are highly structured formats built for industry standard API's and do not readily fit within the classification scheme outlined below.''

First step is to evaluate and divide the various applications and data into their respective category as follows:
*Relational or Tabular data (around 15% of non audio/video data) 
**Generally describes  proprietary data which can be accessible only through application or [[application programming interfaces]] (API)
**Applications that produce structured data are usually database applications.
**This type of data usually brings complex procedures of data evaluation and migration between the storage tiers.
**To ensure adequate quality standards, the classification process has to be monitored by subject matter experts.
*Semi-structured or Poly-structured data (all other non audio/video data that does not conform to a system or platform defined Relational or Tabular form).
**Generally describes data files that have a dynamic or non-relational semantic structure (e.g. documents,XML,JSON,Device or System Log output,Sensor Output).
**Relatively simple process of data classification is criteria assignment.
**Simple process of [[data migration]] between assigned segments of predefined storage tiers.

Types of data classification - ''note that this designation is entirely orthogonal to the application centric designation outlined above. Regardless of structure inherited from application, data may be of the types below''

1. Geographical : i.e. according to area (supposing the rice production of a state or country etc.)
2. Chronological: i.e. according to time (sale of last 3 months)
3. Qualitative  : i.e. according to distinct categories. (E.g.: population on the basis of poor and rich)
4. Quantitative : i.e. according to magnitude(a) discrete and b)continuous

==Basic criteria for semi-structured or poly-structured data classification==
*Time criteria is the simplest and most commonly used where different type of data is evaluated by time of creation, time of access, time of update, etc.
*Metadata criteria as type, name, owner, location and so on can be used to create more advanced classification policy
*Content criteria which involve usage of advanced content classification algorithms are most advanced forms of unstructured data classification

''Note that any of these criteria may also apply to Tabular or Relational data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.''.

==Basic criteria for relational or Tabular data classification==
These criteria are usually initiated by application requirements such as:
*Disaster recovery and Business Continuity rules
*Data centre resources optimization and consolidation
*Hardware performance limitations and possible improvements by reorganization

''Note that any of these criteria may also apply to semi/poly structured data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.''

==Benefits of data classification==
Benefits of effective implementation of appropriate data classification can significantly improve ILM process and save data centre storage resources. If implemented systemically it can generate improvements in data centre performance and utilization. Data classification can also reduce costs and administration overhead. "Good enough" data classification can produce these results:
*Data compliance and easier [[risk management]]. Data are located where expected on predefined storage tier and "point in time"
*Simplification of data encryption because all data need not be encrypted. This saves valuable processor cycles and all related consecutiveness. 
*Data indexing to improve user access times
*Data protection is redefined where RTO ([[Recovery Time Objective]]) is improved.

==See also==
*[[Data classification (business intelligence)]]

==References==

* Josh Judd and Dan Kruger (2005), Principles of SAN Design. Infinity Publishing
* Stephen J. Bigelown (November 2005), SearchStorage.com, http://searchstorage.techtarget.com/news/article/0,289142,sid5_gci1139240,00.html

[[Category:Data management]]
[[Category:Information technology]]
[[Category:Regulations]]</text>
      <sha1>lv6nvshe1uqpd9kljl7bof5320qt67j</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology merging</title>
    <ns>0</ns>
    <id>11270991</id>
    <revision>
      <id>680113476</id>
      <parentid>543030469</parentid>
      <timestamp>2015-09-08T20:09:30Z</timestamp>
      <contributor>
        <username>Faizan</username>
        <id>17823581</id>
      </contributor>
      <minor />
      <comment>clean up, [[WP:AWB/T|typo(s) fixed]]: labour intensive &#8594; labour-intensive using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1014" xml:space="preserve">{{Unreferenced|date=March 2013}}

'''Ontology merging''' defines the act of bringing together two conceptually divergent [[ontology (computer science)|ontologies]] or the instance data associated to two ontologies. This is similar to work in database merging ([[schema matching]]). This merging process can be performed in a number of ways, manually, semi automatically, or automatically. Manual ontology merging although ideal is extremely labour-intensive and current research attempts to find semi or entirely automated techniques to merge ontologies. These techniques are statistically driven often taking into account similarity of concepts and raw similarity of instances through textual [[string metrics]] and semantic knowledge. These techniques are similar to those used in [[information integration]] employing [[string metrics]] from [[open source]] similarity libraries.

==See also==
*[[ontology mapping]]
*[[data integration]]

[[Category:Ontology (information science)]]
[[Category:Data management]]</text>
      <sha1>l071ao967f9i89d7g4fg5w4zzfko9qr</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed concurrency control</title>
    <ns>0</ns>
    <id>13329119</id>
    <revision>
      <id>737010418</id>
      <parentid>737010416</parentid>
      <timestamp>2016-08-31T06:30:15Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/114.143.229.206|114.143.229.206]] to older version. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2745209) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4829" xml:space="preserve">{{POV|Commitment ordering|date=November 2011}}
'''Distributed concurrency control''' is the [[concurrency control]] of a system [[Distributed computing|distributed]] over a [[computer network]] ([[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]). 

In ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') distributed concurrency control refers primarily to the concurrency control of a [[distributed database]]. It also refers to the concurrency control in a multidatabase (and other multi-[[transactional object]]) environment (e.g., [[federated database]], [[grid computing]], and [[cloud computing]] environments. A major goal for distributed concurrency control is distributed [[serializability]] (or [[global serializability]] for multidatabase systems). Distributed concurrency control poses special challenges beyond centralized one, primarily due to communication and computer [[latency (engineering)|latency]]. It often requires special techniques, like [[distributed lock manager]] over fast [[computer network]]s with low latency, like [[switched fabric]] (e.g., [[InfiniBand]]). [[commitment ordering]] (or commit ordering) is a general serializability technique that achieves distributed serializability (and global serializability in particular) effectively on a large scale, without concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets), and thus without performance penalties that are typical to other serializability techniques ([[#Raz92|Raz 1992]]).

The most common distributed concurrency control technique is ''strong strict two-phase locking'' ([[two phase locking#Strong strict two-phase locking|SS2PL]], also named ''rigorousness''), which is also a common centralized concurrency control technique. SS2PL provides both the ''serializability'', ''[[schedule (computer science)#Strict|strictness]]'', and ''commitment ordering'' properties. Strictness, a special case of recoverability, is utilized for effective recovery from failure, and commitment ordering allows participating in a general solution for global serializability. For large-scale distribution and complex transactions, distributed locking's typical heavy performance penalty (due to delays, latency) can be saved by using the [[atomic commitment]] protocol, which is needed in a distributed database for (distributed) transactions' [[Atomicity (database systems)|atomicity]] (e.g., [[Two-phase commit protocol|two-phase commit]], or a simpler one in a reliable system), together with some local commitment ordering variant (e.g., local [[two phase locking#Strong strict two-phase locking|SS2PL]]) instead of distributed locking, to achieve global serializability in the entire system. All the commitment ordering theoretical results are applicable whenever atomic commitment is utilized over partitioned, distributed recoverable (transactional) data, including automatic ''distributed deadlock'' resolution. Such technique can be utilized also for a large-scale [[parallel database]], where a single large database, residing on many nodes and using a distributed lock manager, is replaced with a (homogeneous) multidatabase, comprising many relatively small databases (loosely defined; any process that supports transactions over partitioned data and participates in atomic commitment complies), fitting each into a single node, and using commitment ordering (e.g., SS2PL, strict CO) together with some appropriate atomic commitment protocol (without using a distributed lock manager).

==See also==
*[[Global concurrency control]]

==References==
*&lt;cite id=Bern87&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 &lt;/cite&gt;
*&lt;cite id=Weikum01&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8 &lt;/cite&gt;
*&lt;cite id=Raz92&gt;[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ''Proceedings of the Eighteenth International Conference on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) &lt;/cite&gt;

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]</text>
      <sha1>1pdwwmspfghsdal7bx94kflpubh9x1p</sha1>
    </revision>
  </page>
  <page>
    <title>Core data integration</title>
    <ns>0</ns>
    <id>14124151</id>
    <revision>
      <id>607596684</id>
      <parentid>607596617</parentid>
      <timestamp>2014-05-08T07:49:17Z</timestamp>
      <contributor>
        <username>Doozler</username>
        <id>20649805</id>
      </contributor>
      <minor />
      <comment>finished</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1440" xml:space="preserve">{{Citations missing|date=December 2007}}
'''Core data integration''' is the use of [[data integration]] technology for a significant, centrally planned and managed IT initiative within a company. Examples of core data integration initiatives could include:

* ETL ([[Extract, transform, load]]) implementations
* EAI ([[Enterprise Application Integration]]) implementations
* SOA ([[Service-Oriented Architecture]]) implementations
* ESB ([[Enterprise Service Bus]]) implementations

Core data integrations are often designed to be enterprise-wide integration solutions. They may be designed to provide a data abstraction layer, which in turn will be used by individual core data integration implementations, such as ETL servers or applications integrated through EAI.

Because it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization, IT engineers and even business users create [[edge data integration]], using technology that may be incompatible with that used at the core. In contrast to a core data integration, an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline. 

==See also==
* [[data integration]]
* [[edge data integration]]

== References ==
* http://searchsoa.techtarget.com/tip/0,289483,sid26_gci1171085,00.html* 
* 

[[Category:Data management]]</text>
      <sha1>r07klvtdlkqsp5wmx035bsg1rczcwbg</sha1>
    </revision>
  </page>
  <page>
    <title>Edge data integration</title>
    <ns>0</ns>
    <id>14124901</id>
    <revision>
      <id>577978011</id>
      <parentid>416996951</parentid>
      <timestamp>2013-10-20T14:16:59Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor />
      <comment>General Fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1461" xml:space="preserve">{{Refimprove|date=January 2008}}
An '''edge data integration''' is an implementation of [[data integration]] technology undertaken in an ad hoc or tactical fashion. This is also sometimes referred to as point-to-point integration because it connects two types of data directly to serve a narrow purpose. Many edge integrations, and actually the vast majority of all data integration, involves hand-coded scripts. Some may take the form of [[Business Mashups]] (web application hybrids), [[Rich Internet application]]s, or other browser-based models that take advantage of [[Web 2.0]] technologies to combine data in a Web browser.

Examples of edge data integration projects might be:

* extracting a list of customers from a host [[Sales Force Automation]] application and writing the results to an [[Microsoft Excel|Excel]] spreadsheet
* creating a script-driven framework for managing [[RSS]] feeds
* combining data from a weather Web site, a shipping company's Web site, and a company's internal logistics database to track shipments and estimated arrival times of packages

It has been claimed that edge data integration do not typically require large budgets and centrally managed technologies, which is in contrast to a [[core data integration]].

== See also ==
* [[core data integration]]
* [[Business Mashups]]
* [[Rich Internet application]]
* [[Web 2.0]]
* [[Yahoo! Pipes]]
* [[Microsoft Popfly]]
*[[IBM Mashup Center]]

[[Category:Data management]]</text>
      <sha1>jnpgnxqzqb9o5sjdugcvnx8yccslvst</sha1>
    </revision>
  </page>
  <page>
    <title>British Oceanographic Data Centre</title>
    <ns>0</ns>
    <id>19075690</id>
    <revision>
      <id>737252362</id>
      <parentid>736857128</parentid>
      <timestamp>2016-09-01T16:29:26Z</timestamp>
      <contributor>
        <username>ArmbrustBot</username>
        <id>18663209</id>
      </contributor>
      <minor />
      <comment>/* External links */re-categorisation per [[WP:CFDS|CFDS]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8523" xml:space="preserve">{{Infobox organization
|name         = British Oceanographic Data Centre
|image        = Bodc logo.jpg
|image_border = 
|size         = 150px
|alt          = British Oceanographic Data Centre
|caption      = 
|map          =
|msize        = 
|malt         = 
|mcaption     = 
|abbreviation = 
|motto        = 
|formation    = 1969
|extinction   = 
|type         = 
|status       = 
|purpose      = 
|headquarters = 
|location     = [[Liverpool]], [[UK]]&lt;br&gt;
[[UK postcodes|L3 5DA]]
|region_served =
|membership   = 
|language     =
|leader_title = Head of BODC
|leader_name  = Dr Graham Allen
|main_organ   = 
|parent_organization = [[Natural Environment Research Council]] (NERC)
|affiliations = 
|num_staff    = approx. 500
|num_volunteers =
|budget       = 
|website      = {{URL|http://www.bodc.ac.uk/}}
|remarks      =
}}
The '''British Oceanographic Data Centre''' ('''BODC''') is a national facility for looking after and distributing [[data]] about the [[marine (ocean)|marine]] environment. BODC is the designated marine science data centre for the [[UK]] and part of the [[Natural Environment Research Council]] (NERC). The centre provides a resource for science, education and industry, as well as the general public. BODC is hosted by the [[National Oceanography Centre]] (NOC) &#8212; primarily at its facility in [[Liverpool]], with small number of its staff in [[Southampton]].

[[File:Bidston Observatory.jpg|250px|right|thumb|Bidston Observatory, home of BODC from 1975 to 2004.]]
[[File:Joseph Proudman Building.jpg|250px|right|thumb|Joseph Proudman Building, Liverpool.]]

== History ==
The origins of BODC go back to 1969 when NERC created the '''British Oceanographic Data Service''' ('''BODS'''). Located at the National Institute of [[Oceanography]], [[Wormley, Surrey|Wormley]] in [[Surrey]], its purpose was to: 
* Act as the UK's National Oceanographic Data Centre
* Participate in the international exchange of data as part of the [[Intergovernmental Oceanographic Commission]] (IOC) network of national data centres

In 1975 BODS was transferred to [[Bidston]] Observatory on the [[Wirral Peninsula|Wirral]], near Liverpool, as part of the newly formed Institute of Oceanographic Sciences. The following year BODS became the Marine Information and Advisory Service (MIAS)[http://www.soton.ac.uk/library/about/nol/mias.html]. Its primary activity was to manage the data collected from weather ships, [[oil rigs]] and [[Data Buoys|data buoys]].
The data banking component of MIAS was restructured to form BODC in April 1989. Its mission was to 'operate as a world-class data centre in support of UK marine science'. BODC pioneered a start to finish approach to marine data management. This involved:
* Assisting in the collection of data at sea
* Quality control of data
* Assembling the data for use by the scientists
* The publication of data sets on [[CD-ROM]]
In December 2004, BODC moved to the purpose-built [[Joseph Proudman]] Building on the campus of the [[University of Liverpool]]. A small number of its staff are based in the [[National Oceanography Centre]] (NOC), Southampton.

== Aims ==
* Work alongside scientists during marine research projects
* Provide quality control and archiving of oceanographic data
* Maintain an online source of information and improve public access to marine data
* Provide innovative marine data products

== National role ==
[[File:Current meter inventory.jpg|250px|right|thumb|BODC [[current meter]] data holdings from around the UK.]]
BODC is one of six designated data centres that manage NERC's environmental data and has a number of national roles and responsibilities:
* Performing data management for NERC-funded marine projects
* Maintaining and developing its archive of marine data, the '''National Oceanographic Database''' ('''NODB''')
* Managing, checking and archiving data from [[tide gauge]]s around the UK coast for the [[UK National Tide Gauge Network|National Tide Gauge Network]], which aims to obtain high quality [[tidal]] information and to provide warning of possible flooding of coastal areas around the British Isles. This  is part of the [[National Tidal and Sea Level Facility|National Tidal &amp; Sea Level Facility]] (NTSLF)
* Hosting the Marine Environmental Data and Information Network ([http://www.oceannet.org/ MEDIN])
* Working in partnership with other NERC marine research centres:
** [[British Antarctic Survey]] (BAS)
** [[National Oceanography Centre]] (NOC), Liverpool, formerly [[Proudman Oceanographic Laboratory]] (POL)
** [[National Oceanography Centre]] (NOC), Southampton
** [[Plymouth Marine Laboratory]] (PML)
** [[Scottish Association for Marine Science]] (SAMS)
** [[Sea Mammal Research Unit]] (SMRU)

== International role ==
BODC's international roles and responsibilities include:
* Contributing to the [[International Council for the Exploration of the Sea]] (ICES) Marine Data Management
* Creating, maintaining and publishing the [[General Bathymetric Chart of the Oceans]] (GEBCO) Digital Atlas
* BODC is one of over 60 national oceanographic data centres that form part of the IOC [[International Oceanographic Data and Information Exchange]] (IODE)

==Projects and initiatives==
The following are a selection of the projects that BODC is or has been involved with:
:[[Image:RAPID mooring.JPG|250px|right|thumb|Servicing of a RAPID mooring.]]
* '''Atlantic Meridional Transect (AMT)'''
:The AMT programme [http://www.bodc.ac.uk/projects/uk/amt/] undertook a twice yearly [[transect]] between the UK and the [[Falkland Islands]] to study the factors determining the [[ecological]] and [[biogeochemical]] variability in the [[planktonic]] [[ecosystems]].
* '''Autosub Under Ice (AUI)'''
:The AUI programme [http://www.bodc.ac.uk/projects/uk/aui/] investigated the role of sub-ice shelf processes in the [[climate]] system. The marine environment beneath floating [[ice shelves]] was explored using Autosub, an [[Autonomous_underwater_vehicle|AUV]].
* '''Marine Productivity (MarProd)'''
:MarProd [http://www.bodc.ac.uk/projects/uk/marprod/] helped to develop coupled [[Computer simulation|models]] and observation systems for the [[pelagic]] ecosystem, with emphasis on the physical factors affecting [[zooplankton]] dynamics.
* '''Rapid Climate Change (RAPID)'''
:The RAPID programme [http://www.bodc.ac.uk/projects/uk/rapid/] aimed to improve understanding of the causes of sudden changes in the Earth's climate.
* '''Ocean Margin Exchange (OMEX)'''
:The OMEX project [http://www.bodc.ac.uk/projects/european/omex/] studied, measured and modelled the physical, chemical and biological processes and fluxes at the ocean margin - the interface between the open [[Atlantic ocean]] and the European [[continental shelf]].
* '''SeaDataNet'''
:[[SeaDataNet]] [http://www.bodc.ac.uk/projects/european/seadatanet/] aims to develop a [[standardised]], distributed system providing transparent access to marine data sets and data products from countries in and around [[Europe]].
*'''System of Industry Metocean data for the Offshore and Research Communities (SIMORC)'''
:SIMORC [http://www.bodc.ac.uk/projects/european/simorc/] aimed to create a central index and database of [[metocean]] data sets collected globally by the oil and gas industry.
*'''Vocabulary Server'''
:BODC operates the NERC Vocabulary Server Web Service [http://www.bodc.ac.uk/products/web_services/vocab/], which provides access to [[Controlled_vocabulary|controlled vocabularies]] of relevance to the scientific community.

== External links ==
* [http://www.bodc.ac.uk BODC homepage]
* [http://www.bodc.ac.uk/about/news_and_events/ BODC News and events]
* [http://www.nerc.ac.uk Natural Environment Research Council (NERC) homepage]
* [http://www.nerc.ac.uk/research/sites/data/ NERC Data centres]
* [http://ndg.nerc.ac.uk/discovery NERC Data Discovery Service]
* [http://www.ntslf.org/ National Tidal and Sea Level Facility (NTSLF)]

{{coord|53|24|27.5|N|2|58|8.2|W|type:landmark|display=title}}

[[Category:Oceanographic organizations]]
[[Category:Scientific organisations based in the United Kingdom]]
[[Category:Data management]]
[[Category:Oceanography]]
[[Category:Marine biology]]
[[Category:Marine geology]]
[[Category:Environmental science]]
[[Category:Environment of the United Kingdom]]
[[Category:Public bodies and task forces of the United Kingdom government]]
[[Category:1969 establishments in the United Kingdom]]
[[Category:Scientific organizations established in 1969]]
[[Category:Organisations based in Liverpool]]</text>
      <sha1>ckekksjb207xl82b60r8buh9db5jv83</sha1>
    </revision>
  </page>
  <page>
    <title>SIGMOD Edgar F. Codd Innovations Award</title>
    <ns>0</ns>
    <id>18927887</id>
    <revision>
      <id>723952493</id>
      <parentid>718924736</parentid>
      <timestamp>2016-06-06T07:32:52Z</timestamp>
      <contributor>
        <ip>84.171.123.58</ip>
      </contributor>
      <comment>Added 2016 award details</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2151" xml:space="preserve">The [[Association for Computing Machinery|ACM]] '''[[SIGMOD]] [[Edgar F. Codd]] Innovations Award''' is a lifetime research achievement award given by the ACM Special Interest Group on Management of Data, at its yearly flagship conference (also called SIGMOD). According to its homepage, it is given "for innovative and highly significant contributions of enduring value to the development, understanding, or use of database systems and databases".&lt;ref&gt;http://www.sigmod.org/sigmod-awards&lt;/ref&gt; The award has been given since 1992.

== Recipients ==
{| class="wikitable"
! Year
! Name
|-
|2016
|[[Gerhard Weikum]]
|-
| 2015
| [[Laura M. Haas]]&lt;ref&gt;[http://www.sigmod.org/all-news/dr.-laura-haas-is-the-recipient-of-the-2015-sigmod-edgar-f.-codd-innovation-award Dr. Laura Haas is the recipient of the 2015 SIGMOD Edgar F. Codd Innovation Award], [[SIGMOD]], retrieved 2015-06-21.&lt;/ref&gt;
|-
| 2014
| [[Martin L. Kersten]]&lt;ref&gt;{{cite web |url=https://www.cwi.nl/news/2014/international-innovation-award-big-data-research-martin-kersten |title=International innovation award to Martin Kersten |date=26 June 2014 |website=CWI Amsterdam |accessdate=26 June 2014}}&lt;/ref&gt;
|-
| 2013
| [[Stefano Ceri]]
|-
| 2012
| Bruce Lindsay
|-
| 2011
| [[Surajit Chaudhuri]]
|-
| 2010
| [http://www.hpl.hp.com/people/umesh_dayal/ Umeshwar Dayal]
|-
| 2009
| [[Masaru Kitsuregawa]]
|-
| 2008
| [[Moshe Y. Vardi]]
|-
| 2007
| [[Jennifer Widom]]
|-
| 2006
| [[Jeffrey D. Ullman]]
|-
| 2005
| Michael Carey
|-
| 2004
| [[Ronald Fagin]]
|-
| 2003
| [[Don Chamberlin]]
|-
| 2002
| [[Patricia Selinger]]
|-
| 2001
| [[Rudolf Bayer]]
|-
| 2000
| [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]]
|-
| 1999
| [[Hector Garcia-Molina]]
|-
| 1998
| [[Serge Abiteboul]]
|-
| 1997
| [[David Maier]]
|-
| 1996
| [[C. Mohan]]
|-
| 1995
| [[David DeWitt]]
|-
| 1994
| [[Phil Bernstein|Philip Bernstein]]
|-
| 1993
| [[Jim Gray (computer scientist)|Jim Gray]]
|-
| 1992
| [[Michael Stonebraker]]
|}

== References ==
&lt;references/&gt;

[[Category:Association for Computing Machinery]]
[[Category:Awards established in 1992]]
[[Category:Computer science awards]]
[[Category:Data management]]</text>
      <sha1>gyhxxudu8tb7v1jfnr60itvanlff1x3</sha1>
    </revision>
  </page>
  <page>
    <title>Locks with ordered sharing</title>
    <ns>0</ns>
    <id>21064035</id>
    <revision>
      <id>702144234</id>
      <parentid>701929909</parentid>
      <timestamp>2016-01-28T18:54:44Z</timestamp>
      <contributor>
        <username>G&#252;nniX</username>
        <id>237572</id>
      </contributor>
      <minor />
      <comment>/* References */ISSN using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="853" xml:space="preserve">In [[databases]] and [[transaction processing]] the term '''Locks with ordered sharing''' comprises several variants of the ''[[Two phase locking]]'' (2PL) [[concurrency control]] protocol generated by changing the blocking semantics of locks upon [[Serializability#View and conflict serializability|conflicts]]. One variant is identical to [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering (SCO)]].

==References==

*D. Agrawal, A. El Abbadi, A. E. Lang: [http://portal.acm.org/citation.cfm?id=627615   ''The Performance of Protocols Based on Locks with Ordered Sharing''], IEEE Transactions on Knowledge and Data Engineering, Volume 6, Issue 5, October 1994, PP. 805 &#8211; 818, {{ISSN|1041-4347}}

[[Category:Data management]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]


{{database-stub}}</text>
      <sha1>2rqnwxjqg5tmktugripasqjda3xmfqw</sha1>
    </revision>
  </page>
  <page>
    <title>Write&#8211;read conflict</title>
    <ns>0</ns>
    <id>217741</id>
    <revision>
      <id>731667482</id>
      <parentid>543696786</parentid>
      <timestamp>2016-07-26T19:37:21Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>cap, punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1238" xml:space="preserve">In [[computer science]], in the field of [[database]]s, '''write&#8211;read conflict''', also known as '''reading uncommitted data''', is a computational anomaly associated with interleaved execution of transactions.

Given a schedule S

:&lt;math&gt;S = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;  \\
W(A) &amp; \\
 &amp; R(A) \\
 &amp; W(A)\\
 &amp; R(B) \\
 &amp; W(B) \\
 &amp; Com. \\
R(B) &amp; \\
W(B) &amp; \\
Com. &amp; \end{bmatrix}&lt;/math&gt;

T2 could read a database object A, modified by T1 which hasn't committed. This is a '''''dirty read'''''.

T1 may write some value into A which makes the database inconsistent.  It is possible that interleaved execution can expose this inconsistency and lead to inconsistent final database state, violating [[ACID]] rules.

[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T2 out from performing a Read/Write on A.  Note however that [[Strict two-phase locking|Strict 2PL]] can have a number of drawbacks, such as the possibility of [[deadlock]]s.

== See also ==

* [[Concurrency control]]
* [[Read&#8211;write conflict]]
* [[Write&#8211;write conflict]]

==References==
{{reflist}}
{{Unreferenced|date=August 2009}}

{{DEFAULTSORT:Write-Read Conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>eogg863188kbw0nrr910ydw1nnu6fz2</sha1>
    </revision>
  </page>
  <page>
    <title>Data aggregation</title>
    <ns>0</ns>
    <id>10186403</id>
    <revision>
      <id>754578731</id>
      <parentid>753457884</parentid>
      <timestamp>2016-12-13T11:58:26Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>/* References */ new cat [[Category:Data laws]], I've added it here because of the legal implications section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7000" xml:space="preserve">'''Data aggregation''' is the compiling of [[information]] from [[databases]] with intent to prepare combined datasets for [[data processing]].&lt;ref&gt;{{cite journal|author1=Stanley, Jay  |author2=Steinhardt, Barry|title=Bigger Monster, Weaker Chains: The Growth of an American Surveillance Society|publisher=American Civil Liberties Union|date=January 2003}}&lt;/ref&gt;

==Description==
The source information for data aggregation may originate from public records and [[criminal]] databases. The information is packaged into aggregate reports and then sold to [[business]]es, as well as to [[Local government|local]], [[State government|state]], and government agencies. This information can also be useful for [[marketing]] purposes. In the United States, many data brokers' activities fall under the [[Fair Credit Reporting Act]] (FCRA) which regulates [[Credit bureau|consumer reporting agencies]]. The agencies then gather and package personal information into [[consumer]] reports that are sold to [[creditor]]s, [[employer]]s, [[insurer]]s, and other businesses.

Various reports of information are provided by database aggregators. Individuals may request their own consumer reports which contain basic [[biographical]] information such as name, date of birth, current address, and phone number. Employee [[background check]] reports, which contain highly detailed information such as past addresses and length of residence, [[professional]] [[Licensure|licenses]], and criminal history, may be requested by eligible and qualified third parties. Not only can this data be used in employee background checks, but it may also be used to make decisions about insurance coverage, pricing, and law enforcement. [[Privacy]] activists argue that database aggregators can provide erroneous information.&lt;ref&gt;{{cite web|url=http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |title=Data Aggregators: A Study of Data Quality and Responsiveness |author1=Pierce, Deborah |author2=Ackerman, Linda |publisher=Privacyactivism.org |date=2005-05-19 |accessdate=2007-04-02 |archiveurl=https://web.archive.org/web/20070319220412/http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |archivedate=2007-03-19 |deadurl=yes |df= }}&lt;/ref&gt;

==Role of the Internet==
The potential of the [[Internet]] to consolidate and manipulate information has a new application in data aggregation, also known as ''screen scraping''. The Internet gives users the opportunity to consolidate their [[username]]s and [[password]]s, or PINs. Such consolidation enables consumers to access a wide variety of PIN-protected [[website]]s containing personal information by using one master PIN on a single website. Online account providers include [[financial institution]]s, [[stockbroker]]s, [[airline]] and frequent flyer and other reward programs, and [[e-mail]] accounts. Data aggregators can gather account or other information from designated websites by using account holders' PINs, and then making the users' account information available to them at a single website operated by the aggregator at an account holder's request. Aggregation services may be offered on a standalone basis or in conjunction with other financial services, such as [[portfolio (finance)|portfolio]] tracking and [[Bill (payment)|bill]] payment provided by a specialized website, or as an additional service to augment the online presence of an enterprise established beyond the virtual world. Many established companies with an Internet presence appear to recognize the value of offering an aggregation service to enhance other web-based services and attract visitors. Offering a data aggregation service to a website may be attractive because of the potential that it will frequently draw users of the service to the hosting website.

==Local business data aggregation==
When it comes to compiling location information on local businesses, there are several major data aggregators that collect information such as the business name, address, phone number, website, description and hours of operation. They then validate this information using various validation methods. Once the business information has been verified to be accurate, the data aggregators make it available to publishers like [[Google]] and [[Yelp]].

When Yelp, for example, goes to update their Yelp listings, they will pull data from these local data aggregators. Publishers take local business data from different sources and compare it to what they currently have in their database. They then update their database it with what information they deem accurate.

==Legal implications==
Financial institutions are concerned about the possibility of [[legal liability|liability]] arising from data aggregation activities, potential [[security]] problems, infringement on [[intellectual property]] rights and the possibility of diminishing traffic to the institution's website. The aggregator and financial institution may agree on a data feed arrangement activated on the customer's request, using an Open Financial Exchange (OFX) standard to request and deliver information to the site selected by the customer as the place from which they will view their account data. Agreements provide an opportunity for institutions to negotiate to protect their customers' interests and offer aggregators the opportunity to provide a robust service. Aggregators who agree with information providers to extract data without using an OFX standard may reach a lower level of consensual relationship; therefore, "screen scraping" may be used to obtain account data, but for business or other reasons, the aggregator may decide to obtain prior consent and negotiate the terms on which customer data is made available. "Screen scraping" without consent by the content provider has the advantage of allowing subscribers to view almost any and all accounts they happen to have opened anywhere on the Internet through one website.

==Outlook==
Over time, the transfer of large amounts of account data from the account provider to the aggregator's server could develop into a comprehensive profile of a user, detailing their banking and [[credit card]] transactions, balances, securities transactions and portfolios, and [[travel]] history and preferences. As the sensitivity to data protection considerations grows, it is likely there will be a considerable focus on the extent to which data aggregators may seek to use this data either for their own purposes or to share it on some basis with the operator of a website on which the service is offered or with other third parties.&lt;ref&gt;{{cite web|url=http://www.ffhsj.com/bancmail/bmarts/aba_art.htm|title=Scrape It, Scrub It and Show It: The Battle Over Data Aggregation|author1=Ledig, Robert H.  |author2=Vartanian, Thomas P.|publisher=Fried Frank|date=2002-09-11|accessdate=2007-04-02}}&lt;/ref&gt;

==References==
&lt;references /&gt;

{{DEFAULTSORT:Data Aggregator}}
[[Category:Data management]]
[[Category:Information privacy]]
[[Category:Data laws]]</text>
      <sha1>bft0ee8gkri8cqxvhjmvkix7vnydgy2</sha1>
    </revision>
  </page>
  <page>
    <title>Modular serializability</title>
    <ns>0</ns>
    <id>24906307</id>
    <redirect title="Global serializability" />
    <revision>
      <id>608565392</id>
      <parentid>323027401</parentid>
      <timestamp>2014-05-14T16:38:06Z</timestamp>
      <contributor>
        <username>Christian75</username>
        <id>1306352</id>
      </contributor>
      <comment>remove text from redirect: "Modular serializability"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="158" xml:space="preserve">#REDIRECT [[Global serializability]]

[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]</text>
      <sha1>er9lfkfzmrmxx4xmj3airzil7pjnala</sha1>
    </revision>
  </page>
  <page>
    <title>Operational data store</title>
    <ns>0</ns>
    <id>1740486</id>
    <revision>
      <id>742578922</id>
      <parentid>742571257</parentid>
      <timestamp>2016-10-04T14:51:51Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/202.74.243.46|202.74.243.46]] ([[User talk:202.74.243.46|talk]]) to last version by Mindmatrix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2507" xml:space="preserve">An '''operational data store''' (or "'''ODS'''") is a [[database]] designed to [[data integration|integrate data]] from multiple sources for additional operations on the data. Unlike a master data store, the data is not passed back to [[operational system]]s. It may be passed for further operations and to the [[data warehouse]] for reporting. 

Because the [[data]] originate from multiple sources, the integration often involves [[data cleaning|cleaning]], resolving redundancy and checking against [[business rule]]s for [[data integrity|integrity]].  An ODS is usually designed to contain low-level or atomic (indivisible) data (such as transactions and prices) with limited history that is captured "real time" or "near real time" as opposed to the much greater volumes of data stored in the data warehouse generally on a less-frequent basis.

==General Use==
The general purpose of an ODS is to integrate data from disparate source systems in a single structure, using [[data integration]] technologies like [[Data Virtualization|data virtualization]], [[Federated database system|data federation]], or [[Extract, transform, load|extract, transform, and load]]. This will allow operational access to the data for operational reporting, [[Master Data|master data or reference data management]].

An ODS is not a replacement or substitute for a [[data warehouse]] but in turn could become a source.

==See also==
* Some examples of ODS Architecture Patterns can be found in the article [[Architectural pattern (computer science)#Examples|Architecture Patterns]].

==Publications==
* {{cite book |last1=Inmon |first1=William |author1-link=Bill Inmon |title=Building the Operational Data Store |edition=2nd |location=New York |publisher=[[John Wiley &amp; Sons]] |year=1999 |isbn=0-471-32888-X}}

==External links==
*[[Architectural pattern (computer science)#Examples|ODS Architecture Patterns (EA Reference Architecture)]]
* [http://www.dmreview.com/issues/19980701/469-1.html Bill Inmon Information Management article on ODS]
* [http://www.information-management.com/issues/20000101/1749-1.html Bill Inmon Information Management article on the five classes of ODS]
* [http://www.intelsols.com/documents/Imhoff_10-02.pdf Claudia Imhoff Information Management article on ODS] PDF

{{Data warehouse}}

== See also ==
{{Wikipedia books|Enterprise Architecture}}
* [[Enterprise architecture]]

{{DEFAULTSORT:Operational Data Store}}
[[Category:Data management]]
[[Category:Data warehousing]]

{{database-stub}}</text>
      <sha1>m9slhhsw4itn7u6gobmj4y1grkatoud</sha1>
    </revision>
  </page>
  <page>
    <title>Database-centric architecture</title>
    <ns>0</ns>
    <id>13783336</id>
    <revision>
      <id>759154366</id>
      <parentid>712430992</parentid>
      <timestamp>2017-01-09T15:06:33Z</timestamp>
      <contributor>
        <username>Mkumba</username>
        <id>12450569</id>
      </contributor>
      <comment>added references to overall enterprise data centric architectures</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4232" xml:space="preserve">'''Database-centric Architecture''' or '''data-centric architecture''' has several distinct meanings, generally relating to [[software architecture]]s in which [[database]]s play a crucial role. Often this description is meant to contrast the design to an alternative approach. For example, the characterization of an architecture as "database-centric" may mean any combination of the following:

* using a standard, general-purpose [[relational database management system]], as opposed to customized in-[[Memory (computers)|memory]] or [[Computer file|file]]-based [[data structures]] and [[access method]]s. With the evolution of sophisticated [[Database management system|DBMS]] software, much of which is either free or included with the [[operating system]], application developers have become increasingly reliant on standard database tools, especially for the sake of [[rapid application development]].
* using dynamic, [[Table (database)|table]]-driven logic, as opposed to logic embodied in previously [[compiled]] [[Computer program|program]]s. The use of table-driven logic, i.e. behavior that is heavily dictated by the contents of a database, allows programs to be simpler and more flexible. This capability is a central feature of [[dynamic programming language]]s. See also [[control table]]s for tables that are normally coded and embedded within programs as [[data structures]] (i.e. not compiled statements) but could equally be read in from a [[flat file]], [[database]] or even retrieved from a [[spreadsheet]].
* using [[stored procedure]]s that run on [[database server]]s, as opposed to greater reliance on logic running in middle-tier [[application server]]s in a [[multi-tier architecture]]. The extent to which [[business logic]] should be placed at the back-end versus another tier is a subject of ongoing debate. For example, Toon Koppelaars presents a detailed analysis of alternative [[Oracle Database|Oracle-based]] architectures that vary in the placement of business logic, concluding that a database-centric approach has practical advantages from the standpoint of ease of development and maintainability.&lt;ref&gt;[https://web.archive.org/web/20060525094651/http://www.oracle.com/technology/pub/articles/odtug_award.pdf] A Database-centric approach to J2EE Application Development&lt;/ref&gt;
* using a shared database as the basis for communicating between [[Parallel computing|parallel processes]] in [[distributed computing]] applications, as opposed to direct [[inter-process communication]] via [[message passing]] functions and [[message-oriented middleware]]. A potential benefit of database-centric architecture in [[distributed application]]s is that it simplifies the design by utilizing DBMS-provided [[transaction processing]] and [[Index (database)|indexing]] to achieve a high degree of reliability, performance, and capacity.&lt;ref&gt;{{Citation |author=Lind P, Alm M |title=A database-centric virtual chemistry system |journal=J Chem Inf Model |volume=46 |issue=3 |pages=1034&#8211;9 |year=2006 |pmid=16711722 |doi=10.1021/ci050360b |postscript=. }}&lt;/ref&gt; For example, [[Base One]] describes a database-centric distributed computing architecture for [[Grid computing|grid]] and [[Computer cluster|cluster]] computing, and explains how this design provides enhanced security, fault-tolerance, and [[scalability]].&lt;ref&gt;[http://www.boic.com/dbgrid.htm Database-Centric Grid and Cluster Computing]&lt;/ref&gt;
* an overall enterprise architecture that favors shared data models&lt;ref&gt;{{Cite news|url=http://tdan.com/the-data-centric-revolution/18780|title=The Data Centric Revolution|newspaper=TDAN.com|access-date=2017-01-09}}&lt;/ref&gt; over allowing each application to have its own, idiosyncratic data model. 

==See also==
*[[Control table]]s
*[[:Category:Data-centric programming languages|Data-centric programming languages]]
*The [[data-driven programming]] paradigm, which makes the information used in a system the primary design driver.
*See the [http://datacentricmanifesto.org/ datacentricmanifesto.org] 

==References==
{{Reflist}}

{{Database}}

{{DEFAULTSORT:Database-Centric Architecture}}
[[Category:Software architecture]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]</text>
      <sha1>25uigivesogjv2r9yzzjy6lv7l4gchi</sha1>
    </revision>
  </page>
  <page>
    <title>Archive site</title>
    <ns>0</ns>
    <id>2643012</id>
    <revision>
      <id>732076298</id>
      <parentid>708346259</parentid>
      <timestamp>2016-07-29T12:55:37Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <comment>some [[WP:COPYEDIT|copy-editing]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4032" xml:space="preserve">{{refimprove|date=January 2016}}

In [[web archiving]], an '''archive site''' is a [[website]] that stores information on webpages from the past for anyone to view.

==Common techniques==
Two common techniques for archiving web sites are using a [[web crawler]] or soliciting user submissions:

# '''Using a [[web crawler]]''': By using a web crawler (e.g., the [[Internet Archive]]) the service will not depend on an active community for its content, and thereby can build a larger database faster. However, web crawlers are only able to index and archive information the public has chosen to post to the Internet, or that is available to be crawled, as web site developers and system administrators have the ability to block web crawlers from accessing [certain] web pages (using a [[Robots Exclusion Standard|robots.txt]]).
# '''User submissions''': While it can be difficult to start user submissions services due to potentially low rates of user submission, this system can yield some of the best results. By crawling web pages one is only able to obtain the information the public has chosen to post online; however, potential content providers may not bother to post certain information, assuming no one would be interested in it, because they lack a proper venue in which to post it, or because of copyright concerns.&lt;ref&gt;{{cite news|url=http://dlib.org/dlib/march12/niu/03niu1.html|journal=D-Lib Magazine| date=March&#8211;April 2012 |
 volume =18| number =3/4| title=An Overview of Web Archiving |author=Jinfang Niu,  University of South Florida|doi=10.1045/march2012-niu1}}&lt;/ref&gt; However, users who see someone wants their information may be more apt to submit it.

==Examples==

===Google Groups===
On February 12, 2001, [[Google]] acquired the [[usenet]] discussion group archives from [[Deja.com]] and turned it into their [[Google Groups]] service.&lt;ref&gt;{{cite web |title=Google Acquires Usenet Discussion Service and Significant Assets from Deja.com |work= |date=February 12, 2001 |url=https://googlepress.blogspot.com/2001/02/google-acquires-usenet-discussion.html }}&lt;/ref&gt; They allow users to search old discussions with Google's search technology, while still allowing users to post to the [[mailing list]]s.

===Internet Archive===
The [[Internet Archive]] is building a compendium of websites and [[digital media]]. Starting in 1996, the Archive has been employing a web crawler to build up their database. It is one of the best known archive sites.

===NBCUniversal Archives===
[[NBCUniversal Archives]] offer access to exclusive content from [[NBCUniversal]] and its subsidiaries. Their NBCUniversal Archives website provides easy viewing of past and recent news clips, and it is a prime example of a news archive.&lt;ref&gt;[http://www.nbcuniversalarchives.com/nbcuni/home.do NBCUniversal Archives]&lt;/ref&gt;

===Nextpoint===
[[Nextpoint]] offers an automated [[Cloud computing|cloud]]-based, [[Software as a service|SaaS]] for marketing, compliance, and litigation related needs including electronic discovery.

===PANDORA Archive===
PANDORA ([[Pandora Archive]]), founded in 1996 by the National Library of [[Australia]], stands for Preserving and Accessing Networked Documentary Resources of Australia, which encapsulates their mission. They provide a long-term catalog of select online publications and web sites authored by Australians or that are of an Australian topic. They employ their PANDAS (PANDORA Digital Archiving System) when building their catalog.

===textfiles.com===
[[textfiles.com]] is a large library of old text files maintained by [[Jason Scott Sadofsky]]. Its mission is to archive the old documents that had floated around the [[bulletin board systems]] (BBS) of his youth and to document other people's experiences on the bulletin board systems.

==See also==
* [[Internet Archive]]
* [[Pandora Archive]]
* [[WebCite]]
* [[Web archiving]]

== References ==
{{reflist}}

{{DEFAULTSORT:Archive Site}}
[[Category:Data management]]
[[Category:Online archives]]
[[Category:Web archiving initiatives]]</text>
      <sha1>3d1m8pwnum0r4ds48m5p7yom7p3zssz</sha1>
    </revision>
  </page>
  <page>
    <title>Virtual directory</title>
    <ns>0</ns>
    <id>943527</id>
    <revision>
      <id>675909033</id>
      <parentid>616993640</parentid>
      <timestamp>2015-08-13T13:36:42Z</timestamp>
      <contributor>
        <ip>50.205.49.193</ip>
      </contributor>
      <comment>/* Capabilities */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9367" xml:space="preserve">In [[computing]], the term '''virtual directory''' has a couple of meanings. It may simply designate (for example in [[Internet Information Services|IIS]]) a [[Folder (computing)|folder]] which appears in a [[Path (computing)|path]] but which is not actually a subfolder of the preceding folder in the path. However, this article will discuss the term in the context of [[directory service]]s and [[identity management]].

A virtual directory or '''virtual directory server''' in this context is a software layer that delivers a single access point for [[identity management]] applications and service platforms. A virtual directory operates as a high-performance, lightweight abstraction layer that resides between client applications and disparate types of identity-data repositories, such as proprietary and standard directories, databases, web services, and applications.

A virtual directory receives queries and directs them to the appropriate data sources by abstracting and virtualizing data. The virtual directory integrates identity data from multiple heterogeneous data stores and presents it as though it were coming from one source. This ability to reach into disparate repositories makes virtual directory technology ideal for consolidating data stored in a distributed environment. 

{{As of | 2011}}, virtual directory servers most commonly use the [[Lightweight Directory Access Protocol|LDAP]] protocol, but more sophisticated virtual directories can also support [[SQL]] as well as [[Directory Services Markup Language|DSML]] and [[Service Provisioning Markup Language|SPML]].

Industry experts have heralded the importance of the virtual directory in modernizing the identity infrastructure. According to Dave Kearns of Network World, "Virtualization is hot and a virtual directory is the building block, or foundation, you should be looking at for your next identity management project."&lt;ref&gt;{{cite web | url=http://www.networkworld.com/article/2305608/access-control/virtual-directory-finally-gains-recognition.html | title=Virtual directory finally gains recognition | publisher=NetworkWorld | date=7 August 2006 | accessdate=14 July 2014 | author=Kearns, Dave}}&lt;/ref&gt; In addition, Gartner analyst, Bob Blakley&lt;ref&gt;The Emerging Architecture of Identity Management, Bob Blakley, April 16, 2010.&lt;/ref&gt; said that virtual directories are playing an increasingly vital role. In his report, &#8220;The Emerging Architecture of Identity Management,&#8221; Blakley wrote: &#8220;In the first phase, production of identities will be separated from consumption of identities through the introduction of a virtual directory interface.&#8221;

==Capabilities==
Virtual directories can have some or all of the following capabilities:&lt;ref&gt;{{cite web|url=http://optimalidm.com/resources/blog/virtual-directory-server-2/|title=An Introduction To Virtual Directories|publisher=Optimal Idm|accessdate=15 July 2014}}&lt;/ref&gt;
* Aggregate identity data across sources to create a single point of access.
* Create high-availability for authoritative data stores.
* Act as identity firewall by preventing [[denial-of-service attack]]s on the primary data stores through an additional virtual layer.
* Support a common searchable namespace for centralized authentication.
* Present a unified virtual view of user information stored across multiple systems.
* Delegate authentication to backend sources through source-specific security means.
* Virtualize data sources to support migration from legacy data stores without modifying the applications that rely on them.
* Enrich identities with attributes pulled from multiple data stores, based on a link between user entries. 

Some advanced identity virtualization platforms can also:
* Enable application-specific, customized views of identity data without violating internal or external regulations governing identity data. Reveal contextual relationships between objects through hierarchical directory structures.
* Develop advanced correlation across diverse sources using correlation rules. 
* Build a global user identity by correlating unique user accounts across various data stores, and enrich identities with attributes pulled from multiple data stores, based on a link between user entries. 
* Enable constant data refresh for real-time updates through a persistent cache.

==Advantages ==
Virtual directories:
* Enable faster deployment because users do not need to add and sync additional application-specific data sources 
* Leverage existing identity infrastructure and security investments to deploy new services 
* Deliver high availability of data sources 
* Provide application-specific views of identity data which can help avoid the need to develop a master enterprise schema
* Allow a single view of identity data without violating internal or external regulations governing identity data
* Act as identity firewalls by preventing denial-of-service attacks on the primary data-stores and providing further security on access to sensitive data
* Can reflect changes made to authoritative sources in real-time
* Present a unified virtual view of user information from multiple systems so that it appears to reside in a single system
* Can secure all backend storage locations with a single security policy

==Disadvantages==
An original disadvantage is public perception of "push &amp; pull technologies" which is the general classification of "virtual directories" depending on the nature of their deployment. Virtual directories were initially designed and later deployed with "push technologies" in mind, which also contravened with [[privacy laws of the United States]]. This is no longer the case. There are, however, other disadvantages in the current technologies.

* The classical virtual directory based on proxy cannot modify underlying data structures or create new views based on the relationships of data from across multiple systems. So if an application requires a different structure, such as a flattened list of identities, or a deeper hierarchy for delegated administration, a virtual directory is limited. 
* Many virtual directories cannot correlate same-users across multiple diverse sources in the case of duplicate users
* Virtual directories without advanced caching technologies cannot scale to heterogeneous, high-volume environments.

==Sample terminology==
{{Overly detailed|section=yes|date=July 2014}}
* Unify metadata: Extract schemas from the local data source, map them to a common format, and link the same identities from different data silos based on a unique identifier.
* Namespace joining: Create a single large directory by bringing multiple directories together at the namespace level. For instance, if one directory has the namespace "ou=internal,dc=domain,dc=com" and a second directory has the namespace "ou=external,dc=domain,dc=com," then creating a virtual directory with both namespaces is an example of namespace joining.
* Identity joining: Enrich identities with attributes pulled from multiple data stores, based on a link between user entries.  For instance if the user joeuser exists in a directory as "cn=joeuser,ou=users" and in a database with a username of "joeuser" then the "joeuser" identity can be constructed from both the directory and the database.
* Data remapping: The translation of data inside of the virtual directory. For instance, mapping &#8220;uid&#8221; to &#8220;samaccountname,&#8221; so a client application that only supports a standard LDAP-compliant data source is able to search an Active Directory namespace, as well.
* Query routing: Route requests based on certain criteria, such as &#8220;write operations going to a master, while read operations are forwarded to replicas.&#8221;
* Identity routing: Virtual directories may support the routing of requests based on certain criteria (such as write operations going to a master while read operations being forwarded to replicas).
* Authoritative source: A "virtualized" data repository, such as a directory or database, that the virtual directory can trust for user data.
* Server groups: Group one or more servers containing the same data and functionality. A typical implementation is the multi-master, multi-replica environment in which replicas process "read" requests and are in one server group, while masters process "write" requests and are in another, so that servers are grouped by their response to external stimuli, even though all share the same data.

==Use cases==
The following are sample use cases of virtual directories:
* Integrating multiple directory namespaces to create a central enterprise directory.
* Supporting infrastructure integrations after mergers and acquisitions. 
* Centralizing identity storage across the infrastructure, making identity information available to applications through various protocols (including LDAP, JDBC, and web services). 
* Creating a single access point for [[Web Access Management|web access management]] (WAM) tools. 
* Enabling web [[single sign-on]] (SSO) across varied sources or domains.
* Supporting role-based, fine-grained authorization policies
* Enabling authentication across different security domains using each domain&#8217;s specific credential checking method.
* Improving secure access to information both inside and outside of the firewall.

==References==
&lt;references/&gt;

{{DEFAULTSORT:Virtual Directory}}
[[Category:Data management]]</text>
      <sha1>t7yrvn3k9mgt0mg713nbsgozkimougk</sha1>
    </revision>
  </page>
  <page>
    <title>Reference table</title>
    <ns>0</ns>
    <id>1785206</id>
    <revision>
      <id>731204525</id>
      <parentid>643262171</parentid>
      <timestamp>2016-07-23T19:28:42Z</timestamp>
      <contributor>
        <ip>31.19.67.84</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1628" xml:space="preserve">{{Unreferenced|auto=yes|date=December 2009}}

A '''reference table''' (or table of reference) may mean a set of references that an author may have cited or gained inspiration from whilst writing an article, similar to a [[bibliography]].

It can also mean an [[Table (information)|information table]] that is used as a quick and easy reference for things that are difficult to remember such as comparing [[Imperial unit|imperial]] with [[SI|metric]] measurements. This kind of data is known as [[reference data]].

In the context of [[database design]] a reference table is a table into which an [[enumeration|enumerated]] set of possible values of a certain field data type is divested. For example, in a [[relational model|relational database model]] of a warehouse the entity 'Item' may have a field called 'status' with a predefined set of values such as 'sold', 'reserved', 'out of stock'. In a purely designed database these values would be divested into an extra entity or Reference Table called 'status' in order to achieve [[database normalisation]]. The entity 'status' in this case has no true representative in the real world but rather would an exceptional case where the attribute of a certain database entity is divested into its own table. The advantage of doing this is that internal functionality and optional conditions within the database and the software which utilizes it are easier to modify and extend on that particular aspect.  Establishing an enterprise-wide view of reference tables is called [[master data management]].

{{DEFAULTSORT:Reference Table}}
[[Category:Data management]]

{{Publish-stub}}</text>
      <sha1>63awsp0wpwvpn87vrjwcajwult32iul</sha1>
    </revision>
  </page>
  <page>
    <title>Uniform information representation</title>
    <ns>0</ns>
    <id>1610862</id>
    <revision>
      <id>645177889</id>
      <parentid>645176641</parentid>
      <timestamp>2015-02-01T16:37:38Z</timestamp>
      <contributor>
        <username>Bhny</username>
        <id>285109</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1090" xml:space="preserve">{{unreferenced|date=December 2009}}

'''Uniform information representation'''  allows information from several realms or disciplines to be displayed and worked with as if it came from the same realm or discipline.  It takes information from a number of sources, which may have used different methodologies and metrics in their data collection, and builds a single large collection of information, where some records may be more complete than others across all fields of data

Uniform information representation is particularly important in the fields of [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), where different departments of a large organization may have collected information for different purposes, with different labels and units, until one department realized that data already collected by those other departments could be re-purposed for their own needs&#8212;saving the enterprise the effort and cost of re-collecting the same information.

{{DEFAULTSORT:Uniform Information Representation}}
[[Category:Data management]]

{{Comp-sci-stub}}</text>
      <sha1>f1lgsks1385u4n97tu1jj3qnap3r1h8</sha1>
    </revision>
  </page>
  <page>
    <title>Nonlinear medium</title>
    <ns>0</ns>
    <id>676568</id>
    <revision>
      <id>715774947</id>
      <parentid>681940813</parentid>
      <timestamp>2016-04-17T22:59:07Z</timestamp>
      <contributor>
        <username>DiscantX</username>
        <id>23278095</id>
      </contributor>
      <comment>/* top */ {{Distinguish|Non-linear media}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="455" xml:space="preserve">{{Distinguish|Non-linear media}}
{{Unreferenced|date=December 2009}}
A '''nonlinear medium''' is one which is intended to be accessed in a nonlinear fashion. It is the opposite of a [[Linear_medium|Linear Medium]]. 

Examples include:
* a [[hard drive]]
* a [[newspaper]]
* a [[phone book]]
* a [[dictionary]]

==See also==
*[[nonlinear]]
*[[linear medium]]
*[[Random access]]

{{DEFAULTSORT:Nonlinear Medium}}
[[Category:Data management]]


{{Tech-stub}}</text>
      <sha1>3kbgt07t2eovn9j22zfrgczdb1u1gck</sha1>
    </revision>
  </page>
  <page>
    <title>Photo recovery</title>
    <ns>0</ns>
    <id>24862923</id>
    <revision>
      <id>757502706</id>
      <parentid>684745204</parentid>
      <timestamp>2016-12-31T01:34:16Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>added [[Category:Photography]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6292" xml:space="preserve">{{Advert|date=August 2010}}

'''Photo recovery''' is the process of salvaging digital photographs from damaged, failed, corrupted, or inaccessible [[Computer data storage#Secondary storage|secondary storage]] media when it cannot be accessed normally. Photo Recovery can be considered a subset of
the overall [[Data Recovery]] field. 

Photo loss or deletion failures may be due to both hardware or software failures.

==Recovering data after hardware failure==
An excellent explanation of hardware failures is provided in the section for [[Data Recovery|data recovery]]. Typically, if your
drive or card is so badly damaged that your computer can not recognize that a drive/card has been connected, you
will need to consult a data recovery service provider.

==Recovering data after logical failure==
Logical Damage or the inability to view photos can occur due to many reasons. The most common reasons are:

# Deletion of photos.
# Corruption of boot sector of media.
# Corruption of [[file system]].
# [[Disk formatting]].
# Move or Copy errors.

=== Photo Recovery Using File Carving ===
The majority of photo recovery programs work by using a technique called [[file carving|file carving (data carving)]].
There are many different file carving techniques that are used to recover photos. Most of these techniques
fail in the presence of [[file system fragmentation]]. Simson Garfinkel showed that on average 16% of [[JPEG]]s are fragmented,&lt;ref name=garfinkel_dfrws2007&gt;[[Simson Garfinkel]], ''Carving Contiguous and Fragmented Files with Fast Object Validation'', in Proceedings of the 2007 digital forensics research workshop, DFRWS, Pittsburgh, PA, August 2007&lt;/ref&gt; which
means on average 16% of jpegs are recovered partially or appear corrupt when recovered using techniques that
can't handle fragmented photos.

==== Header-Footer Carving ====
In Header-Footer Carving, a recovery program attempts to recover photos based on the standard starting and ending byte
signature of the photo format. To take an example, all [[JPEG]]s always begin with the hex sequence "FFD8" and they must
end with the hex sequence "FFD9".

Header-Footer Carving cannot be used to recover fragmented photos, and fragmented
photos will appear to be partially recovered or corrupt if incorrect data is added. Header-Footer Carving, along
with Header-Size Carving, are by far the most common techniques for photo recovery. One of the first non-gui/console
based programs to use this technique is [[PhotoRec]].
Use of footers can often truncate a photo, as many JPEGs contain thumbnails as an embedded object.  If a file is terminated with a FFD9 it will be corrupted, unless nested FFD8/FFD9s are counted.

==== Header-Size Carving ====
In Header-Size Carving, a recovery program attempts to recover photos based on the standard starting byte signature of
the photo format, along with the size of the photo that is either derived or explicitly stated in the photo format.
To take an example all 24-bit Windows Bitmaps (*.bmp), begin with the letters "BM", and store the size of the file in
the header. Header-Size Carving cannot be used to recover fragmented photos, and fragmented photos will appear to be
partially recovered or corrupt if incorrect data is added.

==== File-Structure Based Carving ====
A more advanced form of carving, a recovery program attempts to recover photos based on detailed knowledge of the
structure rules of the photo format. This will enable a recovery program to identify when a photo is not complete or
fragmented, but more needs to be done to see if a fragmented photo can be recovered. This technique is rarely
used by most photo recovery programs.

==== Validated Carving ====
In validated carving, a decoder is used to detect any errors in recovery of a photo. More advanced forms of validated
carving occur when each part of the recovered photo is compared against the rest of the photo to see if it "fits"
visually. Validated carving is superb at detecting photos that are either fragmented or have parts over-written or
missing. Validated carving alone cannot be used to recover fragmented photos.&lt;ref name=pal_ieee_ip&gt;A. Pal and N. Memon, [http://digital-assembly.com/technology/research/pubs/ieee-trans-2006.pdf "Automated reassembly of file fragmented images using greedy algorithms"] in IEEE Transactions on Image processing, February 2006, pp 385393&lt;/ref&gt;

==== Log Carving ====
Log Carving occurs when a recovery program uses information left over in either file system structures or the log
to recover a deleted photo. For example, occasionally NTFS will store in the logs the exact location of where the
file was located prior to its deletion. A program using Log Carving will be able to then recover the photo. To be
sure about the quality of recovery, Validated Carving or File-Structure based carving should also be used to
validate the recovered photo.

==== Bi-Fragment Gap Carving ====
A fragmented photo recovery technique where a header and footer are identified and then all combinations of blocks
between the header and footer are validated to determine which combination results in the correct recovery of the
photo.&lt;ref name="garfinkel_dfrws2007"/&gt; This technique will only work if the file is fragmented into two parts.

==== SmartCarving ====
A process by which fragmented photos are recovered by looking at blocks on the disk and determining which block
is the best visual match for the photo being recovered. This is done in parallel for all blocks that are not part
of a recovered file.&lt;ref name=pal_dfrws2008&gt;A. Pal, T. Sencar, N. Memon, [http://digital-assembly.com/technology/research/pubs/dfrws2008.pdf "Detecting File Fragmentation Point Using Sequential Hypothesis Testing"] Digital Forensic Research Workshop, August 2008&lt;/ref&gt;

==References==
&lt;references /&gt;

==Further reading==
* Tanenbaum, A. &amp; Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
*[http://www.informationweek.com/news/windows/showArticle.jhtml?articleID=200000329 What To Do When Windows Vista Crashes: Little-Known Recovery Strategies], from Information Week

[[Category:Data recovery|photo]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Hard disk software|*]]
[[Category:Photography]]</text>
      <sha1>tfwvf3aj3nf2dhb4qdf2thtzug9721g</sha1>
    </revision>
  </page>
  <page>
    <title>Data validation and reconciliation</title>
    <ns>0</ns>
    <id>26683958</id>
    <revision>
      <id>740318391</id>
      <parentid>717520199</parentid>
      <timestamp>2016-09-20T09:32:41Z</timestamp>
      <contributor>
        <username>Nicolaufg</username>
        <id>27551214</id>
      </contributor>
      <minor />
      <comment>A sentence regarding the effect of "error remediation" seemed to be truncated. I added a couple of words that I think do match what should be there.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26483" xml:space="preserve">'''Industrial process data validation and reconciliation''', or more briefly, '''data validation and reconciliation (DVR)''', is a technology that uses process information and mathematical methods in order to automatically correct measurements in industrial processes. The use of DVR allows for extracting accurate and reliable information about the state of industry processes from raw measurement data and produces a single consistent set of data representing the most likely process operation.

==Models, data and measurement errors==
Industrial processes, for example chemical or thermodynamic processes in chemical plants, refineries, oil or gas production sites, or power plants, are often represented by two fundamental means:
# Models that express the general structure of the processes,
# Data that reflects the state of the processes at a given point in time.
Models can have different levels of detail, for example one can incorporate simple mass or compound conservation balances, or more advanced thermodynamic models including energy conservation laws. Mathematically the model can be expressed by a [[nonlinear system|nonlinear system of equations]] &lt;math&gt;F(y)=0\,&lt;/math&gt; in the variables &lt;math&gt;y=(y_1,\ldots,y_n)&lt;/math&gt;, which incorporates all the above-mentioned system constraints (for example the mass or heat balances around a unit). A variable could be the temperature or the pressure at a certain place in the plant.

===Error types===
&lt;gallery caption="Random and systematic errors" widths="300%" perrow="2" align="right"&gt;
File:Normal_no_bias.jpg|Normally distributed measurements without bias.
File:Normal_with_bias.jpg|Normally distributed measurements with bias.
&lt;/gallery&gt;
Data originates typically from [[measurements]] taken at different places throughout the industrial site, for example temperature, pressure, volumetric flow rate measurements etc. To understand the basic principles of DVR, it is important to first recognize that plant measurements are never 100% correct, i.e. raw measurement &lt;math&gt;y\,&lt;/math&gt; is not a solution of the nonlinear system &lt;math&gt;F(y)=0\,\!&lt;/math&gt;. When using measurements without correction to generate plant balances, it is common to have incoherencies. [[Observational error|Measurement errors]] can be categorized into two basic types:
# [[random error]]s due to intrinsic [[sensor]] [[accuracy]] and
# [[systematic errors]] (or gross errors) due to sensor [[calibration]] or faulty data transmission.

[[Random error]]s means that the measurement &lt;math&gt;y\,\!&lt;/math&gt; is a [[random variable]] with [[mean]] &lt;math&gt;y^*\,\!&lt;/math&gt;, where &lt;math&gt;y^*\,\!&lt;/math&gt; is the true value that is typically not known. A [[systematic error]] on the other hand is characterized by a measurement &lt;math&gt;y\,\!&lt;/math&gt; which is a random variable with [[mean]] &lt;math&gt;\bar{y}\,\!&lt;/math&gt;, which is not equal to the true value &lt;math&gt;y^*\,&lt;/math&gt;.  For ease in deriving and implementing an optimal estimation solution, and based on arguments that errors are the sum of many factors (so that the [[Central limit theorem]] has some effect), data reconciliation assumes these errors are [[normal distribution|normally distributed]].   

Other sources of errors when calculating plant balances include process faults such as leaks, unmodeled heat losses, incorrect physical properties or other physical parameters used in equations, and incorrect structure such as unmodeled bypass lines.  Other errors include unmodeled plant dynamics such as holdup changes, and other instabilities in plant operations that violate steady state (algebraic) models.  Additional dynamic errors arise when measurements and samples are not taken at the same time, especially lab analyses.  

The normal practice of using time averages for the data input partly reduces the dynamic problems.  However, that does not completely resolve timing inconsistencies for infrequently-sampled data like lab analyses.  

This use of average values, like a [[moving average]], acts as a [[low-pass filter]], so high frequency noise is mostly eliminated.   The result is that, in practice, data reconciliation is mainly making adjustments to correct systematic errors like biases.

===Necessity of removing measurement errors===
ISA-95 is the international standard for the integration of enterprise and control systems&lt;ref&gt;[http://www.isa-95.com/ "ISA-95: the international standard for the integration of enterprise and control systems"]. isa-95.com.&lt;/ref&gt; It asserts that:
&lt;blockquote&gt;Data reconciliation is a serious issue for enterprise-control integration. The data have to be valid to be useful for the enterprise system. The data must often be determined from physical measurements that have associated error factors. This must usually be converted into exact values for the enterprise system. This conversion may require manual, or intelligent reconciliation of the converted values [...].

Systems must be set up to ensure that accurate data are sent to production and from production. Inadvertent operator or clerical errors may result in too much production, too little production, the wrong production, incorrect inventory, or missing inventory.&lt;/blockquote&gt;

==History==
DVR has become more and more important due to industrial processes that are becoming more and more complex. DVR started in the early 1960s with applications aiming at closing [[mass balance|material balances]] in production processes where raw measurements were available for all [[variable (mathematics)|variables]].&lt;ref&gt;D.R. Kuehn, H. Davidson, ''Computer Control II. Mathematics of Control'', Chem. Eng. Process 57: 44&#8211;47, 1961.&lt;/ref&gt; At the same time the problem of [[systematic error|gross error]] identification and elimination has been presented.&lt;ref&gt;V. Vaclavek, ''Studies on System Engineering I. On the Application of the Calculus of the Observations of Calculations of Chemical Engineering Balances'', Coll. Czech Chem. Commun 34: 3653, 1968.&lt;/ref&gt; In the late 1960s and 1970s unmeasured variables were taken into account in the data reconciliation process.,&lt;ref&gt;V. Vaclavek, M. Loucka, ''Selection of Measurements Necessary to Achieve Multicomponent Mass Balances in Chemical Plant'', Chem. Eng. Sci. 31: 1199&#8211;1205, 1976.&lt;/ref&gt;&lt;ref name="Mah-Stanley-Downing-1976"&gt;[http://gregstanleyandassociates.com/ReconciliationRectificationProcessData-1976.pdf R.S.H. Mah, G.M. Stanley, D.W. Downing, ''Reconciliation and Rectification of Process Flow and Inventory Data'', Ind. &amp; Eng. Chem. Proc. Des. Dev. 15: 175&#8211;183, 1976.]&lt;/ref&gt; DVR also became more mature by considering general nonlinear equation systems coming from thermodynamic models.,&lt;ref&gt;J.C. Knepper, J.W. Gorman, ''Statistical Analysis of Constrained Data Sets'', AiChE Journal 26: 260&#8211;164, 1961.&lt;/ref&gt;
,&lt;ref name="Stanley-Mah-1977"&gt;[http://gregstanleyandassociates.com/AIChEJ-1977-EstimationInProcessNetworks.pdf G.M. Stanley and R.S.H. Mah, ''Estimation of Flows and Temperatures in Process Networks'', AIChE Journal 23: 642&#8211;650, 1977.]&lt;/ref&gt;
&lt;ref&gt;P. Joris, B. Kalitventzeff, ''Process measurements analysis and validation'', Proc. CEF&#8217;87: Use Comput. Chem. Eng., Italy, 41&#8211;46, 1987.&lt;/ref&gt; Quasi steady state dynamics for filtering and simultaneous parameter estimation over time were introduced in 1977 by Stanley and Mah.&lt;ref name="Stanley-Mah-1977"/&gt;  Dynamic DVR was formulated as a nonlinear optimization problem by Liebman et al. in 1992.&lt;ref&gt;M.J. Liebman, T.F. Edgar, L.S. Lasdon, ''Efficient Data Reconciliation and Estimation for Dynamic Processes Using Nonlinear Programming Techniques'', Computers Chem. Eng. 16: 963&#8211;986, 1992.&lt;/ref&gt;

==Data reconciliation==
Data reconciliation is a technique that targets at correcting measurement errors that are due to measurement noise, i.e. [[random error]]s. From a statistical point of view the main assumption is that no [[systematic errors]] exist in the set of measurements, since they may bias the reconciliation results and reduce the robustness of the reconciliation.

Given &lt;math&gt;n&lt;/math&gt; measurements &lt;math&gt;y_i&lt;/math&gt;, data reconciliation can mathematically be expressed as an [[optimization problem]] of the following form:

&lt;math&gt; \begin{align}
 \min_{x,y^*} &amp; \sum_{i=1}^n\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2 \\
\text{subject to  }      &amp; F(x,y^*)=0 \\
&amp; y_\min \le y^*\le y_\max\\
&amp; x_\min \le x\le x_\max,
\end{align}\,\!
&lt;/math&gt;

where
&lt;math&gt;y_i^*\,\!&lt;/math&gt; is the reconciled value of the &lt;math&gt;i&lt;/math&gt;-th measurement (&lt;math&gt;i=1,\ldots,n\,\!&lt;/math&gt;), &lt;math&gt;y_i\,\!&lt;/math&gt; is the measured value of the &lt;math&gt;i&lt;/math&gt;-th measurement (&lt;math&gt;i=1,\ldots,n\,\!&lt;/math&gt;), &lt;math&gt;x_j\,\!&lt;/math&gt; is the &lt;math&gt;j&lt;/math&gt;-th unmeasured variable (&lt;math&gt;j=1,\ldots,m\,\!&lt;/math&gt;),  and &lt;math&gt;\sigma_i\,\!&lt;/math&gt; is the standard deviation of the &lt;math&gt;i&lt;/math&gt;-th measurement (&lt;math&gt;i=1,\ldots,n\,\!&lt;/math&gt;),
&lt;math&gt;F(x,y^*)=0\,\!&lt;/math&gt; are the &lt;math&gt;p\,\!&lt;/math&gt; process equality constraints and
&lt;math&gt;x_{\min}, x_{\max}, y_{\min}, y_{\max}\,\!&lt;/math&gt; are the bounds on the measured and unmeasured variables.

The term &lt;math&gt;\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2\,\!&lt;/math&gt; is called the ''penalty'' of measurement ''i''. The objective function is the sum of the penalties, which will be denoted in the following by &lt;math&gt;f(y^*)=\sum_{i=1}^n\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2&lt;/math&gt;.

In other words, one wants to minimize the overall correction (measured in the least squares term) that is needed in order to satisfy the [[constraint (mathematics)|system constraints]]. Additionally, each least squares term is weighted by the [[standard deviation]]  of the corresponding measurement.

===Redundancy===
&lt;gallery caption="Sensor and topological redundancy" heights="150px" widths="225px" perrow="2" align="right"&gt;
File:sensor_red.jpg|Sensor redundancy arising from multiple sensors of the same quantity at the same time at the same place.
File:topological_red.jpg|Topological redundancy arising from model information, using the mass conservation constraint &lt;math&gt;a=b+c\,\!&lt;/math&gt;, for example one can calculate &lt;math&gt;c\,\!&lt;/math&gt;, when &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt; are known.
&lt;/gallery&gt;
Data reconciliation relies strongly on the concept of redundancy to correct the measurements as little as possible in order to satisfy the process constraints.  Here, redundancy is defined differently from [[Redundancy (information theory)|redundancy in information theory]].  Instead, redundancy arises from combining sensor data with the model (algebraic constraints), sometimes more specifically called "spatial redundancy",&lt;ref name="Stanley-Mah-1977"/&gt; "analytical redundancy", or "topological redundancy". 

Redundancy can be due to [[redundancy (engineering)|sensor redundancy]], where sensors are duplicated in order to have more than one measurement of the same quantity. Redundancy also arises when a single variable can be estimated in several independent ways from separate sets of measurements at a given time or time averaging period, using the algebraic constraints.  

Redundancy is linked to  the concept of [[observability]].  A variable (or system) is observable if the models and sensor measurements can be used to uniquely determine its value (system state).  A sensor is redundant if its removal causes no loss of observability.   Rigorous definitions of observability, calculability, and redundancy, along with criteria for determining it, were established by Stanley and Mah,&lt;ref name="Stanley-Mah-1981a"&gt;
[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981a-ObservabilityRedundancy.pdf Stanley G.M. and Mah, R.S.H., "Observability and Redundancy in Process Data Estimation, Chem. Engng. Sci. 36, 259 (1981)]&lt;/ref&gt; for these cases with set constraints such as algebraic equations and inequalities.    Next, we illustrate some special cases:

Topological redundancy  is intimately linked with the [[degrees of freedom (physics and chemistry)|degrees of freedom]] (&lt;math&gt;dof\,\!&lt;/math&gt;) of a mathematical system,&lt;ref name="vdi"&gt;VDI-Gesellschaft Energie und Umwelt, "Guidelines - VDI 2048 Blatt 1 - Uncertainties of measurements at acceptance tests for energy conversion and power plants - Fundamentals", ''[http://www.vdi.de/401.0.html Association of German Engineers]'', 2000.&lt;/ref&gt; i.e. the minimum number of pieces of information (i.e. measurements) that are required in order to calculate all of the system variables. For instance, in the example above the flow conservation requires that &lt;math&gt;a=b+c\,&lt;/math&gt;.  One needs to know the value of two of the 3 variables in order to calculate the third one. The degrees of freedom for the model in that case is equal to 2.  At least 2 measurements are needed to estimate all the variables, and 3 would be needed for redundancy.

When speaking about topological redundancy we have to distinguish between measured and unmeasured variables. In the following let us denote by &lt;math&gt;x\,\!&lt;/math&gt; the unmeasured variables and &lt;math&gt;y\,\!&lt;/math&gt; the measured variables. Then the system of the process constraints becomes &lt;math&gt;F(x,y)=0\,\!&lt;/math&gt;, which is a nonlinear system in &lt;math&gt;y\,\!&lt;/math&gt; and &lt;math&gt;x\,\!&lt;/math&gt;.
If the system &lt;math&gt;F(x,y)=0\,\!&lt;/math&gt; is calculable with the &lt;math&gt;n\,&lt;/math&gt; measurements given, then the level of topological redundancy is defined as &lt;math&gt;red= n - dof\,\!&lt;/math&gt;, i.e. the number of additional measurements that are at hand on top of those measurements which are required in order to just calculate the system. Another way of viewing the level of redundancy is to use the definition of &lt;math&gt;dof\,&lt;/math&gt;, which is the difference between the number of variables (measured and unmeasured) and the number of equations. Then one gets

:&lt;math&gt;\begin{align}
red= n - dof = n-(n+m-p) = p-m,
\end{align}&lt;/math&gt;

i.e. the redundancy is the difference between the number of equations &lt;math&gt;p\,&lt;/math&gt; and the number of unmeasured variables &lt;math&gt;m\,&lt;/math&gt;. The level of total redundancy is the sum of sensor redundancy and topological redundancy. We speak of positive redundancy if the system is calculable and the total redundancy is positive. One can see that the level of topological redundancy merely depends on the number of equations (the more equations the higher the redundancy) and the number of unmeasured variables (the more unmeasured variables, the lower the redundancy) and not on the number of measured variables. 

Simple counts of variables, equations, and measurements are inadequate for many systems, breaking down for several reasons: (a) Portions of a system might have redundancy, while others do not, and some portions might not even be possible to calculate, and  (b) Nonlinearities can lead to different conclusions at different operating points.  As an example, consider the following system with 4 streams and 2 units.

====Example of calculable and non-calculable systems====
&lt;gallery caption="Calculable and non-calculable systems" heights="150px" widths="225px" perrow="2" align="right"&gt;
File:calculable_system.jpg|Calculable system, from &lt;math&gt;d\,\!&lt;/math&gt; one can compute &lt;math&gt;c\,\!&lt;/math&gt;, and knowing &lt;math&gt;a\,\!&lt;/math&gt; yields &lt;math&gt;b\,\!&lt;/math&gt;.
File:uncalculable_system.jpg|non-calculable system, knowing &lt;math&gt;c\,\!&lt;/math&gt; does not give information about &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt;.
&lt;/gallery&gt;

We incorporate only flow conservation constraints and obtain &lt;math&gt;a+b=c\,\!&lt;/math&gt; and &lt;math&gt;c=d\,\!&lt;/math&gt;.  It is possible that the system &lt;math&gt;F(x,y)=0\,\!&lt;/math&gt; is not calculable, even though &lt;math&gt;p-m\ge 0\,\!&lt;/math&gt;.

If we have measurements for &lt;math&gt;c\,\!&lt;/math&gt; and &lt;math&gt;d\,\!&lt;/math&gt;, but not for &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt;, then the system cannot be calculated (knowing &lt;math&gt;c\,\!&lt;/math&gt; does not give information about &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt;). On the other hand, if &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;c\,\!&lt;/math&gt; are known, but not &lt;math&gt;b\,\!&lt;/math&gt; and &lt;math&gt;d\,\!&lt;/math&gt;, then the system can be calculated.

In 1981, observability and redundancy criteria were proven for these sorts of flow networks involving only mass and energy balance constraints.&lt;ref name="Stanley-Mah-1981b"&gt;[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981b-ObservabilityRedundancyProcessNetworks.pdf Stanley G.M., and Mah R.S.H., "Observability and Redundancy Classification in Process Networks", Chem. Engng. Sci. 36, 1941 (1981) ]&lt;/ref&gt;  After combining all the plant inputs and outputs into an "environment node",  loss of observability corresponds to cycles of unmeasured streams.  That is seen in the second case above, where streams a and b are in a cycle of unmeasured streams.  Redundancy classification follows, by testing for a path of unmeasured streams, since that would lead to an unmeasured cycle if the measurement was removed.  Measurements c and d are redundant in the second case above, even though part of the system is unobservable.

===Benefits===
Redundancy can be used as a source of information to cross-check and correct the measurements &lt;math&gt;y\,\!&lt;/math&gt; and increase their accuracy and precision: on the one hand they reconciled Further, the data reconciliation problem presented above also includes unmeasured variables &lt;math&gt;x\,\!&lt;/math&gt;. Based on information redundancy, estimates for these unmeasured variables can be calculated along with their accuracies. In industrial processes these unmeasured variables that data reconciliation provides are referred to as [[soft sensor]]s or virtual sensors, where hardware sensors are not installed.

==Data validation==
Data validation denotes all validation and verification actions before and after the reconciliation step.

===Data filtering===
Data filtering denotes the process of treating measured data such that the values become meaningful and lie within the range of expected values. Data filtering is necessary before the reconciliation process in order to increase robustness of the reconciliation step. There are several ways of data filtering, for example taking the [[average]] of several measured values over a well-defined time period.

===Result validation===
Result validation is the set of validation or verification actions taken after the reconciliation process and it takes into account measured and unmeasured variables as well as reconciled values. Result validation covers, but is not limited to, penalty analysis for determining the reliability of the reconciliation, or bound checks to ensure that the reconciled values lie in a certain range, e.g. the temperature has to be within some reasonable bounds.

===Gross error detection===
Result validation may include statistical tests to validate the reliability of the reconciled values, by checking whether [[systematic error|gross errors]] exist in the set of measured values. These tests can be for example
* the chi square test (global test)
* the individual test.

If no gross errors exist in the set of measured values, then each penalty term in the objective function is a [[normal distribution|random variable]] that is normally distributed with mean equal to 0 and variance equal to 1. By consequence, the objective function is a random variable which follows a [[chi-square distribution]], since it is the sum of the square of normally distributed random variables. Comparing the value of the objective function &lt;math&gt;f(y^*)\,\!&lt;/math&gt; with a given [[percentile]] &lt;math&gt;P_{\alpha}\,&lt;/math&gt; of the probability density function of a chi-square distribution (e.g. the 95th percentile for a 95% confidence) gives an indication of whether a gross error exists: If &lt;math&gt;f(y^*)\le P_{95}&lt;/math&gt;, then no gross errors exist with 95% probability. The chi square test gives only a rough indication about the existence of gross errors, and it is easy to conduct: one only has to compare the value of the objective function with the critical value of the chi square distribution.

The individual test compares each penalty term in the objective function with the critical values of the normal distribution. If the &lt;math&gt;i&lt;/math&gt;-th penalty term is outside the 95% confidence interval of the normal distribution, then there is reason to believe that this measurement has a gross error.

==Advanced data validation and reconciliation==
Advanced data validation and reconciliation (DVR) is an integrated approach of combining data reconciliation and data validation techniques, which is characterized by
* complex models incorporating besides mass balances also thermodynamics, momentum balances, equilibria constraints, hydrodynamics etc.
* gross error remediation techniques to ensure meaningfulness of the reconciled values,
* robust algorithms for solving the reconciliation problem.

===Thermodynamic models===
Simple models include mass balances only. When adding thermodynamic constraints such as [[First law of thermodynamics|energy balances]] to the model, its scope and the level of [[Data redundancy|redundancy]] increases. Indeed, as we have seen above, the level of redundancy is defined as &lt;math&gt;p-m&lt;/math&gt;, where &lt;math&gt;p&lt;/math&gt; is the number of equations. Including energy balances means adding equations to the system, which results in a higher level of redundancy (provided that enough measurements are available, or equivalently, not too many variables are unmeasured).

===Gross error remediation===
[[image:scheme reconciliation.jpg|thumb|350px|The workflow of an advanced data validation and reconciliation process.]]
Gross errors are measurement systematic errors that may [[bias]] the reconciliation results. Therefore it is important to identify and eliminate these gross errors from the reconciliation process. After the reconciliation [[statistical tests]] can be applied that indicate whether or not a gross error does exist somewhere in the set of measurements. These techniques of gross error remediation are based on two concepts:
* gross error elimination
* gross error relaxation.
Gross error elimination determines one measurement that is biased by a systematic error and discards this measurement from the data set. The determination of the measurement to be discarded is based on different kinds of penalty terms that express how much the measured values deviate from the reconciled values. Once the gross errors are detected they are discarded from the measurements and the reconciliation can be done without these faulty measurements that spoil the reconciliation process. If needed, the elimination is repeated until no gross error exists in the set of measurements.

Gross error relaxation targets at relaxing the estimate for the uncertainty of suspicious measurements so that the reconciled value is in the 95% confidence interval. Relaxation typically finds application when it is not possible to determine which measurement around one unit is responsible for the gross error (equivalence of gross errors). Then measurement uncertainties of the measurements involved are increased.

It is important to note that the remediation of gross errors reduces the quality of the reconciliation, either the redundancy decreases (elimination) or the uncertainty of the measured data increases (relaxation). Therefore it can only be applied when the initial level of redundancy is high enough to ensure that the data reconciliation can still be done (see Section 2,&lt;ref name="vdi" /&gt;).

===Workflow===
Advanced DVR solutions offer an integration of the techniques mentioned above:
# data acquisition from data historian, data base or manual inputs
# data validation and filtering of raw measurements
# data reconciliation of filtered measurements
# result verification
#* range check
#* gross error remediation (and go back to step 3)
# result storage (raw measurements together with reconciled values)
The result of an advanced DVR procedure is a coherent set of validated and reconciled process data.

==Applications==
DVR finds application mainly in industry sectors where either measurements are not accurate or even non-existing, like for example in the [[upstream (fossil-fuel industry)|upstream sector]] where [[flow measurement|flow meters]] are difficult or expensive to position (see &lt;ref&gt;P. Delava, E. Mar&#233;chal, B. Vrielynck, B. Kalitventzeff (1999), ''Modelling of a Crude Oil Distillation Unit in Term of Data Reconciliation with ASTM or TBP Curves as Direct Input &#8211; Application : Crude Oil Preheating Train'', Proceedings of ESCAPE-9 conference, Budapest, May 31-June 2, 1999, supplementary volume, p. 17-20.&lt;/ref&gt;); or where accurate data is of high importance, for example for security reasons in [[nuclear power plants]] (see &lt;ref&gt;M. Langenstein, J. Jansky, B. Laipple (2004), ''Finding Megawatts in nuclear power plants with process data validation'', Proceedings of ICONE12, Arlington, USA, April 25&#8211;29, 2004.&lt;/ref&gt;). Another field of application is [[Performance test (assessment)|performance and process monitoring]] (see &lt;ref&gt;Th. Amand, G. Heyen, B. Kalitventzeff, ''Plant Monitoring and Fault Detection: Synergy between Data Reconciliation and Principal Component Analysis'', Comp. and Chem, Eng. 25, p. 501-507, 2001.&lt;/ref&gt;) in oil refining or in the chemical industry.

As DVR enables to calculate estimates even for unmeasured variables in a reliable way, the German Engineering Society (VDI Gesellschaft Energie und Umwelt) has accepted the technology of DVR as a means to replace expensive sensors in the nuclear power industry (see VDI norm 2048,&lt;ref name="vdi" /&gt;).

==See also==
* [[Process simulation]]
* [[Pinch analysis]]
* [[Industrial processes]]
* [[Chemical engineering]]

==References==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes --&gt;
{{Reflist}}

* Alexander, Dave, Tannar, Dave &amp; Wasik, Larry "Mill Information System uses Dynamic Data Reconciliation for Accurate Energy Accounting" TAPPI Fall Conference 2007.[http://www.tappi.org/Downloads/Conference-Papers/2007/07EPE/07epe87.aspx]
* Rankin, J. &amp; Wasik, L. "Dynamic Data Reconciliation of Batch Pulping Processes (for On-Line Prediction)" PAPTAC Spring Conference 2009.
* S. Narasimhan, C. Jordache, ''Data reconciliation and gross error detection: an intelligent use of process data'', Golf Publishing Company, Houston, 2000.
* V. Veverka, F. Madron, 'Material and Energy Balancing in the Process Industries'', Elsevier Science BV, Amsterdam, 1997.
* J. Romagnoli, M.C. Sanchez, ''Data processing and reconciliation for chemical process operations'', Academic Press, 2000.


{{DEFAULTSORT:Data Validation And Reconciliation}}
[[Category:Data management]]</text>
      <sha1>twch10bd30efz88548j53hih0yl6m8c</sha1>
    </revision>
  </page>
  <page>
    <title>Data stream management system</title>
    <ns>0</ns>
    <id>26760516</id>
    <revision>
      <id>753459639</id>
      <parentid>749432124</parentid>
      <timestamp>2016-12-07T08:12:49Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12955" xml:space="preserve">{{Use dmy dates|date=July 2013}}

A '''Data stream management system''' (DSMS) is a computer program to manage continuous data streams. It is similar to a [[database management system]] (DBMS), which is, however, designed for static data in conventional databases. A DSMS also offers a flexible query processing so that the information need can be expressed using queries. However, in contrast to a DBMS, a DSMS executes a ''continuous query'' that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. Since most DSMS are data-driven, a continuous query produces new results as long as new data arrive at the system. This basic concept is similar to [[Complex event processing]] so that both technologies are partially coalescing.

== Functional principle ==

One of the most important features of a DSMS is the possibility to handle potentially infinite and rapidly changing data streams by offering flexible processing at the same time, although there are only limited resources such as main memory. The following table provides various principles of DSMS and compares them to traditional DBMS.

{| border="1"
! Database management system (DBMS)
! Data stream management system (DSMS)
|-
|Persistent data (relations)
|volatile data streams
|-
|Random access
|Sequential access
|-
|One-time queries 
|Continuous queries
|-
|(theoretically) unlimited secondary storage
|limited main memory
|-
|Only the current state is relevant
|Consideration of the order of the input
|-
|relatively low update rate
|potentially extremely high update rate
|-
|Little or no time requirements
|Real-time requirements
|-
|Assumes exact data 
|Assumes outdated/inaccurate data
|-
|Plannable query processing
|Variable data arrival and data characteristics
|}

== Processing and streaming models ==
One of the biggest challenges for a DSMS is to handle potentially infinite data streams using a fixed amount of memory and no random access to the data. There are different approaches to limit the amount of data in one pass, which can be divided into two classes. For the one hand, there are compression techniques that try to summarize the data and for the other hand there are window techniques that try to portion the data into (finite) parts.

=== Synopses ===
The idea behind compression techniques is to maintain only a synopsis of the data, but not all (raw) data points of the data stream. The algorithms range from selecting random data points called sampling to summarization using histograms, wavelets or sketching. One simple example of a compression is the continuous calculation of an average. Instead of memorizing each data point, the synopsis only holds the sum and the number of items. The average can be calculated by dividing the sum by the number. However, it should be mentioned that synopses cannot reflect the data accurately. Thus, a processing that is based on synopses may produce inaccurate results.

=== Windows ===
Instead of using synopses to compress the characteristics of the whole data streams, window techniques only look on a portion of the data. This approach is motivated by the idea that only the most recent data are relevant. Therefore, a window continuously cuts out a part of the data stream, e.g. the last ten data stream elements, and only considers these elements during the processing. There are different kinds of such windows like sliding windows that are similar to [[FIFO (computing and electronics)|FIFO]] lists or tumbling windows that cut out disjoint parts. Furthermore, the windows can also be differentiated into element-based windows, e.g., to consider the last ten elements, or time-based windows, e.g., to consider the last ten seconds of data. There are also different approaches to implementing windows. There are, for example, approaches that use timestamps or time intervals for system-wide windows or buffer-based windows for each single processing step. Sliding-window query processing is also suitable to being implemented in parallel processors by exploiting parallelism between different windows and/or within each window extent.&lt;ref&gt;{{cite journal|last1=De Matteis|first1=Tiziano|last2=Mencagli|first2=Gabriele|title=Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach|journal=International Journal of Parallel Programming|date=25 March 2016|doi=10.1007/s10766-016-0413-x}}&lt;/ref&gt;

== Query Processing ==
Since there are a lot of prototypes, there is no standardized architecture. However, most DSMS are based on the [[Information retrieval|query]] processing in DBMS by using declarative languages to express queries, which are translated into a plan of operators. These plans can be optimized and executed. A query processing often consists of the following steps.

=== Formulation of continuous queries ===
The formulation of queries is mostly done using declarative languages like [[SQL]] in DBMS. Since there are no standardized query languages to express continuous queries, there are a lot of languages and variations. However, most of them are based on [[SQL]], such as the [[Continuous Query Language]] (CQL), [[StreamSQL]] and [[Event stream processing|EPL]]. There are also graphical approaches where each processing step is a box and the processing flow is expressed by arrows between the boxes.

The language strongly depends on the processing model. For example, if windows are used for the processing, the definition of a window has to be expressed. In [[StreamSQL]], a query with a sliding window for the last 10 elements looks like follows:
&lt;source lang="sql"&gt;
SELECT AVG(price) FROM examplestream [SIZE 10 ADVANCE 1 TUPLES] WHERE value &gt; 100.0
&lt;/source&gt;
This stream continuously calculates the average value of "price" of the last 10 tuples, but only considers those tuples whose prices are greater than 100.0.

In the next step, the declarative query is translated into a logical query plan. A query plan is a directed graph where the nodes are operators and the edges describe the processing flow. Each operator in the query plan encapsulates the semantic of a specific operation, such as filtering or aggregation. In DSMSs that process relational data streams, the operators are equal or similar to the operators of the [[Relational algebra]], so that there are operators for selection, projection, join, and set operations. This operator concept allows the very flexible and versatile processing of a DSMS.

=== Optimization of queries ===
The logical query plan can be optimized, which strongly depends on the streaming model. The basic concepts for optimizing continuous queries are equal to those from [[Query optimizer|database systems]]. If there are relational data streams and the logical query plan is based on relational operators from the [[Relational algebra]], a query optimizer can use the algebraic equivalences to optimize the plan. These may be, for example, to push selection operators down to the sources, because they are not so computationally intensive like join operators.

Furthermore, there are also cost-based optimization techniques like in DBMS, where a query plan with the lowest costs is chosen from different equivalent query plans. One example is to choose the order of two successive join operators. In DBMS this decision is mostly done by certain statistics of the involved databases. But, since the data of a data streams is unknown in advance, there are no such statistics in a DSMS. However, it is possible to observe a data stream for a certain time to obtain some statistics. Using these statistics, the query can also be optimized later. So, in contrast to a DBMS, some DSMS allows to optimize the query even during runtime. Therefore, a DSMS needs some plan migration strategies to replace a running query plan with a new one.

=== Transformation of queries ===
Since a logical operator is only responsible for the semantics of an operation but does not consist of any algorithms, the logical query plan must be transformed into an executable counterpart. This is called a physical query plan. The distinction between a logical and a physical operator plan allows more than one implementation for the same logical operator. The join, for example, is logically the same, although it can be implemented by different algorithms like a [[Nested loop join]] or a [[Sort-merge join]]. Notice, these algorithms also strongly depend on the used stream and processing model.
Finally, the query is available as a physical query plan.

=== Execution of queries ===
Since the physical query plan consists of executable algorithms, it can be directly executed. For this, the physical query plan is installed into the system. The bottom of the graph (of the query plan) is connected to the incoming sources, which can be everything like connectors to sensors. The top of the graph is connected to the outgoing sinks, which may be for example a visualization. Since most DSMSs are data-driven, a query is executed by pushing the incoming data elements from the source through the query plan to the sink. Each time when a data element passes an operator, the operator performs its specific operation on the data element and forwards the result to all successive operators.

== Data Stream Management Systems ==
* [http://www.sqlstream.com/stream-processing/ SQLstream]
* [http://www-db.stanford.edu/stream STREAM] &lt;ref name="StandfordStream"&gt;[http://ilpubs.stanford.edu:8090/641/ Arasu, A., et. al. ''STREAM: The Stanford Data Stream Management System.''  Technical Report. 2004, Stanford InfoLab.]&lt;/ref&gt;
* [http://www.cs.brown.edu/research/aurora/ AURORA],&lt;ref name="aurora"&gt;{{cite conference | author = Abadi | title = Aurora: A Data Stream Management System | conference = SIGMOD 2003 | citeseerx = 10.1.1.67.8671 |display-authors=etal}}&lt;/ref&gt; [http://www.streambase.com/ StreamBase Systems, Inc.]
* [http://telegraph.cs.berkeley.edu/telegraphcq/ TelegraphCQ] &lt;ref name="telegraphcq"&gt;[http://www.cs.berkeley.edu/~franklin/Papers/TCQcidr03.pdf Chandrasekaran, S. et al, "TelegraphCQ: Continuous Dataflow Processing for an Uncertain World." CIDR 2003.]&lt;/ref&gt;
* [http://research.cs.wisc.edu/niagara/ NiagaraCQ],&lt;ref name="niagaracq"&gt;[http://www.cs.wisc.edu/niagara/papers/NiagaraCQ.pdf Chen, J. et al, "NiagaraCQ: A Scalable Continuous Query System for Internet Databases." SIGMOD 2000.]&lt;/ref&gt; 
* [http://wwwdb.inf.tu-dresden.de/research-projects/closed-projects/qstream/ QStream]
* [http://dbs.mathematik.uni-marburg.de/Home/Research/Projects/PIPES PIPES], [http://www.softwareag.com/de/products/wm/events/overview/default.asp webMethods Business Events]
* [http://www-db.in.tum.de/research/projects/StreamGlobe/index.shtml StreamGlobe]
* [http://odysseus.informatik.uni-oldenburg.de/ Odysseus]
* [http://www.microsoft.com/sqlserver/en/us/solutions-technologies/business-intelligence/streaming-data.aspx StreamInsight]
* [http://www-01.ibm.com/software/data/infosphere/streams/ InfoSphere Streams]
* [http://www.sas.com/en_us/software/data-management/event-stream-processing.html SAS Event Stream Processing Engine]
* [http://go.sap.com/uk/product/data-mgmt/complex-event-processing.html  SAP Event Stream Processor]
* [https://www.pipelinedb.com/ Pipeline DB]

== See also ==
* [[Complex Event Processing]]
* [[Event stream processing]]
* [[Relational data stream management system]]

== References ==
{{Reflist}}
* {{Cite book
|last=Aggarwal
|first=Charu C.
|authorlink=
|year=2007
|title=Data Streams: Models and Algorithms
|publisher=Springer
|location=New York
|id=
|isbn=978-0-387-47534-9
}}
* {{Cite book
|first1=Lukasz
|last1=Golab
|first2=M. Tamer
|last2=&#214;zsu
|authorlink=
|year=2010
|title=Data Stream Management
|publisher=Morgan and Claypool
|location=Waterloo, USA
|id=
|isbn=978-1-608-45272-9
}}

==External links==
*[http://www.pam2004.org/papers/113.pdf Using Data Stream Management Systems for Traffic Analysis: A Case Study, last visited 2013-01-10]
*[http://infolab.stanford.edu/stream/ STREAM: Stanford Stream Data Manager, last visited 2013-01-10]
*[http://datalab.cs.pdx.edu/niagara/ NiagaraST: A Research Data Stream Management System at Portland State University, last visited 2013-01-10]
*[http://odysseus.informatik.uni-oldenburg.de Odysseus: An open source Java based framework for Data Stream Management Systems, last visited 2013-01-10]
*[http://home.dei.polimi.it/margara/papers/survey.pdf Processing Flows of Information: From Data Stream to Complex Event Processing] - Survey article on Data Stream and Complex Event Processing Systems, last visited 2013-01-10
*[http://www.streambase.com/developers/docs/latest/streamsql/index.html StreamSQL reference, last visited 2013-01-10]
*[https://web.archive.org/web/20140706215458/http://www.sqlstream.com/stream-processing-with-sql/ Stream processing with SQL] - Introduction to streaming data management with SQL

[[Category:Data management]]</text>
      <sha1>7pr3owkjmzp72bvaem7t5mlwlns9ra7</sha1>
    </revision>
  </page>
  <page>
    <title>Lean integration</title>
    <ns>0</ns>
    <id>28252181</id>
    <revision>
      <id>751022926</id>
      <parentid>582242741</parentid>
      <timestamp>2016-11-22T22:11:48Z</timestamp>
      <contributor>
        <ip>142.239.254.20</ip>
      </contributor>
      <comment>/* Lean integration principles */ Fixed a couple of typos, fixed a grammar error and capitalized the name of the metric 'First Time Through'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9105" xml:space="preserve">'''Lean integration''' is a [[management system]] that emphasizes creating value for customers, continuous improvement, and eliminating waste as a sustainable [[data integration]] and [[system integration]] practice.  Lean integration has parallels with other lean disciplines such as [[lean manufacturing]], [[lean IT]], and [[lean software development]].  It is a specialized collection of tools and techniques that address the unique challenges associated with seamlessly combining information and processes from systems that were independently developed, are based on incompatible data models, and remain independently managed, to achieve a cohesive holistic operation.

==History==

Lean integration was first introduced by John Schmidt in a series of blog articles starting in January 2009 entitled 10 Weeks To Lean Integration.&lt;ref&gt;[http://blogs.informatica.com/perspectives/index.php/2009/01/14/10-weeks-to-lean-integration/ Original Lean Integration Blog Series]&lt;/ref&gt;  This was followed by a white paper&lt;ref&gt;[http://www.cloudyintegration.com/uploads/LEAN_INTEGRATION_AFE_-_John_Schmidt.pdf Lean Integration White Paper]&lt;/ref&gt; on the topic in April 2009 and the book ''Lean Integration, An Integration Factory Approach to Business Agility'' &lt;ref name="Schmidt"&gt;John G.Schmidt, David Lyle (2010) ''Lean Integration: An Integration Factory Approach to Business Agility'', Addison-Wesley Pearson Education, ISBN 0-321-71231-5&lt;/ref&gt; in May 2010.

==Overview==

Lean integration builds on the same set of principles that were developed for [[lean manufacturing]] and [[lean software development]] which is based on the [[Toyota Production System]]. Integration solutions can be broadly categorized as either Process Integration or Data Integration.  

The book&lt;ref name="Schmidt"/&gt; is based on the premise that Integration is an ongoing activity and not a one-time activity;  therefore integration should be viewed as a long term strategy for an organization.  John Schmidt and David Lyle initially articulated in their book the reasons for maintaining an efficient and sustainable integration team.  Lean integration as an integration approach must be ''sustainable'' and ''holistic'' unlike other integration approaches that either tackle only a part of the problem or tackle the problem for a short period of time.  Lean integration drives elimination of waste by adopting reusable elements, high automation and quality improvements.  Lean is a data-driven, fact-based methodology that relies on metrics to ensure that the quality and performance are maintained at a high level. 

An organizational focus is required for the implementation of lean integration principles. The predominant organizational model is the [[Integration Competency Center]] which may be structured as a central group or a more loosely coupled federated team.

==Lean integration principles==

The principles of Lean Integration may at first glance appear similar to that of [[Six Sigma]] but there are some very clear differences between them.  Six-Sigma is an ''analytical technique'' that focuses on quality and reduction of defects while Lean is a ''management system'' that focuses on delivering value to the end customer by continuously improving value delivery processes.  Lean provides a robust framework that facilitates improving efficiency and effectiveness by focusing on critical customer requirements.

As mentioned in lean integration there are seven core ''lean integration principles'' vital for deriving significant and sustainable business benefits. They are as below: 

# Focus on the customer and eliminate waste: Waste elimination should be viewed from the customer perspective and all activities that do not add value to the customer needs to be looked at closely and eliminated or reduced. In an integration context, the customer is often an internal sponsor or group within an organization that uses, benefits from, or pays for, the integrated capabilities.
# Continuously improve: A data driven cycle of hypothesis-validation-implementation should be used to drive innovation and continuously improve the end-to-end process.  Adopting and institutionalizing lessons learned and sustaining integration knowledge are related concepts that assist in the establishment of this principle.
# Empower the team: Creating cross-functional teams and sharing commitments across individuals empower the teams and individuals who have a clear understanding of their roles and the needs of their customers.  The team is also provided the support by senior management to innovate and try new ideas without fear of failure.
# Optimize the whole: Adopt a big-picture perspective of the end-to-end process and optimize the whole to maximize the customer value.  This may at times require performing individual steps and activities that appear to be sub-optimal when viewed in isolation, but aid in streamlining the end-to-end process.
# Plan for change: Application of mass customization techniques like leveraging automated tools, structured processes, and reusable and parameterized integration elements leads to reduction in cost and time in both the build and run stages of the integration life-cycle. Another key technique is a middleware services layer that presents applications with enduring abstractions of data through standardized interfaces, allowing the underlying data structures to change without necessarily impacting the dependent applications.
# Automate processes: Automation of tasks increases the ability to respond to large integration projects as effectively as small changes. In its ultimate form, automation eliminates integration dependencies from the critical implementation path of projects.
# Build quality in : Process excellence is emphasized and quality is built in rather than inspected in. A key metric for this principle is First Time Through (FTT) percentage which is a measure of the number of times an end-to-end process is executed without having to do any rework or repeat any of the steps.

==Benefits of lean integration==

The Lean integration practices transforms integration from an ''art'' into a ''science'', a repeatable and teachable methodology that shifts the focus from integration as a point-in-time activity to integration as a sustainable activity that enables organizational agility.  Once an organization adopts the integration as a science it enhances the organization&#8217;s ability to change rapidly without comprising on the IT risk or quality thereby transforming the organization into an agile data driven enterprise.  The following are the advantages derived by adopting the lean integration practices:

# Efficiency: typical improvements are in the scale of 50% labor productivity improvements and 90% lead-time reduction through continuous efforts to eliminate waste.
# Agility: Reusable components, highly automated processes and self-service delivery models improve the agility of the organization.
# Data quality: quality and reliability of data is enhanced and data becomes a real asset.
# Governance: metrics are established that drive continuous improvement.
# Innovation: innovation is facilitated by using fact-based approach.
# Staff Morale: IT staff is kept engaged with high morale driving bottom-up improvements.

==See also==

* [[Integration Competency Center]]
* [[Lean software development]]
* [[Lean IT]]
* [[Data Integration]]
* [[Toyota Production System]]

==References==

&lt;references/&gt;

==External links==
* [http://www.integrationfactory.com Lean Integration book microsite]
* [http://blogs.informatica.com/perspectives/index.php/2010/04/06/health-care-is-ready-for-lean-integration/ Application of Lean Integration to Health Care]
* [http://www.informatica.com/news_events/press_releases/Pages/02082010_lean.aspx  Press Release about Lean Integration Book]
* [http://www.linkedin.com/in/johnschmidt John Schmidt profile]
* [http://www.linkedin.com/in/davelyle David Lyle profile]
* [http://my.safaribooksonline.com/9780321712363 Lean Integration book publisher website]
* [http://www.baselinemag.com/c/a/IT-Management/How-IT-Runs-Lean-419352/ Slide show overview of Lean Integration]
* [http://www.linkedin.com/groups?gid=2302506 LinkedIn Group for Lean Integration Community]
* [http://www.itbusinessedge.com/cm/blogs/vizard/making-the-case-for-lean-integration/?cs=42547 Book review by Mike Vizard of ITBusinessEdge]
* [http://www.bcs.org/server.php?show=conBlogPost.1685 Book review by John Morris]
* [http://www.itbusinessedge.com/cm/blogs/lawson/lean-principles-can-make-it-better-at-integration/?cs=42041&amp;utm_source=itbe&amp;utm_medium=email&amp;utm_campaign=EEB&amp;nr=EEB John Schmidt and David Lyle Interview by Loraine Lawson]
* [http://www.insurancenetworking.com/blogs/insurance_technology_Lean_IT_manufacturing-25138-1.html Book review by Joe McKendrick]
* [http://www.information-management.com/dmradio/-10017194-1.html David Lyle Interview on DM Radio]

[[Category:Data management]]
[[Category:Software development philosophies]]
[[Category:Agile software development]]
[[Category:Information technology]]
[[Category:Quality]]</text>
      <sha1>ocgted89c4fvt2ajnpqnxilaus67cik</sha1>
    </revision>
  </page>
  <page>
    <title>Novell Storage Manager</title>
    <ns>0</ns>
    <id>28205544</id>
    <revision>
      <id>721088934</id>
      <parentid>721088745</parentid>
      <timestamp>2016-05-19T18:27:04Z</timestamp>
      <contributor>
        <ip>134.216.26.215</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4645" xml:space="preserve">{{Infobox software
|name                       = Novell Storage Manager
|logo                       =
|screenshot                 =
|caption                    =
|collapsible                =
|author                     =
|developer                  = [[Novell]]
|released                   = 2004 &lt;!-- {{Start date|YYYY|MM|DD}} --&gt;
|discontinued               =
|latest release version     = 4.1
|latest release date        = {{Start date|2014|10|07}}
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       =
|operating system           =
|platform                   =
|size                       =
|language                   =
|status                     =
|genre                      = [[System Software]]
|license                    =
|website                    = [http://www.novell.com/products/storage-manager/ Novell Storage Manager]
}}

'''Novell Storage Manager''' is a [[system software]] package released by [[Novell]] in 2004 &lt;ref&gt;{{Citation | last=Greyzdorf| first=Noemi| title=Novell Delivers a New Way of Intelligently Managing Organizations' File-Based Information| journal=IDC #216013 | volume=1| issue=Storage Software: Technology Assessment| year=2009| pages=1&#8211;3| url=http://www.novell.com/docrep/2009/01/Novell%20Delivers%20a%20New%20Way%20of%20Intelligently%20Managing%20Organizations_%20File-Based%20Information_en.pdf}}&lt;/ref&gt; that uses identity, policy and [[Novell eDirectory|directory]] events to automate full lifecycle management of file storage for individual users and organizational groups. By tying storage management to an organization's existing identity infrastructure, it has been pointed out,&lt;ref&gt;{{Citation | last=Greyzdorf| first=Noemi| title=Efficiently Delivering Enterprise-Class File-Based Storage| journal=IDC Spotlight | year=2010| pages=1&#8211;5}}&lt;/ref&gt; Novell Storage Manager enables the administration of users across all file servers "as a single pool rather than [in] separate independently managed domains." Novell Storage Manager is a component of the [[Novell File Management Suite]].

==How It Works==

Novell Storage Manager dynamically manages and provisions storage based on user and group events that occur in the directory, including user creations, group assignments, moves, renames, and deletions. When a change happens in the directory that affects a user&#8217;s file storage needs or user storage policy, Storage Manager applies the appropriate policy and makes the necessary changes at the file system level to address those storage needs.&lt;ref&gt;{{citation| title=Novell Storage Manager for Novell eDirectory | year=2009 | page=4 | url=http://www.novell.com/docrep/2009/04/Novell_Storage_Manager_for_Novell_eDirectory_White_Paper_en.pdf}}&lt;/ref&gt;

The following key components comprise Novell Storage Manager's identity and policy-driven [[state machine]] architecture: Directory services; Storage policies; Novell Storage Manager event monitors; Novell Storage Manager policy engine; Novell Storage Manager agents; and Action objects. This state machine architecture enables the engine to properly deal with transient waits with directory synchronization issues. It also allows recovery from failures involving network communications, a target server or a server running a component of Storage Manager&#8212;including the policy engine itself. If a failure or interruption occurs at any point during operation, Storage Manager will be able to successfully continue the operation from where it was when the interruption occurred.

==Reviews==

Jon Toigo called Novell Storage Manager "a robust and smart approach to corralling user files... into an organized and efficient management scheme".&lt;ref&gt;{{Citation| last = Toigo | first = Jon William | title = Novell Storage Manager Strikes Data Management Gold | url= http://esj.com/articles/2009/04/28/novell-storage-mgr.aspx | accessdate = 26 July 2010}}&lt;/ref&gt; He also said it was "best in class" of the products he'd reviewed.&lt;ref&gt;{{Citation| last = Toigo | first = Jon William | title = Everything We Need to Know About How to Screw Up IT&#8230; | url=http://www.drunkendata.com/?p=2916 | accessdate = 30 July 2010}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://www.novell.com/products/storage-manager/ Novell Storage Manager: Product homepage] - Overview, features, and technical information
*[http://www.storagemanagersupport.com/nsm/ Novell Storage Manager: Support]

{{Novell}}

[[Category:Novell]]
[[Category:Novell software]]
[[Category:Storage software]]
[[Category:Data management]]
[[Category:Identity management]]</text>
      <sha1>ml7elmxzuq5fkgtaicsnq59ruqqw0tx</sha1>
    </revision>
  </page>
  <page>
    <title>Data binding</title>
    <ns>0</ns>
    <id>15592339</id>
    <revision>
      <id>758400077</id>
      <parentid>758300916</parentid>
      <timestamp>2017-01-05T05:20:15Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>v1.41b - [[WP:WCW]] project (Link equal to linktext)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2708" xml:space="preserve">'''Data binding''' is a general technique that binds data sources from the provider and consumer together and [[data synchronization|synchronizes]] them. This is usually done with two data/information sources with different languages as in [[XML data binding]]. In [[UI data binding]], data and information objects of the same language but different logic function are bound together (e.g. [[Java (programming language)|Java]] [[user interface|UI]] elements to Java objects).&lt;ref&gt;{{cite web |url=https://www.techopedia.com/definition/15652/data-binding|title=What is Data Binding? |work=Techopedia.com |accessdate=30 December 2015}}&lt;/ref&gt;

In a data binding process, each data change is reflected automatically by the elements that are bound to the data. The term data binding is also used in cases where an outer representation of data in an element changes, and the underlying data is automatically updated to reflect this change. As an example, a change in a [[text box|&lt;code&gt;TextBox&lt;/code&gt;]] element could modify the underlying data value.&lt;ref&gt;{{cite web |url=https://msdn.microsoft.com/en-us/library/ms752347(v=vs.110).aspx |title=Data Binding Overview |work=Microsoft Developer Network |publisher=Microsoft |access-date=29 December 2016}}&lt;/ref&gt;

== Data binding frameworks and tools ==

=== [[Embarcadero Delphi|Delphi]] ===
* DSharp 3rd-party Data Binding tool
* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd-party Visual Data Binding tool
* LiveBindings

=== [[C Sharp (programming language)|C#]] ===
* [[Windows Presentation Foundation]]

=== [[JavaScript]] ===
* [[AngularJS]]
* [[Backbone.js]]
* BindingJS
* Datum.js&lt;ref&gt;{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}&lt;/ref&gt;
* [[EmberJS]]
* Generic Data Binder
* [[KnockoutJS]]
* [[React (JavaScript library)]]
* SAP/OPEN UI5
* [[Vue.js]]

=== [[Java_(programming_language)|Java]] ===
* [[Google Web Toolkit]]

=== [[Objective-C]] ===
* AKABeacon iOS Data Binding framework

=== [[Scala (programming language)|Scala]] ===
* Binding.scala&lt;ref&gt;{{cite web |url=https://github.com/ThoughtWorksInc/Binding.scala|title=Binding.scala|accessdate=30 December 2016}}&lt;/ref&gt; Reactive data-binding for Scala

==See also==
* [[Windows Presentation Foundation]]
* [[XML data binding]]
* [[UI data binding]]
* [[Bound property]]

==References==
&lt;references/&gt;

==Further reading==
*{{cite book |last=Noyes |first=Brian |title=Data Binding with Windows Forms 2.0: Programming Smart Client Data Applications with .NET |url=https://books.google.com/books?id=RxptHgJ5W2cC |date=12 January 2006 |publisher=Pearson Education |isbn=978-0-321-63010-0}}

{{DEFAULTSORT:Data Binding}}
&lt;!--Categories--&gt;
[[Category:Data management]]</text>
      <sha1>bukuen0dau6i8ggd19mopseiv126nlp</sha1>
    </revision>
  </page>
  <page>
    <title>Learning object metadata</title>
    <ns>0</ns>
    <id>1955471</id>
    <revision>
      <id>734815271</id>
      <parentid>727590586</parentid>
      <timestamp>2016-08-16T22:02:48Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>Removing link(s) to "IMS Global": rm redlink (deleted). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15686" xml:space="preserve">[[Image:LOM base schema.svg|340px|right|thumb|A schematic representation of the hierarchy of elements in the LOM data model]]

'''Learning Object Metadata''' is a data model, usually encoded in XML, used to describe a [[learning object]] and similar digital resources used to support learning. The purpose of learning object metadata is to support the reusability of learning objects, to aid discoverability, and to facilitate their interoperability, usually in the context of online [[learning management systems]] (LMS).

The IEEE 1484.12.1 &#8211; 2002 Standard for Learning Object Metadata is an internationally recognised open standard (published by the [[Institute of Electrical and Electronics Engineers]] Standards Association, New York) for the description of &#8220;[[learning object]]s&#8221;. Relevant attributes of learning objects to be described include: type of object; author; owner; terms of distribution; format; and [[pedagogy|pedagogical]] attributes, such as teaching or interaction style.

== IEEE 1484.12.1 &#8211; 2002 Standard for Learning Object Metadata ==

=== In brief ===
The IEEE working group that developed the standard defined learning objects, ''for the purposes of the standard,'' as being &#8220;any entity, digital or non-digital, that may be used for learning, education or training." This definition has struck many commentators as being rather broad in its scope, but the definition was intended to provide a broad class of objects to which LOM metadata might usefully be associated rather than to give an instructional or pedagogic definition of a learning object. ''IEEE 1484.12.1'' is the first part of a multipart standard, and describes the LOM data model. The LOM data model specifies which aspects of a learning object should be described and what vocabularies may be used for these descriptions; it also defines how this data model can be amended by additions or constraints. Other parts of the standard are being drafted to define bindings of the LOM data model, i.e. define how LOM records should be represented in [[XML]] and [[Resource Description Framework|RDF]] (''IEEE 1484.12.3'' and ''IEEE 1484.12.4'' respectively). This article focuses on the LOM data model rather than issues relating to XML or other bindings.

IMS Global Learning Consortium is an international consortium that contributed to the drafting of the IEEE Learning Object Metadata (together with the ARIADNE Foundation) and endorsed early drafts of the data model as part of the IMS Learning Resource Meta-data specification (IMS LRM, versions 1.0 &#8211; 1.2.2). Feedback and suggestions from the implementers of IMS LRM fed into the further development of the LOM, resulting in some drift between version 1.2 of the IMS LRM specification and what was finally published at the LOM standard. Version 1.3 of the IMS LRM specification realigns the IMS LRM data model with the IEEE LOM data model and specifies that the IEEE XML binding should be used. Thus, we can now use the term 'LOM' in referring to both the IEEE standard and version 1.3 of the IMS specification. The IMS LRM specification also provides an extensive ''Best Practice and Implementation Guide'', and an ''XSL transform'' that can be used to migrate metadata instances from the older versions of the IMS LRM XML binding to the IEEE LOM XML binding.

== Technical details ==

=== How the data model works ===
The LOM comprises a '''hierarchy of elements'''&lt;!--, as shown in the diagram (top right)--&gt;. At the first level, there are nine categories, each of which contains sub-elements; these sub-elements may be simple elements that hold data, or may themselves be aggregate elements, which contain further sub-elements. The semantics of an element are determined by its context: they are affected by the parent or container element in the hierarchy and by other elements in the same container. For example, the various ''Description'' elements (1.4, 5.10, 6.3, 7.2.2, 8.3 and 9.3) each derive their context from their parent element. In addition, description element 9.3 also takes its context from the value of element 9.1 ''Purpose'' in the same instance of ''Classification''.

The data model specifies that some elements may be repeated either individually or as a group; for example, although the elements 9.2 (''Description'') and 9.1 (''Purpose'') can only occur once within each instance of the ''Classification'' container element, the ''Classification'' element may be repeated - thus allowing many descriptions for different purposes.

The data model also specifies the '''value space''' and '''datatype''' for each of the simple data elements. The value space defines the restrictions, if any, on the data that can be entered for that element. For many elements, the value space allows any string of [[Unicode]] character to be entered, whereas other elements entries must be drawn from a declared list (i.e. a [[controlled vocabulary]]) or must be in a specified format (e.g. date and language codes). Some element datatypes simply allow a string of characters to be entered, and others comprise two parts, as described below:
* '''LangString''' items contain Language and String parts, allowing the same information to be recorded in multiple languages
* '''Vocabulary''' items are constrained in such a way that their entries have to be chosen from a controlled list of terms - composed of Source-Value pairs - with the Source containing the name of the list of terms being used and the Value containing the chosen term
* '''DateTime''' and '''Duration''' items contain one part that allows the date or duration to be given in a machine readable format, and a second that allows a description of the date or duration (for example &#8220;mid summer, 1968&#8221;).

When implementing the LOM as a data or service provider, it is not necessary to support all the elements in the data model, nor need the LOM data model limit the information which may be provided. The creation of an [[application profile]] allows a community of users to specify which elements and vocabularies they will use. Elements from the LOM may be dropped and elements from other metadata schemas may be brought in; likewise, the vocabularies in the LOM may be supplemented with values appropriate to that community.

=== Requirements ===
The key requirements for exploiting the LOM as a data or service provider are to:
* Understand user/community needs and to express these as an application profile
* Have a strategy for creating high quality metadata
* Store this metadata in a form which can be exported as LOM records
* Agree a binding for LOM instances when they are exchanged
* Be able to exchange records with other systems either as single instances or ''en masse''.

=== Related specifications ===
There are many metadata specifications; of particular interest is the [[Dublin Core]] Metadata Element Set (commonly known as Simple Dublin Core, standardised as ''ANSI/NISO Z39.85 &#8211; 2001''). Simple Dublin Core (DC) provides a non-complex, loosely defined set of elements which is useful for sharing metadata across a wide range of disparate services. Since the LOM standard used Dublin Core as a starting point, refining the Simple DC schema with qualifiers relevant to learning objects, there is some overlap between the LOM and DC standards.&lt;ref&gt;{{cite book|last1=Miller|first1=Steven J.|title=Metadata for Digital Collections: A How-To-Do-It Manual|date=2011|publisher=ALA Neal-Schuman|location=Chicago|isbn=978-1-55570-746-0|pages=56}}&lt;/ref&gt; The Dublin Core Metadata Initiative is also working on a set of terms which allow the Dublin Core Element Set to be used with greater semantic precision (Qualified Dublin Core). The Dublin Education Working Group aims to provide refinements of [[Dublin Core]] for the specific needs of the education community.

Many other education-related specifications allow for LO metadata to be embedded within XML instances, such as: describing the resources in an IMS Content Package or Resource List; describing the vocabularies and terms in an [[IMS VDEX]] (Vocabulary Definition and Exchange) file; and describing the question items in an IMS QTI (Question and Test Interoperability) file.

The [[IMS VDEX|IMS Vocabulary Definition and Exchange (VDEX) specification]] has a double relation with the LOM, since not only can the LOM provide metadata on the vocabularies in a VDEX instance, but VDEX can be used to describe the controlled vocabularies which are the value space for many LOM elements.

LOM records can be transported between systems using a variety of protocols, perhaps the most widely used being [[OAI-PMH]].

=== Application profiles ===

==== UK LOM Core ====
For UK Further and Higher Education, the most relevant family of application profiles are those based around the ''UK LOM Core''.&lt;ref&gt;http://zope.cetis.ac.uk/profiles/uklomcore/&lt;/ref&gt; The UK LOM Core is currently a draft schema researched by a community of practitioners to identify common UK practice in learning object content, by comparing 12 metadata schemas. UK LOM is currently legacy work, it is not in active development.

==== CanCore ====
''CanCore'' provides detailed guidance for the interpretation and implementation of each data element in the LOM standard.&lt;ref name="CanCore"&gt;{{cite web | url = http://cancore.tru.ca/en/guidelines.html| title = CanCore Guidelines: Introduction | author = [[Norm Friesen]]| publisher = Athabasca University| date = 2003-01-20 | accessdate = 2009-02-23 |display-authors=etal}}&lt;/ref&gt; These guidelines (2004) constitute a 250-page document, and have been developed over three years under the leadership of [[Norm Friesen]], and through consultation with experts across Canada and throughout the world. These guidelines are also available at no charge from the CanCore Website.

==== ANZ-LOM ====
ANZ-LOM is a metadata profile developed for the education sector in Australia and New Zealand. The profile sets obligations for elements and illustrates how to apply controlled vocabularies, including example regional vocabularies used in the "classification" element. The ANZ-LOM profile was first published by The Le@rning Federation (TLF) in January, 2008.

==== Vetadata ====
The Australian Vocational Training and Education (VET) sector uses an application profile of the IEEE LOM called Vetadata. The profile contains five mandatory elements, and makes use of a number of vocabularies specific to the Australian VET sector. This application profile was first published in 2005. The Vetadata and ANZ-LOM profiles are closely aligned.

==== NORLOM ====
NORLOM is the Norwegian LOM profile.
The profile is managed by NSSL (The Norwegian Secretariat for Standardization of Learning Technologies)

==== ISRACore ====
ISRACORE is the Israeli LOM profile.
The Israel Internet Association (ISOC-IL) and Inter University Computational Center (IUCC) have teamed up to manage and establish an e-learning objects database.

====SWE-LOM====
SWE-LOM is the Swedish LOM profile that is managed by IML at [[Ume&#229; University]] as a part of the work with the national standardization group TK450 at [[Swedish Standards Institute]].

====TWLOM====
TWLOM is the Taiwanese LOM profile that is managed by Industrial Development and Promotion of Archives and e-Learning Project

====LOM-FR====
LOM-FR is a metadata profile developed for the education sector in France. This application profile was first published in 2006.

====NL LOM====
NL LOM is the Dutch metadata profile for educational resources in the Netherlands. This application profile was the result of merging the Dutch higher education LOM profile with the one used in primary and secondary Dutch education. The final version was released in 2011.

====LOM-CH====
LOM-CH is a metadata profile developed for the education sector in Switzerland. It is currently available in French and German. This application profile was published in July 2014.

====LOM-ES====
LOM-ES is a metadata profile developed for the education sector in Spain. It is available in Spanish.

====LOM-GR====
LOM-GR, also known as "LOM-GR ''Photodentro''" is the Greek LOM application profile for educational resources, currently being used for resources related to school education. It was published in 2012 and is currently available in Greek and English.&lt;ref&gt;https://git.dschool.edu.gr/photodentro/LOM-GR&lt;/ref&gt; It is maintained by [[CTI DIOPHANTUS]] as part of the "[[Photodentro]] Federated Architecture for Educational Content for Schools" that includes a number of educational content repositories (for Learning Objects, Educational Video, and User Generated Content) and the Greek National Aggregator of Educational Content accumulating metadata from collections stored in repositories of other organizations.&lt;ref name="Photodentro LOR"&gt;{{cite journal|last1=Megalou|first1=Elina|last2=Kaklamanis|first2=Christos|title=PHOTODENTRO LOR, THE GREEK NATIONAL LEARNING OBJECT REPOSITORY|journal=INTED2014 Proceedings|date=10&#8211;12 March 2014|pages=309&#8211;319|url=https://library.iated.org/view/MEGALOU2014PHO|accessdate=7 April 2016|series=8th International Technology, Education and Development Conference|publisher=IATED|location=Valencia, Spain|issn=2340-1079}}&lt;/ref&gt; LOM-GR is a working specification of the TC48/WG3 working group of the [[Hellenic Organization for Standardization]].

==== Others ====
Other application profiles are those developed by the Celebrate project&lt;ref&gt;European Schoolnet, [http://web.archive.org/web/20071225053548/http://www.eun.org/ww/en/pub/celebrate_help/application_profile.htm CELEBRATE Application Profile] (2003).&lt;/ref&gt; and the metadata profile that is part of the SCORM reference model.&lt;ref&gt;ADL, [http://www.adlnet.gov/capabilities/scorm#tab-learn SCORM].&lt;/ref&gt;

== See also ==
* [[Application profile]]
* [[Content package]]
* [[Dublin Core]]
* IMS Global
* [[Learning object]]
* [http://dublincore.org/dcx/lrmi-terms/1.1/ LRMI (Learning Resource Metadata Initiative)]
* [[Metadata]]
* [[Metadata standards|Metadata Standards]]
* [[OAI-PMH]]
* [[SCORM]]
* [[XML]]
* [[:m:Learning Object Metadata]]

==References==
{{Reflist}}

== External links ==
{{wikiversity|Introduction to Learning Objects}}
* [http://cancore.athabascau.ca/en/ cancore.athabascau.ca] is a thorough element-by-element guide to implementing the IEEE LOM.
* [http://www.imsglobal.org/metadata/ www.imsglobal.org: IMS Global Learning Consortium Learning resource meta-data specification].
* [http://ltsc.ieee.org/wg12/files/IEEE_1484_12_03_d8_submitted.pdf ltsc.ieee.org: XML Binding Specification].
* [http://www.intrallect.com/support/metadata/ims2lom_metadata_mapping.htm www.intrallect.com: A mapping between the IEEE LOM and IMS Learning Resource Metadata]
* [http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html www.ontopia.net: Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all], 2004.
{{Prone to spam|date=October 2014}}
{{Z148}}&lt;!--     {{No more links}}

       Please be cautious adding more external links.

Wikipedia is not a collection of links and should not be used for advertising.

     Excessive or inappropriate links will be removed.

 See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.

If there are already suitable links, propose additions or replacements on
the article's talk page, or submit your link to the relevant category at
DMOZ (dmoz.org) and link there using {{Dmoz}}.

--&gt;

{{Use dmy dates|date=October 2010}}

{{DEFAULTSORT:Learning Object Metadata}}
[[Category:Data management]]
[[Category:Educational technology]]
[[Category:Knowledge representation]]
[[Category:Library science]]
[[Category:Metadata]]
[[Category:Standards]]
[[Category:Technical communication]]</text>
      <sha1>e336juo1bmbvyc8i6nqaimy1zgo74ue</sha1>
    </revision>
  </page>
  <page>
    <title>Project workforce management</title>
    <ns>0</ns>
    <id>7217055</id>
    <revision>
      <id>706908758</id>
      <parentid>689141010</parentid>
      <timestamp>2016-02-25T23:54:32Z</timestamp>
      <contributor>
        <username>Keith D</username>
        <id>2278355</id>
      </contributor>
      <comment>Remove plus.google from ref publisher field</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15200" xml:space="preserve">'''Project workforce management''' is the practice of combining the coordination of all logistic elements of a project through a single [[software application]] (or [[workflow engine]]). This includes planning and tracking of schedules and mileposts, cost and revenue, resource allocation, as well as overall management of these project elements.  Efficiency is improved by eliminating manual processes, like [[spreadsheet]] tracking&lt;ref&gt;
{{Cite web
| author      = Seema Haji
| title       = Business Intelligence Cures the Spreadsheet Problem
| url         = http://www.refresher.com/asmhbi.html
| publisher   = Refresher Publications Inc.
| year        = 2009
| accessdate  = October 30, 2009
}}&lt;/ref&gt;  to monitor project progress. It also allows for at-a-glance status updates and ideally integrates with existing legacy applications in order to unify ongoing projects, [[enterprise resource planning]] (ERP) and broader organizational goals.&lt;ref&gt;
{{Cite web
| author      = Rudolf Melik
| title       = The Rise of the Project Workforce
| url         = http://www.projectworkforcebook.com/
| publisher   = Wiley: New York, NY
| year        = 2007
| accessdate  = October 30, 2009
}}&lt;/ref&gt; There are a lot of logistic elements in a project. Different team members are responsible for managing each element and often, the organisation may have a mechanism to manage some logistic areas as well.

By coordinating these various components of [[project management]], [[workforce management]] and financials through a single solution, the process of configuring and changing project and workforce details is simplified.

== Introduction ==
&lt;ref&gt;{{Citation|title = Project workforce management|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt; A project workforce management system defines project tasks, project positions, and assigns personnel to the project positions. The project tasks and positions are correlated to assign a responsible project position or even multiple positions to complete each project task. Because each project position may be assigned to a specific person, the qualifications and availabilities of that person can be taken into account when determining the assignment. By associating project tasks and project positions, a manager can better control the assignment of the workforce and complete the project more efficiently.

When it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined. Therefore, all the logistic processes take place in the workflow engine.

== About ==

=== Technical Field ===
This invention relates to project management systems and methods, more particularly to a software-based system and method for project and workforce management.&lt;ref&gt;{{Citation|title = Project workforce management Technical Field|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Software Usage ===
Due to the software usage, all the project workflow management tasks can be fully automated without leaving many tasks for the project managers. This returns high efficiency to the project management when it comes to project tracking proposes. In addition to different tracking mechanisms, project workforce management software also offer a dashboard for the project team. Through the dashboard, the project team has a glance view of the overall progress of the project elements.

Most of the times, project workforce management software can work with the existing legacy software systems such as ERP (enterprise resource planning) systems. This easy integration allows the organisation to use a combination of software systems for management purposes.&lt;ref&gt;{{Citation|title = Project workforce management Software Use|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Background ===
Good project management is an important factor for the success of a project. A project may be thought of as a collection of activities and tasks designed to achieve a specific goal of the organisation, with specific performance or quality requirements while meeting any subject time and cost constraints. Project management refers to managing the activities that lead to the successful completion of a project. Furthermore, it focuses on finite deadlines and objectives. A number of tools may be used to assist with this as well as with assessment.

Project management may be used when planning personnel resources and capabilities. The project may be linked to the objects in a professional services life cycle and may accompany the objects from the opportunity over quotation, contract, time and expense recording, billing, period-end-activities to the final reporting. Naturally the project gets even more detailed when moving through this cycle.&lt;ref&gt;{{Citation|title = Project workforce management background|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

For any given project, several project tasks should be defined. Project tasks describe the activities and phases that have to be performed in the project such as writing of layouts, customising, testing. What is needed is a system that allows project positions to be correlated with project tasks. Project positions describe project roles like project manager, consultant, tester, etc. Project-positions are typically arranged linearly within the project. By correlating project tasks with project positions, the qualifications and availability of personnel assigned to the project positions may be considered.

== Benefits of Project Management ==
&lt;ref&gt;{{Cite web|title = The advantages of project management and how it can help your business|url = https://www.nibusinessinfo.co.uk/content/advantages-project-management-and-how-it-can-help-your-business|website = nibusinessinfo.co.uk|accessdate = 2015-11-04|last = Migrator}}&lt;/ref&gt; Good project management should:
* Reduce the chance of a project failing
* Ensure a minimum level of quality and that results meet requirements and expectations
* Free up other staff members to get on with their area of work and increase efficiency both on the project and within the business
* Make things simpler and easier for staff with a single point of contact running the overall project
* Encourage consistent communications amongst staff and suppliers
* Keep costs, timeframes and resources to budget

== Workflow Engine ==
When it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined in them. So, all the logistic processes take place in the workflow engine.

The regular and most common types of tasks handled by project workforce management software or a similar workflow engine are:

=== Planning and Monitoring the Project Schedules and Milestones ===
Regularly monitoring your project&#8217;s schedule performance can provide early indications of possible activity-coordination problems, resource conflicts, and possible cost overruns. To monitor schedule performance. Collecting information and evaluating it ensure a project accuracy.&lt;ref&gt;{{Cite web|title = How to Monitor Project-Schedule Performance - For Dummies|url = http://www.dummies.com/how-to/content/how-to-monitor-schedule-performance.html|website = www.dummies.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Tracking the Cost and Revenue aspects of Projects ===
The importance of tracking actual costs and resource usage in projects depends upon the project situation.

Tracking actual costs and resource usage is an essential aspect of the project control function.&lt;ref&gt;{{Cite web|title = Why Track Actual Costs and Resource Usage on Projects?|url = http://www.projecttimes.com/articles/why-track-actual-costs-and-resource-usage-on-projects.html|website = www.projecttimes.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Resource Utilisation and Monitoring ===
Organisational profitability is directly connected to project management efficiency and optimal resource utilisation.To sum up, organisations that struggle with either or both of these core competencies typically experience cost overruns, schedule delays and unhappy customers.&lt;ref&gt;{{Cite web|title = Resource Utilization in Project Management|url = https://www.clarizen.com/work/resource-utilization-in-project-management|website = www.clarizen.com|accessdate = 2015-11-04}}&lt;/ref&gt;

The focus for project management is the analysis of project performance to determine whether a change is needed in the plan for the remaining project activities to achieve the project goals.&lt;ref&gt;{{Cite web|title = Project Management Guru Monitoring and Controlling Tools|url = http://www.projectmanagementguru.com/controlling.html|website = www.projectmanagementguru.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Other Management Aspects of the Project Management&lt;ref&gt;{{Cite web|title = Project Management Guide - How to Manage a Project {{!}} TeamGantt|url = http://teamgantt.com/guide-to-project-management/|website = teamgantt.com|accessdate = 2015-11-04}}&lt;/ref&gt; ===

==== Project risk management ====
Risk identification consists of determining which risks are likely to affect the project and documenting the characteristics of each.

==== Project communication management ====
Project communication management is about how communication is carried out during the course of the project

==== Project quality management ====
It is of no use completing a project within the set time and budget if the final product is of poor quality. The project manager has to ensure that the final product meets the quality expectations of the stakeholders. This is done by good: &#131;

===== ''Quality Planning:'' =====
Identifying what quality standards are relevant to the project and determining how to meet them.

===== ''Quality Assurance:'' =====
Evaluating overall project performance on a regular basis to provide confidence that the project will satisfy the relevant quality standards.

===== ''Quality Control:'' =====

Monitoring specific project results to determine if they comply with relevant quality standards and identifying ways to remove causes of poor performance.

==Project Workforce Management vs. Traditional Management==
There are three main differences between Project Workforce Management and traditional [[project management]] and [[workforce management]] disciplines and solutions:&lt;ref&gt;{{Cite web
|author = Rudolf Melik|title = The Rise of the Project Workforce|url = https://books.google.co.uk/books?id=0b2RB81RqyQC&amp;pg=PA121&amp;lpg=PA121&amp;dq=the+rise+of+project+workforce+pdf&amp;source=bl&amp;ots=_Io_xYQd2Q&amp;sig=4KO0i1Gr5m_XoybVwJHqfP0enHk&amp;hl=en&amp;sa=X&amp;ved=0CDIQ6AEwBGoVChMImJOZ3a33yAIVQ70aCh1yGAq4#v=onepage&amp;q=the%20rise%20of%20project%20workforce%20pdf&amp;f=false|publisher = Wiley: New York, NY|year = 2007|accessdate = November 4, 2015}}&lt;/ref&gt;

=== Workflow-driven ===
All project and workforce processes are designed, controlled and audited using a built-in graphical workflow engine. Users can design, control and audit the different processes involved in the project. The graphical workflow is quite attractive for the users of the system and allows the users to have a clear idea of the workflow engine.&lt;ref&gt;{{Cite book|title = Flexibility of Data-Driven Process Structures|url = http://link.springer.com/chapter/10.1007/11837862_19|publisher = Springer Berlin Heidelberg|date = 2006-09-04|isbn = 978-3-540-38444-1|pages = 181&#8211;192|series = Lecture Notes in Computer Science|first = Dominic|last = M&#252;ller|first2 = Manfred|last2 = Reichert|first3 = Joachim|last3 = Herbst|editor-first = Johann|editor-last = Eder|editor-first2 = Schahram|editor-last2 = Dustdar}}&lt;/ref&gt;

=== Organisation and Work Breakdown Structures ===
Project Workforce Management provides organization and work breakdown structures to create, manage and report on functional and approval hierarchies, and to track information at any level of detail. Users can create, manage, edit and report work breakdown structures. Work breakdown structures have different abstraction levels, so the information can be tracked at any level. Usually, project workforce management has approval hierarchies. Each workflow created will go through several records before it becomes an organisational or project standard. This helps the organisation to reduce the inefficiencies of the process, as it is audited by many stakeholders.&lt;ref&gt;{{Cite web|title = Organisational Breakdown Structure|url = http://www.successful-project-management.com/organisational-breakdown-structure.html|website = www.successful-project-management.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Connected Project, Workforce and Financial Processes ===
Unlike traditional disconnected project, workforce and billing management systems that are solely focused on tracking IT projects, internal workforce costs or billable projects, Project Workforce Management is designed to unify the coordination of all project and workforce processes, whether internal, shared (IT) or billable.

== Summary ==
A project workforce management system defines project tasks, project positions and assigns personnel to the project positions. The project tasks and project positions are correlated to assign a responsible project position or positions to complete each project task. Because each project position may be assigned to a specific person, the qualification and availabilities of the person can be taken into account when determining the assignment. By correlating the project tasks and project positions, a manager can better control the assignment of the workforce and complete projects more efficiently.&lt;ref&gt;{{Citation|title = Project workforce management abstract|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

Project workflow management is one of the best methods for managing different aspects of project. If the project is complex, then the outcomes for the project workforce management could be more effective.

For simple projects or small organisations, project workflow management may not add much value, but for more complex projects and big organisations, managing project workflow will make a big difference. This is because that small organisations or projects do not have a significant overhead when it comes to managing processes. There are many project workforce management, but many organisations prefer to adopt unique solutions.

Therefore, organisation gets software development companies to develop custom project workflow managing systems for them. This has proved to be the most suitable way of getting the best project workforce management system acquired for the company.

==Literature==
*{{Cite book
 | first = Rudolf
 | last = Melik
 | authorlink =
 | year = 2007
 | title = The Rise of the Project Workforce
 | edition =
 | publisher = Willey
 | location = New York, NY
 | isbn = 0-470-12430-X
}}

==References==
{{Wikiquote}}
{{Reflist}}

[[Category:Data management]]
[[Category:ERP software]]
[[Category:Project management]]
[[Category:Workflow technology]]</text>
      <sha1>jzl9pgkv8qovm896b97m5mo9g34pjq2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data analysis</title>
    <ns>14</ns>
    <id>14482748</id>
    <revision>
      <id>720925070</id>
      <parentid>711443396</parentid>
      <timestamp>2016-05-18T19:52:47Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>moved [[Category:Statistics]] one level down</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="122" xml:space="preserve">{{Commons category|Data analysis}}
{{cat main|Data analysis}}

[[Category:Analysis]]
[[Category:Data management|Analysis]]</text>
      <sha1>7z3xmbvynnpo01zsqbuqodxmv6zwzgk</sha1>
    </revision>
  </page>
  <page>
    <title>Record linkage</title>
    <ns>0</ns>
    <id>978951</id>
    <revision>
      <id>758742592</id>
      <parentid>756022920</parentid>
      <timestamp>2017-01-07T08:35:25Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor />
      <comment>/* Data preprocessing */Journal cites, set missing volume/pages parameter,  using [[Project:AWB|AWB]] (12142)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="30872" xml:space="preserve">'''Record linkage''' (RL) refers to the task of finding [[Record (database)|records]] in a data set that refer to the same [[entity]] across different data sources (e.g., data files, books, websites, databases).  Record linkage is necessary when [[Join (SQL)|joining]] data sets based on entities that may or may not share a common identifier (e.g., [[Relational model|database key]], [[Uniform Resource Identifier|URI]], [[National identification number]]), as may be the case due to differences in record shape, storage location, and/or curator style or preference.  A data set that has undergone RL-oriented reconciliation may be referred to as being ''cross-linked''. 
Record Linkage is called Data Linkage in many jurisdictions, but is the same process.

== History ==
The initial idea of record linkage goes back to [[Halbert L. Dunn]] in his 1946 article titled "Record Linkage" published in the ''[[American Journal of Public Health]]''.&lt;ref&gt;{{cite journal
 | first = Halbert L. | last = Dunn | authorlink = Halbert L. Dunn
 | title = Record Linkage
 | journal = [[American Journal of Public Health]]
 |date=December 1946 | volume = 36 | issue = 12 | pages = ''pp.'' 1412&amp;ndash;1416
 | url = http://www.ajph.org/cgi/reprint/36/12/1412
 | format = PDF
 | accessdate = 2008-05-31
 | doi = 10.2105/AJPH.36.12.1412
}}&lt;/ref&gt;  Howard Borden Newcombe laid the probabilistic foundations of modern record linkage theory in a 1959 article in ''[[Science (journal)|Science]]'',&lt;ref&gt;{{cite journal|last=Newcombe|first=H. B. |author2=J.M. Kennedy |author3=S.J. Axford |author4=A. P. James|title=Automatic Linkage of Vital Records|journal=Science|date=October 1959|volume=130|issue=3381|pages=954&#8211;959|doi=10.1126/science.130.3381.954|pmid=14426783}}&lt;/ref&gt; which were then formalized in 1969 by [[Ivan Fellegi]] and Alan Sunter who proved that the probabilistic decision rule they described was optimal when the comparison attributes were conditionally independent.  Their pioneering work "A Theory For Record Linkage"&lt;ref name=FellegiSunter&gt;{{cite journal 
 | first = Ivan  | last = Fellegi | authorlink = Ivan Fellegi 
 |author2=Sunter, Alan 
  | title = A Theory for Record Linkage 
 | journal = [[Journal of the American Statistical Association]] 
 |date=December 1969 | volume = 64 | issue = 328 | pages = ''pp.'' 1183&amp;ndash;1210  
 | jstor = 2286061
 | doi = 10.2307/2286061 
| url = http://courses.cs.washington.edu/courses/cse590q/04au/papers/Felligi69.pdf | format = PDF
 }}&lt;/ref&gt; remains the mathematical foundation for many record linkage applications even today.

Since the late 1990s, various machine learning techniques have been developed that can, under favorable conditions, be used to estimate the conditional probabilities required by the Fellegi-Sunter (FS) theory.  Several researchers have reported that the conditional independence assumption of the FS algorithm is often violated in practice; however, published efforts to explicitly model the conditional dependencies among the comparison attributes have not resulted in an improvement in record linkage quality.
{{Citation needed|date=May 2007}} On the other hand, machine learning or neural network algorithms that do not rely on these assumptions often provide far higher accuracy, when sufficient labeled training data is available.&lt;ref name="ReferenceA"&gt;{{cite conference | first = D. Randall | last = Wilson, D. Randall | title = Beyond Probabilistic Record Linkage: Using Neural Networks and Complex Features to Improve Genealogical Record Linkage | conference = Proceedings of International Joint Conference on Neural Networks | location = San Jose, California, USA | date = July 31 &#8211; August 5, 2011 | url = http://axon.cs.byu.edu/~randy/pubs/wilson.ijcnn2011.beyondprl.pdf}}&lt;/ref&gt;

Record linkage can be done entirely without the aid of a computer, but the primary reasons computers are often used for record linkage are to reduce or eliminate manual review and to make results more easily reproducible.  Computer matching has the advantages of allowing central supervision of processing, better quality control, speed, consistency, and better reproducibility of results.&lt;ref&gt;{{cite web|last=Winkler|first=William E.|title=Matching and Record Linkage|url=http://www.census.gov/srd/papers/pdf/rr93-8.pdf|publisher=U.S. Bureau of the Census|accessdate=12 November 2011}}&lt;/ref&gt;

== Naming conventions ==
"Record linkage" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity.  Commercial mail and database applications refer to it as "merge/purge processing" or "list washing".  [[computer science|Computer scientists]] often refer to it as "data matching" or as the "object identity problem".  Other names used to describe the same concept include: "coreference/entity/identity/name/record resolution", "entity disambiguation/linking", "duplicate detection", "deduplication", "record matching", "(reference) reconciliation", "object identification", "data/information integration" and "conflation".&lt;ref&gt;http://homes.cs.washington.edu/~pedrod/papers/icdm06.pdf&lt;/ref&gt;  This profusion of terminology has led to few cross-references between these research communities.&lt;ref&gt;[http://datamining.anu.edu.au/linkage.html Cristen, P &amp; T: Febrl - Freely extensible biomedical record linkage (Manual, release 0.3) p.9]&lt;/ref&gt;&lt;ref&gt;
{{cite journal
 | first = Ahmed  | last = Elmagarmid 
 |author2=Panagiotis G. Ipeirotis |author3=Vassilios Verykios 
  | title = Duplicate Record Detection: A Survey 
 | journal = IEEE Transactions on Knowledge and Data Engineering 
 |date=January 2007  | volume = 19  | issue = 1  | pages = ''pp.'' 1&amp;ndash;16 
 | url = http://www.cs.purdue.edu/homes/ake/pub/TKDE-0240-0605-1.pdf | format = PDF  | accessdate = 2009-03-30
 | doi = 10.1109/TKDE.2007.9
}}
&lt;/ref&gt;

While they share similar names, record linkage and [[Linked Data]] are two separate concepts.  Whereas record linkage focuses on the more narrow task of identifying matching entities across different data sets, Linked Data focuses on the broader methods of structuring and publishing data to facilitate the discovery of related information.

== Methods ==

=== Data preprocessing ===
Record linkage is highly sensitive to the quality of the data being linked, so all data sets under consideration (particularly their key identifier fields) should ideally undergo a [[data quality assessment]] prior to record linkage.  Many key identifiers for the same entity can be presented quite differently between (and even within) data sets, which can greatly complicate record linkage unless understood ahead of time.  For example, key identifiers for a man named William J. Smith might appear in three different data sets as so:

{| class="wikitable"
|-
! Data set !! Name !! Date of birth !! City of residence
|-
| Data set 1 || William J. Smith || 1/2/73 || Berkeley, California
|-
| Data set 2 || Smith, W. J. || 1973.1.2 || Berkeley, CA
|-
| Data set 3 || Bill Smith || Jan 2, 1973 || Berkeley, Calif.
|}

In this example, the different formatting styles lead to records that look different but in fact all refer to the same entity with the same logical identifier values.  Most, if not all, record linkage strategies would result in more accurate linkage if these values were first ''normalized'' or ''standardized'' into a consistent format (e.g., all names are "Surname, Given name", and all dates are "YYYY/MM/DD").  Standardization can be accomplished through simple rule-based [[data transformation]]s or more complex procedures such as lexicon-based [[Tokenization (lexical analysis)|tokenization]] and probabilistic hidden Markov models.&lt;ref&gt;{{cite journal|last=Churches|first=Tim|author2=Peter Christen |author3=Kim Lim |author4=Justin Xi Zhu |title=Preparation of name and address data for record linkage using hidden Markov models|journal=BMC Medical Informatics and Decision Making|date=13 December 2002|volume=2|doi=10.1186/1472-6947-2-9|url=http://www.biomedcentral.com/1472-6947/2/9 |pages=9}}&lt;/ref&gt;  Several of the packages listed in the ''Software Implementations'' section provide some of these features to simplify the process of data standardization.

===Entity resolution===
'''Entity resolution''' is an operational [[intelligence]] process, typically powered by an entity resolution engine or [[middleware]], whereby organizations can connect disparate data sources with a [[Opinion|view]] to understanding possible entity matches and non-obvious relationships across multiple [[data silos]]. It analyzes all of the [[information]] relating to individuals and/or entities from multiple sources of data, and then applies likelihood and probability scoring to determine which identities are a match and what, if any, non-obvious relationships exist between those identities.

Entity resolution engines are typically used to uncover [[risk]], [[fraud]], and conflicts of interest, but are also useful tools for use within [[Customer Data Integration]] (CDI) and [[Master Data Management]] (MDM) requirements. Typical uses for entity resolution engines include terrorist screening, insurance fraud detection, [[USA Patriot Act]] compliance, [[Organized retail crime]] ring detection and applicant screening.

For example: Across different data silos - employee records, vendor data, watch lists, etc. - an organization may have several variations of an entity named ABC, which may or may not be the same individual. These entries may, in fact, appear as ABC1, ABC2, or ABC3 within those data sources. By comparing similarities between underlying attributes such as [[Address (geography)|address]], [[date of birth]], or [[social security number]], the user can eliminate some possible matches and confirm others as very likely matches.

Entity resolution engines then apply rules, based on common sense logic, to identify hidden relationships across the data. In the example above, perhaps ABC1 and ABC2 are not the same individual, but rather two distinct people who share common attributes such as address or phone number.

====Data Matching====
While entity resolution solutions include data matching technology, many data matching offerings do not fit the definition of entity resolution. Here are four factors that distinguish entity resolution from data matching, according to John Talburt, director of the [[Ualr|UALR]] Center for Advanced Research in Entity Resolution and Information Quality:

* Works with both structured and unstructured records, and it entails the process of extracting references when the sources are unstructured or semi-structured
* Uses elaborate business rules and concept models to deal with missing, conflicting, and corrupted information
* Utilizes non-matching, asserted linking (associate) information in addition to direct matching
* Uncovers non-obvious relationships and association networks (i.e. who's associated with whom)

In contrast to data quality products, more powerful identity resolution engines also include a rules engine and workflow process, which apply business intelligence to the resolved identities and their relationships. These advanced technologies make automated decisions and impact business processes in real time, limiting the need for human intervention.

=== Deterministic record linkage ===
The simplest kind of record linkage, called ''deterministic'' or ''rules-based record linkage'', generates links based on the number of individual identifiers that match among the available data sets.&lt;ref&gt;{{cite journal|last=Roos|first=LL|author2=Wajda A |title=Record linkage strategies. Part I: Estimating information and evaluating approaches.|journal=Methods of Information in Medicine|date=April 1991|volume=30|issue=2|pages=117&#8211;123|pmid=1857246}}&lt;/ref&gt;  Two records are said to match via a deterministic record linkage procedure if all or some identifiers (above a certain threshold) are identical.  Deterministic record linkage is a good option when the entities in the data sets are identified by a common identifier, or when there are several representative identifiers (e.g., name, date of birth, and sex when identifying a person) whose quality of data is relatively high.

As an example, consider two standardized data sets, Set A and Set B, that contain different bits of information about patients in a hospital system.  The two data sets identify patients using a variety of identifiers: [[Social Security Number]] (SSN), name, date of birth (DOB), sex, and [[ZIP code]] (ZIP).  The records in two data sets (identified by the "#" column) are shown below:

{| class="wikitable"
|-
! Data Set !! # !! SSN !! Name !! DOB !! Sex !! ZIP
|-
| rowspan="4" | Set A || 1 || 000956723 || Smith, William || 1973/01/02 || Male || 94701
|- style="background:#f0f0f0;"
| 2 || 000956723 || Smith, William || 1973/01/02 || Male || 94703
|-
| 3 || 000005555 || Jones, Robert || 1942/08/14 || Male || 94701
|- style="background:#f0f0f0;"
| 4 || 123001234 || Sue, Mary || 1972/11/19 || Female || 94109
|-
| rowspan="2" | Set B || 1 || 000005555 ||Jones, Bob || 1942/08/14 || ||
|- style="background:#f0f0f0;"
| 2 || || Smith, Bill || 1973/01/02 || Male || 94701
|}

The most simple deterministic record linkage strategy would be to pick a single identifier that is assumed to be uniquely identifying, say SSN, and declare that records sharing the same value identify the same person while records not sharing the same value identify different people.  In this example, deterministic linkage based on SSN would create entities based on A1 and A2; A3 and B1; and A4.  While A1, A2, and B2 appear to represent the same entity, B2 would not be included into the match because it is missing a value for SSN.

Handling exceptions such as missing identifiers involves the creation of additional record linkage rules.  One such rule in the case of missing SSN might be to compare name, date of birth, sex, and ZIP code with other records in hopes of finding a match.  In the above example, this rule would still not match A1/A2 with B2 because the names are still slightly different: standardization put the names into the proper (Surname, Given name) format but could not discern "Bill" as a nickname for "William".  Running names through a [[phonetic algorithm]] such as [[Soundex]], [[NYSIIS]], or [[metaphone]], can help to resolve these types of problems (though it may still stumble over surname changes as the result of marriage or divorce), but then B2 would be matched only with A1 since the ZIP code in A2 is different.  Thus, another rule would need to be created to determine whether differences in particular identifiers are acceptable (such as ZIP code) and which are not (such as date of birth).

As this example demonstrates, even a small decrease in data quality or small increase in the complexity of the data can result in a very large increase in the number of rules necessary to link records properly.  Eventually, these linkage rules will become too numerous and interrelated to build without the aid of specialized software tools.  In addition, linkage rules are often specific to the nature of the data sets they are designed to link together.  One study was able to link the Social Security [[Death Master File]] with two hospital registries from the [[Midwestern United States]] using SSN, NYSIIS-encoded first name, birth month, and sex, but these rules may not work as well with data sets from other geographic regions or with data collected on younger populations.&lt;ref&gt;{{cite journal|last=Grannis|first=SJ|author2=Overhage JM |author3=McDonald CJ |title=Analysis of identifier performance using a deterministic linkage algorithm|journal=Proc AMIA Symp.|year=2002|pages=305&#8211;9|pmid=12463836|pmc=2244404}}&lt;/ref&gt;  Thus, continuous maintenance testing of these rules is necessary to ensure they continue to function as expected as new data enter the system and need to be linked.  New data that exhibit different characteristics than was initially expected could require a complete rebuilding of the record linkage rule set, which could be a very time-consuming and expensive endeavor.

=== Probabilistic record linkage ===
''Probabilistic record linkage'', sometimes called ''fuzzy matching'' (also ''probabilistic merging'' or ''fuzzy merging'' in the context of merging of databases), takes a different approach to the record linkage problem by taking into account a wider range of potential identifiers, computing weights for each identifier based on its estimated ability to correctly identify a match or a non-match, and using these weights to calculate the probability that two given records refer to the same entity.  Record pairs with probabilities above a certain threshold are considered to be matches, while pairs with probabilities below another threshold are considered to be non-matches; pairs that fall between these two thresholds are considered to be "possible matches" and can be dealt with accordingly (e.g., human reviewed, linked, or not linked, depending on the requirements).  Whereas deterministic record linkage requires a series of potentially complex rules to be programmed ahead of time, probabilistic record linkage methods can be "trained" to perform well with much less human intervention.

Many probabilistic record linkage algorithms assign match/non-match weights to identifiers by means of two probabilities called ''u'' and ''m''. The ''u'' probability is the probability that an identifier in two ''non-matching'' records will agree purely by chance.  For example, the ''u'' probability for birth month (where there are twelve values that are approximately uniformly distributed) is 1/12 &#8776; 0.083; identifiers with values that are not uniformly distributed will have different ''u'' probabilities for different values (possibly including missing values).  The ''m'' probability is the probability that an identifier in ''matching'' pairs will agree (or be sufficiently similar, such as strings with high [[Jaro-Winkler distance]] or low [[Levenshtein distance]]).  This value would be 1.0 in the case of perfect data, but given that this is rarely (if ever) true, it can instead be estimated.  This estimation may be done based on prior knowledge of the data sets, by manually identifying a large number of matching and non-matching pairs to "train" the probabilistic record linkage algorithm, or by iteratively running the algorithm to obtain closer estimations of the ''m'' probability.  If a value of 0.95 were to be estimated for the ''m'' probability, then the match/non-match weights for the birth month identifier would be:

{| class="wikitable"
|-
! Outcome !! Proportion of links !! Proportion of non-links !! Frequency ratio !! Weight
|-
| Match || ''m'' = 0.95 || ''u'' &#8776; 0.083 || ''m''/''u'' &#8776; 11.4 || ln(''m''/''u'')/ln(2) &#8776; 3.51
|-
| Non-match || 1&#8722;''m'' = 0.05 || 1-''u'' &#8776; 0.917 || (1-''m'')/(1-''u'') &#8776; 0.0545 || ln((1-''m'')/(1-''u''))/ln(2) &#8776; -4.20
|}

The same calculations would be done for all other identifiers under consideration to find their match/non-match weights.  Then, every identifier of one record would be compared with the corresponding identifier of another record to compute the total weight of the pair: the ''match'' weight is added to the running total whenever a pair of identifiers agree, while the ''non-match'' weight is added (i.e. the running total decreases) whenever the pair of identifiers disagrees.  The resulting total weight is then compared to the aforementioned thresholds to determine whether the pair should be linked, non-linked, or set aside for special consideration (e.g. manual validation).&lt;ref name="prl"&gt;{{cite journal|last=Blakely|first=Tony|author2=Salmond, Clare |title=Probabilistic record linkage and a method to calculate the positive predictive value|journal=International Journal of Epidemiology|date=December 2002|volume=31|issue=6|pages=1246&#8211;1252|doi=10.1093/ije/31.6.1246|pmid=12540730|url=http://ije.oxfordjournals.org/content/31/6/1246.full}}&lt;/ref&gt;

Determining where to set the match/non-match thresholds is a balancing act between obtaining an acceptable [[Sensitivity and specificity#Sensitivity|sensitivity]] (or ''recall'', the proportion of truly matching records that are linked by the algorithm) and [[positive predictive value]] (or ''precision'', the proportion of records linked by the algorithm that truly do match).  Various manual and automated methods are available to predict the best thresholds, and some record linkage software packages have built-in tools to help the user find the most acceptable values.  Because this can be a very computationally demanding task, particularly for large data sets, a technique known as ''blocking'' is often used to improve efficiency.  Blocking attempts to restrict comparisons to just those records for which one or more particularly discriminating identifiers agree, which has the effect of increasing the positive predictive value (precision) at the expense of sensitivity (recall).&lt;ref name=prl /&gt;  For example, blocking based on a phonetically coded surname and ZIP code would reduce the total number of comparisons required and would improve the chances that linked records would be correct (since two identifiers already agree), but would potentially miss records referring to the same person whose surname or ZIP code was different (due to marriage or relocation, for instance).  Blocking based on birth month, a more stable identifier that would be expected to change only in the case of data error, would provide a more modest gain in positive predictive value and loss in sensitivity, but would create only twelve distinct groups which, for extremely large data sets, may not provide much net improvement in computation speed.  Thus, robust record linkage systems often use multiple blocking passes to group data in various ways in order to come up with groups of records that should be compared to each other.

===Machine learning===
In recent years, a variety of machine learning techniques have been used in record linkage.  It has been recognized&lt;ref name="ReferenceA"/&gt; that a classic algorithm for probabilistic record linkage is equivalent to the [[Naive Bayes]] algorithm in the field of machine learning,&lt;ref&gt;Quass, Dallan, and Starkey, Paul. &#8220;Record Linkage for Genealogical Databases,&#8221; ACM SIGKDD &#8217;03 Workshop on Data Cleaning, Record Linkage, and Object Consolidation, August 24&#8211;27, 2003, Washington, D.C.&lt;/ref&gt; and suffers from the same assumption of the independence of its features (an assumption that is typically not true).&lt;ref&gt;Langley, Pat, Wayne Iba, and Kevin Thompson. &#8220;An Analysis of Bayesian Classifiers,&#8221; In Proceedings of the 10th National Conference on Artificial Intelligence, (AAAI-92), AAAI Press/MIT Press, Cambridge, MA, pp. 223-228, 1992.&lt;/ref&gt;&lt;ref&gt;Michie, D., D. Spiegelhalter, and C. Taylor. Machine Learning, Neural and Statistical Classification, Ellis Horwood, Hertfordshire, England. Book 19, 1994.&lt;/ref&gt;  Higher accuracy can often be achieved by using various other machine learning techniques, including a single-layer [[perceptron]].&lt;ref name="ReferenceA"/&gt;

== Mathematical model ==
In an application with two files, A and B, denote the rows (''records'') by &lt;math&gt;\alpha (a)&lt;/math&gt; in file A and &lt;math&gt;\beta (b)&lt;/math&gt; in file B. Assign &lt;math&gt;K&lt;/math&gt; ''characteristics'' to each record. The set of records that represent identical entities is defined by

&lt;math&gt; M = \left\{ (a,b); a=b; a \in A; b \in B \right\} &lt;/math&gt;

and the complement of set &lt;math&gt;M&lt;/math&gt;, namely set &lt;math&gt;U&lt;/math&gt; representing different entities is defined as

&lt;math&gt; U = \{ (a,b); a \neq b; a \in A, b \in B \} &lt;/math&gt;.

A vector, &lt;math&gt;\gamma&lt;/math&gt; is defined, that contains the coded agreements and disagreements on each characteristic:

&lt;math&gt; \gamma \left[ \alpha ( a ), \beta ( b ) \right] = \{ \gamma^{1} \left[ \alpha ( a ) , \beta ( b ) \right] ,...,	\gamma^{K} \left[ \alpha ( a ), \beta ( b ) \right] \} &lt;/math&gt;

where &lt;math&gt;K&lt;/math&gt; is a subscript for the characteristics (sex, age, marital status, etc.) in the files. The conditional probabilities of observing a specific vector &lt;math&gt;\gamma&lt;/math&gt; given &lt;math&gt;(a, b) \in M&lt;/math&gt;, &lt;math&gt;(a, b) \in U&lt;/math&gt; are defined as

&lt;math&gt;
 m(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in M \right\} =
 \sum_{(a, b) \in M} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot
                 P \left[ (a, b) | M\right]
&lt;/math&gt;

and

&lt;math&gt;
 u(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in U \right\} =
 \sum_{(a, b) \in U} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot
                 P \left[ (a, b) | U\right],
&lt;/math&gt;
respectively.&lt;ref name=FellegiSunter /&gt;

== Applications ==

===Master data management===
Most [[Master data management]] (MDM) products use a record linkage process to identify records from different sources representing the same real-world entity. This linkage is used to create a "golden master record" containing the cleaned, reconciled data about the entity. The techniques used in MDM are the same as for record linkage generally. MDM expands this matching not only to create a "golden master record" but to infer relationships also. (i.e. a person has a same/similar surname and same/similar address, this might imply they share a household relationship).

=== Data warehousing and business intelligence ===
Record linkage plays a key role in [[data warehousing]] and [[business intelligence]].  Data warehouses serve to combine data from many different operational source systems into one [[logical data model]], which can then be subsequently fed into a business intelligence system for reporting and analytics.  Each operational source system may have its own method of identifying the same entities used in the logical data model, so record linkage between the different sources becomes necessary to ensure that the information about a particular entity in one source system can be seamlessly compared with information about the same entity from another source system.  Data standardization and subsequent record linkage often occur in the "transform" portion of the [[extract, transform, load]] (ETL) process.

=== Historical research ===
Record linkage is important to social history research since most data sets, such as [[census|census records]] and parish registers were recorded long before the invention of [[National identification number]]s.  When old sources are digitized, linking of data sets is a prerequisite for [[longitudinal study]].  This process is often further complicated by lack of standard spelling of names, family names that change according to place of dwelling, changing of administrative boundaries, and problems of checking the data against other sources.  Record linkage was among the most prominent themes in the [[History and computing]] field in the 1980s, but has since been subject to less attention in research.{{Citation needed|date=November 2011}}

=== Medical practice and research ===
&lt;!-- any experts out there? --&gt;
Record linkage is an important tool in creating data required for examining the health of the public and of the health care system itself. It can be used to improve data holdings, data collection, quality assessment, and the dissemination of information. Data sources can be examined to eliminate duplicate records, to identify under-reporting and missing cases (e.g., census population counts), to create person-oriented health statistics, and to generate disease registries and health surveillance systems. Some cancer registries link various data sources (e.g., hospital admissions, pathology and clinical reports, and death registrations) to generate their registries. Record linkage is also used to create health indicators. For example, fetal and infant mortality is a general indicator of a country's socioeconomic development, public health, and maternal and child services. If infant death records are matched to birth records, it is possible to use birth variables, such as birth weight and gestational age, along with mortality data, such as cause of death, in analyzing the data.  Linkages can help in follow-up studies of cohorts or other groups to determine factors such as vital status, residential status, or health outcomes. Tracing is often needed for follow-up of industrial cohorts, clinical trials, and longitudinal surveys to obtain the cause of death and/or cancer.  An example of a successful and long-standing record linkage system allowing for population-based medical research is the [[Rochester Epidemiology Project]] based in [[Rochester, Minnesota]].&lt;ref name="data resource profile"&gt;{{cite journal | author1=St. Sauver JL  | author2=Grossardt BR | author3=Yawn BP | author4=Melton LJ 3rd | author5=Pankratz JJ |author6=Brue SM | author7=Rocca WA. | title = Data Resource Profile: The Rochester Epidemiology Project (REP) medical records-linkage system | journal = Int J Epidemiol | volume=41 | issue=6 | pages=1614&#8211;24 | year = 2012 | pmid = 23159830 | doi=10.1093/ije/dys195 | url=http://ije.oxfordjournals.org/content/41/6/1614.long}}&lt;/ref&gt;

== Criticism of existing software implementations==
The main reasons cited are:
* '''Project costs''': costs typically in the hundreds of thousands of dollars
* '''Time''': lack of enough time to deal with large-scale [[data cleansing]] software
* '''Security''': concerns over sharing information, giving an application access across systems, and effects on legacy systems

== See also ==
* [[Capacity optimization]]
* [[Content-addressable storage]]
* [[Data deduplication]]
* [[Delta encoding]]
* [[Entity linking]]
* [[Entity-attribute-value model]]
* [[Identity resolution]]
* [[Linked data]]
* [[Named-entity recognition]]
* [[Open data]]
* [[Schema matching]]
* [[Single-instance storage]]

== Notes and references ==
{{Reflist|2}}

== External links ==
* [http://pike.psu.edu/linkage/ Data Linkage Project at Penn State, USA]
* [http://www.datadecision.com Datadecision - Data matching online tool]
* [http://www.nameapi.org/en/demos/name-matcher/ NameAPI - Name Matcher]
* [http://sourceforge.net/projects/oysterer/ OYSTER Entity Resolution]
* [http://datamining.anu.edu.au/ Febrl - Freely Extensible Biomedical Record Linkage]
* [http://infolab.stanford.edu/serf/ Stanford Entity Resolution Framework]
* [http://dbs.uni-leipzig.de/de/research/projects/large_scale_object_matching/ Dedoop - Deduplication with Hadoop]
* [https://sourceforge.net/projects/erframework/ BlockingFramework A framework for blocking-based Entity Resolution]
* [http://www.ipdln.org/ International Population Data Linkage Network]
* [https://github.com/yahoo/FEL Yahoo Fast Entity Linker Core]

{{DEFAULTSORT:Record Linkage}}
[[Category:Data management]]</text>
      <sha1>acbyzgoc9m1uqnmjuvn18yscshm2o4p</sha1>
    </revision>
  </page>
  <page>
    <title>Meta-data management</title>
    <ns>0</ns>
    <id>3198327</id>
    <revision>
      <id>751386758</id>
      <parentid>681726753</parentid>
      <timestamp>2016-11-25T09:29:54Z</timestamp>
      <contributor>
        <username>Jane023</username>
        <id>1213535</id>
      </contributor>
      <comment>/* Wikipedia metadata */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4754" xml:space="preserve">[[File:Linnaeus - Regnum Animale (1735).png|thumb|[[Linnaean taxonomy]], a metadata system used historically for grouping animals in zoos, first published in 1735]]
[[File:6123034166 card catalog.jpg|thumb|[[Card catalog]] and digital media access point]]
'''Meta-data management''' (also known as [[metadata]] management, without the hyphen) involves managing [[data]] about ''other data'', whereby this "other data" is generally referred to as ''content'' data. The term is used most often in relation to [[Digital media]], but older forms of metadata are catalogs, dictionaries, and taxonomies. For example, the [[Dewey Decimal Classification]] is a metadata management system for books developed in 1876 for libraries.

==Metadata schema==
Metadata management can be defined as the end-to-end process and governance framework for creating, controlling, enhancing, attributing, defining and managing a metadata schema, model or other structured aggregation system, either independently or within a repository and the associated supporting processes (often to enable the management of content). For web-based systems, [[Uniform Resource Locator|URL]]s, images, video etc. may be referenced from a triples table of object, attribute and value.
==Scope==
With specific [[knowledge domain]]s, the boundaries of the metadata for each must be managed, since a general [[ontology]] is not useful to experts in one field whose language is knowledge-domain specific.
==Metadata manager==
If one is in the process of making a knowledge management solution, creating a metadata schema and developing a system in which metadata is managed are very important. In such a project, a dedicated metadata manager may be appointed in order to maintain adherence to metadata and information management standards. {{Citation needed|date=January 2011}} This is a person who will be responsible for the metadata strategy, and possibly, the implementation. A metadata manager does not need to know about and be involved with everything concerning the solution, but it does help to have an understanding of as much of the process as possible to make sure a relevant schema is developed.
==Metadata management over time==

Managing the metadata in a knowledge management solution is an important step in a metadata strategy. It is part of the strategy to make sure that the metadata are complete, current and correct at any given time. Managing a metadata project is also about making sure that users of the system are aware of the possibilities allowed by a well-designed metadata system and how to maximize the benefits of metadata. Regularly monitoring the metadata to ensure that the schema remains relevant is advised.

===Wikipedia metadata===
Wikipedia is a project that actively manages metadata for its articles and files. For example, volunteer editors carefully curate new biographical articles based on the notability (''claim to fame''), name, birth, and/or death dates.&lt;ref&gt;See the internal Wikipedia project on the English Wikipedia called [[Wikipedia:WikiProject Biography]]&lt;/ref&gt; Similarly, volunteer editors carefully curate new architectural articles based on name, municipality, or [[geo coordinates]].&lt;ref&gt;See [[Wikipedia:WikiProject Architecture]]&lt;/ref&gt; When new articles with a valid alternate spelling are added to Wikipedia that match up to existing articles based on metadata, these are then manually checked and if needed, tagged for merging.&lt;ref&gt;See [[Wikipedia:WikiProject Merge]]&lt;/ref&gt; When new articles are added that are considered out of scope or otherwise unfit for Wikipedia, these are nominated for deletion.&lt;ref&gt;See [[Wikipedia:Articles for deletion]]&lt;/ref&gt; To help keep track of metadata on Wikipedia, the new Wikimedia project [[Wikidata]] was established in 2012. Click on the pictures to view more metadata about these images:
&lt;gallery&gt;
File:Sta-eulalia.jpg|This picture of the [[Barcelona Cathedral]] was uploaded to the English Wikipedia in 2003 to illustrate its Wikipedia article, and was transferred to [[Wikimedia Commons]] in 2007 so it could be used in other language versions of Wikipedia.
File:Article catedral pantalla estreta.png|This screenprint of the [[Catalan Wikipedia]] page on the cathedral features several photos including this one. The screenprint was uploaded to Wikimedia Commons in 2007 soon after the photo was available there, but [[:ca:Catedral de Barcelona|that article]] on the Catalan Wikipedia has since been expanded.
&lt;/gallery&gt;

== See also ==
* [[Data Defined Storage]] 
* [[Metadata discovery]]
* [[Metadata publishing]]
* [[Metadata registry]]
* [[ISO/IEC 11179]]
* [[Dublin core]]

 
==References==
{{reflist}}
{{DEFAULTSORT:Meta-Data Management}}
[[Category:Metadata]]
[[Category:Data management]]</text>
      <sha1>arwzqjj1zibrqba3j0m5xw786h4jdm2</sha1>
    </revision>
  </page>
  <page>
    <title>Data monetization</title>
    <ns>0</ns>
    <id>32714470</id>
    <revision>
      <id>761647211</id>
      <parentid>759776788</parentid>
      <timestamp>2017-01-24T01:18:27Z</timestamp>
      <contributor>
        <username>EdwardUK</username>
        <id>28516722</id>
      </contributor>
      <comment>Undid revision 756473370 by [[Special:Contributions/Hitesh kapasia|Hitesh kapasia]] ([[User talk:Hitesh kapasia|talk]]) test edit?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29588" xml:space="preserve">'''Data monetization''', a form of [[monetization]], is generating [[revenue]] from available data sources or real time streamed data by instituting the discovery, capture, storage, analysis, dissemination, and use of that data.  Said differently, it is the process by which data producers, data aggregators and data consumers, large and small, exchange sell or trade data. Data monetization leverages data generated through business operations as well as data associated with individual actors and with electronic devices and sensors participating in the [[internet of things]].  The ubiquity of the [[internet of things]] is generating [[location data]] and other data from sensors and [[mobile devices]] at an ever increasing rate. When this data is collated against traditional databases, the value and utility of both sources of data increases, leading to tremendous potential to mine data for social good, research and discovery, and achievement of business objectives.  Closely associated with data monetization are the emerging [[data as a service]] models for transactions involving data by the data item.

There are three [[ethical]] and regulatory vectors involved in data monetization due to the sometimes conflicting interests of actors involved in the [[data supply chain]].  The individual data creator who generates files and records through his own efforts or owns a device such as a sensor or a mobile phone that generates data has a claim to ownership of data.  The business entity that generates data in the course of its operations, such as its transactions with financial institutions or [[risk factors]] discovered through feedback from customers also has a claim on data captured through their systems and platforms. However, the person that contributed the data may also have a legitimate claim on the data.  Internet platforms and service providers, such as [[Google]] or [[Facebook]] that require a user to forgo some ownership interest in their data in exchange for use of the platform also have a legitimate claim on the data.  Thus the practice of data monetization, although common since 2000, is now getting increasing attention from regulators.  The [[European Union]] and the [[United States Congress]] have begun to address these issues.  For instance, in the financial services industry, regulations involving data are included in the [[Gramm&#8211;Leach&#8211;Bliley Act]] and [[Dodd-Frank]].  Some individual creators of data are shifting to using [[personal data vaults]]&lt;ref&gt;http://www.freepatentsonline.com/y2014/0032267.html&lt;/ref&gt; and implementing [[vendor relationship management]]&lt;ref&gt;[[Vendor Relationship Management]]&lt;/ref&gt; concepts as a reflection of an increasing resistance to their data being federated or aggregated and resold without compensation.  Groups such as the [[Personal Data Ecosystem Consortium]],&lt;ref&gt;http://personaldataecosystem.org&lt;/ref&gt; [[Patient Privacy Rights]],&lt;ref&gt;http://patientprivacyrights.org/&lt;/ref&gt; and others are also challenging corporate cooptation of data without compensation.

[[Financial services]] companies are a relatively good example of an industry focused on generating revenue by leveraging data.  [[Credit card]] issuers and [[retail banks]] use customer transaction data to improve targeting of [[cross-sell]] offers.  Partners are increasingly promoting merchant based [[reward programs]] which leverage a bank&#8217;s data and provide discounts to customers at the same time.

==Steps==
# Identification of available data sources &#8211; this includes data currently available for monetization as well as other external data sources that may enhance the value of what&#8217;s currently available.
# Connect, aggregate, attribute, validate, authenticate, and exchange data - this allows data to be converted directly into actionable or revenue generating insight or services.
# Set terms and prices and facilitate data trading - methods for data vetting, storage, and access. For example, many global corporations have locked and siloed data storage infrastructures, which stymies efficient access to data and cooperative and real time exchange. 
# Perform [[Research]] and [[analytics]] &#8211; draw predictive insights from existing data as a basis for using data for to reduce [[risk]], enhance product development or performance, or improve [[customer experience]] or business outcomes.
# Action and leveraging &#8211; the last phase of monetizing data includes determining alternative or improved datacentric products, ideas, or services.  Examples may include real time actionable triggered notifications or enhanced channels such as web or mobile response mechanisms.

==Pricing Variables and Factors==
*  A fee for use of a platform to connect buyers and sellers
*  A fee for use of a platform to configure, organize, and otherwise process data included in a data trade 
*  A fee for connecting or including a device or sensor into a data supply chain
*  A fee for connecting and credentialing a creator of a data source and a data buyer - often through a [[federated identity]]
*  A fee for connecting a data source to other data sources to be included into a data supply chain
*  A fee for use of an internet service or other transmission service for uploading and downloading data - sometimes, for an individual, through a [[personal cloud]]
*  A price or exchange or other trade value assigned by a data creator or generator to a data item or a data source 
*  A price or exchange or other trade value offered by a data buyer to a data creator 
*  A price or exchange or other trade value assigned by a data buyer for a data item or a data source formatted according to criteria set by a data buyer
*  An incremental fee assigned by a data buyer for a data item or a data set scaled to the reputation of the data creator
*  A fee for use of encrypted keys to achieve secure data transfer
*  A fee for use of a search algorithm specifically designed to tag data sources that contain data points of value to the data buyer
*  A fee for linking a data creator or generator to a data collection protocol or form
*  A fee for server actions - such as a notification - triggered by an update to a data item or data source included into a data supply chain

==Benefits==
*  Improved decision-making that leads to [[real time (media)|real time]] [[crowd sourced]] research, improved profits, decreased costs, reduced risk and improved compliance
*  More impactful decisions (e.g., make real time decisions)
*  More timely (lower latency) decisions (e.g., a vendor making purchase recommendations while the customer is still on the phone or in the store, a customer connecting with multiple vendors to discover a best price, triggered notifications when thresholds are reached for data values )
*  More granular decisions (e.g., localized pricing decisions at an individual or device or sensor level versus larger aggregates).

==Frameworks==
There are a wide variety of industries, firms and business models related to data monetization.  The following frameworks have been offered to help understand the types of business models that are used:

Doug Laney of [[Gartner]], a leading IT research and advisory firm, has posited a model for a range of data monetization methods:

* Indirect Data Monetization
**Using data to improve efficiencies
**Using data to measurably reduce risks
**Using data to develop new products, markets
**Using data to build and solidify partner relationships
**Publishing Branded indices
* Direct Data Monetization
**Bartering or trading with information
**Information-enhanced products or services
**Selling raw data through brokers
**Offering data/report subscriptions

He also suggests a set of feasibility tests and questions for any data monetization ideas being considered:

{| class="wikitable"
|-
! Type of Feasibility !! Feasibility Question 
|-
| Practical || Is the idea utilitarian, or merely interesting/cool? Is it usable?
|-
| Marketable || Would the idea have sufficiently broad appeal, internally or externally?
|-
| Scalable || Can the idea be developed and implemented to the extent required or intended?
|-
| Manageable || Do you have the skills to oversee the development &amp; implementation of the idea?
|-
| Technological || Do you have the tools, information and skills to develop and rollout the idea?
|-
| Economical || Will the idea require too much investment or generate sufficient return on investment?
|-
| Legal || Does the idea conform to local laws where it will be used or implemented?
|-
| Ethical || Will the idea be something that has the potential for customer/user/public backlash?
|-
| Example || Will the idea cause significant positive vs. negative impact on the environment?
|}

Roger Ehrenberg of IA Ventures, a VC firm that invests in this space has defined three basic types of data product firms:
:"'''Contributory databases'''. The magic of these businesses is that a customer provides their own data in exchange for receiving a more robust set of aggregated data back that provides insight into the broader marketplace, or provides a vehicle for expressing a view. Give a little, get a lot back in return &#8211;  a pretty compelling value proposition, and one that frequently results in a payment from the data contributor in exchange for receiving enriched, aggregated data. Once these contributory databases are developed and customers become reliant on their insights, they become extremely valuable and persistent data assets.
:
:'''Data processing platforms'''. These businesses create barriers through a combination of complex data architectures, proprietary algorithms and rich analytics to help customers consume data in whatever form they please. Often these businesses have special relationships with key data providers, that when combined with other data and processed as a whole create valuable differentiation and competitive barriers. Bloomberg is an example of a powerful data processing platform. They pull in data from a wide array of sources (including their own home grown data), integrate it into a unified stream, make it consumable via a dashboard or through an API, and offer a robust analytics suite for a staggering number of use cases. Needless to say, their scale and profitability is the envy of the industry.
:
:'''Data creation platforms'''. These businesses solve vexing problems for large numbers of users, and by their nature capture a broad swath of data from their customers. As these data sets grow, they become increasingly valuable in enabling companies to better tailor their products and features, and to target customers with highly contextual and relevant offers. Customers don&#8217;t sign up to directly benefit from the data asset; the product is so valuable that they simply want the features offered out-of-the-box. As the product gets better over time, it just cements the lock-in of what is already a successful platform. Mint was an example of this kind of business. People saw value in the core product. But the product continued to get better as more customer data was collected and analyzed. There weren&#8217;t network effects, per se, but the sheer scale of the data asset that was created was an essential element of improving the product over time."&lt;ref&gt;{{cite web|last=Ehrenberg |first=Roger |title=Creating competitive advantage through data |url=http://www.iaventures.com/creating-competitive-advantage-through-data |publisher=IA Ventures' blog |accessdate=23 November 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131203023719/http://www.iaventures.com/creating-competitive-advantage-through-data |archivedate=3 December 2013 |df= }}&lt;/ref&gt;

Selvanathan and Zuk &lt;ref&gt;Big Data Realized: Developing New Data-Driven Products and Services to Drive Growth Perspective&lt;/ref&gt; offer a framework that includes "monetization methods that are outside the bounds of the
traditional value capture systems employed by an enterprise... tuned to match the context and consumption models for the target customer."  They offer examples of "four distinct approaches: platforms, applications, data-as-a-service, and professional services."

Ethan McCallum and Ken Gleason published an O'Rielly eBook titled ''Business Models for the Data Economy''
:Collect/Supply
:Store/Host
:Filter/Refine
:Enhance/Enrich
:Simplify Access
:Analyze
:Obscure
:Consult/Advise&lt;ref&gt;{{cite book|last=Gleason|first=Ken|title=Business Models for the Data Economy|year=2013|publisher=O'Reilly|isbn=978-1-449-37223-1|url=http://www.oreilly.com/data/free/business-models-for-the-data-economy.csp}}&lt;/ref&gt;

==Examples==
*  Packaging of data (with analytics) to be resold to customers for things such as wallet share, [[market share]] and [[benchmarking]]
*  Integration of data (with analytics) into new products as a value-added differentiator such as [[On-Star]] for [[General Motors]] cars
* [[GPS]] enabled [[smartphones]]
* [[Geolocation]]-based offers and location discounts, such as those offered by [[Facebook]]&lt;ref&gt;https://www.theguardian.com/technology/2011/jan/31/facebook-places-deals-uk-europe&lt;/ref&gt; and [[Groupon]]&lt;ref&gt;http://mashable.com/2011/05/10/groupon-now-launches/&lt;/ref&gt; are other prime examples of data monetization leveraging new emerging channels
* CRM based ad targeting and media attribution, such as those offered by Circulate

==Intellectual property landscape==
Some of the patents issued since 2010 by the [[USPTO]] for monetizing data generated by individuals include; 8,271,346, 8,612,307, 8,560,464, 8,510,176, and 7,860,760.  These are usually in the class 705 related to electronic commerce, data processing, and cost and price determination. Some of these patents use the term, the [[data supply chain]] to reflect emerging technology to federate and aggregate data in real time from many individuals and devices linked together through the [[internet of things]].  Another emerging term is [[information banking]].

An unexplored but potentially disruptive arena for data monetization is the use of [[Bitcoin micropayments]] for data transactions.  Because Bitcoins are emerging as competitors with payment services like Visa or PayPal that can readily enable and reduce or eliminate transaction costs, transactions for as little as a single data item can be facilitated. Consumers as well as enterprises who desire to monetize their participation in a data supply chain may soon be able to access social network enabled Bitcoin exchanges and platforms.&lt;ref&gt;Lomas, Natasha, Techcrunch, August 18, 2014&lt;/ref&gt;  [[Clickbait]] and data hijacking may wither as micropayments for data are ubiquitous and enabled. Potentially, even the current need to build out data broker managed data trading exchanges may be bypassed.  Stanley Smith,&lt;ref&gt;http://www.linkedin.com/pub/stan-smith/9/3ab/b37/&lt;/ref&gt; who introduced the notion of the data supply chain, has said that simple micropayments for data monetization are the key to evolution of ubiquitous implementation of user configurable data supply schemata, enabling data monetization on a universal scale for all data creators, including the burgeoning internet of things.

==Presentations and Publications ==

2016
* [https://www.gartner.com/doc/3267517 How CIOs and CDOs Can Use Infonomics to Identify, Justify and Fund Initiatives], Douglas Laney and Michael Smith, [[Gartner]] 29 March 2016
* [http://www.wsj.com/articles/accountings-21st-century-challenge-how-to-value-intangible-assets-1458605126 Accounting's 21st Century Challenge: How to Value Intangible Assets], [[WSJ]] CFO Journal, 22 March 2016 
* [https://s3.amazonaws.com/files.technologyreview.com/whitepapers/MIT_Oracle+Report-The_Rise_of_Data_Capital.pdf The Rise of Data Capital], [[Oracle Corporation|Oracle]] and [[MIT]] Technology Review Custom, 2016
* [http://www.gartner.com/smarterwithgartner/treating-information-as-an-asset/ Treating Information as an Asset], Christy Pettey, Douglas Laney and Michael M. Moran, Smarter With [[Gartner]], 17 February 2016
* [http://www.reuters.com/article/us-europe-data-competition-idUSKCN0VF0KV German competition watchdog wants 'big data' hoards considered in merger probes], [[Reuters]], 6 Feb 2016
* [https://www.gartner.com/doc/3188917 Shift From a Project to an Asset Perspective to Properly Value and Fund IT Investments], Michael Smith and Douglas Laney, [[Gartner]], 16 January 2016
* [http://www.iri.com/blog/iri/business/infonomics-and-you/ Infonomics and You], Eric Leohner, [[IRI CoSort]], January 2016
* [http://www.nowozin.net/sebastian/blog/the-fair-price-to-pay-a-spy-an-introduction-to-the-value-of-information.html The Fair Price to Pay a Spy: An Introduction to the Value of Information], Sebastian Nowozin, Nowozin.net blog, 9 January 2016 
2015
* [https://www.gartner.com/doc/3173343 Measure Your Information Yield to Maximize Return on Information and Analytics Investments], Frank Buytendijk, Andrew White, Douglas Laney and Thomas W. Oestreich, [[Gartner]], 1 December 2015
* [https://www.gartner.com/doc/3162520 IBM Storms Information, IoT Markets by Buying The Weather Company], Douglas Laney, [[Gartner]], 4 November 2015
* [https://www.gartner.com/doc/3158117 How to Adopt Open Data for Business Data and Analytics &#8212; And Why You Should], Alan D. Duncan &amp; Douglas Laney, [[Gartner]], 28 October 2015
* [https://www.gartner.com/doc/3151321 Seven Steps to Monetizing Your Information Assets], Douglas Laney, [[Gartner]], 15 October 2015
* [http://www.gartner.com/smarterwithgartner/why-and-how-to-value-your-information-as-an-asset/ Why and How to Value Your Information as an Asset] Heather Levy &amp; Douglas Laney, Smarter With [[Gartner]] blog, 3 September 2015
* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don't HDOs Appreciate Healthcare Infonomics?], Laura Craft &amp; Douglas Laney, [[Gartner]], 5 August 2015
* In August 20, 2015 [[Gartner]] Analyst Doug Laney gave a publicly-available webinar (with replay available) on [http://www.gartner.com/webinar/3098518 Methods for Monetizing Your Data]. This is a reprise of the presentation he has given at various [[Gartner]] summits and symposia around the world. 
* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don't HDOs Appreciate Healthcare Infonomics?], Laura Craft &amp; Douglas Laney, [[Gartner]], 5 August 2015
* [https://www.gartner.com/doc/3106719 Why and How to Measure the Value of Your Information Assets], Douglas Laney, [[Gartner]], 5 August 2015
* [http://prezi.com/xirqf54fix68/?utm_campaign=share&amp;utm_medium=copy&amp;rc=ex0share Applied Infonomics: Measuring the Economic Value of Information Assets], [http://www.mitcdoiq.org/ MIT Chief Data Officer Symposium], Doug Laney, [[Gartner]], 22 July 2015
* [http://www.kpmg.com/US/en/topics/data-analytics/Documents/kpmg-d-a-main-report-for-web-28-june-2015.pdf Data and Analytics: A New Driver of Performance and Valuation], [[Institutional Investor Research]] and [[KPMG]], 28 June 2015
* [http://www.thesummits.org/watch.htm?id=132356411 The Convergence of Information Economics and Economic Information] [[Corp Development Summit]] presentation replay, Doug Laney, [[Gartner]], 1 July 2015
* [http://smartdatacollective.com/rk-paleru/319941/data-opportunity-are-you-monetizing-information Data = Opportunity: But Are You Monetizing Information?] [[Smart Data Collective]], RK Paleru, 28 May 2015
* [http://blogs.gartner.com/doug-laney/keeping-busy-with-data-strategy/ Keeping Busy with Data Strategy], [[Gartner]] Blog Network, Doug Laney, 26 May 2015
* [http://blogs.wsj.com/cio/2015/05/20/dollar-value-of-data-radioshack-other-bankrupt-firms-auction-customer-data-to-pay-debt/ Dollar Value of Data: RadioShack, Other Bankrupt Firms Auction Customer Data to Pay Debt], [[Wall Street Journal]], Kim Nash, 20 May 2015
* [https://www.gartner.com/doc/3024417 The Benefits and Risks of Using Open Data], Doug Laney, [[Gartner]], 8 April 2015
* [http://goodstrat.com/2015/01/30/consider-this-does-all-data-have-value/ Consider this: Does all data have value?] Good Strategy blog, Martyn Jones, 30 January 2015
* [http://www.rsd.com/en/resources/white-papers/theory-infonomics-valuating-corporate-information-assets The Theory of Infonomics: Valuating Corporate Information Assets - white paper], [[RSD (company)|RSD]], January 2015
* [http://www.firstpost.com/business/customer-data-valuable-asset-treat-way-2046119.html Customer data is a valuable asset. Why not treat it that way?], [[F.Business]], Ajay Kelkar, 14 January 2015
* [https://www.youtube.com/watch?v=du4YVpu4VHE The Rise of Data Capital] (video), [[Oracle Corporation|Oracle]], 8 January 2015

2014
* [http://www.cmswire.com/cms/information-management/quantifying-the-value-of-your-data-026674.php Quantifying the Value of Your Data], [[CMS Wire]], Bassam Zarkout, 30 September 2014
* [http://www.rsd.com/en/blog/201409/what-infonomics What is Infonomics?], Ed Hallock, [[RSD (company)|RSD]] blog, 9 September 2014
* [http://cisr.mit.edu/blog/documents/2014/08/21/2014_0801_datamonetization_wixom.pdf/ Cashing In on Your Data], [[MIT Sloan]] Center for Information Systems Research, Barbara H. Wixom, Volume XIV, Number 8, August 2014
* [https://www.gartner.com/doc/2813227 Increase the Return on Your Information Investments With the Information Yield Curve], [[Gartner]], Andrew White and Douglas Laney, 31 July 2014
* [http://www.forbes.com/sites/gartnergroup/2014/07/21/the-hidden-shareholder-boost-from-information-assets/ The Hidden Shareholder Boost from Information Assets], [[Forbes]], Doug Laney, 21 July 2014
* [http://searchcio.bitpipe.com/data/demandEngage.action?resId=1401817861_376 CIO Decisions: The new infonomics reality: Determining the value of data], [[TechTarget]] SearchCIO, June 2014
* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[TechTarget]] SearchCIO, Linda Tucci, 13 May 2014
* [http://searchcio.techtarget.com/feature/Six-ways-to-measure-the-value-of-your-information-assets Six ways to measure the value of your information assets], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014
* [http://searchcio.techtarget.com/feature/Infonomics-treats-data-as-a-business-asset Infonomics treats data as a business asset], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014
* [http://pv.tl/blog/2014/04/13/the-economics-of-information-management/?utm_content=buffer29c67&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer, The economics of information management], PVTL Blog, Felix Barbalet, 13 April 2014
* [http://www.forbes.com/sites/gartnergroup/2014/03/27/the-hidden-tax-advantage-of-monetizing-your-data/ The Hidden Tax Advantage of Monetizing Your Data], [[Forbes]], Doug Laney, 27 March 2014
* [http://blogs.teradata.com/anz/the-chief-data-officer-managing-the-value-of-data/#comment-2147 The Chief Data Officer &#8211; Managing the Value of Data], [[Teradata]] ANZ Blog, Renato Manongdo, March 2014
* [https://www.gartner.com/doc/2677518 How Organizations Can Monetize Customer Data], [[Gartner]], Olive Huang, Doug Laney, 6 March 2014
* [https://www.gartner.com/doc/2677515?ref=QuickSearch&amp;sthkw=infonomics Improving the Value of Customer Data Through Applied Infonomics], [[Gartner]] Research Publication, Douglas Laney, Olive Huang, 6 March 2014
* [http://blogs.gartner.com/andrew_white/2014/02/14/information-value-accrual-and-its-asymmetry/ Information Value Accrual and Its Asymmetry], [[Gartner]] Blog Network, Andrew White, 14 February 2014
* [http://blogs.gartner.com/andrew_white/2014/01/29/does-information-utility-suffer-a-half-life/ Does Information Utility Suffer a Half Life?], [[Gartner]] Blog Network, Andrew White, 29 January 2014

2013
* [http://www.rsd.com/en/blog/201312/what-information-information-governance What is the "Information" in "Information Governance"?], [[RSD (company)|RSD]] Blog, James Amsler, 30 December 2013
* [http://blogs.gartner.com/doug-laney/to-twitter-youre-worth-101-70/ To Twitter You're Worth $101.70] [[Gartner]] Blog Network, by Douglas Laney, 12 November 2013
* [http://www.economistgroup.com/leanback/big-data-2/treat-data-like-money Treat data like money. CMO's Advice: Marketers must develop an investment strategy for data], [[The Economist Group]], Jim Davis, SVP &amp; CMO, SAS, October 2013
* [http://www.ft.com/intl/cms/s/0/205ddf5c-1bf0-11e3-b678-00144feab7de.html#axzz2g8PCOrV3 Infonomics: The New Economics of Information], [[The Financial Times]], Doug Laney, VP Research, Gartner, September 2013
* [https://www.youtube.com/watch?v=mQk_5Q3VJv4 Value of Information], GigaOM presentation by Dave McCrory, SVP at [[Warner Music Group]], July 2013   
* [http://www.bankingtech.com/147432/accounting-for-the-value-of-big-data/ Accounting for the value of (big) data], [[Banking Technology Magazine]], David Bannister, 11 June 2013
* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[SearchCIO]] Journal, Linda Tucci, May 2013
* On March 19, 2013 the Chicago Chapter of the [[Product Development and Management Association]] (PDMA) held an event titled "Monetizing Data: An Evening with Eight of Chicago's Data Product Management Leaders"&lt;ref&gt;http://www.builtinchicago.org/blog/check-out-ppt-deck-monetizing-data-evening-eight-chicagos-data-product-management-leaders&lt;/ref&gt;

2012
* [http://www.informationweek.com/big-data/news/big-data-analytics/whats-your-big-data-worth/240144449 What's Your Big Data Worth], [[InformationWeek]], Ellis Booker, 17 December 2012
* [https://www.gartner.com/doc/2278915 Future of Money: Infonomics Monetizing Value in Big Data Information Assets], Mary Knox, [[Gartner]], 14 December 2012
* [http://www.information-age.com/technology/information-management/2134803/an-introduction-to-infonomics An Introduction to Infonomics], [[InformationAge]], Pete Swabey, 26 November 2012
* [https://www.gartner.com/doc/2186116 The Birth of Infonomics: the New Economics of Information], [[Gartner]] research publication, Douglas Laney, 3 October 2012 (public summary, full text available to Gartner clients)
* [http://blogs.gartner.com/doug-laney/tobins-q-a-evidence-of-informations-real-market-value-2/ Tobin&#8217;s Q &amp; A: Evidence of Information&#8217;s Real Market Value], [[Gartner]] Blog Network, Douglas Laney, 14 Aug 2012
* [http://www.ft.com/intl/cms/s/0/27476ad4-a6a5-11e1-968b-00144feabdc0.html#axzz1vzOCxVYw Extracting Value from Information], [[Financial Times]], interview with Douglas Laney by Paul Taylor, 25 May 2012 (free registration required)
* [http://www.forbes.com/sites/gartnergroup/2012/05/22/infonomics-the-practice-of-information-economics/ Infonomics: The Practice of Information Economics], [[Forbes]], by Douglas Laney, 22 May 2012
* [http://blogs.wsj.com/cio/2012/05/03/to-facebook-youre-worth-80-95/?mod=wsjcio_hps_cioreport# To Facebook You're Worth $80.95], [[Wall Street Journal]], by Douglas Laney, 3 May 2012
* [https://www.gartner.com/doc/1958016 Introducing Infonomics: Valuing Information as a Corporate Asset], [[Gartner]] research publication, Douglas Laney, 21 March 2012 (public summary, full text available to Gartner clients)
* [http://www.ijikm.org/Volume7/IJIKMv7p177-199Evans0650.pdf Barriers to the Effective Deployment of Information Assets: An Executive Management Perspective], [[Interdisciplinary Journal of Information, Knowledge, and Management]], Nina Evans and James Price, Volume 7, 2012

Older
* [http://imcue.com/wp-content/uploads/2011/06/What-is-EIM.pdf/ What is Enterprise Information Management (EIM)] by John Ladley, Morgan Kaufmann, 2010
* [http://blogs.informatica.com/perspectives/2010/01/26/data-as-an-asset/#comment-1072 Data as an Asset] blog series by John Schmidt, 2010
* [http://www.amazon.com%2FInformation-Driven-Business-Information-Maximum-Advantage%2Fdp%2F0470625775%2Fref%3Dsr_1_1%3Fie%3DUTF8%26s%3Dbooks%26qid%3D1267263302%26sr%3D8-1/ Information Driven Business: How to Manage Data and Information for Maximum Advantage] by Rob Hillard, Wiley 2010
* [http://www.amazon.com%2FHow-Measure-Anything-Intangibles-Business%2Fdp%2F0470539399%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316207185%26sr%3D1-1/ How to Measure Anything: Finding the Value of Intangibles in Business] by Douglas W. Hubbard, Wiley 2010
* [http://www.amazon.com%2FIntangible-Assets-Valuation-Economic-Benefit%2Fdp%2F0471671312%2Fref%3Dsr_1_3%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206576%26sr%3D1-3/ Intangible Assets: Valuation and Economic Benefit] by Jeffrey A. Cohen, Wiley 2005
* [http://www.amazon.com%2FValue-Driven-Intellectual-Capital-Intangible%2Fdp%2F0471351040%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206774%26sr%3D1-1/ Value Driven Intellectual Capital: How to Convert Intangible Corporate Assets Into Market Value] by Patrick H. Sullivan, Wiley, 2000
* [http://www.vldb.org/conf/1998/p641.pdf Bank of America Case Study: The Information Currency Advantage], [[Teradata]], Felipe Carino and Mark Jahnke, Proceedings of the 24th VLDB Conference, New York, NY, 1998
* [http://www.amazon.com/Information-Payoff-Transformation-Work-Electronic/dp/0029317207/ Information Payoff: The Transformation of Work in the Electronic Age] by Paul A. Strassmann, The Free Press, 1985

== See also ==
*[[Infonomics]]
*[[Monetization]]
*[[Business intelligence]]
*[[Analytics]]
*[[Bitcoin]]
*[[Data as a service]]

== References ==
{{Reflist|33em}}

[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]</text>
      <sha1>fw61ylki6p9q12t2rqxpb4fqbwl051t</sha1>
    </revision>
  </page>
  <page>
    <title>ADO.NET</title>
    <ns>0</ns>
    <id>1434840</id>
    <revision>
      <id>737015454</id>
      <parentid>735899502</parentid>
      <timestamp>2016-08-31T07:24:37Z</timestamp>
      <contributor>
        <username>Louisvdw</username>
        <id>1011282</id>
      </contributor>
      <minor />
      <comment>Add missing "it" to sentence</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4527" xml:space="preserve">{{No footnotes|date=March 2009}}
{{Infobox software
|
| operating system       = [[Microsoft Windows]]
| genre                  = [[Software framework]]
| license                = [[Proprietary software]] ([[Base Class Library|BCL]] portion under [[MIT license]]; source code under [[Ms-RSL]])
| website                = {{url|http://msdn2.microsoft.com/en-us/library/aa286484.aspx}}
| Nonprofit              =
}}
'''ADO.NET''' is a data access technology from the [[Microsoft]] [[.NET Framework]] which provides communication between relational and non-relational systems through a common set of components.
ADO.NET is a set of computer software components that programmers can use to access data and data services from the database. It is a part of the [[Base Class Library|base class library]] that is included with the Microsoft .NET Framework. It is commonly used by programmers to access and modify data stored in [[Relational DBMS|relational database systems]], though it can also access data in non-relational sources. ADO.NET is sometimes considered an evolution of [[ActiveX Data Objects]] (ADO) technology, but was changed so extensively that it can be considered an entirely new product.

== Architecture ==
{{Main|ADO.NET data provider}}
[[Image:DotNet3.0.svg|thumb|right|240px|This [[technology]] forms a part of [[.NET Framework 3.0]] (having been part of the framework since version 1.0)]]

ADO.NET is conceptually divided into ''[[ADO.NET consumer|consumers]]'' and ''[[ADO.NET provider|data providers]]''. The consumers are the applications that need access to the data, and the providers are the software components that implement the interface and thereby provide the data to the consumer.

Functionality exists in [[Microsoft Visual Studio|Visual Studio]] IDE to create specialized subclasses of the DataSet classes for a particular database schema, allowing convenient access to each field through strongly typed [[Property (programming)|properties]]. This helps catch more programming errors at compile-time and enhances the IDE's [[Intellisense]] feature.

== O/R Mapping ==
{{main|Object-relational mapping}}

=== Entity Framework ===
{{main|Entity Framework}}

Entity Framework (EF) is an open source object-relational mapping (ORM) framework for ADO.NET, part of .NET Framework. It is a set of technologies in ADO.NET that support the development of data-oriented software applications. Architects and developers of data-oriented applications have typically struggled with the need to achieve two very different objectives. The Entity Framework enables developers to work with data in the form of domain-specific objects and properties, such as customers and customer addresses, without having to concern themselves with the underlying database tables and columns where this data is stored. With the Entity Framework, developers can work at a higher level of abstraction when they deal with data, and can create and maintain data-oriented applications with less code than in traditional applications.

=== LINQ to SQL ===
{{main|LINQ to SQL}}

LINQ to SQL (formerly called DLINQ) allows [[LINQ]] to be used to query Microsoft SQL Server databases, including SQL Server Compact databases. Since SQL Server data may reside on a remote server, and because SQL Server has its own query engine, it does not use the query engine of LINQ. Instead, it converts a LINQ query to a SQL query that is then sent to SQL Server for processing. However, since SQL Server stores the data as relational data and LINQ works with data encapsulated in objects, the two representations must be mapped to one another. For this reason, LINQ to SQL also defines a mapping framework. The mapping is done by defining classes that correspond to the tables in the database, and containing all or a certain subset of the columns in the table as data members.

==See also==
* [[Comparison of ADO and ADO.NET]]

==External links==
;ADO.NET
* [http://msdn2.microsoft.com/en-us/library/aa286484.aspx ADO.NET Overview on MSDN]
* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO Programmer]
* [http://www.devlist.com/ConnectionStringsPage.aspx ADO.NET Connection Strings]

{{.NET Framework}}
{{Microsoft APIs}}{{Windows-software-stub}}{{Microsoft-stub}}{{Programming-software-stub}}

{{DEFAULTSORT:Ado.Net}}
[[Category:Data management]]
[[Category:.NET Framework terminology]]
[[Category:Microsoft application programming interfaces]]
[[Category:SQL data access]]
[[Category:ADO.NET Data Access technologies]]</text>
      <sha1>0oqg6svxujfk78ztvwql823cusaybtg</sha1>
    </revision>
  </page>
  <page>
    <title>Database server</title>
    <ns>0</ns>
    <id>815760</id>
    <revision>
      <id>760952346</id>
      <parentid>760949520</parentid>
      <timestamp>2017-01-20T00:42:16Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Cn}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4086" xml:space="preserve">{{refimprove|date=September 2014}}
A '''database server''' is a [[computer program]] that provides [[database]] services to other computer programs or to [[computer]]s, as defined by the [[client&#8211;server]] [[software modeling|model]].{{cn|date=January 2017}} The term may also refer to a computer dedicated to running such a program. [[Database management system]]s frequently provide database-server functionality, and some [[database management system]]s (DBMSs) (such as [[MySQL]]) rely exclusively on the client&#8211;server model for database access.

Users access a database server either through a "[[Front and back ends|front end]]" running on the user's computer - which displays requested data - or through the "[[Front and back ends|back end]]", which runs on the server and handles tasks such as data analysis and storage.

In a [[Master-slave (technology)|master-slave]] model, database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as [[proxy server|proxies]].

Most database servers respond to a [[query language]]. Each database understands its query language and converts each submitted [[query (disambiguation) | query]] to server-readable form and executes it to retrieve results.

Examples of proprietary database servers include [[Oracle Database|Oracle]], [[IBM DB2|DB2]], [[Informix]], and [[Microsoft SQL Server]]. Examples of [[GNU General Public Licence]] database servers include [[Ingres (database)|Ingres]] and [[MySQL]].  Every server uses its own query logic and structure. The [[SQL]] (Structured Query Language) query language is more or less the same on all [[relational database]] servers.

[[DB-Engines]] lists over 200 DBMSs in its ranking.&lt;ref&gt;
{{cite web
|url= http://db-engines.com/en/ranking 
|title= DB-Engines Ranking 
|publisher= DB-Engines.com 
|date= 2013-12-01 
|accessdate= 2013-12-28
}}
&lt;/ref&gt;

==History ==
The foundations for modeling large sets of data were first introduced by [[Charles Bachman]] in 1969.&lt;ref name="dbhist"&gt;[http://knol.google.com/k/databases-history-early-development# Databases - History &amp; Early Development]&lt;/ref&gt; Bachman introduced [[Data structure diagram|Data Structure Diagrams (DSDs)]] as a means to graphically represent data. DSDs provided a means to represent the relationships between different data entities. In 1970, [[Edgar F. Codd|Codd]] introduced the concept that users of a database should be ignorant of the "inner workings" of the database.&lt;ref name="dbhist"/&gt; Codd proposed the "relational view" of data which later evolved into the [[Relational Model]] which most databases use today. In 1971, the Database Task Report Group of [[CODASYL]] (the driving force behind the development of the programming language [[COBOL]]) first proposed a "data description language for describing a database, a data description language for describing that part of the data base known to a program, and a data manipulation language." &lt;ref name="dbhist"/&gt; Most of the research and development of databases focused on the relational model during the 1970s.

In 1975 Bachman demonstrated how the relational model and the data structure set were similar and "congruent" ways of structuring data while working for the [[Honeywell]].&lt;ref name="dbhist"/&gt; The [[Entity-relationship model]] was first proposed in its current form by [[Peter Chen]] in 1976 while he was conducting research at [[MIT]].&lt;ref&gt;[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.1085 The Entity-Relationship Model: Toward a Unified View of Data (1976)]&lt;/ref&gt; This model became the most frequently used model to describe relational databases. Chen was able to propose a model that was superior to the navigational model and was more applicable to the "real world" than the relational model proposed by Codd.&lt;ref name="dbhist"/&gt;

== References ==
{{Reflist}}

==See also==
* [[Replication (computer science)#Database replication|Database replication]]

{{Database}}

[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Databases]]</text>
      <sha1>cf6mtdep12vqq18ocaal52q9q9t5khq</sha1>
    </revision>
  </page>
  <page>
    <title>Data grid</title>
    <ns>0</ns>
    <id>35951900</id>
    <revision>
      <id>753458812</id>
      <parentid>748820489</parentid>
      <timestamp>2016-12-07T08:01:35Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 6 sources and tagging 3 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="42762" xml:space="preserve">[[File:High Level View Data Grid V1.jpg|200px|right|High Level View Data Grid Topology]]

A '''data grid''' is an [[architecture]] or set of services that gives individuals or groups of users the ability to access, modify and transfer extremely large amounts of geographically distributed [[data]] for research purposes.&lt;ref&gt;Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. Data Grid tools: enabling science on big distributed data&lt;/ref&gt; Data grids make this possible through a host of [[middleware]] [[Application software|applications]] and [[Service (systems architecture)|services]] that pull together data and [[Resource (computer science)|resources]] from multiple [[administrative domain]]s and then present it to users upon request. The data in a data grid can be located at a single site or multiple sites where each site can be its own administrative domain governed by a set of security restrictions as to who may access the data.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.37&lt;/ref&gt; Likewise, multiple [[replica]]s of the data may be distributed throughout the grid outside their original administrative domain and the security restrictions placed on the original data for who may access it must be equally applied to the replicas.&lt;ref&gt;Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids. p.15&lt;/ref&gt; Specifically developed data grid middleware is what handles the integration between users and the data they request by controlling access while making it available as efficiently as possible. The diagram to the right depicts a high level view of a data grid.

==Middleware==
Middleware provides all the services and applications necessary for efficient management of [[dataset]]s and [[Computer file|files]] within the data grid while providing users quick access to the datasets and files.&lt;ref&gt;Padala, Pradeep. A survey of data middleware for Grid systems p.1&lt;/ref&gt; There are a number of concepts and tools that must be available to make a data grid operationally viable. However, at the same time not all data grids require the same capabilities and services because of differences in access requirements, security and location of resources in comparison to users. In any case, most data grids will have similar middleware services that provide for a universal [[namespace|name space]], data transport service, data access service, data replication and resource management service. When taken together, they are key to the data grids functional capabilities.

===Universal namespace===
Since sources of data within the data grid will consist of data from multiple separate systems and [[Computer network|networks]] using different file [[naming convention]]s, it would be difficult for a user to locate data within the data grid and know they retrieved what they needed based solely on existing physical file names (PFNs). A universal or unified name space makes it possible to create logical file names (LFNs) that can be referenced within the data grid that map to PFNs.&lt;ref&gt;Padala, Pradeep. A survey of data middleware for Grid systems&lt;/ref&gt; When an LFN is requested or queried, all matching PFNs are returned to include possible replicas of the requested data. The end user can then choose from the returned results the most appropriate replica to use. This service is usually provided as part of a management system known as a [[Storage Resource Broker]] (SRB).&lt;ref&gt;Arcot, Rajasekar; Wan, Michael; Moore, Reagan; Schroeder, Wayne; Kremenek. Storage resource broker &#8211; managing distributed data in a grid&lt;/ref&gt; Information about the locations of files and mappings between the LFNs and PFNs may be stored in a [[metadata]] or replica catalogue.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.11&lt;/ref&gt; The replica catalogue would contain information about LFNs that map to multiple replica PFNs.

===Data transport service===
Another middleware service is that of providing for data transport or data transfer. Data transport will encompass multiple functions that are not just limited to the transfer of bits, to include such items as fault tolerance and data access.&lt;ref&gt;Coetzee, Serena. Reference model for a data grid approach to address data in a dynamic SDI p.16&lt;/ref&gt; Fault tolerance can be achieved in a data grid by providing mechanisms that ensures data transfer will resume after each interruption until all requested data is received.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.21&lt;/ref&gt; There are multiple possible methods that might be used to include starting the entire transmission over from the beginning of the data to resuming from where the transfer was interrupted. As an example, [[GridFTP]] provides for fault tolerance by sending data from the last acknowledged byte without starting the entire transfer from the beginning.

The data transport service also provides for the low-level access and connections between [[Host (network)|hosts]] for file transfer.&lt;ref&gt;Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.&lt;/ref&gt; The data transport service may use any number of modes to implement the transfer to include parallel data transfer where two or more data streams are used over the same [[Channel (communications)|channel]] or striped data transfer where two or more steams access different blocks of the file for simultaneous transfer to also using the underlying built-in capabilities of the network hardware or specifically developed [[Protocol (object-oriented programming)|protocols]] to support faster transfer speeds.&lt;ref&gt;Izmailov, Rauf; Ganguly, Samrat; Tu, Nan. Fast parallel file replication in data grid p.2&lt;/ref&gt; The data transport service might optionally include a [[network overlay]] function to facilitate the routing and transfer of data as well as file [[I/O]] functions that allow users to see remote files as if they were local to their system. The data transport service hides the complexity of access and transfer between the different systems to the user so it appears as one unified data source.

===Data access service===
Data access services work hand in hand with the data transfer service to provide security, access controls and management of any data transfers within the data grid.&lt;ref&gt;Raman, Vijayshankar; Narang, Inderpal; Crone, chris; Hass, Laura; Malaika, Susan. Services for data access and data processing on grids&lt;/ref&gt; Security services provide mechanisms for authentication of users to ensure they are properly identified. Common forms of security for authentication can include the use of passwords or [[Kerberos (protocol)]]. Authorization services are the mechanisms that control what the user is able to access after being identified through authentication. Common forms of authorization mechanisms can be as simple as file permissions. However, need for more stringent controlled access to data is done using [[Access Control List]]s (ACLs), [[Role-Based Access Control]] (RBAC) and Tasked-Based Authorization Controls (TBAC).&lt;ref&gt;Thomas, R. K. and Sandhu R. S. Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management&lt;/ref&gt; These types of controls can be used to provide granular access to files to include limits on access times, duration of access to granular controls that determine which files can be read or written to. The final data access service that might be present to protect the confidentiality of the data transport is encryption.&lt;ref&gt;Sreelatha, Malempati. Grid based approach for data confidentiality. p.1&lt;/ref&gt; The most common form of encryption for this task has been the use of [[Transport Layer Security|SSL]] while in transport. While all of these access services operate within the data grid, access services within the various administrative domains that host the datasets will still stay in place to enforce access rules. The data grid access services must be in step with the administrative domains access services for this to work.

===Data replication service===
To meet the needs for scalability, fast access and user collaboration, most data grids support replication of datasets to points within the distributed storage architecture.&lt;ref&gt;Chervenak, Ann; Schuler, Robert; Kesselman, Carl; Koranda, Scott; Moe, Brian. Wide area data replication for scientific collaborations&lt;/ref&gt; The use of replicas allows multiple users faster access to datasets and the preservation of bandwidth since replicas can often be placed strategically close to or within sites where users need them. However, replication of datasets and creation of replicas is bound by the availability of storage within sites and bandwidth between sites. The replication and creation of replica datasets is controlled by a replica management system. The replica management system determines user needs for replicas based on input requests and creates them based on availability of storage and bandwidth.&lt;ref&gt;Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments&lt;/ref&gt; All replicas are then cataloged or added to a directory based on the data grid as to their location for query by users. In order to perform the tasks undertaken by the replica management system, it needs to be able to manage the underlying storage infrastructure. The data management system will also ensure the timely updates of changes to replicas are propagated to all nodes.

====Replication update strategy====
There are a number of ways the replication management system can handle the updates of replicas. The updates may be designed around a centralized model where a single master replica updates all others, or a decentralized model, where all peers update each other.&lt;ref&gt;Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments&lt;/ref&gt; The topology of node placement may also influence the updates of replicas. If a hierarchy topology is used then updates would flow in a tree like structure through specific paths. In a flat topology it is entirely a matter of the peer relationships between nodes as to how updates take place. In a hybrid topology consisting of both flat and hierarchy topologies updates may take place through specific paths and between peers.

====Replication placement strategy====
There are a number of ways the replication management system can handle the creation and placement of replicas to best serve the user community. If the storage architecture supports replica placement with sufficient site storage, then it becomes a matter of the needs of the users who access the datasets and a strategy for placement of replicas.&lt;ref&gt;Padala, Pradeep. A survey of data middleware for Grid systems&lt;/ref&gt; There have been numerous strategies proposed and tested on how to best manage replica placement of datasets within the data grid to meet user requirements. There is not one universal strategy that fits every requirement the best. It is a matter of the type of data grid and user community requirements for access that will determine the best strategy to use. Replicas can even be created where the files are encrypted for confidentiality that would be useful in a research project dealing with medical files.&lt;ref&gt;Kranthi, G. and Rekha, D. Shashi. Protected data objects replication in data grid p.40&lt;/ref&gt; The following section contains several strategies for replica placement.

=====Dynamic replication=====
Dynamic replication is an approach to placement of replicas based on popularity of the data.&lt;ref&gt;Belalem, Ghalem and Meroufel, Bakhta. Management and placement of replicas in a hierarchical data grid&lt;/ref&gt; The method has been designed around a hierarchical replication model. The data management system keeps track of available storage on all nodes. It also keeps track of requests (hits) for which data clients (users) in a site are requesting. When the number of hits for a specific dataset exceeds the replication threshold it triggers the creation of a replica on the server that directly services the user&#8217;s client. If the direct servicing server known as a father does not have sufficient space, then the father&#8217;s father in the hierarchy is then the target to receive a replica and so on up the chain until it is exhausted. The data management system algorithm also allows for the dynamic deletion of replicas that have a null access value or a value lower than the frequency of the data to be stored to free up space. This improves system performance in terms of response time, number of replicas and helps load balance across the data grid. This method can also use dynamic algorithms that determine whether the cost of creating the replica is truly worth the expected gains given the location.&lt;ref&gt;Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments&lt;/ref&gt;

=====Adaptive replication=====
This method of replication like the one for dynamic replication has been designed around a hierarchical replication model found in most data grids. It works on a similar algorithm to dynamic replication with file access requests being a prime factor in determining which files should be replicated. A key difference, however, is the number and frequency of replica creations is keyed to a dynamic threshold that is computed based on request arrival rates from clients over a period of time.&lt;ref&gt;Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids&lt;/ref&gt; If the number of requests on average exceeds the previous threshold and shows an upward trend, and storage utilization rates indicate capacity to create more replicas, more replicas may be created. As with dynamic replication, the removal of replicas that have a lower threshold that were not created in the current replication interval can be removed to make space for the new replicas.

=====Fair-share replication=====
Like the adaptive and dynamic replication methods before, fair-share replication is based on a hierarchical replication model. Also, like the two before, the popularity of files play a key role in determining which files will be replicated. The difference with this method is the placement of the replicas is based on access load and storage load of candidate servers.&lt;ref&gt;Rasool, Qaisar; Li, Jianzhong; Oreku, George S.; Munir, Ehsan Ullah. Fair-share replication in data grid&lt;/ref&gt; A candidate server may have sufficient storage space but be servicing many clients for access to stored files. Placing a replicate on this candidate could degrade performance for all clients accessing this candidate server. Therefore, placement of replicas with this method is done by evaluating each candidate node for access load to find a suitable node for the placement of the replica. If all candidate nodes are equivalently rated for access load, none or less accessed than the other, then the candidate node with the lowest storage load will be chosen to host the replicas. Similar methods to the other described replication methods are used to remove unused or lower requested replicates if needed. Replicas that are removed might be moved to a parent node for later reuse should they become popular again.

=====Other replication=====
The above three replica strategies are but three of many possible replication strategies that may be used to place replicas within the data grid where they will improve performance and access. Below are some others that have been proposed and tested along with the previously described replication strategies.&lt;ref&gt;Ranganathan, Kavitha and Foster, Ian. Identifying dynamic replication strategies for a high performance data grid&lt;/ref&gt; 
* '''Static''' &#8211; uses a fixed replica set of nodes with no dynamic changes to the files being replicated.
* '''Best Client''' &#8211; Each node records number of requests per file received during a preset time interval; if the request number exceeds the set threshold for a file a replica is created on the best client, one that requested the file the most; stale replicas are removed based on another algorithm. 
* '''Cascading''' &#8211; Is used in a hierarchical node structure where requests per file received during a preset time interval is compared against a threshold. If the threshold is exceeded a replica is created at the first tier down from the root, if the threshold is exceeded again a replica is added to the next tier down and so on like a waterfall effect until a replica is placed at the client itself.
* '''Plain Caching''' &#8211; If the client requests a file it is stored as a copy on the client.
* '''Caching plus Cascading''' &#8211; Combines two strategies of caching and cascading.
* '''Fast Spread''' &#8211; Also used in a hierarchical node structure this strategy automatically populates all nodes in the path of the client that requests a file.

===Tasks scheduling and resource allocation===
Such characteristics of the data grid systems as large scale and heterogeneity require specific methods of tasks scheduling and resource allocation. To resolve the problem, majority of systems use extended classic methods of scheduling.&lt;ref&gt;Epimakhov, Igor; Hameurlain, Abdelkader ; Dillon, Tharam; Morvan, Franck. Resource Scheduling Methods for Query Optimization in Data Grid Systems&lt;/ref&gt; Others invite fundamentally different methods based on incentives for autonomous nodes, like virtual money or reputation of a node.
Another specificity of data grids, dynamics, consists in the continuous process of connecting and disconnecting of nodes and local load imbalance during an execution of tasks. That can make obsolete or non-optimal results of initial resource allocation for a task. As a result, much of the data grids utilize execution-time adaptation techniques that permit the systems to reflect to the dynamic changes: balance the load, replace disconnecting nodes, use the profit of newly connected nodes, recover a task execution after faults.

===Resource management system (RMS)===
The resource management system represents the core functionality of the data grid. It is the heart of the system that manages all actions related to storage resources. In some data grids it may be necessary to create a federated RMS architecture because of different administrative policies and a diversity of possibilities found within the data grid in place of using a single RMS. In such a case the RMSs in the federation will employ an architecture that allows for interoperability based on an agreed upon set of protocols for actions related to storage resources.&lt;ref&gt;Krauter, Klaus; Buyya, Rajkumar; Maheswaran, Muthucumaru. A taxonomy and survey of grid resource management systems for distributed computing&lt;/ref&gt;

====RMS functional capabilities====
* Fulfillment of user and application requests for data resources based on type of request and policies; RMS will be able to support multiple policies and multiple requests concurrently
* Scheduling, timing and creation of replicas
* Policy and security enforcement within the data grid resources to include authentication, authorization and access 
* Support systems with different administrative policies to inter-operate while preserving site autonomy
* Support quality of service (QoS) when requested if feature available
* Enforce system fault tolerance and stability requirements
* Manage resources, i.e. disk storage, network bandwidth and any other resources that interact directly or as part of the data grid 
* Manage trusts concerning resources in administrative domains, some domains may place additional restrictions on how they participate requiring adaptation of the RMS or federation.
* Supports adaptability, extensibility, and scalability in relation to the data grid.

==Topology==
[[File:Data Grid Multiple Topologies 1.jpg|right|Possible Data Grid Topologies]]
Data grids have been designed with multiple topologies in mind to meet the needs of the scientific community. On the right are four diagrams of various topologies that have been used in data grids.&lt;ref&gt;Zhu, Lichun. Metadata management in grid database federation&lt;/ref&gt; Each topology has a specific purpose in mind for where it will be best utilized. Each of these topologies is further explained below.

'''Federation topology''' is the choice for institutions that wish to share data from already existing systems. It allows each institution control over their data. When an institution with proper authorization requests data from another institution it is up to the institution receiving the request to determine if the data will go to the requesting institution. The federation can be loosely integrated between institutions, tightly integrated or a combination of both.

'''Monadic topology''' has a central repository that all collected data is fed into. The central repository then responds to all queries for data. There are no replicas in this topology as compared to others. Data is only accessed from the central repository which could be by way of a web portal. One project that uses this data grid topology is the [[Network for Earthquake Engineering Simulation| Network for Earthquake Engineering Simulation (NEES)]] in the United States.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.16&lt;/ref&gt; This works well when all access to the data is local or within a single region with high speed connectivity.

'''Hierarchical topology''' lends itself to collaboration where there is a single source for the data and it needs to be distributed to multiple locations around the world. One such project that will benefit from this topology would be [[CERN]] that runs the [[Large Hadron Collider]] that generates enormous amounts of data. This data is located at one source and needs to be distributed around the world to organizations that are collaborating in the project.

'''Hybrid Topology''' is simply a configuration that contains an architecture consisting of any combination of the previous mentioned topologies. It is used mostly in situations where researchers working on projects want to share their results to further research by making it readily available for collaboration.

==History==
The need for data grids was first recognized by the [[scientific community]] concerning [[climate modeling]], where [[terabyte]] and [[petabyte]] sized [[data set]]s were becoming the norm for transport between sites.&lt;ref&gt;Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.&lt;/ref&gt; More recent research requirements for data grids have been driven by the [[Large Hadron Collider]] (LHC) at [[CERN]], the [[LIGO|Laser Interferometer Gravitational Wave Observatory (LIGO)]], and the [[Sloan Digital Sky Survey|Sloan Digital Sky Survey (SDSS)]]. These examples of scientific instruments produce large amounts of data that need to be accessible by large groups of geographically dispersed researchers.&lt;ref&gt;Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. p.571&lt;/ref&gt;&lt;ref&gt;Tierney, Brian L. Data grids and data grid performance issues. p.7&lt;/ref&gt; Other uses for data grids involve governments, hospitals, schools and businesses where efforts are taking place to improve services and reduce costs by providing access to dispersed and separate data systems through the use of data grids.&lt;ref&gt;Thibodeau, P. Governments plan data grid projects&lt;/ref&gt;
 
From its earliest beginnings, the concept of a Data Grid to support the scientific community was thought of as a specialized extension of the &#8220;grid&#8221; which itself was first envisioned as a way to link super computers into meta-computers.&lt;ref&gt;Heingartner, douglas. The grid: the next-gen internet&lt;/ref&gt; However, that was short lived and the grid evolved into meaning the ability to connect computers anywhere on the web to get access to any desired files and resources, similar to the way electricity is delivered over a grid by simply plugging in a device. The device gets electricity through its connection and the connection is not limited to a specific outlet. From this the data grid was proposed as an integrating architecture that would be capable of delivering resources for distributed computations. It would also be able to service numerous to thousands of queries at the same time while delivering gigabytes to terabytes of data for each query. The data grid would include its own management infrastructure capable of managing all aspects of the data grids performance and operation across multiple wide area networks while working within the existing framework known as the web.&lt;ref&gt;Heingartner, douglas. The grid: the next-gen internet&lt;/ref&gt; 
 
The data grid has also been defined more recently in terms of usability; what must a data grid be able to do in order for it to be useful to the scientific community. Proponents of this theory arrived at several criteria.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.1&lt;/ref&gt; One, users should be able to search and discover applicable resources within the data grid from amongst its many datasets. Two, users should be able to locate datasets within the data grid that are most suitable for their requirement from amongst numerous replicas. Three, users should be able to transfer and move large datasets between points in a short amount of time. Four, the data grid should provide a means to manage multiple copies of datasets within the data grid. And finally, the data grid should provide security with user access controls within the data grid, i.e. which users are allowed to access which data.

The data grid is an evolving technology that continues to change and grow to meet the needs of an expanding community. One of the earliest programs begun to make data grids a reality was funded by the [[DARPA|Defense Advanced Research Projects Agency (DARPA)]] in 1997 at the [[University of Chicago]].&lt;ref&gt;Globus. About the globus toolkit&lt;/ref&gt; This research spawned by DARPA has continued down the path to creating open source tools that make data grids possible. As new requirements for data grids emerge projects like the [[Globus Toolkit]] will emerge or expand to meet the gap. Data grids along with the "Grid" will continue to evolve.

== Notes ==
{{Reflist}}

== References ==
*{{cite journal
|last1= Allcock
|first1= Bill |last2= Chervenak |first2= Ann |last3= Foster |first3= Ian |last4= Kesselman |first4= Carl |last5= Livny |first5= Miron 
|year= 2005
|title= Data Grid tools: enabling science on big distributed data
|journal= Journal of Physics: Conference Series
|volume= 16
|pages= 571&#8211;575
|publisher= Institute of Physics Publishing
|doi= 10.1088/1742-6596/16/1/079
|url= http://iopscience.iop.org/1742-6596/16/1/079
|accessdate= April 15, 2012}}

*{{cite journal
|last1=Allcock
|first1=Bill |last2=Foster |first2=Ian |last3=Nefedova  |first3= Veronika  l|last4= Chervenak |first4= Ann |last5= Deelman |first5= Ewa |last6= Kesselman |first6= Carl |last7= Lee |first7= Jason |last8= Sim |first8= Alex |last9= Shoshani |first9= Arie |last10= Drach |first10=Bob |last11= Williams |first11= Dean    
|title= High-performance remote access to climate simulation data: A challenge problem for data grid technologies
|work =
|publisher =  [[ACM Press]]
|year = 2001
|citeseerx = 10.1.1.64.6603
|format =
|doi =
|accessdate = &lt;!-- April 20, 2012 --&gt; }}

*{{cite web
 |last1=Arcot 
 |first1=Rajasekar 
 |last2=Wan 
 |first2=Michael 
 |last3=Moore 
 |first3=Reagan 
 |last4=Schroeder 
 |first4=Wayne 
 |last5=Kremenek 
 |first5=George 
 |title=Storage resource broker &#8211; managing distributed data in a grid 
 |work= 
 |publisher= 
 |date= 
 |url=http://www.npaci.edu/DICE/Pubs/CSI-paper-sent.doc 
 |format= 
 |doi= 
 |accessdate=April 28, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20060507193028/http://www.npaci.edu:80/DICE/Pubs/CSI-paper-sent.doc 
 |archivedate=May 7, 2006 
 |df= 
}}

*{{cite journal
|last1= Belalem
|first1= Ghalem |last2= Meroufel |first2= Bakhta 
|year= 2011
|title= Management and placement of replicas in a hierarchical data grid 
|journal= International Journal of Distributed and Parallel Systems (IJDPS)
|volume= 2
|issue= 6
|pages= 23&#8211;30
|location =
|publisher=
|doi= 10.5121/ijdps.2011.2603
|url= http://www.scribd.com/doc/75105419/Management-and-Placement-of-Replicas-in-a-Hierarchical-Data-Grid
|accessdate= April 28, 2012}}

*{{cite journal
|last1= Chervenak
|first1= A.|last2= Foster |first2= I. |last3= Kesselman |first3= C.|last4= Salisbury |first4= C. |last5= Tuecke |first5= S.
|year= 2001
|title= The data grid: towards an architecture for the distributed management and analysis of large scientific datasets 
|journal= Journal of Network and Computer Applications
|volume= 23
|issue= 
|pages= 187&#8211;200
|location =
|publisher=
|doi= 10.1006/jnca.2000.0110
|url= http://www.globus.org/alliance/publications/papers/JNCApaper.pdf
|accessdate= April 11, 2012}}

*{{cite web
|last1= Chervenak
|first1= Ann |last2= Schuler |first2= Robert |last3= Kesselman | first3= Carl |last4= Koranda |first4= Scott |last5= Moe |first5= Brian
|title= Wide area data replication for scientific collaborations
|work= |publisher = [[IEEE]]
|date = November 14, 2005
|url= http://www.globus.org/alliance/publications/papers/chervenakGrid2005.pdf
|format=
|doi=
| accessdate = April 25, 2012 }}

*{{cite journal
 |last1=Coetzee 
 |first1=Serena 
 |year=2012 
 |title=Reference model for a data grid approach to address data in a dynamic SDI 
 |journal=Geoinformatica 
 |volume=16 
 |issue=1 
 |pages=111&#8211;129 
 |location= 
 |publisher= 
 |doi=10.1007/s10707-011-0129-4 
 |url=http://web.up.ac.za/sitefiles/file/48/16053/Coetzee_2011_ReferenceModelForDataGridApproach(2).pdf 
 |accessdate=April 28, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

*{{cite conference
|last1= Epimakhov
|first1= Igor 
|last2= Hameurlain
|first2= Abdelkader 
|last3= Dillon
|first3= Tharam 
|last4= Morvan
|first4= Franck 
|title= Resource Scheduling Methods for Query Optimization in Data Grid Systems 
|booktitle = Advances in Databases and Information Systems. 15th International Conference, ADBIS 2011
|pages = 185&#8211;199
|publisher = Springer Berlin Heidelberg
|year = 2011
|location = Vienna, Austria
|url = http://link.springer.com/chapter/10.1007%2F978-3-642-23737-9_14
|doi = 10.1007/978-3-642-23737-9_14
|id = 
|accessdate = September 20, 2011 }}

*{{cite web
|last1= Globus
|first1= 
|title= About the globus toolkit
|work=
|publisher= [[Globus Alliance|Globus]]
|year= 2012
|url= http://www.globus.org/toolkit/about.html
|doi=
|accessdate = May 27, 2012 }}

*{{cite news
 |last1=Heingartner 
 |first1=Douglas 
 |title=The Grid: The Next-Gen Internet 
 |work=Wired 
 |publisher= 
 |date=March 8, 2001 
 |url=http://www.wired.com/science/discoveries/news/2001/03/42230 
 |format= 
 |doi= 
 |accessdate=May 13, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20120504035536/http://www.wired.com:80/science/discoveries/news/2001/03/42230 
 |archivedate=May 4, 2012 
 |df= 
}}

*{{cite web
 |last1=Izmailov 
 |first1=Rauf 
 |last2=Ganguly 
 |first2=Samrat 
 |last3=Tu 
 |first3=Nan 
 |title=Fast parallel file replication in data grid 
 |work= 
 |publisher= 
 |year=2004 
 |url=http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf 
 |format= 
 |doi= 
 |accessdate=May 10, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20120421081052/http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf 
 |archivedate=April 21, 2012 
 |df= 
}}

*{{cite journal
 |last1=Kranthi 
 |first1=G. Aruna 
 |last2=Rekha 
 |first2=D. Shashi 
 |year=2012 
 |title=Protected data objects replication in data grid 
 |journal=International Journal of Network Security &amp; Its Applications (IJNSA) 
 |volume=4 
 |issue=1 
 |pages=29&#8211;41 
 |location= 
 |publisher= 
 |doi=10.5121/ijnsa.2012.4103 
 |url=http://journaldatabase.org/articles/protected_data_objects_replication.html 
 |issn=0975-2307 
 |accessdate=April 1, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20131008004646/http://journaldatabase.org/articles/protected_data_objects_replication.html 
 |archivedate=October 8, 2013 
 |df= 
}}

*{{cite journal
|last1= Krauter
|first1= Klaus |last2= Buyya |first2= Rajkumar |last3= Maheswaran |first3= Muthucumaru
|year= 2002
|title= A taxonomy and survey of grid resource management systems for distributed computing
|journal= Software Practice and Experience (SPE)
|volume= 32
|issue= 2
|pages= 135&#8211;164 
|location = 
|publisher= 
|doi=10.1002/spe.432
|citeseerx = 10.1.1.38.2122
|accessdate= &lt;!-- April 17, 2012 --&gt;}}


*{{cite conference
|last1= Lamehamedi
|first1= Houda |last2= Szymanski |first2= Boleslaw |last3= Shentu |first3= Zujun |last4= Deelman |first4= Ewa  
|title = Data replication strategies in grid environments
|booktitle = Fifth International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP&#8217;02)
|pages = 378&#8211;383
|publisher = Press
|year = 2002
|location =
|citeseerx = 10.1.1.11.5473
|doi =
|accessdate = &lt;!-- April 5, 2012 --&gt; }}

*{{cite journal
|last1= Padala
|first1= Pradeep
|title= A survey of data middleware for Grid systems
|work =
|publisher = 
|date = 
|citeseerx = 10.1.1.114.1901
|format =
|doi =
|accessdate = &lt;!-- April 28, 2012 --&gt; }}

*{{cite web
|last1= Raman 
|first1= Vijayshankar |last2= Narang |first2= Inderpal |last3= Crone |first3= Chris |last4= Hass |first4= Laura |last5= Malaika |first5= Susan 
|title= Services for data access and data processing on grids
|work =
|publisher = 
|date = February 9, 2003
|url = http://www.ogf.org/documents/GFD.14.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite conference
|last1= Ranganathan
|first1= Kavitha | last2= Foster |first2= Ian
|title= Identifying dynamic replication strategies for a high performance data grid
|booktitle= In Proc. of the International Grid Computing Workshop
|pages= 75&#8211;86
|publisher = 
|year= 2001
|location=
|citeseerx = 10.1.1.20.6836
|format=
|doi= 10.1007/3-540-45644-9_8
|accessdate = &lt;!-- May 15, 2012 --&gt; }}

*{{cite journal
|last1= Rasool
|first1= Qaisar |last2= Li |first2= Jianzhong |last3= Oreku| first3= George S.|last4= Munir |first4= Ehsan Ullah
|year= 2008
|title= Fair-share replication in data grid
|journal= Information Technology Journal
|volume= 7
|issue= 5
|pages= 776&#8211;782
|publisher=
|doi= 10.3923/itj.2008.776.782
|url= http://scialert.net/abstract/?doi=itj.2008.776.782
|accessdate= April 27, 2012 }}

*{{cite journal
|last1= Shorfuzzaman
|first1= Mohammad |last2= Graham |first2= Peter |last3= Eskicioglu |first3= Rasit 
|year= 2010
|title= Adaptive replica placement in hierarchical data grids
|journal= Journal of Physics: Conference Series
|volume= 256
|issue= 1
|pages= 1&#8211;18
|location =
|publisher= [[IOP Publishing Ltd]]
|doi= 10.1088/1742-6596/256/1/012020
|url= http://iopscience.iop.org/1742-6596/256/1/012020
|accessdate= April 15, 2012}}

*{{cite journal
|last1= Sreelatha
|first1= Malempati
|year= 2011
|title= Grid based approach for data confidentiality 
|journal= International Journal of Computer Applications
|volume= 25
|issue= 9
|pages= 1&#8211;5
|location =
|publisher=
|doi= 10.5120/3063-4186
|issn = 0975-8887
|url= http://www.ijcaonline.org/volume25/number9/pxc3874186.pdf
|accessdate= April 28, 2012}}

*{{cite journal
|last1= Thibodeau
|first1=P.   
|title= Governments plan data grid projects
|journal= Computerworld
|volume= 39
|issue= 42
|pages= 14
|location= United States
|publisher= Computerworld
|date = May 30, 2005
|url = http://www.computerworld.com/s/article/102119/Governments_Plan_Data_Grid_Projects
|format =
|doi=
|issn= 0010-4841
|accessdate = April 28, 2012 }}

*{{cite web
|last1= Thomas
|first1= R. K. |last2= Sandhu |first2= R. S. 
|title= Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management
|work =
|publisher = 
|year = 1997
|url = http://profsandhu.com/confrnc/ifip/i97tbac.pdf
|format =
|doi =
|accessdate = April 28, 2012 }}

*{{cite web
|last1=Tierney
|first1=Brian L.   
|title= Data grids and data grid performance issues
|work =
|publisher = 
|year = 2000
|url = http://www-didc.lbl.gov/presentations/CSC2000-tierney.pdf
|format =
|doi =
|accessdate = April 28, 2012 }}

*{{cite journal
|last1= Venugopal
|first1= Srikumar |last2= Buyya |first2= Rajkumar |last3= Ramamohanarao |first3= Kotagiri
|year= 2006
|title= A taxonomy of data grids for distributed data sharing, management and processing
|journal= ACM Computing Surveys
|volume= 38
|issue= 1
|pages= 1&#8211;60 
|location = New York
|publisher= [[Association for Computing Machinery]]
|doi=
|url= http://www.cloudbus.org/reports/DataGridTaxonomy.pdf
|accessdate= April 10, 2012}}

*{{cite web
 |last1=Zhu 
 |first1=Lichun 
 |title=Metadata management in grid database federation 
 |work= 
 |publisher= 
 |date= 
 |url=http://cs.uwindsor.ca/richard/cs510/lichun_zhu_survey.pdf 
 |format= 
 |doi= 
 |accessdate=May 15, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

==Further reading==

*{{cite web
|last1= Allcock
|first1= W.
|authorlink = W. Allcock
|title= Gridftp: protocol extensions to ftp for the grid
|work=
|publisher = [[Argonne National Laboratory]]
|date = April 2003
|url = http://www.globus.org/alliance/publications/papers/GFD-R.0201.pdf
|format = 
|doi =
| accessdate = April 20, 2012 }}

*{{cite web
|last1= Allcock
|first1= W.|last2= Bresnahan |first2= J. |last3= Kettimuthu |first3= R.|last4= Link |first4= M.|last5= Dumitrescu |first5= C.|last6= Raicu |first6= I.|last7= Foster |first7= I.
|title= The globus striped gridftp framework and server
|work=
|publisher= [[ACM Press]]
|date= November 2005
|url= http://www.globus.org/alliance/publications/papers/gridftp_final.pdf
|format=
|doi=
|accessdate = April 20, 2012 }}

*{{cite journal
|last1= Foster
|first1= Ian |last2= Kesselman |first2= Carl |last3= Tuecke |first3= Steven
|year= 2001
|title= The anatomy of the grid enabling scalable virtual organizations
|journal= [[International Journal of High Performance Computing Applications]]
|volume= 15
|issue= 3
|pages= 200&#8211;222 
|location = Thousand Oaks
|publisher= [[Sage Publications]]
|doi=10.1177/109434200101500302
|url= http://www.globus.org/alliance/publications/papers/anatomy.pdf
|accessdate= April 10, 2012}}

*{{cite web
 |last1=Foster 
 |first1=Ian 
 |last2=Kesselman 
 |first2=Carl 
 |last3=Nick 
 |first3=Jeffrey M. 
 |last4=Tuecke 
 |first4=Steven 
 |title=The physiology of the grid: an open grid services architecture for distributed systems integration 
 |work= 
 |publisher= 
 |date=June 22, 2002 
 |url=http://forge.gridforum.org/sf/go/doc13483?nav=1 
 |format= 
 |doi= 
 |accessdate=May 10, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20080322035911/http://forge.gridforum.org:80/sf/go/doc13483?nav=1 
 |archivedate=March 22, 2008 
 |df= 
}}

*{{cite journal
|last1= Hancock
|first1= B.
|year= 2009
|title= A simple data grid using the inferno operating system
|journal= Library Hi Tech
|volume= 27
|issue= 3
|pages= 382&#8211;392 
|location =
|publisher= [[Emerald Group Publishing Limited]]
|doi= 10.1108/07378830910988513
|url= }}&lt;!--|accessdate= April 10, 2012--&gt;

*{{cite web
 |last1=Hoschek 
 |first1=W. 
 |last2=McCance 
 |first2=G. 
 |title=Grid enabled relational database middleware 
 |work= 
 |publisher=[[Global Grid Forum]] 
 |date=October 10, 2001 
 |url=http://ppewww.ph.gla.ac.uk/preprints/2001/11/GGF3Rome2001.pdf 
 |format= 
 |doi= 
 |accessdate=April 22, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20060128234459/http://ppewww.ph.gla.ac.uk:80/preprints/2001/11/GGF3Rome2001.pdf 
 |archivedate=January 28, 2006 
 |df= 
}}

*{{cite web
|last1= Kunszt
|first1= Peter Z.|last2= Guy |first2= Leanne P.
|title= The open grid services architecture and data grids
|work =
|publisher = 
|date = July 7, 2002
|url = http://www.computing.surrey.ac.uk/courses/csm23/Papers/data_grid.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite web
|last1= Moore
|first1= Reagan W. 
|title= Evolution of data grid concepts
|work =
|publisher = 
|date = 
|url = http://www.nesc.ac.uk/events/GGF10-DA/programme/papers/06-Moore-Grid-evolution.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite conference
|last1= Rajkumar
|first1= Kettimuthu |last2= Allcock |first2= William |last3= Liming |first3= Lee |last4= Navarro |first4= John-Paul |last5= Foster |first5= Ian
| title = GridCopy moving data fast on the grid
| booktitle = International parallel and distributed processing symposium (IPDPS 2007)
| pages = 1&#8211;6
| publisher = IEEE International
| date = March 30, 2007
| location =  Long Beach
| url = http://www.globus.org/alliance/publications/papers/GridCopy.pdf
| doi =
| id =
| accessdate = April 29, 2012 }}

*{{cite journal
|last1= Thenmozhi
|first1= N. |last2= Madheswaran |first2= M. 
|year= 2011
|title= Content based data transfer mechanism for efficient bulk data transfer in grid computing environment
|journal= International Journal of Grid Computing &amp; Applications (IJGCA)
|volume= 2
|issue= 4
|pages= 49&#8211;62
|location =
|publisher=
|doi= 10.5121/ijgca.2011.2405
|issn= 2229-3949
|url= http://www.scribd.com/doc/78611092/Content-Based-Data-Transfer-Mechanism-for-Efficient-Bulk-Data-Transfer-in-Grid-Computing-Environment
|accessdate= April 28, 2012}}

*{{cite journal
 |last1=Tu 
 |first1=Manghui 
 |last2=Li 
 |first2=Peng 
 |last3=I-Ling 
 |first3=Yen 
 |last4=Thuraisingham 
 |first4=Bhavani 
 |last5=Khan 
 |first5=Latifur 
 |year=2010 
 |title=Secure data objects replication in data grid 
 |journal=IEEE Transactions on Dependable and Secure Computing 
 |volume=7 
 |issue=1 
 |pages=50&#8211;64 
 |publisher=[[IEEE]] 
 |doi=10.1109/tdsc.2008.19 
 |url=http://www.utdallas.edu/~lkhan/papers/Secure_Data_Objects_Replication_in_Data_Grid.pdf 
 |accessdate=April 26, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

[[Category:Data management]]</text>
      <sha1>3tu5no0jl5rdox5zurrzsxdfzlupkxp</sha1>
    </revision>
  </page>
  <page>
    <title>Classora</title>
    <ns>0</ns>
    <id>35915041</id>
    <revision>
      <id>576035764</id>
      <parentid>542703533</parentid>
      <timestamp>2013-10-06T19:45:14Z</timestamp>
      <contributor>
        <username>Lemnaminor</username>
        <id>17971031</id>
      </contributor>
      <comment>Disambiguated: [[ETL]] &#8594; [[Extract, transform, load]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7236" xml:space="preserve">{{Refimprove|date=June 2012}}

'''Classora''' is a [[knowledge base]] for the [[Internet]] oriented to [[data]] analysis. From a practical point of view, Classora is a [[digital repository]] that stores structured [[information]] and allows it to be displayed in multiple formats: analytically, graphically, geographically (through maps); as well as carry out [[OLAP]] analysis. The information contained in Classora comes from public sources&lt;ref&gt;[http://revista.mundo-r.com/contido/%E2%80%9Cclassora-evita-tener-que-bucear-entre-resultados-google-o-wikipedia%E2%80%9D Interview in R Technological Magazine (Spanish)]&lt;/ref&gt; and is uploaded into the system through bots and [[Extract, transform, load|ETL]] processes. The [[Knowledge Base]] has a '''commercial API'''&lt;ref&gt;[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Classora API in Official Weblog]&lt;/ref&gt; for semantic enhancement, and an '''open web'''&lt;ref&gt;[http://www.classora.com Open Web of Classora Knowledge Base]&lt;/ref&gt; through which any user can access to part of the information collected (it also allows users to complete data and share opinions).

Internally, Classora is organized into '''Knowledge Units''' and '''Reports'''. A &#171;Knowledge Unit&#187; is any element of the World about which information may be stored and presented in the form of a data sheet (a person, a company, a country, etc.) A &#171;Report&#187; is a group of Knowledge Units: a ranking of companies, a sport classification table, a survey about people, etc. In fact, one of the technical capabilities of Classora is that it allows the comparison of reports and knowledge units gathered from different sources, thereby generating an added value for the media in which this information is published: digital media, interactive TV, etc.

== Key definitions ==

=== Knowledge unit ===
The '''units of knowledge''' (also known as ''entries'') in Classora are data sheets that have a certain semantic equivalence with the articles on the Wikipedia: they store information about any element of the world, be it a film, a country, a company or an animal. However, they differ from Wikipedia in that Classora stores structured information, enriched with a metadata layer; and therefore it is able to automatically interpret the meaning of each unit of knowledge.

=== Data report ===
A '''report''' is a group of units of knowledge in which the repetition of elements is not allowed. This definition includes any list, poll, ranking, etc.; and, in general, any consultation that involves more than one unit of knowledge. Classora excels at the reports management due to its visualization capabilities, being able to display data in the form of tables, graphs and maps. 

Types of reports:

* '''Sports scores''': Sports competitions results sanctioned by the competent institution.
* '''Rankings and lists''': All types of interesting and curious lists, whether they have an implicit order or not.
* '''Polls''': Units of knowledge that are ranked according to users&#8217; votes.
* '''Queries to the Knowledge Base''': Questions from users using [[Contextual Query Language|CQL]].
* '''Networks of connections''': automatically calculated from the reports and the taxonomy of each Knowledge Unit.

=== Organizational taxonomy ===
An '''organizational taxonomy''' (also referred to as '''entry type''') is a data sheet that brings together the common attributes of a set of units of knowledge. For instance, the organizational taxonomy ''F1 Driver'' displays attributes such as date of debut, team, etc.; and the organizational taxonomy ''Football Club'' presents attributes such as city, stadium, etc.

In Classora, taxonomies are hierarchically organized, so that they inherit attributes from their parent taxonomies. For instance, ''F1 Driver'' is a subsidiary taxonomy of ''Sportsperson'', which is a subsidiary taxonomy of ''Person'', which in turn is a subsidiary taxonomy of ''Organism''.

The simplest type of entry in Classora is '''Classora Object'''. All the other taxonomies are its subsidiaries and inherit its attributes. In fact, the only attribute Classora Object possesses is ''name'' (all units of knowledge are required to have one name at least).

== Architecture of Classora ==

=== Data Extraction Module ===
The Data Extraction Module consists of a set of robots coordinated by software that also manages the potential incidents. Most of the information available in Classora is automatically uploaded through those robots, which connect to the main online public sources to gather all types of data. There are three categories of robots:
* '''Extraction robots''': responsible for the massive uploading of reports from official public sources (FIFA, CIA, IMF, Eurostat...). They are used for either absolute or incremental data uploading.
* '''Data scanner robots''': responsible for looking for and updating the data of a unit of knowledge. They use specific sources to perform this task: Wikipedia, IMDB, World Bank, etc.
* '''Content aggregators''':  they don&#8217;t connect to external sources. Instead, they generate new information using Classora&#8217;s internal database.

=== Participatory Module ===
In Classora&#8217;s Open Website, Internet users may participate providing their knowledge as they would on the Wikipedia. There are different ways to participate: adding or correcting data in the Knowledge Base, voting in surveys (participatory rankings) and creating new Knowledge Units and Data Reports.

=== Connectivity Module ===
The Knowledge Base is designed to be embedded in multi-platform, multi-channel systems, thus enabling its integration into mobile devices, tablets, interactive TV, etc. This integration may be carried out through specific plugins (for [[navigators]] or other devices) or an [[API]] [[REST]] that provides content in [[XML]] or [[JSON]] formats. The API is divided into three blocks of operations. The first one is the block of '''general utility tools''' (ranging from autosuggest components about geographical hierarchies to operations to obtain the list of today&#8217;s celebrity birthdays, using [[Contextual Query Language|CQL]]). The second one is the block of '''operations for widget generation''' (graphs, maps, rankings) using information from the knowledge base. Finally, there is a block of '''operations designed for the publication of free-source content'''.&lt;ref&gt;[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Post about API in Classora official weblog]&lt;/ref&gt;

== Project statistics ==
As of April 2012, 2,000,000 Knowledge Units, 15,000 Reports, around 10,000 Maps and several million potential Comparative Analyses had been added to Classora. According to the site of web metrics Alexa, Classora Open Website is ranked at 100,557 globally and at 2,880 in the Spanish traffic ranking.&lt;ref&gt;[http://www.alexa.com/siteinfo/http%3A%2F%2Fwww.classora.com Alexa metrics for Classora Open Web]&lt;/ref&gt; Users spend an average of 9 &#189; minutes in Classora.

== External links ==
* [http://www.classora.com Open Website of Classora Knowledge Base]

== References ==
&lt;references/&gt;

[[Category:Knowledge bases]]
[[Category:Data management]]
[[Category:Semantic Web]]
[[Category:Knowledge representation software]]</text>
      <sha1>t8d4ctshnw8bjrtv5egnrk4bijjix2k</sha1>
    </revision>
  </page>
  <page>
    <title>Data set (IBM mainframe)</title>
    <ns>0</ns>
    <id>1042727</id>
    <revision>
      <id>703227916</id>
      <parentid>703227879</parentid>
      <timestamp>2016-02-04T07:15:08Z</timestamp>
      <contributor>
        <username>DMacks</username>
        <id>712163</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/14.139.120.66|14.139.120.66]] ([[User talk:14.139.120.66|talk]]) to last version by Deeday-UK</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6698" xml:space="preserve">{{about|mainframe computer file|a general meaning in computing field|Data set}}
{{Refimprove|date=September 2014}}

In the context of [[IBM]] [[mainframe computer]]s, a '''data set''' (IBM preferred) or  '''dataset''' is a [[computer file]] having a [[record-oriented file|record organization]]. Use of this term began with [[OS/360]] and is still used by its successors, including the current [[z/OS]]. Documentation for these systems historically preferred this term rather than ''[[computer file|file]]''.

A data set is typically stored on a [[direct access storage device]] (DASD) or [[magnetic tape]], however unit record devices, such as punch card readers, card punch, and line printers can provide input/output (I/O) for a data set (file).&lt;ref&gt;http://publib.boulder.ibm.com/infocenter/zvm/v5r4/index.jsp?topic=/com.ibm.zvm.v54.hcpa7/hcse7b3050.htm&lt;/ref&gt;

Data sets are not unstructured streams of [[byte]]s, but rather are organized in various logical record and block structures determined by the &lt;code&gt;DSORG&lt;/code&gt; (data set organization), &lt;code&gt;RECFM&lt;/code&gt; (record format), and other parameters. These parameters are specified at the time of the data set allocation (creation), for example with [[Job Control Language]] &lt;code&gt;DD&lt;/code&gt; statements. Inside a job they are stored in the [[Data Control Block]] (DCB), which is a data structure used to access data sets, for example using [[access method]]s.

==Data set organization==
For OS/360, the DCB's DSORG parameter specifies how the data set is organized. It may be physically sequential ("PS"), indexed sequential ("IS"), partitioned ("PO"), or Direct Access ("DA"). Data sets on tape may only be DSORG=PS. The choice of organization depends on how the data is to be accessed, and in particular, how it is to be updated.

Programmers utilize various [[access method]]s (such as [[Queued Sequential Access Method|QSAM]] or [[VSAM]]) in programs for reading and writing data sets. Access method depends on the given data set organization.

==Record format (RECFM)==
Regardless of organization, the physical structure of each record is essentially the same, and is uniform throughout the data set. This is specified in the DCB &lt;code&gt;RECFM&lt;/code&gt; parameter. &lt;code&gt;RECFM=F&lt;/code&gt; means that the records are of fixed length, specified via the &lt;code&gt;LRECL&lt;/code&gt; parameter, and &lt;code&gt;RECFM=V&lt;/code&gt; specifies a variable-length record. V records when stored on media are prefixed by a Record Descriptor Word (RDW) containing the integer length of the record in bytes. With &lt;code&gt;RECFM=FB&lt;/code&gt; and &lt;code&gt;RECFM=VB&lt;/code&gt;, multiple logical records are grouped together into a single [[Block (data storage)|physical block]] on tape or disk. FB and VB are &lt;code&gt;fixed-blocked&lt;/code&gt;, and &lt;code&gt;variable-blocked&lt;/code&gt;, respectively. The &lt;code&gt;BLKSIZE&lt;/code&gt; parameter specifies the maximum length of the block. &lt;code&gt;RECFM=FBS&lt;/code&gt; could be also specified, meaning &lt;code&gt;fixed-blocked standard&lt;/code&gt;, meaning all the blocks except the last one were required to be in full &lt;code&gt;BLKSIZE&lt;/code&gt; length. &lt;code&gt;RECFM=VBS&lt;/code&gt;, or &lt;code&gt;variable-blocked spanned&lt;/code&gt;, means a logical record could be spanned across two or more blocks, with flags in the RDW indicating whether a record segment is continued into the next block and/or was continued from the previous one.

This mechanism eliminates the need for using any "delimiter" byte value to separate records. Thus data can be of any type, including binary integers, floating point, or characters, without introducing a false end-of-record condition. The data set is an abstraction of a collection of records, in contrast to files as unstructured streams of bytes.

=={{anchor|Partitioned datasets}}Partitioned data sets==
A '''partitioned data set''' ('''PDS''') is a data set containing multiple ''members'', each of which holds a separate sub-data set, similar to a [[directory (file systems)|directory]] in other types of [[file system]]s. This type of data set is often used to hold executable programs (''load modules''), source program libraries (especially Assembler macro definitions), and [[Job Control Language]]. A PDS may be compared to a [[ZIP (file format)|Zip]] file or [[COM Structured Storage]].

A Partitioned Data Set can only allocate on a single volume with the maximum size of 65535 tracks.

Besides members, a PDS consists also of their directory. Each member can be accessed directly using the directory structure. Once a member is located, the data stored in that member is handled in the same manner as a PS (sequential) data set.

Whenever a member is deleted, the space it occupied is unusable for storing other data. Likewise, if a member is re-written, it is stored in a new spot at the back of the PDS and leaves wasted &#8220;dead&#8221; space in the middle. The only way to recover &#8220;dead&#8221; space is to perform frequent file compression, that moves all members to the front of the data space and leaves free usable space at the back.  (Note that in modern parlance, this kind of operation might be called [[defragmentation]] or [[garbage collection (computer science)|garbage collection]]; [[data compression]] nowadays refers to a different, more complicated concept.)  PDS files can only reside on disk in order to use the directory structure to access individual members, not on tape. They are most often used for storing multiple JCL files, utility control statements and executable modules.

An improvement of this scheme is a Partitioned Data Set Extended (PDSE or PDS/E, sometimes just ''libraries'') introduced with [[MVS/XA]] system.

PDS/E structure is similar to PDS and is used to store the same types of data. However, PDS/E files have a better directory structure which does not require pre-allocation of directory blocks when the PDS/E is defined (and therefore does not run out of directory blocks if not enough were specified). Also, PDS/E automatically stores members in such a way that compression operation is not needed to reclaim "dead" space. PDS/E files can only reside on disk in order to use the directory structure to access individual members.

==See also==
* [[Volume table of contents]] (VTOC), a structure describing data sets stored on the disk
* [[Distributed Data Management Architecture]]

==References==
{{Reflist}}
* [http://publib-b.boulder.ibm.com/Redbooks.nsf/RedbookAbstracts/sg246366.html Introduction to the New Mainframe: z/OS Basics], Ch. 5, "Working with data sets", March 29, 2011. ISBN 0738435341

{{Mainframe I/O access methods}}

{{DEFAULTSORT:Data Set (IBM Mainframe)}}
[[Category:Data management]]
[[Category:IBM mainframe operating systems]]
[[Category:Computer file systems]]
[[Category:Computer files]]</text>
      <sha1>pn0k1cl4n1a1pejkb9wab7btxi3d2k7</sha1>
    </revision>
  </page>
  <page>
    <title>Content-oriented workflow models</title>
    <ns>0</ns>
    <id>38907061</id>
    <revision>
      <id>738697827</id>
      <parentid>716152100</parentid>
      <timestamp>2016-09-10T14:50:06Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* Distributed Document-oriented */clean up; HTTP&amp;rarr;HTTPS for [[Github]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18553" xml:space="preserve">{{Orphan|date=May 2014}}

The goal of '''content-oriented workflow models''' is to articulate workflow progression by the presence of content units (like data-records/objects/documents).
Most content-oriented workflow approaches provide a life-cycle model for content units, such that workflow progression can be qualified by conditions on the state of the units.
Most approaches are research and work in progress and the content models and life-cycle models are more or less formalized.

The term ''content-oriented workflows'' is an umbrella term for several scientific workflow approaches, namely "data-driven", "resource-driven", "artifact-centric", "object-aware", and "document-oriented". Thus, the meaning of "content" ranges from simple data attributes to self-contained documents; the term "content-oriented workflows" appeared at first in &lt;ref name="Neumann2010" /&gt; as an umbrella term. Such general term, independent from a specific approach, is necessary to contrast the content-oriented modelling principle with traditional activity-oriented workflow models (like [[Petri net]]s or [[Business Process Model and Notation|BPMN]]) where a workflow is driven by a control flow and where the content production perspective is neglected or even missing.

The term "content" was chosen to subsume the different levels in granularity of the content units in the respective workflow models; it was also chosen to make associations with [[content management]]. Both terms "artifact-centric" and "data-driven" would also be good candidates for an umbrella term, but each is closely related to a specific approach of a single working group. The "artifact-centric" group itself (i.e. IBM Research) has generalized the characteristics of their approach and has used "information-centric" as an umbrella term in.&lt;ref name="Kumaran2008" /&gt; Yet, the term [[information]] is too unspecific in the context of computer science, thus, "content-orientated workflows" is considered as good compromise.

== Workflow Model Approaches ==

=== Data-driven ===

The data-driven process structures provides a sophisticated workflow model being specialized on hierarchical write-and-review-processes.
The approach provides interleaved synchronization of sub-processes and extends activity diagrams.
Unfortunately, the COREPRO prototype implementation is not publicly available.

Research on the project had been ceased. The general idea has been continued by Reichert in form of the [[#Object-aware]] approach.

; Synonyms
: data-driven process structures / data-driven modeling and coordination
;Protagonists
: Dr. Dominic M&#252;ller (University of Twente), Joachim Herbst (DaimlerChrysler Research), and Manfred Reichert (at this time [http://wwwhome.cs.utwente.nl/~reichertm/index01.htm Assoc. Prof. at Univ. of Twente], currently [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. at Ulm Univ.])
;Organization(s)
: University of Twente, DaimlerChrysler
;Period
: 2005 - 2007
;Selected publications
:&lt;ref name="Mueller2006" /&gt;&lt;ref name="Mueller2007" /&gt;
;Implementation
: [http://www.utwente.nl/ewi/is/research/completed_projects/completed_projects/corepro.doc/ COREPRO]

=== Resource-driven ===

The resource-driven workflow system is an early approach that considered workflows from a content-oriented perspective and emphasizes on the missing support for plain document-driven processes by traditional activity-oriented workflow engines.
The resource-driven approach demonstrated the application of database triggers for handling workflow events.
Still the system implementation is centralized and the workflow schema is statically defined.
The project appeared in 2005 but many aspects are considered future work by the authors.

Research did not continue on the project. Wang completed his PhD thesis in 2009, yet, his thesis does not mention the resource-driven approach to workflow modelling but is about discrete event simulation.

;Synonyms
: Resource-based Workflows / Document-Driven Workflow Systems
;Protagonists
: Jianrui Wang and [http://www.personal.psu.edu/axk41/ Prof. Akhil Kumar]
;Organization
: Pennsylvania State University
;Period
: 2005 - today
;Selected publications
:&lt;ref name="Wang2005" /&gt;&lt;ref name="Kumar2010" /&gt;
;Implementation
: N/A

=== Artifact-centric ===
{{See also|Artifact-centric business process model}}

The artifact-centric approach appears as a mature framework for general purpose content-oriented workflows.
The distribution of the enterprise application landscape with its business services is considered, yet, the workflow engine itself seems to be centralized.
The process enactment seems to be tightly coupled with a technically pre-integrated database management system infrastructure.
The latter makes it most suitable for manufacturing process or for organizational processes within a well-defined institutional scope.
The approach remains work in progress, still, it is a relatively old and established project on content-oriented workflows.
Funded by IBM, it has comparably high number of developers.
It is a promising approach.

;Synonyms
: artifact-centric business process models / artifact-based business process (ACP) / artifact-centric workflows
;Protagonists
: [http://domino.research.ibm.com/comm/research_people.nsf/pages/hull.index.html Richard Hull] and Dr. Kamal Bhattacharya as well as Cagdas E. Gerede and Jianwen Su
;Organization
: IBM (T.J. Watson Research Center, NY)
;Period
: 2007 - today
;Selected publications
:&lt;ref name="Bhattacharya2007" /&gt;&lt;ref name="Calvanese2009" /&gt;
;Implementation
: [http://domino.research.ibm.com/comm/research_projects.nsf/pages/artifact.index.html ArtiFact]

=== Object-aware ===

The object-aware approach manages a set of object types and generates forms for creating object instances.
The form completion flow is controlled by transitions between object configurations each describing a progressing set of mandatory attributes.
Each object configuration is named by an object state.
The data production flow is user-shifting and it is discrete by defining a sequence of object states.
The discussion is currently limited to a centralized system, without any workflows across different organizations.
However, the approach is of great relevance to many domains like concurrent engineering.
Finally, the object-aware approach and its PHILharmonicFlows system are going to provide general-purpose workflow systems for generic enactment of data production processes.

;Synonyms
: object-aware process management / datenorientiertes Prozess-Management-System
;Protagonists
: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/vera-kuenzle.html Vera K&#252;nzle] and [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. Manfred Reichert]
;Organization
: Ulm University
;Period
: 2009 - today
;Selected publications
:&lt;ref name="Kuenzle2009" /&gt;&lt;ref name="Kuenzle2010" /&gt;
;Implementation
: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/research/projects/philharmonic-flows.html PHILharmonicFlows]

=== Distributed Document-oriented ===

Distributed document-oriented process management (dDPM) enables distributed case handling in heterogeneous system environments and it is based on document-oriented [[semantic integration|integration]].
The workflow model reflects the paper-based working practice in inter-institutional healthcare scenarios.
It targets distributed knowledge-driven ad-hoc workflows, wherein distributed information systems are required to coordinate work with initially unknown sets of actors and activities.

The distributed workflow engine supports process planning &amp; process history as well as participant management and process template creation with import/export.
The workflow engine embeds a functional fusion of 1) group-based instant messaging 2) with a shared work list editor 3) with version control.
The software implementation of dDPM is &#945;-Flow which is available as open source.
dDPM and &#945;-Flow provide a content-oriented approach to schema-less workflows.

The complete distributed case handling application is provided in form of a single active Document ("&amp;alpha;-Doc").
The &#945;-Doc is a case file (as information carrier) with an embedded workflow engine (in form of active properties).
Inviting process participants is equivalent to providing them with a copy of an &#945;-Doc, copying it like an ordinary desktop file.
All &#945;-Docs that belong to the same case can synchronize each other, based on the participant management, electronic postboxes, store-and-forward messaging, and an offline-capable synchronization protocol.

;Synonyms
: distributed document-oriented process management (dDPM), distributed case handling via active documents
;Protagonists
: [http://www6.informatik.uni-erlangen.de/people/cpn/ Christoph P. Neumann] and [http://www6.informatik.uni-erlangen.de/people/lenz/ Prof. Richard Lenz]
;Organization
: Friedrich-Alexander-Universit&amp;auml;t Erlangen-N&amp;uuml;rnberg
;Period
: 2009 - 2012
;Selected Publications
:&lt;ref name="Neumann2011" /&gt;&lt;ref name="Neumann2012" /&gt; and a PhD thesis &lt;ref name="DissNeumann2012" /&gt;
;Implementation
: [https://github.com/cpnatwork/alphaflow_dev &amp;alpha;-Flow (open source)]

== Related Concepts ==

=== Content Management ===

The bandwidth of [[Content management system]]s (CMS) reaches from [[Web content management system]]s (WCMS) and [[Document management system]] (DMS) to [[Enterprise Content Management]] (ECM). Mature DMS products support document production workflows in a basic form, primarily focusing on review cycle workflows concerning a single document. Market leaders are [[Alfresco (software)|Alfresco]] , [[eXo Platform]] and EMC with [[Documentum]].

=== Groupware and Computer-Supported Cooperative Work ===
[[Groupware]] focuses on messaging (like E-Mail, Chat, and Instant Messaging), shared calendars (e.g. Lotus Notes, Microsoft Outlook with Exchange Server), and conferencing (e.g. Skype).
Groupware overlaps with [[Computer-supported cooperative work]] (CSCW), that originated from shared multimedia editors (for live drawing/sketching) and synchronous multi-user applications like [[desktop sharing]]. The extensive conceptual claim of CSWC must be put into perspective by its actual solution scope, that is available as the [[CSCW#CSCW Matrix|CSCW Matrix]].

=== Case Handling ===

The case handling paradigm stems from Prof. van der Aalst and gained momentum in 2005. The core features are:
(a) provide all information available, i.e. present the case as a whole rather than showing bits and pieces,
(b) decide about activities on the basis of the information available rather than the activities already executed,
(c) separate work distribution from authorization and allow for additional types of roles, not just the execute role, and
(d) allow workers to view and add/modify data before or after the corresponding activities have been executed.

In healthcare, the flow of a patient between healthcare professionals is considered as a workflow - with activities that include all kinds of diagnostic or therapeutic treatments. The workflow is considered as a case, and workflow management in healthcare is to handle these cases.

Case handling is orthogonal to content-oriented workflows. Some content-oriented workflow approaches are not related to case handling, but, for example, to automated manufacturing. In contrast, systems that are considered to be case handling systems (CHS) but which do not apply a content-oriented workflow model are, for example, BPMone (formerly PROTOS and FLOWer) from Pallas Athena, ECHO from Digital, CMDT from ICL, and Vectus from London Bridge Group. In conclusion, those content-oriented workflow approaches that are tightly related to case handling are the [[#Resource-driven]] workflow model and the [[#Distributed Document-oriented]] workflow model.

;Protagonists
: [http://wwwis.win.tue.nl/~wvdaalst/ Prof. Wil van der Aalst] and Associate Professor Dr. [http://reijers.com/ Hajo Reijers] (with focus on healthcare)
;Organization
: Univ. of Technology, Eindhoven
;Period
: 2001 - today
;Selected publication
:&lt;ref name="Reijers2003" /&gt;&lt;ref name="Aalst2005" /&gt;

== See also ==
* [[Process philosophy]]
* [[Workflow]]
* [[Adam Smith]]
* [[Process control]]
* [[Process management]]
* [[Business process]]
* [[Business process automation]]
* [[Business process management]]
* [[Business Process Model and Notation]]
* [[Advanced case management]]
* [[Content management system]]
* [[Web content management system]]
* [[Document management system]]
* [[Enterprise Content Management]]

== References ==
&lt;references&gt;
&lt;ref name="Neumann2010"&gt;Christoph P. Neumann and Richard Lenz: [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5541988 The alpha-Flow Use-Case of Breast Cancer Treatment - Modeling Inter-Institutional Healthcare Workflows by Active Documents]. In: Proc of the 8th Int'l Workshop on Agent-based Computing for Enterprise Collaboration (ACEC) at the 19th Int'l Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises (WETICE 2010), Larissa, Greece, June 2010. ([http://www6.informatik.uni-erlangen.de/publications/public/2010/acec2010_neumann_alphaUC.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kumaran2008"&gt;Kumaran, S., R. Liu, and F. Wu. [http://www.springerlink.com/content/a837212173812011/ On the Duality of Information-Centric and Activity-Centric Models of Business Processes]. In: Advanced Information Systems Engineering. 2008. p. 32-47.&lt;/ref&gt;
&lt;ref name="Mueller2006"&gt;Dominic M&#252;ller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/kpm9454157514825/ Flexibility of Data-Driven Process Structures]. In: Business Process Management Workshops Lecture Notes in Computer Science, 2006, Volume 4103/2006, 181-192. ([http://dbis.eprints.uni-ulm.de/111/2/Mueller06-DPM.pdf PDF])&lt;/ref&gt;
&lt;ref name="Mueller2007"&gt;Dominic M&#252;ller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/2771670210653747/ Data-Driven Modeling and Coordination of Large Process Structures]. On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS Lecture Notes in Computer Science, 2007, Volume 4803/2007, 131-149. ([http://dbis.eprints.uni-ulm.de/116/1/Mueller07-CoopIS.pdf PDF])&lt;/ref&gt;
&lt;ref name="Bhattacharya2007"&gt;Kamal Bhattacharya, Cagdas Gerede, Richard Hull, Rong Liu, and Jianwen Su. 2007. [http://dl.acm.org/citation.cfm?id=1793141 Towards formal analysis of artifact-centric business process models]. In Proceedings of the 5th international conference on Business process management (BPM'07), Gustavo Alonso, Peter Dadam, and Michael Rosemann (Eds.). Springer-Verlag, Berlin, Heidelberg, 288-304. (pages 289ff in: [http://alumni.cs.ucsb.edu/~gerede/research/papers/bghls-bpm07-Artifact.pdf PDF])&lt;/ref&gt;
&lt;ref name="Calvanese2009"&gt;Diego Calvanese, Giuseppe De Giacomo, Richard Hull und Jianwen Su. [http://www.springerlink.com/content/t58342lj86807111/ Artifact-Centric Workflow Dominance]. Lecture Notes in Computer Science, 2009, Volume 5900/2009, 130-143. ([http://www.dis.uniroma1.it/~degiacom/papers/2009/ICSOC09.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kuenzle2009"&gt;Vera K&#252;nzle und Manfred Reichert. [http://www.springerlink.com/content/n6448q47g0474242/ Towards Object-Aware Process Management Systems: Issues, Challenges, Benefits]. In: Enterprise, Business-Process and Information Systems Modeling Lecture Notes in Business Information Processing, 2009, Volume 29, Part 1, Part 5, 197-210, DOI: 10.1007/978-3-642-01862-6_17 ([http://dbis.eprints.uni-ulm.de/526/1/BPMDS09_Kuenzle_Reichert.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kuenzle2010"&gt;K&#252;nzle, Vera and Reichert, Manfred. 2010. [http://dbis.eprints.uni-ulm.de/647/ Herausforderungen bei der Integration von Benutzern in Datenorientierten Prozess-Management-Systemen]. EMISA Forum, 30 (1). pp. 11-28. ISSN 1610-3351 ([http://dbis.eprints.uni-ulm.de/647/2/KuRe10.pdf PDF])&lt;/ref&gt;
&lt;ref name="Wang2005"&gt;Wang, J. and A. Kumar. [http://www.springerlink.com/content/79k8v7terwchn5ct/ A Framework for Document-Driven Workflow Systems]. In: Business Process Management. 2005. p. 285-301. ([http://php.scripts.psu.edu/faculty/a/x/axk41/BPM05-jerry-reprint.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kumar2010"&gt;Akhil Kumar und Jianrui Wang. [http://www.springerlink.com/content/n218t085521q1347/ A Framework for Designing Resource-Driven Workflows]. In: Handbook on Business Process Management 1, International Handbooks on Information Systems, 2010, Part III, 419-440.&lt;/ref&gt;
&lt;ref name="Neumann2011"&gt;Christoph P. Neumann, Peter K. Schwab, Andreas M. Wahl and Richard Lenz. alpha-Adaptive: Evolutionary Workflow Metadata in Distributed Document-Oriented Process Management. In: Proc of the 4th Int'l Workshop on Process-oriented Information Systems in Healthcare (ProHealth'11) in conjunction with the 9th Int'l Conf on Business Process Management (BPM'11), Clermont-Ferrand, France, August 2011. ([http://www6.informatik.uni-erlangen.de/publications/public/2011/prohealth2011_neumann.pdf PDF])&lt;/ref&gt;
&lt;ref name="Neumann2012"&gt;Christoph P. Neumann and Richard Lenz. The alpha-Flow Approach to Inter-Institutional Process Support in Healthcare. International Journal of Knowledge-Based Organizations. IGI Global, 2012.&lt;/ref&gt;
&lt;ref name="DissNeumann2012"&gt;Christoph P. Neumann. [http://www.dr.hut-verlag.de/978-3-8439-0919-8.html Distributed Case Handling]. PhD thesis (German 'Dissertation'). Friedrich-Alexander-Universit&amp;auml;t Erlangen-N&amp;uuml;rnberg. 2012.&lt;/ref&gt;
&lt;ref name="Reijers2003"&gt;Hajo Reijers , Jaap Rigter , Wil Van Der Aalst. [http://www.worldscinet.com/ijcis/12/1203/S0218843003000784.html The Case Handling Case]. International Journal of Cooperative Information Systems (IJCIS), Volume: 12, Issue: 3(2003) pp. 365-391. ([http://is.tm.tue.nl/staff/hreijers/H.A.%20Reijers%20Bestanden/chc.pdf PDF])&lt;/ref&gt;
&lt;ref name="Aalst2005"&gt;[[Wil M.P. van der Aalst]], [[Mathias Weske]], Dolf Gr&#252;nbauer. [http://www.sciencedirect.com/science/article/pii/S0169023X04001296 Case handling: a new paradigm for business process support]. In: Data &amp;amp; Knowledge Engineering, Volume 53, Issue 2, May 2005, Pages 129-162, ISSN 0169-023X, 10.1016/j.datak.2004.07.003. ([http://www.imamu.edu.sa/Scientific_selections/abstracts/AbstratctIT1/Case%20handling%20a%20new%20paradigm%20for%20business%20process%20support.pdf PDF])&lt;/ref&gt;
&lt;/references&gt;

{{DEFAULTSORT:Content-oriented Workflows}}
&lt;!--Categories--&gt;
[[Category:Workflow technology]]
[[Category:Data management]]</text>
      <sha1>oyyp9598ggb3s69ce7is6nr7s7jf40e</sha1>
    </revision>
  </page>
  <page>
    <title>XSA</title>
    <ns>0</ns>
    <id>15323614</id>
    <revision>
      <id>667005266</id>
      <parentid>667005265</parentid>
      <timestamp>2015-06-15T05:31:30Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contributions/59.32.95.83|59.32.95.83]] to version by X2Fusion. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (2273970) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1877" xml:space="preserve">In computer science, '''XSA''' (better known as '''Cross-Server Attack''') is a networking security intrusion method which allows for a malicious client to compromise security over a website or service on a server by using implemented services on the server that may not be secure.

In general, XSA is demonstrated against websites, yet sometimes it is used in conjunction with other services located on the same server.

== Basics ==
XSA is a method that allows for a malicious client to use services that a remote server implements in order to attack another service on the same server or network.

Most website hosting companies that offer hosting for large or even little amounts of separate websites are vulnerable to this method of attack, because of the amount of access services such as [[PHP]] and the webserver itself give to a client that allows the client to access other website configurations, files, passwords and the like.

== History ==

The term 'XSA' was first coined by DeadlyData, a prominent [[Computer hacker]] during the early 2000s, over the voice communications software [[TeamSpeak]]. While he had not invented or pioneered this method of intrusion, he coined it as a shortened term to describe the act of performing Cross-Server Attacks (XSAs).

It was then used further in the community and now supports for most of the methods and subsets of the method that give both [[Computer hacker]] and malicious individuals the terminology to attack websites using software that is located on the same server.

== See also ==
{{Portal|Software Testing}}
*[[SQL Injection]] (SQLi)
*[[Cross-Site Scripting]] (XSS)
*[[Cross-Site Request Forgery]] (CSRF)
*[[Buffer Overflow]]

{{DEFAULTSORT:XSA}}
[[Category:Data management]]
[[Category:Computer security exploits]]
[[Category:Computer network security]]
[[Category:World Wide Web]]
[[Category:Web development]]</text>
      <sha1>3osnpcdw5oa7o192dg5145tii64e33b</sha1>
    </revision>
  </page>
  <page>
    <title>Content repository</title>
    <ns>0</ns>
    <id>18877735</id>
    <revision>
      <id>750173701</id>
      <parentid>677710153</parentid>
      <timestamp>2016-11-18T04:50:43Z</timestamp>
      <contributor>
        <ip>61.82.109.244</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2343" xml:space="preserve">A '''content  repository''' or '''content store''' is a database of digital content with an associated set of data management, search and access methods allowing application-independent access to the content, rather like a digital library, but with the ability to store and modify content in addition to searching and retrieving. The content repository acts as the storage engine for a larger application such as a [[Content Management System]] or a [[Document Management System]], which adds a [[user interface]] on top of the repository's [[application programming interface]].&lt;ref&gt;[http://openacs.org/doc/acs-content-repository/design.html Content Repository Design], [http://openacs.org/doc/acs-content-repository/ ACS Content Repository], [http://openacs.org/ OpenACS.org].&lt;/ref&gt;

==Advantages provided by repositories==

*Common rules for data access allow many applications to work with the same content without interrupting the data.
*They give out signals when changes happen, letting other applications using the repository know that something has been modified, which enables collaborative data management.
*Developers can deal with data using programs that are more compatible with the desktop programming environment.
*The data model is scriptable when users use a content repository.

== Content repository features ==
A content repository may provide functionality such as:
* Add/edit/delete content
* Hierarchy and sort order management
* Query / search
* Versioning
* Access control
* Import / export
* Locking
* Life-cycle management
* Retention and holding / records management

== Examples ==

* [[Apache Jackrabbit]]
* ModeShape

==Applications==
*[[Content management]]
*[[Document management system|Document management]]
*[[Digital asset management]]
*[[Records management]]
*[[Revision control]]
*[[Social collaboration]]
*[[Web content management system|Web content management]]

== Standards and specification ==
*[[Content repository API for Java]]
*[[WebDAV]]
*[[Content Management Interoperability Services]]

== See also ==
* [[Information repository]]
* [[Content (media)]]

== References ==
{{reflist}}

==External links==
* [http://db-engines.com/en/ranking/content+store DB-Engines Ranking of Content Stores] by popularity, updated monthly

[[Category:Data management]]
[[Category:Content management systems]]</text>
      <sha1>bbh79vfx07038mk4sprn7q3xf4pjpj7</sha1>
    </revision>
  </page>
  <page>
    <title>Small data</title>
    <ns>0</ns>
    <id>41530851</id>
    <revision>
      <id>755120538</id>
      <parentid>743759064</parentid>
      <timestamp>2016-12-16T09:47:00Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Definition */clean up; http&amp;rarr;https for [[The Guardian]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4793" xml:space="preserve">
==Introduction==
The term small data did not exist before the word big data which came into use in the 1990's. What we once called data is now called small data.  Small data was not made useless by the advent of big data. Most data we see in our life is small data, and it should not be overlooked in any field. 

This article helps to define small data and to give examples in marketing and recruiting to help in understanding.

==Definition==
'''Small data''' (''sm&#8217;a&#275;&#8217;&#257;ll DH(&#601;)ta'') is data that is 'small' enough for human comprehension.&lt;ref&gt;{{cite news|author=Rufus Pollock |url=https://www.theguardian.com/news/datablog/2013/apr/25/forget-big-data-small-data-revolution |title=Forget big data, small data is the real revolution &amp;#124; News |newspaper=[[The Guardian]] |date= |accessdate=2016-10-02}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://jwork.org/main/node/18 |title="Small data". Never heard this term? |website=jWork.ORG |date= |accessdate=2016-10-02}}&lt;/ref&gt; It is data in a volume and format that makes it accessible, informative and actionable.&lt;ref&gt;{{cite web|url=http://whatis.techtarget.com/definition/small-data |title=What is small data? - Definition from WhatIs.com |website=Whatis.techtarget.com |date=2016-08-18 |accessdate=2016-10-02}}&lt;/ref&gt;

The term "big data" is about machines and "small data" is about people.&lt;ref&gt;{{cite web|author=Eric Lundquist |url=http://www.eweek.com/enterprise-apps/small-data-analysis-the-next-big-thing-advocates-assert.html/ |title='Small Data' Analysis the Next Big Thing, Advocates Assert |website=Eweek.com |date=2013-09-10 |accessdate=2016-10-02}}&lt;/ref&gt; This is to say that eye witness observations or five pieces of related data could be small data. Small data is what we used to think of as data. The only way to comprehend [[Big data]] is to reduce the data into small, visually-appealing objects representing various aspects of large data sets (such as
[[histogram]], [[chart]]s, and scatter plots). So sometimes big data is simplified to be like small data. 

A formal definition of small data has been proposed by Allen Bonde, VP of Innovation at [[Actuate Corporation|Actuate]]: "Small data connects people with timely, meaningful insights (derived from big data and/or &#8220;local&#8221; sources), organized and packaged &#8211; often visually &#8211; to be accessible, understandable, and actionable for everyday tasks."&lt;ref&gt;{{cite web|url=http://smalldatagroup.com/2013/10/18/defining-small-data/ |title=Defining Small Data |publisher=Small Data Group |date= |accessdate=2016-10-02}}&lt;/ref&gt;
 
Another definition of '''small data''' is:
* The small set of specific attributes produced by the [[Internet of Things]]. These are typically a small set of sensor data such as temperature, wind speed, vibration and status.&lt;ref&gt;{{cite web|author= |url=http://www.forbes.com/sites/mikekavis/2015/02/25/forget-big-data-small-data-is-driving-the-internet-of-things/#4a72ffad661b |title=Forget Big Data - Small Data Is Driving The Internet Of Things |website=Forbes.com |date= |accessdate=2016-10-02}}&lt;/ref&gt;


==Some Examples of Uses in Business==

===Marketing===

Bonde has written extensively about the topic for Forbes,&lt;ref&gt;{{cite web|author= |url=http://www.forbes.com/sites/markfidelman/2012/10/30/these-smart-social-apps-bring-big-data-down-to-size/ |title=These Smart, Social Apps Bring Big Data Down to Size |website=Forbes.com |date= |accessdate=2016-10-02}}&lt;/ref&gt; Direct Marketing News,&lt;ref&gt;{{cite web|url=http://www.dmnews.com/why-small-data-is-the-next-big-thing-for-marketers/article/308376/ |title=Why Small Data Is the Next Big Thing for Marketers - DMN |website=Dmnews.com |date=2013-08-22 |accessdate=2016-10-02}}&lt;/ref&gt; CMO.com&lt;ref&gt;{{cite web|last=Bonde |first=Allen |url=http://www.cmo.com/features/articles/2013/11/20/think_small_time_for.html |title=Think Small: Time For Marketers To Move Beyond The Big Data Hype |website=Cmo.com |date=2013-12-12 |accessdate=2016-10-02}}&lt;/ref&gt; and other publications.  

According to Martin Lindstrom, in his book, [[Small Data: The Tiny Clues that Uncover Huge Trends|Small Data:]] "{In customer research, small data is} Seemingly insignificant behavioral observations containing very specific attributes pointing towards an unmet customer need. Small data is the foundation for break through ideas or completely new ways to turnaround brands."&lt;ref&gt;{{cite web|url=https://www.martinlindstrom.com/small-data/ |title=Small Data - Martin Lindstrom - Bestselling Author |publisher=Martin Lindstrom |date= |accessdate=2016-10-02}}&lt;/ref&gt;

==Conclusion==
Small Data is what we used to call data. The hype about Big Data should not cause us to look down on Small Data. Small Data's practicality and depth of insight is often better than big data.

==References==
{{Reflist}}

[[Category:Data management]]</text>
      <sha1>g8630n1o1obzla4jd773zzjyf5rge7f</sha1>
    </revision>
  </page>
  <page>
    <title>Data management</title>
    <ns>0</ns>
    <id>759312</id>
    <revision>
      <id>759832794</id>
      <parentid>759831987</parentid>
      <timestamp>2017-01-13T11:20:35Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/Suraj jain2|Suraj jain2]] ([[User talk:Suraj jain2|talk]]) to last version by AnomieBOT</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11095" xml:space="preserve">{{lead too short|date=October 2014}}
'''Data management''' comprises all the [[List of academic disciplines|disciplines]] related to managing [[data]] as a valuable resource.

== Overview ==
The official definition provided by [[DAMA]] International, the professional organization for those in the data management profession, is: "Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise." This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as [[relational database]] management.

[[File:The Data Lifecycle.jpg|thumb|The data lifecycle]]

Alternatively, the definition provided in the DAMA Data Management Body of Knowledge (&lt;ref&gt;[https://technicspub.com/dmbok/ DAMA-DMBOK]&lt;/ref&gt;) is:
"Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets."&lt;ref&gt;"DAMA-DMBOK Guide (Data Management Body of Knowledge) Introduction &amp; Project Status" (Note: PDF no longer available online at https://www.dama.org, current version available for purchase)&lt;/ref&gt;

The concept of "Data Management" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to [[random access]] processing.  Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that "Data Management" was more important than "[[Process Management]]" used arguments such as "a customer's home address is stored in 75 (or some other large number) places in our computer systems."  During this period, random access processing was not competitively fast, so those suggesting "Process Management" was more important than "Data Management" used batch processing time as their primary argument.  As applications moved into real-time, [[interactive]] applications, it became obvious to most practitioners that both management processes were important.  If the data was not well defined, the data would be mis-used in applications.  If the process wasn't well defined, it was impossible to meet user needs.

==Corporate Data Quality Management==
[[Corporate Data Quality Management]] (CDQM) is, according to the [[EFQM|European Foundation for Quality Management]] and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.&lt;ref&gt;[https://benchmarking.iwi.unisg.ch/Framework_for_CDQM.pdf EFQM ; IWI-HSG: EFQM Framework for Corporate Data Quality Management. Brussels : EFQM Press, 2011]&lt;/ref&gt;
* '''Strategy for Corporate Data Quality''': As CDQM is affected by various business drivers and requires involvement of multiple divisions in an  organization; it must be considered a company-wide endeavor.
* '''Corporate Data Quality Controlling''': Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.
* '''Corporate Data Quality Organization''': CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.
* '''Corporate Data Quality Processes and Methods''': In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company&#8217;s daily processes.
* '''Data Architecture for Corporate Data Quality''': The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.
* '''Applications for Corporate Data Quality''': Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.

== Topics in Data Management ==
Topics in Data Management, grouped by the DAMA DMBOK Framework,&lt;ref&gt;[http://dama-dach.org/dama-dmbok-functional-framework DAMA-DMBOK Functional Framework v3]&lt;/ref&gt; include:
{{colbegin|2}}
# [[Data governance]]
#* [[Data asset]]
#* [[Data governance]]
#* [[Data steward]]
# Data Architecture, Analysis and Design
#* [[Data analysis]]
#* [[Data architecture]]
#* [[Data modeling]]
# Database Management
#* [[Data maintenance]]
#* [[Database administration]]
#* [[Database management system]]
# Data Security Management
#* [[Data access]]
#* [[Data erasure]]
#* [[Data privacy]]
#* [[Data security]]
# Data Quality Management
#* [[Data cleansing]]
#* [[Data integrity]]
#* [[Data enrichment]]
#* [[Data quality]]
#* [[Data quality assurance]]
# Reference and Master Data Management
#* [[Data integration]]
#* [[Master data management]]
#* [[Reference data]]
# Data Warehousing and Business Intelligence Management
#* [[Business intelligence]]
#* [[Data mart]]
#* [[Data mining]]
#* Data movement ([[Extract, transform, load ]])
#* [[Data warehouse]]
# Document, Record and Content Management
#* [[Document management system]]
#* [[Records management]]
# Meta Data Management
#* [[Meta-data management]]
#* [[Metadata]]
#* [[Metadata discovery]]
#* [[Metadata publishing]]
#* [[Metadata registry]]
# Contact Data Management
#* [[Business continuity planning]]
#* [[Marketing operations]]
#* [[Customer data integration]]
#* [[Identity management]]
#* [[Identity theft]]
#* [[Data theft]]
#* [[ERP software]]
#* [[CRM software]]
#* [[Address (geography)]]
#* [[Postal code]]
#* [[Email address]]
#* [[Telephone number]]
{{colend}}

==Body of Knowledge==
The DAMA Guide to the Data Management Body of Knowledge" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009.

==Usage==

In modern [[management fad|management usage]], one can easily discern a trend away from the term "data" in composite expressions to the term "[[information]]" or even "[[knowledge]]" when talking in a non-technical context. Thus there exists not only data management, but also [[information management]] and [[knowledge management]]. This is a misleading trend as it obscures that traditional data are managed or somehow [[data processing|processed]] on second looks.{{cn|date=June 2016}} The distinction between data and derived values can be seen in the [[information ladder]].{{cn|date=June 2016}} While data can exist as such, "information" and "knowledge" are always in the "eye" (or rather the brain) of the beholder and can only be measured in relative units.

Several organisations have established a data management centre (DMC)&lt;ref&gt;
For example: {{cite book
| last1                 = Kumar
| first1                = Sangeeth
| last2                 = Ramesh
| first2                = Maneesha Vinodini
| chapter               = Lightweight Management framework (LMF) for a Heterogeneous Wireless Network for Landslide Detection
| editor1-last          = Meghanathan
| editor1-first         = Natarajan
| editor2-last          = Boumerdassi
| editor2-first         = Selma
| editor3-last          = Chaki
| editor3-first         = Nabendu
| editor4-last          = Nagamalai
| editor4-first         = Dhinaharan
| title                 = Recent Trends in Networks and Communications: International Conferences, NeCoM 2010, WiMoN 2010, WeST 2010,Chennai, India, July 23-25, 2010. Proceedings
| url                   = https://books.google.com/books?id=8i5qCQAAQBAJ
| series                = Communications in Computer and Information Science
| volume                = 90
| publisher             = Springer
| publication-date      = 2010
| page                  = 466
| isbn                  = 9783642144936
| accessdate            = 2016-06-16
| quote                 = 4.4 Data Management Center (DMC)[:] The Data Management Center is the data center for all of the deployed cluster networks. Through the DMC, the LMF allows the user to list the services in any cluster member belonging to any cluster [...].
}}
&lt;/ref&gt;
for their operations.

==Integrated data management==

'''Integrated data management''' (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its [[Information Lifecycle Management|lifetime]].&lt;ref&gt;[http://www.ibm.com/developerworks/data/library/techarticle/dm-0807hayes/?S_TACT=105AGX11&amp;S_CMP=FP#ibm-content  Integrated Data Management: Managing data across its lifecycle] by Holly Hayes&lt;/ref&gt;&lt;ref&gt;[http://www.ibmsystemsmagmainframedigital.com/nxtbooks/ibmsystemsmag/mainframe_20090708/index.php#/34 Organizations thrive on Data] by Eric Naiburg&lt;/ref&gt;&lt;ref&gt;[http://download.boulder.ibm.com/ibmdl/pub/software/data/sw-library/data-management/optim/reports/fragmented.pdf Fragmented Management Across The Data Life Cycle Increases Cost And Risk] - A commissioned study conducted by Forrester Consulting on behalf of IBM October 2008&lt;/ref&gt;&lt;ref&gt;[http://publib.boulder.ibm.com/infocenter/idm/v2r1/index.jsp integrated IBM Data Management information center]&lt;/ref&gt; IDM's purpose is to:
*Produce enterprise-ready applications faster
*Improve data access, speed iterative testing
*Empower collaboration between architects, developers and DBAs
*Consistently achieve service level targets
*Automate and simplify operations
*Provide contextual intelligence across the [[solution stack]]
*Support business growth
*Accommodate new initiatives without expanding infrastructure
*Simplify application upgrades, consolidation and retirement
*Facilitate alignment, consistency and governance
*Define business policies and standards up front;  share, extend, and apply throughout the lifecycle

==See also==
{{colbegin|2}}
* [[Open data]]
* [[Information architecture]]
* [[Information management]]
* [[Enterprise architecture]]
* [[Information design]]
* [[Information system]]
* [[Controlled vocabulary]]
* [[Data curation]]
* [[Data retention]]
* [[Data governance]]
* [[Data quality]]
* [[Data modeling]]
* [[Data management plan]]
* [[Information lifecycle management]]
* [[Computer data storage]]
* [[Data proliferation]]
* [[Digital preservation]]
* [[Digital perpetuation]]
* [[Document management]]
* [[Enterprise content management]]
* [[Hierarchical storage management]]
* [[Information repository]]
* [[Records management]]
* [[System integration]]
{{colend}}

== References ==
{{Reflist}}

==External links==
* {{dmoz|Computers/Software/Master_Data_Management/Articles/}}

{{DEFAULTSORT:Data Management}}
[[Category:Data management| ]]
[[Category:Information technology management]]</text>
      <sha1>brrnhrscuhleamr2lud3zz3hh9w4siu</sha1>
    </revision>
  </page>
  <page>
    <title>Tuple</title>
    <ns>0</ns>
    <id>132729</id>
    <revision>
      <id>760773110</id>
      <parentid>754254433</parentid>
      <timestamp>2017-01-19T00:14:56Z</timestamp>
      <contributor>
        <ip>49.149.201.73</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15122" xml:space="preserve">{{About|the mathematical concept|the musical term|Tuplet}}
{{Redir|Octuple|the type of rowing boat|Octuple scull}}
{{Redir|Duodecuple|the term in music|Twelve-tone technique}}

A '''tuple''' is a finite ordered list of [[Element (mathematics)|elements]].  In [[mathematics]], an '''{{math|''n''}}-tuple''' is a [[sequence]] (or ordered list) of {{math|''n''}} elements, where {{math|''n''}} is a non-negative integer. There is only one 0-tuple, an empty sequence. An {{math|''n''}}-tuple is [[Recursive definition|defined inductively]] using the construction of an [[ordered pair]]. Tuples are usually written by listing the elements within parentheses "&lt;math&gt;(\text{ })&lt;/math&gt;" and separated by commas; for example, &lt;math&gt;(2, 7, 4, 1, 7)&lt;/math&gt; denotes a 5-tuple. Sometimes other symbols are used to surround the elements, such as square brackets "[ ]" or angle brackets "&lt; &gt;". Braces "{ }" are only used in defining arrays in some programming languages such as [[Java (programming language)|Java]], but not in mathematical expressions, as they are the standard notation for [[set (mathematics)|sets]]. Tuples are often used to describe other mathematical objects, such as [[Vector (mathematics and physics)|vectors]]. In computer science, tuples are directly implemented as [[product type]]s in most [[functional programming|functional programming languages]].{{citation needed|date=January 2016}} More commonly, they are implemented as [[Record (computer science)|record types]], where the components are labeled instead of being identified by position alone.{{citation needed|date=January 2016}} This approach is also used in [[relational algebra]]. Tuples are also used in relation to programming the [[semantic web]]  with the [[Resource Description Framework]] (RDF). Tuples are also used in [[linguistics]]&lt;ref&gt;{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199202720.001.0001/acref-9780199202720-e-2276|title=N&#8208;tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}&lt;/ref&gt; and [[philosophy]].&lt;ref&gt;{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199541430.001.0001/acref-9780199541430-e-2262|title=Ordered n-tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}&lt;/ref&gt;

==Etymology==
The term originated as an abstraction of the sequence: single, double, triple, quadruple, quintuple, sextuple, septuple, octuple, ..., {{math|''n''}}&#8209;tuple, ..., where the prefixes are taken from the [[Latin]] names of the numerals. The unique 0&#8209;tuple is called the null tuple. A 1&#8209;tuple is called a singleton, a 2&#8209;tuple is called an ordered pair and a 3&#8209;tuple is a triple or triplet. {{math|''n''}} can be any nonnegative [[integer]]. For example, a [[complex number]] can be represented as a 2&#8209;tuple, a [[quaternion]] can be represented as a 4&#8209;tuple, an [[octonion]] can be represented as an 8&#8209;tuple and a [[sedenion]] can be represented as a 16&#8209;tuple.

Although these uses treat ''&#8209;tuple'' as the suffix, the original suffix was ''&#8209;ple'' as in "triple" (three-fold) or "decuple" (ten&#8209;fold). This originates from  [[medieval Latin]]  ''plus'' (meaning "more") related to [[Greek language|Greek]] &#8209;&#960;&#955;&#959;&#8166;&#962;, which replaced the classical and late antique  ''&#8209;plex'' (meaning "folded"), as in "duplex".&lt;ref&gt;''OED'', ''s.v.'' "triple", "quadruple", "quintuple", "decuple"&lt;/ref&gt;

===Names for tuples of specific lengths===

{| class="wikitable"
|-
! Tuple Length &lt;math&gt;n&lt;/math&gt; !! Name !! Alternative names
|-
| align="right" | 0 || empty tuple || unit / empty sequence
|-
| align="right" | 1 || single || [[Singleton_(mathematics)|singleton]] / monuple 
|-
| align="right" | 2 || double || couple / (ordered) pair / dual / twin / product
|-
| align="right" | 3 || triple || treble / triplet / triad
|-
| align="right" | 4 || quadruple || quad
|-
| align="right" | 5 || quintuple || pentuple
|-
| align="right" | 6 || sextuple ||hextuple
|-
| align="right" | 7 || septuple ||heptuple
|-
| align="right" | 8 || octuple || 
|-
| align="right" | 9 || nonuple || 
|-
| align="right" | 10 || decuple || 
|-
| align="right" | 11 || undecuple || hendecuple
|-
| align="right" | 12 || duodecuple || 
|-
| align="right" | 13 || tredecuple || 
|-
| align="right" | 14 || quattuordecuple || 
|-
| align="right" | 15 || quindecuple ||
|-
| align="right" | 16 || sexdecuple ||
|-
| align="right" | 17 || septendecuple ||
|-
| align="right" | 18 || octodecuple ||
|-
| align="right" | 19 || novemdecuple ||
|-
| align="right" | 20 || vigintuple ||
|-
| align="right" | 21 || unvigintuple ||
|-
| align="right" | 22 || duovigintuple ||
|-
| align="right" | 23 || trevigintuple ||
|-
| align="right" | 24 || quattuorvigintuple ||
|-
| align="right" | 25 || quinvigintuple ||
|-
| align="right" | 26 || sexvigintuple ||
|-
| align="right" | 27 || septenvigintuple ||
|-
| align="right" | 28 || octovigintuple ||
|-
| align="right" | 29 || novemvigintuple ||
|-
| align="right" | 30 || trigintuple ||
|-
| align="right" | 31 || untrigintuple ||
|-
| align="right" | 40 || quadragintuple ||
|-
| align="right" | 50 || quinquagintuple ||
|-
| align="right" | 60 || sexagintuple ||
|-
| align="right" | 70 || septuagintuple ||
|-
| align="right" | 80 || octogintuple ||
|-
| align="right" | 90 || nongentuple ||
|-
| align="right" | 100 || centuple ||
|-
| align="right" | 1,000 || milluple ||
|-
|}

==Properties==
The general rule for the identity of two {{math|''n''}}-tuples is
: &lt;math&gt;(a_1, a_2, \ldots, a_n) = (b_1, b_2, \ldots, b_n)&lt;/math&gt; [[if and only if]] &lt;math&gt;a_1=b_1,\text{ }a_2=b_2,\text{ }\ldots,\text{ }a_n=b_n.&lt;/math&gt;

Thus a tuple has properties that distinguish it from a [[Set (mathematics)|set]].
# A tuple may contain multiple instances of the same element, so {{break|
}}tuple &lt;math&gt;(1,2,2,3) \neq (1,2,3)&lt;/math&gt;; but set &lt;math&gt;\{1,2,2,3\} = \{1,2,3\}&lt;/math&gt;.
# Tuple elements are ordered: tuple &lt;math&gt;(1,2,3) \neq (3,2,1)&lt;/math&gt;, but set &lt;math&gt;\{1,2,3\} = \{3,2,1\}&lt;/math&gt;.
# A tuple has a finite number of elements, while a set or a [[multiset]] may have an infinite number of elements.

==Definitions==

There are several definitions of tuples that give them the properties described in the previous section.

===Tuples as functions===
If we are dealing with sets, an {{math|''n''}}-tuple can be regarded as a [[Function (mathematics)#Definition|function]], {{math|''F''}},  whose domain is the tuple's implicit set of element indices, {{math|''X''}}, and whose codomain, {{math|''Y''}}, is the tuple's set of elements. Formally:
: &lt;math&gt;(a_1, a_2, \dots, a_n) \equiv (X,Y,F)&lt;/math&gt;
where:
: &lt;math&gt;
    \begin{align}
      X &amp; = \{1, 2, \dots, n\}                       \\
      Y &amp; = \{a_1, a_2, \ldots, a_n\}                \\
      F &amp; = \{(1, a_1), (2, a_2), \ldots, (n, a_n)\}. \\
    \end{align}
  &lt;/math&gt;
In slightly less formal notation this says:
:&lt;math&gt; (a_1, a_2, \dots, a_n) := (F(1), F(2), \dots, F(n)).&lt;/math&gt;

===Tuples as nested ordered pairs===
Another way of modeling tuples in Set Theory is as nested [[ordered pair]]s. This approach assumes that the notion of ordered pair has already been defined; thus a 2-tuple 
# The 0-tuple (i.e. the empty tuple) is represented by the empty set &lt;math&gt;\emptyset&lt;/math&gt;.
# An {{math|''n''}}-tuple, with {{math|''n'' &gt; 0}}, can be defined as an ordered pair of its first entry and an {{math|(''n'' &#8722; 1)}}-tuple (which contains the remaining entries when {{math|''n'' &gt; 1)}}:
#: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = (a_1, (a_2, a_3, \ldots, a_n))&lt;/math&gt;
This definition can be applied recursively to the {{math|(''n'' &#8722; 1)}}-tuple:
: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = (a_1, (a_2, (a_3, (\ldots, (a_n, \emptyset)\ldots))))&lt;/math&gt;

Thus, for example:
: &lt;math&gt;
    \begin{align}
         (1, 2, 3) &amp; = (1, (2, (3, \emptyset)))      \\
      (1, 2, 3, 4) &amp; = (1, (2, (3, (4, \emptyset)))) \\
    \end{align}
  &lt;/math&gt;

A variant of this definition starts "peeling off" elements from the other end:
# The 0-tuple is the empty set &lt;math&gt;\emptyset&lt;/math&gt;.
# For {{math|''n'' &gt; 0}}:
#: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = ((a_1, a_2, a_3, \ldots, a_{n-1}), a_n)&lt;/math&gt;
This definition can be applied recursively:
: &lt;math&gt;(a_1, a_2, a_3, \ldots, a_n) = ((\ldots(((\emptyset, a_1), a_2), a_3), \ldots), a_n)&lt;/math&gt;

Thus, for example:
: &lt;math&gt;
    \begin{align}
         (1, 2, 3) &amp; = (((\emptyset, 1), 2), 3)      \\
      (1, 2, 3, 4) &amp; = ((((\emptyset, 1), 2), 3), 4) \\
    \end{align}
  &lt;/math&gt;

===Tuples as nested sets===
Using [[ordered pair#Kuratowski definition|Kuratowski's representation for an ordered pair]], the second definition above can be reformulated in terms of pure [[set theory]]:
# The 0-tuple (i.e. the empty tuple) is represented by the empty set &lt;math&gt;\emptyset&lt;/math&gt;;
# Let &lt;math&gt;x&lt;/math&gt; be an {{math|''n''}}-tuple &lt;math&gt;(a_1, a_2, \ldots, a_n)&lt;/math&gt;, and let &lt;math&gt;x \rightarrow b \equiv (a_1, a_2, \ldots, a_n, b)&lt;/math&gt;. Then, &lt;math&gt;x \rightarrow b \equiv \{\{x\}, \{x, b\}\}&lt;/math&gt;.  (The right arrow, &lt;math&gt;\rightarrow&lt;/math&gt;, could be read as "adjoined with".)

In this formulation:
: &lt;math&gt;
   \begin{array}{lclcl}
     ()      &amp; &amp;                     &amp;=&amp; \emptyset                                    \\
             &amp; &amp;                     &amp; &amp;                                              \\
     (1)     &amp;=&amp; ()    \rightarrow 1 &amp;=&amp; \{\{()\},\{(),1\}\}                          \\
             &amp; &amp;                     &amp;=&amp; \{\{\emptyset\},\{\emptyset,1\}\}            \\
             &amp; &amp;                     &amp; &amp;                                              \\
     (1,2)   &amp;=&amp; (1)   \rightarrow 2 &amp;=&amp; \{\{(1)\},\{(1),2\}\}                        \\
             &amp; &amp;                     &amp;=&amp; \{\{\{\{\emptyset\},\{\emptyset,1\}\}\},     \\
             &amp; &amp;                     &amp; &amp; \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\}    \\
             &amp; &amp;                     &amp; &amp;                                              \\
     (1,2,3) &amp;=&amp; (1,2) \rightarrow 3 &amp;=&amp; \{\{(1,2)\},\{(1,2),3\}\}                    \\
             &amp; &amp;                     &amp;=&amp; \{\{\{\{\{\{\emptyset\},\{\emptyset,1\}\}\}, \\
             &amp; &amp;                     &amp; &amp; \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\}\}, \\
             &amp; &amp;                     &amp; &amp; \{\{\{\{\{\emptyset\},\{\emptyset,1\}\}\},   \\
             &amp; &amp;                     &amp; &amp; \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\},3\}\}                                       \\
    \end{array}
  &lt;/math&gt;

=={{anchor|n-tuple}}{{math|''n''}}-tuples of {{math|''m''}}-sets ==

In [[discrete mathematics]], especially [[combinatorics]] and finite [[probability theory]], {{math|''n''}}-tuples arise in the context of various counting problems and are treated more informally as ordered lists of length {{math|''n''}}.&lt;ref&gt;{{harvnb|D'Angelo|West|2000|p=9}}&lt;/ref&gt; {{math|''n''}}-tuples whose entries come from a set of {{math|''m''}} elements are also called ''arrangements with repetition'', ''permutations of a multiset'' and, in some non-English literature, ''[[Variation (disambiguation)#Mathematics|variations]] with repetition''. The number of {{math|''n''}}-tuples of an {{math|''m''}}-set is {{math|''m''&lt;sup&gt;''n''&lt;/sup&gt;}}. This follows from the combinatorial [[rule of product]].&lt;ref&gt;{{harvnb|D'Angelo|West|2000|p=101}}&lt;/ref&gt; If {{math|''S''}} is a finite set of [[cardinality]] {{math|''m''}}, this number is the cardinality of the {{math|''n''}}-fold [[Cartesian_product#Cartesian_power | Cartesian power]] {{math|''S'' &#215; ''S'' &#215; ... ''S''}}. Tuples are elements of this product set.

== Type theory ==
{{main|Product type}}
In [[type theory]], commonly used in [[programming language]]s, a tuple has a [[product type]]; this fixes not only the length, but also the underlying types of each component. Formally:
: &lt;math&gt;(x_1, x_2, \ldots, x_n) : \mathsf{T}_1 \times \mathsf{T}_2 \times \ldots \times \mathsf{T}_n&lt;/math&gt;
and the [[Projection (mathematics)|projection]]s are term constructors:
: &lt;math&gt;\pi_1(x) : \mathsf{T}_1,~\pi_2(x) : \mathsf{T}_2,~\ldots,~\pi_n(x) : \mathsf{T}_n&lt;/math&gt;

The tuple with labeled elements used in the [[#Relational_model|relational model]] has a [[Record (computer science)|record type]]. Both of these types can be defined as simple extensions of the [[simply typed lambda calculus]].&lt;ref name="pierce2002"&gt;{{cite book|last=Pierce|first=Benjamin|title=Types and Programming Languages|publisher=MIT Press|year=2002|isbn=0-262-16209-1|pages=126&#8211;132}}&lt;/ref&gt;

The notion of a tuple in type theory and that in set theory are related in the following way: If we consider the natural [[model theory|model]] of a type theory, and use the Scott brackets to indicate the semantic interpretation&lt;!-- do not link; that article needs to be a dab first--&gt;, then the model consists of some sets &lt;math&gt;S_1, S_2, \ldots, S_n&lt;/math&gt; (note: the use of italics here that distinguishes sets from types) such that:
: &lt;math&gt;[\![\mathsf{T}_1]\!] = S_1,~[\![\mathsf{T}_2]\!] = S_2,~\ldots,~[\![\mathsf{T}_n]\!] = S_n&lt;/math&gt;
and the interpretation of the basic terms is:
: &lt;math&gt;[\![x_1]\!] \in [\![\mathsf{T}_1]\!],~[\![x_2]\!] \in [\![\mathsf{T}_2]\!],~\ldots,~[\![x_n]\!] \in [\![\mathsf{T}_n]\!]&lt;/math&gt;.

The {{math|''n''}}-tuple of type theory has the natural interpretation as an {{math|''n''}}-tuple of set theory:&lt;ref&gt;Steve Awodey, [http://www.andrew.cmu.edu/user/awodey/preprints/stcsFinal.pdf ''From sets, to types, to categories, to sets''], 2009, [[preprint]]&lt;/ref&gt;
: &lt;math&gt;[\![(x_1, x_2, \ldots, x_n)]\!] = (\,[\![x_1]\!], [\![x_2]\!], \ldots, [\![x_n]\!]\,)&lt;/math&gt;
The [[unit type]] has as semantic interpretation the 0-tuple.

==See also==
{{Wiktionary|tuple}}
* [[Arity]]
* [[Exponential object]]
* [[Formal language]]
* [[Multidimensional Expressions#MDX data types|OLAP: Multidimensional Expressions]]
* [[Prime k-tuple]]
* [[Relation (mathematics)]]
* [[Tuplespace]]

==Notes==
{{Reflist}}

==References==

{{refbegin}}
* {{citation|first1=John P.|last1=D'Angelo|first2=Douglas B.|last2=West|title=Mathematical Thinking/Problem-Solving and Proofs|year=2000|edition=2nd|publisher=Prentice-Hall|isbn=978-0-13-014412-6}}
* [[Keith Devlin]], ''The Joy of Sets''. Springer Verlag, 2nd ed., 1993, ISBN 0-387-94094-4, pp.&amp;nbsp;7&#8211;8
* [[Abraham Adolf Fraenkel]], [[Yehoshua Bar-Hillel]], [[Azriel L&#233;vy]], ''[https://books.google.com/books?q=Foundations+of+set+theory&amp;btnG=Search+Books Foundations of set theory]'', Elsevier Studies in Logic Vol. 67, Edition 2, revised, 1973, ISBN 0-7204-2270-1, p.&amp;nbsp;33
* [[Gaisi Takeuti]], W. M. Zaring, ''Introduction to Axiomatic Set Theory'', Springer [[Graduate texts in mathematics|GTM]] 1, 1971, ISBN 978-0-387-90024-7, p.&amp;nbsp;14
* George J. Tourlakis, ''[https://books.google.com/books?as_isbn=9780521753746 Lecture Notes in Logic and Set Theory. Volume 2: Set theory]'', Cambridge University Press, 2003, ISBN 978-0-521-75374-6, pp.&amp;nbsp;182&#8211;193

{{refend}}
{{Set theory}}

&lt;!--Interwikies--&gt;

&lt;!--Categories--&gt;
{{Authority control}}
[[Category:Data management]]
[[Category:Mathematical notation]]
[[Category:Sequences and series]]
[[Category:Basic concepts in set theory]]
[[Category:Type theory]]
[[ar:&#1586;&#1608;&#1580; &#1605;&#1585;&#1578;&#1576;]]</text>
      <sha1>l07rhz3l24h92mfbfw2ryxc0yhcy71n</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of CDMI server implementations</title>
    <ns>0</ns>
    <id>40002906</id>
    <revision>
      <id>669096545</id>
      <parentid>667318977</parentid>
      <timestamp>2015-06-28T21:16:34Z</timestamp>
      <contributor>
        <username>Communal t</username>
        <id>25091450</id>
      </contributor>
      <minor />
      <comment>Added [[:Template:DEFAULTSORT]] with sortkey ''CDMI server implementation comparison''.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="25815" xml:space="preserve">
{| class="wikitable"
|-
! Implementation !! SNIA Reference Implementation !! CDMI-Serve !! CDMI-Proxy !! CDMI for OpenStack's Swift !! CDMI-z !! onedata
|-
| Version || [http://www.snia.org/forums/csi/programs/CDMIportal 1.0e] || [https://github.com/koenbollen/cdmi-serve 238c28fc7c] || [https://github.com/livenson/vcdm 0.1] || [https://github.com/osaddon/cdmi f0e3ad9bac] || 1 || [http://packages.onedata.org/oneprovider-Linux.rpm 2.0]
|-
| [[CDMI]] Version || 1.0.2 || ? || 1.0.1 || ? || 1.0.2 || 1.0.2
|-
| colspan="7" align="center" | '''HTTP features'''
|-
| [[HTTPS]] || ? || ? || {{Yes}} || ? || ? || {{Yes}}
|-
| [[Basic authentication]] || ? || ? || {{Yes}} || ? || ? || ?
|-
| [[Digest authentication]] || ? || ? || {{Yes}} || ? || ? || ?
|-
| [[X.509|X.509 authentication]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| X.509-VOMS authentication || ? || ? || ? || ? || ? || {{Yes}}
|-
| Token based authentication || ? || ? || ? || ? || ? || {{Yes}}
|-
| colspan="7" align="center" | '''Data access methods'''
|-
| [[Filesystem in Userspace|FUSE]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| [[GridFTP]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[iSCSI]] || {{Yes}} || ? || ? || ? || ? || {{No}}
|-
| [[WebDAV]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[Network File System|NFS]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[Browser user interface|BUI]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| colspan="7" align="center" | '''System-Wide CDMI Capabilities'''
|-
| cdmi_domains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false}}
|-
| cdmi_export_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_dataobjects || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_export_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_occi_iscsi || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_metadata_maxitems || 1024 || ? || ? || ? || 4096 || 1024
|-
| cdmi_metadata_maxsize || 4096 || ? || ? || ? || 4096 || 4096
|-
| cdmi_metadata_maxtotalsize || &#8734; || ? || ? || ? || 1048576 || 1048576
|-
| cdmi_notification || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_logging || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_regex || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_contains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_tags || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_value || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_queues || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_security_access_control || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_security_audit || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_data_integrity || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_encryption || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_immutability || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_sanitization || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialization_json || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_snapshots || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_references || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_object_move_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_from_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_copy_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_object_copy_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_access_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_post_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_post_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_deserialize_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_dataobject_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_domain_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_container_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_queue_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_create_reference_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Data Object Capabilities'''
|-
| cdmi_read_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}|| {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Container Capabilities'''
|-
| cdmi_list_children || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_list_children_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_snapshot || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_domain || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_create_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_post_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_post_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_create_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_create_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_create_reference || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_occi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_move_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_copy_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_move_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_copy_dataobject" || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Domain Object Capabilities'''
|-
| cdmi_create_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_delete_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_domain_summary || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_domain_members || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_list_children || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Queue Object Capabilities'''
|-
| cdmi_read_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_deserialize_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_move_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_reference_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|}

{{DEFAULTSORT:CDMI server implementation comparison}}
[[Category:Cloud storage]]
[[Category:Data management]]</text>
      <sha1>1rafup6bbybjnu6wnp9dxf08a046o7a</sha1>
    </revision>
  </page>
  <page>
    <title>Data warehouse automation</title>
    <ns>0</ns>
    <id>48752218</id>
    <revision>
      <id>751539838</id>
      <parentid>751372607</parentid>
      <timestamp>2016-11-26T09:59:09Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* General */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3873" xml:space="preserve">'''Data warehouse automation''' or DWA refers to the process of accelerating and automating the [[data warehouse]] development cycles, while assuring quality and consistency. DWA is believed to provide automation of the entire lifecycle of a data warehouse, from source [[Systems analysis|system analysis]] to [[Software testing|testing]] to  [[documentation]]. It helps improve productivity, reduce cost, and improve overall quality.&lt;ref&gt;{{cite web|title=Automate and accelerate your data transformations|url=http://www.attunity.com/products/prepare-data-compose|website=www.attunity.com|publisher=Attunity|accessdate=7 December 2015}}&lt;/ref&gt;

==General==
Data warehouse automation primarily focuses on automation of each and every step involved in the lifecycle of a data warehouse, thus reducing the efforts required in managing it.&lt;ref&gt;{{cite web|title=New Buzzword! Data Warehouse Automation|url=http://blogs.jetreports.com/2015/03/05/new-buzzword-data-warehouse-automation/|website=blogs.jetreports.com|publisher=jetreports|accessdate=7 December 2015}}&lt;/ref&gt;
Data warehouse automation works on the principles of design patterns. It comprises a central repository of design patterns, which encapsulate architectural standards as well as best practices for data design, data management, data integration, and data usage.&lt;ref&gt;{{cite web|url=https://www.wherescape.com/media/1988/data-warehouse-automation-decision-guide.pdf|title=Data Warehouse Automation - A Decision Guide|website=www.wherescape.com|publisher=David L. Wells, Infocentric LLC|accessdate=7 December 2015}}&lt;/ref&gt;
In November 2015, an analyst firm has published a guide ''Which Data Warehouse Automation Tool is Right for You?'' covering four of the leading products in the DWA space.&lt;ref&gt;{{cite web|title=Which Data Warehouse Automation Tool is Right for You?|url=http://eckerson.com/register?content=which-data-warehouse-automation-tool-is-right-for-you|website=eckerson.com|publisher= Wayne Eckerson|accessdate=9 December 2015}}&lt;/ref&gt; In November 2015, an international software and technology services company engaged in developing &#8216;agile tools&#8217; for the data integration industry, was named by CIO Review as one of the 20 most promising productivity tools solution providers 2015 &lt;ref&gt;{{cite web|title=CIO Magazine Award - 20 Most promising productivity tools|url=http://analytixds.com/latest_news/analytix-data-services-wins-cio-reviews-2015/|website=www.analtyixds.com|publisher=AnalytiX DS|accessdate=25 November 2016}}&lt;/ref&gt;

==Benefits==
Data warehouse automation can provide advantages like source data exploration, warehouse data models, ETL generation, test automation, metadata management, managed deployment, scheduling, change impact analysis and easier maintenance and modification of the data warehouse.&lt;ref&gt;{{cite web|title=Data Warehouse Automation (DWA)?|url=http://www.timextender.com/software/data-warehouse-automation/business-value/|website=timextender.com|publisher=TimeXtender Software 2015|accessdate=7 December 2015}}&lt;/ref&gt;
More important than the technical features of DWA tools, however, is the ability to deliver projects faster and with less resources.&lt;ref&gt;{{cite web|title=Deliver Faster|url=http://kalido.com/products/kalido-information-engine/deliver-faster/|website=kalido.com|publisher=Magnitude Software|accessdate=9 December 2015}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* {{Dmoz|Computers/Software/Databases/Data_Warehousing/Data_Warehouse_Automation|Data Warehouse Automation}}
* [http://analytixds.com/wp-content/uploads/2016/07/analytix-data-services-top-20-cio-review.pdf CIO Magazine Award - 20 Most promising productivity tools 'CIO Review', November 10, 2015]

==See also==
*[[Data warehouse]]
*[[Data mart]]
*[[Data warehouse appliance]]
*[[Data integration]]

{{Data_warehouse}}

[[Category:Data management]]
[[Category:Data warehousing]]</text>
      <sha1>m69cq0ecgpq63st6auycrmikgfzorz4</sha1>
    </revision>
  </page>
  <page>
    <title>Astroinformatics</title>
    <ns>0</ns>
    <id>28326718</id>
    <revision>
      <id>748836457</id>
      <parentid>740645884</parentid>
      <timestamp>2016-11-10T17:50:35Z</timestamp>
      <contributor>
        <username>Pleasantville</username>
        <id>3058640</id>
      </contributor>
      <comment>added [[Category:Computational fields of study]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13725" xml:space="preserve">'''Astroinformatics''' is an interdisciplinary field of study involving the combination of [[astronomy]], [[data science]], [[informatics]], and [[Information technology|information]]/[[Communications technologies|communications]] technologies.&lt;ref name="astroinfo" /&gt;&lt;ref name=pdf&gt;[http://www.math.bas.bg/~nkirov/zip/SEEDI_astro_presentation.pdf Astroinformatics and digitization of astronomical heritage], Nikolay Kirov. The fifth SEEDI International Conference Digitization of cultural and scientific heritage, May 19&#8211;20, 2010, Sarajevo. Retrieved 1 November 2012.&lt;/ref&gt;

==Background==

Astroinformatics is primarily focused on developing the tools, methods, and applications of [[computational science]], [[data science]], and [[statistics]] for research and education in data-oriented astronomy.&lt;ref name="astroinfo"&gt;{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy Research and Education|url=http://link.springer.com/article/10.1007%2Fs12145-010-0055-2|website=Journal of Earth Science Informatics, June 2010, Volume 3, Issue 1, pp 5-17|publisher=Springer Link, Netherlands|accessdate=11 January 2016}}&lt;/ref&gt; Early efforts in this direction included [[data discovery]], [[metadata standards]] development, [[data modeling]], astronomical [[data dictionary]] development, [[data access]], [[information retrieval]],&lt;ref&gt;{{cite arXiv|last1=Borne|first1=Kirk|title=Science User Scenarios for a Virtual Observatory Design Reference Mission: Science Requirements for Data Mining|arxiv=astro-ph/0008307}}&lt;/ref&gt; [[data integration]], and [[data mining]]&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Scientific Data Mining in Astronomy|url=https://www.crcpress.com/Next-Generation-of-Data-Mining/Kargupta-Han-Yu-Motwani-Kumar/9781420085860|website=CRC Press, pp. 91-114|publisher=Taylor &amp; Francis Group|accessdate=11 January 2016}}&lt;/ref&gt; in the astronomical [[Virtual Observatory]] initiatives.&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Distributed Data Mining in the National Virtual Observatory|url=http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=764620|website=SPIE Digital Library|publisher=SPIE|accessdate=11 January 2016}}&lt;/ref&gt;&lt;ref name="VOdm" /&gt;&lt;ref&gt;{{cite web|last1=Laurino|first1=O.|title=Astroinformatics of galaxies and quasars: a new general method for photometric redshifts estimation|url=http://mnras.oxfordjournals.org/content/418/4/2165|website=Monthly Notices of the Royal Astronomical Society, vol.418, pp. 2165-2195|publisher=Oxford Journals|accessdate=12 January 2016|display-authors=etal}}&lt;/ref&gt; Further development of the field, along with astronomy community endorsement, was presented to the [[National Research Council (United States)]] in 2009 in the Astroinformatics "State of the Profession" Position Paper for the 2010 [[Astronomy and Astrophysics Decadal Survey]].&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: A 21st Century Approach to Astronomy|url=http://adsabs.harvard.edu/abs/2009astro2010P...6B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}&lt;/ref&gt; That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper '''Astroinformatics: Data-Oriented Astronomy Research and Education'''.&lt;ref name="astroinfo" /&gt;

Astroinformatics as a distinct field of research was inspired by work in the fields of [[Bioinformatics]] and [[Geoinformatics]], and through the [[eScience]] work&lt;ref&gt;{{cite web|title='Online Science'|url=http://research.microsoft.com/en-us/um/people/gray/JimGrayTalks.htm|website=Talks by Jim Gray|publisher=Microsoft Research|accessdate=11 January 2015}}&lt;/ref&gt; of [[Jim Gray (computer scientist)]] at [[Microsoft Research]], whose legacy was remembered and continued through the Jim Gray eScience Awards.&lt;ref&gt;{{cite web|title=Jim Gray eScience Award|url=http://research.microsoft.com/en-us/collaboration/focus/escience/jim-gray-award.aspx|website=Microsoft Research}}&lt;/ref&gt;

Though the primary focus of Astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well&#8212;using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to [[Digital data|digitize]] historical and recent astronomical observations and images in a large [[database]] for efficient retrieval through [[World wide web|web]]-based interfaces.&lt;ref name=pdf/&gt;&lt;ref&gt;[http://www.casca.ca/lrp2010/Docs/LRPReports/astroinformatics_lrp.pdf Astroinformatics in Canada], Nicholas M. Ball, David Schade. Retrieved 1 November 2012.&lt;/ref&gt; Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.&lt;ref&gt;{{cite web|title='Astroinformatics' helps Astronomers explore the sky|url=http://phys.org/news/2013-10-astroinformatics-astronomers-exploring-sky.html|website=Phys.org|publisher=Heidelberg University|accessdate=11 January 2015}}&lt;/ref&gt;

Astroinformatics is described as the '''Fourth Paradigm''' of astronomical research.&lt;ref&gt;{{cite web|title=The Fourth Paradigm: Data-Intensive Scientific Discovery|url=https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/|website=Microsoft Research}}&lt;/ref&gt; There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science.&lt;ref name="VOdm"&gt;{{cite web|last1=Borne|first1=Kirk|title=Virtual Observations, Data Mining, and Astroinformatics|url=http://link.springer.com/referenceworkentry/10.1007/978-94-007-5618-2_9|website=Planets, Stars and Stellar Systems, Volume 2: Astronomical Techniques, Software, and Data, pp.403-443|publisher=Springer Link, Netherlands|accessdate=11 January 2015}}&lt;/ref&gt; [[Data mining]] and [[machine learning]] play significant roles in Astroinformatics as a [[Scientific method|scientific research]] discipline due to their focus on "knowledge discovery from data" ([[data mining|KDD]]) and "learning from data".&lt;ref&gt;{{cite web|last1=Ball|first1=N.M.|last2=Brunner|first2=R.J.|title=Data Mining and Machine Learrning in Astronomy|url=http://www.worldscientific.com/doi/abs/10.1142/S0218271810017160|website=International Journal of Modern Physics D|publisher=World Scientific Publishing|accessdate=12 January 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=The LSST Data Mining Research Agenda|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059074|website=Classification and Discovery in Large Astronomical Surveys, pp.347-351|publisher=American Institute of Physics|accessdate=12 January 2016}}&lt;/ref&gt;

The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the [[Large Synoptic Survey Telescope]] and into the exabytes with the [[Square Kilometre Array]].&lt;ref&gt;{{cite web|last1=Ivezi&#263;|first1=&#381;.|title=Parametrization and Classification of 20 Billion LSST Objects|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059076|website=Classification and Discovery in Large Astronomical Surveys, pp.359-365|publisher=American Institute of Physics|accessdate=12 January 2016|display-authors=etal}}&lt;/ref&gt; This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part, due to this data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing sub-disciplines information and data intensive to an extent that these sub-disciplines are now becoming (or have already become) stand alone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, the most likely will in the near future.

[[Informatics]] has been recently defined as "the use of digital data, information, and related services for research and knowledge generation". However the usual, or commonly used definition is "informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support." Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., [[taxonomy (general)|taxonomies]], [[ontology (information science)|ontologies]], [[folksonomy|folksonomies]], and/or collaborative [[Tag (metadata)|tagging]]&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of the ASIS&amp;T|publisher=American Society for Information Science and Technology|accessdate=11 January 2016}}&lt;/ref&gt;) plus '''[[Astrostatistics]]''' will also be heavily involved. '''[[Citizen science]]''' projects (such as [[Galaxy Zoo]]) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.

In 2012, two position papers&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics in a Nutshell|url=https://asaip.psu.edu/Articles/astroinformatics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Feigelson|first1=Eric|title=Astrostatistics in a Nutshell|url=https://asaip.psu.edu/Articles/astrostatistics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}&lt;/ref&gt; were presented to the Council of the [[American Astronomical Society]] that led to the establishment of formal working groups in Astroinformatics and [[Astrostatistics]] for the profession of [[astronomy]] within the USA and elsewhere.&lt;ref&gt;{{cite arXiv|last1=Feigelson|first1=E.|last2=Ivezi&#263;|first2=&#381;.|last3=Hilbe|first3=J.|last4=Borne|first4=K.|title=New Organizations to Support Astroinformatics and Astrostatistics|arxiv=1301.3069}}&lt;/ref&gt;

Astroinformatics provides a natural context for the integration of education and research.&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=The Revolution in Astronomy Education: Data Science for the Masses|url=http://adsabs.harvard.edu/abs/2009astro2010P...7B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}&lt;/ref&gt; The experience of research can now be implemented within the classroom to establish and grow '''[[Data Literacy]]''' through the easy re-use of data.&lt;ref&gt;{{cite web|title=Using Data in the Classroom|url=http://serc.carleton.edu/usingdata/index.html|website=Science Education Resource Center at Carleton College|publisher=National Science Digital Library|accessdate=11 January 2016}}&lt;/ref&gt; It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.&lt;ref&gt;{{cite book|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy|location=George Mason University, USA|url=http://www.iccs-meeting.org/iccs2009/PosterPapers/Poster-paper18.pdf|accessdate=January 21, 2015}}&lt;/ref&gt;

==Conferences==

{| class="wikitable"
|-
! Year
! Place
! Link
|-
| 2016
| [[Sorrento]], [[Italy]]
| [http://www.iau.org/science/meetings/future/symposia/1158/]
|-
| 2015
| [[Dubrovnik]], [[Dalmatia]]
| [http://iszd.hr/AstroInfo2015/]
|-
| 2014
| [[University of Chile]]
| [http://eventos.cmm.uchile.cl/astro2014/]
|-
| 2013
| [[Australia Telescope National Facility]], [[CSIRO]]
| [http://www.atnf.csiro.au/research/workshops/2013/astroinformatics/]
|-
| 2012
| [[Microsoft Research]]
| [http://www.astro.caltech.edu/ai12/]
|-
| 2011
| [[Sorrento]], [[Italy]]
| [http://dame.dsf.unina.it/astroinformatics2011.html]
|-
| 2010
| [[Caltech]]
| [http://www.astro.caltech.edu/ai10/]
|}

==See also==

*''[[Astronomy and Computing]]''
*[[Astrophysics Data System]]
*[[Astrophysics Source Code Library]]
*[[Astrostatistics]]
*[[Galaxy Zoo]]
*[[International Astrostatistics Association]]
*[[International Virtual Observatory Alliance]] (IVOA)
*[[MilkyWay@home]]
*[[Virtual Observatory]]
*[[WorldWide Telescope]]
*[[Zooniverse (citizen science project)|Zooniverse]]

== External links ==

* [http://www.adass.org/ Astronomical Data Analysis Software and Systems] (ADASS)
* [https://asaip.psu.edu/ Astrostatistics and Astroinformatics Portal]
* [https://asaip.psu.edu/organizations/iaa/iaa-working-group-of-cosmostatistics/ Cosmostatistics Initiative] (COIN)
* [http://www.iau.org/science/scientific_bodies/commissions/B3/ Astroinformatics and Astrostatistics Commission of the International Astronomical Union]

==References==
{{reflist}}

[[Category:Astronomy]]
[[Category:Astrophysics]]
[[Category:Big data]]
[[Category:Computational astronomy]]
[[Category:Data management]]
[[Category:Information science by discipline]]
[[Category:Applied statistics]]
[[Category:Computational fields of study]]</text>
      <sha1>ado2tm0fispvol59sdb5pztsgsavfql</sha1>
    </revision>
  </page>
  <page>
    <title>GB &amp; Smith</title>
    <ns>0</ns>
    <id>48206970</id>
    <revision>
      <id>751710808</id>
      <parentid>751710679</parentid>
      <timestamp>2016-11-27T11:36:28Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>link and remove what smells like advertising</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7921" xml:space="preserve">{{ad|date=August 2016}}
{{Orphan|date=April 2016}}
{{Use dmy dates|date=September 2016}}
{{Infobox company
| name = GB &amp; Smith
| logo = GB &amp; Smith.png
| caption =
| type = Private
| traded_as =
| successor =
| foundation = 2007
| founder =  Sebastien Goiffon and Alexandre Biegala
| defunct =
| location_city =
| location_country =
| location =
| locations =
| area_served =
| key_people =
| industry = Software
| products =
| services =
| revenue =
| operating_income =
| net_income =
| owner =
| num_employees = 70
| parent =
| divisions =
| subsid =
| homepage = {{url|gbandsmith.com}}
| footnotes =
| intl =
}}

'''GB &amp; Smith''' is an independent [[software]] editor which provides layer independent matrix-based console allowing instant visual review on any supported [[computing platform]].

== History ==

GB &amp; Smith was founded in 2007 by Sebastien Goiffon and Alexandre Biegala.&lt;ref&gt;{{cite web|title=Price Entrepreneur of the Year 2014: Winners North region|url=http://business.lesechos.fr/entrepreneurs/success-stories/prix-de-l-entrepreneur-de-l-annee-2014-les-laureats-region-nord-103724.php|publisher=[[Les &#201;chos (newspaper)|Les &#201;chos]]|accessdate=20 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=S'implanter &#224; Londres ? Les cl&#233;s du succ&#232;s selon GB &amp; Smith|url=http://www.lesechos.fr/02/04/2015/lesechos.fr/0204265266931_s-implanter-a-londres---les-cles-du-succes-selon-gb---smith.htm|publisher=[[Les &#201;chos (newspaper)|Les &#201;chos]]|accessdate=19 March 2016}}&lt;/ref&gt;

As of 2016 company is becoming agnostic gradually offering it is security administration solutions for [[Microsoft]], [[Oracle Corporation|Oracle]], [[IBM]], [[Tableau Software|Tableau]].&lt;ref name=les&gt;{{cite web|title=GB &amp; Smith secures corporate data|url=http://www.lesechos.fr/26/12/2013/LesEchos/21592-075-ECH_gb---smith-securise-les-donnees-d-entreprise.htm|publisher=[[Les &#201;chos (newspaper)|Les &#201;chos]]|accessdate=20 March 2016}}&lt;/ref&gt;

Alexandre Biegala and Sebastien Goiffon invented a method around [[identity access management]] (IAM) to easily review and administer security rights of various applications and over multiple technologies through a single User interface.&lt;ref name=les /&gt;&lt;ref&gt;{{cite web|title=GB &amp; Smith Lille : + 761% de croissance en cinq ans!|url=http://www.lavoixdunord.fr/economie/gb-smith-lille-761-de-croissance-en-cinq-ans-ia0b0n1806557|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}&lt;/ref&gt;

GbandSmith was granted a patent for a solution on security administration of rights and has become the Security Administration company and known to be a pioneer in Administration Intelligence and real Self-service security administration.&lt;ref&gt;{{cite web|title=US Patent Issued to GB &amp; Smith on Feb. 10 for "Matrix Security Management System for Managing User Accounts and Security Settings"|url=https://www.highbeam.com/doc/1P3-3585409721.html|website=Highbeam.com|accessdate=19 March 2016}}&lt;/ref&gt;

[[BusinessObjects]] co-founder Denis Payre joined GB &amp; Smith on 1 April 2016. In 1996, Denis Payre and his partner, Bernard Liautaud were ranked by ''Business Week'' among the "Best Entrepreneurs", alongside [[Steve Jobs]] and [[Steven Spielberg]].&lt;ref&gt;{{cite web|title=GB&amp;SMITH Announces Denis Payre, Co-Founder of Business Objects, to Join its Board of Directors|url=http://www.bizjournals.com/prnewswire/press_releases/2016/04/01/NE60812|website=[[The Business Journals]]|accessdate=1 April 2016}}&lt;/ref&gt;

==Solutions==

=== 365Suite Agnostic Self-Service Security Administration ===
365Suite is a set of agnostic tools solutions focused on Security administrations such as access rights management, security policy audits and related metadata. 365Suite enables centralizing security administration into a single console managing multiple applications. 365 runs on top of solutions such as Microsoft [[SharePoint]], [[Microsoft Active Directory]], SAP [[SAP BusinessObjects|BO]], [[SAP HANA|Hana]], Oracle [[Oracle Business Intelligence Suite Enterprise Edition|OBIEE,]] [[Oracle Database|ODB]], [[Tableau Software|Tableau]], etc.&lt;ref&gt;{{cite web|title=Le Comparateur assurance remporte le premier prix du Fast50 avec une croissance de + 1 562% en 4 ans|url=http://www.lavoixdunord.fr/economie/le-comparateur-assurance-remporte-le-premier-prix-du-fast50-ia0b0n3165825|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}&lt;/ref&gt;

365 solutions consists in two solutions:
* 365View: Single security administration console to operate multiple IT solutions simultaneously (sharepoint, Oarcle BI).
* 365Eyes: Centralized Metadata repository focused on security administration with ability to operate, monitor, restore and compare metadata from multiple IT solutions.

=== 360Suite ===

360Suite consists in a suite of eight solutions focused around [[SAP SE|SAP]] BusinessObjects:  
* 360Plus: Backup, incremental backup,Promotion, including ability to restore deleted files.&lt;ref&gt;{{cite web|title=Le Fran&#231;ais GB &amp; Smith invente le concept prometteur d'administration intelligence|url=http://www.channelnews.fr/le-francais-gb-a-smith-invente-le-concept-prometteur-dadministration-intelligence-21842|publisher=ChannelNews|accessdate=19 March 2016}}&lt;/ref&gt;
* 360View: Security administration, via a security matrix crossing Ressources and Users, Bulk updates (UNV to UNX, unbounded documents)&lt;ref&gt;{{cite web|title=GB &amp; Smith, un esprit de conqu&#234;te et un esprit libre|url=http://www.lopinion.fr/2-decembre-2014/gb-smith-esprit-conquete-esprit-libre-18979|publisher=[[L'Opinion (newspaper)|L'Opinion]]|accessdate=19 March 2016}}&lt;/ref&gt;
* 360Cast: Schedule and burst dynamically reports.&lt;ref&gt;{{cite web|title=La Soci&#233;t&#233; Ugitech Choisit Les Solutions 360suite De Gb &amp; Smith Pour Administrer Sa Plateforme Sap Businessobjects Bi 4.0|url=http://www.decideo.fr/La-Societe-UGITECH-choisit-les-solutions-360suite-de-GB-SMITH-pour-administrer-sa-plateforme-SAP-BusinessObjects-BI-4-0_a6323.html|publisher=Decideo|accessdate=19 March 2016}}&lt;/ref&gt;
* 360Eyes: Explore and analyze BO [[metadata]] and perform impact analysis.
* 360Eyes compliance: Compliance to ensure BO compliance.
* 360Vers: Facilitate and monitor BO versioning.
* 360Bind: Automate BO Non regression tests. With ability to compare results and pixels from Webi, Deski and Crystal reports.
* 360Init: Initialize and import your BO security.

== Recognition ==

* 2013-15 [[Deloitte Fast 500#Fast 500 EMEA|Deloitte EMEA technology Fast 500]].&lt;ref&gt;{{cite web|title=Technology Fast 500 EMEA 2013 Ranking|url=http://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/dttl_TMT-Event-Fast-500-2013-winners-ranking.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=2015 Technology Fast 500TM Europe, Middle East &amp; Africa (EMEA) Ranking|url=https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/gx-deloitte-tmt-emea-fast500-2015-rankings.pdf|publisher=[[Deloitte Fast 500]]|accessdate=19 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Technology Fast 50|url=http://www2.deloitte.com/content/dam/Deloitte/fr/Documents/technology%20fast%2050/Deloitte_Palmar%C3%A8s-Fast50_2014.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}&lt;/ref&gt;
* 2014 [[Ernst &amp; Young]] emerging company.&lt;ref&gt;{{cite web|title=S&#233;bastien GOIFFON et Alexandre BIEGALA re&#231;oivent le Prix de l'Entreprise d'Avenir de l'Ann&#233;e 2014 pour le Nord de France|url=http://blog.gbandsmith.com/wp-content/uploads/2014/10/cp_resultats_ceremonie_nord_de_france_2014_gbs2_0.pdf|publisher=[[Ey.com]]|accessdate=19 March 2016}}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
*{{Official website|http://www.gbandsmith.com}}

{{DEFAULTSORT:GB and Smith}}
[[Category:Computer access control]]
[[Category:Business intelligence companies]]
[[Category:Identity management]]
[[Category:Data analysis software]]
[[Category:Data management]]</text>
      <sha1>dnalpre5swuup8pz0grt1k1jukbl8hn</sha1>
    </revision>
  </page>
  <page>
    <title>NoSQL</title>
    <ns>0</ns>
    <id>23968131</id>
    <revision>
      <id>762054203</id>
      <parentid>762054101</parentid>
      <timestamp>2017-01-26T10:16:07Z</timestamp>
      <contributor>
        <ip>212.56.218.234</ip>
      </contributor>
      <comment>Undid revision 762054101 by [[Special:Contributions/212.56.218.234|212.56.218.234]] ([[User talk:212.56.218.234|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27391" xml:space="preserve">{{Redirect|Structured storage|the Microsoft technology also known as structured storage|COM Structured Storage}}
A '''NoSQL''' (originally referring to "non SQL", "non relational" or "not only SQL")&lt;ref&gt;http://nosql-database.org/ "NoSQL DEFINITION: Next Generation Databases mostly addressing some of the points: being non-relational, distributed, open-source and horizontally scalable"&lt;/ref&gt; database provides a mechanism for [[Computer data storage|storage]] and [[data retrieval|retrieval]] of data which is modeled in means other than the tabular relations used in [[relational database]]s. Such databases have existed since the late 1960s, but did not obtain the "NoSQL" moniker until a surge of popularity in the early twenty-first century,{{r|leavitt}} triggered by the needs of [[Web 2.0]] companies such as [[Facebook]], [[Google]], and [[Amazon.com]].&lt;ref&gt;{{cite conference |title=History Repeats Itself: Sensible and NonsenSQL Aspects of the NoSQL Hoopla |first=C. |last=Mohan |conference=Proc. 16th Int'l Conf. on Extending Database Technology |year=2013 |url=http://openproceedings.eu/2013/conf/edbt/Mohan13.pdf}}&lt;/ref&gt;&lt;ref&gt;http://www.eventbrite.com/e/nosql-meetup-tickets-341739151 "Dynamo clones and BigTables"&lt;/ref&gt;&lt;ref&gt;http://www.wired.com/2012/01/amazon-dynamodb/ "Amazon helped start the &#8220;NoSQL&#8221; movement."&lt;/ref&gt; NoSQL databases are increasingly used in [[big data]] and [[real-time web]] applications.&lt;ref&gt;{{cite web|url= http://db-engines.com/en/blog_post/23 |title= RDBMS dominate the database market, but NoSQL systems are catching up |publisher= DB-Engines.com |date= 21 Nov 2013 |accessdate= 24 Nov 2013 }}&lt;/ref&gt;   NoSQL systems are also sometimes called "Not only SQL" to emphasize that they may support [[SQL]]-like query languages.&lt;ref&gt;{{cite web|url=http://searchdatamanagement.techtarget.com/definition/NoSQL-Not-Only-SQL|title=NoSQL (Not Only SQL)|quote=NoSQL database, also called Not Only SQL}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url = http://martinfowler.com/bliki/NosqlDefinition.html | title = NosqlDefinition | first = Martin | last = Fowler | authorlink = Martin Fowler | quote = many advocates of NoSQL say that it does not mean a "no" to SQL, rather it means Not Only SQL }}&lt;/ref&gt;

Motivations for this approach include: simplicity of design, simpler [[Horizontal scaling#Horizontal and vertical scaling|"horizontal" scaling]] to [[cluster computing|clusters]] of machines  (which is a problem for relational databases),&lt;ref name="leavitt"&gt;{{cite journal |first=Neal |last=Leavitt |title=Will NoSQL Databases Live Up to Their Promise? |journal=[[IEEE Computer]] |year=2010 |url=http://www.leavcom.com/pdf/NoSQL.pdf}}&lt;/ref&gt; and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as "more flexible" than relational database tables.&lt;ref&gt;http://www.allthingsdistributed.com/2012/01/amazon-dynamodb.html "Customers like SimpleDB&#8217;s table interface and its flexible data model. Not having to update their schemas when their systems evolve makes life much easier"&lt;/ref&gt;

Many NoSQL stores compromise [[consistency (database systems)|consistency]] (in the sense of the [[CAP theorem]]) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.&lt;ref&gt;{{cite web
| url         = http://www.journalofcloudcomputing.com/content/pdf/2192-113X-2-22.pdf
| title       = Data management in cloud environments: NoSQL and NewSQL data stores
| first1 = K. | last1 = Grolinger | first2 = W. A. | last2 = Higashino | first3 = A. | last3 = Tiwari | first4 = M. A. M. | last4 = Capretz
| date = 2013
| publisher   = Aira, Springer
| accessdate  = 8 Jan 2014
}}
&lt;/ref&gt;
Most NoSQL stores lack true [[ACID]] transactions, although a few databases, such as [[MarkLogic]], [[Aerospike database|Aerospike]], FairCom [[c-treeACE]], Google [[Spanner (database)|Spanner]] (though technically a [[NewSQL]] database), Symas [[Lightning Memory-Mapped Database|LMDB]], and [[OrientDB]] have made them central to their designs. (See [[#ACID and JOIN Support|ACID and JOIN Support]].)

Instead, most NoSQL databases offer a concept of "eventual consistency" in which database changes are propagated to all nodes "eventually" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.&lt;ref&gt;https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads&lt;/ref&gt;  Additionally, some NoSQL systems may exhibit lost writes and other forms of [[data loss]].&lt;ref&gt;Martin Zapletal: Large volume data analysis on the Typesafe Reactive Platform, ScalaDays 2015, [http://www.slideshare.net/MartinZapletal/zapletal-martinlargevolumedataanalytics Slides]&lt;/ref&gt; Fortunately, some NoSQL systems provide concepts such as [[write-ahead logging]] to avoid data loss.&lt;ref&gt;http://www.dummies.com/how-to/content/10-nosql-misconceptions.html "NoSQL databases lose data" section&lt;/ref&gt; For [[distributed transaction processing]] across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases "do not allow referential integrity constraints to span databases."&lt;ref&gt;https://iggyfernandez.wordpress.com/2013/07/28/no-to-sql-and-no-to-nosql/&lt;/ref&gt; There are few systems that maintain both [[ACID]] transactions and [[X/Open XA]] standards for distributed transaction processing.

== History ==
The term ''NoSQL'' was used by Carlo Strozzi in 1998 to name his lightweight, [[Strozzi NoSQL (RDBMS)|Strozzi NoSQL open-source relational database]] that did not expose the standard [[SQL|Structured Query Language]] (SQL) interface, but was still relational.&lt;ref name=":0"&gt;{{cite web
| url         = http://publications.lib.chalmers.se/records/fulltext/123839.pdf
| title       = Investigating storage solutions for large data: A comparison of well performing and scalable data storage solutions for real time extraction and batch insertion of data
| first       = Adam
| last        = Lith
| first2 = Jakob | last2 = Mattson
| date        = 2010
| publisher   = Department of Computer Science and Engineering, Chalmers University of Technology
| location    = G&#246;teborg
| page        = 70
| accessdate  = 12 May 2011
| quote       = Carlo Strozzi first used the term NoSQL in 1998 as a name for his open source relational database that did not offer a SQL interface[...]
}}
&lt;/ref&gt;  His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases.  Strozzi suggests that, because the current NoSQL movement "departs from the relational model altogether, it should therefore have been called more appropriately 'NoREL'",&lt;ref&gt;{{cite web|url=http://www.strozzi.it/cgi-bin/CSA/tw7/I/en_US/nosql/Home%20Page |title=NoSQL Relational Database Management System: Home Page |publisher=Strozzi.it |date=2 October 2007 |accessdate=29 March 2010}}&lt;/ref&gt; referring to 'No Relational'.

Johan Oskarsson of [[Last.fm]] reintroduced the term ''NoSQL'' in early 2009 when he organized an event to discuss "open source [[distributed database|distributed, non relational databases]]".&lt;ref&gt;{{cite web|url= http://blog.sym-link.com/2009/05/12/nosql_2009.html |title= NoSQL 2009 |publisher= Blog.sym-link.com |date= 12 May 2009 |accessdate= 29 March 2010 }}&lt;/ref&gt; The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google's BigTable/MapReduce and Amazon's Dynamo. Most of the early NoSQL systems did not attempt to provide [[ACID|atomicity, consistency, isolation and durability]] guarantees, contrary to the prevailing practice among relational database systems.&lt;ref&gt;{{cite web|url= http://databases.about.com/od/specificproducts/a/acid.htm |title= The ACID Model| first = Mike | last = Chapple }}&lt;/ref&gt;

Based on 2014 revenue, the NoSQL market leaders are [[MarkLogic]], [[MongoDB]], and [[Datastax]].&lt;ref&gt;{{cite web|accessdate=2015-11-17|url=http://wikibon.com/hadoop-nosql-software-and-services-market-forecast-2013-2017/|title=Hadoop-NoSQL-rankings}}&lt;/ref&gt; Based on 2015 popularity rankings, the most popular NoSQL databases are [[MongoDB]], [[Apache Cassandra]], and [[Redis]].&lt;ref&gt;{{cite web|accessdate=2015-07-31|url=http://db-engines.com/en/ranking|title=DB-Engines Ranking}}&lt;/ref&gt;

== Types and examples of NoSQL databases ==
There have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:
* '''[[Column (data store)|Column]]''': [[Accumulo]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[HBase]], [[Vertica]], [[SAP HANA]]
* '''[[Document-oriented database|Document]]''': [[Apache CouchDB]], [[Clusterpoint]], [[Couchbase]], [[DocumentDB]], [[HyperDex]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[OrientDB]], [[Qizx]], [[RethinkDB]]
* '''[[Key-value store|Key-value]]''': [[Aerospike database|Aerospike]], [[Couchbase]], [[Dynamo (storage system)|Dynamo]], FairCom [[c-treeACE]], [[FoundationDB]], [[HyperDex]], [[MemcacheDB]], [[MUMPS]], [[Oracle NoSQL Database]], [[OrientDB]], [[Redis]], [[Riak]], [[Berkeley DB]]
* '''[[Graph database|Graph]]''': [[AllegroGraph]], ArangoDB, [[InfiniteGraph]], [[Apache Giraph]], [[MarkLogic]], [[Neo4J]], [[OrientDB]], [[Virtuoso Universal Server|Virtuoso]], [[Stardog]]
* '''[[Multi-model database|Multi-model]]''': Alchemy Database, ArangoDB, CortexDB, [[Couchbase]], [[FoundationDB]], [[MarkLogic]], [[OrientDB]]

A more detailed classification is the following, based on one from Stephen Yen:&lt;ref&gt;{{cite web|url=https://dl.dropboxusercontent.com/u/2075876/nosql-steve-yen.pdf|format=PDF|title=NoSQL is a Horseless Carriage|last=Yen|first=Stephen|publisher=NorthScale|accessdate=2014-06-26}}.&lt;/ref&gt;

{| style="text-align: left;" class="wikitable sortable"
|-
! Type !! Examples of this type
|-
| Key-Value Cache || [[Oracle Coherence|Coherence]], [[IBM WebSphere eXtreme Scale|eXtreme Scale]], [[GigaSpaces]],  GemFire, [[Hazelcast]], [[Infinispan]], JBoss Cache, [[Memcached]], Repcached, [[Terracotta, Inc.|Terracotta]], [[Velocity (memory cache)|Velocity]]
|-
| Key-Value Store || Flare, Keyspace, RAMCloud, SchemaFree, [[Hyperdex]], [[Aerospike database|Aerospike]]
|-
| Key-Value Store (Eventually-Consistent) || DovetailDB, [[Oracle NoSQL Database]], [[Dynamo (storage system)|Dynamo]], [[Riak]], Dynomite, MotionDb, [[Voldemort (distributed data store)|Voldemort]], SubRecord
|-
| Key-Value Store (Ordered) || Actord, [[FoundationDB]], Lightcloud, [[Lightning Memory-Mapped Database|LMDB]], Luxio, [[MemcacheDB]],  NMDB, Scalaris, TokyoTyrant
|-
| Data-Structures Server || [[Redis]]
|-
| Tuple Store || [[Jini|Apache River]], Coord, [[GigaSpaces]]
|-
| Object Database || DB4O, [[Objectivity/DB]], [[Perst]], Shoal, [[Zope Object Database|ZopeDB]]
|-
| Document Store || [[Clusterpoint]], [[Couchbase]], [[CouchDB]], [[DocumentDB]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[Qizx]], [[RethinkDB]], [[XML database|XML-databases]]
|-
| [[Wide column store|Wide Column Store]] || [[BigTable]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[Apache HBase|HBase]], [[Hypertable]], KAI, KDI, OpenNeptune, Qbase
|}

[[Correlation database]]s are model-independent, and instead of row-based or column-based storage, use value-based storage.

=== Key-value store ===
{{main|Key-value database}}
Key-value (KV) stores use the [[associative array]] (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.&lt;ref&gt;{{cite web
| accessdate =1 January 2012
| publisher = Stackexchange
| location = http://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database
| title = Key Value stores and the NoSQL movement
| author = Sandy
| date = 14 January 2011
| url = http://dba.stackexchange.com/a/619
| quote = Key-value stores allow the application developer to store schema-less data. This data usually consists of a string that represents the key, and the actual data that is considered the value in the "key-value" relationship. The data itself is usually some kind of primitive of the programming language (a string, an integer, or an array) or an object that is being marshaled by the programming language's bindings to the key-value store. This structure replaces the need for a fixed data model and allows proper formatting.}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| accessdate =1 January 2012
| publisher = Marc Seeger
| location = http://blog.marc-seeger.de/2009/09/21/key-value-stores-a-practical-overview/
| title = Key-Value Stores: a practical overview
| first = Marc | last = Seeger
| date = 21 September 2009
| url = http://blog.marc-seeger.de/assets/papers/Ultra_Large_Sites_SS09-Seeger_Key_Value_Stores.pdf
| quote = Key-value stores provide a high-performance alternative to relational database systems with respect to storing and accessing data. This paper provides a short overview of some of the currently available key-value stores and their interface to the Ruby programming language.}}&lt;/ref&gt;

The key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in [[Lexicographical order|lexicographic order]]. This extension is computationally powerful, in that it can efficiently retrieve selective key ''ranges''.&lt;ref&gt;{{cite web
| accessdate =8 May 2014
| publisher = Ilya Katsov
| title = NoSQL Data Modeling Techniques 
| first = Ilya | last = Katsov
| date = 1 March 2012
| url = http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/}}&lt;/ref&gt;

Key-value stores can use [[consistency model]]s ranging from [[eventual consistency]] to [[serializability]]. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ [[solid-state drive]]s or [[hard disk drive|rotating disks]].

Examples include [[Oracle NoSQL Database]], [[Redis]], and [[dbm]].

=== Document store ===
{{main|Document-oriented database|XML database}}
The central concept of a document store is the notion of a "document". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, [[YAML]], and [[JSON]] as well as binary forms like [[BSON]].  Documents are addressed in the database via a unique ''key'' that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents.

Different implementations offer different ways of organizing and/or grouping documents:
* Collections
* Tags
* Non-visible metadata
* Directory hierarchies

Compared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.

=== Graph ===
{{main|Graph database}}

This kind of database is designed for data whose relations are well represented as a [[graph (discrete mathematics)|graph]] consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.

; Graph databases and their query language
{| style="text-align: left;" class="wikitable sortable"
 ! Name !! Language(s) !! Notes
 |-
 | [[AllegroGraph]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store
 |-
 | [[DEX (Graph database)|DEX/Sparksee]] || [[C++]], [[Java (programming language)|Java]], [[.NET Framework|.NET]], [[Python (programming language)|Python]] || [[Graph database]]
 |-
 | [[FlockDB]] || [[Scala (programming language)|Scala]] || [[Graph database]]
 |-
 | [[IBM DB2]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store added in DB2 10
 |-
 | [[InfiniteGraph]] || [[Java (programming language)|Java]] || [[Graph database]]
 |-
 | [[MarkLogic]] || [[Java (programming language)|Java]], [[JavaScript]], [[SPARQL]], [[XQuery]] || Multi-model [[Document-oriented database|document database]] and [[Resource Description Framework|RDF]] triple store
 |-
 | [[Neo4j]] || [[Cypher Query Language|Cypher]] || [[Graph database]]
 |-
 | [[Ontotext|OWLIM]] || [[Java (programming language)|Java]], [[SPARQL|SPARQL 1.1]]|| [[Resource Description Framework|RDF]] triple store
 |-
 |-
 | [[Oracle Database|Oracle]] || [[SPARQL|SPARQL 1.1]] || [[Resource Description Framework|RDF]] triple store added in 11g
 |-
 | [[OrientDB]] || [[Java (programming language)|Java]], SQL || Multi-model [[Document-oriented database|document]] and [[graph database]]
 |-
 | [[sqrrl|Sqrrl Enterprise]] || [[Java (programming language)|Java]] || [[Graph database]]
 |-
 | [[Virtuoso Universal Server|OpenLink Virtuoso]] || [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]] || [[Middleware]] and [[database engine]] hybrid
 |-
 | [[Stardog]] || [[Java (programming language)|Java]], [[SPARQL]] || [[Graph database]]
|}

=== Object database ===
{{main|Object database}}
* [[db4o]]
* [[Gemstone (database)|GemStone/S]]
* [[InterSystems Cach&#233;]]
* [[JADE (programming language)|JADE]]
* [[ObjectDatabase++]]
* [[ObjectDB]]
* [[Objectivity/DB]]
* [[ObjectStore]]
* [[Odaba|ODABA]]
* [[Perst]]
* [[Virtuoso Universal Server|OpenLink Virtuoso]]
* [[Versant Object Database]]
* [[ZODB]]

=== Tabular ===
* [[Apache Accumulo]]
* [[BigTable]]
* [[HBase|Apache Hbase]]
* [[Hypertable]]
* [[Mnesia]]
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Tuple store ===
* [[Apache River]]
* [[GigaSpaces]]
* [[Tarantool]]
* [[TIBCO Software|TIBCO]] ActiveSpaces
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Triple/quad store (RDF) database ===
{{main|Triplestore|Named graph}}
* [[AllegroGraph]]
* [[Jena (framework)|Apache JENA]] (It is a framework, not a database)
* [[MarkLogic]]
* [[Ontotext|Ontotext-OWLIM]]
* [[Oracle NoSQL Database|Oracle NoSQL database]]
* [[Virtuoso Universal Server]]
* [[Stardog]]

=== Hosted ===
* [[Amazon DynamoDB]]
* [[Amazon SimpleDB]]
* [[Appengine|Datastore on Google Appengine]]
* [[Clusterpoint|Clusterpoint database]]
* [[Cloudant|Cloudant Data Layer (CouchDB)]]
* [[Freebase (database)|Freebase]]
* [[Microsoft Azure#Table Service|Microsoft Azure Tables]]&lt;ref&gt;http://azure.microsoft.com/en-gb/services/storage/tables/&lt;/ref&gt;
* [[DocumentDB|Microsoft Azure DocumentDB]]&lt;ref&gt;http://azure.microsoft.com/en-gb/services/documentdb/&lt;/ref&gt;
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Multivalue databases ===
* D3 [[Pick database]]
* [[Extensible Storage Engine]] (ESE/NT)
* [[InfinityDB]]
* [[InterSystems Cach&#233;]]
* jBASE [[Pick database]]
* [[Northgate Information Solutions]] Reality, the original Pick/MV Database
* [[OpenQM]]
* Revelation Software's [[OpenInsight]]
* [[Rocket U2]]

=== Multimodel database ===

* [[Couchbase]]
* [[FoundationDB]]
* [[MarkLogic]]
* [[OrientDB]]

== Performance ==
Ben Scofield rated different categories of NoSQL databases as follows:&lt;ref&gt;{{cite web|url=http://www.slideshare.net/bscofield/nosql-codemash-2010|title=NoSQL - Death to Relational Databases(?)|last=Scofield|first=Ben |date=2010-01-14|accessdate=2014-06-26}}&lt;/ref&gt;

{| style="text-align: left;" class="wikitable sortable"
|-
! Data Model !! Performance !! Scalability !! Flexibility !! Complexity !! Functionality
|-
| Key&#8211;Value Store ||  high || high || high || none || variable (none)
|-
| Column-Oriented Store || high || high || moderate || low || minimal
|-
| Document-Oriented Store || high || variable (high) || high || low || variable (low)
|-
| Graph Database || variable || variable || high || high || [[graph theory]]
|-
| Relational Database || variable || variable || low || moderate || [[relational algebra]]
|}

Performance and scalability comparisons are sometimes done with the [[YCSB]] benchmark.

{{see also|Comparison of structured storage software}}

== Handling relational data ==
Since most NoSQL databases lack ability for joins in queries, the [[database schema]] generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)

=== Multiple queries ===
Instead of retrieving all the data with one query, it's common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.

=== Caching/replication/non-normalized data ===
Instead of only storing foreign keys, it's common to store actual foreign values along with the model's data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.&lt;ref name="DataModeling-Couchbase.com_December_5_2014c"&gt;{{cite web |url=http://www.couchbase.com/sites/default/files/uploads/all/whitepapers/Couchbase_Whitepaper_Transitioning_Relational_to_NoSQL.pdf |title=Making the Shift from Relational to NoSQL
 |newspaper=Couchbase.com |accessdate= December 5, 2014}}&lt;/ref&gt;

=== Nesting data ===
With document databases like MongoDB it's common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.

== ACID and JOIN Support ==

If a database is marked as supporting [[ACID]] or [[Join (SQL)|joins]], then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess.
{| class="wikitable"
|-
! Database !! ACID !! Joins
|-
| [[Aerospike (company)|Aerospike]] || {{Yes}} || {{No}}
|-
| ArangoDB || {{Yes}} || {{Yes}}
|-
| [[CouchDB]] || {{Yes}} || {{Yes}}
|-
| [[c-treeACE]] || {{Yes}} || {{Yes}}
|-
| [[HyperDex]] || {{Yes}}{{refn|name=HyperDexAcid|group=nb|HyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.}} || {{Yes}}
|-
| [[InfinityDB]] || {{Yes}} || {{No}}
|-
| [[Lightning Memory-Mapped Database|LMDB]] || {{Yes}} || {{No}}
|-
| [[MarkLogic]] || {{Yes}} || {{Yes}}{{refn|name=MarkLogicJoins|group=nb|Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.&lt;ref&gt;http://www.gennet.com/big-data/cant-joins-marklogic-just-matter-semantics/&lt;/ref&gt;}}
|-
| [[OrientDB]] || {{Yes}} || {{Yes}}

|}

{{reflist|group=nb}}

== See also ==
&lt;!-- please do not list specific implementations here --&gt;
* [[CAP theorem]]
* [[Comparison of object database management systems]]
* [[Comparison of structured storage software]]
* [[Correlation database]]
* [[Distributed cache]]
* [[Faceted search]]
* [[MultiValue]] database
* [[Multi-model database]]
* [[Triplestore]]
* [[Schema-agnostic databases]]

== References ==
{{Reflist|33em}}

== Further reading ==
*{{cite book
 | first1 = Pramod | last1 = Sadalage | first2 = Martin | last2 = Fowler | authorlink2 = Martin Fowler
 | date = 2012
 | title = NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence
 | publisher = Addison-Wesley
 | isbn = 0-321-82662-0
}}
*{{cite book
 | first1 = Dan | last1 = McCreary | first2 = Ann | last2 =Kelly
 | date = 2013
 | title = Making Sense of NoSQL: A guide for managers and the rest of us
 | isbn = 9781617291074
}}
*{{cite book
 | first1 = Lena | last1 = Wiese 
 | date = 2015
 | title =  Advanced Data Management for SQL, NoSQL, Cloud and Distributed Databases 
 | publisher = DeGruyter/Oldenbourg
 | isbn = 978-3-11-044140-6
}}
* {{cite web| first = Christof | last = Strauch | date = 2012|title=NoSQL Databases|url=http://www.christof-strauch.de/nosqldbs.pdf}}
* {{cite journal| last1 = Moniruzzaman | first1 = A. B. | last2 = Hossain | first2 = S. A. | date = 2013|title=NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison|arxiv=1307.0191}}
* {{cite journal| first = Kai | last = Orend | date = 2013|title=Analysis and Classification of NoSQL Databases and Evaluation of their Ability to Replace an Object-relational Persistence Layer|citeseerx = 10.1.1.184.483 }}
* {{cite web| first1 = Ganesh | last1 = Krishnan | first2 = Sarang | last2 = Kulkarni | first3 = Dharmesh Kirit | last3 = Dadbhawala | title=Method and system for versioned sharing, consolidating and reporting information|url=https://www.google.com/patents/US7383272?pg=PA1&amp;dq=ganesh+krishnan&amp;hl=en&amp;sa=X}}

== External links ==
* {{cite web|url=http://www.christof-strauch.de/nosqldbs.pdf|title=NoSQL whitepaper| first = Christoph | last = Strauch|publisher=Hochschule der Medien|location = Stuttgart}}
* {{cite web|url=http://nosql-database.org/|title=NoSQL database List| first = Stefan | last = Edlich}}
* {{cite web|year=2010|url=http://www.infoq.com/articles/graph-nosql-neo4j|title=Graph Databases, NOSQL and Neo4j| first = Peter | last = Neubauer}}
* {{cite web|year=2012|url=http://www.networkworld.com/article/2160905/tech-primers/a-vendor-independent-comparison-of-nosql-databases--cassandra--hbase--mongodb--riak.html|title=A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak| first = Sergey | last = Bushik|publisher=NetworkWorld}}
* {{cite web|year=2014|url=http://www.odbms.org/category/downloads/nosql-data-stores/nosql-data-stores-articles/|title=NoSQL Data Stores &#8211; Articles, Papers, Presentations|first = Roberto V. | last = Zicari|website=odbms.org}}
{{Use dmy dates|date=February 2012}}
{{Databases}}



[[Category:NoSQL| ]]
[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Structured storage]]</text>
      <sha1>tahrawjhozu8h7utzb512vontdy6iyt</sha1>
    </revision>
  </page>
  <page>
    <title>Small Data: The Tiny Clues that Uncover Huge Trends</title>
    <ns>0</ns>
    <id>49959657</id>
    <revision>
      <id>747019496</id>
      <parentid>728578353</parentid>
      <timestamp>2016-10-31T00:00:32Z</timestamp>
      <contributor>
        <username>Filedelinkerbot</username>
        <id>20611691</id>
      </contributor>
      <comment>Bot: Removing [[Commons:File:Small Data Cover.JPG]] ([[:File:Small Data Cover.JPG|en]]). It was deleted on Commons by [[:Commons:User:INeverCry|INeverCry]] (per [[Commons:Commons:Deletion requests/Files of User:Pookiegalore]]).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2129" xml:space="preserve">{{npov|date=May 2016}}

{{Infobox book
| name = Small Data: The Tiny Clues That Uncover Huge Trends
| author = Martin Lindstrom
| language = English
| country = United States
| publisher = St. Martins
| isbn = 978-1250080684
}}
Small Data: the Tiny Clues that Uncover Huge Trends is [[Martin Lindstrom]]'s seventh book. It chronicles his work as a branding expert, working with consumers across the world to better understand their behavior. The theory behind the book is that businesses can better create products and services based on observing consumer behavior in their homes, as opposed to relying solely on [[big data]].

== Content ==
The book is based on a several year period of consumer studies for major corporations across the globe.&lt;ref&gt;{{Cite web|url=https://www.martinlindstrom.com/small-data/|title=Small Data - Martin Lindstrom - Bestselling Author|website=MartinLindstrom.com - Martin Lindstrom - Branding Expert &amp; Consultant|language=en-US|access-date=2016-03-27}}&lt;/ref&gt; It features case studies of the author's work interviewing consumers in their homes and using his observations to create hypotheses as to why they use products the way that they do.

== Public Reception ==

The book was a New York Times Bestseller&lt;ref&gt;{{Cite web|url=http://www.nytimes.com/best-sellers-books/2016-03-13/advice-how-to-and-miscellaneous/list.html|title=Best Sellers - The New York Times|website=www.nytimes.com|access-date=2016-03-27}}&lt;/ref&gt; upon release and was positively reviewed on several websites, Including [[Entrepreneur (magazine)|Entrepreneur]]&lt;ref&gt;{{Cite web|url=http://www.entrepreneur.com/article/271992|title=From an Elon Musk Bio to Malcolm Gladwell's 'Blink', These 9 Books Are Must-Reads|last=Agius|first=Aaron|website=Entrepreneur|access-date=2016-03-27}}&lt;/ref&gt; and [[Forbes]]&lt;ref&gt;{{Cite web|url=http://www.forbes.com/sites/davidburkus/2016/01/10/16-must-read-business-books-for-2016/#4ce073648bae|title=16 Must-Read Business Books For 2016|website=Forbes|access-date=2016-03-27}}&lt;/ref&gt;

==References==
{{Reflist}}



[[Category:2016 books]]
[[Category:2016 non-fiction books]]
[[Category:Data management]]</text>
      <sha1>ofsjz227aeku4sa3cwtz1up5fflm3zz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information governance</title>
    <ns>14</ns>
    <id>50290347</id>
    <revision>
      <id>729370852</id>
      <parentid>719818507</parentid>
      <timestamp>2016-07-11T19:10:01Z</timestamp>
      <contributor>
        <username>Le Deluge</username>
        <id>8853961</id>
      </contributor>
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="115" xml:space="preserve">{{catmain}}

[[Category:Data management]]
[[Category:Data security]]
[[Category:Information technology management]]</text>
      <sha1>fn19kszaz0vgwni78qqy728txk0qyjz</sha1>
    </revision>
  </page>
  <page>
    <title>Imprima iRooms</title>
    <ns>0</ns>
    <id>51364737</id>
    <revision>
      <id>761663431</id>
      <parentid>736859457</parentid>
      <timestamp>2017-01-24T03:25:05Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor />
      <comment>/* top */added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3945" xml:space="preserve">{{Orphan|date=January 2017}}

{{Infobox company
| name = Imprima iRooms
| logo = Imprima-logo-hiRes-300dpi.jpg
| logo_size = 
| logo_alt = Imprima-logo-hiRes-300dpi.jpg
| logo_caption = 
| type = [[Private company|Private]]
| industry = [[Virtual Data Room]], [[Technology]]
| founded = 1910&lt;br/&gt;2001 &lt;small&gt;(relaunched)&lt;/small&gt;
| founder = 
| hq_location = London, United Kingdom
| area_served = Worldwide
| website = {{URL|www.imprima.com}}
}}
'''Imprima iRooms''' is a [[private company]] headquartered in London. It provides [[Virtual Data Room]] services to organisations worldwide, including the likes of [[Morgan Stanley]], [[HSBC]] and others.&lt;ref name="printweek"&gt;{{cite news|last1=Francis|first1=Jo|title=MBO at Imprima print operation {{!}} PrintWeek|url=http://www.printweek.com/print-week/news/1148840/mbo-imprima-print-operation|accessdate=22 August 2016|publisher=[[PrintWeek]]}}&lt;/ref&gt; It also has offices in Paris, Frankfurt, Amsterdam and New York.&lt;ref name="growthbusiness"&gt;{{cite news|title=The 21st century virtual data room: A how-to guide|url=http://www.growthbusiness.co.uk/growing-a-business/technology-for-business/2471012/the-21st-century-virtual-data-room-a-howto-guide.thtml|accessdate=22 August 2016|publisher=Growth Business UK}}&lt;/ref&gt;

==History==
Imprima was founded over 100 years ago and during this time has served the financial sector in a variety of different capacities.&lt;ref&gt;{{cite web|title=Imprima de Bussy Limited: Private Company Information|url=http://www.bloomberg.com/research/stocks/private/snapshot.asp?privcapId=613207|publisher=[[Bloomberg Businessweek]]|accessdate=22 August 2016}}&lt;/ref&gt; By 1990, the company was a leading provider of Financial Print solutions, involved in the publication and delivery of sensitive and business-critical communications for their clients.&lt;ref&gt;{{cite web|title=Companies and products...|url=http://www.ukauthority.com/market-report/news/4614/companies-and-products|publisher=UK Authority|accessdate=25 August 2016}}&lt;/ref&gt; In doing so, Imprima amassed an impressive customer list featuring some of the world&#8217;s most reputable financial advisors, law firms and corporations.&lt;ref name="printweek" /&gt;&lt;ref name="cloudnewsdaily"&gt;{{cite web|title=Virtual Data Room Providers|url=http://cloudnewsdaily.com/virtual-data-room/|publisher=Cloud News Daily|accessdate=22 August 2016}}&lt;/ref&gt;

==iRooms==
In 2001, Imprima launched their [[Virtual Data Room]] platform, iRooms.&lt;ref name="teletrader" /&gt; iRooms is used by organisations worldwide for Projects requiring secure online file storage and collaboration. Key use cases include [[Mergers &amp; Acquisitions]] (M&amp;A) activity and [[Real estate transaction|Real estate transactions]].&lt;ref name="Francis"&gt;{{cite news|title=Rebrand for Imprima Financial Print|url=http://www.printweek.com/print-week/news/1154511/rebrand-for-imprima-financial-print|accessdate=22 August 2016|work=www.printweek.com|publisher=[[PrintWeek]]}}&lt;/ref&gt; In 2012, iRooms software was completely revamped and receives regular upgrades.&lt;ref name="cloudnewsdaily" /&gt;&lt;ref name="Francis" /&gt;

==New Ownership==
In 2014, iRoom was acquired by its current owner, OTM Participation. At that time, Imprima operated two product lines: Financial Print and iRooms (Virtual Data Rooms).&lt;ref name="growthbusiness" /&gt; In November 2014, OTM Participation took the decision to divest away the Financial Print division, whose directors carried out an MBO.&lt;ref name="Francis" /&gt;&lt;ref name="teletrader"&gt;{{cite web|title=Imprima Adds Multiple Language Interfaces To New iRooms Release|url=http://www.teletrader.com/news/details/6743031?ts=1471881604044|publisher=www.teletrader.com|accessdate=22 August 2016}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:1910 establishments in the United Kingdom]]
[[Category:Companies based in London]]
[[Category:Technology companies established in 1910]]
[[Category:Technology companies of the United Kingdom]]</text>
      <sha1>baucb0kwll3ycntum9xhq6ixu2iazv1</sha1>
    </revision>
  </page>
  <page>
    <title>StoredIQ</title>
    <ns>0</ns>
    <id>51073850</id>
    <revision>
      <id>741004297</id>
      <parentid>741004060</parentid>
      <timestamp>2016-09-24T19:16:07Z</timestamp>
      <contributor>
        <username>Eustachiusz</username>
        <id>20008731</id>
      </contributor>
      <comment>+[[Category:Data management]]; +[[Category:Data warehousing]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4398" xml:space="preserve">{{Underlinked|date=July 2016}}

&lt;!-- Don't mess with this line! --&gt;&lt;!-- Write your article below this line --&gt;
'''StoredIQ''' was a company founded for [[information lifecycle management]] (ILM) of unstructured data. Founded in 2001 as Deepfile&lt;ref&gt;{{cite news|title=Deepfile Comes to the Surface|url=http://www.networkcomputing.com/storage/deepfile-comes-surface/865316998|publisher=Network Computing}}&lt;/ref&gt; in [[Austin, Texas]] by Jeff Erramouspe, Jeff Bone, Russell Turpin, Rudy Rouhana, Laura Arbilla and Brett Funderburg.&lt;ref&gt;{{cite news|title=Enterprise file management made easy|url=http://www.networkworld.com/article/2332452/wireless/deepfile.html|publisher=Network World}}&lt;/ref&gt; The company changed its name in 2005 to StoredIQ&lt;ref&gt;{{cite news|title=Deep file Becomes StoredIQ|url=http://www.networkcomputing.com/storage/deepfile-becomes-storediq/1788209585|publisher=Network Computing}}&lt;/ref&gt; and continued to operate successfully for over a decade until it was acquired in 2012 by IBM.&lt;ref&gt;{{cite web|title=IBM Extends ILG Suite and Big Data Governance with StoredIQ Acquisition|url=http://public.dhe.ibm.com/software/data/sw-library/ecm-programs/Parity_Research_StoredIQ_Whitepaper.pdf|website=IBM}}&lt;/ref&gt; It now serves as a platform for IBM's information life cycle governance, [[big data]] governance and [[enterprise content management]] technologies.&lt;ref&gt;{{cite web|title=StoredIQ is now an IBM Company|url=https://www-01.ibm.com/software/info/storediq/|website=IBM}}&lt;/ref&gt;

StoredIQ was awarded five patents by the USPTO. The first, originally filed in 2003, enabled unstructured data in file systems to be manipulated in a similar way to information stored in databases.&lt;ref&gt;{{cite web|title=Method and apparatus for managing file systems and file-based data storage|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}&lt;/ref&gt; Subsequent patents only added to StoredIQ's market dominance by building upon the patented actionable file system with further enhancements specific to Enterprise Policy Management  and expanding the reach of StoredIQ's management capability all the way to individual desktops.&lt;ref&gt;{{cite web|title=Patents by Assignee Storediq, Inc.|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}&lt;/ref&gt;

In 2008 StoredIQ was recognized as "Best in Compliance" by Network Products Guide.&lt;ref&gt;{{cite web|title=StoredIQ Wins Network Products Guide Award For Best In Compliance|url=http://www.datastorageconnection.com/doc/storediq-network-products-best-in-compliance-0001|publisher=Data Storage Connection}}&lt;/ref&gt; At the same time, StoredIQ was being recognized as a "Top 5 Provider" by the prestigious Socha-Gelbmann eDiscovery survey.&lt;ref&gt;{{cite web|title=StoredIQ Recognized With "Top 5 Provider" Rating In Socha-Gelbmann eDiscovery Survey|url=http://www.datastorageconnection.com/doc/torediq-ediscovery-survey-storage-0001|publisher=Data Storage Connection}}&lt;/ref&gt; This incredible breath of information governance capability is what originally drew the attention of [[EMC Corporation]], StoredIQ's first potential acquirer. Initially a strategic investor in StoredIQ, many experts{{Who|date=August 2016}} predicted an inevitable acquisition. However, the company shunned their first suitor; leaving EMC to acquire a competitor.&lt;ref&gt;{{cite web|title=EMC Acquires Kazeon, Stiffs StoredIQ|url=http://www.informationweek.com/software/information-management/emc-acquires-kazeon-stiffs-storediq/d/d-id/1082836?|publisher=Information Week}}&lt;/ref&gt;

The company published a whitepaper titled ''The Truth About Big Data''. This promotion combined with StoredIQ's patented, technology led to [[IBM]] selecting StoredIQ as the basis for some products.&lt;ref&gt;{{cite news|last1=Butta|first1=Tom|title=The Truth Behind IBM&#8217;s Plans to Acquire Big Data Company, StoredIQ|url=http://www.huffingtonpost.com/entry/ibm-storediq_b_2377339|publisher=Huffington Post|date=2012-12-31}}&lt;/ref&gt;

==References==
{{reflist}}
&lt;!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --&gt;



[[Category:Companies established in 2001]]
[[Category:Companies based in Austin, Texas]]
[[Category:Information technology management]]
[[Category:Data management]]
[[Category:Data warehousing]]</text>
      <sha1>gkflrxv80n0q265ztsryte67dw3x0w9</sha1>
    </revision>
  </page>
  <page>
    <title>Atomicity (database systems)</title>
    <ns>0</ns>
    <id>373991</id>
    <revision>
      <id>760351490</id>
      <parentid>757041021</parentid>
      <timestamp>2017-01-16T12:58:01Z</timestamp>
      <contributor>
        <username>JordanMussi</username>
        <id>16764318</id>
      </contributor>
      <minor />
      <comment>Fix disambiguation link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5317" xml:space="preserve">{{Other uses|Atomicity (disambiguation)}}

In [[database system]]s, '''atomicity''' (or '''atomicness'''{{Citation needed|date=February 2016}}; from [[Greek language|Greek]] ''atomos'', ''undividable'') is one of the [[ACID]] [[database transaction|transaction]] properties. An '''atomic transaction''' is an ''indivisible'' and ''irreducible'' series of database operations such that either ''all'' occur, or ''nothing'' occurs.&lt;ref&gt;{{cite web
| accessdate = 2011-03-23
| location = http://www.webopedia.com/
| publisher = Webopedia
| title = atomic operation
| quote = An operation during which a processor can simultaneously read a location and write it in the same bus operation. This prevents any other processor or I/O device from writing or reading memory until the operation is complete.
| url = http://www.webopedia.com/TERM/A/atomic_operation.html}}&lt;/ref&gt; A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).

An example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a [[Data consistency|consistent state]], that is, money is not lost nor created if either of those two operations fail.&lt;ref&gt;{{cite web
| url = http://archive.oreilly.com/pub/a/onjava/2001/11/07/atomic.html
| title = Atomic File Transactions, Part 1 - O'Reilly Media
| last = Amsterdam
| first = Jonathan
| website = archive.oreilly.com
| access-date = 2016-02-28
}}&lt;/ref&gt;

==Orthogonality==
Atomicity does not behave completely [[Orthogonal (computing)|orthogonally]] with regard to the other [[ACID]] properties of the transactions. For example, [[Isolation (database systems)|isolation]] relies on atomicity to roll back changes in the event of isolation failures such as [[deadlock]]; [[Consistency (database systems)|consistency]] also relies on rollback in the event of a consistency-violation by an illegal transaction. Finally, atomicity itself relies on [[Durability (database systems)|durability]] to ensure the atomicity of transactions even in the face of external failures.

As a result of this, failure to detect errors and roll back the enclosing transaction may cause failures of isolation and consistency.

==Implementation==
Typically, systems implement Atomicity by providing some mechanism to indicate which transactions have started and which finished; or by keeping a copy of the data before any changes occurred ([[read-copy-update]]).  Several filesystems have developed methods for avoiding the need to keep multiple copies of data, using journaling (see [[journaling file system]]). Databases usually implement this using some form of logging/journaling to track changes. The system synchronizes the logs (often the [[metadata]]) as necessary once the actual changes have successfully taken place. Afterwards, crash recovery simply ignores incomplete entries. Although implementations vary depending on factors such as concurrency issues, the principle of atomicity &#8212; i.e. complete success or complete failure &#8212; remain.

Ultimately, any application-level implementation relies on [[operating system|operating-system]] functionality.  At the file-system level, [[POSIX]]-compliant systems provide [[system call]]s such as &lt;code&gt;open(2)&lt;/code&gt; and &lt;code&gt;flock(2)&lt;/code&gt; that allow applications to atomically open or lock a file. At the process level, [[POSIX Threads]] provide adequate synchronization primitives.

The hardware level requires [[linearizability|atomic operations]] such as [[Test-and-set]], [[Fetch-and-add]], [[Compare-and-swap]], or [[Load-Link/Store-Conditional]], together with [[memory barrier]]s.  Portable operating systems cannot simply block interrupts to implement synchronization, since hardware that lacks actual concurrent execution such as [[hyper-threading]] or [[multi-processing]] is now extremely rare.{{Citation needed|date=December 2016}}

In [[NoSQL (concept)|NoSQL]] [[data store]]s with eventual consistency, the atomicity is also weaker specified than in relational database systems, and exists only in ''row''s (i.e. [[Column family|column families]]).&lt;ref&gt;{{cite web
| accessdate = 2011-03-23
| author = Olivier Mallassi
| date = 2010-06-09
| location = http://blog.octo.com/en/
| publisher = OCTO Talks!
| title = Let&#8217;s play with Cassandra&#8230; (Part 1/3)
| quote = Atomicity is also weaker than what we are used to in the relational world. Cassandra guarantees atomicity within a &lt;code&gt;ColumnFamily&lt;/code&gt; so for all the columns of a row.
| url = http://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/}}&lt;/ref&gt;

==See also==
* [[Atomic operation]]
* [[Transaction processing]]
* [[Long-running transaction]]
* [[Read-copy-update]]

==References==
{{reflist}}

{{DEFAULTSORT:Atomicity (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>n3ihdq16xc1ngi3q437r55v46tduw06</sha1>
    </revision>
  </page>
  <page>
    <title>Rasdaman</title>
    <ns>0</ns>
    <id>36377941</id>
    <revision>
      <id>751624882</id>
      <parentid>732639512</parentid>
      <timestamp>2016-11-26T21:58:53Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12886" xml:space="preserve">{{Infobox software
| name = rasdaman
| logo = [[Image:Rasdaman logo.png|frame|center|x250px|alt=rasdaman logo (used with permission of copyright holder)|rasdaman logo (used with permission of copyright holder)]]
| developer = rasdaman GmbH
| latest_release_version = rasdaman v9.2.1
| latest_release_date = {{release date |2016|02|17}}
| status = Active
| operating_system = most [[Unix-like]] operating systems
| programming language = [[C++]]&lt;ref&gt;{{cite web |url=https://www.openhub.net/p/rasdaman |title=The rasdaman Open Source Project on Open Hub |work=Open Hub |publisher=Black Duck Software |accessdate=2016-08-01}}&lt;/ref&gt;
| genre = [[Array DBMS]]
| license = [[GNU General Public License|GPL v3]]/[[GNU Lesser General Public License|LGPL v3]] or [[Proprietary software|proprietary]]&lt;ref&gt;{{cite web|url=http://rasdaman.org/wiki/License |title=Rasdaman License |publisher=rasdaman.org |date= |accessdate=2016-08-01}}&lt;/ref&gt;
| website = {{URL|http://rasdaman.org}}, {{URL|http://rasdaman.com}}
}}

'''Rasdaman''' ("raster data manager") is an [[Array DBMS]], that is: a [[Database Management System]] which adds capabilities for storage and retrieval of massive multi-dimensional [[array data structure|arrays]], such as sensor, image, and statistics data. A frequently used synonym to arrays is raster data, such as in 2-D [[raster graphics]]; this actually has motivated the name ''rasdaman''. However, rasdaman has no limitation in the number of dimensions - it can serve, for example, 1-D measurement data, 2-D satellite imagery, 3-D x/y/t image time series and x/y/z exploration data, 4-D ocean and climate data, and even beyond spatio-temporal dimensions.

== History ==

In 1989, [[Peter Baumann (computer scientist)|Peter Baumann]] started a research on database support for images, then at [[Fraunhofer Society|Fraunhofer Computer Graphics Institute]]. Following an in-depth investigation on raster data formalizations in imaging, in particular the AFATL Image Algebra, he established a database model for multi-dimensional arrays, including a data model and declarative query language.&lt;ref&gt;Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/journals/vldb/vldb3.html#Baumann94 On the Management of Multidimensional Discrete Data]. VLDB Journal 4(3)1994, Special Issue on Spatial Database Systems, pp. 401 - 444&lt;/ref&gt;

At [[Technical University Munich|TU Munich]], in the EU funded basic research project ''RasDaMan'', a first prototype was established, on top of the O2 [[Object-oriented database|object-oriented DBMS]], and tested in Earth and Life science applications.&lt;ref name="cordis.europa.eu/"&gt;http://cordis.europa.eu/result/rcn/20754_en.html&lt;/ref&gt; Over further EU funded projects, this system was completed and extended to support relational DBMSs.
A dedicated research spin-off, rasdaman GmbH,&lt;ref name="Rasdaman.com"&gt;http://www.rasdaman.com&lt;/ref&gt; was established to give commercial support in addition to the research which subsequently has been continued at [[Jacobs University Bremen|Jacobs University]].&lt;ref name="Rasdaman.com/Archive"&gt;http://www.rasdaman.com/News/archive.php&lt;/ref&gt; Since then, both entities collaborate on the further development and use of the rasdaman technology.

== Concepts ==

=== Data model ===

Based on an array algebra&lt;ref&gt;Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/ngits/ngits99.html#Baumann99 A Database Array Algebra for Spatio-Temporal Data and Beyond]. Proc. NGITS&#8217;99, LNCS 1649, Springer 1999, pp.76-93&lt;/ref&gt; specifically developed for database purposes, rasdaman adds a new attribute type, array, to the relational model. As this array definition is parametrized it constitutes a [[Second-order logic|second-order]] construct or [[Template (C++)|template]]); this fact is reflected by the second-order functionals in the algebra and query language.

For historical reasons, [[Table (database)|tables]] are called ''collections'', as initial design emphasized an embedding into the object-oriented database standard, [[ODMG]]. Anticipating a full integration with SQL, rasdaman collections represent a binary relation with the first attribute being an [[object identifier]] and the second being the array. This allows to establish [[Foreign key|foreign key references]] between arrays and regular [[Tuple|relational tuples]].

=== Raster Query Language ===

The rasdaman query language, rasql, embeds itself into standard SQL and its set-oriented processing.
On the new attribute type, multi-dimensional arrays, a set of extra operations is provided which all are based on a minimal set of algebraically defined core operators, an ''array constructor'' (which establishes a new array and fills it with values) and an ''array condenser'' (which, similarly to SQL aggregates, derives scalar summary information from an array). The query language is declarative (and, hence, optimizable) and safe in evaluation - that is: every query is guaranteed to return after a finite number of processing steps.

The rasql query guide&lt;ref&gt;n.n.: [http://rasdaman.org/browser/manuals_and_examples/manuals/doc-guides/ql-guide.pdf Rasdaman Query Language Guide]&lt;/ref&gt; provides details, here some examples may illustrate its use:

* "From all 4-D x/y/z/t climate simulation data cubes, a cutout which contains all in x, a y extract between 100 and 200, all available along z, and a slice at position 42 (effectively resulting in a 3-D x/y/z cube)":
&lt;source lang="sql"&gt;
select c[ *:*, 100:200, *:*, 42 ] 
from   ClimateSimulations as c 
&lt;/source&gt;

* "In all Landsat satellite images, suppress all non-green areas":
&lt;source lang="sql"&gt;
select img * (img.green &gt; 130)
from   LandsatArchive as img
&lt;/source&gt;

Note: this is a ''very'' naive phrasing of vegetation search; in practice one would use the [[NDVI]] formula, use null values for cloud masking, and several more techniques.

* "All MRI images where, in some region defined by the bit masks, intensity exceeds a threshold of 250":
&lt;source lang="sql"&gt;
select img
from   MRI as img, Masks as m
where  some_cells( img &gt; 250 and m )
&lt;/source&gt;

* "A 2-D x/y slice from all 4-D climate simulation data cubes, each one encoded in PNG format": 
&lt;source lang="sql"&gt;
select png( c[ *:*, *:*, 100, 42 ] )
from   ClimateSimulations as c 
&lt;/source&gt;

== Architecture ==

=== Storage management ===

[[Image:Sample tiling of an array for storage in rasdaman.png|frame|x110px|alt=Sample rasdaman tiling|Sample array tiling in rasdaman]]

Raster objects are maintained in a standard relational database, based on the partitioning of an raster object into ''tiles''.&lt;ref&gt;
Furtado, P., Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/icde/icde99.html#FurtadoB99 Storage of Multidimensional Arrays based on Arbitrary Tiling]. Proc. ICDE'99, March 23&#8211;26, 1999, Sydney, Australia, pp. 328-336&lt;/ref&gt; Aside from a regular subdivision, any user or system generated partitioning is possible. As tiles form the unit of disk access, it is of critical importance that the tiling pattern is adjusted to the query access patterns; several tiling strategies assist in establishing a well-performing tiling. A geo index is employed to quickly determine the tiles affected by a query. Optionally, tiles are compressed using one of various choices, including lossless and lossy (wavelet) algorithms; independently from that, query results can be comressed for transfer to the client. Both tiling strategy and compression comprise database tuning parameters.

Tiles and tile index are stored as [[Binary large object|BLOBs]] in a relational database which also holds the data dictionary needed by rasdaman&#8217;s dynamic type system. Adaptors are available for several relational systems, among them open-source [[Postgresql|PostgreSQL]].
For arrays larger than disk space, hierarchical storage management (HSM) support has been developed.

=== Query processing ===

Queries are parsed, optimised, and executed in the rasdaman server. The parser receives the query string and generates the operation tree. Further, it applies algebraic optimisation rules to the query tree where applicable; of the 150 algebraic rewriting rules, 110 are actually optimising while the other 40 serve to transform the query into canonical form. Parsing and optimization together take less than a millisecond on a laptop.

Execution follows a ''tile streaming'' paradigm: whenever possible, array tiles addressed by a query are fetched sequentially, and each tile is discarded after processing. This leads to an architecture scalable to data volumes exceeding server main memory by orders of magnitude.
   
Query execution is parallelised. First, rasdaman offers inter-query parallelism: A dispatcher schedules requests into a pool of server processes on a per-transaction basis. Intra-query parallelism transparently distributes query subtrees across available cores, GPUs, or cloud nodes.

=== Client APIs ===

The primary interface to rasdaman is the query language. Embeddings into C++ and Java APIs allow invocation of queries, as well as client-side convenience functions for array handling. Arrays per se are delivered in the main memory format of the client language and processor architecture, ready for further processing. Data format codecs allow to retrieve arrays in common raster formats, such as [[Comma-separated values|CSV]], [[Portable Network Graphics|PNG]], and [[Netcdf|NetCDF]].

A Web design toolkit, raswct, is provided which allows to establish Web query frontends easily, including graphical widgets for parametrized query handling, such as sliders for thresholds in queries.

=== Geo Web Services ===

A [[Java (programming language)|Java]] servlet, ''petascope'', running as a rasdaman client offers Web service interfaces specifically for geo data access, processing and filtering. 
The following [[Open Geospatial Consortium|OGC]] standards are supported: [[Web Map Service|WMS]], [[Web Coverage Service|WCS]], [[Web Coverage Processing Service|WCPS]], and [[Web Processing Service|WPS]].

For [[Web Coverage Service|WCS]] and [[Web Coverage Processing Service|WCPS]], rasdaman is the [[reference implementation]].

== Status and license model ==

Today, rasdaman is a fully-fledged implementation offering select / insert / update / delete array query functionality. It is being used in both research and commercial installations.

In a collaboration of the original code owner, rasdaman GmbH&lt;ref name="Rasdaman.com"/&gt; and [[Jacobs University]], a code split was performed in 2008 - 2009 resulting in ''rasdaman community'',&lt;ref&gt;http://www.rasdaman.org&lt;/ref&gt; an [[open source]] branch, and ''rasdaman enterprise'', the commercial branch. Since then, ''rasdaman community'' is being maintained by Jacobs University whereas ''rasdaman enterprise'' remains proprietary to rasdaman GmbH.
The difference between both variants mainly consists of performance boosters (such as specific optimization techniques) intended to support particularly large databases, user numbers, and complex queries; Details are available on the ''rasdaman community'' website.&lt;ref&gt;[http://rasdaman.eecs.jacobs-university.de/trac/rasdaman/wiki/License rasdaman license model]&lt;/ref&gt;

The ''rasdaman community'' license releases the server in [[GPL]] and all client parts in [[LGPL]], thereby allowing to use the system in any kind of license environment.

== Impact and Use ==

Being the first Array DBMS shipped (first prototype available in 1996), rasdaman has shaped this recent database research domain. Concepts of the data and query model (declarativeness, sometimes choice of operators) find themselves in more recent approaches.

In 2008, the [[Open Geospatial Consortium]] released the [[Web Coverage Processing Service]] standard which defines a raster query language based on the concept of a [[Coverage data|coverage]]. Operator semantics&lt;ref&gt;Baumann, P.: [http://www.springerlink.com/openurl.asp?genre=article&amp;id=doi:10.1007/s10707-009-0087-2 The OGC Web Coverage Processing Service (WCPS) Standard]. Geoinformatica, 14(4)2010, pp. 447-479&lt;/ref&gt; is influenced by the rasdaman array algebra.

EarthLook&lt;ref&gt;http://standards.rasdaman.org/&lt;/ref&gt; is a showcase for [[Open Geospatial Consortium|OGC]] [[Coverage data|coverage]] standards in action, offering 1-D through 4-D use cases of raster data access and ad-hoc processing. EarthLook is built on rasdaman.

A sample large project in which rasdaman is being used for large-scale services in all [[Earth sciences]] is EarthServer:&lt;ref&gt;http://www.earthserver.eu&lt;/ref&gt; six services with a volume of at least 100 Terabyte each are being set up for integrated data / metadata retrieval and distributed query processing.

==References==
{{Reflist}}

{{DEFAULTSORT:rasdaman}}
[[Category:Free database management systems]]
[[Category:Proprietary database management systems]]
[[Category:NoSQL]]
[[Category:Data management]]
[[Category:Query languages]]</text>
      <sha1>229cr7hjoihp68hzim7docklms7kav0</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:Cloudiway</title>
    <ns>118</ns>
    <id>52991127</id>
    <revision>
      <id>762064649</id>
      <parentid>762057798</parentid>
      <timestamp>2017-01-26T12:16:39Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <comment>/* External links */ See [[WP:ELNO]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4071" xml:space="preserve">{{New unreviewed article
| source = ArticleWizard
| date = January 2017
}}

{{Infobox company
 | name = Cloudiway
| logo = 
| type = [[Privately held company]]
| founder = Emmanuel Dreux
| area_served = Worldwide
| industry = [[Information technology]]&lt;br/&gt;[[Computer software]]&lt;br/&gt;[[Cloud computing]]&lt;br&gt;[[Email management]]
| products = Coexistence, mail migration, file migration, group migration, site migration, on-premises to cloud migration, mail archive migration, data migration
| foundation = 2010
| location_city = 
| location_country = 
| location = Annecy, France
| locations = 
| homepage = [http://www.cloudiway.com/ www.cloudiway.com]
}}

'''Cloudiway''' is an international [[Software as a service|SaaS]] company founded in 2010 in Annecy, France. The company builds cloud-based tools such as enterprise coexistence (free/busy calendar sharing, mail routing and automatic address list synchronisation), data migration to the cloud, and identity access management tools.

Cloudiway are currently the only company to offer enterprise coexistence between G Suite, Office 365 and Microsoft Exchange.

== Product history ==
After releasing CloudAnywhere, a locally-installed identity access management tool, Cloudiway began working on mail migration tools to enable migration between remote systems such as [[Gmail|G Mail]], [[Office 365]], [[Internet Message Access Protocol|IMAP]] and [[Microsoft Exchange Server|Exchange]]. Soon after, file migration and site migration utilities were launched, followed by enterprise coexistence in 2016&lt;ref&gt;{{Cite web|url=http://inpublic.globenewswire.com/releaseDetails.faces?rId=1999484|title=GlobeNewswire: Cloudiway is launching its solution for Coexistence between Google apps and Office 365/Exchange|website=inpublic.globenewswire.com|access-date=2017-01-26}}&lt;/ref&gt;. Mail archive migration and group migration utilities were launched shortly after, also in 2016. 

With a stable set of utilities under its belt, Cloudiway introduced a wider variety of mail migration sources and destinations, including [[Lotus Notes Mail|Lotus Notes mail]], [[Zimbra]] and Amazon WorkMail. File migration, which had been launched with [[Google Drive]], [[OneDrive]] and [[SharePoint]], was expanded in early 2017 to include file systems (such as Windows servers), Azure [[Binary large object|blob]] storage and [[Amazon S3]] storage.

== Company history ==
Cloudiway is headed by Emmanuel Dreux, a [[Microsoft Most Valuable Professional]] (MVP), who worked at IBM/Lotus and [[Microsoft]] before working on CloudAnywhere &#8212; Cloudiway's first product. The company moved to new headquarters in 2016&lt;ref&gt;{{Cite news|url=http://www.finanznachrichten.de/nachrichten-2016-06/37631705-cloudiway-cloudiway-continues-to-expand-its-global-expansion-and-announces-new-location-in-miami-florida-399.htm|title=CLOUDIWAY: Cloudiway continues to expand its global expansion and announces New location in Miami, Florida|newspaper=FinanzNachrichten.de|language=de|access-date=2017-01-26}}&lt;/ref&gt; to accommodate growing staff numbers. 

Cloudiway are a [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner].  

The company offers consulting services in English and French, with offices in Florida, USA and Annecy, France. Through their consulting services and partnerships, the company has provided cloud migration solutions to a broad range of businesses, including education, the public sector, small private businesses and global enterprises. 


== References ==
{{Reflist}}

== External links ==
* [http://www.cloudiway.com Official Cloudiway website]
* [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner Microsoft Partner page]
* [http://www.distributique.com/actualites/lire-cloudiway-%C2%A0sur-le-cloud-le-marche-francais-n-etait-pas-pret%C2%A0-25414.html Article in French about Cloudiway]


[[Category:Data management]]
[[Category:Cloud computing]]
[[Category:As a service]]
[[Category:Software companies of France]]</text>
      <sha1>hk2g411n9pc776lxmh77srpiiwgyc4d</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information architecture</title>
    <ns>14</ns>
    <id>52912241</id>
    <revision>
      <id>762078286</id>
      <parentid>762078257</parentid>
      <timestamp>2017-01-26T14:32:26Z</timestamp>
      <contributor>
        <username>Swpb</username>
        <id>1921264</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="436" xml:space="preserve">{{cat main}}

Concepts, methodologies and topics related to the practice and theory of information architecture.

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Information architects]]
[[Category:Information governance]]
[[Category:Information science]]
[[Category:Information technology management]]
[[Category:Information technology]]
[[Category:Records management]]
[[Category:Technical communication]]</text>
      <sha1>poidxuv0ublit5ehuakhbxuc2ekdtox</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Video storage</title>
    <ns>14</ns>
    <id>764184</id>
    <revision>
      <id>620148985</id>
      <parentid>543806562</parentid>
      <timestamp>2014-08-06T21:48:02Z</timestamp>
      <contributor>
        <username>Jdaloner</username>
        <id>4460044</id>
      </contributor>
      <minor />
      <comment>Changed how this is sorted in "Video" category.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="120" xml:space="preserve">[[Category:Video|Storage]]
[[Category:Electronic documents]]
[[Category:Information storage]]
[[Category:Storage media]]</text>
      <sha1>6q44jesv56i5ac1rlsw4iuebb18knx1</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic Document Professional</title>
    <ns>0</ns>
    <id>17463457</id>
    <revision>
      <id>697529947</id>
      <parentid>693388930</parentid>
      <timestamp>2015-12-31T02:38:53Z</timestamp>
      <contributor>
        <username>Mccalpin</username>
        <id>468039</id>
      </contributor>
      <minor />
      <comment>/* History */ fixed the mangled edit from Dec 2, 2015</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7533" xml:space="preserve">The '''EDP''' ('''Electronic Document Professional''') is a professional designation awarded by [[Xplor International]] for participants in the [[electronic document]] industry who have satisfied a number of criteria.

The EDP is not a measure of specific knowledge, but awardees normally have a broad knowledge of the electronic document industry along with specific knowledge in one or more areas. Thus, the EDP differs from certifications such as the [[CDIA+]] from CompTIA in which the awardee has passed a formal exam. Rather, the EDP is more closely related to the older MIT and LIT designations from [[AIIM]]. Generally, the EDP does show that the awardee has been in the industry for at least 5 years, and has participated in at least 3 major projects showing competence in a number of 'bodies of knowledge'.

The EDP program is regulated by the EDP Commission of [[Xplor International]], which is a body of senior professionals in the electronic document industry who set the standards and judge the qualifications of the applicants.

== History ==

The first EDP 'class' (as the annual group of awardees is called) was in 1990, when 12 industry professionals were given the award. In 1991, the class was much smaller (only 6), but in 1992 and in all the years following, the number of awardees has generally been double digits, with as many as 25 at one time. The Dutch Chapter of Xplor International has particularly stressed the EDP designation as an essential part of being a professional in the electronic document industry, with the result that in some years, more than ten Dutch members alone were named EDPs, and there are more EDPs per capita in The Netherlands today than in any other country.

In 2009, Xplor relaunched its certification program, so that there is now a three-level certification process to help employers benchmark their staff.&lt;ref&gt;http://www.printingnews.com/web/online/Industry-News/Xplor-Expands-EDP-Certification/1$10502&lt;/ref&gt;

The EDP and the Master-EDP awards are presented once a year at the annual international conference of Xplor. While the first EDPs were awarded at the 1990 conference in Nashville, Tennessee (USA), the first 'class' of Master-EDPs was awarded at the association's annual event in St. Petersburg, Florida (USA) in March, 2010. At that event, the following industry professionals received the association's highest certification:&lt;ref name="m-edp"&gt;[http://www.xplor.org/EDP/MEDPlist.cfm]&lt;/ref&gt;
* William Broddy, M-EDP
* Ernie Crawford, M-EDP
* Scott Draeger, M-EDP
* Oscar Dubbeldam, M-EDP
* William J. "Bill" McCalpin, M-EDP, CDIA, MIT, LIT
* Walter Riddock, M-EDP, CMDSM
* Donald Scrima, M-EDP

At the association's next annual event in April, 2011, two more industry professionals will be named M-EDPs:&lt;ref name="m-edp"/&gt;
* Pat McGrew, M-EDP
* Carrie Murphy, M-EDP

On the other hand, the new EDA designation is awarded at the point that the individual's application is accepted and verified, throughout the year. Currently (December 2010), there are 48 industry professionals who have received this designation,&lt;ref&gt;http://www.xplor.org/EDP/EDA.cfm?viewpage=EDAlist&lt;/ref&gt; nearly all of whom received the designation as a result of attending courses certified by Xplor and taught by [http://www.acadami.org ''acadami''], two of whose principals are M-EDPs.

== Levels of certification ==

=== Electronic Document Associate (EDA) ===
The EDA designation recognizes electronic document sales, development and support specialists who have shown significantly more knowledge of the industry than someone in another discipline.
It requires candidates to be in the industry for 2+ years and have successfully completed 5 days of Xplor Continuing Education Unit (CEU) certified courses, or the equivalent.&lt;ref&gt;http://www.xplor.org/EDP/EDA.cfm&lt;/ref&gt;

=== Electronic Document Professional (EDP) ===
EDPs have clearly shown enough working knowledge of the process to make significant decisions regarding technology or process deployment. For example, management should trust them to lead projects, or support teams.
To become certified as an EDP, a candidate must be in the industry for 5+ years, have successfully completed 10 days of Xplor CEU training (or the equivalent), and have shown their working knowledge and experience through 3 work examples.&lt;ref&gt;http://www.xplor.org/EDP/EDP.cfm&lt;/ref&gt;

=== Master Electronic Document Professional (M-EDP) ===
M-EDPs are the recognized experts on specific technologies, processes, or management skills. For example, an M-EDP may have co-developed a composition or print stream transform system. Another might be the expert on print costing, or statement design. By earning their M-EDP, they are clearly recognized as one of the &#8216;go to&#8217; people in the industry.

To earn the M-EDP, a candidate must have been in the industry for at least 10 years, have been an EDP for at least 5 years, and be able to prove their area of expertise through published material.&lt;ref&gt;http://www.xplor.org/EDP/MEDP.cfm&lt;/ref&gt;

== Designees ==
In all, there are more than 500 industry professionals with the EDP designation, including:
* [http://www.xplorcanada.org/media/eastern/Broddy-Bio-2005.pdf William Broddy, M-EDP] - formerly the Vice Chair of the EDP Commission
* [http://www.mccalpin.com William J. 'Bill' McCalpin, M-EDP] - author of ''The Document Dilemma'' and former General Manager of Xplor International 
* [http://www.acadami.org/about.html Dr. Michael Turton, EDP] - recognized expert in the design of transaction documents and the use of color in transaction documents
* Scott Kelly, EDP - current Xplor International board member
* [http://www.crawfordtech.com/ManagementTeam.htm Ernie Crawford, M-EDP] - President of Crawford Technologies
* [http://www.nautilussolutions.com Stephen Poe, EDP] - Principal at Nautilus Solutions
* [http://www.gmc.net Scott Draeger, M-EDP] - Vice President of Product at GMC Software
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Loic Avenel, EDP] - EMEA Product Marketing Manager at HP Exstream
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Olivier DOILLON, EDP] - EMEA South Presales Manager at HP Exstream
* Roberta McKee-Jackson, EDP - Principal at RSM Consulting and current Vice-Chair of EDP Commission
* Skip Henk, EDP - Current President/CEO of Xplor International
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Kent Lewis, EDP] - Product Manager, HP Exstream
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Matt Riley, EDP] - Solution Support Manager, HP Exstream
* Donald A. Scrima, M-EDP - Current Chair of EDP Commission (2009-20120) and Principal at AFP Education &amp; Consulting
* [http://www.deberichtenfabriek.nl/wie-wij-zijn/dick-paul-en-rene-joor Ren&#233; M. Joor, EDP] - Current member of the EDP Commission, Partner at De Berichtenfabriek BV, the Netherlands 
* Kenneth H. Pugh, EDP - Senior Output Analyst at Aon Corporation
* Kathy Rixham, EDP - Print System Administrator at RR Donnelley
* Kees van de Graaf, EDP - Consultant at PostNL
* James Shand, EDP - Xplor International member of +30-years, Current President Xplor UK &amp; Ireland and member of the Xplor International Board of Directors

==References==
{{reflist}}

==External links==
* [http://www.xplor.org/ Xplor International Web Page]
* [http://www.xplor.org/edp/index.cfm Direct Link to Xplor International Certification Program]

[[Category:Electronic documents]]</text>
      <sha1>foqirqtg6cfxxy5kirqecdvibl4moob</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Content management systems</title>
    <ns>14</ns>
    <id>691651</id>
    <revision>
      <id>701125595</id>
      <parentid>694311607</parentid>
      <timestamp>2016-01-22T17:11:25Z</timestamp>
      <contributor>
        <username>Horcrux92</username>
        <id>10845682</id>
      </contributor>
      <comment>removed [[Category:Data management]]; added [[Category:Data management software]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="575" xml:space="preserve">{{Category diffuse}}
{{Cat main|Content management system}}
{{Commons category}}

A '''[[content management system]]''' ('''CMS''') is a system used to organize and facilitate collaborative content creation. Recently, the term has been used specifically to refer to programs on [[WWW]] [[Web server|servers]], but it can also refer to hardware devices that manage documents on a large network.

[[Category:Web software]]
[[Category:Internet Protocol based network software]]
[[Category:Data management software]]
[[Category:Office software]]
[[Category:Electronic documents]]</text>
      <sha1>1c2iimulxcdelecy24wfon2tnlnuk01</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic document</title>
    <ns>0</ns>
    <id>430436</id>
    <revision>
      <id>729641538</id>
      <parentid>721262317</parentid>
      <timestamp>2016-07-13T16:13:54Z</timestamp>
      <contributor>
        <username>Drchriswilliams</username>
        <id>19349497</id>
      </contributor>
      <comment>improvement template added</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2331" xml:space="preserve">{{Unreferenced|date=July 2016}}
[[File:Text-txt.svg|thumb|An example of a [[text file]] icon, one of the common representations of an electronic document.]]
An '''electronic document''' is any electronic media content (other than computer programs or system files) that are intended to be used in either an electronic form or as printed output.
Originally, any computer data were considered as something internal &amp;mdash; the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. And the improvements in electronic display technologies mean that in most cases it is possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).

However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem &amp;mdash; e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.

Even more problems are connected with complex file formats of various [[word processor]]s, [[spreadsheet]]s and [[graphics software]]. To alleviate the problem, many software companies distribute free [[file viewer]]s for their proprietary file formats (one example is [[Adobe Systems|Adobe]]'s [[Portable Document Format|Acrobat Reader]]). The other solution is the development of standardized non-[[Proprietary software|proprietary]] file formats (such as [[HTML]] and [[OpenDocument]]), and electronic documents for specialized uses have specialized formats &amp;ndash; the specialized [[electronic article]]s in physics  use [[TeX]] or [[PostScript]].
{{reflist}} 
==See also==
*[[Digital era governance]]
*[[Electronic paper]]
*[[Paperless office]]
*[[Bureaucrat]]
*[[E-Government Act of 2002]]
*[[E-government]]
*[[Public administration]]

== External links ==
* [http://people.ischool.berkeley.edu/~buckland/digdoc.html What is a digital document]
* [http://www.msimaging.com/faq Digital Imaging Frequent Questions]

[[Category:Electronic documents]]
[[Category:Word processors]]</text>
      <sha1>j4y4r4z2pqczwji8tl2lplodfxx1mjq</sha1>
    </revision>
  </page>
  <page>
    <title>Email management</title>
    <ns>0</ns>
    <id>21888954</id>
    <revision>
      <id>748189400</id>
      <parentid>748044152</parentid>
      <timestamp>2016-11-06T22:00:00Z</timestamp>
      <contributor>
        <username>DMacks</username>
        <id>712163</id>
      </contributor>
      <comment>requires cite...predates all other date-identified content here Undid revision 748044152 by [[Special:Contributions/YatesByron|YatesByron]] ([[User talk:YatesByron|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7991" xml:space="preserve">'''Email management''' is a specific field of [[communications management]] for managing high volumes of inbound electronic mail received by organizations. Today, email management is an essential component of customer service management.  Customer service call centers currently employ email response management agents along with telephone support agents, and typically use software solutions to manage emails.&lt;ref&gt;"Communications Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp &lt;http://mdg.mit.edu/email-lab-interests.asp&gt;] on 15 Nov 2011&lt;/ref&gt;&lt;ref&gt;How to use e-mail to improve customer service. Inc.com, Guide E-mail Customer Service, Retrieved from web [http://www.inc.com/guides/cust_email/20909.html &lt;http://www.inc.com/guides/cust_email/20909.html &gt;] on 20 January 2012&lt;/ref&gt;

==Background==
Email management evolved from [[database management]] and [[customer relationship management]] (CRM).  Database management began in the 1960s. IBM provided one of the earliest solutions and established standards for database management.  Prominent database management platforms include Oracle, SQL Server etc.&lt;ref&gt;"Database Management - History Of Database Management." Free Encyclopedia of Ecommerce. Net Industries, n.d. Retrieved from Web. [http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html  &lt;http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html&gt;]. on 19 Dec 2011.&lt;/ref&gt;  Vern Watts, inventor of [[IBM Information Management System|IBM's Information Management System]] (IMS), and [[Larry Ellison]], founder of [[Oracle Corporation]], are pioneers in database management systems.&lt;ref&gt;Luongo, C. et al. (2008). The tale of Vern Watts. [Web Video]. Retrieved from [www.youtube.com/watch?v=x98hgieE08o  &lt;www.youtube.com/watch?v=x98hgieE08o&gt;]. on 19 December 2011&lt;/ref&gt;&lt;ref&gt;"Larry Ellison Biography." Academy of Achievement. American Academy of Achievement, 16 Feb 2010. Web. 19 Dec 2011. &lt;http://www.achievement.org/autodoc/page/ell0bio-1&gt;.&lt;/ref&gt;

As database management solutions became more sophisticated in functionality, marketing and customer service departments of large organizations started using information about customers for [[database marketing]].  Customer service managers soon realized that they could extend database marketing to store and retrieve all customer communications to improve visibility with key clients.  This led to the development of CRM systems which managed communication with customers and prospective customers using various media, including phone, direct mail, web site, and email.&lt;ref&gt;"The history of CRM -- evolving beyond the customer database." CRM Software Guide. crm-software-guide.com, n.d. Retrieved from Web. [http://www.crm-software-guide.com/history-of-crm.htm &lt;http://www.crm-software-guide.com/history-of-crm.htm&gt;]. on 19 Dec 2011.&lt;/ref&gt;  Pioneers in CRM include [[David Duffield]], creator of [[PeopleSoft]], and [[Thomas Siebel|Tom Siebel]], founder of [[Siebel Systems]].&lt;ref&gt;"PeopleSoft Inc." International Directory of Company Histories. 2000. In Retrieved Encyclopedia.com Retrieved from web [http://www.encyclopedia.com/doc/1G2-2843700094.html  &lt;http://www.encyclopedia.com/doc/1G2-2843700094.html&gt;] on 19 December 2011&lt;/ref&gt;&lt;ref&gt;Thomas Siebel 1952- Biography - Early life and education, Oracle, Siebel systems. ND. Reference for Business Encyclopedia of Business, 2nd ed. Retrieved from web [http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b &lt; http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b &gt;] on 20 January 2012&lt;/ref&gt;

As email became one of the most prevalent business-to-customer communication media in the 1990s, customer service departments needed specialized systems of tools and trained staff to manage email communication with their customers and prospective customers.

==History==
In 1994, Information Cybernetics, a company in Cambridge, Massachusetts, developed tools for pattern analysis and categorization of emails and other electronic communication channels. The platform of tools was called [[EchoMail]].  The first company to adopt  EchoMail was [[AT&amp;T]]. [[J. C. Penney|JC Penney]] adopted EchoMail in 1997.&lt;ref&gt;Callaway, Erin. "Return to Sender." PC Week Executive. 14 July 1997: 111, 114. Print.&lt;/ref&gt;&lt;ref&gt;O'Brien, J. A. (2002). Introduction to information systems. (10 ed., p. 370). McGraw-Hill Irwin. Retrieved from Web. [http://www.mcm.edu/~palafoxt/sixth.htm &lt;http://www.mcm.edu/~palafoxt/sixth.htm&gt;]. On 8 Dec 2011&lt;/ref&gt;&lt;ref&gt;"The EchoMail Digital Refinery." www.echomail.com. EchoMail, Inc., n.d. Retrieved from Web [http://echomail.com/technology-for-email-management-detailed/ &lt;http://echomail.com/technology-for-email-management-detailed/&gt;]. on 8 Dec 2011&lt;/ref&gt;

Another early company that developed email management software systems was Fort&#200; Internet Software, which produced Adante.&lt;ref&gt;Pavita, H. (1997, June 24). Forte introduces adante 1.0 server software for managing internet-based customer service and communications. Business Wire, Retrieved from Web [http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024 &lt;http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024&gt;] on 8 Dec 2011&lt;/ref&gt;  By late 1999, companies such as KANA Software, Inc., also emerged to support this effort.&lt;ref&gt;"Email Response System - Intelligent Message Handling :: KANA." www.kana.com. KANA Software, n.d. Retrieved from Web. [http://www.kana.com/customer-service/email-response-system.php  &lt;http://www.kana.com/customer-service/email-response-system.php&gt;]. on 8 Dec 2011&lt;/ref&gt;  Eventually, companies such as Siebel CRM Systems, Inc., incorporated components of email management into their CRM systems.&lt;ref&gt;"Bookshelf v7.5: Overview of Siebel eMail Response." docs.oracle.com. ORACLE Corporation, 21 April 2003. Retrieved from Web. [http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html  &lt;http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html&gt;]. on 8 Dec 2011&lt;/ref&gt;

==Typical system components==
An email management system consists of various components to handle different phases of the email management process.&lt;ref&gt;"EMAIL Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp &lt;http://mdg.mit.edu/email-lab-interests.asp&gt;] on 15 Nov 2011&lt;/ref&gt;  These components include: 
*Email ticketing system - One of the key tasks performed by email [[management system]]s is to allocate reference numbers to all incoming [[email]]s. This process is known as ticketing. All subsequent emails relating to one matter can then be grouped under the same reference. This allows users to track their correspondence in a more time effective and productive way.
*Email receipt module - Receives emails, filters out spam and unwanted content to a separate queue (sometimes called [[email filtering]]), and assigns unique ticket numbers based on certain conditions.
*Bayesian spam filters - Statistical technique of filtering spam that most current email management systems utilize.
*Data enhancement module - Adds tags to each email for further processing and may include the ability to connect to remote databases and retrieve specific information about the email author and his/her transactions with the organization.
*Intelligent Analysis module - Reads the subject, message, and attachments, and any tags added by the data enhancement module, analyzing its content in an attempt to understand the subject matter of the email.  This module may store this 'intelligence' as additional tags.

==References==
{{reflist|30em}}

{{DEFAULTSORT:E-Mail Ticketing System}}
[[Category:Email]]
[[Category:Communication software]]
[[Category:Electronic documents]]
[[Category:Records management]]</text>
      <sha1>0kgk02zzpg92fnpvqcjst41xhziqmdf</sha1>
    </revision>
  </page>
  <page>
    <title>Archival Resource Key</title>
    <ns>0</ns>
    <id>24485224</id>
    <revision>
      <id>725244681</id>
      <parentid>725244538</parentid>
      <timestamp>2016-06-14T13:11:56Z</timestamp>
      <contributor>
        <username>Except</username>
        <id>1116146</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6336" xml:space="preserve">An '''Archival Resource Key''' ('''ARK''') is a [[Uniform Resource Locator]] (URL) that is a multi-purpose [[persistent identifier]] for information objects of any type.  An ARK contains the label '''ark:''' after the URL's hostname, which sets the expectation that, when submitted to a web browser, the URL terminated by '?' returns a brief metadata record, and the URL terminated by '??' returns metadata that includes a commitment statement from the current service provider.  The ARK and its inflections ('?' and '??') gain access to three facets of a provider's ability to provide persistence.

Implicit in the design of the ARK scheme is that persistence is purely '''a matter of service''' and not a property of a naming syntax.  Moreover, that a "persistent identifier" cannot be born persistent, but an identifier from any scheme may only be proved persistent over time.  The inflections provide information with which to judge an identifier's likelihood of persistence.

ARKs can be maintained and resolved locally using open source software such as [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers)] or via services such as [http://ezid.cdlib.org EZID] and the central [http://n2t.net N2T (Name-to-Thing)] resolver.

== Structure ==
 &lt;nowiki&gt;[http://NMAH/]ark:/NAAN/Name[Qualifier]&lt;/nowiki&gt;

* NAAN: Name Assigning Authority Number - mandatory unique identifier of the organization that originally named the object
* NMAH: Name Mapping Authority Host - optional and replaceable hostname of an organization that currently provides service for the object
* Qualifier: optional string that extends the base ARK to support access to individual '''hierarchical''' subcomponents of an object,&lt;ref&gt;Hierarchy qualifiers begin with a slash character.&lt;/ref&gt; and to '''variants''' (versions, languages, formats) of components.&lt;ref&gt;Variant qualifiers begin with a dot character.&lt;/ref&gt;

== Name Assigning Authority Numbers (NAANs) ==
A complete NAAN registry&lt;ref&gt;[http://www.cdlib.org/services/uc3/naan_table.html Name Assigning Authority Number registry]&lt;/ref&gt; is maintained by the [[California Digital Library]] and replicated at the [[Biblioth&#232;que nationale de France|Biblioth&#232;que Nationale de France]] and the [[National Library of Medicine|US National Library of Medicine]]. In 2015 it contained over 395 entries, some of which appear below.

* 12025: [[National Library of Medicine]]
* 12148: [[Biblioth&#232;que Nationale de France]]
* 13030: [[California Digital Library]]
* 13038: [[World Intellectual Property Organization]]
* 13960: [[Internet Archive]]
* 14023: Revista de Arte, Ci&#234;ncia e Comunica&#231;&#227;o
* 15230: [[Rutgers University]]
* 17101: [[Centre for Ecology &amp; Hydrology]]
* 20775: [[University of California, San Diego]]
* 21198: [[University of California Los Angeles]]
* 25031: [[University of Kansas]]
* 25593: [[Emory University]]
* 25652: [[&#201;cole nationale sup&#233;rieure des mines de Paris]]
* 26677: [[Library and Archives Canada]]
* 27927: Portico/Ithaka Electronic-Archiving Initiative
* 28722: [[University of California Berkeley]]
* 29114: [[University of California San Francisco]]
* 35911: [[IEEE]]
* 39331: [[National Library of Hungary]]
* 45487: Russian Linguistic Bulletin (&#1056;&#1086;&#1089;&#1089;&#1080;&#1081;&#1089;&#1082;&#1080;&#1081; &#1051;&#1080;&#1085;&#1075;&#1074;&#1080;&#1089;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080;&#1081; &#1041;&#1102;&#1083;&#1083;&#1077;&#1090;&#1077;&#1085;&#1100;)
* 48223: [[UNESCO]]
* 52327: [[Biblioth&#232;que et Archives Nationales du Qu&#233;bec]]
* 61001: [[University of Chicago]]
* 62624: [[New York University]]
* 64269: [[Digital Curation Centre]]
* 65323: [[University of Calgary]]
* 67531: [[University of North Texas]]
* 78319: [[Google]]
* 78428: [[University of Washington]]
* 80444: [[Northwest Digital Archives]]
* 81055: [[British Library]]
* 88435: [[Princeton University]]
* 87925: [[University College Dublin]]

== Generic Services ==
Three generic ARK services have been defined. They are described below in protocol-independent terms. Delivering these services may be implemented through many possible methods given available technology (today&#8217;s or future).

=== Access Service (access, location) ===
*Returns (a copy of) the object or a redirect to the same, although a sensible object proxy may be substituted (for instance a table of contents instead of a large document).
*May also return a discriminated list of alternate object locators.
*If access is denied, returns an explanation of the object&#8217;s current (perhaps permanent) inaccessibility.

=== Policy Service (permanence, naming, etc.) ===
*Returns declarations of policy and support commitments for given ARKs.
*Declarations are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.
*Policy subareas may be addressed in separate requests, but the following areas should be covered:
**object permanence,
**object naming,
**object fragment addressing, and
**operational service support.

=== Description Service ===
*Returns a description of the object. Descriptions are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.
*A description must at a minimum answer the '''who''', '''what''', '''when''', and '''where''' questions concerning an expression of the object.
*Standalone descriptions should be accompanied by the modification date and source of the description itself.
*May also return discriminated lists of ARKs that are related to the given ARK.

== See also ==
* [[Persistent identifier]]
* [[Digital object identifier]] (DOI)
* [[Handle System]] (Handle)
* [[Persistent uniform resource locator]] (PURL)
* [[Uniform resource name]] (URN)
* [[Info URI scheme]]

== Notes and references ==
&lt;references/&gt;

== External links ==
* [http://www.cdlib.org/inside/diglib/ark/ ARK (Archival Resource Key)], [[California Digital Library]]
* [https://confluence.ucop.edu/download/attachments/16744455/arkcdl.pdf Towards Electronic Persistence Using ARK Identifiers], California Digital Library
* [http://tools.ietf.org/html/draft-kunze-ark  The ARK Identifier Scheme], [[Internet Engineering Task Force]]
* [http://n2t.net Name-to-Thing Resolver]
* [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers) open source software]
* [http://ezid.cdlib.org EZID identifier manager]

[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]</text>
      <sha1>ig485glzazh4xqtg6u7m94oh1749ybq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data security</title>
    <ns>14</ns>
    <id>4842680</id>
    <revision>
      <id>544355757</id>
      <parentid>542975087</parentid>
      <timestamp>2013-03-15T12:17:03Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6160030]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="300" xml:space="preserve">{{Commons category|Computer security}}
{{Portal|Computer security}}
{{Cat main|Data security}}

[[Category:Computer security]]
[[Category:Computer data|Security]]
[[Category:Electronic documents]]
[[Category:National security]]
[[Category:Information technology management]]

[[fi:Luokka:Tietoturva]]</text>
      <sha1>dzq3p3ptwuo4vm8m26wt0618xxnnrp3</sha1>
    </revision>
  </page>
  <page>
    <title>Kune (software)</title>
    <ns>0</ns>
    <id>32895691</id>
    <revision>
      <id>747110047</id>
      <parentid>734140546</parentid>
      <timestamp>2016-10-31T14:09:55Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor />
      <comment>/* History */clean up, replace deprecated parameters in cite interview templates; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27090" xml:space="preserve">{{Infobox software
|name                       = Kune
|logo                       = [[File:Kune-logo.svg|frameless|upright]]
|screenshot                 = [[File:Concurrent-edit-and-chat.png|frameless|center]]
|caption                    =
|collapsible                = yes
|author                     = [[Comunes Collective]]
|developer                  = [[Comunes Collective]], IEPALA Foundation
|released                   = {{start date and age|2007}} 
|discontinued               = 
|latest release version     = 1.0.0 (Codename "free-riders")&lt;ref name=release1.0.0&gt;{{cite web|title=Released Kune Version 1.0.0 Codename "free-riders"|url=http://kune.ourproject.org/2015/03/released-kune-version-1-0-0-codename-free-riders/|accessdate=2015-06-23|date=2015-03-18|website=Kune Blog}}&lt;/ref&gt;
|latest release date        = {{release date and age|2015|3|18}} 
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       = Java-based [[Google Web Toolkit]]
|operating system           = 
|platform                   = [[Cross-platform]]
|size                       =
|language                   = Multi-language (more than 10)
|status                     = Active (as of 2015-05)
|genre                      = [[Web application]] [[Collaborative software]] [[Distributed social network]]
|license                    = [[Affero General Public License|AGPLv3]] 
|website                    = {{URL|http://kune.ourproject.org/}} {{URL|https://kune.cc/}}
}}

'''Kune''' is a [[free software|free/open source]] distributed social network focused on collaboration rather than just on communication.&lt;ref name="kune.op.org"&gt;{{cite web|title=Kune development site|url=http://kune.ourproject.org|accessdate=3 February 2011}}&lt;/ref&gt; That is, it focuses on online [[Collaborative real-time editor|real-time collaborative editing]], [[Distributed social network|decentralized social networking]] and web publishing, while focusing on workgroups rather than just on individuals.&lt;ref&gt;{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboraci&#243;n libre para grupos|url=http://barrapunto.com/article.pl?sid=11/08/21/2240235|language= Spanish|accessdate=28 August 2011|newspaper=Barrapunto (Spanish Slashdot)|date=22 August 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboraci&#243;n libre para grupos|url=http://www.meneame.net/story/presentando-proyecto-kune-redes-sociales-colaboracion-libre|language= Spanish|accessdate=28 August 2011|newspaper=Men&#233;ame (Spanish Digg)|date=23 August 2011}}&lt;/ref&gt; It aims to allow for the creation of online spaces for collaborative work where organizations and individuals can build projects online, coordinate common agendas, set up virtual meetings, publish on the web, and join organizations with similar interests. It has a special focus on [[Free culture movement|Free Culture]] and [[social movements]] needs.&lt;ref&gt;{{cite web|title=Kune FAQ|url=http://kune.ourproject.org/faq|accessdate=7 July 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite news
| title       = Das neue Internet
| first       = Niels
| last        = Boeing
| authorlink  = 
| url         = http://www.zeit.de/zeit-wissen/2012/05/Das-alternative-Netz/komplettansicht
| format      = 
| agency      = 
| newspaper   = [[Die Zeit]]
| publisher   = 
| location    = Germany
| isbn        = 
| issn        = 
| oclc        = 
| pmid        = 
| pmd         = 
| bibcode     = 
| doi         = 
| id          = 
| date        = 31 August 2012
| page        = 
| pages       = 
| at          = 
| accessdate  = 5 September 2012
| language    = German
| trans_title = The new internet
| quote       = 
| archiveurl  = 
| archivedate =
| deadurl     =
| ref         = 
}}&lt;/ref&gt; Kune is a project of the [[Comunes Collective]].

== Technical details ==
Kune is programmed using the [[Java (programming language)|Java]]-based [[Google Web Toolkit|GWT]] in the client-side, integrating [[Apache Wave]] (formerly [[Google Wave]]) and using mainly the open protocols [[XMPP]] and [[Wave Federation Protocol]]. GWT Java sources on the client side generates [[Code obfuscation|obfuscated]] and deeply optimized [[JavaScript]] conforming a [[single page application]]. Wave extensions (gadgets, bots) run on top of Kune (as in [[Facebook apps]]) and can be programmed in Java+GWT, JavaScript or Python.

The current version has been under development since 2007,&lt;ref name="video2008"&gt;{{cite video |people= |date= 26 January 2008|title= Video: Status of Kune development (Jan 2008)|url=http://kune.ourproject.org/2008/01/status-jan08/|format= AVI |medium= |trans_title= |publisher= |location= |archiveurl= |archivedate= |accessdate=28 August 2011|time= |id= |isbn= |oclc= |quote= |ref= }}&lt;/ref&gt; with a constant, stable growth and an established codebase.&lt;ref&gt;{{cite web|title=Kune project in Ohloh|url=http://www.ohloh.net/p/kune|author=[[Ohloh]]|accessdate=28 August 2011}}&lt;/ref&gt; Nowadays the code is hosted in the GIT of [[Gitorious]],&lt;ref&gt;{{Cite web
|title=Kune repository in Gitorious
|url=https://gitorious.org/kune
| accessdate = 2 September 2012
| author = 
| last = 
| first = 
| authorlink = 
| work = 
| publisher = [[Gitorious]]
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; it has a development site&lt;ref name="kune.op.org" /&gt; and its main node&lt;ref&gt;{{Cite web
|title=Kune node "Kune.cc"
|url=http://kune.cc
| accessdate = 5 September 2012
| author = 
| last = 
| first = 
| authorlink = 
| work = 
| publisher = Maintained by [[Comunes Collective]]
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; maintained by the [[Comunes Collective]].

Kune is 100% free software and was built only using free software. Its software is licensed under the [[Affero GPL]] license while the art is under a [[Creative Commons]] BY-SA.

== Philosophy ==

Kune was born in order to face a growing concern from the community behind it. Nowadays, groups (a group of friends, activists, a NGO, a small start-up) that need to work together typically will use multiple [[Free like beer|free (like beer)]] commercial centralized for-profit services (e.g. [[Google Docs]], [[Google Groups]], [[Facebook]], [[Wordpress.com]], [[Dropbox (service)|Dropbox]], [[Flickr]], [[eBay]] ...) in order to communicate and collaborate online. However, "If you're not paying for it, you're the product".&lt;ref&gt;{{cite news|title=If You&#8217;re Not Paying for It; You&#8217;re the Product|url=http://lifehacker.com/5697167/if-youre-not-paying-for-it-youre-the-product|accessdate=7 July 2012|newspaper=Lifehacker|date=23 November 2010}}&lt;/ref&gt; In order to avoid that, such groups of users may ask a technical expert to build them mailing lists, a webpage and maybe to set up an [[etherpad]]. However, technicians are needed for any new list (as they cannot configure e.g. [[GNU Mailman]]), configuration change, etc., creating a strong dependency and ultimately a bottle-neck.&lt;ref&gt;{{cite web|title=Kune 0.0.9 published (codename "15M")|url=http://kune.ourproject.org/2011/08/kune-0-0-9-published-codename-15m/|accessdate=12 April 2012|publisher = Kune Blog|date=4 August 2011}}&lt;/ref&gt;

Kune aims to cover all those needs of groups to communicate and collaborate, in an usable way and thus without depending on technical experts
.&lt;ref&gt;{{cite news|title=Software libre, hardware libre, &#191;servicios libres?|url=http://libertonia.escomposlinux.org/story/2009/5/27/12014/3120|accessdate=3 February 2011|newspaper=Libertonia News|date=27 May 2009}}&lt;/ref&gt; It aims to be a free/libre web service (and thus in [[Internet|the cloud]]), but decentralized as email so a user can choose the server they want and still interoperate transparently with the rest.

Opposite to most distributed social networks, this software focuses on collaboration and building, not only on communication and sharing. Thus, Kune does not aim to ultimately replace Facebook, but also all the above-mentioned commercial services. Kune has a strong focus on the construction of [[Free culture movement|Free Culture]] and eventually facilitate [[Commons-based peer production]].&lt;ref&gt;
{{Cite book
| publisher = IOS Press
| isbn = 9781614990642
|last1= Mass Araya|first1= Elizabeth Roxana |last2= Borsetti Gregorio Vidotti|first2= Silvana Aparecida
|editor1-first=  Ana Alice|editor1-last=Baptista
|editor2-first= Peter|editor2-last= Linde
|editor3-first= Niklas|editor3-last= Lavesson
|editor4-first=Miguel |display-editors = 3 |editor4-last=  Abrunhosa de Brito
| title = Social Shaping of Digital Publishing: Exploring the Interplay Between Culture and Technology
|url= http://www.booksonline.iospress.nl/Content/View.aspx?piid=30613
|chapter= Creative Commons: a Convergence Model Between the Ideal of Commons and the Possibilities of Creation in Contemporary TimesOpposed to Copyright Impediments
| date = 15 July 2012
|accessdate= 19 August 2012
|pages= 3&#8211;11
}}&lt;/ref&gt;

== History ==
{| class="wikitable" style="float:right; text-align:center; margin-left:1em; margin-right:0"
|-
! rowspan=1 | Version
! rowspan=1 | Code name
! rowspan=1 | Release date
|-
| 0.0.1
| --
| colspan="2" {{Version |o | 2007}}
|-
| 0.0.9
| [[15-M Movement|15M]]
| colspan="2" {{Version |o | 2011-08-04}}
|-
| 0.1.0
| [[We are the 99%|99%]]&lt;ref&gt;{{cite news|title=Kune new release "99%" &amp; production site|url=https://tech.occupy.net/2012/04/24/kune-new-release-99-production-site/|accessdate=9 June 2012|date=24 April 2012|newspaper= #Occupy Tech News}}&lt;/ref&gt;
| colspan="2" {{Version |o| 2012-04-13}}
|-
| 0.2.0
| [[Elinor Ostrom|Ostrom]]&lt;ref name=releaseOstrom&gt;{{cite news|title=New release of collaborative distributed social network Kune: "Ostrom"|url=https://tech.occupy.net/2012/11/26/new-release-of-collaborative-distributed-social-network-kune-ostrom/|accessdate=26 November 2012|date=26 November 2012|newspaper= #Occupy Tech News}}&lt;/ref&gt;
| colspan="2" {{Version |o | 2012-10-22}}
|-
| 1.0.0
| "Free-riders"&lt;ref name=release1.0.0 /&gt;
| colspan="2" {{Version |c | 2015-03-18}}

|-
| colspan="99" | &lt;small&gt;{{Version |l |show=011101}}&lt;/small&gt;
|}

The origin of Kune relies on the community behind [[Ourproject.org]]. Ourproject&lt;ref&gt;{{cite news|title=There's Life after Microsoft - Free Software Advocates|url=http://www.ipsnews.net/interna.asp?idnews=22073|accessdate=3 February 2011|newspaper=Inter Press Service News Agency|date=24 January 2004}}&lt;/ref&gt; aimed to provide for [[Free culture movement|Free Culture]] (social/cultural projects) what [[Sourceforge]] and other [[software forge]]s meant for [[free software]]: a collection of communication and collaboration tools that would boost the emergence of community-driven free projects.&lt;ref&gt;{{Cite book
| last = Camino
| first = S.
|author2=F. Javier |author3=M. Jim&#233;nez Ga&#241;&#225;n |author4=S. Frutos Cid
 | chapter = Collaborative Development within Open Source Communities
| title =Encyclopedia of Networked and Virtual Organizations
|publisher= IGI Global, Information Science Reference
|isbn = 978-1-59904-885-7
| year = 2008
}}&lt;/ref&gt; However, although Ourproject was relatively successful, it was far from the original aims. The analysis of the situation in 2005&lt;ref&gt;{{cite press release
 | title = Towards a new manager of free projects (Hacia un nuevo gestor de proyectos libres)
 | publisher = [[Ourproject.org]]
 | date = 6 December 2005
 | url = http://ourproject.org/moin/Hacia_un_nuevo_gestor_de_Proyectos_Libres
 | accessdate = 22 April 2012
}}&lt;/ref&gt; concluded that only the groups that had a [[geek|techie]] among them (who would manage [[GNU Mailman|Mailman]] or install a [[Content Management System|CMS]]) were able to move forward, while the rest would abandon the service. Thus, new free collaborative tools were needed, more usable and suitable for anyone, as the available free tools required a high degree of technical expertise. This is why Kune, whose name means "together" in [[Esperanto]], was developed.

The first prototypes of Kune were developed using [[Ruby on Rails]] and [[Pyjamas (software)|Pyjamas]]. However, with the [[Java (software platform)#Licensing|release of Java]] and the [[Google Web Toolkit]] as free software, the community embraced these technologies since 2007.&lt;ref name="video2008" /&gt; In 2009, with a stable codebase and about to release a major version of Kune,&lt;ref&gt;{{cite news|title=&#161;Colabora con Kune! Llamado a desarrolladores/as|url=http://www.apesol.org/news/341|publisher=Peru Free Software Association|date=5 May 2009|accessdate=3 February 2011}}&lt;/ref&gt; Google announced the [[Google Wave]] project and promised it would be released as free software. Wave was using the same technologies of Kune (Java + GWT, Guice, XMPP protocol) so it would be easy to integrate after its release. Besides, Wave was offering an open federated protocol, easy extensibility (through gadgets), easy control versioning, and very good real-time edition of documents. Thus, the community decided to halt the development of Kune, and wait for its release... in the meanwhile developing gadgets that would be integrated in Kune later on.&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = MassMob: Meetings and Smart Mobs 
 | work =
 | publisher = [[Comunes Collective]]
 | year = 2009
 | url = http://massmob.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Troco project: an experimental peer-to-peer currency
 | work = 
 | publisher = [[Comunes Collective]]
 | origyear =2009| year =2010
 | url = http://troco.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Karma: A Reputation Rating System
 | work =
 | publisher = [[Comunes Collective]]
 | origyear = 2009| year = 2010
 | url = http://karma.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt; In this same period, the community established the [[Comunes Association]] (with an acknowledged inspiration in [[Software in the Public Interest]]) as a non-profit legal umbrella for free software tools for encouraging the [[Commons]] and facilitating the work of [[social movements]].&lt;ref&gt;{{cite interview |last =  |first =  |subjectlink = Interview to [[Comunes Collective]] |interviewer = Serotonina EH |title = |url = http://ondaexpansiva.net/?p=1001  |work = Free Culture Forum 2011 |publisher = Radio Onda Expansiva |location = [[Burgos]], [[Spain]] |date = 9 November 2011 |accessdate =11 April 2012 }}&lt;/ref&gt; The umbrella covered Ourproject, Kune and Move Commons,&lt;ref&gt;{{cite news
|title=Move Commons, crowdfunding y etiquetado de proyectos sociales
|url=http://www.misapisportuscookies.com/2011/12/move-commons/
|accessdate=11 April 2012
|newspaper=Mis APIs por tus Cookies
|date=1 December 2012
}}&lt;/ref&gt; together with some other minor projects.

In November 2010, the free [[Apache Wave]] (previously Wave-in-a-Box) was released, under the umbrella of the [[Apache Foundation]]. Since then, the community began integrating its source code within the Kune previous codebase,&lt;ref&gt;{{Cite web
| url = http://ecosistemaurbano.org/castellano/move-commons-y-kune-herramientas-libres-para-el-activismo-y-la-colaboracion/
| title = Move Commons &amp; Kune: free tools for activism and collaboration (Move Commons y Kune: herramientas libres para el activismo y la colaboraci&#243;n)
| accessdate = 11 April 2012
| author = 
| last = Toledo
| first = Jorge
| authorlink = 
| date = 14 February 2012
| work = 
| publisher = Ecosistema Urbano
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; and with the support of the IEPALA Foundation.&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Presenting status of Kune development Jan-2011
 | work =
 | publisher =
 | date = 24 January 2011
 | url =http://kune.ourproject.org/2011/01/status-jan2011/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt; Kune released its Beta and moved to production in April 2012.

Since then, Kune has been catalogued as "activism 2.0"&lt;ref&gt;{{Cite web
| url = https://pilargonzalo.wordpress.com/2011/11/04/activismo-2-0-y-empoderamiento-ciudadano-en-red-i/
| title = Activism 2.0 and citizen empowerment in the net (I) (Activismo 2.0 y empoderamiento ciudadano en red (I))
| accessdate = 11 April 2012
| author = 
| last = Gonzalo
| first = Pilar
| authorlink = 
| date = 4 November 2011
| work = 
| publisher = 
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; and citizen tool,&lt;ref&gt;{{cite journal|title=Free Knowledge: Collective intelligence for developing free tools and community resources (Conocimiento libre: Inteligencia colectiva para desarrollar herramientas libres y recursos comunitarios)|journal=&#161;Rebelaos!|year=2012|volume=1|pages=10|accessdate=11 April 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite news
|title= Cooperation, Collaboration and citizen power (Cooperaci&#243;n, colaboraci&#243;n y poder ciudadano)
|url=http://www.sindikos.com/2012/01/cooperacion-colaboracion-y-poder-ciudadano/
|accessdate=11 April 2012
|newspaper=Sindikos
|date=20 January 2012
}}&lt;/ref&gt; a tool for NGOs,&lt;ref&gt;{{Cite web
| url = http://www.democraciaycooperacion.net/contenidos-sitio-web/espanol/fidc/entre-foros/iii-taller-internacional-del/informacion-398/article/las-redes-de-organizaciones
| title = Las redes de organizaciones sociales del CIS generan propuestas para la internacionalizaci&#243;n de la acci&#243;n
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| date = 5 March 2011
| work = 
| publisher = Foro Internacional Democracia y Cooperaci&#243;n
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt;&lt;ref&gt;{{Cite report
 | author     = &lt;!-- or |last= and |first= --&gt;
 | authorlink =
 | coauthors  = 
 | date       = February 2012
 | title      = Document Summary of the Rapporteur of Second Regional Workshop Latin America and the Caribbean
 | url        = http://www.democraciaycooperacion.net/IMG/pdf/Summary_Rapporteur_and_context.pdf
 | publisher  = [[Ministry of Foreign Affairs and Cooperation (Spain)]]
 | format     =
 | others     =
 | edition    =
 | location   = [[Mexico City]]
 | chapter    =
 | section    =
 | page       =
 | pages      = 15
 | docket     =
 | accessdate = 12 April 2012
 | quote      =
}}&lt;/ref&gt; multi-tool for general purpose&lt;ref&gt;{{Cite web
| url = http://www.contenidosenred.com/blog/kune/
| title = Kune
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| authors = Lucrecia Baquero, Clara Alba
| date = 17 February 2012
| work = 
| publisher = Contenidos en Red
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; (and following that, criticized for the risk of falling on the [[second-system effect]]&lt;ref&gt;{{Cite web
| url = http://jotarp.org/2011/10/internet/contra-las-redes-sociales.html
| title = Against social networks (Contra las redes sociales)
| accessdate = 11 April 2012
| author = 
| last = Palacios
| first = J. Ram&#243;n
| authorlink = 
| date = 24 October 2011
| work = 
| publisher = Jotarp
| pages = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt;) and example of the new paradigm.&lt;ref&gt;{{Cite web
| url = https://semillasdeinnovacion.wordpress.com/2012/03/13/sobre-la-necesidad-de-acercar-la-ciudad-al-campo-y-viceversa/
| title = On the need to bring closer city and country (Sobre la necesidad de acercar la ciudad al campo y viceversa)
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| authors = Lucrecia Baquero, Clara Alba
| date = 13 March 2012
| work = 
| publisher = Semillas de Innovaci&#243;n
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}&lt;/ref&gt; It was selected as "open website of the week" by the [[Open University of Catalonia]]&lt;ref&gt;{{cite news
|title= Open website of the week: Kune
|url=http://mentesabiertas.uoc.edu/webabiertas/webabiertadelasemanakune?lang=en
|accessdate=11 April 2012
|newspaper=Open Minds, [[Open University of Catalonia]] 
|date=5 March 2012
}}&lt;/ref&gt; and as one of the [[Occupy movement|#Occupy]] Tech projects.&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = #Occupy Tech projects
 | work =
 | publisher =
 | url =https://tech.occupy.net/projects/
 | format =
 | doi =
 | accessdate = 22 April 2012}}&lt;/ref&gt; Nowadays there are plans of another federated social network, Lorea (based on [[Elgg (software)|Elgg]]), to connect with Kune.&lt;ref&gt;{{cite news
|title= Radical Community Manager
|url=https://ncomuneszgz.wordpress.com/2012/01/08/radical-community-manager/
|accessdate=11 April 2012
|newspaper=Nociones Comunes
|date=17 March 2012
}}&lt;/ref&gt;

&lt;!--
&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title =
 | work =
 | publisher =
 | url =
 | format =
 | doi =
 | accessdate = }}&lt;/ref&gt;

&lt;ref&gt;{{cite press release
 | title =
 | publisher =
 | url =
 | accessdate = }}&lt;/ref&gt;

&lt;ref&gt;{{Cite book
| last = Camino
| first = S.
|author2=F. Javier |author3=M. Jim&#233;nez Ga&#241;&#225;n |author4=S. Frutos Cid
 | chapter = Collaborative Development within Open Source Communities
| title =Encyclopedia of Networked and Virtual Organizations
|publisher= IGI Global, Information Science Reference
|isbn = 978-1-59904-885-7
| year = 2008
}}&lt;/ref&gt;

&lt;ref&gt;{{Cite journal
| volume = 32
| issue = 3
| pages = 1&#8211;1
| last = Machado
| first = H.
|author2=A. Suset |author3=GJ Mart&#237;n |author4=FR Funes-Monzote
 | title = From the reductionist approach to the system approach in Cuban agriculture: a necessary change of vision
| journal = Pastos y Forrajes
| year = 2009
}}&lt;/ref&gt;

&lt;ref&gt;{{cite news
|title=
|url=
|accessdate=11 April 2012
|newspaper=
|date=29 January 2004
}}&lt;/ref&gt;

--&gt;

== Feature list ==

* All the functionalities of [[Apache Wave]], that is collaborative federated real-time editing, plus
* Communication
** Chat and chatrooms compatible with Gmail and Jabber through XMPP (with several XEP extensions), as it integrates Emite&lt;ref&gt;{{cite web
 | last =
 | first =
 | authorlink =
 | title = Emite: XMPP &amp; GWT
 | work =
 | publisher =
 | url =http://emite.googlecode.com/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}&lt;/ref&gt;
** Social networking (federated)
* Real-time collaboration for groups in:
** Documents: as in [[Google Docs]]
** Wikis
** Lists: as in [[Google Groups]] but minimizing emails, through waves
** Group Tasks
** Group Calendar: as in [[Google Calendar]], with ical export
** Group Blogs
** Web-creation: aiming to publish contents directly on the web (as in [[WordPress]], with a dashboard and public view) (in development)
** Bartering: aiming to decentralize bartering as in [[eBay]]
* Advanced email
** Waves: aims to replace most uses of email
** Inbox: as in email, all your conversations and documents in all kunes are controlled from your inbox
** Email notifications (Projected: replies from email)
* Multimedia &amp; Gadgets
** Image or Video galleries integrated in any doc
** Maps, mindmaps, Twitter streams, etc.
** Polls, voting, events, etc.
** and more via Apache Wave extensions, easy to program (as in [[Facebook apps]], they run on top of Kune)
* Federation
** Distributed Social Networking the same way as e-mail: from one inbox you control all your activity in all kunes, and you can collaborate with anyone or any group regardless of the kune where they were registered.
** Interoperable with any Kune server or Wave-based system
** Chat interoperable with any XMPP server
* Usability
** Strong focus on usability for any user
** Animated tutorials for each tool
** [[Drag and drop|Drag&amp;Drop]] for sharing contents, add users to a doc, change roles, delete contents, etc.
** Shortcuts
* Free culture
** Developed using free software and released under [[Affero General Public License|AGPL]]
** Easy assistant for choosing content licenses for groups. Default license is [[Creative Commons]] BY-SA.
* Developer-friendly
** Debian/Ubuntu package for easy installation
** Wave Gadgets can be programmed in Java+GWT, [[JavaScript]] or [[Python (programming language)|Python]]

== Supporters and adopters ==
Kune has the active support of several organizations and institutions:
* [[Comunes Association]], whose community is behind Kune development. It hosts a Kune server for free projects: [https://kune.cc/#! https://kune.cc]
* IEPALA Foundation,&lt;ref&gt;{{cite web|title=IEPALA Foundation homepage|url=http://www.iepala.es|accessdate=22 April 2012}}&lt;/ref&gt; which is supporting the development with economical and technical resources. It hosts a Kune server for [[non-governmental organizations]]: [http://social.gloobal.net "Social Gloobal"] (previously EuroSur).
* Grasia Software Agent Research Group&lt;ref&gt;{{cite web|title=Grasia Research Group homepage|url=http://grasia.fdi.ucm.es/main/|accessdate=22 April 2012}}&lt;/ref&gt; of the [[Complutense University of Madrid]] has provided technical resources. It seeks to host a Kune server for academic article collaboration.
* Interns from the Master of Free Software from the [[King Juan Carlos University]] are participating in the development.
* Trainees from the [[American University of Science and Technology]] (Lebanon) participate in the system administration.
* [[Paulo Freire Institute]] in Brazil participated in the early design and prototypes.
* The Kune workgroup of the Medialab Prado&lt;ref&gt;{{cite web|title=Medialab-Prado (Madrid) homepage|url=http://medialab-prado.es|accessdate=22 April 2012}}&lt;/ref&gt; are participating in the beta-testing.&lt;ref&gt;{{cite web|title=Comunes profile in Medialab-Prado|url=http://medialab-prado.es/person/comunes|accessdate=22 April 2012}}&lt;/ref&gt;

== See also ==
* [[Apache Wave]]
* [[Comunes Collective]]
* [[Distributed social network]]
* [[Comparison of software and protocols for distributed social networking]]
* [[List of AGPL web applications]]
* [[Ourproject.org]]
* [[Wave Federation Protocol]]

== References ==
{{Reflist|2}}

== External links ==
* [https://kune.cc/ Kune.cc main site]
* [http://kune.cc/?locale=de#!kune.wiki.17.1678 Sites using kune]
* [http://kune.ourproject.org Kune information webpage]

{{Cleanup|reason=[[WP:OVERCAT]]|date=May 2016}}

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Project hosting websites]]
[[Category:Creative Commons-licensed websites]]
[[Category:Collaborative projects]]
[[Category:Virtual communities]]
[[Category:Online communities for social change]]
[[Category:Free groupware]]
[[Category:Free project management software]]
[[Category:Multilingual websites]]
[[Category:Community websites]]
[[Category:Social networking services]]
[[Category:Web applications]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Blog software]]
[[Category:Collaborative real-time editors]]
[[Category:2012 software]]
[[Category:Electronic documents]]
[[Category:Free software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Cross-platform free software]]
[[Category:Internet properties established in 2007]]
[[Category:Software using the GNU AGPL license]]</text>
      <sha1>dmwdk11wc50bxeljm4eydfp4famhcrf</sha1>
    </revision>
  </page>
  <page>
    <title>IMail</title>
    <ns>0</ns>
    <id>30943549</id>
    <revision>
      <id>761300173</id>
      <parentid>680171791</parentid>
      <timestamp>2017-01-22T05:04:21Z</timestamp>
      <contributor>
        <username>L2d4y3</username>
        <id>13015377</id>
      </contributor>
      <comment>Fix broken link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5829" xml:space="preserve">{{Multiple issues|
{{refimprove|date=February 2011}}
{{notability|date=February 2011}}
{{more footnotes|date=February 2011}}
{{COI|date=February 2011}}
}}

{{lowercase title}}

'''Invisible mail''', also referred to as '''iMail''', '''i-mail''' or '''Bote mail''', is a method of exchanging [[Digital data|digital]] messages from an author to one or more recipients in a secure and untraceable way. It is an open protocol and its java implementation (I2P-Bote) is free and open source software, licensed under the GPLv3.&lt;ref&gt;http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/license.txt&lt;/ref&gt;

As with [[email]], one can send and receive iMails. However, normal [[email]]s are visible to an [[ISP]] and to the administrators of the mail servers providing the service. Https, or secure, connections still allow the server admin to view the content of an email and its related IP number. In invisible mails both the mail's content, and the identities (of the sender as well as the receiver) remain unknown to a third party observer or attacker. Furthermore, all iMails are automatically and transparently [[end-to-end principle|end-to-end]] encrypted.

At present, iMail cannot be sent to regular email accounts. iMail addresses are called iMail destinations. They are much longer than the average email addresses and do not carry the "@" sign nor a domain. They already include the encryption key, so using an iMail destination is not harder than using standard email with [[GNU Privacy Guard|gpg]] encryption. The destination is two in one: the "address" as well as the public key. In contrast to gpg- or pgp-encrypted emails, I2P-Bote also encrypts the mail headers.

I2P-Bote also works as an anonymous or pseudonymous remailer. iMails are sent via the [[I2P]] network, a secure and pseudonymous p2p overlay network on the internet and sender and receiver need not be online at the same time ([[store-and-forward]] model). The entire system is serverless and fully distributed. iMail [[Peer-to-peer|peer]]s accept, forward, store and deliver messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly for as long as it takes to send or receive messages.

An iMail message consists of three components, the message ''envelope'', the message ''header'', and the message ''body''. The message header contains control information, including, minimally one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.

iMails can carry international typesets and have small  multi-media content attachments, a process standardized in [[Request for Comments|RFC]] 2045 through 2049. Collectively, these RFCs have come to be called [[Multipurpose Internet Mail Extensions]] (MIME).

==Features==
* ''secure messages'': All iMail messages are automatically end-to-end encrypted from the sender to the receiver.
* ''message authentication'': All iMail messages that are not sent without any information on the originator are automatically signed and the message's integrity and authenticity is checked by the receiver.
* ''anonymous messages'': iMails can also be sent without any information about the originator.
&lt;ref&gt;http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/doc/techdoc.txt&lt;/ref&gt;

===Attachment size limitations===
{{Main|Email attachment}}
iMail messages may have one or more attachments. Attachments serve the purpose of delivering binary or text files of unspecified size. In principle there is no technical intrinsic restriction in the I2P-Bote protocol limiting the size or number of attachments. In practice, however, the slow speeds, overheads and data volume due to redundancy limit the viable size of files or the size of an entire message.

===Email spoofing===
{{Main|Email spoofing}}
[[Email spoofing]] occurs when the header information of an email is altered to make the message appear to come from a known or trusted source. In the case of iMails, this is countered by [[cryptography|cryptographically]] signing each iMail with its originator's key.

===Tracking of sent mails===
The I2P-Bote mail service provides no mechanisms for tracking a transmitted message, but a means to verify that it has been delivered, which however does not necessarily mean it has been read.

===Drawbacks===
iMails can only be received or sent via the web interface, there is no implementation of POP3 or SMTP for iMail yet. Furthermore, there are no bridges that allow for sending from I2P-Bote to a standard internet email account or vice versa.

==See also==

===Related services===
* [[Email]]
* [[I2P]]
* [[Data security]]
* [[Email encryption]]
* [[Email client]], [[Comparison of email clients]]
* [[Email hosting service]]
* [[Internet mail standard]]s
* [[Mail transfer agent]]
* [[Mail user agent]]
* [[Unicode and email]]
* [[Webmail]]
* [[Anonymous remailer]]
* [[Disposable email address]]
* [[Email encryption]]
* [[Email tracking]]
* [[Electronic mailing list]]
* [[Mailing list archive]]

===Protocols===
* [[IMAP]]
* [[POP3]]
* [[SMTP]]
* [[UUCP]]
* [[X400]]

==References==
{{reflist}}
* http://i2pbote.i2p/src.zip  (source code)
* http://i2pbote.i2p/history.txt (history.txt)
* http://awxcnx.de/handbuch_55.htm (German Privacy Foundation)
* http://www.unitethecows.com/other-p2p-clients/48940-i2pbote-0-1-2-released.html

==External links==
{{Wiktionary|iMail|email|outbox}}
* http://i2pbote.i2p (I2P-internal)
* http://www.i2p2.de
&lt;!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links --&gt;

{{Computer-mediated communication}}
{{Email clients}}

[[Category:Email]]
[[Category:Internet terminology]]
[[Category:American inventions]]
[[Category:Electronic documents]]</text>
      <sha1>mf3dmkrtipsjb1ggxpp95az365zbhb0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Electronic lab notebook</title>
    <ns>14</ns>
    <id>49921430</id>
    <revision>
      <id>714733841</id>
      <parentid>711596622</parentid>
      <timestamp>2016-04-11T14:44:57Z</timestamp>
      <contributor>
        <username>Le Deluge</username>
        <id>8853961</id>
      </contributor>
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="148" xml:space="preserve">{{catmain}}

[[Category:Electronic documents]]
[[Category:Data management software]]
[[Category:Science software]]
[[Category:Scientific documents]]</text>
      <sha1>bj7uf1uzmpcxlcrw74wenxw129p5oz8</sha1>
    </revision>
  </page>
  <page>
    <title>Digital signature</title>
    <ns>0</ns>
    <id>59644</id>
    <revision>
      <id>761554978</id>
      <parentid>760605590</parentid>
      <timestamp>2017-01-23T16:27:58Z</timestamp>
      <contributor>
        <username>Dako98</username>
        <id>29823984</id>
      </contributor>
      <comment>/* Non-repudiation */ fixed typo in "Online"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33347" xml:space="preserve">A '''digital signature'''  is a mathematical scheme for demonstrating the authenticity of digital messages or documents. A valid digital signature gives a recipient reason to believe that the message was created by a known sender ([[authentication]]), that the sender cannot deny having sent the message ([[non-repudiation]]), and that the message was not altered in transit ([[Data integrity|integrity]]).

Digital signatures are a standard element of most [[cryptographic protocol]] suites, and are commonly used for software distribution, financial transactions, [[contract management software]], and in other cases where it is important to detect forgery or tampering.

== Explanation ==
Digital signatures are often used to implement [[electronic signature]]s, a broader term that refers to any electronic data that carries the intent of a signature,&lt;ref&gt;[http://frwebgate.access.gpo.gov/cgi-bin/getdoc.cgi?dbname=106_cong_public_laws&amp;docid=f:publ229.106.pdf US ESIGN Act of 2000]&lt;/ref&gt; but not all electronic signatures use digital signatures.&lt;ref&gt;[http://enterprise.state.wi.us/home/strategic/esig.htm State of WI]&lt;/ref&gt;&lt;ref&gt;[http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html National Archives of Australia] {{webarchive |url=https://web.archive.org/web/20141109/http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html |date=November 9, 2014 }}&lt;/ref&gt; In some countries, including the United States, [[Turkey]], [[India]], Brazil, Indonesia, Saudi Arabia,&lt;ref&gt;{{cite book|first=Government of India|title=The Information Technology Act, 2000|url=http://www.dot.gov.in/sites/default/files/itbill2000_0.pdf}}&lt;/ref&gt; [[Switzerland]] and the countries of the [[European Union]],&lt;ref name=Cryptomathic_MajorStandardsDigSig&gt;{{cite web|last1=Turner|first1=Dawn|title=Major Standards and Compliance of Digital Signatures - A World-Wide Consideration|url=http://www.cryptomathic.com/news-events/blog/major-standards-and-compliance-of-digital-signatures-a-world-wide-consideration|publisher=Cryptomathic|accessdate=7 January 2016}}&lt;/ref&gt;&lt;ref name=CryptomathicDigSigServicesAshiqJA&gt;{{cite web|last1=JA|first1=Ashiq|title=Recommendations for Providing Digital Signature Services|url=http://www.cryptomathic.com/news-events/blog/recommendations-for-providing-digital-signature-services|publisher=Cryptomathic|accessdate=7 January 2016}}&lt;/ref&gt; electronic signatures have legal significance.

Digital signatures employ [[asymmetric key algorithm|asymmetric cryptography]]. In many instances they provide a layer of validation and security to messages sent through a nonsecure channel: Properly implemented, a digital signature gives the receiver reason to believe the message was sent by the claimed sender.  Digital seals and signatures are equivalent to handwritten signatures and stamped seals.&lt;ref&gt;[http://www.arx.com/industries/engineering/regulatory-compliance/ Regulatory Compliance: Digital signatures and seals are legally enforceable ESIGN (Electronic Signatures in Global and National Commerce) Act]&lt;/ref&gt; Digital signatures are equivalent to traditional handwritten signatures in many respects, but properly implemented digital signatures are more difficult to forge than the handwritten type. Digital signature schemes, in the sense used here, are cryptographically based, and must be implemented properly to be effective. Digital signatures can also provide [[non-repudiation]], meaning that the signer cannot successfully claim they did not sign a message, while also claiming their [[private key]] remains secret; further, some non-repudiation schemes offer a time stamp for the digital signature, so that even if the private key is exposed, the signature is valid. Digitally signed messages may be anything representable as a [[bitstring]]: examples include [[electronic mail]], [[contract]]s, or a message sent via some other [[cryptographic protocol]].

==Definition of Digital Signature==
{{Main article|Public-key cryptography}}
A digital signature scheme typically consists of three algorithms;
* A ''[[key generation]]'' algorithm that selects a ''private key'' [[Uniform distribution (discrete)|uniformly at random]] from a set of possible private keys. The algorithm outputs the private key and a corresponding ''public key''.
* A ''signing'' algorithm that, given a message and a private key, produces a signature.
* A ''signature verifying'' algorithm that, given the message, public key and signature, either accepts or rejects the message's claim to authenticity.

Two main properties are required.  First, the authenticity of a signature generated from a fixed message and fixed private key can be verified by using the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party without knowing that party's private key.
A digital signature is an authentication mechanism that enables the creator of the message to attach a code that acts as a signature. 
The [[Digital Signature Algorithm]] (DSA), developed by the [[National Institute of Standards and Technology]], is one of [[Digital signature#Some digital signature algorithms|many examples]] of a signing algorithm.

In the following discussion, 1&lt;sup&gt;''n''&lt;/sup&gt; refers to a [[Unary numeral system|unary number]].

Formally, a '''digital signature scheme''' is a triple of probabilistic polynomial time algorithms, (''G'', ''S'', ''V''), satisfying:
* ''G'' (key-generator) generates a public key, ''pk'', and a corresponding private key, ''sk'', on input 1&lt;sup&gt;''n''&lt;/sup&gt;, where ''n'' is the security parameter.
* ''S'' (signing) returns a tag, ''t'', on the inputs: the private key, ''sk'', and a string, ''x''.
* ''V'' (verifying) outputs ''accepted'' or ''rejected'' on the inputs: the public key, ''pk'', a string, ''x'', and a tag, ''t''. 
For correctness, ''S'' and ''V'' must satisfy

: Pr [ (''pk'', ''sk'') &#8592; ''G''(1&lt;sup&gt;''n''&lt;/sup&gt;), ''V''( ''pk'', ''x'', ''S''(''sk'', ''x'') ) = ''accepted'' ] = 1.&lt;ref&gt;Pass, def 135.1&lt;/ref&gt;

A digital signature scheme is '''secure''' if for every non-uniform probabilistic polynomial time [[Adversary (cryptography)|adversary]], ''A''

: Pr [ (''pk'', ''sk'') &#8592; ''G''(1&lt;sup&gt;''n''&lt;/sup&gt;), (''x'', ''t'') &#8592; ''A''&lt;sup&gt;''S''(''sk'', &#183; )&lt;/sup&gt;(''pk'', 1&lt;sup&gt;''n''&lt;/sup&gt;), ''x'' &#8713; ''Q'', ''V''(''pk'', ''x'', ''t'') = ''accepted''] &lt; [[Negligible function|negl]](''n''),

where ''A''&lt;sup&gt;''S''(''sk'', &#183; )&lt;/sup&gt; denotes that ''A'' has access to the [[Oracle machine|oracle]], ''S''(''sk'', &#183; ), and ''Q'' denotes the set of the queries on ''S'' made by ''A'', which knows the public key, ''pk'', and the security parameter, ''n''. Note that we require any adversary cannot directly query the string, ''x'', on ''S''.&lt;ref&gt;Goldreich's FoC, vol. 2, def 6.1.2. Pass, def 135.2&lt;/ref&gt;

==History of Digital Signature==
In 1976, [[Whitfield Diffie]] and [[Martin Hellman]] first described the notion of a digital signature scheme, although they only conjectured that such schemes existed.&lt;ref&gt;"New Directions in Cryptography", IEEE Transactions on Information Theory, IT-22(6):644&#8211;654, Nov. 1976.&lt;/ref&gt;&lt;ref name=lysythesis&gt;"[http://theory.lcs.mit.edu/~cis/theses/anna-phd.pdf Signature Schemes and Applications to Cryptographic Protocol Design]", Anna Lysyanskaya, PhD thesis, [[Massachusetts Institute of Technology|MIT]], 2002.&lt;/ref&gt;  Soon afterwards, [[Ronald Rivest]], [[Adi Shamir]], and [[Len Adleman]] invented the [[RSA (algorithm)|RSA]] algorithm, which could be used to produce primitive digital signatures&lt;ref name="rsa"&gt;
{{cite journal
 | first = R. | last = Rivest
 | author2 = A. Shamir; L. Adleman
 | url = http://people.csail.mit.edu/rivest/Rsapaper.pdf
 | title = A Method for Obtaining Digital Signatures and Public-Key Cryptosystems
 | journal = Communications of the ACM
 | volume = 21  | issue = 2 | pages = 120&#8211;126 | year = 1978
 | doi = 10.1145/359340.359342
}}&lt;/ref&gt; (although only as a proof-of-concept &amp;ndash; "plain" RSA signatures are not secure&lt;ref&gt;For example any integer, ''r'', "signs" ''m''=''r''&lt;sup&gt;''e''&lt;/sup&gt; and the product, ''s''&lt;sub&gt;1&lt;/sub&gt;''s''&lt;sub&gt;2&lt;/sub&gt;, of any two valid signatures, ''s''&lt;sub&gt;1&lt;/sub&gt;, ''s''&lt;sub&gt;2&lt;/sub&gt; of ''m''&lt;sub&gt;1&lt;/sub&gt;, ''m''&lt;sub&gt;2&lt;/sub&gt; is a valid signature of the product, ''m''&lt;sub&gt;1&lt;/sub&gt;''m''&lt;sub&gt;2&lt;/sub&gt;.&lt;/ref&gt;). The first widely marketed software package to offer digital signature was [[Lotus Notes]] 1.0, released in 1989, which used the RSA algorithm.&lt;ref&gt;{{cite web|title=The History of Notes and Domino|url=http://www.ibm.com/developerworks/lotus/library/ls-NDHistory/|website=developerWorks|accessdate=17 September 2014}}&lt;/ref&gt;

Other digital signature schemes were soon developed after RSA, the earliest being [[Lamport signature]]s,&lt;ref&gt;"Constructing digital signatures from a one-way function.", [[Leslie Lamport]], Technical Report CSL-98, SRI International, Oct. 1979.&lt;/ref&gt; [[Merkle tree|Merkle signatures]] (also known as "Merkle trees" or simply "Hash trees"),&lt;ref&gt;"A certified digital signature", Ralph Merkle, In Gilles Brassard, ed., Advances in Cryptology &#8211; [[CRYPTO]] '89, vol. 435 of Lecture Notes in Computer Science, pp. 218&amp;ndash;238, Spring Verlag, 1990.&lt;/ref&gt; and [[Rabin signature]]s.&lt;ref&gt;"Digitalized signatures as intractable as factorization."  [[Michael O. Rabin]], Technical Report MIT/LCS/TR-212, MIT Laboratory for Computer Science, Jan. 1979&lt;/ref&gt;

In 1988, [[Shafi Goldwasser]], [[Silvio Micali]], and [[Ronald Rivest]] became the first to rigorously define the security requirements of digital signature schemes.&lt;ref name="SJC 17(2)"&gt;"A digital signature scheme secure against adaptive chosen-message attacks.", Shafi Goldwasser, Silvio Micali, and Ronald Rivest. SIAM Journal on Computing, 17(2):281&amp;ndash;308, Apr. 1988.&lt;/ref&gt; They described a hierarchy of attack models for signature schemes, and also presented the [[GMR (cryptography)|GMR signature scheme]], the first that could be proved to prevent even an existential forgery against a chosen message attack.&lt;ref name="SJC 17(2)"/&gt;

==How they work==
To create RSA signature keys, generate a RSA key pair containing a modulus, ''N'', that is the product of two large primes, along with integers, ''e'' and ''d'', such that ''e&amp;nbsp;d''&amp;nbsp;[[Modular arithmetic|&#8801;]]&amp;nbsp;1&amp;nbsp;(mod&amp;nbsp;&#966;(''N'')), where &#966; is the [[Euler's totient function|Euler phi-function]]. The signer's public key consists of ''N'' and ''e'', and the signer's secret key contains ''d''.

To sign a message, ''m'', the signer computes a signature, &#963;, such that &#963; &#8801; ''m''&lt;sup&gt;''d''&lt;/sup&gt; (mod ''N''). To verify, the receiver checks that &#963;&lt;sup&gt;''e''&lt;/sup&gt; &#8801; ''m'' (mod ''N'').

As noted earlier, this basic scheme is not very secure. To prevent attacks, one can first apply a [[cryptographic hash function]] to the message, ''m'', and then apply the RSA algorithm described above to the result. This approach is secure assuming the hash function is a [[random oracle model|random oracle]].

Most early signature schemes were of a similar type: they involve the use of a [[trapdoor permutation]], such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite, ''n.'' A trapdoor permutation family is a family of [[permutation]]s, specified by a parameter, that is easy to compute in the forward direction, but is difficult to compute in the reverse direction without already knowing the private key ("trapdoor").  Trapdoor permutations can be used for digital signature schemes, where computing the reverse direction with the secret key is required for signing, and computing the forward direction is used to verify signatures.

Used directly, this type of signature scheme is vulnerable to a key-only existential forgery attack. To create a forgery, the attacker picks a random signature &#963; and uses the verification procedure to determine the message, ''m'', corresponding to that signature.&lt;ref&gt;"Modern Cryptography: Theory &amp; Practice", Wenbo Mao, Prentice Hall Professional Technical Reference, New Jersey, 2004, pg. 308.  ISBN 0-13-066943-1&lt;/ref&gt; In practice, however, this type of signature is not used directly, but rather, the message to be signed is first [[cryptographic hash function|hashed]] to produce a short digest that is then signed. This forgery attack, then, only produces the hash function output that corresponds to &#963;, but not a message that leads to that value, which does not lead to an attack. In the random oracle model, this [[Full domain hash|hash-then-sign]] form of signature is existentially unforgeable, even against a [[chosen-plaintext attack]].&lt;ref name=lysythesis /&gt;{{Clarify|reason=Please give a page number or theorem number.|date=September 2010}}

There are several reasons to sign such a hash (or message digest) instead of the whole document.

;For efficiency: The signature will be much shorter and thus save time since hashing is generally much faster than signing in practice.
;For compatibility: Messages are typically bit strings, but some signature schemes operate on other domains (such as, in the case of RSA, numbers modulo a composite number ''N''). A hash function can be used to convert an arbitrary input into the proper format.
;For integrity: Without the hash function, the text "to be signed" may have to be split (separated) in blocks small enough for the signature scheme to act on them directly. However, the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order.

==Notions of security==
In their foundational paper, Goldwasser, Micali, and Rivest lay out a hierarchy of attack models against digital signatures:&lt;ref name="SJC 17(2)"/&gt;

# In a ''key-only'' attack, the attacker is only given the public verification key.
# In a ''known message'' attack, the attacker is given valid signatures for a variety of messages known by the attacker but not chosen by the attacker.
# In an ''adaptive chosen message'' attack, the attacker first learns signatures on arbitrary messages of the attacker's choice.

They also describe a hierarchy of attack results:&lt;ref name="SJC 17(2)"/&gt;

# A ''total break'' results in the recovery of the signing key.
# A [[universal forgery]] attack results in the ability to forge signatures for any message.
# A [[selective forgery]] attack results in a signature on a message of the adversary's choice.
# An [[existential forgery]] merely results in some valid message/signature pair not already known to the adversary.

The strongest notion of security, therefore, is security against existential forgery under an adaptive chosen message attack.

==Applications of digital signatures==

As organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory.  The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.  Universities including Penn State, [[University of Chicago]], and Stanford are publishing electronic student transcripts with digital signatures.

Below are some common reasons for applying a digital signature to communications:

===Authentication===
Although messages may often include information about the entity sending a message, that information may not be accurate.  Digital signatures can be used to authenticate the source of messages. When ownership of a digital signature secret key is bound to a specific user, a valid signature shows that the message was sent by that user. The importance of high confidence in sender authenticity is especially obvious in a financial context. For example, suppose a bank's branch office sends instructions to the central office requesting a change in the balance of an account. If the central office is not convinced that such a message is truly sent from an authorized source, acting on such a request could be a grave mistake.

===Integrity===
In many scenarios, the sender and receiver of a message may have a need for confidence that the message has not been altered during transmission. Although encryption hides the contents of a message, it may be possible to ''change'' an encrypted message without understanding it. (Some encryption algorithms, known as [[Malleability (cryptography)|nonmalleable]] ones, prevent this, but others do not.) However, if a message is digitally signed, any change in the message after signature invalidates the signature. Furthermore, there is no efficient way to modify a message and its signature to produce a new message with a valid signature, because this is still considered to be computationally infeasible by most cryptographic hash functions (see [[collision resistance]]).

===Non-repudiation===
Non-repudiation,&lt;ref name="Cryptomathic_MajorStandardsDigSig" /&gt; or more specifically ''non-repudiation of origin'', is an important aspect of digital signatures. By this property, an entity that has signed some information cannot at a later time deny having signed it. Similarly, access to the public key only does not enable a fraudulent party to fake a valid signature.

Note that these authentication, non-repudiation etc. properties rely on the secret key ''not having been revoked ''prior to its usage.  Public revocation of a key-pair is a required ability, else leaked secret keys would continue to implicate the claimed owner of the key-pair. Checking revocation status requires an "online" check; e.g., checking a [[certificate revocation list]] or via the &lt;ref name="CryptomathicDigSigServicesAshiqJA" /&gt;[[Online Certificate Status Protocol]].   Very roughly this is analogous to a vendor who receives credit-cards first checking online with the credit-card issuer to find if a given card has been reported lost or stolen.   Of course, with stolen key pairs, the theft is often discovered only after the secret key's use, e.g., to sign a bogus certificate for espionage purpose.

==Additional security precautions==

===Putting the private key on a smart card===
All public key / private key cryptosystems depend entirely on keeping the private key secret. A private key can be stored on a user's computer, and protected by a local password, but this has two disadvantages:

* the user can only sign documents on that particular computer
* the security of the private key depends entirely on the [[computer insecurity|security]] of the computer

A more secure alternative is to store the private key on a [[smart card]]. Many smart cards are designed to be tamper-resistant (although some designs have been broken, notably by [[Ross J. Anderson (professor)|Ross Anderson]] and his students). In a typical digital signature implementation, the hash calculated from the document is sent to the smart card, whose CPU signs the hash using the stored private key of the user, and then returns the signed hash. Typically, a user must activate his smart card by entering a [[personal identification number]] or PIN code (thus providing [[two-factor authentication]]). It can be arranged that the private key never leaves the smart card, although this is not always implemented. If the smart card is stolen, the thief will still need the PIN code to generate a digital signature. This reduces the security of the scheme to that of the PIN system, although it still requires an attacker to possess the card. A mitigating factor is that private keys, if generated and stored on smart cards, are usually regarded as difficult to copy, and are assumed to exist in exactly one copy. Thus, the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked. Private keys that are protected by software only may be easier to copy, and such compromises are far more difficult to detect.

===Using smart card readers with a separate keyboard===
Entering a PIN code to activate the smart card commonly requires a [[numeric keypad]]. Some card readers have their own numeric keypad. This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer's keyboard. Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a [[keystroke logging|keystroke logger]], potentially compromising the PIN code. Specialized card readers are also less vulnerable to tampering with their software or hardware and are often [[Evaluation Assurance Level|EAL3]] certified.

===Other smart card designs===
Smart card design is an active field, and there are smart card schemes which are intended to avoid these particular problems, though so far with little security proofs.

===Using digital signatures only with trusted applications===
One of the main differences between a digital signature and a written signature is that the user does not "see" what he signs. The user application presents a hash code to be signed by the digital signing algorithm using the private key. An attacker who gains control of the user's PC can possibly replace the user application with a foreign substitute, in effect replacing the user's own communications with those of the attacker. This could allow a malicious application to trick a user into signing any document by displaying the user's original on-screen, but presenting the attacker's own documents to the signing application.

To protect against this scenario, an authentication system can be set up between the user's application (word processor, email client, etc.) and the signing application. The general idea is to provide some means for both the user application and signing application to verify each other's integrity. For example, the signing application may require all requests to come from digitally signed binaries.

===Using a network attached [[hardware security module]]===
One of the main differences between a [[cloud]] based digital signature service and a locally provided one is risk.  Many risk averse companies, including governments, financial and medical institutions,  and payment processors require more secure standards, like [[FIPS 140-2]] level 3 and [[FIPS 201]] certification, to ensure the signature is validated and secure.&lt;ref&gt;[http://www.arx.com/products/privateserver-hsm/overview/ PrivateServer HSM Overview]&lt;/ref&gt; &lt;!--To finish: current and future applications, actual algorithms, standards, why not as adopted as widely as expected, etc.--&gt;

===WYSIWYS===
{{Main article|WYSIWYS}}
Technically speaking, a digital signature applies to a string of bits, whereas humans and applications "believe" that they sign the semantic interpretation of those bits. In order to be semantically interpreted, the bit string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system. The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content. It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed. From a semantic perspective this creates uncertainty about what exactly has been signed. [[WYSIWYS]] (What You See Is What You Sign) &lt;ref name=WYSIWYS_SeminalPaper&gt;{{cite journal|last1=Landrock|first1=Peter|last2=Pedersen|first2=Torben|title=WYSIWYS? -- What you see is what you sign?|journal=Information Security Technical Report|date=1998|volume=3|issue=2|pages=55&#8211;61}}&lt;/ref&gt; means that the semantic interpretation of a signed message cannot be changed. In particular this also means that a message cannot contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied. WYSIWYS is a necessary requirement for the validity of digital signatures, but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems. The term WYSIWYS was coined by [[Peter Landrock]] and [[Cryptomathic|Torben Pedersen]] to describe some of the principles in delivering secure and legally binding digital signatures for Pan-European projects.&lt;ref name=WYSIWYS_SeminalPaper /&gt;

===Digital signatures versus ink on paper signatures===

An ink signature could be replicated from one document to another by copying the image manually or digitally, but to have credible signature copies that can resist some scrutiny is a significant manual or technical skill, and to produce ink signature copies that resist professional scrutiny is very difficult.

Digital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document. Paper contracts sometimes have the ink signature block on the last page, and the previous pages may be replaced after a signature is applied.  Digital signatures can be applied to an entire document, such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered, but this can also be achieved by signing with ink and numbering all pages of the contract.

==Some digital signature algorithms==
*[[RSA (algorithm)|RSA]]-based signature schemes, such as [[RSA-PSS]]
*[[Digital Signature Algorithm|DSA]] and its [[elliptic curve cryptography|elliptic curve]] variant [[Elliptic Curve Digital Signature Algorithm|ECDSA]]
*[[EdDSA|Edwards-curve Digital Signature Algorithm]] and its [[EdDSA#Ed25519|Ed25519]] variant.
*[[ElGamal signature scheme]] as the predecessor to DSA, and variants [[Schnorr signature]] and [[Pointcheval&#8211;Stern signature algorithm]]
*[[Rabin signature algorithm]]
*[[Pairing]]-based schemes such as [[Boneh&#8211;Lynn&#8211;Shacham|BLS]]
*[[Undeniable signature]]s
*[[Aggregate signature]] - a signature scheme that supports aggregation: Given n signatures on n  messages from n users, it is possible to aggregate all these signatures into a single signature whose size is constant in the number of users. This single signature will convince the verifier that the n users did indeed sign the n original messages.
*[[Signatures with efficient protocols]] - are signature schemes that facilitate efficient cryptographic protocols such as [[zero-knowledge proofs]] or [[secure computation]].

==The current state of use &amp;ndash; legal and practical ==
{{Globalize|section|date=November 2009}}
All digital signature schemes share the following basic prerequisites regardless of cryptographic theory or legal provision:

#;Quality algorithms: Some public-key algorithms are known to be insecure, as practical attacks against them having been discovered.
#
#; Quality implementations: An implementation of a good algorithm (or [[cryptographic protocol|protocol]]) with mistake(s) will not work.
#
#; Users (and their software) must carry out the signature protocol properly.
#
#; The private key must remain private: If the private key becomes known to any other party, that party can produce ''perfect'' digital signatures of anything whatsoever.
#
#; The public key owner must be verifiable: A public key associated with Bob actually came from Bob. This is commonly done using a [[public key infrastructure]] (PKI) and the public key&#8596;user association is attested by the operator of the PKI (called a [[certificate authority]]). For 'open' PKIs in which anyone can request such an attestation (universally embodied in a cryptographically protected  [[identity certificate]]), the possibility of mistaken attestation is non-trivial. Commercial PKI operators have suffered several publicly known problems. Such mistakes could lead to falsely signed, and thus wrongly attributed, documents. 'Closed' PKI systems are more expensive, but less easily subverted in this way.

Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.

Legislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in [[Utah]] in the United States, followed closely by the states [[Massachusetts]] and [[California]]. Other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time. These enactments (or proposed enactments) vary from place to place, have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying [[cryptographic engineering]], and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable. Adoption of technical standards for digital signatures have lagged behind much of the legislation, delaying a more or less unified engineering position on [[interoperability]], [[algorithm]] choice, [[key length]]s, and so on what the engineering is attempting to provide.

{{see also|ABA digital signature guidelines}}

==Industry standards==
{{unreferenced section|date=January 2015}}
Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the [[Automotive Network Exchange]] for the automobile industry and the [[SAFE-BioPharma Association]] for the healthcare industry.

===Using separate key pairs for signing and encryption===
In several countries, a digital signature has a status somewhat like that of a traditional pen and paper signature, like in the [http://europa.eu/legislation_summaries/information_society/l24118_en.htm EU digital signature legislation].&lt;ref name=Cryptomathic_MajorStandardsDigSig /&gt; Generally, these provisions mean that anything digitally signed legally binds the signer of the document to the terms therein. For that reason, it is often thought best to use separate key pairs for encrypting and signing. Using the encryption key pair, a person can engage in an encrypted conversation (e.g., regarding a real estate transaction), but the encryption does not legally sign every message he sends. Only when both parties come to an agreement do they sign a contract with their signing keys, and only then are they legally bound by the terms of a specific document. After signing, the document can be sent over the encrypted link.  If a signing key is lost or compromised, it can be revoked to mitigate any future transactions.  If an encryption key is lost, a backup or [[key escrow]] should be utilized to continue viewing encrypted content.  Signing keys should never be backed up or escrowed unless the backup destination is securely encrypted.

==See also==
* [[21 CFR 11]]
* [[Blind signature]]
* [[Detached signature]]
* [[Digital certificate]]
* [[Digital signature in Estonia]]
* [[Electronic lab notebook]]
* [[Electronic signature]]
* [[Electronic signatures and law]]
* [[eSign (India)]]
* [[GNU Privacy Guard]]
* [[Global Trust Center]]
* [[PAdES]]
* [[Public key infrastructure]]
* [[Server-based signatures]]

==Notes==
{{Reflist}}

==References==
*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography I: Basic Tools|date=2001|publisher=Cambridge University Press|location=Cambridge|isbn=978-0-511-54689-1}}
*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography II: Basic Applications|date=2004|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=978-0-521-83084-3|edition=1. publ.}}
*{{citation|last1=Pass|first1=Rafael|title=A Course in Cryptography|url=https://www.cs.cornell.edu/courses/cs4830/2010fa/lecnotes.pdf|accessdate=31 December 2015}}

==Further reading==
* J. Katz and Y. Lindell, "Introduction to Modern Cryptography" (Chapman &amp; Hall/CRC Press, 2007)
* Stephen Mason, Electronic Signatures in Law (4th edition, Institute of Advanced Legal Studies for the SAS Digital Humanities Library, School of Advanced Study, University of London, 2016). ISBN 978-1-911507-00-0.
* Lorna Brazell, Electronic Signatures and Identities Law and Regulation (2nd edn, London: Sweet &amp; Maxwell, 2008);
* Dennis Campbell, editor, E-Commerce and the Law of Digital Signatures (Oceana Publications, 2005).
* M. H. M Schellenkens, Electronic Signatures Authentication Technology from a Legal Perspective, (TMC Asser Press, 2004).
* Jeremiah S. Buckley, John P. Kromer, Margo H. K. Tank, and R. David Whitaker, The Law of Electronic Signatures (3rd Edition, West Publishing, 2010).
* [http://journals.sas.ac.uk/deeslr/ ''Digital Evidence and Electronic Signature Law Review''] Free open source

{{Cryptography navbox | public-key}}

{{DEFAULTSORT:Digital Signature}}
[[Category:Public-key cryptography]]
[[Category:Electronic documents]]
[[Category:Key management]]
[[Category:Notary]]
[[Category:Signature]]
[[Category:Records management technology]]</text>
      <sha1>kmvmxqlcysvqr83qc5e1ettxhkhde5b</sha1>
    </revision>
  </page>
  <page>
    <title>Scientific Information Database</title>
    <ns>0</ns>
    <id>40601299</id>
    <revision>
      <id>697042515</id>
      <parentid>675299087</parentid>
      <timestamp>2015-12-27T21:12:28Z</timestamp>
      <contributor>
        <username>Rathfelder</username>
        <id>398607</id>
      </contributor>
      <comment>removed [[Category:Iran]]; added [[Category:Science and technology in Iran]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1332" xml:space="preserve">{{Refimprove|date=February 2014}}
{{Notability|Web|date=December 2013}}
{{Infobox company
| name             = Scientific Information Database, Iran
| type             = [Journal Citation Database]
| location         = Tehran, Iran
| homepage         = [http://www.sid.ir]
| footnotes        = &#1662;&#1575;&#1740;&#1711;&#1575;&#1729; &#1575;&#1591;&#1604;&#1575;&#1593;&#1575;&#1578; &#1593;&#1604;&#1605;&#1740; &#1608; &#1662;&#1688;&#1608;&#1726;&#1588;&#1740;
}}

'''Scientific Information Database''' (or '''SID''') is the Iranian database for the calculation of Persian and English articles citation. It is the like the [[Institute for Scientific Information]] '''ISI''', a local citation counting manager.&lt;ref&gt;{{cite web|url=http://sid.ir |title=Scientific Information Database |publisher=Sid.ir |date= |accessdate=2014-02-03}}&lt;/ref&gt;

==Categories==
This database, does not include just the journal citation reports, it has different categories:
* English Journals, English Journals Database of Iran
* Persian Journals, &#1576;&#1575;&#1606;&#1705; &#1606;&#1588;&#1585;&#1740;&#1575;&#1578; &#1601;&#1575;&#1585;&#1587;&#1740; &#1575;&#1740;&#1585;&#1575;&#1606;
* Research Projects, &#1591;&#1585;&#1581; &#1726;&#1575;&#1740; &#1662;&#1688;&#1608;&#1726;&#1588;&#1740;
* English Scientific Community
* Persian Scientific Community, &#1576;&#1575;&#1606;&#1705; &#1605;&#1580;&#1575;&#1605;&#1593; &#1593;&#1604;&#1605;&#1740; &#1601;&#1575;&#1585;&#1587;&#1740; &#1575;&#1740;&#1585;&#1575;&#1606; 
* Science Centers, &#1576;&#1575;&#1606;&#1705; &#1605;&#1585;&#1575;&#1705;&#1586; &#1593;&#1604;&#1605;&#1740; &#1575;&#1740;&#1585;&#1575;&#1606;

==References==
{{Reflist}}

==External links==
* [http://sid.ir Homepage of SID]

[[Category:Science and technology in Iran]]
[[Category:Citation indices]]</text>
      <sha1>mxs6coorgdapb2osn63cjfm8x62lubp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Telephone directory publishing companies</title>
    <ns>14</ns>
    <id>5154927</id>
    <revision>
      <id>653601083</id>
      <parentid>382986830</parentid>
      <timestamp>2015-03-26T12:56:53Z</timestamp>
      <contributor>
        <username>Stefanomione</username>
        <id>186638</id>
      </contributor>
      <comment>refine category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="142" xml:space="preserve">[[Category:Publishing companies by medium]]
[[Category:Directories]]
[[Category:Directory assistance services]]
[[Category:Telephone numbers]]</text>
      <sha1>a6yk1509m9z06tqy1kqkok2zjy6m94j</sha1>
    </revision>
  </page>
  <page>
    <title>High Weirdness by Mail</title>
    <ns>0</ns>
    <id>3350777</id>
    <revision>
      <id>718909524</id>
      <parentid>718909467</parentid>
      <timestamp>2016-05-06T11:05:20Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <minor />
      <comment>fix of last edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1676" xml:space="preserve">{{italic title}}
'''''High Weirdness By Mail &#8211; A Directory of the Fringe: Crackpots, Kooks &amp; True Visionaries''''', by [[Ivan Stang]] (ISBN 0-671-64260-X) is a 1988 book dedicated to an examination of "weird culture" by actually putting the reader in touch with it by mail.

The book is divided into sections&amp;mdash;"Weird Science," "UFO Contactees," "Drug Stuff," and others, and each section contains a variety of mini-articles describing organizations. Each organization article concludes with a mailing address (and in some cases, phone numbers).

Several years after the book's publication, Stang reported on the [[newsgroup]] [[alt.slack]] that his inclusion of entries for [[white supremacist]] groups in the book caused his name to be mentioned by those groups as a possible target for retaliation.  (The book's commentaries on various [[hate group]]s were less than flattering.)  Stang reported this incident to the [[FBI]], but did not receive any actual harassment or threats from the groups in question.

The [[Association for Consciousness Exploration]] produced a follow-up lecture by Rev. Stang on cassette entitled ''High Weirdness by Mail'', recorded live at the 1993 WinterStar Symposium.

== Controversy ==

[[Bob Black]] claims that his review of ''High Weirdness By Mail'' was the cause of his being sent a small 'prank' mail bomb. [http://www.inspiracy.com/black/bomb.html]

== External links ==
* [http://www.subgenius.com Home Page of the Church of the SubGenius]
* [http://subgenius.com/hwbw.htm The Return of ''High Weirdness by Mail'']

[[Category:1988 books]]
[[Category:Church of the SubGenius]]
[[Category:Directories]]

{{Nonfiction-book-stub}}</text>
      <sha1>bow55i4w320nxxlivlabaac0xmx8vkw</sha1>
    </revision>
  </page>
  <page>
    <title>Vsya Moskva</title>
    <ns>0</ns>
    <id>11017700</id>
    <revision>
      <id>736928670</id>
      <parentid>710445841</parentid>
      <timestamp>2016-08-30T18:57:42Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* top */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 500 to &#8594; between 500 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3782" xml:space="preserve">{{italic title}}
{{unreferenced|date=December 2008}}
'''''Vsya Moskva''''' (literally translated "''All Moscow''" or "''The Entire Moscow''") was a series of [[city directories]] of [[Moscow]], Russia, published on a yearly basis from 1872 to 1936 by [[Aleksei Sergeevich Suvorin]]. 
The directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices,  public services and medium and large businesses present in the city. Each volume was anywhere between 500 and 1500 pages long.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.

==List of residents==
Each directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an [[alphabetical]] list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed. 

The following information can be found:
*Person's surname and first name
*[[Patronymic]]
*Street address with apartment number
*[[Profession]]
*Telephone numbers (few private residents could afford a [[telephone]] before 1918)

==List of occupants of each building on every street and square==
A section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.

==Other sections==
The following information can also be found in each directory:

*Maps of the city
*Interior theater seating plan layouts
*Lists of personnel in state, public and private institutions
*Original Advertising

== Interruption in the series ==

No volumes were published in the following years:
*1918
*1919
*1920
*1921

This was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].

== Termination of series ==

Publication came to a halt after the edition of 1936, coinciding with the time of [[Joseph Stalin]]'s [[great purge]]s and [[Moscow Trials]].

==Historical and genealogical value==
Because numerous residents emigrated from Moscow after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city, and under which address.

==Availability==
Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the [[United States]], [[Europe]] (including [[The Baltics]], [[Finland]] the [[United Kingdom]] and [[Germany]]) however most only have an incomplete collection.

==Other city directories in Russia==
Suvorin also published city directories for [[Saint Petersburg]] under the title ''[[Ves Petersburg]]'' (''All Petersburg'') for the years 1894 to 1940 and for the whole country under the titles ''[[Vsya Rossiya]]'' (''All Russia'') from 1895 to 1923 and continued under than name ''[[Ves SSSR]]'' (''All USSR'') from 1924 to 1931.

== See also ==

*''[[Ves Petersburg]]''
*''[[Vsya Rossiya]]''

== External links ==
*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]

[[Category:Directories]]
[[Category:History of Moscow]]
[[Category:Russian non-fiction books]]
[[Category:Media in Moscow]]
[[Category:1872 books]]</text>
      <sha1>t82br6q6jpd9qhmgwcpyd3telkoelj0</sha1>
    </revision>
  </page>
  <page>
    <title>Yellowikis</title>
    <ns>0</ns>
    <id>1430812</id>
    <revision>
      <id>755476314</id>
      <parentid>749201096</parentid>
      <timestamp>2016-12-18T07:11:57Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Legal issues */clean up; http&amp;rarr;https for [[The Guardian]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5987" xml:space="preserve">{{Infobox Website
| name = Yellowikis
| favicon =
| logo = [[Image:Yelwiki.png]]
| screenshot =
| url = [http://yellowikis.wikia.com/wiki/Main_Page yellowikis.wikia.com]
| commercial =
| alexa =
| type = [[MediaWiki]]
| registration = Optional
| owner = [[Wikia]]
| author =
| launch date = 2005
| current status = Inactive
| revenue =
}}
'''Yellowikis''' was a [[MediaWiki]] [[website]] collecting basic information about businesses. This information included basic contact details such as company name, address, websites, and telephone numbers, as well as internal Yellowiki [[wikilink]]s to competitors. Yellowikis was launched in January 2005. {{As of|2011|3}}, the Yellowikis main page had been translated into more than 25 different languages.{{citation needed|date=April 2015}}

Some users also entered a number of codes including a two letter country code as well as an [[International Standard Industrial Classification]], [[North American Industry Classification System|North American Industry Classification]] or US [[Standard Industrial Classification]]. Some users are also adding [[geocode]]s and [[Skype]] ids.

==Legal issues==
A commercial business listing company, [[Yell Limited]], requested that the founders of Yellowikis, Paul Youlten and Rosa Blaus, amend their site, claiming that Yellowikis was "passing itself off" as being associated with Yell.com and that people would confuse the two organisations.&lt;ref&gt;{{cite news
 | title =Legal threat to wiki listing site
 | work =BBC News
 | date = 12 July 2006
 | url =http://news.bbc.co.uk/1/hi/technology/5169674.stm
 | accessdate =2006-07-12 }}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Teenager faces action over listings website|author=Bobbie Johnson|date=2006-08-02|work=[[The Guardian]]|url=https://www.theguardian.com/uk_news/story/0,,1835233,00.html|publisher=Guardian News and Media Ltd | location=London}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Yell threatens to sue wiki rival|author=Jane Hoskyn|work=vnunet.com|date=2006-07-14|url=http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|publisher=VNU Business Publications Ltd|archiveurl=https://web.archive.org/web/20070930195439/http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|archivedate=2007-09-30}}&lt;/ref&gt; This might be considered to be anti-competitive behaviour/anti-competitive in the eyes of certain commentators, however, such claim is unlikely to carry water from a legal perspective. Yell's claim is given considerable weight by the slogan on Yellowiki's front page that they are "Yellow Pages for the 21st Century" although in their public protestations, Yellowikis claim that they are not trying to create association between themselves and Yellow Pages.&lt;ref&gt;{{cite web
 | last =The Yellowikis Community
 | title =Response to Yell
 | work =
 | publisher =Yellowikis
 | year =2006
 | url =http://www.yellowikis.org/wiki/index.php/Response_to_Yell
 | accessdate =2006-07-14 }}&lt;/ref&gt;

[[Yellow Pages]] is a registered [[trade mark]] in many countries including the UK. In some territories, however, the mark has lost its distinctiveness as a source of origin of goods and services.

From 9 to 14 October 2006, the domain address redirected to the new [http://www.owikis.org.uk/ Owikis] website, which stated "The trademark dispute between Yell Limited and Paul Youlten concerning the Yellowikis website has been satisfactorily resolved".

On 15 October 2006, the Yellowikis website reappeared, with the explanation that [[United Kingdom]] users would have to use Owikis, with the word ''Yell'' from the domain name and the color yellow from the logo; international users could continue to use Yellowikis. {{As of|2008|5}}, the Owikis site is not yet available.

As of at least May 2014, the [http://yellowikis.wikia.com/wiki/Main_Page Wikia page] is dead.&lt;ref&gt;{{cite web|url=http://yellowikis.wikia.com/wiki/Main_Page |title=Web Archive |accessdate=2014-05-28 |deadurl=yes |archiveurl=https://web.archive.org/web/20140109015130/http://yellowikis.wikia.com/wiki/Main_Page |archivedate=January 9, 2014 }}&lt;/ref&gt;

==References==
{{Reflist}}

==Further reading==
* {{cite news|url=http://www.researchbuzz.org/2005/06/business_information_in_wiki_f.shtml|title=Business Information in Wiki Format|date=2005-06-22|publisher=ResearchBuzz}}
* {{cite news|url=http://competia.com/competia_w/site/fiche/1954|title=Yellowikis|date=2005-07-26|publisher=Competia}}
* {{cite web|url=http://alina_stefanescu.typepad.com/totalitarianism_today/2005/05/a_wiki_worth_wa.html|title=A wiki worth watching|work=totalitarianism today|accessdate=2005-10-07}}
* {{cite web|url=http://www.stabani.com/index.php?s=yellowikis|title=Why I think Yellowikis is a good idea |work=site spotlight|accessdate=2006-01-16|author=S.Tabani}}
* [[n:Emily Chang|Emily Chang]] {{cite web|url=http://www.emilychang.com/go/ehub/|title=Emily Chang's eHub|work=eHub|accessdate=2005-10-09}} 
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=58|title=Yellowikis - A Case Study of a Web 2.0 Business, Part 1|date=2005-10-15}}
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=59|title=Yellowikis: a Web 2.0 Case Study, Part 2 - Industry Disruption and The Competition|date=2005-10-16}}
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=62|title=Yellowikis: Demonstrating Web 2.0 principles|date=2005-10-17}}
* {{cite news|url=http://news.independent.co.uk/media/article1096343.ece|title=New Media: Who are the real winners now we've all gone Wiki-crazy?|date=2006-06-26|publisher=[[The Independent]] | location=London | first=Kathy | last=Marks}}
* [[n:Yell threatens to shut down Yellowikis|Yell threatens to shut down Yellowikis]] from [[Wikinews]], 2006-07-05

==External links==
* [http://yellowikis.wikia.com/wiki/Main_Page Yellowikis] on [[Wikia]]

[[Category:Directories]]
[[Category:MediaWiki websites]]
[[Category:Internet properties established in 2005]]
[[Category:Yellow pages]]</text>
      <sha1>pan7bs6i0qeoc850nipblbja5ztr7fl</sha1>
    </revision>
  </page>
  <page>
    <title>Artists' Bluebook</title>
    <ns>0</ns>
    <id>19928728</id>
    <revision>
      <id>659247876</id>
      <parentid>578214597</parentid>
      <timestamp>2015-04-26T05:37:15Z</timestamp>
      <contributor>
        <username>GoodDay</username>
        <id>589223</id>
      </contributor>
      <comment>per [[WP:BOLDTITLE]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="706" xml:space="preserve">{{primary sources|date=October 2013}}
The '''Artists' Bluebook''' is an international [[database]] of over 270,000 visual artists developed by AskART since 1999 (http://www.askart.com/AskART/help/AskART_about_us.aspx). Revised from its original 1993 print and CD format to digital online access, the Artists' Bluebook is considered a favorite resource for research into artists' lives, artworks and values, and where to buy or sell.

==External links==
* [http://www.askart.com/AskART/index.aspx The Artists' Bluebook website]
* [http://www.ala.org/rusa/sections/mars/marspubs/marsbestfreewebsites/marsbestref2003 AskART Bluebook 2003 - review by American Library Association(ALA)]
[[Category:Directories]]</text>
      <sha1>suuag61hctndf2cdhsnpqsbobs5xu3h</sha1>
    </revision>
  </page>
  <page>
    <title>Ves Peterburg</title>
    <ns>0</ns>
    <id>11002640</id>
    <revision>
      <id>736631806</id>
      <parentid>710445647</parentid>
      <timestamp>2016-08-28T20:33:16Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* top */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 500 to &#8594; between 500 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5217" xml:space="preserve">{{italic title}}
'''''Ves Peterburg''''' (/v&#690;es&#690; p&#690;&#618;t&#690;&#618;r&#712;burg/, Literally translated "''All Petersburg''" or "''The Entire Saint Petersburg''"
") (Full name in [[cyrillic]] "Ves Petersburg; Adresnaja i spravo&#269;naja kniga g. Petersburga") (often referred to as the ''Suvorin directories'' from the publisher's name) was the title of a series of [[city directory|city directories]] of [[Saint Petersburg]], [[Russia]] published on a yearly basis from 1894 to 1940 by [[Aleksei Sergeevich Suvorin]]. Each volume was anywhere between 500 and 1500 pages long. After changes in the name of the city the directories were called '''''Ves Petrograd''''' from 1914 to 1923 and '''''Ves Leningrad''''' from 1924 to 1940.

The directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices, public services and medium and large businesses present in the city.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.

== List of residents of St. Petersburg ==

Each directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an alphabetical list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed.

The following information can be found:
*Person's surname and first name
*[[Patronymic]]
*Street address with apartment number
*Profession
*Telephone numbers (only appear sparingly as few private residents could afford a telephone before 1918)

== List of occupants of each building on every street and square ==

A section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.

== Other sections ==

The following information can also be found in each directory

*information on the royal family
*Maps of the city
*cultural establishments (with interior theatre hall layouts and seating plans)
*Lists of personnel in state, public and private institutions
*information on academic institutions of all ranks
*information on churches and monasteries of St. Petersburg
*Original commercial advertisements of Russian and foreign companies which had offices in St. Petersburg

== Historical and genealogical value ==

Because numerous residents emigrated from Saint Petersburg after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city.

== Interruption in the series ==

No volumes were published in the following years:
*1918
*1919
*1920
*1921

This was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].

The edition of 1922 was very concise and only contained details of businesses in the city but not residents.

== Termination of series ==

Publication came to a halt after the edition of 1935, coinciding with the time of [[Joseph Stalin]]'s [[great purge]]s and [[Moscow Trials]]. The only further volumes were issued in 1939 and 1940, but these (like the edition in 1922) only contained details of state run businesses and public and governmental offices, but not residents.

== Availability ==

Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including [[The Baltic]], Finland the United Kingdom and Germany) however most only have an incomplete collection. The [[Russian National Library]] in Saint Petersburg has a complete run of all volumes published available.

== Other city directories ==

Suvorin also published city directories for [[Moscow]] under the title ''[[Vsia Moskva]]'' (All Moscow) for the years 1875 to 1936 and for the whole country under the titles ''[[Vsia Rossiia]]'' (All Russia) continued under than name ''[[Ves SSSR]]'' (All USSR) from 1924 to 1931.

Since 1993 a telephone directory under the title "Ves Petersburg" has been published annually by the publishing House Presskom but this is vastly different in content then the original directories and does not list residents.

== Sources ==

http://www.encspb.ru/en/article.php?kod=2804017249
Ves Peterburg - http://www.allinform.ru

==See also==

*''[[Vsia Moskva]]''
*''[[Vsia Rossiia]]''

== External links ==
*[http://www.nlr.ru Official website of the Russian National Library in Saint Petersburg]
*[http://surname.litera-ru.ru/ A russian website offering a search engine in cyrillic for some city directories.]

[[Category:Directories]]
[[Category:History of Saint Petersburg]]
[[Category:Russian non-fiction books]]
[[Category:Media in Saint Petersburg]]
[[Category:1894 books]]</text>
      <sha1>o9acvk45ghyemaapiac19k69eg201ad</sha1>
    </revision>
  </page>
  <page>
    <title>City directory</title>
    <ns>0</ns>
    <id>30018447</id>
    <revision>
      <id>709117334</id>
      <parentid>695750919</parentid>
      <timestamp>2016-03-09T06:40:23Z</timestamp>
      <contributor>
        <username>Except</username>
        <id>1116146</id>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1661" xml:space="preserve">{{Multiple issues|
{{Refimprove|date=July 2015}}
{{Citation style|date=July 2015}}
}}

A '''city directory''' is a listing of residents, streets,  businesses, organizations or institutions, giving their location in a [[city]].  Antedating [[telephone directories]], they have been in use for centuries.

Examples include [[Kelly's Directory]] and the [[Boston Directory]].

==See also==
* [[:de:Adressbuch]]

==References==
{{refbegin}}
*{{cite journal |title=How Reliable is the Modern City Directory? | volume= 30| issue =2| pages =154&#8211;158|date=June 1986 |journal=Canadian Geographer |author=Richard Harris, Ben Moffat |doi=10.1111/j.1541-0064.1986.tb01040.x}}
*{{cite web |url=http://www.ancestry.com/learn/library/article.aspx?article=4062 |title= City vs. Telephone Directories |work=[[Ancestry.com]] |author=George G. Morgan}}
*{{cite book 
|author=A. V. Williams 
|title=The Development and Growth of City Directories 
|publisher=
|location=Cincinnati
|year=1913 
|url=http://catalog.hathitrust.org/Record/008698693
}}
*{{cite book |author=Florence May Hopkins |title=Reference Guides that Should be Known and how to Use Them: Atlases; City Directories; Gazetteers  |publisher=The Willard Company |location= |year=1919  |pages= |isbn= |url=https://books.google.com/books?id=SPEVAAAAIAAJ |doi= |accessdate=}}
{{refend}}

==Further reading==
* {{citation |title=Direct Me NYC 1786: A History of City Directories in the United States and New York City |author= Philip Sutton |year=2012 |publisher=New York Public Library |url= http://www.nypl.org/blog/2012/06/08/direct-me-1786-history-city-directories-US-NYC |work=NYPL Blogs }}

[[Category:Directories]]</text>
      <sha1>8jkeuy27kze5xz5kcwus8cosqewrzwc</sha1>
    </revision>
  </page>
  <page>
    <title>Who Owns Whom</title>
    <ns>0</ns>
    <id>24167579</id>
    <revision>
      <id>748145834</id>
      <parentid>722514951</parentid>
      <timestamp>2016-11-06T16:18:38Z</timestamp>
      <contributor>
        <username>W w smith</username>
        <id>29497539</id>
      </contributor>
      <minor />
      <comment>fixed broken link (404) for external reference "gapbooks"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2736" xml:space="preserve">'''''Who Owns Whom''''' is a set of annual directories published by GAP Books in association with [[Dun &amp; Bradstreet]] (D&amp;B).  They provide the relationship between companies worldwide showing who is the ultimate [[parent company]] and who are their [[subsidiaries]].  Details include parent name, address and telephone number, country of incorporation and [[SIC code]] for each ultimate parent company along with the names of the subsidiaries, where they are based, and who owns whom for each subsidiary.  The set of directories are broken down into seven geographic regions: UK &amp; Ireland; West Europe; North Europe; South, Central &amp; East Europe; North &amp; South America; Australasia, Asia, Africa &amp; Middle East.

''Who Owns Whom'' was first published in 1958 by D&amp;B, who still own the rights to the data.  Within the set of ten directories are listed approximately 2,500,000 [[corporate groups]], ranging from companies with hundreds of members across all continents to single-country groups with only a handful of members.

==Criteria for entry==
In order to maintain full coverage, all entries in ''Who Owns Whom'' are published entirely [[free of charge]].  Every effort is made by D&amp;B, who supply the data, to include all corporate groups within the coverage area.  The corporate group may be a vast [[multinational corporation|multinational]] with a range of subsidiaries spanning many different countries and industries, or it may consist of two companies &#8211; a parent and a subsidiary.  There is no size criterion for entry.  All corporate groups, which come to the attention of D&amp;B, are included regardless of their [[revenue|sales turnover]] or number of employees.  Companies and organisations are only included if ownership is greater than 50% by other companies or organisations.

==Coverage==
All types of industries are covered, including [[agriculture]], [[forestry]] and [[fishing]], [[mining]], [[construction]], [[manufacturing]], [[transportation]], [[communication]] and [[public utilities]], [[wholesale]], [[retail]], [[finance]] and [[insurance]], [[public services|services]] and [[public administration]].  Both public and private companies are covered, along with many companies owned by official bodies such as governments, nationalised industries and state holding companies which have subsidiaries, but are not themselves independently registered.

==External links==
* http://library.dialog.com/bluesheets/html/bl0522.html
* http://www.gapbooks.com/shop/dandb-who-owns-whom/
* http://www.loc.gov/rr/business/duns/duns30.html

[[Category:Directories]]
[[Category:Yearbooks]]
[[Category:Corporate subsidiaries]]
[[Category:Books about multinational companies]]


{{globalization-book-stub}}
{{Ref-book-stub}}</text>
      <sha1>5hf7q2c4idjaa7kp1y3o3ips4aapoye</sha1>
    </revision>
  </page>
  <page>
    <title>Almanach de Gotha</title>
    <ns>0</ns>
    <id>747726</id>
    <revision>
      <id>749887621</id>
      <parentid>749885832</parentid>
      <timestamp>2016-11-16T18:05:41Z</timestamp>
      <contributor>
        <username>LouisAlain</username>
        <id>14909828</id>
      </contributor>
      <comment>link to new article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22637" xml:space="preserve">{{Multiple issues|
{{refimprove|date=July 2014}}
{{primary sources|date=July 2014}}
}}
{{italic title}}
{{Use dmy dates|date=June 2013}}	
{{Infobox book series
| name             = Almanach de Gotha
| image            =  DeGotha1851.jpg
| image_caption    = The Almanach de Gotha  1851
| books            = 
| author           =
| editors          = 
| title_orig       = 
| translator       = 
| illustrator      = 
| cover_artist     = 
| country          =
| language         =
| genre            =
| discipline       =
| publisher        = [[Johann Christian Dieterich|J.C. Dieterich]]&lt;br&gt;C.W. Ettinger&lt;br&gt;C.G. Ettinger&lt;br&gt;[[Justus Perthes (publishing company)|Justus Perthes]]&lt;br&gt;Almanach de Gotha, Ltd.
| pub_date         = 1763-1944&lt;br&gt;1998-
| english_pub_date = 1998-
| media_type       = 
| number_of_books  = 
| list_books       = 
| preceded by      = 
| followed by      = 
}}

The '''''Almanach de Gotha''''' was a directory of Europe's [[Royal family|royalty]] and higher [[nobility]], also including the major [[government]]al, [[military]] and [[diplomatic corps|diplomatic]] [[corps]], as well as statistical data by country. First published in 1763 by C.W. Ettinger in [[Gotha (town)|Gotha]] at the [[duke|ducal]] [[court]] of [[Frederick III, Duke of Saxe-Gotha-Altenburg|Frederick III]], Duke of [[Saxe-Gotha-Altenburg]], it came to be regarded as an authority in the classification of monarchies and their courts, reigning and former dynasties, princely and ducal families, and the genealogical, biographical and titulary details of Europe's highest level of aristocracy. It was published from 1785 annually by [[Justus Perthes (publishing company)|Justus Perthes]] Publishing House in Gotha, until 1944 when the [[Soviet]]s destroyed the ''Almanach de Gotha's'' archives.

In 1998, a London-based publisher acquired the rights for use of the title of ''Almanach de Gotha'' from Justus Perthes Verlag Gotha GmbH. Perthes regard the resultant volumes as new works, and not as a continuation of the editions which Perthes had published from 1785 to 1944.&lt;ref name=Perthes&gt;{{Cite web|url=http://www.perthes.de/geschichte_justus_perthes/almanach_de_gotha/almanach_de_gotha_english.html |title=Almanach de Gotha |accessdate=9 June 2008 |publisher=Justus Perthes }}&lt;/ref&gt; Two volumes have been printed since 1998, with Volume I containing lists of the sovereign, formerly sovereign and mediatised houses of Europe, and a diplomatic and statistical directory; and Volume II containing lists of the non-sovereign princely and ducal houses of Europe.

==Gotha publication, 1763&#8211;1944==
The original ''Almanach de Gotha'' provided detailed facts and statistics on nations of the world, including their [[reign]]ing and formerly reigning houses, those of [[Europe]] being more complete than those of other continents. It also named the highest incumbent [[Great Officer of State (disambiguation)|officers of state]], members of the [[diplomatic corps]], and Europe's upper nobility with their families. Although at its most extensive the ''Almanach'' numbered more than 1200 pages, fewer than half of which were dedicated to monarchical or aristocratic data,&lt;ref name="gotha"&gt;Almanach de Gotha. [[Justus Perthes]], Gotha, 1944, pp. 7-12, 131, 169, 363-364, 558, 581-584. French.&lt;/ref&gt; it acquired a reputation for the breadth and precision of its information on royalty and nobility compared to other [[almanac]]s.&lt;ref name="diesbach"&gt;{{Cite book|title=Secrets of the Gotha|last=de Diesbach|first=Ghislain|authorlink = Ghislain de Diesbach|year=1967|publisher=Chapman &amp; Hall|location=UK|pages=21, 23&#8211;24, 28&#8211;30}}&lt;/ref&gt;
[[File:London Library book, Gothaisches Genealogisches Taschenbuch der Freiherrlichen H&#228;user, 1910, Justus Perthes, Gotha.jpg|thumb|[[London Library]]'s copy of ''Gothaisches Genealogisches Taschenbuch der Freiherrlichen H&#228;user'', 1910.]]
The ''Almanach'''s publication by [[Justus Perthes]] began at the ducal court of [[Saxe-Coburg and Gotha]] in Germany and, its reigning dynasty was listed first therein well into the 19th century, usually followed by kindred sovereigns of the [[House of Wettin]] and then, in alphabetical order, other families of princely rank, ruling and non-ruling. Although always published in French, other almanacs in French and English were more widely sold internationally. The almanac's structure changed and its scope expanded over the years. The second portion, called the ''Annuaire diplomatique et statistique'' ("Diplomatic and Statistical Yearbook"), provided [[demography|demographic]] and governmental information by nation, similar to other [[almanac]]s. Its first portion, called the ''Annuaire g&#233;n&#233;alogique'' ("Genealogical Yearbook"), came to consist essentially of three sections: reigning and formerly reigning families, [[mediatization|mediatized families]] and non-sovereign families at least one of whose members bore the title of prince or duke.&lt;ref name="diesbach"/&gt;

The first section always listed Europe's [[sovereignty|sovereign]] houses, whether they ruled as emperor, king, grand duke, duke, prince (or some other title, e.g., [[prince elector]], [[margrave]], [[landgrave]], [[count palatine]] or [[pope]]). Until 1810 these sovereign houses were listed alongside such families and entities as Barbiano-Belgiojoso, Clary, Colloredo, Furstenberg, the Emperor, Genoa, Gonzaga, Hatzfeld, Jablonowski, Kinsky, Ligne, Paar, Radziwill, Starhemberg, Thurn and Taxis, Turkey, Venice and the [[Order of Malta]] and the [[Teutonic Knights]]. In 1812, these entries began to be listed in groups.&lt;ref name="diesbach"/&gt; First were German sovereigns who held the rank of grand duke or prince elector and above (the Duke of Saxe-Gotha was, however, listed here along with, but before, France&#8212;see below).

Listed next were Germany's reigning ducal and princely dynasties under the heading "College of Princes", e.g., [[Hohenzollern]], [[County of Isenburg|Isenburg]], [[Leyen]], [[Liechtenstein]] and the other [[Ernestine duchies|Saxon duchies]]. They were followed by heads of non-German monarchies, i.e. Austria, Brazil, Great Britain, etc. Fourthly were listed non-reigning dukes and princes, whether mediatized or not, including [[Arenberg]], [[House of Cro&#255;|Croy]], [[F&#252;rstenberg (princely family)|Furstenberg]] alongside [[Batthyany]], [[Jablonowski]], [[Sulkowski]], Porcia and [[Prince of Benevento|Benevento]].

In 1841 a third group was added to those of the sovereign dynasties and the non-reigning princely and ducal families. It was composed exclusively of the mediatized families of comital rank recognized as belonging, since 1825, to the same historical category and sharing some of the same privileges as reigning dynasties by the various states of the [[German Confederation]]; these families were German with a few exceptions (e.g. [[Bentinck]], [[Van Rechteren|Rechteren-Limpurg]]). The 1815 treaty of the [[Congress of Vienna]] had authorized &#8212; and Article 14 of the German Confederation's ''Bundesakt'' (charter) recognized &#8212; retention from the [[Holy Roman Empire|German Imperial]] regime of [[Royal intermarriage|equality of birth]] for marital purposes of mediatized families (called ''Standesherren'') to reigning dynasties.&lt;ref name="diesbach"/&gt; The almanac added a third section consisting exclusively of mediatized families of comital rank.

In 1877, the mediatized comital families were moved from section III to section II A, where they joined the princely mediatized families. For the first time in the century of its existence, the largely non-German, un-mediatized princely and ducal families of the ''Almanach de Gotha'' were removed from the same section as other non-reigning families bearing princely titles.&lt;ref name="diesbach"/&gt; While non-mediatized German and Austrian families (e.g. [[Prince Lichnowsky|Lichnowsky]], [[Wrede]]), were likewise relocated from the almanac's second to its third section, the second section's new preponderance of German families, princely and comital, which were henceforth recognized as possessing the exclusive privilege of inter-marriage with reigning dynasties was salient:&lt;ref name="diesbach"/&gt; Excluded were members of such historically notable families as the [[House of Rohan|Rohan]]s, [[Orsini]]s, [[Duke of Ursel|Ursels]], [[Duke of Norfolk|Norfolks]], [[Czartoryski]]s, [[Galitzine]]s, [[Duc de La Rochefoucauld|La Rochefoucaulds]], [[House of Kinsky|Kinskys]], [[Radziwi&#322;&#322; family|Radziwills]], [[De M&#233;rode|Merodes]], [[Dohna (Disambiguation)#People|Dohnas]] and [[Duke of Alba|Albas]].

Although theoretically mediatized families were distinguished from Europe's other nobility by the former status of their territories as ''[[Imperial State|Reichsstand]]'' and their exercise within the Holy Roman Empire of "semi-sovereignty" or [[imperial immediacy]] (''Reichsunmittelbarkeit''), many ''Standesherr'' families, especially those bearing the [[count|comital]] title, had not been fully recognized as legally possessing immediate status within the Empire prior to its collapse in 1806. No other families whose highest title was count were admitted to any section of the almanac.&lt;ref name="diesbach"/&gt;

Moreover, other [[deposition (politics)|deposed]] European dynasties (e.g. [[House of Arenberg|Arenberg]], [[Ernst-Johann Biron, Prince of Courland|Biron]], [[Dadiani]], [[Boncompagni]]-[[Ludovisi (family)|Ludovisi]], [[Giray dynasty|Giray]], [[House of Murat|Murat]]) did not benefit ''vis-a-vis'' the almanac from a similar interpretation of their historical status. Many princely or ducal families were listed only in its third, non-dynastic section or were excluded altogether, evoking criticism in the 20th century from such genealogists as [[Cyril Toumanoff]], [[Jean-Engelbert d'Arenberg|Jean-Engelbert, Duke d'Arenberg]] and [[William Addams Reitwiesner]],&lt;ref&gt;Fra Cyril Toumanoff, "Genealogical Imperialism" (1985) vol 6 (no 134) (NS) Coat of Arms pp. 145, 147.&lt;/ref&gt;&lt;ref&gt;Duke and Prince Jean Engelbert d'[[Arenberg]], "The Lesser Princes of the Holy Roman Empire in the Napoleonic Era" dissertation, Washington, DC, 1950, published as Les Princes du St-Empire &#224; l'&#233;poque napol&#233;onienne (Louvain, 1951) 15ff, quoted in Almanach de Gotha (Almanach de Gotha, London, 1998) pp. 275&#8211;286.&lt;/ref&gt; the latter commenting that the changes displayed "pan-German triumphalism" and even a "fairly nasty bit of Germanic chauvinism."&lt;ref&gt;{{Cite web|url= http://www.wargs.com/essays/mediatize.html |title= Mediatization |accessdate= 19 April 2011 |last= Reitwiesner |first= William Addams |date=January 1998 |authorlink= William Addams Reitwiesner}}&lt;/ref&gt;

Even in the early 19th century the almanac's retention of [[deposition (politics)|deposed]] dynasties evoked objections, although not necessarily the desired changes. The elected Emperor [[Napoleon]] protested in writing to his foreign minister, [[Jean-Baptiste Nomp&#232;re de Champagny|Champagny]]: &lt;blockquote&gt;''Monsieur de Champagny, this year's "Almanach de Gotha" is badly done. First comes the Comte de Lille [title used in exile by [[Louis XVIII of France|Louis de Bourbon, Count of Provence]] -- future King Louis XVIII of France], followed by all the princes of the [[Confederation of the Rhine|Confederation]] as if no change has been made in the constitution of Germany; the family of France is named inappropriately therein. Summon the Minister of Gotha, who is to be made to understand that in the next Almanach all of this is to be changed. The House of France must be referred to as in the [French] Imperial Almanac; there must be no further mention of the Comte de Lille, nor of any German prince other than those retained by the Articles of Confederation of the Rhine. You are to insist that the article be transmitted to you prior to publication. If other almanacs are printed in my allies' realms with inappropriate references to the Bourbons and the House of France, instruct my ministers to make it known that you have taken note, and that this is to be changed by next year.''&lt;ref&gt;{{Cite book|url= https://books.google.com/books?id=ScM3AQAAMAAJ&amp;pg=PA124&amp;dq=Napol%C3%A9on+13275+Champagny&amp;hl=en&amp;sa=X&amp;ei=W7-5U8PHMpHooATfkYCoBg&amp;ved=0CCEQ6AEwAA#v=onepage&amp;q=Napol%C3%A9on%2013275%20Champagny&amp;f=false |title= Correspondance de Napol&#233;on I|volume=XVI |publisher=Imprimerie Imp&#233;riale|date=1864|location=France|accessdate=6 July 2014}}&lt;/ref&gt;&lt;/blockquote&gt;

The response of the publishers was to humour Napoleon by producing two editions: one for France, with the recently ennobled, and another which included dynasties deposed since abolition of the [[Holy Roman Empire]]. A merged version, whose first section including recently reigning dynasties but also families which lost sovereignty after the fall of Napoleon in 1815, remained in publication until 1944, and has been replicated in subsequent dynastic compilations (e.g., ''Genealogisches Handbuch des Adels, F&#252;rstliche H&#228;user'', ''Le Petit Gotha'', Ruvigny's "Titled Nobility of Europe").

In 1887 the ''Almanach'' began to include non-European dynasties in its first section, with the inclusion of one of the ruling families of India.

===World War II and aftermath===
When Soviet troops entered [[Gotha (town)|Gotha]] in 1945, they systematically destroyed all archives of the ''Almanach de Gotha''.{{Citation needed|date=September 2010}}

In 1951 a different publisher, C.A. Starke, began publication of a multi-volume German-language publication entitled the ''Genealogisches Handbuch des Adels'' ([[:de:Genealogisches Handbuch des Adels|GHdA]]). The publication is divided into subsets; the ''F&#252;rstliche H&#228;user'' subset is largely equivalent to the German language ''Gothaischer Hofkalender'' and its ''F&#252;rstlichen H&#228;user'' volume which was also published by Perthes, or sections 1, 2 and 3 of the ''Almanach de Gotha''. However, no single volume of the ''F&#252;rstliche H&#228;user'' includes all the families included in the ''Hofkalender'' or ''Almanach de Gotha''. It is necessary to use multiple volumes to trace the majority of European royal families.

==London publication, since 1998==
[[File:2014 Almanach de Gotha Covers.jpg|right|200px|thumb|''Almanach de Gotha'', 2014, Volumes I &amp; II]]

In 1989 the family of [[Justus Perthes]] re-established its right to the use of the name ''Almanach de Gotha''. The family then sold these rights in 1995 to a new company, Almanach de Gotha Limited, formed in London.&lt;ref&gt;[https://www.thegazette.co.uk/notice/L-60158-1601227 Notice of Disclaimer]&lt;/ref&gt; The new publishers launched with the 182nd edition on 16 March 1998 at [[Claridge's Hotel]].&lt;ref&gt;{{Cite web|url= http://www.almanachdegotha.com/site/modern.htm|title=The Modern Gotha |accessdate=30 May 2008 |publisher=Almanach de Gotha |archiveurl=https://web.archive.org/web/20060211154624/http://www.almanachdegotha.com/site/modern.htm |archivedate=11 February 2006}}&lt;/ref&gt;&lt;ref&gt;Jury, Louise. [http://www.independent.co.uk/news/upper-crust-toasts-aristocrat-studbook-1150076.html Upper crust toasts aristocrat studbook] The Independent (14 March 1998)&lt;/ref&gt;  It was written in English instead of French as the editor felt that English was now the language of diplomacy.&lt;ref name=Runciman&gt;{{Cite news|first= Steven |last= Runciman |authorlink=Steven Runciman |title=The first book of kings |url=http://findarticles.com/p/articles/mi_qa3724/is_199805/ai_n8792875 |publisher=[[The Spectator]] |date=2 May 1998 |accessdate=6 June 2008 }}&lt;/ref&gt; Charlotte Pike served as editor of the 1998 edition only and John Kennedy as managing director and publisher. The new publishers also revived the Committee of Patrons under the presidency of King [[Juan Carlos I of Spain]] and chairmanship of King [[Michael I of Romania]].&lt;ref&gt;{{Cite web|url= http://www.gotha1763.com/society.html|title=The Soci&#233;t&#233; des Amis de l'Almanach de Gotha |accessdate=1 May 2014 |publisher=Almanach de Gotha}}&lt;/ref&gt;

The London publisher produced a further four editions of volume I (1999, 2000, 2003 and 2004) based on the 1998 edition of volume I which include Europe's and South America's reigning, formerly reigning, and mediatised princely houses, and a single edition of volume II in 2001 edited by John Kennedy and Ghislain Crassard which include other non-sovereign princely and ducal houses of Europe.&lt;ref name=Hardman&gt;{{Cite news|first=Robert |last=Hardman |title=Family almanac will unmask the noble pretenders |url=http://www.telegraph.co.uk/news/worldnews/europe/germany/1317982/Family-almanac-will-unmask-the-noble-pretenders.html |publisher=[[Daily Telegraph]] |date=19 June 2001 |accessdate=6 June 2008 }}&lt;/ref&gt; A review in ''[[The Economist]]'' criticised the low editorial standards and attacked volume II for a lack of genealogical accuracy.&lt;ref name=review_economist&gt;{{Cite web| title=The Almanach de Gotha -- Gothic horror | url=http://www.economist.com/books/displayStory.cfm?Story_ID=949183 | publisher=[[The Economist]] | date = 24 January 2002 | accessdate=7 October 2007}}&lt;/ref&gt; After a gap of eight years a new edition of volume I was published in 2012 under the editorship of John James.&lt;ref&gt;{{Cite web|url=http://www.boydellandbrewer.com/store/viewitem.asp?idproduct=13798 |title=Almanach de Gotha 2012. Volume I, parts I &amp; II |accessdate=8 February 2012 |publisher=[[Boydell &amp; Brewer]] }}&lt;/ref&gt; A review in ''[[The Times Literary Supplement]]'' praised the 2012 volume I for a "punctilious itemization of titles, lineage and heraldry [aiming] for scholarship rather than sensation...Some family legends&amp;nbsp;&#8211; such as the Ottoman boast of descent from a grandson of Noah&amp;nbsp;&#8211; do not merit inclusion in a work with authoritative aspirations. Most quixotically of all, the title page displays the word 'Annual', although it has been eight years since the last edition appeared."&lt;ref&gt;{{Cite web|url=http://www.the-tls.co.uk/tls/public/article1139358.ece |title=And dark the Sun and Moon, and the Almanach de Gotha... |accessdate=2013-01-17 |publisher=The Times Literary Supplement |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20130117050945/http://www.the-tls.co.uk/tls/public/article1139358.ece |archivedate=17 January 2013 |df=dmy }}&lt;/ref&gt;

==Structure==
As it was the practice of the diplomatic corps to employ official titles, adhere to local [[Order of precedence|precedence]] and etiquette, and to tender congratulations and condolences to members of the dynasty of the nation to which they were assigned, the almanac included a ''Calendrier des Diplomates'' ("Diplomats' Calendar") section, which detailed major national holidays, anniversaries, ceremonies and royal birthdates.&lt;ref name="gotha"/&gt;

Following [[World War I]] and the fall of many [[royal house]]s, fewer regulatory authorities remained to authenticate use of titles; however the ''Almanach de Gotha'' continued the practice of strict verification of information, requesting certified copies of [[letters patent]], genealogies confirmed by competent authorities, documents, decrees and references for titles claimed.&lt;ref name="gotha"/&gt; Europe's middle and lower nobility (families whose principal title ranked below that of prince or duke&amp;nbsp;&#8212; except [[German mediatisation|mediatized]] families, listed in a section of their own) were not included in the almanac. Nor were the [[grandee]]s or [[Portuguese dukedoms|ducal families]] of Portugal and Spain (where titles, being habitually transmissible through both male and [[cognatic|female lines]], were often inherited by relatives of non-[[patrilineality|patrilineal]] lineage). Families of some Italian and East European nations (e.g., Russia, Romania), where the princely title was claimed by many, were also incomplete. Yet the reigning, formerly reigning and noble families included in the almanac numbered in the hundreds by the time it ceased publication in 1944.&lt;ref name="gotha"/&gt;

In 1890 the almanac renamed II A to section II, and II B to section III. Dynasties ruling non-European nations were located in section I B. Families which became extinct were listed for the final time in the year following death of the last member, male or female, and subsequent editions referred readers to that volume.&lt;ref name="gotha"/&gt;

Families that ceased to be included for other reasons, such as lack of proof of a family's legitimate descendants or discovery that it did not hold a valid princely or ducal title, were henceforth excluded but added, along with dates of previous insertion, to a list following the last section of each ''Annuaire Genealogique'' (Genealogical Yearbook), which page was entitled ''Liste des Maisons authrefois publiees dans la 3e partie de l'Almanach de Gotha'' ("List of Houses formerly published in the 3rd section of the ''Almanach de Gotha''.") &lt;ref name="gotha"/&gt;

From 1927, the almanac ceased to include all families in each year's edition, henceforth rotating entries every few years. Where titles and [[style (manner of address)|style]]s (such as [[Serene Highness]]) had ceased to be recognized by national governments (e.g. Germany, Austria, Czechoslovakia), the almanac provided associated dates and details, but continued to attribute such titles and styles to individuals and families, consistent with its practice since the [[French revolution]]; deposed sovereigns and dynasties continued to be accorded their former titles and rank, but dates of deposition were noted,&lt;ref name="diesbach"/&gt; and titles exclusively associated with sovereignty (e.g. emperor, queen, grand duke, crown princess) were not accorded to those who had not borne them during the monarchy. Titles of [[pretender|pretence]] below sovereign rank were accorded to members of formerly reigning dynasties as reported by heads of their houses, otherwise self-assumed titles were not used. The almanac included an explicit disclaimer announcing that known biographical details, such as birthdates and divorces, would not be suppressed.&lt;ref name="gotha"/&gt;

==See also==
*[[Burke's Peerage]]
*[[Debrett's|Debrett&#8217;s Peerage &amp; Baronetage]]

==References==
{{Reflist|2}}

==Further reading==
*[[Ghislain de Diesbach|Diesbach, Ghislain de]]. ''Secrets of the Gotha''. Meredith Press, 1964.

==External links==
*[http://gallica.bnf.fr/ Scanned versions of the old almanachs]
*[https://archive.org/search.php?query=almanach+de+gotha%20AND%20mediatype%3Atexts Almanach de Gotha at Internet Archive]
*[http://www.gotha1763.com/ Almanach de Gotha]

{{DEFAULTSORT:Almanach De Gotha}}
[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Publications established in 1763]]
[[Category:Almanacs]]</text>
      <sha1>bkxw2iiltuxunrru1tb3u026etq5wdn</sha1>
    </revision>
  </page>
  <page>
    <title>R.L. Polk &amp; Company</title>
    <ns>0</ns>
    <id>20018380</id>
    <revision>
      <id>760215654</id>
      <parentid>759123719</parentid>
      <timestamp>2017-01-15T17:32:28Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* Company history */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: south Dakota &#8594; South Dakota using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10507" xml:space="preserve">{{Advert|date=July 2009}}
{{Infobox company
|name = R.L. Polk &amp; Company
|logo =
|type = Acquired by [[IHS Inc.]]
|foundation = [[Detroit, MI]] {1870}
|location_city =  [[Southfield, Michigan|Southfield]], MI
|area_served = Worldwide
|founder = [[Ralph Lane Polk]]
|key_people = Stephen R. Polk
* Chairman, President and CEO
Tim Rogers
* President, Polk
Richard Raines
* President, CARFAX
Michelle Goff
* Senior Vice President/Chief Financial Officer
|homepage = [http://www.polk.com www.polk.com]
|industry = Automotive
}}

'''R. L. Polk &amp; Company''' is a provider of [[automotive]] information and marketing [[solution]]s to the automotive industry, insurance companies, and related businesses.&lt;ref name="usa.polk.com"&gt;[http://usa.polk.com/Company/WhoWeAre/ R. L. Polk &amp; Company :: Our Company :: Who We Are :: Index] Retrieved on 10/31/08  {{webarchive |url=https://web.archive.org/web/20080702225115/http://usa.polk.com/Company/WhoWeAre/ |date=July 2, 2008 }}&lt;/ref&gt;

Polk was acquired by [[IHS Inc.]] on July 15, 2013 &lt;ref&gt;http://press.ihs.com/press-release/corporate-financial/ihs-completes-acquisition-rl-polk-co&lt;/ref&gt; and is based in Southfield, Michigan with operations in several countries, including the United States, Canada, Germany, United Kingdom, France, Japan, China and Australia.&lt;ref name="usa.polk.com"/&gt;

==Company history==
[[Image:1880 spine Illinois State Gazetteer by Polk &amp; Co.png|thumb|100px|left|Polk's ''Illinois State Gazetteer'', 1880]]
R. L. Polk &amp; Company was founded by [[Ralph Lane Polk]] in 1870 in Detroit, MI as a publisher of business directories. In 1872, the company first published a City Directory, for Evansville, Indiana, plus a listing of post offices in nine states. Additional directories followed in the ensuing years as the business grew.&lt;ref name=heritage&gt;[http://usa.polk.com/Company/Heritage/ R.L. Polk : Heritage]  {{webarchive |url=https://web.archive.org/web/20091229120305/http://usa.polk.com/Company/Heritage/ |date=December 29, 2009 }}&lt;/ref&gt; claiming 1000 directories by 1960.&lt;ref&gt;{{cite book|title=Polk's Abilene (Taylor County, Texas) City Directory, 1960|date=1960|publisher=R. L. Polk &amp; Co|page=7|url=http://texashistory.unt.edu/ark:/67531/metapth160223/m1/7/|accessdate=27 September 2014}}&lt;/ref&gt;  Affiliates included the Polk-Husted Directory Co. of Oakland, California.&lt;ref&gt;{{cite book |url=https://books.google.com/books?id=TNlKAQAAIAAJ&amp;pg=PA547 |title=Polk's San Jose City and Santa Clara County Directory |year=1907 }}&lt;/ref&gt; In addition to city directories, the company published bank directories.

In 1907, R.L. Polk &amp; Co. was publishing a "[[Gazetteer]]" Business directory for the State of Michigan and Windsor and Walkerville Ontario, as well as gazetteers for Alaska, Arkansas, California, Idaho, Illinois, Oklahoma, Indiana, Iowa, Kansas, Kentucky, Maryland, Minnesota, North Dakota, South Dakota, Montana, Missouri, Nevada, Oregon, Washington State, Pennsylvania, Tennessee, Texas, Utah, West Virginia, and Wisconsin.&lt;ref&gt;{{cite book|title=Michigan State Gazetteer and Business Directory|date=1907|publisher=R.L. Polk &amp; Co.|location=Detroit|page=2|edition=1907-1908|url=https://books.google.com/books?id=absfAQAAMAAJ&amp;lpg=PA250&amp;dq=%22manitou%22%20steamship%20charlevoix&amp;pg=PA81#v=onepage&amp;q=%22manitou%22%20steamship%20charlevoix&amp;f=false|accessdate=7 June 2016}}&lt;/ref&gt;

In 1921, a conversation between Ralph Lane Polk II and [[Alfred P. Sloan]] (who later became president of General Motors) helped fuel R. L. Polk &amp; Company's entry into the automotive industry. During the conversation, Sloan asked Polk to impartially tabulate and publish statistical information on cars and trucks in operation. R.L. Polk &amp; Company launched its motor vehicle statistical operations in 1922, when the first car registration reports were published.&lt;ref&gt;http://web.archive.org/web/20071116145915/http://www.salesforce.com/customers/business-services/case-studies/rlpolk.jsp Retrieved on 11/4/08&lt;/ref&gt; In 1922, R.L. Polk &amp; Co. published its first Passenger Car Registration Report, covering 58 makes and accounting for 9.2 million passenger automobiles on America's highways.

From 1951 to 1958, the company pioneered the use of electronic punch card tabulating equipment. In 1956, Polk's reporting services included monthly statistics on boats, business aircraft, motorcycles, commercial trailers, and recreational vehicles. In 1976, the National Vehicle Population Profile (NVPP) was introduced.

===1990s===
In 1993, Polk combined their Canadian activities with Blackburn Marketing Services to form Blackburn / Polk Marketing Services Inc. (BPMSI).  Polk also acquired a 35% interest in CARFAX from Blackburn Marketing Services.  In 1995, Polk entered an alliance with Marketing Systems GmBH and acquired a substantial minority interest in The Ultimate Perspective (T.U.P).

In 1996, Polk completed acquisition of the Blackburn / Polk operations and renamed it Polk Canada Marketing Services Inc. (PCMSI).  This acquisition unified and strengthened their North American operations in Polk's strategy to be a global information services provider.  They also announced their first Automotive Loyalty Award winners.

In 1997, Polk acquired the MSS division of Automatic Data Processing's European Operations.

In 1999, Polk completed acquisition of CARFAX and sold Advertising Unlimited, Inc. to Norwood Promotional Products.

===2000 and Beyond===
In 2000, Polk sells its Consumer Information Solutions (CIS) business units Direct Marketing, Data Information Services / Polk Verity, City Directory, and the Compusearch and Prospects Unlimited units of Polk Canada to Equifax.

Polk launches Garage Predictors and Polk Canada, Inc. announces Polk Canada Net. Polk also completes its acquisition of Marketing Systems Group.

Ralph Lane Polk II is inducted into the prestigious Automotive Hall of Fame (AHF) located in Dearborn, Michigan in 2001. Stephen R. Polk is also a part of the AHF as a director&lt;ref&gt;http://ias.net/ahof/v1n3/ Retrieved on 11/5/08&lt;/ref&gt; and R. L. Polk &amp; Co. is also considered a Sapphire Level Supporter.&lt;ref&gt;http://ias.net/ahof/v1n3/ Retrieved 11/5/08&lt;/ref&gt;

In 2002, Polk launches the Polk Vehicle Lifecycle System and the Polk Cross Sell is introduced.

Also in 2002, Ralph Lane Polk II is inducted into The Direct Marketing Association (DMA) Hall of Fame, the highest professional honor in direct and interactive marketing. DMA inducts into "The Hall of Fame" as many as four individuals each year for the significant impact these leaders have had on the growth of the direct and interactive process.&lt;ref&gt;http://www.the-dma.org/awards/halloffame.shtml Retrieved on 11/4/08&lt;/ref&gt;

In 2003, PolkInsight is launched. Polk Total Market Predictor (Polk TMP) is also introduced.

In 2004, R. L. Polk &amp; Company launches Polk Cross Sell Report and RLPTechnologies, a new wholly owned subsidiary, is established. Also, The [[Software Engineering Institute|Software Engineering Institute (SEI)]] awards R. L. Polk &amp; Company with a Level II Capability Maturity Model Integrated (CMMI) rating.

In 2005, R. L. Polk &amp; Company introduces the Polk Inventory Efficiency Award. The Polk Inventory Efficiency Award recognizes and rewards outstanding aftermarket companies for process improvements relative to inventory efficiency.&lt;ref&gt;http://www.reuters.com/article/pressRelease/idUS147801+21-May-2008+PRN20080521 Retrieved 11/14/08&lt;/ref&gt;

In 2007, R. L. Polk &amp; Co. acquire a majority interest in ROADTODATA, a rapidly growing supplier of automotive price and specifications data.&lt;ref&gt;http://japan.polk.com/News/LatestNews/R.+L.+Polk+and+ROADTODATA+Merge.htm Retrieved 12/26/08&lt;/ref&gt;

In 2010, R. L. Polk &amp; Company partners with Citytwist.&lt;ref&gt;https://www.ihs.com/Customer/citytwist-auto-excellence-award.html&lt;/ref&gt;

In 2013, IHS, Inc announced a $1.4B purchase of R.L. Polk.&lt;ref&gt;http://www.mlive.com/auto/index.ssf/2013/06/information_company_ihs_to_pur.html&lt;/ref&gt;

The company's business-to-business marketing services include PolkInsight, the National Vehicle Population Profile (NVPP), Blackburn / Polk Marketing Services Inc. (BPMSI), Polk Dealer Marketing Manager,&lt;ref&gt;http://google.com/search?q=cache:2uEMeAtCgckJ:findarticles.com/p/articles/mi_hb6674/is_/ai_n26650183+polk+and+Marketing+Systems+GmBH&amp;hl=en&amp;ct=clnk&amp;cd=7&amp;gl=us Retrieved on 11/4/08&lt;/ref&gt; The Ultimate Perspective (T.U.P), Polk Canada Net, Polk Vehicle Lifecycle System, Polk CrossSell Reports,&lt;ref&gt;http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&amp;STORY=/www/story/01-27-2004/0002097303&amp;EDATE= Retrieved on 11/4/08&lt;/ref&gt; and Polk Total Market Predictor (Polk TMP).{{Citation needed|date=July 2009}}

==CARFAX==

The Polk Company announced on August 2, 1999 that it had completed acquisition of [[Carfax (company)|Carfax]]. Polk had previously owned 35 percent of Carfax, in partnership with the Blackburn Group, Inc., of London, Ontario, [[Canada]], and has now acquired the remaining 65 percent.&lt;ref name="theautochannel.com"&gt;http://www.theautochannel.com/articles/press/date/19990802/press027618.html Retrieved 11/7/08&lt;/ref&gt; Carfax compiles vehicle histories from various sources, with about 75 percent of the information coming from Polk data.  Using the [[vehicle identification number]] (VIN), each history provides potential buyers with all available facts about a used car being considered for purchase.  This may include original use of the vehicle odometer records, number of owners, and other items that might affect a purchase decision.&lt;ref name="theautochannel.com"/&gt;

==See also==
* [[St. Louis City Directories]]

==References==
{{reflist}}

==Further reading==
* {{cite book |url=https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 |title=Directory of Directories |publisher=R.L. Polk &amp; Co. |location=NY |year=1916 }}

==External links==
{{commons category|R.L. Polk &amp; Co.}}
* Internet Archive. [https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 Works published by R.L. Polk &amp; Co.], various dates
* Hathi Trust. [http://catalog.hathitrust.org/Search/Home?checkspelling=true&amp;lookfor=%22polk+%26+co%22&amp;type=publisher&amp;sethtftonly=true&amp;submit=Find Works related to R.L. Polk &amp; Co.], various dates
* OCLC WorldCat. [http://www.worldcat.org/search?q=au%3A%22polk+%26+co Works related to R.L. Polk &amp; Co.], various dates

{{DEFAULTSORT:Polk and Co.}}
[[Category:Companies based in Detroit]]
[[Category:Directories|polk]]
[[Category:Publishing companies established in 1870]]
[[Category:American companies established in 1870]]</text>
      <sha1>h2kgxkh3oh7kj9d8byt0qp6k7s50c7s</sha1>
    </revision>
  </page>
  <page>
    <title>Polk's Directory</title>
    <ns>0</ns>
    <id>39021356</id>
    <redirect title="R.L. Polk &amp; Company" />
    <revision>
      <id>564060306</id>
      <parentid>548870957</parentid>
      <timestamp>2013-07-13T04:16:38Z</timestamp>
      <contributor>
        <username>AvocatoBot</username>
        <id>14893258</id>
      </contributor>
      <minor />
      <comment>Robot: Fixing double redirect to [[R.L. Polk &amp; Company]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="164" xml:space="preserve">#REDIRECT [[R.L. Polk &amp; Company]]

[[Category:Histories of cities in the United States]]
[[Category:Publications established in the 1870s]]
[[Category:Directories]]</text>
      <sha1>8vy9c7etmdq3c25rtpxl3m3k9d0g16l</sha1>
    </revision>
  </page>
  <page>
    <title>Search.ch</title>
    <ns>0</ns>
    <id>661012</id>
    <revision>
      <id>595895732</id>
      <parentid>573133340</parentid>
      <timestamp>2014-02-17T16:48:53Z</timestamp>
      <contributor>
        <username>Wizardman</username>
        <id>713860</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2014" xml:space="preserve">'''search.ch''' is a [[search engine]] and [[web portal]] for [[Switzerland]]. It was founded in 1995 &lt;ref name="founding" /&gt; by Rudolf R&#228;ber and Bernhard Seefeld as a regional search engine. In the following years many other services were added, such as a phonebook in 1999, a free [[Short message service|SMS]] service in 2000 (now reduced to only one free SMS per week).

The search technology is home grown. The user can restrict his search to regions of Switzerland, such as a [[cantons of Switzerland|canton]] or a [[city]]. The [[web crawler]] looks only at sites in the [[.ch]] and [[.li]] [[top-level domain]]s and a number of automatically and manually updated list of Swiss websites on other domains. The index is updated weekly.

== External links ==
* [http://www.search.ch/ search.ch]
* [http://tel.search.ch/ tel.search.ch] phonebook
* [http://map.search.ch/ map.search.ch] Swiss maps
* [http://meteo.search.ch/ meteo.search.ch] Swiss weather
* [http://news.search.ch/ news.search.ch] Swiss news
* [http://timetable.search.ch/ timetable.search.ch] Swiss public transport timetable
* [http://tv.search.ch/ tv.search.ch] Swiss TV programme
* [http://kino.search.ch/ kino.search.ch] Swiss cinema programme
* [http://sms.search.ch/ sms.search.ch] sms service
* [http://immo.search.ch/ immo.search.ch] immo portal search service
* [http://www.post.ch/ Swiss Post] &lt;ref name="post" /&gt; Swiss Post acquired search.ch
* [http://www.tamedia.ch Tamedia] &lt;ref name="tamedia" /&gt; Tamedia akquired a 75% stake from Swiss Post

==References==
{{Reflist|refs=
&lt;ref name="founding"&gt;http://www.moneyhouse.ch/en/u/search_ch_ag_CH-130.0.009.911-2.htm&lt;/ref&gt;
&lt;ref name="tamedia"&gt;http://about.search.ch/archives/2004/06/04/post-kauft-search-ch/&lt;/ref&gt;
&lt;ref name="post"&gt;http://www.post.ch/post-startseite/post-konzern/post-medien/post-archive/2009/post-mm09-fruehzustellung/post-medienmitteilungen.htm&lt;/ref&gt;
}}

{{DEFAULTSORT:Search.Ch}}
[[Category:Web portals]]
[[Category:Internet search engines]]
[[Category:Directories]]</text>
      <sha1>2t0l1ccf53l5btxwjpu8l263t8f9nso</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Indexes</title>
    <ns>14</ns>
    <id>1789509</id>
    <revision>
      <id>743809874</id>
      <parentid>724690762</parentid>
      <timestamp>2016-10-11T11:24:02Z</timestamp>
      <contributor>
        <username>Fayenatic london</username>
        <id>1639942</id>
      </contributor>
      <comment>update link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="329" xml:space="preserve">An '''index''' is something that points the reader to other [[information]]. This category is for articles about indexes.
{{distinguish|Category:Index numbers}}
{{Cat more|Index (publishing)}}
{{For|Wikipedia content|Category:Wikipedia indexes}}

[[Category:Index (publishing)]]
[[Category:Publications]]
[[Category:Directories]]</text>
      <sha1>ms524i5me7c29vucrr4zfk3445w38vr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Library cataloging and classification</title>
    <ns>14</ns>
    <id>7117782</id>
    <revision>
      <id>604572664</id>
      <parentid>548841861</parentid>
      <timestamp>2014-04-17T09:39:00Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="269" xml:space="preserve">{{Commons cat|Library cataloging and classification}}

{{Cat main|Library catalog|Library classification|Inventory (library)}}

[[Category:Library science|Cataloging and classification]]
[[Category:Classification systems]]
[[Category:Metadata]]
[[Category:Directories]]</text>
      <sha1>isgvmea7ts839xupwzemczbrqb31l7c</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Public records</title>
    <ns>14</ns>
    <id>20252940</id>
    <revision>
      <id>746188490</id>
      <parentid>740552358</parentid>
      <timestamp>2016-10-25T20:18:39Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Administration]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="133" xml:space="preserve">{{Cat main|Public records}}

[[Category:Directories]]
[[Category:Documents]]
[[Category:Government information]]
[[Category:Privacy]]</text>
      <sha1>7don096yp4sjbvgj6rplj68qdwgpviz</sha1>
    </revision>
  </page>
  <page>
    <title>Business directory</title>
    <ns>0</ns>
    <id>1725756</id>
    <revision>
      <id>739439273</id>
      <parentid>739437627</parentid>
      <timestamp>2016-09-14T18:04:03Z</timestamp>
      <contributor>
        <username>Jmuldrow</username>
        <id>29175695</id>
      </contributor>
      <minor />
      <comment>/* Formats */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2104" xml:space="preserve">{{unreferenced|date=June 2009}}

[[File:PigotDirectory1839Kent.jpg|thumb|An example page from [[Pigot's Directory|Pigot's 1839 directory]] of businesses in the counties of Kent, Surrey and Sussex in England.]]
A '''business directory''' is a website or [[print media|printed]] listing of [[information]] which lists all businesses within some category. Businesses can be categorized by business, location, activity, or size. Business may be compiled either manually or through an automated online search software.  Online [[yellow pages]] are a type of business directory, as is the traditional [[phone book]].

The details provided in a business directory varies from business to business. They may include the business name, addresses, telephone numbers, location, type of service or products the business provides, number of employees, the service region and any [[professional association]]s. Some directories include a section for user reviews, comments, and feedback. Business directories in the past would take a printed format but have recently been upgraded to websites due to the advent of the internet.

Many business directories offer complimentary listings in addition to the premium options. There are many business directories and some of these have moved over to the [[internet]] and away from printed format. Whilst not being [[search engine]]s, business directories often have a search facility.

== Formats ==
Business directories can be in either [[hard copy]] or in [[Digital formats|digital format]]. Ease of use and distribution means that many trade directories have digital version.

Online Business Directories vary in quality and content. There is a balance between professional advertising, value for money and quality of service. Business owners are looking for ROI, web traffic, exposure for their business, plus [[Search engine optimization|SEO]] benefits of [[Backlink|backlinks]].

==See also==
*[[Web directory]]
*[[Kelly's Directory]]
*[[Surplus Record Machinery &amp; Equipment Directory]]

{{DEFAULTSORT:Business Directory}}
[[Category:Business]]
[[Category:Directories]]</text>
      <sha1>mwn912yxn3y72oscdvuidyfntws2ear</sha1>
    </revision>
  </page>
  <page>
    <title>Index Herbariorum</title>
    <ns>0</ns>
    <id>44490466</id>
    <revision>
      <id>733795450</id>
      <parentid>733273595</parentid>
      <timestamp>2016-08-10T04:53:52Z</timestamp>
      <contributor>
        <username>Rpyle731</username>
        <id>46515</id>
      </contributor>
      <minor />
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1748" xml:space="preserve">{{Italic title}}
The '''Index Herbariorum''' provides a global directory of [[herbaria]] and their associated staff. This searchable online index allows scientists rapid access to data related to 3,400 locations where a total of 350&amp;nbsp;million botanical [[Biological specimen|specimens]] are permanently housed (singular, [[herbarium]]; plural, herbaria). The Index Herbariorum has its own staff and website. Overtime, six editions of the Index were published from 1952 to 1974. The Index became available on-line in 1997.&lt;ref name=IH&gt;{{cite web|url=http://sciweb.nybg.org/science2/IndexHerbariorum.asp|title=Index Herbariorum|publisher=sciweb.nybg.org|accessdate=2014-11-23}}&lt;/ref&gt;

The index was originally published by the [[International Association for Plant Taxonomy]], which sponsored the first six editions (1952&#8211;1974); subsequently the [[New York Botanical Garden]] took over the responsibility for the index. The Index provides the supporting institution's name (often a university, botanical garden, or not-for-profit organization) its city and state, each herbarium's acronym, along with contact information for staff members along with their research specialties and the important holdings of each herbarium's collection.

==Editors==
*6th edition (1974)  was co-edited by [[Patricia Holmgren]], Director of the  New York Botanical Garden, and
*7th printed edition ed. by  Patricia Holmgren. 
*8th printed editions, ed. by  Patricia Holmgren.
*Online edition, prepared by Noel Holmgren of the New York Botanical Garden
*2006+, ed. by Barbara M. Thiers, Director of the New York Botanical Garden  Herbarium &lt;ref name=IH /&gt;
&lt;ref name=IH /&gt;

==References==
{{Reflist}}

[[Category:Directories]]
[[Category:Herbaria]]


{{botany-stub}}</text>
      <sha1>9ea1tojfo5gtq1kr36j10jmmljdtmum</sha1>
    </revision>
  </page>
  <page>
    <title>Western Australia Post Office Directory</title>
    <ns>0</ns>
    <id>6013482</id>
    <revision>
      <id>700698936</id>
      <parentid>650122944</parentid>
      <timestamp>2016-01-20T02:23:14Z</timestamp>
      <contributor>
        <username>Kerry Raymond</username>
        <id>300735</id>
      </contributor>
      <comment>added [[Category:Postal system of Australia]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4474" xml:space="preserve">{{Use Australian English|date=March 2015}}
{{Use dmy dates|date=March 2015}}

The '''''Western Australia Post Office Directory''''', also known as ''Wise Directories'' or ''Wise Street Directories'' were published in Perth from 1893-1949.

They were published by H. Pierssen&#233;&lt;ref&gt;{{Citation | author1=Pierssen&#233;, Herbert | title=The Western Australian directory | publication-date=1893 | publisher=H. Pierssene | url=http://trove.nla.gov.au/work/28621466 | accessdate=6 March 2015 }}&lt;/ref&gt; and later by H. Wise &amp; Co.&lt;ref&gt;{{Citation | author1=Wise &amp; Co | title=Wise's Western Australia post office directory | publication-date=1938 | publisher=H. Wise &amp; Co. Pty Ltd | url=http://trove.nla.gov.au/work/19293522 | accessdate=6 March 2015 }}&lt;/ref&gt;  They listed household, business, society, and Government contacts in [[Perth]], [[Freemantle]], [[Kalgoorlie, Western Australia|Kalgoorlie]], [[Boulder, Western Australia|Boulder]] and [[Coolgardie, Western Australia|Coolgardie]] including some rural areas of [[Western Australia]].

==Publishers==
The ''Western Australian Directory'' was published by H. Pierssene between 1893-1895. Herbert Pierssene was a merchant and importer of English Continental and Ceylonese goods. He was an agent for McCulluch Carrying Company and a bottler of West Australian wines.&lt;ref&gt;{{cite web|title=Thomas Herbert Pierssen&#233;|url=http://www.territorystories.nt.gov.au/handle/10070/244383|website=Territory Stories|publisher=Northern Territory Department of Arts and Museums|accessdate=6 March 2015}}&lt;/ref&gt;

The ''Western Australia Post Office Directory'' was published by Wise &amp; Co. between the years 1895-1949 with the exception of 1943 and 1948.

==Wise Directories== 	
The directories provide information by locality, individual surname, government service, and by trade or profession. The addresses of householders and businesses throughout Western Australia are included.&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article77356821 |title=POST-OFFICE DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=27 April 1909 |accessdate=6 March 2015 |page=2 Edition: THIRD EDITION |publisher=National Library of Australia}}&lt;/ref&gt;  Maps were sometimes published with an edition of the directory.&lt;ref&gt; {{cite news |url=http://nla.gov.au/nla.news-article77329156 |title=WESTERN AUSTRALIA DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=5 March 1908 |accessdate=6 March 2015 |page=6 Edition: THIRD EDITION |publisher=National Library of Australia}}&lt;/ref&gt;  The towns section of the directories normally contained separate street directories of Perth and suburbs, Fremantle and Suburbs, Kalgoorlie, Boulder and Coolgardie.&lt;ref&gt;{{Citation | author1=Wise's Directories | author2=Archive CD Books Australia | title=Western Australia Post Office directory (Wise's) 1905 | publication-date=2004 | publisher=Archive CD Books Australia | isbn=978-1-920978-23-5 }}&lt;/ref&gt;

Known colloquially to users and  book collectors as 'Wise Directories' or 'Wise Street Directories' the red covered directories were published between 1893 and 1949.  Due to the annual changes, the directories are valuable historical documents for Western Australian History.  They are scarce in the Australian rare book market.  

The directories have been invaluable referent points for such projects as the [[Dictionary of Western Australians]] and others where the street lists in the directory provide details of inhabitants and houses in some streets in the more built-up residential areas.  Country towns in the directory have name lists only. 

They have been available in microfilm form in [[J S Battye Library]], and more recently have become online (see link below) in one of the J S Battye Library digitization projects.

==References==
{{reflist}}

==External links==
* http://www.slwa.wa.gov.au/find/wa_resources/post_office_directories

==See also==
* [[Australia Post]]
*[[Australian Dictionary of Biography]]
*[[Cyclopedia of Western Australia]]
*[[Dictionary of Australian Biography]]
*[[J S Battye Library]]
*[[State Records Office of Western Australia]]

[[Category:Books about Western Australia]]
[[Category:History of Western Australia|Western Australia Post Office Directory]]
[[Category:Australian directories]]
[[Category:Directories]]
[[Category:Gazetteers]]
[[Category:Postal system of Australia]]</text>
      <sha1>h0aob8t3n29rpvhgwo26zs3sc8zn40s</sha1>
    </revision>
  </page>
  <page>
    <title>Yachting Pages</title>
    <ns>0</ns>
    <id>47332933</id>
    <revision>
      <id>733274543</id>
      <parentid>733274518</parentid>
      <timestamp>2016-08-06T17:05:10Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2247" xml:space="preserve">{{Use dmy dates|date=September 2013}}
{{Use British English|date= September 2013}}
{{Infobox company
| logo     = [[File:Yachting Pages Logo Black KNOCKOUT.svg]]
| foundation       = [[Antibes]], [[France]] (2003)
| location         = [[Bristol]], [[Somerset]]
[[United Kingdom]]
| key_people       = 
| num_employees    = 34 (2013)
| industry         = [[Superyacht]]
| homepage         = {{url|www.yachting-pages.com/}}
}}
'''''Yachting Pages''''' is a [[superyacht]] business with a range of products aimed at [[Captain (nautical)|captains]] and [[crew]], [[shipyards]], refit yards and all within the superyacht industry. ''Yachting Pages'' is available either in its original printed form, or online. Since the first edition of ''Yachting Pages'' was released in 2004, the book has grown rapidly into an established superyacht [[Trade directory|directory]].

The annual print directory is available in three separate editions:

*''Mediterranean, Europe, Africa &amp; Middle East''
*''USA, the Americas &amp; Caribbean''
*''Australasia, Asia Pacific &amp; Far East''

The [[Port]] Maps section at the front of every edition totals over 350 detailed maps of the world's superyacht [[marinas]]. Copies of the printed directory are hand delivered free of charge directly to superyachts by uniformed crew, and also to superyacht marinas and land-based superyacht businesses in over 92 countries.&lt;ref&gt; Yachtingpages.com &lt;/ref&gt;

'''The Company'''

''Yachting Pages'' was founded in May 2003 from current CEO Steve Crowe's spare bedroom in [[Antibes]], [[France]], with only one other member of staff. The company is now based in [[Bristol]], [[United Kingdom]] with 34 staff members, many of whom are [[multi-lingual]]. 

The first copy of ''Yachting Pages'' was launched at the Genoa Charter Show, in May 2004. 

Since then, growth of the business has created more products: ''Yachtingpages.com, Yachting Pages Refit, Yachting Pages Delivers and Superyacht Owners' Guide (SYOG).'' 

'''Awards'''

Queens award for Enterprise: International Trade 2009. 
EADP European B2B Award 2009 

==References==
{{Reflist}}
{{refimprove|date=August 2013}}

==External links==
* [http://www.yachting-pages.com/ Yachting Pages]

[[Category:Directories]]
[[Category:Yachting]]</text>
      <sha1>alt2ut4u4fjgoth4586atb3daryb4nw</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Internet search engines</title>
    <ns>14</ns>
    <id>699876</id>
    <revision>
      <id>666714753</id>
      <parentid>666713139</parentid>
      <timestamp>2015-06-13T03:52:22Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>-Category:Data search engines (redundant)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="335" xml:space="preserve">{{Commons category|Internet search engines}}
General [[search engine (computing)|search engine]]s that search for information on the [[Internet]]. 

[[Category:Websites|Search engines]]
[[Category:Internet search]]
[[Category:Online databases]]
[[Category:Indexes]]
[[Category:Aggregation websites]]
[[Category:Search engine software]]</text>
      <sha1>hnpjlqhdvf7bl1lb4vllx0k7m91ej7n</sha1>
    </revision>
  </page>
  <page>
    <title>Prospective search</title>
    <ns>0</ns>
    <id>3345817</id>
    <revision>
      <id>687750579</id>
      <parentid>687750527</parentid>
      <timestamp>2015-10-27T14:30:41Z</timestamp>
      <contributor>
        <ip>90.29.27.135</ip>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2268" xml:space="preserve">{{one source|date=April 2014}}
'''Prospective search''', or '''persistent search''', is a method of [[Search engine technology|searching]] on the [[Internet]] where the query is given first and the information for the results are then acquired. This differs from traditional, or "retrospective", search such as [[search engines]], where the information for the results is acquired and then queried.&lt;ref name=globalpr2005&gt;{{cite web|url=http://www.globalprblogweek.com/2005/09/21/wyman-reputation-management/|title=Blogs &amp; Prospective Search Technology for Corporate Reputation Management|year=2005|author=Bob Wyman|publisher=Global PR Blog Week website}}&lt;/ref&gt;

== Comparison to retrospective search ==
Retrospective search starts by gathering the information, indexing it, then letting users query the information. The results don't change until the index is rebuilt, often months apart. Prospective search starts with the user's queries, gathers the information in a targeted way, indexing it and then providing the results as they arrive. Sometimes [[Ping blog|Ping Servers]] are used to gather notification of changes to websites so that the information received is as fresh as possible. Users can be notified in a number of ways of new results.

Prospective search is well suited to queries where the results change over time, such as the current news, [[blog]]s and trends.

== See also ==
* [[PubSub]]
* [[Google Alerts]]
* Google AppEngine Prospective Search Service&lt;ref&gt;https://code.google.com/appengine/docs/python/prospectivesearch/&lt;/ref&gt; (deprecated as of December 1st 2015&lt;ref&gt;https://cloud.google.com/appengine/docs/deprecations/prospective_search?hl=en&lt;/ref&gt;)
* [[Selective dissemination of information]]
* [[Superfeedr]] ('tracker' API&lt;ref&gt;http://blog.superfeedr.com/full-text-trackers/&lt;/ref&gt;)

== Quotes ==
{{quote|Prospective search is emerging as a way of keeping up-to-date on any subject of interest. This technology constantly monitors relevant blogs and Web feeds for matches to users&#8217; subscriptions and delivers results in real time. Thus, users are notified whenever something new appears on their subject of choice|Global PR Blog Week&lt;ref name=globalpr2005/&gt;}}

==References==
{{reflist}}

[[Category:Internet search]]


{{compu-prog-stub}}</text>
      <sha1>lwz4e9pua2z3eo15528hftnpyolxa7n</sha1>
    </revision>
  </page>
  <page>
    <title>VisualRank</title>
    <ns>0</ns>
    <id>17303714</id>
    <revision>
      <id>684863454</id>
      <parentid>684863392</parentid>
      <timestamp>2015-10-09T07:06:32Z</timestamp>
      <contributor>
        <ip>125.22.103.70</ip>
      </contributor>
      <comment>/* Methods */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3277" xml:space="preserve">'''VisualRank''' is a system for [[image retrieval|finding]] and ranking images by analysing and comparing their content, rather than searching image names, Web links or other text.  [[Google]] scientists made their VisualRank work public in a paper describing applying [[PageRank]] to Google image search at the International World Wide Web Conference in [[Beijing]] in 2008.
&lt;ref name=Jing08&gt;
{{cite journal
 | author = Yushi Jing and Baluja, S.
 | title = VisualRank: Applying PageRank to Large-Scale Image Search
 | journal = Pattern Analysis and Machine Intelligence, IEEE Transactions on
 | year = 2008
 | volume = 30
 | number = 11
 | pages = 1877&#8211;1890
 | ISSN = 0162-8828
 | doi = 10.1109/TPAMI.2008.121}}.
&lt;/ref&gt;

&lt;blockquote&gt;
We cast the image-ranking problem into the task of identifying "authority" nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images. The images found to be "authorities" are chosen as those that answer the image-queries well. 
&lt;/blockquote&gt;

==Methods==
Both [[computer vision]] techniques and [[locality-sensitive hashing]] (LSH) are used in the VisualRank [[algorithm]].  Consider an image search initiated by a text query.  An existing search technique based on image metadata and surrounding text is used to retrieve the initial result candidates ([[PageRank]]), which along with other images in the index are clustered in a [[Graph (data structure)|graph]] according to their similarity (which is precomputed).  [[Centrality]] is then measured on the clustering, which will return the most canonical image(s) with respect to the query.  The idea here is that agreement between users of the web about the image and its related concepts will result in those images being deemed more similar.  VisualRank is defined iteratively by &lt;math&gt;VR = S^* \times VR&lt;/math&gt;, where &lt;math&gt;S^*&lt;/math&gt; is the image similarity matrix.  As matrices are used, [[eigenvector centrality]] will be the measure applied, with repeated multiplication of &lt;math&gt;VR&lt;/math&gt; and &lt;math&gt;S^*&lt;/math&gt; producing the [[eigenvector]] we're looking for.  Clearly, the image similarity measure is crucial to the performance of VisualRank since it determines the underlying graph structure.

The main VisualRank system begins with local feature vectors being extracted from images using [[scale-invariant feature transform]] (SIFT).  Local feature descriptors are used instead of color histograms as they allow similarity to be considered between images with potential rotation, scale, and perspective transformations. Locality-sensitive hashing is then applied to these feature vectors using the [[locality-sensitive hashing#methods|p-stable distribution scheme]].  In addition to this, LSH amplification using AND/OR constructions are applied.  As part of the applied scheme, a [[Gaussian distribution]] is used under the [[L2 norm#Euclidean norm|&lt;math&gt;l_2&lt;/math&gt; norm]].

==References==
{{Reflist}}

==External links==
*[http://www.nytimes.com/2008/04/28/technology/28google.html?adxnnl=1&amp;ref=business&amp;adxnnlx=1210140241-DOwaJr/5AjMPCYJDerw++Q New York Times article]
*[http://tech.slashdot.org/article.pl?sid=08/04/28/1852254&amp;from=rss Slashdot article]
[[Category:Internet search]]
[[Category:Image processing]]</text>
      <sha1>8rsfn6od6jutfs0knmdoln12c6h5371</sha1>
    </revision>
  </page>
  <page>
    <title>IFACnet</title>
    <ns>0</ns>
    <id>7344222</id>
    <revision>
      <id>666879145</id>
      <parentid>666704099</parentid>
      <timestamp>2015-06-14T09:12:55Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* External links */Removed invisible unicode characters + other fixes, removed: &#8206; using [[Project:AWB|AWB]] (11140)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2733" xml:space="preserve">'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.

The following 31 organizations participate in IFACnet:

*[[American Institute of Certified Public Accountants]] (AICPA)
*[[Association of Chartered Certified Accountants]] (ACCA)
*[[Canadian Institute of Chartered Accountants]]
*[[Certified General Accountants Association of Canada]]
*[[Chartered Institute of Management Accountants]] (CIMA)
*[[Chartered Institute of Public Finance and Accountancy]]
*[[CMA Canada]]
*[[Compagnie Nationale des Commissaires aux Comptes]]
*[[Conseil Sup&#233;rieur de l'Ordre des Experts-Comptables]]
*[[Consiglio Nazionale Dottori Commercialisti]]
*[[CPA Australia]]
*[[D&#233;l&#233;gation Internationale Pour l'Audit et la Comptabilit&#233;]]
*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)
*[[International Federation of Accountants]]  (IFAC)
*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)
*[[Institute of Certified Public Accountants in Ireland]]
*[[Institute of Certified Public Accountants of Singapore]]
*[[Institute of Chartered Accountants of Australia]]
*[[Institute of Chartered Accountants in England &amp; Wales]] (ICAEW)
*[[Institute of Chartered Accountants in Ireland]]
*[[Institute of Chartered Accountants of India]]
*[[Institute of Chartered Accountants of Pakistan]]
*[[Institute of Chartered Accountants of Scotland]] (ICAS)
*[[Institute of Management Accountants]]
*[[Japanese Institute of Certified Public Accountants]] (JICPA)
*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)
*[[Malaysian Institute of Accountants]]
*[[Malta Institute of Accountants]]
*[[National Association of State Boards of Accountancy]] (NASBA)
*[[South African Institute of Chartered Accountants]] (SAICA)
*[[Union of Chambers of Certified Public Accountants of Turkey]] (T&#220;RMOB)

==External links==
*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]
*[http://www.ifac.org/ International Federation of Accountants Homepage]

[[Category:Information retrieval organizations]]
[[Category:Internet search engines]]
[[Category:Accounting organizations]]</text>
      <sha1>4vyql77us7cl15p1emnr3xawj85yjhd</sha1>
    </revision>
  </page>
  <page>
    <title>International Society for Music Information Retrieval</title>
    <ns>0</ns>
    <id>30882491</id>
    <revision>
      <id>757776187</id>
      <parentid>738542537</parentid>
      <timestamp>2017-01-01T17:49:24Z</timestamp>
      <contributor>
        <username>Yurichev</username>
        <id>19013952</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13312" xml:space="preserve">{{Infobox non-profit
| Non-profit_name   = The International Society for Music Information Retrieval
| Non-profit_logo   = [[Image:LogoInternationalSocietyMIR.png|250px]]
| Non-profit_type   = Non-profit organization
| founded_date      = 2008
| founder           = 
| location          = [[Canada]]
| origins           = International Symposium for Music Information Retrieval
| key_people        = 
| area_served       = Worldwide
| focus             = [[Music information retrieval|Music Information Retrieval (MIR)]]
| method            = Conferences, publications
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| owner             = 
| Non-profit_slogan = The world's leading research forum on processing, searching, organising and accessing music-related data
| homepage          = {{URL|http://www.ismir.net/}}
| tax_exempt        = 
| dissolved         = 
| footnotes         = 
}}

The '''[http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]''' is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000&lt;ref&gt;[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11], {{ISSN|1082-9873}}.&lt;/ref&gt; which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.&lt;ref&gt;[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]&lt;/ref&gt;

==Purpose==
Given the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.

As the term [[Music Information Retrieval|Music Information Retrieval (MIR)]]  indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science, electrical engineering and many others.

==Annual conferences==
Since its inception in 2000, ISMIR has been the world&#8217;s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.

* ISMIR 2019, Delft (The Netherlands)
* ISMIR 2018, Paris (France)
* ISMIR 2017, Suzhou (China)
* [http://ismir2016.ismir.net ISMIR 2016], 8&#8211;12 August 2016, New York City (USA) [http://dblp.uni-trier.de/db/conf/ismir/ismir2016.html proceedings]
* [http://ismir2015.ismir.net ISMIR 2015], 26&#8211;30 October 2015, Malaga (Spain) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2015.html proceedings]
* [http://ismir2014.ismir.net ISMIR 2014], 27&#8211;31 October 2014, Taipei (Taiwan) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2014.html proceedings]
* [http://ismir2013.ismir.net ISMIR 2013], 4&#8211;8 November 2013, Curitiba (Brazil) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2013.html proceedings]
* [http://ismir2012.ismir.net ISMIR 2012], 8&#8211;12 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2012'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2011.ismir.net ISMIR 2011], 24&#8211;28 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2011'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2010.ismir.net ISMIR 2010], 9&#8211;13 August 2010, Utrecht (The Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2010'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2009.ismir.net ISMIR 2009], 26&#8211;30 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2009'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2008.ismir.net ISMIR 2008], 14&#8211;18 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2008'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2007.ismir.net ISMIR 2007], 23&#8211;30 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2007'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2006.ismir.net ISMIR 2006], 8&#8211;12 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2006'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2005.ismir.net ISMIR 2005], 11&#8211;15 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2005'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2004.ismir.net ISMIR 2004], 10&#8211;15 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2004'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2003.ismir.net ISMIR 2003], 26&#8211;30 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2003'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2002.ismir.net ISMIR 2002], 13&#8211;17 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2002'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2001.ismir.net ISMIR 2001], 15&#8211;17 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2001'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2000.ismir.net ISMIR 2000], 23&#8211;25 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='200'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]

The [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences. An overview of all papers published at ISMIR can be found at [http://dblp.uni-trier.de/db/conf/ismir/index.html DBLP].

==Research areas and topics==
The following list gives an overview of the main research areas and topics that are within the scope of 
[[Music Information Retrieval]].

===MIR data and fundamentals===
*    music signal processing
*    symbolic music processing
*    metadata, linked data and semantic web
*    social tags and user generated data
*    natural language processing, text and web mining
*    multi-modal approaches to MIR

===Methodology===
*    methodological issues and philosophical foundations
*    evaluation methodology
*    corpus creation
*    legal, social and ethical issues

===Domain knowledge===
*    representation of musical knowledge and meaning
*    music perception and cognition
*    computational music theory
*    computational musicology and ethnomusicology

===Musical features and properties===
*    melody and motives
*    harmony, chords and tonality
*    rhythm, beat, tempo
*    structure, segmentation and form
*    timbre, instrumentation and voice
*    musical style and genre
*    musical affect, emotion and mood
*    expression and performative aspects of music

===Music processing===
*    sound source separation
*    music transcription and annotation
*    optical music recognition
*    alignment, synchronization and score following
*    music summarization
*    music synthesis and transformation
*    fingerprinting
*    automatic classification
*    indexing and querying
*    pattern matching and detection
*    similarity metrics

===Application===
*    user behavior and modelling
*    user interfaces and interaction
*    digital libraries and archives
*    music retrieval systems
*    music recommendation and playlist generation
*    music and health, well-being and therapy
*    music training and education
*    MIR applications in music composition, performance and production
*    music and gaming
*    MIR in business and marketing

==MIREX==
The ''Music Information Retrieval Evaluation eXchange'' (MIREX) is an annual evaluation campaign for MIR algorithms, coupled to the ISMIR conference. Since it started in 2005, MIREX has fostered advancements both in specific areas of MIR and in the general understanding of how MIR systems and algorithms are to be evaluated.&lt;ref name=DownieEBJ10&gt;
{{citation 
|author1=J. Stephen Downie |author2=Andreas F. Ehmann |author3=Mert Bay |author4=M. Cameron Jones |title=The Music Information Retrieval Evaluation eXchange: Some Observations and Insights
|journal=Advances in Music Information Retrieval, Springer
|year=2010
|pages=93&#8211;115
|doi=10.1007/978-3-642-11674-2_5}}
&lt;/ref&gt;&lt;ref name=DownieEEV05_ISMIR&gt;
{{cite journal
|last1=Downie
|first1=J. Stephen
|last2=West
|first2=Kris 
|last3=Ehmann
|first3=Andreas F.
|last4=Vincent
|first4=Emmanuel
|title=The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview
|journal=Proceedings of the International Conference on Music Information Retrieval
|year=2005
|pages=320&#8211;323}}
&lt;/ref&gt; MIREX is to the MIR community what the [[Text Retrieval Conference]] (TREC) is to the text information retrieval community: A set of community-defined formal evaluations through which a wide variety of state-of-the-art systems, algorithms and techniques are evaluated under controlled conditions. MIREX is managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana-Champaign (UIUC).&lt;ref name="DownieIMIRSEL"&gt;{{cite web|last1=Downie|first1=J. Stephen|title=The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) Project|url=http://www.music-ir.org/evaluation/|publisher=University of Illinois|accessdate=22 April 2016}}&lt;/ref&gt;

==Related conferences==
* [[ACM Multimedia]]
* [[International Computer Music Conference|International Computer Music Conference (ICMC)]]
* [[International Conference on Acoustics, Speech, and Signal Processing|International Conference on Acoustics, Speech, and Signal Processing (ICASSP)]]
* [[International Conference on Digital Audio Effects|International Conference on Digital Audio Effects (DAFx)]]
* [[New Interfaces for Musical Expression|International Conference on New Interfaces for Musical Expression (NIME)]]
* International Symposium on Computer Music Modeling and Retrieval (CMMR)
* [[Sound and Music Computing Conference|Sound and Music Computing Conference (SMC)]]

==Related journals==
* [[Computer Music Journal|Computer Music Journal (CMJ)]]
* [http://asmp.eurasipjournals.springeropen.com/ EURASIP Journal on Audio, Speech, and Music Processing]
* [http://www.signalprocessingsociety.org/publications/periodicals/taslp/ IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)]
* [http://www.signalprocessingsociety.org/tmm/ IEEE Transactions on Multimedia (TMM)]
* [http://mp.ucpress.edu/ Music Perception]
* [[Journal of New Music Research|Journal of New Music Research (JNMR)]]

==Further links==
* [[Audio Engineering Society]]
* [http://www.signalprocessingsociety.org/technical-committees/list/audio-tc/ Audio and Acoustic Signal Processing]
* [[Music Technology]]
* [http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]
* [[Sound and music computing|Sound and Music Computing]]

==References==
{{Reflist}}

[[Category:Music information retrieval]]
[[Category:Computer science conferences]]
[[Category:Music technology]]
[[Category:Multimedia]]
[[Category:Information retrieval organizations]]
[[Category:Music search engines]]</text>
      <sha1>kk7h6oum4j32q5ipc33fbisu8x0euj6</sha1>
    </revision>
  </page>
  <page>
    <title>Concept Searching Limited</title>
    <ns>0</ns>
    <id>17770654</id>
    <revision>
      <id>730991874</id>
      <parentid>682544518</parentid>
      <timestamp>2016-07-22T05:43:12Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>/* top */USA is deprecated, per [[MOS:NOTUSA]], and correct [[MOS:OVERLINK|overlinking]] of common places using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3364" xml:space="preserve">{{Infobox company |
  name   = Concept Searching Limited |
  logo = [[Image:conceptSearching.jpg]] |
  slogan = "Retrieval Just Got Smarter" |
  type   =  [[Privately held company|Private]] |
  foundation     = 2002|
  location       = UK, United States |
  area_served    = Global |
  industry       = [[Information retrieval]] |
  products       = conceptSearch&lt;br/&gt;conceptClassifier&lt;br/&gt;conceptClassifier for SharePoint&lt;br/&gt;conceptClassifier for SharePoint Online&lt;br/&gt;Taxonomy Manager&lt;br/&gt;Taxonomy Workflow |
  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]
}}

'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].

==History==
Concept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].&lt;ref&gt;[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&amp;ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173&lt;/ref&gt;&lt;ref&gt;[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval&lt;/ref&gt;

Compound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.&lt;ref&gt;[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008&lt;/ref&gt;

The company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.&lt;ref&gt;[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile&lt;/ref&gt;

Concept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center &lt;ref&gt;[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework&lt;/ref&gt;

== Awards ==
* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014/2015 &lt;ref&gt;{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-102189.aspx |title=KMWorld Magazine}}&lt;/ref&gt;
* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014/2015 &lt;ref&gt;{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx |title=Trend-Setting Products}}&lt;/ref&gt;

==See also==
* [[Compound term processing]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept Search]]

==References==
{{Reflist}}

==External links==
*[http://www.conceptsearching.com/ Company Website]

[[Category:Information retrieval organizations]]
[[Category:Privately held companies of the United Kingdom]]</text>
      <sha1>2i28kjk6e5spyzo3hdu0xfd651m4t0h</sha1>
    </revision>
  </page>
  <page>
    <title>Conference and Labs of the Evaluation Forum</title>
    <ns>0</ns>
    <id>27511028</id>
    <revision>
      <id>752813387</id>
      <parentid>748808507</parentid>
      <timestamp>2016-12-03T13:46:20Z</timestamp>
      <contributor>
        <username>Varepsilon i</username>
        <id>24584483</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2436" xml:space="preserve">The '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to maintain an underlying framework for testing [[information retrieval]] systems and to create [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].&lt;ref name="Peters"&gt;{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | citeseerx = 10.1.1.109.7647 }}&lt;/ref&gt;
The organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,&lt;ref&gt;{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1&#8211;2 | year = 2004 }}&lt;/ref&gt;&lt;ref&gt;Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in ''Information Processing &amp; Management''
vol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}&lt;/ref&gt;

For example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.&lt;ref name="ImageCLEFmed"&gt;{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
* [http://www.clef-initiative.eu/ CLEF homepage]

[[Category:Information retrieval organizations]]


{{Compu-conference-stub}}</text>
      <sha1>ponqlkrm70l24e2xwncg80qoqirbsdp</sha1>
    </revision>
  </page>
  <page>
    <title>Artificial Solutions</title>
    <ns>0</ns>
    <id>40218456</id>
    <revision>
      <id>729468129</id>
      <parentid>725107594</parentid>
      <timestamp>2016-07-12T11:44:13Z</timestamp>
      <contributor>
        <username>Nick Number</username>
        <id>1526960</id>
      </contributor>
      <minor />
      <comment>repaired link(s) to disambiguation pages ([[WP:DPL|you can help]]) - Mountain View</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9555" xml:space="preserve">{{Infobox company
|name= Artificial Solutions
|logo=[[Image:Artificial Solutions Logo.png]]
|type=[[Private company]]
|foundation=(2001)
|founder=Johan &#197;hlund, Johan Gustavsson and Michael S&#246;derstr&#246;m 
|location=[[Barcelona]], [[Spain]]
|locations=Offices worldwide with R&amp;D centers in [[Barcelona]], [[Hamburg]], [[London]], [[Mountain View, California|Mountain View]], [[Milan]], [[Utrecht]] and [[Stockholm]] 
|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], 
|products= Teneo platform
|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]
}}

'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.&lt;ref&gt;{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}&lt;/ref&gt; The company's natural language solutions have been deployed in a wide range of industries including finance,&lt;ref&gt;{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}&lt;/ref&gt; telecoms,&lt;ref&gt;{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtj&#228;nst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}&lt;/ref&gt; the public sector,&lt;ref&gt;{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tj&#228;nst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}&lt;/ref&gt; retail&lt;ref&gt;{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}&lt;/ref&gt; and travel.&lt;ref&gt;{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}&lt;/ref&gt;

==History==
Artificial Solutions was founded in Stockholm in 2001 by friends Johan &#197;hlund, Johan Gustavsson and Michael S&#246;derstr&#246;m to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though &#197;hlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.&lt;ref&gt;{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=L&#246;jlig aff&#228;rside vinstlott f&#246;r Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}&lt;/ref&gt;

The company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.&lt;ref&gt;{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}&lt;/ref&gt; Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.&lt;ref&gt;{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}&lt;/ref&gt;

In 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.&lt;ref&gt;{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}&lt;/ref&gt;
[[Elbot]], Artificial Solutions&#8217; test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.&lt;ref&gt;[[Loebner Prize]]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &amp;#124; England &amp;#124; Berkshire &amp;#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}&lt;/ref&gt;

With a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.&lt;ref&gt;{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=S&#229; effektiv er Ikeas chat-robot: Har v&#230;ret p&#229; 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}&lt;/ref&gt;
In 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.&lt;ref&gt;{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &amp;#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}&lt;/ref&gt;
A new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.&lt;ref&gt;{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &amp;#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}&lt;/ref&gt;

In February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].&lt;ref&gt;{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}&lt;/ref&gt;

==References==
{{Reflist|30em}}

==External links==
*[http://www.hello-indigo.com Indigo]
*[http://www.elbot.com Elbot]

[[Category:Natural language processing software]]
[[Category:Intelligent software assistants]]
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval organizations]]</text>
      <sha1>hzubh30h9s774s7cem7jcxlehujhtq0</sha1>
    </revision>
  </page>
  <page>
    <title>Datanet</title>
    <ns>0</ns>
    <id>13555870</id>
    <revision>
      <id>666920057</id>
      <parentid>666920031</parentid>
      <timestamp>2015-06-14T16:12:22Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval organizations</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5892" xml:space="preserve">{{Use mdy dates|date=September 2011}}
'''DataNet''', or '''Sustainable Digital Data Preservation and Access Network Partner''' was a research program of the U.S. [[National Science Foundation]] Office of Cyberinfrastructure.  The office announced a request for proposals with this title on September 28, 2007.&lt;ref name="datanetprogram"&gt;{{cite web
|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary
|date=September 28, 2007
|accessdate=October 3, 2007
}}&lt;/ref&gt;  The lead paragraph of its synopsis describes the program as:

&lt;blockquote&gt;Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.&lt;/blockquote&gt;

The introduction in the solicitation&lt;ref name="datanetsolicitation"&gt;{{cite web
|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements &amp; Information
|date=September 28, 2007
|accessdate=October 3, 2007
}}&lt;/ref&gt; goes on to say:

&lt;blockquote&gt;Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF&#8217;s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which &#8220;science and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.&#8221; The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.&lt;/blockquote&gt;

The initial plan called for a $100 million initiative: five awards of $20&amp;nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],&lt;ref&gt;{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,&lt;ref&gt;{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. 

For the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.&lt;ref&gt;{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}&lt;/ref&gt; Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,&lt;ref&gt;{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,&lt;ref&gt;{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',&lt;ref&gt;{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.

==References==
{{reflist|30em}}

==External links==
* [http://www.dataone.org DataONE]
* [http://dataconservancy.org/ Data Conservancy]
* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]
* [http://datafed.org/ DataNet Federation Consortium]
* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] 
 

[[Category:National Science Foundation]]
[[Category:Science and technology in the United States]]
[[Category:Information retrieval organizations]]
[[Category:Digital library projects]]</text>
      <sha1>t2bp6o8ah9knr94665fl7wyl00jpf09</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Alphabet Inc.</title>
    <ns>14</ns>
    <id>47562417</id>
    <revision>
      <id>742942805</id>
      <parentid>676714196</parentid>
      <timestamp>2016-10-06T19:59:55Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1323" xml:space="preserve">{{Commons category|Alphabet Inc.}}
{{Portal|Alphabet|Google}}
*'''[[Alphabet Inc.]]''' &#8212; an {{C|Multinational companies headquartered in the United States|American multinational}} {{C|Conglomerate companies of the United States|conglomerate company}} based in {{C|Mountain View, California|Mountain View}}, {{C|San Francisco Bay Area}}, {{C|California}}.
:::::*It is the parent corporation of {{C|Google}}; and other [[information technology]], investment, life sciences, and research companies.


{{clr}}
::{{Cat main|Alphabet Inc.}}

{{Alphabet Inc.|state=collapsed}}
{{Google Inc.}}

[[Category:Conglomerate companies of the United States]]
[[Category:Holding companies of the United States]]
[[Category:Multinational companies headquartered in the United States]]
[[Category:Technology companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Wikipedia categories named after conglomerate companies of the United States]]
[[Category:Wikipedia categories named after information technology companies of the United States]]











[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Information retrieval organizations]]
[[Category:Internet companies of the United States]]</text>
      <sha1>gvt4pncmua2caxiai9p7qh7p802g54q</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Waymo</title>
    <ns>14</ns>
    <id>52604208</id>
    <revision>
      <id>755169306</id>
      <timestamp>2016-12-16T16:58:58Z</timestamp>
      <contributor>
        <username>Zubairudalhatu</username>
        <id>6242228</id>
      </contributor>
      <comment>page created</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="331" xml:space="preserve">{{Commons category|Google}}
{{Cat main|Waymo}}

[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Web portals]]
[[Category:Information retrieval organizations]]
[[Category:Alphabet Inc.]]
[[Category:Wikipedia categories named after websites]]</text>
      <sha1>1h3g2c57tshzw4d0t8scq18pzfnh9j0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Music search engines</title>
    <ns>14</ns>
    <id>28073505</id>
    <revision>
      <id>747503928</id>
      <parentid>747464621</parentid>
      <timestamp>2016-11-02T19:24:39Z</timestamp>
      <contributor>
        <username>Bamyers99</username>
        <id>12311825</id>
      </contributor>
      <comment>Undid revision 747464621 by [[Special:Contributions/182.186.123.39|182.186.123.39]] ([[User talk:182.186.123.39|talk]]) not the place to add links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="210" xml:space="preserve">[[Category:Information retrieval systems]]
[[Category:Music software|Search engines]]
[[Category:Internet search engines]]
[[Category:Online music and lyrics databases]]
[[Category:Music information retrieval]]</text>
      <sha1>lm87mi1jyt8pwrtn8mc27meskbh80mo</sha1>
    </revision>
  </page>
  <page>
    <title>Reverse DNS lookup</title>
    <ns>0</ns>
    <id>1286913</id>
    <revision>
      <id>758928774</id>
      <parentid>755018458</parentid>
      <timestamp>2017-01-08T08:45:36Z</timestamp>
      <contributor>
        <username>Cnwilliams</username>
        <id>10190671</id>
      </contributor>
      <minor />
      <comment>Disambiguated: [[octet]] &#8594; [[octet (computing)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7443" xml:space="preserve">{{Redirect|Reverse DNS|Java-like naming convention|Reverse domain name notation}}
{{Refimprove|date=September 2016}}

In [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' ('''rDNS''') is the determination of a [[domain name]] associated with an [[IP address]] via querying [[Domain Name System|DNS]] &#8211; the reverse of the usual "forward" DNS lookup of an IP from a domain name.

The process of reverse resolving an IP address uses [[PTR record]]s. The reverse DNS database of the Internet is rooted in the [[.arpa|arpa]] [[top-level domain]].

Although the informational RFC 1912 (Section 2.1) specifies that "Every Internet-reachable host should have a name" and that "For every IP address, there should be a matching PTR record...", it is not an [[Internet Standard]] requirement, and not all IP addresses have a reverse entry.

== Implementation details ==
===IPv4 reverse resolution===
Reverse DNS lookups for [[IPv4]] addresses use the special domain &lt;code&gt;in-addr.arpa&lt;/code&gt;. In this domain, an IPv4 address is represented as a concatenated sequence of four decimal numbers, separated by dots, to which is appended the second level domain suffix &lt;code&gt;.in-addr.arpa&lt;/code&gt;. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four [[octet (computing)|octet]]s and converting each octet into a decimal number. These decimal numbers are then concatenated in the order: least significant octet first (leftmost), most significant octet last (rightmost). It is important to note that this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses in textual form.

For example, to do a reverse lookup of the IP address &lt;code&gt;8.8.4.4&lt;/code&gt; the PTR record for the domain name &lt;code&gt;4.4.8.8.in-addr.arpa&lt;/code&gt; would be looked up, and found to point to &lt;code&gt;google-public-dns-b.google.com&lt;/code&gt;. 

If the [[A record]] for &lt;code&gt;google-public-dns-b.google.com&lt;/code&gt; in turn pointed back to &lt;code&gt;8.8.4.4&lt;/code&gt; then it would be said to be [[Forward-confirmed reverse DNS|forward-confirmed]].

====Classless reverse DNS method====
Historically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A. By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using [[CNAME record]]s.

===IPv6 reverse resolution===
Reverse DNS lookups for [[IPv6]] addresses use the special domain &lt;code&gt;ip6.arpa&lt;/code&gt; (previously &lt;code&gt;ip6.int&lt;/code&gt;&lt;ref&gt;RFC 4159&lt;/ref&gt;). An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address &lt;code&gt;2001:db8::567:89ab&lt;/code&gt; is &lt;code&gt;b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa&lt;/code&gt;.

===Multiple pointer records===
While most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended{{by whom|date=August 2016}}, unless there is a specific need. For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically. Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.&lt;ref&gt;[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]&lt;/ref&gt; In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause the query to be requested over TCP when they exceed the DNS 512 byte UDP message limit.

===Records other than PTR records===
Record types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]], [[Secure Shell|SSH]] and [[Internet Key Exchange|IKE]]. [[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.&lt;ref&gt;{{Citation | publisher = IETF | title = RFC&#8239;6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}&lt;/ref&gt; Less standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.

==Uses==
The most common uses of the reverse DNS include:

* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.
* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, or dynamically assigned addresses unlikely to be used by legitimate mail servers. Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com." Some anti-spam filters assume that email that originates from such addresses is likely to be spam, and may refuse connection.&lt;ref&gt;[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]&lt;/ref&gt;&lt;ref&gt;[http://postmaster.aol.com/info/rdns.html reference page from AOL] {{webarchive |url=https://web.archive.org/web/20061210223820/http://postmaster.aol.com/info/rdns.html |date=December 10, 2006 }}&lt;/ref&gt;
* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, since [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually cannot achieve forward validation when they use [[zombie computer]]s to forge domain records.
* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address.

==References==
{{reflist}}

==External links==
* {{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}
* [http://dns.icann.org ICANN DNS Operations]
* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]
* RDNS policies: [https://web.archive.org/web/20121106162649/http://postmaster.aol.com:80/Postmaster.Errors.php#554rlyb1#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]

[[Category:Information retrieval systems]]
[[Category:Domain name system]]

[[nl:Domain Name System#Omgekeerde lookups]]</text>
      <sha1>b01bjll8l2i0uon6dxxsxnw8b45mf41</sha1>
    </revision>
  </page>
  <page>
    <title>Wolfram Alpha</title>
    <ns>0</ns>
    <id>21903944</id>
    <revision>
      <id>762129243</id>
      <parentid>760402531</parentid>
      <timestamp>2017-01-26T20:56:46Z</timestamp>
      <contributor>
        <username>Pleasantville</username>
        <id>3058640</id>
      </contributor>
      <comment>/* Licensing partners */ refine verb tense</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20588" xml:space="preserve">{{Use mdy dates|date=October 2014}}
{{Infobox website
| name = Wolfram Alpha
| logo =  Wolfram Alpha December 2016.svg
| caption = Wolfram Alpha is based on the computational platform [[Mathematica]], written by British scientist [[Stephen Wolfram]] in 1988.
| url = {{URL|http://www.wolframalpha.com/}}.
| slogan             = Making the world&#8217;s knowledge computable.&lt;ref&gt;[http://www.wolframalpha.com/about.html Wolfram Alpha About page]&lt;/ref&gt;
| commercial = Yes
| type =  [[Answer engine]]
| registration = Optional
| owner = Wolfram Alpha LLC
| author = [[Wolfram Research]]
| alexa  = {{Decrease}} 1,932 ({{as of|2015|26|31|alt=March 2015}})&lt;ref name="alexa"&gt;{{cite web|url= http://www.alexa.com/siteinfo/wolframalpha.com |title= Wolframalpha.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2015-03-31}}&lt;/ref&gt;
| num_employees &#8776; 200 (as of 2012)
| programming_language = [[Wolfram Language]]
| launch date = {{start date and age|2009|5|18}}&lt;ref name="launch date"&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |date=May 8, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; (official launch)&lt;br&gt;{{start date|2009|5|15}}&lt;ref name="updated launch detail"&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/12/going-live-and-webcasting-it/ |work=Wolfram&amp;#124;Alpha Blog |title=Going Live&#8212;and Webcasting It |publisher=Wolfram Alpha |date=May 12, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; (public launch)
| current status = Active
}}

'''Wolfram Alpha''' (also styled '''WolframAlpha''' and '''Wolfram|Alpha''') is a computational knowledge engine&lt;ref name=Guardiandatasource&gt;{{cite news |title=Where does Wolfram Alpha get its information? |author=Bobbie Johnson |publisher=The Guardian |date=May 21, 2009 |accessdate=2013-03-08 |url=https://www.theguardian.com/technology/2009/may/21/1 }}&lt;/ref&gt; or [[answer engine]] developed by [[Wolfram Research]], which was founded by [[Stephen Wolfram]]. It is an online service that answers factual queries directly by computing the answer from externally sourced "curated data",&lt;ref&gt;{{Cite web|title = About Wolfram{{!}}Alpha: Making the World's Knowledge Computable|url = http://www.wolframalpha.com/about.html|website=wolframalpha.com|accessdate = 2015-11-25}}&lt;/ref&gt; rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] might.&lt;ref&gt;{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=March 9, 2009 |work=The Guardian |publisher=Guardian News and Media |location=UK |accessdate=2013-02-09}}&lt;/ref&gt;

Wolfram Alpha, which was released on May 18, 2009, is based on Wolfram's earlier flagship product [[Wolfram Mathematica]], a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities.&lt;ref name="launch date" /&gt; Additional data is gathered from both academic and commercial websites such as the CIA's ''[[The World Factbook]]'', the United States Geological Survey, a Cornell University Library publication called ''All About Birds'', ''Chambers Biographical Dictionary'', [[Dow Jones]], the ''Catalogue of Life'',&lt;ref name=Guardiandatasource /&gt; [[CrunchBase]],&lt;ref name=techcrunch&gt;{{cite news |last=Dillet |first=Romain |title=Wolfram Alpha Makes CrunchBase Data Computable Just In Time For Disrupt SF |url=http://techcrunch.com/2012/09/07/wolfram-alpha-makes-crunchbase-data-computable-just-in-time-for-disrupt/ |publisher=TechCrunch |date=September 7, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; [[Best Buy]],&lt;ref&gt;{{cite news |last=Golson |first=Jordan |title=Wolfram Delivers Siri-Enabled Shopping Results From Best Buy |url=http://www.macrumors.com/2011/12/16/wolfram-delivers-siri-enabled-shopping-results-from-best-buy/ |publisher=MacRumors |date=December 16, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; the [[Federal Aviation Administration|FAA]]&lt;ref&gt;{{cite news |last=Barylick |first=Chris |title=Wolfram Alpha search engine now tracks flight paths, trajectory information |url=http://www.engadget.com/2011/11/19/wolfram-alpha-search-engine-now-tracks-flight-paths-trajectory/ |publisher=Engadget |date=November 19, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; and optionally a user's Facebook account.

== Overview ==
Users submit queries and computation requests via a text field.  Wolfram Alpha then computes answers and relevant visualizations from a [[knowledge base]] of [[Data curation|curated]], [[structured data]] that come from other sites and books. The site "use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review."&lt;ref&gt;{{cite web |title=Data in Wolfram&amp;#124;Alpha |url=http://www.wolframalpha.com/faqs5.html |website=Wolfram Alpha |accessdate=4 August 2015}}&lt;/ref&gt; The curated data makes Alpha different from [[semantic search]] engines, which index a large number of answers and then try to match the question to one.

Wolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased [[natural language understanding|natural language]] fact-based questions such as "Where was [[Mary Robinson]] born?" or more complex questions such as "How old was [[Queen Elizabeth II]] in 1974?" It displays its "Input interpretation" of such a question, using standardized phrases such as "age | of Queen Elizabeth II (royalty) | in 1974", the answer of which is "Age at start of 1974: 47 years", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as "What is the difference between the Julian and the Gregorian calendars?" but will answer factual or computational questions such as "June 1 in Julian calendar".

Mathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, "lim(x-&gt;0) (sin x)/x" yields the correct [[limit (functions)|limit]]ing value of 1, as well as a plot, up to 235 terms ({{as of|2013|lc=y}}) of the [[Taylor series]], and (for registered users) a possible derivation using [[L'H&#244;pital's rule]]. It is also able to perform calculations on data using more than one source. For example, "What is the [[List of countries by GDP (nominal) per capita|fifty-second smallest]] country by [[GDP per capita]]?" yields [[Nicaragua]], $1160 per year.

== Technology ==
Wolfram Alpha is written in 15 million lines of [[Wolfram Language]] code&lt;ref&gt;{{cite web |author=WolframResearch |url=https://www.youtube.com/watch?v=56ISaies6Ws#t=927s |title=Stephen Wolfram: The Background and Vision of Mathematica |publisher=Youtube.com |date=October 10, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; and runs on more than 10,000 CPUs.&lt;ref&gt;{{cite news |first=Frederic |last=Lardinois |url=http://readwrite.com/2009/04/25/wolframalpha_our_first_impressions |title=Wolfram&amp;#124;Alpha: Our First Impressions |date=April 25, 2009 |publisher=ReadWriteWeb |accessdate=2013-02-09}}&lt;/ref&gt;&lt;ref&gt;{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2009/05/15/wolframalpha-is-launching-made-possible-by-mathematica/ |title=Wolfram&amp;#124;Alpha Is Launching: Made Possible by ''Mathematica'' |work=WolframAlpha Blog |publisher=Wolfram Alpha |date=May 15, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; The database currently includes hundreds of datasets, such as "All Current and Historical Weather." The datasets have been accumulated over several years.&lt;ref&gt;{{cite web |title=Taking a first bite out of Wolfram Alpha | first=Jane Fae | last=Ozimek |work=The Register |date=May 18, 2009 |url=http://www.theregister.co.uk/2009/05/18/wolfram_alpha/ |accessdate=2013-02-09}}&lt;/ref&gt; The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are "acceptable".&lt;ref name=semanticabyss&gt;{{cite web |title=The Semantic Abyss - Plumbing the Semantic Web: Exploring the depths of the semantic gap between the Semantic Web and real world users and consumers |url=http://semanticabyss.blogspot.ca/2009/03/what-is-curated-data.html |year=2009 |author=Jack Krupansky}}&lt;/ref&gt;{{unreliable source?|date=September 2015}}

One example of a live dataset that Wolfram Alpha can use is the profile of a [[Facebook]] user, through inputting the "facebook report" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a "personal analytics" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information.&lt;ref name=techland&gt;{{cite news |first=Thomas E. |last=Weber |url=http://techland.time.com/2012/09/05/wolfram-alphas-facebook-analytics-tool-digs-deep-into-your-social-life/ |title=Wolfram Alpha's Facebook Analytics Tool Digs Deep into Your Social Life |work=Tech |publisher=Time Magazine |date=September 5, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; Within two weeks of launching the Facebook analytics service, 400,000 users had used it.&lt;ref&gt;{{cite news |last=R. |first=A. |title=Visualising Facebook Who am I? |url=http://www.economist.com/blogs/graphicdetail/2012/09/visualising-facebook |publisher=The Economist |work=Graphic detail |date=September 21, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; Downloadable query results are behind a pay wall but summaries are accessible to free accounts.&lt;ref name=publiclibraries&gt;{{cite web |url=http://publiclibrariesonline.org/2013/03/a-wolf-or-a-ram-what-is-wolfram-alpha/ |title=A Wolf or a Ram? What is Wolfram Alpha? |author=Joanna Nelson |date=March 4, 2013 |publisher=Public Libraries Online }}&lt;/ref&gt;

== Licensing partners ==
Wolfram Alpha has been used to power some searches in the [[Microsoft]] [[Bing (search engine)|Bing]] and [[DuckDuckGo]] search engines.&lt;ref&gt;{{cite news |first=Tom |last=Krazit |url=http://news.cnet.com/8301-30684_3-10315117-265.html |title=Bing strikes licensing deal with Wolfram Alpha |publisher=CNET |date=August 21, 2009 |accessdate=2013-02-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Team |date=April 18, 2011 |url=http://blog.wolframalpha.com/2011/04/18/wolframalpha-and-duckduckgo-partner-on-api-binding-and-search-integration/ |title=Wolfram&amp;#124;Alpha and DuckDuckGo Partner on API Binding and Search Integration |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}&lt;/ref&gt; For factual [[question answering]], it is also queried by Apple's [[Siri (software)|Siri]], Samsung's [[S Voice]], as well as Dexetra's [[speech recognition]] software for the [[Android (operating system)|Android]] platform, Iris, and the voice control software on [[BlackBerry 10]].&lt;ref&gt;{{cite web|url=http://www.berryreview.com/2013/10/21/blackberry-teams-up-with-wolfram-alpha-for-blackberry-10-voice-control/|title=BlackBerry Teams Up with Wolfram Alpha For BlackBerry 10 Voice Control|work=BerryReview}}&lt;/ref&gt;

== History ==
Launch preparations began on May 15, 2009 at 7&amp;nbsp;pm [[Central Daylight Time (North America)#Central Daylight Time|CDT]] and were broadcast live on [[Justin.tv]]. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.&lt;ref name="BBC"&gt;{{cite news |url=http://news.bbc.co.uk/1/hi/technology/8052798.stm |title=Wolfram 'search engine' goes live |publisher=BBC News |date=May 18, 2009 |accessdate=2013-02-09}}&lt;/ref&gt;

Wolfram Alpha has received mixed reviews.&lt;ref name="spivack"&gt;{{cite web |first=Nova |last=Spivack |title=Wolfram Alpha is Coming &#8211; and It Could be as Important as Google |date=March 7, 2009 |url=http://www.novaspivack.com/uncategorized/wolfram-alpha-is-coming-and-it-could-be-as-important-as-google |accessdate=2013-02-09 |publisher=Nova Spivack &#8211; Minding the Planet}}&lt;/ref&gt;&lt;ref&gt;{{cite news |first=Ryan |last=Singel |title=Wolfram&amp;#124;Alpha Fails the Cool Test |date=May 18, 2009 |url=http://www.wired.com/epicenter/2009/05/wolframalpha-fails-the-cool-test/ |publisher=Wired |accessdate=2013-02-09}}&lt;/ref&gt; Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.&lt;ref name="spivack"/&gt;

On December 3, 2009, an [[iPhone]] app was introduced. Some users&lt;ref name="ios-price"&gt;{{cite web |first=MG |last=Siegler |url=http://techcrunch.com/2009/12/03/wolfram-alpha-iphone-app/ |title=Nice Try, Wolfram Alpha. Still Not Paying $50 For Your App. |publisher=TechCrunch |date=December 3, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; considered the initial $50 price of the [[iOS]] app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site.&lt;ref name="mobile-format"&gt;{{cite news |url=http://www.tuaw.com/2009/12/03/wolframalpha-iphone-formatted-web-page-no-longer-available/ |first=TJ |last=Luoma |title=WolframAlpha iPhone-formatted web page no longer available |publisher=TUAW |date=December 3, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; Wolfram responded by lowering the price to $2, offering a refund to existing customers&lt;ref name="refund"&gt;{{cite web|last=Broida |first=Rick |url=http://reviews.cnet.com/8301-19512_7-10471978-233.html |title=Get Wolfram Alpha app for $1.99-and a refund if you paid more |publisher=CNET |date=April 1, 2010 |accessdate=2012-02-28}}&lt;/ref&gt; and re-instating the mobile site.

On October 6, 2010 an Android version of the app was released&lt;ref&gt;{{cite news |url=http://techcrunch.com/2010/10/06/wolframalphas-android-app-now-available/ |title=Wolfram Alpha's Android app now available |first=Leena |last=Rao |publisher=TechCrunch |date=October 6, 2010 |accessdate=2013-02-09}}&lt;/ref&gt; and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.&lt;ref&gt;{{cite web |url=http://products.wolframalpha.com/mobile/ |title=Wolfram&amp;#124;Alpha: Mobile &amp; Tablet Apps |year=2013 |accessdate=2013-02-09 |publisher=Wolfram Alpha}}&lt;/ref&gt;

== Wolfram Alpha Pro ==
On February 8, 2012, Wolfram Alpha Pro was released,&lt;ref name="WAProAnnounce"&gt;{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2012/02/08/announcing-wolframalpha-pro/ |title=Announcing Wolfram&amp;#124;Alpha Pro |date=February 8, 2012 |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}&lt;/ref&gt; offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data&#8212;including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats&#8212;for automatic analysis. Other features include an extended keyboard, interactivity with [[Computable Document Format|CDF]], data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results&lt;ref name="Hachman"&gt;{{cite news |last=Hachman |first=Mark |title=Data Geeks, Meet Wolfram Alpha Pro |publisher=[[PC Magazine]] |date=February 7, 2012 |url=http://www.pcmag.com/article2/0,2817,2399911,00.asp |accessdate=2012-02-15}}&lt;/ref&gt; and extra computation time.&lt;ref name="WAProAnnounce" /&gt;

Along with new premium features, Wolfram Alpha Pro has led to some changes in the free version of the site:
* An increase in advertisements on the free site.
* Text and PDF export options now require the user to set up a free account&lt;ref name="WAProAnnounce" /&gt; even though they existed before the introduction of Wolfram Alpha accounts.&lt;ref&gt;{{cite web|url=http://hplusmagazine.com/2009/06/24/users-guide-wolframalpha/|title=A User's Guide to Wolfram Alpha|first=Surfdaddy|last=Orca|publisher=H+ Magazine|date=2009-06-24|accessdate=2013-04-24}}&lt;/ref&gt;
* The option to request extra time for a long calculation used to be free&lt;ref name="extra-time-before"&gt;{{cite web|url=http://web.mst.edu/~jkmq53/school/Fall_2011/English_160/files/Marlowe_Usability_Test.docx|title=Wolfram Alpha Usability Test Survey|first=James|last=Marlowe|year=2011|accessdate=2013-04-24}}&lt;/ref&gt; but is now only available to subscribers.&lt;ref name="WAProAnnounce" /&gt;
* Step-by-Step limited to 3 for free users (previously uncapped)(no longer available).&lt;ref name="StepByStep"&gt;{{cite web|url=http://blog.wolframalpha.com/2009/12/01/step-by-step-math/|title=Step-by-Step Math}}&lt;/ref&gt;

== Copyright claims ==
''[[InfoWorld]]'' published an article&lt;ref name="copyright"&gt;{{cite web |last=McAllister |first=Neil |url=http://www.infoworld.com/d/developer-world/how-wolfram-alpha-could-change-software-248 |title=How Wolfram Alpha could change software |publisher=InfoWorld |date=July 29, 2009 |accessdate=2012-02-28}}&lt;/ref&gt; warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. [[Free software movement|Free software]] advocate [[Richard Stallman]] also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.&lt;ref name="fsf"&gt;{{cite mailing list |url=http://lists.essential.org/pipermail/a2k/2009-August/004865.html |title=How Wolfram Alpha's Copyright Claims Could Change Software |date=August 4, 2009 |accessdate=2012-02-17 |mailinglist=[http://lists.essential.org/mailman/listinfo/a2k Access 2 Knowledge] |archiveurl=https://web.archive.org/web/20130428041345/http://lists.essential.org/pipermail/a2k/2009-August/004865.html |archivedate=April 28, 2013 |last=Stallman |first=Richard |authorlink=Richard Stallman}}&lt;/ref&gt;

== See also ==
* [[Commonsense knowledge problem]]
* [[Artificial general intelligence|Strong AI]]
* [[Watson (computer)]]

== References ==
{{Reflist|colwidth=30em}}

== Further reading ==
* [http://www.businessweek.com/the_thread/techbeat/archives/2009/03/wolfram_alpha_a.html Wolfram Alpha: A New Way To Search?], Stephen Wildstrom, ''BusinessWeek'', March 9, 2009.
* [http://www.informationweek.com/news/internet/search/showArticle.jhtml?articleID=215801388&amp;subSection=News Stephen Wolfram's Answer To Google: If Wolfram/Alpha works as advertised, it will be able to do something Google can't: provide answers that don't already exist in indexed documents.] by Thomas Claburn, ''InformationWeek'', March 10, 2009.
* [http://bits.blogs.nytimes.com/2009/03/09/better-search-doesnt-mean-beating-google/ Better Search Doesn&#8217;t Mean Beating Google] by Saul Hansell, ''The New York Times'', March 9, 2009.
* [http://www.pcworld.com/article/160904/wolfram_alpha_will_take_your_questions_any_questions.html Wolfram Alpha will Take Your Questions &#8211; Any Questions], Ian Paul, ''PC World'', March 9, 2009.
* [http://www.hplusmagazine.com/articles/ai/wolframalpha-searching-truth Wolfram Alpha: Searching for Truth: Stephen Wolfram talks with Rudy Rucker about his Upcoming Release] by [[Rudy Rucker]], ''H+ Magazine''.
*  [http://www.boston.com/business/technology/articles/2009/05/05/a_hungry_little_number_cruncher/ "A hungry little number cruncher: Wolfram Alpha search tool mines databases to yield math-based replies"] by [[Hiawatha Bray]], ''[[The Boston Globe]]'', May 5, 2009
* [http://newsbreaks.infotoday.com/NewsBreaks/Wolfram-Alpha-Semantic-Search-Is-Born-53892.asp "Wolfram Alpha: Semantic Search is Born" by [[Woody Evans]], May 21, 2009.]

== External links ==
* {{official website}}

{{Wolfram Research|state=uncollapsed}}
{{computable knowledge}}
{{Intelligent personal assistant software}}

[[Category:Agent-based software]]
[[Category:Computer algebra systems]]
[[Category:Educational math software]]
[[Category:Educational websites]]
[[Category:Information retrieval systems]]
[[Category:Intelligent software assistants]]
[[Category:Internet properties established in 2009]]
[[Category:Mathematics education]]
[[Category:Natural language processing software]]
[[Category:Open educational resources]]
[[Category:Physics education]]
[[Category:Semantic Web]]
[[Category:Software calculators]]
[[Category:Web analytics]]
[[Category:Websites which mirror Wikipedia]]
[[Category:Wolfram Research]]</text>
      <sha1>4rgcusbfax4wrgvkta098rd434utjua</sha1>
    </revision>
  </page>
  <page>
    <title>Pleade</title>
    <ns>0</ns>
    <id>35952152</id>
    <revision>
      <id>720657870</id>
      <parentid>692590571</parentid>
      <timestamp>2016-05-17T05:35:03Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Examples */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5340" xml:space="preserve">{{Infobox software
| name                   = Pleade-infoxbox
| title                  = Pleade
| logo                   = [[File:Pleade-logo.png]]
| logo caption           = Logo de Pleade
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = AJLSM
| developer              = AJLSM
| released               = &lt;!-- {{Start date|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 3.4
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = &lt;!-- 3.5 --&gt;
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]
| operating system       = [[Unix-like]], [[Microsoft Windows]]
| platform               = 
| size                   = 
| language               = French, English, German, Chinese
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| status                 = Active
| genre                  = Digital Library
| license                = GNU General Public License
| alexa                  = 
| website                = {{URL|http://www.pleade.com/}}
}}

'''Pleade''' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[Secure Document Exchange|SDX]] platform, it is a very flexible web application.

== History ==
The software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.&lt;ref&gt;[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]&lt;/ref&gt;

==Technologies==
Pleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.

It is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]] is under active development.&lt;ref&gt;[http://pleade.com/ Pleade 2012 : les imprim&#233;s num&#233;ris&#233;s et les formats XML METS / ALTO]&lt;/ref&gt;

== Features ==
* Customizable publication ;
* Customizable index creation ;
* Customizable search form ;
* Simple and advanced search among publish documents ;
* Federate search among different bases (e.g. EAD, METS) ;
* basket (for database and for images), a search history, printing, etc. ;
* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;
* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;
* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;
* Printing resulting and finding aids as PDF documents (with embedded images) ;
* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;
* Ability to import metadata from an [[Integrated library system|ILS]].

=== Pleade-Entreprise ===
* Pleade-Entreprise extended features to others XML format, such as [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]].

== Examples ==
These are examples of websites based on Pleade:
{{columns-list|2|
* Archival portals
** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]
** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]
** [http://odysseo.org/ Odysseo: Resources for the history of immigration]
** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]
** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]
** [http://jubilotheque.upmc.fr/ Jubiloth&#232;que, UPMC's scientific digital library]
** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault's Library "les Mots et les Choses" ENS]
* Portals documentary
** [http://www.michael-culture.org/fr/home Michael]
** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]
* Digital Libraries
** Digital Library of Lille
** Lille III
** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]
}}

== Related resources ==
* {{Official website|http://pleade.com}}
* [http://demo.pleade.com Official demo]
* [http://www.pleadeenpratique.org/ Pleade in practice]
* [http://www.ajlsm.com/produits/sdx SDX]
* [http://www.ajlsm.com AJLSM company]

== References ==
&lt;references/&gt;

[[Category:Digital library software]]
[[Category:Free software]]
[[Category:Information retrieval systems]]
[[Category:Archival science]]</text>
      <sha1>ebujfg66n9mmti7dd8taj14g3bwp9y8</sha1>
    </revision>
  </page>
  <page>
    <title>Spearman's rank correlation coefficient</title>
    <ns>0</ns>
    <id>235623</id>
    <revision>
      <id>750510792</id>
      <parentid>746638684</parentid>
      <timestamp>2016-11-20T05:23:41Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Covariance and correlation]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20456" xml:space="preserve">[[File:spearman fig1.svg|300px|thumb|A Spearman correlation of 1 results when the two variables being compared are monotonically related, even if their relationship is not linear. This means that all data-points with greater x-values than that of a given data-point will have greater y-values as well. In contrast, this does not give a perfect Pearson correlation.]][[File:spearman fig2.svg|300px|thumb|When the data are roughly elliptically distributed and there are no prominent outliers, the Spearman correlation and Pearson correlation give similar values.]]
[[File:spearman fig3.svg|300px|thumb|The Spearman correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples. That is because Spearman's rho limits the outlier to the value of its rank.]] In [[statistics]], '''Spearman's rank correlation coefficient''' or '''Spearman's rho''', named after [[Charles Spearman]] and often denoted by the Greek letter [[rho (letter)|&lt;math&gt;\rho&lt;/math&gt;]] (rho) or as &lt;math&gt;r_s&lt;/math&gt;, is a [[non-parametric statistics|nonparametric]] measure of [[rank correlation]] ([[correlation and dependence|statistical dependence]] between the [[ranking]] of two [[Variable (mathematics)#Applied statistics|variables]]). It assesses how well the relationship between two variables can be described using a [[monotonic]] function.

The '''Spearman correlation''' between two variables is equal to the [[Pearson product-moment correlation coefficient|Pearson correlation]] between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or &#8722;1 occurs when each of the variables is a perfect monotone function of the other.

Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) [[Ranking|rank]] (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of -1) rank between the two variables.

Spearman's coefficient is appropriate for both [[continuous variable|continuous]] and [[discrete variable]]s, including [[Level of measurement#Ordinal scale|ordinal]] variables.&lt;ref&gt;[[Level of measurement#Typology|Scale types]]&lt;/ref&gt;&lt;ref&gt;{{cite book|title=Jmp For Basic Univariate And Multivariate Statistics: A Step-by-step Guide|last=Lehman|first=Ann|publisher=SAS Press|year=2005|isbn=1-59047-576-3|location=Cary, NC|page=123}}&lt;/ref&gt; Both Spearman's &lt;math&gt;\rho&lt;/math&gt; and [[Kendall tau rank correlation coefficient|Kendall's &lt;math&gt;\tau&lt;/math&gt;]] can be formulated as special cases of a more [[general correlation coefficient]].

==Definition and calculation==
The Spearman correlation coefficient is defined as the [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]] between the [[Ranking|ranked variables]].&lt;ref name="myers2003"&gt;{{Cite book | last1=Myers | first1=Jerome L. | first2=Arnold D.  |last2= Well | title=Research Design and Statistical Analysis | publisher=Lawrence Erlbaum | year=2003 | edition=2nd | isbn=0-8058-4037-0 | pages=508}}&lt;/ref&gt;

For a sample of size ''n'', the ''n'' [[raw score]]s &lt;math&gt;X_i, Y_i&lt;/math&gt; are converted to ranks &lt;math&gt;\operatorname{rg} X_i, \operatorname{rg} Y_i&lt;/math&gt;, and &lt;math&gt;r_s&lt;/math&gt; is computed from:

:&lt;math&gt;r_s = \rho_{\operatorname{rg}_X,\operatorname{rg}_Y} = \frac {\operatorname{cov}(\operatorname{rg}_X,\operatorname{rg}_Y)} { \sigma_{\operatorname{rg}_X} \sigma_{\operatorname{rg}_Y} }&lt;/math&gt;
:: where
::* &lt;math&gt;\rho&lt;/math&gt; denotes the usual [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]], but applied to the rank variables.
::* &lt;math&gt;\operatorname{cov}(\operatorname{rg}_X, \operatorname{rg}_Y)&lt;/math&gt; is the [[covariance]] of the rank variables.
::* &lt;math&gt;\sigma_{\operatorname{rg}_X}&lt;/math&gt; and &lt;math&gt;\sigma_{\operatorname{rg}_Y}&lt;/math&gt; are the [[standard deviation]]s of the rank variables.

Only if all ''n'' ranks are ''distinct integers'', it can be computed using the popular formula

:&lt;math&gt; r_s = {1- \frac {6 \sum d_i^2}{n(n^2 - 1)}}.&lt;/math&gt;
:: where
::* &lt;math&gt;d_i = \operatorname{rg}(X_i) - \operatorname{rg}(Y_i)&lt;/math&gt;, is the difference between the two ranks of each observation.
::* ''n'' is the number of observations

Identical values are usually{{citation needed|date=May 2016}} each assigned [[Ranking#Fractional ranking .28.221 2.5 2.5 4.22 ranking.29|fractional ranks]] equal to the average of their positions in the ascending order of the values, which is equivalent to averaging over all possible permutations.

If ties are present in the data set, this equation yields incorrect results: Only if in both variables all ranks are distinct, then &lt;math&gt;\sigma_{\operatorname{rg}_X} \sigma_{\operatorname{rg}_Y} = \operatorname{Var}{\operatorname{rg}_X} = \operatorname{Var}{\operatorname{rg}_Y} = n(n^2 - 1)/6&lt;/math&gt; (cf. [[tetrahedral number]] &lt;math&gt;T_{n-1}&lt;/math&gt;).
The first equation&#8212;normalizing by the standard deviation&#8212;may even be used even when ranks are normalized to [0;1] ("relative ranks") because it is insensitive both to translation and linear scaling.
&lt;!-- For example, if [1,2,3,4,5] vs. [1,3,3,3,5] has r_s=0.894, but the simplified formula yields 0.877. --&gt;

This method should also not be used in cases where the data set is truncated; that is, when the Spearman correlation coefficient is desired for the top X records (whether by pre-change rank or post-change rank, or both), the user should use the Pearson correlation coefficient formula given above.{{citation needed|date=September 2015}}

The standard error of the coefficient (''&#963;'') was determined by Pearson in 1907 and Gosset in 1920. It is

: &lt;math&gt; \sigma_{r_s} = \frac{ 0.6325 }{ \sqrt{n-1} } &lt;/math&gt;

==Related quantities==
{{Main article|Correlation and dependence}}

There are several other numerical measures that quantify the extent of [[statistical dependence]] between pairs of observations. The most common of these is the [[Pearson product-moment correlation coefficient]], which is a similar correlation method to Spearman's rank, that measures the &#8220;linear&#8221; relationships between the raw numbers rather than between their ranks.

An alternative name for the Spearman [[rank correlation]] is the &#8220;grade correlation&#8221;;&lt;ref name="Yule and Kendall"&gt;{{cite book |last=Yule |first=G. &#8201;U. |last2=Kendall |first2=M. &#8201;G. |orig-year=1950 |title=An Introduction to the Theory of Statistics |edition=14th |year=1968 |publisher=Charles Griffin &amp; Co. |page=268 }}&lt;/ref&gt; in this, the &#8220;rank&#8221; of an observation is replaced by the &#8220;grade&#8221;. In continuous distributions, the grade of an observation is, by convention, always one half less than the rank, and hence the grade and rank correlations are the same in this case. More generally, the &#8220;grade&#8221; of an observation is proportional to an estimate of the fraction of a population less than a given value, with the half-observation adjustment at observed values. Thus this corresponds to one possible treatment of tied ranks. While unusual, the term &#8220;grade correlation&#8221; is still in use.&lt;ref&gt;{{cite journal |last=Piantadosi |first=J. |last2=Howlett |first2=P. |last3=Boland |first3=J. |year=2007 |title=Matching the grade correlation coefficient using a copula with maximum disorder |journal=Journal of Industrial and Management Optimization |volume=3 |issue=2 |pages=305&#8211;312 |doi= |url=http://aimsciences.org/journals/pdfs.jsp?paperID=2265&amp;mode=abstract }}&lt;/ref&gt;

==Interpretation==
{| style="float: right;"
|+ '''Positive and negative Spearman rank correlations'''
|- 
| [[File:spearman fig5.svg|300px|left|thumb|A positive Spearman correlation coefficient corresponds to an increasing monotonic trend between ''X'' and ''Y''.]]
| [[File:spearman fig4.svg|300px|thumb|A negative Spearman correlation coefficient corresponds to a decreasing monotonic trend between ''X'' and ''Y''.]]
|}

The sign of the Spearman correlation indicates the direction of association between ''X'' (the independent variable) and ''Y'' (the dependent variable).  If  ''Y'' tends to increase when ''X'' increases, the Spearman correlation coefficient is positive.  If ''Y'' tends to decrease when ''X'' increases, the Spearman correlation coefficient is negative.  A Spearman correlation of zero indicates that there is no tendency for ''Y'' to either increase or decrease when ''X'' increases.  The Spearman correlation increases in magnitude as ''X'' and ''Y'' become closer to being perfect monotone functions of each other.  When ''X'' and ''Y'' are perfectly monotonically related, the Spearman correlation coefficient becomes 1.  A perfect monotone increasing relationship implies that for any two pairs of data values {{math|''X''&lt;sub&gt;''i''&lt;/sub&gt;, ''Y''&lt;sub&gt;''i''&lt;/sub&gt;}} and {{math|''X''&lt;sub&gt;''j''&lt;/sub&gt;, ''Y''&lt;sub&gt;''j''&lt;/sub&gt;}}, that {{math|''X''&lt;sub&gt;''i''&lt;/sub&gt; &#8722; ''X''&lt;sub&gt;''j''&lt;/sub&gt;}} and {{math|''Y''&lt;sub&gt;''i''&lt;/sub&gt; &#8722; ''Y''&lt;sub&gt;''j''&lt;/sub&gt;}} always have the same sign.  A perfect monotone decreasing relationship implies that these differences always have opposite signs.

The Spearman correlation coefficient is often described as being "nonparametric".  This can have two meanings:  First, a perfect Spearman correlation results when ''X'' and ''Y'' are related by any [[monotonic function]]. Contrast this with the Pearson correlation, which only gives a perfect value when ''X'' and ''Y'' are related by a ''linear'' function. The other sense in which the Spearman correlation is nonparametric in that its exact sampling distribution can be obtained without requiring knowledge (''i.e.'', knowing the parameters) of the joint [[probability distribution]] of ''X'' and ''Y''.

==Example==
In this example, the raw data in the table below is used to calculate the correlation between the [[IQ]] of a person with the number of hours spent in front of [[TV]] per week.
{| class="wikitable sortable" style="text-align:right;"
|-
![[IQ]], &lt;math&gt;X_i&lt;/math&gt;
!Hours of [[TV]] per week, &lt;math&gt;Y_i&lt;/math&gt;
|-
|106
|7
|-
|86
|0
|-
|100
|27
|-
|101
|50
|-
|99
|28
|-
|103
|29
|-12
|97
|20
|-
|113
|12
|-
|112
|6
|-
|110
|17
|}

Firstly, evaluate &lt;math&gt;d^2_i&lt;/math&gt;. To do so use the following steps, reflected in the table below.
# Sort the data by the first column (&lt;math&gt;X_i&lt;/math&gt;). Create a new column &lt;math&gt;x_i&lt;/math&gt; and assign it the ranked values 1,2,3,...''n''.
# Next, sort the data by the second column (&lt;math&gt;Y_i&lt;/math&gt;). Create a fourth column &lt;math&gt;y_i&lt;/math&gt; and similarly assign it the ranked values 1,2,3,...''n''.
# Create a fifth column &lt;math&gt;d_i&lt;/math&gt; to hold the differences between the two rank columns (&lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;y_i&lt;/math&gt;).
# Create one final column &lt;math&gt;d^2_i&lt;/math&gt; to hold the value of column &lt;math&gt;d_i&lt;/math&gt; squared.

{| class="wikitable sortable" style="text-align:right;"
|-
![[IQ]], &lt;math&gt;X_i&lt;/math&gt;
!Hours of [[TV]] per week, &lt;math&gt;Y_i&lt;/math&gt;
!rank &lt;math&gt;x_i&lt;/math&gt;
!rank &lt;math&gt;y_i&lt;/math&gt;
!&lt;math&gt;d_i&lt;/math&gt;
!&lt;math&gt;d^2_i&lt;/math&gt;
|-
|86
|0
|1
|1
|0
|0
|-
|97
|20
|2
|6
| &#8722;4
|16
|-
|99
|28
|3
|8
| &#8722;5
|25
|-
|100
|27
|4
|7
| &#8722;3
|9
|-
|101
|50
|5
|10
| &#8722;5
|25
|-
|103
|29
|6
|9
| &#8722;3
|9
|-
|106
|7
|7
|3
|4
|16
|-
|110
|17
|8
|5
|3
|9
|-
|112
|6
|9
|2
|7
|49
|-
|113
|12
|10
|4
|6
|36
|}

With &lt;math&gt;d^2_i&lt;/math&gt; found, add them to find &lt;math&gt;\sum d_i^2 = 194&lt;/math&gt;. The value of ''n'' is 10. These values can now be substituted back into the equation: &lt;math&gt; \rho = 1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}}.&lt;/math&gt; to give

:&lt;math&gt; \rho = 1- {\frac {6\times194}{10(10^2 - 1)}}&lt;/math&gt;

which evaluates to {{math|1=''&#961;'' = -29/165 = &#8722;0.175757575...}}
with a [[P-value]] = 0.627188 (using the [[Student's t-distribution|t distribution]])

[[File:Spearman's Rank chart.png|thumb|Chart of the data presented. It can be seen that there might be a negative correlation, but that the relationship does not appear definitive.]]
This low value shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above).

==Determining significance==
One approach to test whether an observed value of &#961; is significantly different from zero (''r'' will always maintain &#8722;1 &#8804; ''r'' &#8804; 1) is to calculate the probability that it would be greater than or equal to the observed ''r'', given the [[null hypothesis]], by using a [[Resampling (statistics)#Permutation tests|permutation test]]. An advantage of this approach is that it automatically takes into account the number of tied data values there are in the sample, and the way they are treated in computing the rank correlation.

Another approach parallels the use of the [[Fisher transformation]] in the case of the Pearson product-moment correlation coefficient. That is, [[confidence intervals]] and [[hypothesis test]]s relating to the population value &#961; can be carried out using the Fisher transformation:

: &lt;math&gt;F(r) = {1 \over 2}\ln{1+r \over 1-r} = \operatorname{artanh}(r).&lt;/math&gt;

If ''F''(''r'') is the Fisher transformation of ''r'', the sample Spearman rank correlation coefficient, and ''n'' is the sample size, then

:&lt;math&gt;z = \sqrt{\frac{n-3}{1.06}}F(r)&lt;/math&gt;

is a [[standard score|z-score]] for ''r'' which approximately follows a standard [[normal distribution]] under the [[null hypothesis]] of [[statistical independence]] ({{math|1=''&#961;'' = 0}}).&lt;ref&gt;{{cite journal |last=Choi |first=S. C. |year=1977 |title=Tests of Equality of Dependent Correlation Coefficients |journal=[[Biometrika]] |volume=64 |issue=3 |pages=645&#8211;647 |doi=10.1093/biomet/64.3.645 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |last=Fieller |first=E. C. |last2=Hartley |first2=H. O. |last3=Pearson |first3=E. S. |year=1957 |title=Tests for rank correlation coefficients. I |journal=Biometrika |volume=44 |issue= |pages=470&#8211;481 |doi=10.1093/biomet/44.3-4.470}}&lt;/ref&gt;

One can also test for significance using

:&lt;math&gt;t = r \sqrt{\frac{n-2}{1-r^2}}&lt;/math&gt;

which is distributed approximately as [[Student's t distribution]] with {{math|''n'' &#8722; 2}} degrees of freedom under the [[null hypothesis]].&lt;ref&gt;{{cite book |last=Press |last2=Vettering |last3=Teukolsky |last4=Flannery |year=1992 |title=Numerical Recipes in C: The Art of Scientific Computing |edition=2nd |page=640 }}&lt;/ref&gt; A justification for this result relies on a permutation argument.&lt;ref&gt;{{cite book |last=Kendall |first=M. G. |last2=Stuart |first2=A. |year=1973 |title=The Advanced Theory of Statistics, Volume 2: Inference and Relationship |publisher=Griffin |isbn=0-85264-215-6 }} (Sections 31.19, 31.21)&lt;/ref&gt;

pvrank&lt;ref&gt;{{cite web|last1=Amerise|first1=I.L.|last2=Marozzi|first2=M.|last3=Tarsitano|first3=A.|title=R package pvrank|url=https://cran.r-project.org/web/packages/pvrank/index.html}}&lt;/ref&gt; is a very recent [[R (programming language)|R]] package that computes rank correlations and their p-values with various options for tied ranks. It is possible to compute exact Spearman coefficient test p-values for ''n'' &#8804; 26.

A generalization of the Spearman coefficient is useful in the situation where there are three or more conditions, a number of subjects are all observed in each of them, and it is predicted that the observations will have a particular order.  For example, a number of subjects might each be given three trials at the same task, and it is predicted that performance will improve from trial to trial.  A test of the significance of the trend between conditions in this situation was developed by E. B. Page&lt;ref&gt;{{cite journal |author=Page, E. B. |title=Ordered hypotheses for multiple treatments: A significance test for linear ranks |journal=Journal of the American Statistical Association |volume=58 |pages=216&#8211;230 |year=1963 |doi=10.2307/2282965 |issue=301}}
&lt;/ref&gt; and is usually referred to as [[Page's trend test]] for ordered alternatives.

==Correspondence analysis based on Spearman's rho==
Classic [[correspondence analysis]] is a statistical method that gives a score to every value of two nominal variables. In this way the Pearson [[Pearson product-moment correlation coefficient|correlation coefficient]] between them is maximized.

There exists an equivalent of this method, called [[grade correspondence analysis]], which maximizes Spearman's rho or [[Kendall's tau]].&lt;ref&gt;{{cite book|editor1-last=Kowalczyk|editor1-first=T.|editor2-last=Pleszczy&#324;ska|editor2-first=E.|editor3-last=Ruland|editor3-first=F.| year=2004|title=Grade Models and Methods for Data Analysis with Applications for the Analysis of Data Populations|series=Studies in Fuzziness and Soft Computing |volume=151|publisher=Springer Verlag|location=Berlin Heidelberg New York|isbn=978-3-540-21120-4}}&lt;/ref&gt;

==See also==
{{Portal|Statistics}}
* [[Kendall tau rank correlation coefficient]]
* [[Chebyshev's sum inequality]], [[rearrangement inequality]] (These two articles may shed light on the mathematical properties of Spearman's &#961;.)
*[[Distance correlation]]

==References==
{{Reflist|30em}}

==Further reading==
* Corder, G.W. &amp; Foreman, D.I. (2014). Nonparametric Statistics: A Step-by-Step Approach, Wiley. ISBN 978-1118840313.
* {{cite book |last=Daniel |first=Wayne W. |chapter=Spearman rank correlation coefficient |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=0-534-91976-6 |pages=358&#8211;365 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&amp;pg=PA358 }}
* {{Cite journal |author=Spearman C |title=The proof and measurement of association between two things |journal=American Journal of Psychology |volume=15 |year=1904 |pages=72&#8211;101 |doi=10.2307/1412159}}
* {{Cite journal |author=Bonett DG, Wright, TA |title=Sample size requirements for Pearson, Kendall, and Spearman correlations |journal=Psychometrika |volume=65 |year=2000 |pages=23&#8211;28 |doi=10.1007/bf02294183}}
* {{Cite book |author=Kendall MG |title=Rank correlation methods |location=London |publisher=Griffin |year=1970 |edition=4th |isbn=978-0-852-6419-96 |oclc=136868}}
* {{Cite book |vauthors=Hollander M, Wolfe DA |title=Nonparametric statistical methods |location=New York |publisher=Wiley |year=1973 |isbn=978-0-471-40635-8 |oclc=520735}}
* {{Cite journal |vauthors=Caruso JC, Cliff N |title=Empirical size, coverage, and power of confidence intervals for Spearman's Rho |journal=Educational and Psychological Measurement |volume=57 |year=1997 |pages=637&#8211;654 |doi=10.1177/0013164497057004009}}

==External links==
{{Wikiversity}}
*[http://www.crystalballservices.com/Resources/ConsultantsCornerBlog/EntryId/73/Copulas-Vs-Correlation.aspx "Understanding Correlation vs. Copulas in Excel"] by Eric Torkia, Technology Partnerz 2011
*[http://www.sussex.ac.uk/Users/grahamh/RM1web/Rhotable.htm Table of critical values of &#961; for significance with small samples]
*[http://www.maccery.com/maths Spearman's rank online calculator]
*[https://www.answerminer.com/calculators/correlation-test Spearman correlation calculator with human-readable explanation]
*[http://faculty.vassar.edu/lowry/webtext.html Chapter 3 part 1 shows the formula to be used when there are ties]
*[http://statistical-research.com/wp-content/uploads/2012/08/Spearman.pdf An example of how to calculate Spearman's Rho along with basic R code.]
* [https://www.rgs.org/NR/rdonlyres/4844E3AB-B36D-4B14-8A20-3A3C28FAC087/0/OASpearmansRankExcelGuidePDF.pdf Spearman&#8217;s Rank Correlation Coefficient &#8211; Excel Guide]: sample data and formulae for Excel, developed by the [[Royal Geographical Society]].
*[http://udel.edu/~mcdonald/statspearman.html Spearman's rank correlation]: Simple notes for students with an example of usage by biologists and a spreadsheet for [[Microsoft Excel]] for calculating it (a part of materials for a ''Research Methods in Biology'' course).
{{Statistics|descriptive}}

{{DEFAULTSORT:Spearman's Rank Correlation Coefficient}}
[[Category:Covariance and correlation]]
[[Category:Information retrieval evaluation]]
[[Category:Nonparametric statistics]]
[[Category:Statistical tests]]</text>
      <sha1>hnu81ng3vb2oe4vfiz61fqf93blv5mr</sha1>
    </revision>
  </page>
  <page>
    <title>Query likelihood model</title>
    <ns>0</ns>
    <id>29979321</id>
    <revision>
      <id>702405809</id>
      <parentid>685020836</parentid>
      <timestamp>2016-01-30T10:48:50Z</timestamp>
      <contributor>
        <username>Pyrexed</username>
        <id>27449922</id>
      </contributor>
      <minor />
      <comment>/* Calculating the likelihood */ The formula had confused N with M.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2580" xml:space="preserve">The '''query likelihood model''' is a [[language model]] used in [[information retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.

==Calculating the likelihood==
Using [[Bayes' theorem|Bayes' rule]], the probability &lt;math&gt;P&lt;/math&gt; of a document &lt;math&gt;d&lt;/math&gt;, given a query &lt;math&gt;q&lt;/math&gt; can be written as follows:

:&lt;math&gt;
 P(d|q) = \frac{P(q|d) P(d)}{P(q)}
&lt;/math&gt;

Since the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.

:&lt;math&gt;
 P(d|q) = P(q|d)
&lt;/math&gt;

Documents are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:
:&lt;math&gt;
 P(q|M_d) = K_q \prod_{t \in V} P(t|M_d)^{tf_{t,q}}
&lt;/math&gt;,where the multinomial coefficient is &lt;math&gt;K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tN,q}!)&lt;/math&gt; for query {{math|q}}, 

and &lt;math&gt;L_q = \sum_{1 \leq i \leq N}tf_{t_i,q}&lt;/math&gt; is the length of query {{math|q}} given the term frequencies {{math|tf}} in the query vocabulary {{math|N}}.

In practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document &lt;math&gt;d&lt;/math&gt;). The language model &lt;math&gt;M_d&lt;/math&gt; should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So &lt;math&gt;P(t|M_d)&lt;/math&gt; is the probability of term &lt;math&gt;t&lt;/math&gt; being generated by the language model &lt;math&gt;M_d&lt;/math&gt; of document &lt;math&gt;d&lt;/math&gt;. This probability is multiplied for all terms from query &lt;math&gt;q&lt;/math&gt; to get a rank for document &lt;math&gt;d&lt;/math&gt; in the interval &lt;math&gt;[0,1]&lt;/math&gt;. The calculation is repeated for all documents to create a ranking of all documents in the document collection.

&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch&#252;tze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009&lt;/ref&gt;

==References==
 &lt;references/&gt;

[[Category:Information retrieval techniques]]</text>
      <sha1>kx74px07ic8x0mxxsbnerzf1yanz736</sha1>
    </revision>
  </page>
  <page>
    <title>Stemming</title>
    <ns>0</ns>
    <id>30874683</id>
    <revision>
      <id>741820710</id>
      <parentid>741819972</parentid>
      <timestamp>2016-09-29T21:33:52Z</timestamp>
      <contributor>
        <username>Jim Carnicelli</username>
        <id>2186481</id>
      </contributor>
      <minor />
      <comment>/* Suffix-stripping algorithms */ Added link to "Lemmatisation"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27719" xml:space="preserve">{{about||the skiing technique|Stem (skiing)|the climbing technique|Glossary of climbing terms#stem}}
{{Expert needed|date=October 2010}}
In [[linguistic morphology]] and [[information retrieval]], '''stemming''' is the process of reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form&#8212;generally a written word form. The stem need not be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.

Stemming programs are commonly referred to as stemming algorithms or stemmers.

==Examples==
A stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stems", "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".&lt;!-- using the Porter algorithm --&gt;

==History==
The first published stemmer was written by [[Julie Beth Lovins]] in 1968.&lt;ref&gt;{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22&#8211;31 }}&lt;/ref&gt; This paper was remarkable for its early date and had great influence on later work in this area.

A later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal ''Program''. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.

Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [[free software]] (mostly [[BSD licenses|BSD]]-licensed) implementation&lt;ref&gt;http://tartarus.org/~martin/PorterStemmer/&lt;/ref&gt; of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.

==Algorithms==
There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.

A simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach are that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.

A lookup approach may use preliminary part-of-speech tagging to avoid overstemming.&lt;ref&gt;Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html ''Y-stemmer'']&lt;/ref&gt;

===The production technique===

The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.

===Suffix-stripping algorithms===
Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:
* if the word ends in 'ed', remove the 'ed'
* if the word ends in 'ing', remove the 'ing'
* if the word ends in 'ly', remove the 'ly'

Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. [[Lemmatisation]] attempts to improve upon this challenge.

Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.

====Additional algorithm criteria====
Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.

It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term ''friendlies'', the algorithm may identify the ''ies'' suffix and apply the appropriate rule and achieve the result of ''friendl''. ''friendl'' is likely not found in the lexicon, and therefore the rule is rejected.

One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ''ies'' with ''y''. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ''ies'' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, ''friendlies'' becomes ''friendly'' instead of ''friendl''.

Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term ''friendly'', where the ''ly'' stripping rule is likely identified and accepted. In summary, ''friendlies'' becomes (via substitution) ''friendly'' which becomes (via stripping) ''friend''.

This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for ''friendlies'' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form ''friend''. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.

===Lemmatisation algorithms===
A more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.

This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).

===Stochastic algorithms===
[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).

Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.

===''n''-gram analysis===
Some stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.&lt;ref name="CEUR Proceedings"&gt;{{cite journal|last1=McNamee|first1=Paul|title=Exploring New Languages with HAIRCUT at CLEF 2005|journal=CEUR Workshop Proceedings|date=September 21&#8211;22, 2005|volume=1171|url=http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-McNamee2005.pdf|accessdate=3/6/15}}&lt;/ref&gt;

===Hybrid approaches===
Hybrid approaches use two or more of the approaches described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran =&gt; run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.

===Affix stemmers===
In [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. A study of affix stemming for several European languages can be found here.&lt;ref&gt;Jongejan, B.; and Dalianis, H.; ''Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike'', in the ''Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2&#8211;7, 2009'', pp. 145-153
[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]&lt;/ref&gt;

===Matching algorithms===
Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").

==Language challenges==
While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.&lt;ref&gt;Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf ''Stemming Approaches for East European Languages (CLEF 2007)'']&lt;/ref&gt;&lt;ref&gt;Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 ''Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages''], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2&lt;/ref&gt;&lt;ref&gt;Popovi&#269;, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384&#8211;390&lt;/ref&gt;&lt;ref&gt;[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf ''Stemming in Hungarian at CLEF 2005'']&lt;/ref&gt;&lt;ref&gt;Viera, A. F. G. &amp; Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html ''Uma revis&#227;o dos algoritmos de radicaliza&#231;&#227;o em l&#237;ngua portuguesa''], Information Research, 12(3), paper 315&lt;/ref&gt;

Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.

===Multilingual stemming===
Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{Citation needed|date=October 2013}}.

==Error metrics==
There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been&#8212;a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not&#8212;a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.

For example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.

An example of understemming in the Porter stemmer is "alumnus" &#8594; "alumnu", "alumni" &#8594; "alumni", "alumna"/"alumnae" &#8594; "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.

==Applications==
Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".

===Information retrieval===
Stemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.&lt;ref&gt;Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); ''Modern Information Retrieval'', ACM Press/Addison Wesley&lt;/ref&gt; An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, stemmers may provide greater benefits in other languages than English.&lt;ref&gt;Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbj&#246;rnsson, B&#246;rkur (2004); ''Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval'', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); ''Comparative Evaluation of Multilingual Information Access Systems'', Springer Verlag, pp. 152&#8211;165&lt;/ref&gt;&lt;ref&gt;Airio, Eija (2006); ''Word Normalization and Decompounding in Mono- and Bilingual IR'', Information Retrieval '''9''':249&#8211;271&lt;/ref&gt;

===Domain Analysis===
Stemming is used to determine domain vocabularies in [[domain analysis]].
&lt;ref&gt;Frakes, W.; Prieto-Diaz, R.; &amp; Fox, C. (1998); ''DARE: Domain Analysis and Reuse Environment'', Annals of Software Engineering (5), pp. 125-141&lt;/ref&gt;

===Use in commercial products===
Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.&lt;ref&gt;[http://www.dtsearch.co.uk/language.htm ''Language Extension Packs''], dtSearch&lt;/ref&gt;&lt;ref&gt;[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true ''Building Multilingual Solutions by using Sharepoint Products and Technologies''], Microsoft Technet&lt;/ref&gt;

The [[Snowball (programming language)|Snowball]] stemmers have been compared with commercial lexical stemmers with varying results.&lt;ref&gt;[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]&lt;/ref&gt;&lt;ref&gt;[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]&lt;/ref&gt;

[[Google search]] adopted word stemming in 2003.&lt;ref&gt;[http://www.google.com/support/bin/static.py?page=searchguides.html&amp;ctx=basics#stemming ''The Essentials of Google Search''], Web Search Help Center, [[Google|Google Inc.]]&lt;/ref&gt; Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".

==See also==
* [[Root (linguistics)]] - linguistic definition of the term "root"
* [[Stem (linguistics)]] - linguistic definition of the term "stem"
* [[Morphology (linguistics)]]
* [[Lemma (morphology)]] - linguistic definition
* [[Lemmatization]]
* [[Lexeme]]
* [[Inflection]]
* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation
* [[Natural language processing]] - stemming is generally regarded as a form of NLP
* [[Text mining]] - stemming algorithms play a major role in commercial NLP software
* [[Computational linguistics]]
* [[Snowball (programming language)]] - designed for creating stemming algorithms

{{Natural Language Processing}}

==References==
{{reflist|2}}

==Further reading==
{{refbegin|2}}
* Dawson, J. L. (1974); ''Suffix Removal for Word Conflation'', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33&#8211;46
* Frakes, W. B. (1984); ''Term Conflation for Information Retrieval'', Cambridge University Press
* Frakes, W. B. &amp; Fox, C. J. (2003); ''Strength and Similarity of Affix Removal Stemming Algorithms'', SIGIR Forum, 37: 26&#8211;30
* Frakes, W. B. (1992); ''Stemming algorithms, Information retrieval: data structures and algorithms'', Upper Saddle River, NJ: Prentice-Hall, Inc.
* Hafer, M. A. &amp; Weiss, S. F. (1974); ''Word segmentation by letter successor varieties'', Information Processing &amp; Management 10 (11/12), 371&#8211;386
* Harman, D. (1991); ''How Effective is Suffixing?'', Journal of the American Society for Information Science 42 (1), 7&#8211;15
* Hull, D. A. (1996); ''Stemming Algorithms&amp;nbsp;&#8211; A Case Study for Detailed Evaluation'', JASIS, 47(1): 70&#8211;84
* Hull, D. A. &amp; Grefenstette, G. (1996); ''A Detailed Analysis of English Stemming Algorithms'', Xerox Technical Report
* Kraaij, W. &amp; Pohlmann, R. (1996); ''Viewing Stemming as Recall Enhancement'', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); ''Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18&#8211;22'', pp.&amp;nbsp;40&#8211;48
* Krovetz, R. (1993); ''Viewing Morphology as an Inference Process'', in ''Proceedings of ACM-SIGIR93'', pp.&amp;nbsp;191&#8211;203
* Lennon, M.; Pierce, D. S.; Tarry, B. D.; &amp; Willett, P. (1981); ''An Evaluation of some Conflation Algorithms for Information Retrieval'', Journal of Information Science, 3: 177&#8211;183
* Lovins, J. (1971); ''[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]'', JASIS, 22: 28&#8211;40
* Lovins, J. B. (1968); ''Development of a Stemming Algorithm'', Mechanical Translation and Computational Linguistics, 11, 22&#8212;31
* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf ''Conservative Stemming for Search and Indexing'']
* Paice, C. D. (1990); ''[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]'', SIGIR Forum, 24: 56&#8211;61
* Paice, C. D. (1996) ''[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]'', JASIS, 47(8): 632&#8211;649
* Popovi&#269;, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&amp;nbsp;384&#8211;390
* Porter, Martin F. (1980); ''[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]'', Program, 14(3): 130&#8211;137
* Savoy, J. (1993); ''[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&amp;SRETRY=0 Stemming of French Words Based on Grammatical Categories]'' Journal of the American Society for Information Science, 44(1), 1&#8211;9
* Ulmschneider, John E.; &amp; Doszkocs, Tamas (1983); ''[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]'', Online Review, 7(4), 301&#8211;318
* Xu, J.; &amp; Croft, W. B. (1998); ''[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]'', ACM Transactions on Information Systems, 16(1), 61&#8211;81
{{refend}}

==External links==
*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers
* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)
* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)
* [http://snowballstem.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages
* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)
* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]
* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API
* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API
* [http://www.oleandersolutions.com/stemming.html Oleander Porter's algorithm] - stemming library in C++ released under BSD
* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages
* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages
* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK
* [https://www.uea.ac.uk/computing/word-stemming/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK
* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]
* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language
* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages
* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java
* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi
* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech
* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]
* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]

{{FOLDOC}}

[[Category:Linguistic morphology]]
[[Category:Natural language processing]]
[[Category:Tasks of natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]</text>
      <sha1>7ib34q4dkhfzsu7zebl7z5r22jxxem7</sha1>
    </revision>
  </page>
  <page>
    <title>Tag (metadata)</title>
    <ns>0</ns>
    <id>1707086</id>
    <revision>
      <id>760969232</id>
      <parentid>758748634</parentid>
      <timestamp>2017-01-20T02:54:01Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Triple tags */HTTP&amp;rarr;HTTPS for [[Yahoo!]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21508" xml:space="preserve">{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}
[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]

In [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.

Tagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.

==History==

Labeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.

[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In "The Equator" the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular ''Tags'', and did not share ''Tags'' at that point.

In 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.&lt;ref&gt;[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.&lt;/ref&gt; [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.&lt;ref&gt;[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr's Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person&#8217;s photos with a particular tag."&lt;/ref&gt; The success of Flickr and the influence of Delicious popularized the concept,&lt;ref&gt;An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.&lt;/ref&gt; and other [[social software]] websites&amp;nbsp;&#8211; such as [[YouTube]], [[Technorati]], and [[Last.fm]]&amp;nbsp;&#8211; also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].

Tagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or &#8220;tags&#8221;) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.

Knowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.&lt;ref&gt;{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}&lt;/ref&gt; Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].

Websites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.

Tags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.&lt;ref&gt;[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.&lt;/ref&gt;

==Examples==

===Within a blog===
Many [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.

===For an event===
An official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].

===In research===
A researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.

==Special types==

===Triple tags===
{{see also|Microformat}}
A '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "&lt;nowiki&gt;geo:long=50.123456&lt;/nowiki&gt;" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.

The triple tag format was first devised for geolicious&lt;ref&gt;{{cite web |url=http://brainoff.com/weblog/2004/11/05/124 |title=geo.lici.us: geotagging hosted services |first1=Mikel |last1=Maron |date=November 5, 2004}}&lt;/ref&gt; in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers&lt;ref&gt;[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.&lt;/ref&gt; to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.&lt;ref&gt;[https://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.&lt;/ref&gt;

Specialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].&lt;ref&gt;[https://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.&lt;/ref&gt;

===Hashtags===
{{main|Hashtag}}
A hashtag is a kind of metadata tag marked by the prefix &lt;code&gt;#&lt;/code&gt;, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].

===Knowledge tags===
A knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.

Capturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).&lt;ref&gt;
{{Citation
 | last=Wiig | first=K. M.
 | year= 1997
 | title=Knowledge Management: An Introduction and Perspective
 | journal=Journal of Knowledge Management
 | volume=1 | issue=1
 | pages=6&#8211;14
 | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/
 | doi=10.1108/13673279710800682
}}
&lt;/ref&gt; These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise.

Knowledge tags, in fact, manifest themselves in any number of ways &#8211; conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.&lt;ref&gt;
{{citation
 | last=Getting | first=Brian
 | year= 2007
 | title=What Are "Tags" And What Is "Tagging?
 | publisher=Practical eCommerce
 | url=http://www.practicalecommerce.com/articles/589
}}
&lt;/ref&gt;

Knowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information. Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.&lt;ref&gt;
{{Citation
 | last=Alavi | first=Maryam
 | last2=Leidner
 | year= 1999
 | title=Knowledge Management Systems: Issues, Challenges, and Benefits
 | journal=Communications of the Association for Information Systems
 | volume=1 | issue=7
 | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf
}}
&lt;/ref&gt;

==Advantages and disadvantages==
{{procon|date=November 2012}}

In a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.&lt;ref name="Smith2008"&gt;Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0&lt;/ref&gt; The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.

When users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.&lt;ref&gt;Golder, Scott A. Huberman, Bernardo A. (2005).
"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.&lt;/ref&gt; For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),&lt;ref&gt;[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.&lt;/ref&gt; which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.

===Complex system dynamics===

Despite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,&lt;ref name="WWW07-ref"&gt;Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]&lt;/ref&gt; (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.&lt;ref name="WWW07-ref"/&gt; Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].

===Spamming===

Tagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.&lt;ref&gt;[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.&lt;/ref&gt; The number of tags allowed may also be limited to reduce spam.

==Syntax==
Some tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.

A syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value "tag" (i.e., &lt;code&gt;rel="tag"&lt;/code&gt;) to indicate that the linked-to page acts as a tag for the current context.&lt;ref&gt;[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.&lt;/ref&gt;

==See also==
{{colbegin||27em}}
* [[Collective intelligence]]
* [[Concept map]]
* [[Enterprise 2.0]]
* [[Enterprise bookmarking]]
* [[Explicit knowledge]]
* [[Faceted classification]]
* [[Folksonomy]]
* [[Information ecology]]
* [[Knowledge representation]]
* [[Knowledge transfer]]
* [[Metaknowledge]]
* [[Ontology (information science)]]
* [[Organisational memory]]
* [[Semantic web]]
* [[SciCrunch]]
* [[Tag cloud]]
* [[Web 2.0]]
{{colend}}
'''Others'''
{{colbegin||27em}}
* [[Collective unconscious]]
* [[Human-computer interaction]]
* [[Social network aggregation]]
* [[Enterprise social software]]
* [[Expert system]]
* [[Knowledge]]
* [[Knowledge base]]
* [[Knowledge worker]]
* [[Management information system]]
* [[Microformats]]
* [[Social network]]
* [[Social software]]
* [[Sociology of knowledge]]
* [[Tacit knowledge]]
{{colend}}

==References==
{{reflist|30em}}

'''General'''
{{refbegin}}
*{{Citation
 | surname1=Nonaka | given1=Ikujiro
 | year=1994
 | title= A dynamic theory of organizational knowledge creation
 | journal= Organization Science |volume=5 |issue=1
 | pages=14&#8211;37
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
 | doi=10.1287/orsc.5.1.14
}}
*{{Citation
 | surname1=Wigg | given1=Karl M
 | year=1993
 | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge
 | journal= Arlington: Schema Press
 | pages=153
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
}}
*{{Citation
 | surname1=Alavi | given1=Maryam
 | surname2=Leidner | given2=Dorothy E.
 | year=1999
 | title=Knowledge management systems: issues, challenges, and benefits
 | journal=Communications of the AIS
 | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117
}}
*{{Citation
 | surname1=Kemsley | given1=Sandy
 | year=2009
 | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2&#8217;09
 | journal=BPM, Enterprise 2.0 and technology trends in business
 | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/
}}
{{refend}}

==External links==
* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.
* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.

{{Web syndication}}

{{DEFAULTSORT:Tag (Metadata)}}
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]</text>
      <sha1>r6mrkxdlpdiyp94mjirwr2lw3m1hws0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Concordances (publishing)</title>
    <ns>14</ns>
    <id>43967942</id>
    <revision>
      <id>731145894</id>
      <parentid>724690765</parentid>
      <timestamp>2016-07-23T08:31:48Z</timestamp>
      <contributor>
        <username>Uanfala</username>
        <id>11049176</id>
      </contributor>
      <comment>+[[Category:Corpus linguistics]]; &#177;[[Category:Linguistics]]&#8594;[[Category:Textual criticism]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="260" xml:space="preserve">{{Cat main|Concordance (publishing)}}
[[Category:Biblical studies]]
[[Category:Index (publishing)]]
[[Category:Textual criticism]]
[[Category:Reference works]]
[[Category:Information retrieval techniques]]
[[Category:Hypertext]]
[[Category:Corpus linguistics]]</text>
      <sha1>sja3ebiyoqmljcnfqw3eo8gn7ujov48</sha1>
    </revision>
  </page>
  <page>
    <title>Learning to rank</title>
    <ns>0</ns>
    <id>25050663</id>
    <revision>
      <id>759280081</id>
      <parentid>759127247</parentid>
      <timestamp>2017-01-10T07:00:12Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Approaches */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27393" xml:space="preserve">{{machine learning bar}}
'''Learning to rank'''&lt;ref name="liu"&gt;{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|series=Foundations and Trends in Information Retrieval
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225&#8211;331
|journal=Foundations and Trends in Information Retrieval
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
&lt;/ref&gt; or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.&lt;/ref&gt; [[Training data]] consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], and [[online advertising]].

A possible architecture of a machine-learned search engine is shown in the figure to the right.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
&lt;!-- "assessor" is the more standard term, used e.g. by TREC conference --&gt;
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used &#8212; only the top few documents, retrieved by some existing ranking models are checked. &lt;!--
  TODO: write something about selection bias caused by pooling
--&gt; Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),&lt;ref name="Joachims2002"&gt;{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}&lt;/ref&gt; ''query chains'',&lt;ref&gt;{{citation
 |author1=Joachims T. |author2=Radlinski F. | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
}}&lt;/ref&gt; or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.&lt;ref&gt;{{citation
 |author1=B. Cambazoglu |author2=H. Zaragoza |author3=O. Chapelle |author4=J. Chen |author5=C. Liao |author6=Z. Zheng |author7=J. Degenhardt. | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. 
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}&lt;/ref&gt; First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,&lt;ref&gt;{{citation
 |author1=Broder A. |author2=Carmel D. |author3=Herscovici M. |author4=Soffer A. |author5=Zien J. | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the twelfth international conference on Information and knowledge management
 | year=2003
 | pages=426&#8211;434
 | isbn=1-58113-723-0
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 }}&lt;/ref&gt; and [[Okapi BM25|BM25]]. This phase is called ''top-&lt;math&gt;k&lt;/math&gt; document retrieval'' and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.&lt;ref name="manning-q-eval"&gt;{{citation
 |author1=Manning C. |author2=Raghavan P. |author3=Sch&#252;tze H. | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]&lt;/ref&gt; In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;&lt;ref name="Duh09"&gt;{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}&lt;/ref&gt;
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.&lt;ref name="Duh09" /&gt;
* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.&lt;ref&gt;Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.&lt;/ref&gt;

== Feature vectors ==
For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such an approach is sometimes called ''bag of features'' and is analogous to the [[bag of words]] model and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features &#8212; those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.&lt;ref name="manning-q-eval" /&gt;&lt;ref&gt;
{{cite conference
 | first=M. |last=Richardson |author2=Prakash, A. |author3=Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707&#8211;715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}&lt;/ref&gt;
* ''Query-dependent'' or ''dynamic'' features &#8212; those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:&lt;ref name="letor3"&gt;[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]&lt;/ref&gt;
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
* [[Information retrieval#Mean average precision|Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]]
* [[Spearman's rank correlation coefficient|Spearman's Rho]]

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.&lt;ref&gt;http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt&lt;/ref&gt; Other metrics such as MAP, MRR and precision, are defined only for binary judgements.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* [[Expected reciprocal rank]] (ERR);&lt;ref&gt;{{citation
|author1=Olivier Chapelle |author2=Donald Metzler |author3=Ya Zhang |author4=Pierre Grinspan |title=Expected Reciprocal Rank for Graded Relevance
|url=https://web.archive.org/web/20120224053008/http://research.yahoo.com/files/err.pdf
|journal=CIKM
|year=2009
|pages=
}}&lt;/ref&gt;
* [[Yandex]]'s pfound.&lt;ref&gt;{{citation
|author1=Gulin A. |author2=Karpovich P. |author3=Raskovalov D. |author4=Segalovich I. |title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163&#8211;168
}} (in Russian)&lt;/ref&gt;
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] has analyzed existing algorithms for learning to rank problems in his paper "Learning to Rank for Information Retrieval".&lt;ref name="liu" /&gt; He categorized them into three groups by their input representation and [[loss function]]:

=== Pointwise approach ===
In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem &#8212; given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case learning-to-rank problem is approximated by a classification problem &#8212; learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class="wikitable sortable"
! Year || Name || Type || Notes
|-
| 1989 || OPRF &lt;ref name="Fuhr1989"&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183&#8211;204 
 | year=1989
 | doi=10.1145/65943.65944
}}&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR &lt;ref name="Cooperetal1992"&gt;{{citation
 |author1=Cooper, William S. |author2=Gey, Frederic C. |author3=Dabney, Daniel P. | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval 
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198&#8211;210 
 | year=1992
 | doi=10.1145/133160.133199
}}&lt;/ref&gt;   || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise || Staged logistic regression
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||  A more recent exposition is in,&lt;ref name="Joachims2002" /&gt; which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.20.378 | title = Pranking }}&lt;/ref&gt; || &lt;span style="display:none"&gt;1&lt;/span&gt; pointwise || Ordinal regression.
|-
| 2003 &lt;!-- or 1998? --&gt; || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || pairwise/listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || 
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || &lt;span style="display:none"&gt;1&lt;/span&gt; pointwise ||
|-
| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || RankGP&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.90.220 | title = RankGP }}&lt;/ref&gt; || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
Regularized least-squares based ranking. The work is extended in
&lt;ref name=pahikkala2009efficient&gt;{{Citation|last=Pahikkala|first=Tapio |author2=Tsivtsivadze, Evgeni |author3=Airola, Antti |author4=J&#228;rvinen, Jouni |author5=Boberg, Jorma |title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129&#8211;165|doi=10.1007/s10994-008-5097-z|postscript=.}}&lt;/ref&gt; to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM&lt;sup&gt;map&lt;/sup&gt;] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || pairwise/listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.&lt;ref&gt;C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].&lt;/ref&gt;
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]&lt;ref&gt;Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]&lt;ref&gt;Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.&lt;/ref&gt;  || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]&lt;ref&gt;Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || A method combines Plackett-Luce Model and neural network to minimize the expected Bayes risk, related to NDCG, from the decision-making aspect.
|-
| 2010 || [https://people.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]&lt;ref&gt;Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.&lt;/ref&gt; || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise &amp; listwise || 
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise &amp; pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;&lt;ref name="Fuhr1992"&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243&#8211;255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
}}&lt;/ref&gt; a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.&lt;ref name="Fuhr1989" /&gt; Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 &lt;ref name="Cooperetal1992" /&gt; and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.&lt;ref&gt;{{citation |author1=Manning C. |author2=Raghavan P. |author3=Sch&#252;tze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]&lt;/ref&gt;  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.&lt;ref&gt;Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]&lt;/ref&gt;&lt;ref&gt;{{US Patent|7197497}}&lt;/ref&gt;

[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,&lt;ref&gt;[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]&lt;/ref&gt;{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced&lt;ref name="snezhinsk"&gt;[http://webmaster.ya.ru/replies.xml?item_no=5707&amp;ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)&lt;/ref&gt; that it had significantly increased its [[search quality]] due to deployment of a new proprietary [[MatrixNet]] algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.&lt;ref&gt;The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].&lt;/ref&gt; Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"&lt;ref&gt;[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]&lt;/ref&gt; based on their own search engine's production data. Yahoo has announced a similar competition in 2010.&lt;ref&gt;[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]&lt;/ref&gt;

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.&lt;ref&gt;{{cite web
  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
  | archiveurl = http://www.webcitation.org/5sq8irWNM
  | archivedate = 2010-09-18
  | title = Are Machine-Learned Models Prone to Catastrophic Errors?
  | date = 2008-05-24
  | last = Rajaraman
  | first = Anand
  | authorlink = Anand Rajaraman}}&lt;/ref&gt; [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".&lt;ref&gt;{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = http://www.webcitation.org/5sq7DX3Pj
  | archivedate = 2010-09-15
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}&lt;/ref&gt;

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]
* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]
[[Category:Ranking functions]]</text>
      <sha1>7jrapcs7hclbs67527gdg2vs9bskkuu</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic compression</title>
    <ns>0</ns>
    <id>34087348</id>
    <revision>
      <id>751833052</id>
      <parentid>666858233</parentid>
      <timestamp>2016-11-28T02:33:18Z</timestamp>
      <contributor>
        <username>Anticontradictor</username>
        <id>26909770</id>
      </contributor>
      <minor />
      <comment>A minor edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5233" xml:space="preserve">In [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build 
a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. 
As a result, the same ideas can be represented using a smaller set of words.

Semantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document 
cannot be reconstructed in a reverse process.

==Semantic compression by generalization==
Semantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:
#	determining cumulated term frequencies to identify target lexicon,
#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.&lt;ref&gt;[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010&lt;/ref&gt;

Step 1 requires assembling word frequencies and 
information on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, 
a cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:
&lt;math&gt;cum f(k_{i}) = f(k_{i}) + \sum_{j} cum f(k_{j})&lt;/math&gt; where &lt;math&gt;k_{i}&lt;/math&gt; is a hypernym of &lt;math&gt;k_{j}&lt;/math&gt;.
Then, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.

In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence 
of a less frequent hyponym as its hypernym in output text.

;Example

The below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.

&lt;blockquote&gt;They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' 
in very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects 
'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the 
'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of 
'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.&lt;/blockquote&gt;

The procedure outputs the following text:

&lt;blockquote&gt;They are both '''facility''' building '''insect''', but '''insect'''s and honey '''insects''' '''arrange''' their '''biological groups''' 
in very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects 
'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the 
'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of 
'''organic processes''', and there are '''impinging differences of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.&lt;/blockquote&gt;

==Implicit semantic compression==
A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)
.&lt;ref&gt;[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],
COLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982&lt;/ref&gt;

==Applications and advantages==
In the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less 
[[Computational complexity theory|computational complexity]] and a positive influence on efficiency. 

Semantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).&lt;ref&gt;[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010&lt;/ref&gt; This is due to more precise descriptors (reduced effect of language diversity &#8211; limited language redundancy, a step towards a controlled dictionary).

As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).

==See also==
* [[Text simplification]]
* [[Lexical substitution]]
* [[Information theory]]
* [[Quantities of information]]

==References==
&lt;references/&gt;

==External links==
* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
[[Category:Quantitative linguistics]]
[[Category:Computational linguistics]]</text>
      <sha1>j4iipzm6qxr7dq1zzm2kxmrpmvw6z6s</sha1>
    </revision>
  </page>
  <page>
    <title>Index term</title>
    <ns>0</ns>
    <id>6118940</id>
    <revision>
      <id>740030891</id>
      <parentid>733603028</parentid>
      <timestamp>2016-09-18T17:05:41Z</timestamp>
      <contributor>
        <username>Narky Blert</username>
        <id>22041646</id>
      </contributor>
      <comment>/* In web search engines */ Link to DAB page repaired</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4471" xml:space="preserve">An '''index term''', '''subject term''', '''subject heading''', or '''descriptor''', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.

Keywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it's inefficient. Almost every English-language site on the Internet has the article "''the''", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.

The term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[Thesaurus (information retrieval)|thesaurus]].

The [[Simple Knowledge Organization System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].&lt;ref name="auto"&gt;{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}&lt;/ref&gt;

==In web search engines==
Most [[web search engine]]s are designed to search for words anywhere in a document&#8212;the title, the body, and so on. This being the case, a keyword can be any term that exists within the document. However, priority is given to words that occur in the title, words that recur numerous times, and words that are explicitly assigned as keywords within the coding.&lt;ref&gt;Cutts, Matt. (2010, March 4). ''How search works.'' Retrieved from https://www.youtube.com/watch?v=BNHR6IQJGZs&lt;/ref&gt; Index terms can be further refined using [[Boolean algebra|Boolean operators]] such as "AND, OR, NOT." "AND" is normally unnecessary as most search engines infer it. "OR" will search for results with one search term or another, or both. "NOT" eliminates a word or phrase from the search, getting rid of any results that include it. Multiple words can also be enclosed in quotation marks to turn the individual index terms into a specific index ''phrase''. These modifiers and methods all help to refine search terms, to better maximize the accuracy of search results.&lt;ref&gt;CLIO. ''Keyword search''. Columbia University Libraries. Retrieved from http://www.columbia.edu/cu/lweb/help/clio/keyword.html&lt;/ref&gt;

==Author keywords==
Many journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document ''relative'' to the other documents in the database. Author keywords are an integral part of literature.&lt;ref name="auto"/&gt;

==Examples==
*[[Canadian Subject Headings]] (CSH)
*[[Library of Congress Subject Headings]] (LCSH)
*[[Medical Subject Headings]] (MeSH)
*[[Polythematic Structured Subject Heading System]] (PSH)
*[[Subject Headings Authority File]] (SWD)

==See also==
*[[Dynamic keyword insertion]]
*[[Tag cloud]]
*[[Keyword density]]
*[[Search engine optimization]]
*[[Tag (metadata)]]
*[[Subject (documents)]]

==References==
{{reflist}}

{{Authority control}}
[[Category:Information retrieval techniques]]</text>
      <sha1>2wst6ho99lixeoo7mfmjfl4oeluwr1h</sha1>
    </revision>
  </page>
  <page>
    <title>Negative search</title>
    <ns>0</ns>
    <id>21692300</id>
    <revision>
      <id>666859965</id>
      <parentid>641411527</parentid>
      <timestamp>2015-06-14T05:41:22Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3471" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=March 2009}}
{{orphan|date=February 2009}}
{{confusing|date=March 2009}}
}}

'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.

Negative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.

Negative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.

Negative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.

Examples of Negative Intent are:

- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.

- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.

- An investigator is looking for a car but has no other information on that car on which to base a search.

==Negative Search Classifiers==

If there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.

[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.

==Irrelevancy as a Desirable Construct==

Positive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.

It follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.

==Degrees of Passivity==

Positive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\'s Eve|New Years Eve]]."

Discovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know"

Negative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don't care where as long as its good."

Searchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.

==References==
{{Reflist}}

[[Category:Information retrieval techniques]]</text>
      <sha1>i34d66mir39pqkx3o7zj1fnd84zn80q</sha1>
    </revision>
  </page>
  <page>
    <title>Cosine similarity</title>
    <ns>0</ns>
    <id>8966592</id>
    <revision>
      <id>746257427</id>
      <parentid>745993620</parentid>
      <timestamp>2016-10-26T07:39:27Z</timestamp>
      <contributor>
        <username>Pengo</username>
        <id>35807</id>
      </contributor>
      <comment>/* top */ copyedit so it doesn't sound like two editors fighting with each other</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13288" xml:space="preserve">'''Cosine similarity''' is a measure of similarity between two non zero vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0&#176; is 1, and it is less than 1 for any other angle. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90&#176; have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The name derives from the term "direction cosine": in this case, note that unit vectors are maximally "similar" if they're parallel and maximally "dissimilar" if they're orthogonal (= perpendicular).  It should not escape the alert reader's attention that this is analogous to cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.

Note that these bounds apply for any number of dimensions, and cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[information retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.&lt;ref&gt;Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35&#8211;43.&lt;/ref&gt;

The technique is also used to measure cohesion within clusters in the field of [[data mining]].&lt;ref&gt;P.-N. Tan, M. Steinbach &amp; V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.&lt;/ref&gt;

''Cosine distance'' is a term often used for the complement in positive space, that is: &lt;math&gt;D_C(A,B) = 1 - S_C(A,B)&lt;/math&gt;. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the [[triangle inequality]] property&#8212;or, more formally, the [[Schwarz inequality]]&#8212;and it violates the coincidence axiom; to repair the triangle inequality property while maintaining the same ordering, it is necessary to convert to angular distance (see below.)

One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.

==Definition==

The cosine of two non zero vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:

:&lt;math&gt;\mathbf{a}\cdot\mathbf{b}
=\left\|\mathbf{a}\right\|\left\|\mathbf{b}\right\|\cos\theta&lt;/math&gt;

Given two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(&#952;)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vector space|magnitude]] as

:&lt;math&gt;\text{similarity} = \cos(\theta) = {\mathbf{A} \cdot \mathbf{B} \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i  B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{A_i^2}}  \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} }&lt;/math&gt; , where &lt;math&gt;A_i&lt;/math&gt; and &lt;math&gt;B_i&lt;/math&gt; are [[Euclidean vector#Decomposition|components]] of vector &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; respectively.

The resulting similarity ranges from &amp;minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating orthogonality (decorrelation), and in-between values indicating intermediate similarity or dissimilarity.

For text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.

In the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&amp;nbsp;90&#176;.

If the attribute vectors are normalized by subtracting the vector means (e.g., &lt;math&gt;A - \bar{A}&lt;/math&gt;), the measure is called centered cosine similarity and is equivalent to the [[Pearson product-moment correlation coefficient#For a sample|Pearson Correlation Coefficient]].

=== Angular distance and similarity ===

The term "cosine similarity" is sometimes used to refer to different definition of similarity provided below. However the most common use of "cosine similarity" is as defined above and the similarity and distance metrics defined below are referred to as "angular similarity" and "angular distance" respectively. The normalized angle between the vectors is a formal [[distance metric]] and can be calculated from the similarity score defined above. This angular distance metric can then be used to compute a similarity function bounded between 0 and 1, inclusive.

When the vector elements may be positive or negative:

:&lt;math&gt;\text{distance} = \frac{ \cos^{-1}( \text{similarity} ) }{ \pi } &lt;/math&gt;

:&lt;math&gt;\text{similarity} = 1 - \text{distance} &lt;/math&gt;

Or, if the vector elements are always positive:

:&lt;math&gt;\text{distance} = \frac{ 2 \cdot \cos^{-1}( \text{similarity} ) }{ \pi }&lt;/math&gt;

:&lt;math&gt;\text{similarity} = 1 - \text{distance}&lt;/math&gt;

Although the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However, for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.

=== Confusion with "Tanimoto" coefficient ===

The cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:

:&lt;math&gt;T(A,B) = {A \cdot B \over \|A\|^2 +\|B\|^2 - A \cdot B}&lt;/math&gt;

In fact, this algebraic form [[Jaccard index#Tanimoto Similarity and Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.

=== Ochiai coefficient ===
This coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:&lt;ref&gt;''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. &#8470; 9. P. 526-530.&lt;/ref&gt;&lt;ref&gt;''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. &#8211; Assen. Van Gorcum. 1958. 628 p.&lt;/ref&gt;
:&lt;math&gt;K =\frac{n(A \cap B)}{\sqrt{n(A) \times n(B)}}&lt;/math&gt;
Here, &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are sets, and &lt;math&gt;n(A)&lt;/math&gt; is the number of elements in &lt;math&gt;A&lt;/math&gt;. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.

== Properties ==
Cosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual &lt;math&gt;\|A - B\|&lt;/math&gt;, and observe that

:&lt;math&gt;\|A - B\|^2 = (A - B)^\top (A - B) = \|A\|^2 + \|B\|^2 - 2 A^\top B&lt;/math&gt;

by [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, &lt;math&gt;\|A\|^2 = \|B\|^2 = 1&lt;/math&gt; so the previous is equal to

:&lt;math&gt;2 (1 - \cos(A, B))&lt;/math&gt;

'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of &lt;math&gt;1/n&lt;/math&gt; (where &lt;math&gt;n&lt;/math&gt; is the number of dimensions), and although the distribution is bounded between -1 and +1, as &lt;math&gt;n&lt;/math&gt; grows large the distribution is increasingly well-approximated by the [[normal distribution]].&lt;ref&gt;{{cite journal
 | author = Spruill, Marcus C
 | year = 2007
 | title = Asymptotic distribution of coordinates on high dimensional spheres
 | journal = Electronic communications in probability
 | volume = 12 | pages = 234&#8211;247
 | doi = 10.1214/ECP.v12-1294
}}&lt;/ref&gt;&lt;ref&gt;[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]&lt;/ref&gt;
For other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.&lt;ref&gt;{{cite journal
 | author = Graham L. Giller 
 | year = 2012
 | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity
 | journal = Giller Investments Research Notes
 | number = 20121024/1
 | doi = 10.2139/ssrn.2167044
}}&lt;/ref&gt;

== Soft cosine measure ==
Soft cosine measure
is a measure of &#8220;soft&#8221; similarity between two vectors, i.e., the measure that considers similarity of pairs of features.&lt;ref&gt;{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=G&#243;mez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computaci&#243;n y Sistemas|volume=18|issue=3|pages=491&#8211;504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}&lt;/ref&gt; The traditional cosine similarity considers the [[vector space model]] (VSM) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).

For example, in the field of [[natural language processing]] (NLP) the similarity among features is quite intuitive. Features such as words, n-grams or syntactic n-grams&lt;ref&gt;{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hern&#225;ndez|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1&#8211;11|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}&lt;/ref&gt; can be quite similar, though formally they are considered as different features in the VSM. For example, words &#8220;play&#8221; and &#8220;game&#8221; are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).

For calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.

Given two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:

:&lt;math&gt;\begin{align}
    \operatorname{soft\_cosine}_1(a,b)=
    \frac{\sum\nolimits_{i,j}^N s_{ij}a_ib_j}{\sqrt{\sum\nolimits_{i,j}^N s_{ij}a_ia_j}\sqrt{\sum\nolimits_{i,j}^N s_{ij}b_ib_j}},
\end{align}
&lt;/math&gt;

where {{math|''s&lt;sub&gt;ij&lt;/sub&gt;'' {{=}} similarity(feature&lt;sub&gt;''i''&lt;/sub&gt;, feature&lt;sub&gt;''j''&lt;/sub&gt;)}}.

If there is no similarity between features ({{math|''s&lt;sub&gt;ii&lt;/sub&gt;'' {{=}} 1}}, {{math|''s&lt;sub&gt;ij&lt;/sub&gt;'' {{=}} 0}} for {{math|''i'' &#8800; ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.

The complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be transformed to subquadratic.{{citation needed|date=December 2015}}

== See also ==
* [[S&#248;rensen similarity index|S&#248;rensen's quotient of similarity]]
* [[Hamming distance]]
* [[Correlation]]
* [[Dice's coefficient]]
* [[Jaccard index]]
* [[SimRank]]
* [[Information retrieval]]

==References==
{{reflist}}

== External links ==
* [http://mathforum.org/kb/message.jspa?messageID=5658016&amp;tstart=0 Weighted cosine measure]
* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]
* [http://www.rxnlp.com/api-reference/text-similarity-api-reference/ Web API to Compute Cosine, Jaccard and Dice for Text in Any Language]

{{DEFAULTSORT:Cosine Similarity}}
[[Category:Information retrieval techniques]]</text>
      <sha1>mjcpqykfp5mv0l3sm9lwhcbd12rj6yj</sha1>
    </revision>
  </page>
  <page>
    <title>Policy framework</title>
    <ns>0</ns>
    <id>21828505</id>
    <revision>
      <id>666920801</id>
      <parentid>666918345</parentid>
      <timestamp>2015-06-14T16:18:41Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Globalize}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4351" xml:space="preserve">{{refimprove|date=March 2009}}
{{globalize|date=June 2015}}
A '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.

==Principles==
[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.&lt;ref&gt;http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm&lt;/ref&gt;

===Availability===
Government departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....

===Coverage===
Government departments should make the following information increasingly available on an electronic basis:
* all published material or material already in the public domain
* all policies that could be released publicly
* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)
* all documents that the public may be required to complete
* corporate documentation in which the public would be interested

===Pricing=== 
a) Free dissemination of Government-held information is appropriate where:
* dissemination to a target audience is desirable for a public policy purpose, or
* a charge to recover the cost of dissemination is not feasible or cost-effective

b) Pricing to recover the cost of dissemination is appropriate where:
* there is no particular public policy reason to disseminate the information, and 
* a charge to recover the cost of dissemination is both feasible and cost effective

c) Pricing to recover the cost of transformation is appropriate where:
* pricing to recover the cost of dissemination is appropriate, and
* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination

d) Pricing to recover the full costs of information production and dissemination is appropriate where:
* the information is created for the commercial purpose of sale at a profit, and 
* to do so would not breach the other pricing principles

===Ownership===
Government-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.

===Stewardship===
Government departments are stewards of Government-held information, and it is their responsibility to implement good information management.

===Collection===
Government departments should only collect information for specified public policy, operational business or legislative purposes.

===Copyright===
Information created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.
 
===Preservation===
Government-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.

===Quality===
The key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.

===Integrity===
The integrity of Government-held information will be achieved when:
* all guarantees and conditions surrounding the information are met
* the principles are clear and communicated
* any situation relating to Government-held information is handled openly and consistently
* those affected by changes to Government-held information are consulted on those changes
* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well
* there are minimum exceptions to the principles.

===Privacy===
The principles of the Privacy Act 1993 apply.

==References==
{{reflist}}

{{DEFAULTSORT:Policy Framework}}
[[Category:Information retrieval techniques]]
[[Category:Government of New Zealand]]</text>
      <sha1>0vbv3fowr1ex45pvsfmr4mfvvbtx97p</sha1>
    </revision>
  </page>
  <page>
    <title>Music alignment</title>
    <ns>0</ns>
    <id>49926925</id>
    <revision>
      <id>721229225</id>
      <parentid>715647044</parentid>
      <timestamp>2016-05-20T13:31:04Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes / WPC #106 list using [[Project:AWB|AWB]] (12016)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8943" xml:space="preserve">[[Image:MusicAlignment_BeethovenFifth.png|thumb|300px|right|First theme of Symphony No. 5 by Ludwig van Beethoven in a sheet music, audio,
and piano-roll representation. The red bidirectional arrows indicate the aligned time positions of corresponding note events in the different representations.]]

[[Music]] can be described and represented in many different ways including [[sheet music]], symbolic representations, and audio recordings. For each of these representations, there may exist different versions that correspond to the same musical work. The general goal of '''music alignment''' (sometimes also referred to as '''music synchronization''') is to automatically link the various data streams, thus interrelating the multiple information sets related to a given musical work. More precisely, music alignment is taken to [[mean]] a procedure which, for a given position in one representation of a piece of music, determines the corresponding position within another representation.&lt;ref name=Mueller15_Chapter3FMP_SPRINGER&gt;
{{cite book
| last = M&#252;ller
| first = Meinard
| title = Music Synchronization. In Fundamentals of Music Processing, chapter 3, pages 115-166
| url = http://www.music-processing.de
| publisher = Springer
| year = 2015
| doi = 10.1007/978-3-319-21945-5
| isbn = 978-3-319-21944-8 }}
&lt;/ref&gt; In the figure on the right, such an alignment is visualized by the red bidirectional arrows. Such [[synchronization]] results form the basis for novel interfaces that allow users to access, search, and browse musical content in a convenient way.&lt;ref name=DammFTCKM12_DML_IJDL&gt;
{{cite journal
|last1=Damm
|first1=David 
|last2=Fremerey
|first2=Christian
|last3=Thomas
|first3=Verena
|last4=Clausen
|first4=Michael
|last5=Kurth
|first5=Frank
|last6=M&#252;ller
|first6=Meinard
|title=A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction
|url = http://link.springer.com/article/10.1007%2Fs00799-012-0087-y 
|journal=International Journal on Digital Libraries: Special Issue on Music Digital Libraries
|volume=12
|issue=2-3
|year=2012
|pages=53&#8211;71
|doi=10.1007/s00799-012-0087-y}}
&lt;/ref&gt;&lt;ref name=MuellerCKEF10_Sync_ISR&gt;
{{cite journal
|last1=M&#252;ller
|first1=Meinard 
|last2=Clausen
|first2=Michael
|last3=Konz
|first3=Verena
|last4=Ewert
|first4=Sebastian
|last5=Fremerey
|first5=Christian 
|title=A Multimodal Way of Experiencing and Exploring Music
|url = https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/03-publications/2010_MuellerClausenKonzEwertFremerey_MusicSynchronization_ISR.pdf 
|journal=Interdisciplinary Science Reviews (ISR)
|volume=35
|issue=2
|year=2010
|pages=138&#8211;153
|doi=10.1179/030801810X12723585301110}}&lt;/ref&gt;

==Basic procedure==
[[File:MusicAlignment Procedure.png|thumb|300px|right|Overview of the processing pipeline of a typical music alignment procedure.]]

Given two different music representations, typical music alignment approaches proceed in two steps.&lt;ref name=Mueller15_Chapter3FMP_SPRINGER/&gt; In the first step, the two representations are transformed into sequences of suitable features. In general, such feature representations need to find a compromise between two conflicting goals. On the one hand, features should show a large degree of [[robustness]] to variations that are to be left unconsidered for the task at hand. On the other hand, features should capture enough characteristic information to accomplish the given task. For music alignment, one often uses '''[[chroma feature|chroma-based features]]''' (also called [[chromagram]]s or [[harmonic pitch class profiles|pitch class profiles]]), which capture harmonic and melodic characteristics of music, while being robust to changes in timbre and instrumentation, are being used.

In the second step, the derived feature sequences have to be brought into (temporal) correspondence. To this end, techniques related to '''[[dynamic time warping|dynamic time warping (DTW)]]''' or '''[[hidden Markov model|hidden Markov models (HMMs)]]''' are used to compute an optimal alignment between two given feature sequences.

==Related tasks==
Music alignment and related synchronization tasks have been studied extensively within the field of [[music information retrieval]]. In the following, we give some pointers to related tasks. Depending upon the respective types of music representations, one can distinguish between various synchronization scenarios. For example, audio alignment refers to the task of temporally aligning two different audio recordings of a piece of music. Similarly, the goal of score&#8211;audio alignment is to coordinate note events given in the score representation with audio data. In the  [[offline]] scenario, the two data streams to be aligned are known prior to the actual alignment. In this case, one can use global optimization procedures such as [[dynamic time warping|dynamic time warping (DTW)]] to find an optimal alignment. In general, it is harder to deal with scenarios where the data streams are to be processed online. One prominent online scenario is known as '''[[score following]]''', where a musician is performing a piece according to a given musical score. The goal is then to identify the currently played musical events depicted in the score with high accuracy and low latency.&lt;ref&gt;
{{cite journal
|last1=Cont
|first1=Arshia
|title=A Coupled Duration-Focused Architecture for Real-Time Music-to-Score Alignment
|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence
|volume=32
|issue=6
|year=2010
|pages=974&#8211;987
|issn=0162-8828
|doi=10.1109/TPAMI.2009.106}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
|last1=Orio
|first1=Nicola
|last2=Lemouton
|first2=Serge
|last3=Schwarz
|first3=Diemo
|title=Score following: State of the art and new developments
|url = http://recherche.ircam.fr/equipes/temps-reel/suivi/resources/orio.2002.nime.pdf
|journal=Proceedings of the International Conference on New Interfaces for Musical Expression (NIME)
|date=2003
|pages=36&#8211;41}}&lt;/ref&gt; In this scenario, the score is known as a whole in advance, but the performance is known only up to the current point in time. In this context, alignment techniques such as hidden Markov models or particle filters have been employed, where the current score position and tempo are modeled in a statistical sense.&lt;ref&gt;
{{cite journal
|last1=Duan
|first1=Zhiyao
|last2=Pardo
|first2=Bryan
|journal = Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
|title=A state space model for online polyphonic audio-score alignment
|url = http://www.ece.rochester.edu/~zduan/resource/DuanPardo_ScoreFollowing_ICASSP11.pdf
|year=2011
|pages=197&#8211;200
|doi=10.1109/ICASSP.2011.5946374}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
|last1=Montecchio
|first1=Nicola
|last2=Cont
|first2=Arshia
|title=A unified approach to real time audio-to-score and audio-to-audio alignment using sequential Montecarlo inference techniques
|url = http://articles.ircam.fr/textes/Montecchio11a/index.pdf
|year=2011
|pages=193&#8211;196
|doi=10.1109/ICASSP.2011.5946373}}&lt;/ref&gt; As opposed to classical DTW, such an online synchronization procedure inherently has a running time that is linear in the duration of the performed version. However, as a main disadvantage, an online strategy is very sensitive to local tempo variations and deviations from the score - once the procedure is out of sync, it is very hard to recover and return to the right track. A further online synchronization problem is known as '''[[Pop music automation#Automatic accompaniment|automatic accompaniment]]'''. Having a solo part played by a musician, the task of the computer is to accompany the musician according to a given score by adjusting the tempo and other parameters in real time. Such systems were already proposed some decades ago.&lt;ref&gt;{{cite journal
|last1=Dannenberg
|first1=Roger B.
|title=An on-line algorithm for real-time accompaniment
|journal=Proceedings of the International Computer Music Conference (ICMC)
|url = http://www.cs.cmu.edu/~rbd/papers/icmc84accomp.pdf
|date=1984
|pages=193&#8211;198}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|last1=Raphael
|first1=Christopher
|title=A probabilistic expert system for automatic musical accompaniment
|journal = Journal of Computational and Graphical Statistics
|url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6559&amp;rep=rep1&amp;type=pdf
|year=2001
|pages=487&#8211;512
}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
|last2=Raphael
|first2=Christopher
|year=2006
|title=Music score alignment and computer accompaniment
|url=http://www.cs.cmu.edu/~rbd/papers/accompaniment-cacm-06.pdf
|journal=Communications of the ACM
|volume=49
|issue=8
|pages=38&#8211;43
|doi=10.1145/1145287.1145311
|issn=0001-0782
|last1=Dannenberg
|first1=Roger B.}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Music information retrieval]]
[[Category:Music technology]]
[[Category:Musicology]]
[[Category:Information retrieval techniques]]</text>
      <sha1>j7cc05i1ljp5wzh9gv46iwbi1pmomvh</sha1>
    </revision>
  </page>
  <page>
    <title>Temporal information retrieval</title>
    <ns>0</ns>
    <id>35804330</id>
    <revision>
      <id>747933069</id>
      <parentid>666716437</parentid>
      <timestamp>2016-11-05T08:00:13Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Temporal visualization (T-interfaces) */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="81943" xml:space="preserve">'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.

According to [[information theory]] science (Metzger, 2007),&lt;ref name="Metzger2007"&gt;{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078&#8211;2091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}&lt;/ref&gt; timeliness or currency is one of the key five aspects that determine a document&#8217;s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company&#8217;s earnings or information on already-happened or invalid predictions.

T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.

This page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.

== Temporal dynamics (T-dynamics) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar &amp; A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&amp;nbsp;117 &#8211; 130). Lisbon, Portugal. September 11&#8211;13: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||
|-
|'''Cho, J., &amp; Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||
|-
| '''Fetterly, D., Manasse, M., Najork, M., &amp; Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&amp;nbsp;669 &#8211; 678). Budapest, Hungary. May 20&#8211;24: ACM Press. || 2003 || WWW || T-Dynamics ||
|-
| '''Ntoulas, A., Cho, J., &amp; Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&amp;nbsp;1 &#8211; 12). New York, NY, United States. May 17&#8211;22: ACM Press. || 2004 || WWW || T-Dynamics ||
|-
| '''Vlachos, M., Meek, C., Vagena, Z., &amp; Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;131 &#8211; 142). Paris, France. June 13&#8211;18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., &amp; Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Bordino, I., Boldi, P., Donato, D., Santini, M., &amp; Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4734022&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&amp;nbsp;909 &#8211; 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||
|-
| '''Adar, E., Teevan, J., Dumais, S. T., &amp; Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;282 &#8211; 291). Barcelona, Spain. February 9&#8211;12: ACM Press. || 2009 || WSDM || T-Dynamics ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 &#8211; 701). Boston, MA, United States. July 19&#8211;23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Elsas, J. L., &amp; Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;1 &#8211; 10). New York, United States. February 3&#8211;06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;1123 &#8211; 1124). Raleigh, United States. April 26&#8211;30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, &amp; Prediction'' (pp.&amp;nbsp;273 &#8211; 281). Washington DC, United States. March 30&#8211;31: Springer-Verlag. || 2010 || SBP || T-Dynamics ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., &amp; Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;167 &#8211; 176). Hong Kong, China. February 9&#8211;12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||
|-
| '''Campos, R., Jorge, A., &amp; Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;13 &#8211; 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&amp;nbsp;1171 &#8211; 1172). Beijing, China. July 24&#8211;28: ACM Press. || 2011 || SIGIR || T-Dynamics ||
|-
| '''Dias, G., Campos, R., &amp; Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, &amp; H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&amp;nbsp;581 &#8211; 596). Lisboa, Portugal. October 10&#8211;13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 &#8211; 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Yeung, C.-m. A., &amp; Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1231 &#8211; 1240). Glasgow, Scotland, UK. October 24&#8211;28: ACM Press. || 2011 || CIKM || C-Memory ||
|-
| '''Costa, M., &amp; Silva, M. J., &amp; Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&amp;nbsp;757--766). Gold Coast, Australia. July 6&#8211;11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal markup languages (T-MLanguages) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Setzer, A., &amp; Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||
|-
| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||
|-
| '''Ferro, L., Mani, I., Sundheim, B., &amp; Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||
|-
| '''Pustejovsky, J., Casta&#241;o, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&amp;nbsp;28 &#8211; 34). Tilburg, Netherlands. January 15&#8211;17. || 2003 || IWCS || T-MLanguages ||
|-
| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., &amp; Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||
|}

== Temporal taggers (T-taggers) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., &amp; Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&amp;nbsp;69 &#8211; 76). Hong Kong, China. October 1&#8211;8: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||
|-
| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., &amp; Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&amp;nbsp;168 &#8211; 175). Philadelphia, PA, United States. July 6&#8211;12: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||
|-
| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Str&#246;tgen, J., &amp; Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&amp;acc=OPEN&amp;CFID=82473711&amp;CFTOKEN=13661527&amp;__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&amp;nbsp;321 &#8211; 324). Uppsala, Sweden. July 11&#8211;16.|| 2010 || ACL - SemEval || T-Taggers ||
|-
| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. &amp; Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| '''Chang, A., &amp; Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Str&#246;tgen, J., &amp; Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||
|-
| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. &amp; Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]
|}

== Temporal indexing (T-indexing) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 &#8211; 598). Seattle, Washington, United States. August 6&#8211;11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Berberich, K., Bedathur, S., Neumann, T., &amp; Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;519 &#8211; 526). Amsterdam, Netherlands. July 23&#8211;27: ACM Press. || 2007 || SIGIR || W-Archives ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 &#8211; 224). Shanghai, China. December 21&#8211;22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Song, S., &amp; JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 &#8211; 1121). Fortaleza, Ceara, Brazil. March 16&#8211;20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2&#8211;6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Arikan, I., Bedathur, S., &amp; Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9&#8211;12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 &#8211; 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&amp;nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;235 &#8211; 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||
|}

== Temporal query understanding (TQ-understanding) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Vlachos, M., Meek, C., Vagena, Z., &amp; Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;131 &#8211; 142). Paris, France. June 13&#8211;18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., &amp; Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Dakka, W., Gravano, L., &amp; Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1437 &#8211; 1438). Napa Valley, California, United States. October 26&#8211;30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;182 &#8211; 191). Barcelona, Spain. February 9&#8211;12: ACM Press. || 2009 || WSDM || TQ-Understanding ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 &#8211; 701). Boston, MA, United States. July 19&#8211;23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''K&#246;nig, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;347 &#8211; 354). Boston, MA, United States. July 19&#8211;23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 &#8211; 175). Suwon, Republic of Korea. January 14&#8211;15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;11 &#8211; 20). New York, United States. February 3&#8211;6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Kanhabua, N., &amp; N&#248;rv&#229;g, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6&#8211;10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., &amp; Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&amp;nbsp;1129 &#8211; 1139). Massachusetts, United States. October 9&#8211;11: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., &amp; Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;167 &#8211; 176). Hong Kong, China. February 9&#8211;12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&amp;nbsp;1325). Beijing, China. July 24&#8211;28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., &amp; Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;13 &#8211; 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;1171 &#8211; 1172). Beijing, China. July 24&#8211;28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&amp;CFID=102654836&amp;CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&amp;nbsp;41 &#8211; 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||
|-
| '''Shokouhi, M., &amp; Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;601 &#8211; 610). Portland, United States. August 12&#8211;16.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&amp;dl=ACM&amp;coll=DL&amp;CFID=204979644&amp;CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;2035 &#8211; 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., Dias, G., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&amp;nbsp;1 &#8211; 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Time-aware retrieval/ranking models (T-RModels) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Li, X., &amp; Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;469 &#8211; 475). New Orleans, Louisiana, United States. November 2&#8211;8: ACM Press. || 2003 || CIKM || T-RModels ||
|-
| '''Sato, N., Uehara, M., &amp; Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=1232026&amp;contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&amp;nbsp;215 &#8211; 220). Prague, Czech Republic. September 1&#8211;5: IEEE. || 2003 || DEXA || T-RModels ||
|-
| '''Berberich, K., Vazirgiannis, M., &amp; Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.im/1150474885&amp;page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&amp;linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||
|-
| '''Cho, J., Roy, S., &amp; Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;551 &#8211; 562). Baltimore, United States. June 13&#8211;16: ACM Press. || 2005 || SIGMOD || T-RModels ||
|-
| '''Perki&#246;, J., Buntine, W., &amp; Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;647 &#8211; 648). Salvador, Brazil. August 15&#8211;16: ACM Press. || 2005 || SIGIR || T-RModels ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 &#8211; 1121). Fortaleza, Ceara, Brazil. March 16&#8211;20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Dakka, W., Gravano, L., &amp; Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1437 &#8211; 1438). Napa Valley, California, United States. October 26&#8211;30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 &#8211; 224). Shanghai, China. December 21&#8211;22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Arikan, I., Bedathur, S., &amp; Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9&#8211;12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., &amp; Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&amp;nbsp;165 &#8211; 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 &#8211; 701). Boston, MA, United States. July 19&#8211;23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2&#8211;6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 &#8211; 175). Suwon, Republic of Korea. January 14&#8211;15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Elsas, J. L., &amp; Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;1 &#8211; 10). New York, United States. February 3&#8211;06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;629 &#8211; 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;11 &#8211; 20). New York, United States. February 3&#8211;6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Berberich, K., Bedathur, S., Alonso, O., &amp; Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&amp;nbsp;13 &#8211; 25). Milton Keynes, UK. March 28&#8211;31: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||
|-
| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., &amp; Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&amp;dl=ACM&amp;coll=DL&amp;CFID=204979644&amp;CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;331 &#8211; 340). Raleigh, United States. April 26&#8211;30: ACM Press. || 2010 || WWW || T-RModels ||
|-
| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., &amp; Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&amp;nbsp;331 &#8211; 340). Atlanta, United States. June 11&#8211;15: AAAI Press. || 2010 || AAAI || T-RModels ||
|-
| '''Dai, N., &amp; Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;114 &#8211; 121). Geneve, Switzerland. July 19&#8211;23: ACM Press. || 2010 || SIGIR || T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 &#8211; 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Kanhabua, N., &amp; N&#248;rv&#229;g, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6&#8211;10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Efron, M., &amp; Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;495 &#8211; 504). Beijing, China. July 24&#8211;28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Dai, N., Shokouhi, M., &amp; Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;95 &#8211; 104). Beijing, China. July 24&#8211;28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Kanhabua, N., Blanco, R., &amp; Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&amp;dl=ACM&amp;coll=DL&amp;CFID=102654836&amp;CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;755 &#8211; 764). Beijing, China. July 24&#8211;28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., &amp; Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;1101 &#8211; 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||
|-
| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||
|-
| '''Kanhabua, N., &amp; N&#248;rv&#229;g, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;2463 &#8211; 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||
|-
| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;163 &#8211; 172). Rome, Italy. February 4&#8211;8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Costa, M., &amp; Silva, M. J., &amp; Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&amp;nbsp;757--766). Gold Coast, Australia. July 6&#8211;11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal clustering (T-clustering) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., &amp; Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&amp;nbsp;165 &#8211; 174). Houston, United States. November 27&#8211;30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 &#8211; 598). Seattle, Washington, United States. August 6&#8211;11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 &#8211; 342). Hong Kong, China. December 18&#8211;22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&amp;nbsp;115 &#8211; 124). Austin, United States. June 15&#8211;19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&amp;nbsp;292 &#8211; 296). Funchal - Madeira, Portugal. October 6&#8211;8. || 2009 || KDIR || T-Clustering ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2&#8211;6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 &#8211; 175). Suwon, Republic of Korea. January 14&#8211;15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 &#8211; 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Campos, R., Jorge, A., Dias, G., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&amp;nbsp;1 &#8211; 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Temporal text classification (T-classification) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Jong, F., Rode, H., &amp; Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&amp;nbsp;161 &#8211; 168). Amsterdam, Netherlands. September 14&#8211;17 || 2005 || AHC || T-Classification ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&amp;nbsp;233 &#8211; 241). Edinburgh, Scotland. May 23&#8211;26: ACM Press. || 2006 || WWW || T-Classification ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&amp;nbsp;129 &#8211; 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Jatowt, A., Kawai, Y., &amp; Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&amp;nbsp;137 &#8211; 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Kanhabua, N., &amp; N&#248;rv&#229;g, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&amp;nbsp;358 &#8211; 370). Aarhus, Denmark. September 14&#8211;19: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 &#8211; 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Str&#246;tgen, J., Alonso, O., &amp; Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&amp;nbsp;33 &#8211; 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||
|-
| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7&#8211;13. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]
|}

== Temporal visualization (T-interfaces) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Swan, R., &amp; Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;49 &#8211; 56). Athens, Greece. July 24&#8211;28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Swan, R., &amp; Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, &amp; N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&amp;nbsp;73 &#8211; 80). Boston, Massachusetts, United States. August 20&#8211;23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| [https://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||
|-
| '''Cousins, S., &amp; Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||
|-
| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&amp;nbsp;125 &#8211; 137). Seattle, Washington, United States. August 17&#8211;19: ACM Press. || 1994 || ISSTA || T-Interfaces ||
|-
| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., &amp; Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&amp;nbsp;221 &#8211; 227). Vancouver, British Columbia, Canada. April 13&#8211;18: ACM Press. || 1996 || CHI || T-Interfaces ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;151 &#8211; 160). Salzburg, Austria. September 6&#8211;9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., &amp; Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, &amp; ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&amp;nbsp;119 &#8211; 120). Palo Alto, California, United States. July 11&#8211;14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Catizone, R., Dalli, A., &amp; Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 24&#8211;26: ELDA. || 2006 || LREC || T-Interfaces ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 &#8211; 342). Hong Kong, China. December 18&#8211;22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, Y., &amp; Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&amp;nbsp;1221 &#8211; 1222). Beijing, China. April 21&#8211;25: ACM Press. || 2008 || WWW || W-Archives ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 8&#8211;10: ACM Press. || 2008 || WikiSym || T-Interfaces ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, &amp; L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&amp;nbsp;601 &#8211; 604). Aveiro, Portugal. October 12&#8211;15. || 2009 || EPIA || T-Interfaces ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 &#8211; 175). Suwon, Republic of Korea. January 14&#8211;15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 &#8211; 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., &amp; Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&amp;nbsp;549 &#8211; 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||
|}

== Temporal search engines (T-SEngine) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 &#8211; 598). Seattle, Washington, United States. August 6&#8211;11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 &#8211; 224). Shanghai, China. December 21&#8211;22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2&#8211;6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 &#8211; 175). Suwon, Republic of Korea. January 14&#8211;15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 &#8211; 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|}

== Temporal question answering (T-QAnswering) ==
{| class="wikitable sortable"
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 &#8211; 1121). Fortaleza, Ceara, Brazil. March 16&#8211;20: ACM Press. || 2008 || SAC || T-QAnswering ||
|}

== Temporal snippets (T-snippets) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 20&#8211;24: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, &amp; F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&amp;nbsp;26 &#8211; 31). Pisa, Italy. October 17&#8211;21.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||
|-
| '''Svore, K. M., Teevan, J., Dumais, S. T., &amp; Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;1045 &#8211; 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||
|}

== Future information retrieval (F-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, &amp; J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 15&#8211;19: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&amp;nbsp;115 &#8211; 124). Austin, United States. June 15&#8211;19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 &#8211; 175). Suwon, Republic of Korea. January 14&#8211;15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;1123 &#8211; 1124). Raleigh, United States. April 26&#8211;30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 &#8211; 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Dias, G., Campos, R., &amp; Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Kanhabua, N., Blanco, R., &amp; Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&amp;dl=ACM&amp;coll=DL&amp;CFID=82290723&amp;CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;755 &#8211; 764). Beijing, China. July 24&#8211;28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Kanazawa, K., Jatowt, A., &amp; Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;278 &#8211; 283). Lyon, France. August 22&#8211;27: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, &amp; H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&amp;nbsp;581 &#8211; 596). Lisboa, Portugal. October 10&#8211;13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 &#8211; 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Weerkamp, W., &amp; Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||
|-
| '''Radinski, K., &amp; Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;255 &#8211; 264). Rome, Italy. February 4&#8211;8: ACM Press. || 2013 || WSDM || F-IRetrieval ||
|}

== Temporal image retrieval (T-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Dias, G., Moreno, J. G., Jatowt, A., &amp; Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calder&#243;n-Benavides, L., Gonz&#225;lez-Caro, C., Ch&#225;vez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&amp;nbsp;199 &#8211; 204). Cartagena de Indias, Colombia. October 21&#8211;25: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||
|-
| '''Palermo, F., Hays, J., &amp; Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&amp;nbsp;499 &#8211; 512). Firenze, Italy. October 07&#8211;13: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||
|-
| '''Kim, G., &amp; Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;163 &#8211; 172). Rome, Italy. February 4&#8211;8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Martin, P., Doucet, A., &amp; Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||
|}

== Collective memory (C-memory) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||
|-
| '''Hall, D., Jurafsky, D., &amp; Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&amp;nbsp;363 &#8211; 371). Waikiki, Honolulu, Hawaii. October 25&#8211;27: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||
|-
| '''Shahaf, D., &amp; Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&amp;nbsp;623 &#8211; 632). Washington, United States. July 25&#8211;28: ACM Press. || 2010 || KDD || C-Memory ||
|-
| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., &amp; Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;83 &#8211; 92). Eindhoven, Netherlands. June 6&#8211;9: ACM Press. || 2011 || HT || C-Memory ||
|-
| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||
|-
| '''Yeung, C.-m. A., &amp; Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1231 &#8211; 1240). Glasgow, Scotland, UK. October 24&#8211;28: ACM Press. || 2011 || CIKM || C-Memory ||
|}

== Web archives (W-archives) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||
|-
| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&amp;ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&amp;ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&amp;nbsp;72 &#8211; 73. || 1997 || SAM || W-Archives ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;151 &#8211; 160). Salzburg, Austria. September 6&#8211;9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., &amp; Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, &amp; ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&amp;nbsp;119 &#8211; 120). Palo Alto, California, United States. July 11&#8211;14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., &amp; Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;135 &#8211; 144). Odense, Denmark. August 22&#8211;25: ACM Press. || 2006 || HT || W-Archives ||
|-
| '''Adar, E., Dontcheva, M., Fogarty, J., &amp; Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, &amp; M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&amp;nbsp;239 &#8211; 248). Monterey, CA, United States. October 19&#8211;22: ACM Press. || 2008 || UIST || W-Archives ||
|-
| '''Song, S., &amp; JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Gomes, D., Miranda, J., &amp; Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&amp;nbsp;408 &#8211; 420). Berlin, Germany. September 25&#8211;29: Springer-Verlag || 2011 || TPDL || W-Archives ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;235 &#8211; 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||
||
|-
| '''Costa, M., &amp; Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&amp;nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||
|}

== Topic detection and tracking (TDT) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Allan, J., Carbonell, J., Doddington, G., &amp; Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&amp;nbsp;194 &#8211; 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||
|-
| '''Swan, R., &amp; Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;38 &#8211; 45). Kansas City, Missouri, United States. November 2&#8211;6: ACM Press. || 1999 || CIKM || TDT ||
|-
| '''Swan, R., &amp; Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, &amp; N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&amp;nbsp;73 &#8211; 80). Boston, Massachusetts, United States. August 20&#8211;23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| '''Swan, R., &amp; Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;49 &#8211; 56). Athens, Greece. July 24&#8211;28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Makkonen, J., &amp; Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, &amp; I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&amp;nbsp;393 &#8211; 404). Trondheim, Norway. August 17&#8211;22: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., &amp; Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&amp;nbsp;165 &#8211; 174). Houston, United States. November 27&#8211;30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 &#8211; 342). Hong Kong, China. December 18&#8211;22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&amp;nbsp;227 &#8211; 242). New York, United States. || 2004 || TALIP || TDT ||
|}

==References==
{{reflist}}

[[Category:Information retrieval genres]]</text>
      <sha1>gtls71q6am18ikvkrmm46fyvai4ot5p</sha1>
    </revision>
  </page>
  <page>
    <title>Cross-language information retrieval</title>
    <ns>0</ns>
    <id>296950</id>
    <revision>
      <id>735708554</id>
      <parentid>707776937</parentid>
      <timestamp>2016-08-22T15:50:42Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Filled in 1 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2389" xml:space="preserve">{{refimprove|date=September 2014}}

'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.&lt;ref&gt;"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011.&lt;/ref&gt;  CLIR techniques can be classified into different categories based on different translation resources: 
* Dictionary-based CLIR techniques
* Parallel corpora based CLIR techniques
* Comparable corpora based CLIR techniques
* Machine translator based CLIR techniques

The first workshop on CLIR was held in Z&#252;rich during the SIGIR-96 conference.&lt;ref&gt;The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.&lt;/ref&gt; Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).

The term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.

[[Google Search]] had a cross-language search feature that was removed in 2013.&lt;ref&gt;{{cite web|url=http://searchengineland.com/google-drops-translated-foreign-pages-search-option-due-to-lack-of-use-160157|title=Google Drops "Translated Foreign Pages" Search Option Due To Lack Of Use|date=20 May 2013|publisher=}}&lt;/ref&gt;

==See also==
*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)

==References==
&lt;references /&gt;

==External links==
*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]
*[http://www.2lingual.com/ A search engine for CLIR]
{{DEFAULTSORT:Cross-Language Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Natural language processing]]


{{linguistics-stub}}</text>
      <sha1>cn90554bq43so8c0q7jjbisqc708hcv</sha1>
    </revision>
  </page>
  <page>
    <title>Question answering</title>
    <ns>0</ns>
    <id>360030</id>
    <revision>
      <id>762383617</id>
      <parentid>761838673</parentid>
      <timestamp>2017-01-28T13:26:41Z</timestamp>
      <contributor>
        <ip>118.102.129.98</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26428" xml:space="preserve">{{other uses|question|answer}}
{{multiple issues|
{{cleanup|date=January 2012|reason='''appearance of plagiarised text (now in extensive footnote), use of draft rather than published sources, extensive appearance of text violating [[WP:VERIFY]] and/or [[WP:OR]], use of jargon to define jargon, etc.'''}}
{{cleanup rewrite|date=January 2012}}
{{more footnotes|date=February 2014}}
{{citation style|date=January 2016}}
}}

'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].

A QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents.

Some examples of natural language document collections used for QA systems include:
* a local collection of reference texts
* internal organization documents and web pages
* compiled [[newswire]] reports
* a set of [[Wikipedia]] pages
* a subset of [[World Wide Web]] pages

 with a wide range of question types including: fact, list, [[definition]], ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.

* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease &lt;ref&gt;Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer's Disease. CLEF 2012 Evaluation Labs and Workshop. September 17, 2012&lt;/ref&gt;
* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.

==History==
{{unreferenced section|date=January 2016}}
Two early QA systems were BASEBALL&lt;ref&gt;{{cite journal|last1=GREEN JR|first1=Bert F|title=Baseball: an automatic question-answerer.|journal=western joint IRE-AIEE-ACM computer conference|date=1961|pages=219&#8211;224|display-authors=etal}}&lt;/ref&gt; and LUNAR.&lt;ref&gt;{{cite journal|last1=Woods|first1=William A|last2=Kaplan|first2=R.|title=Lunar rocks in natural English: Explorations in natural language question answering|journal=Linguistic structures processing 5|date=1977|volume=5|pages=521&#8211;569}}&lt;/ref&gt; BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.

[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.

In the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with treport[[expert system|&lt;nowiki/&gt;]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.

The 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.

Recently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.

==Architecture==
{{refimprove section|date=January 2016}}
Most modern QA systems use [[natural language]] text documents as their underlying knowledge source.{{citation needed|date=January 2016}}  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted.{{citation needed|date=January 2016}} An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge; however, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates, etc.) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.{{citation needed|date=January 2016}}

In an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s.{{citation needed|date=January 2016}} It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system.{{citation needed|date=January 2016}} Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors.{{citation needed|date=January 2016}} An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.{{citation needed|date=January 2016}}

As of 2001, QA systems typically included a ''question classifier'' module that determines the type of question and the type of answer.&lt;ref&gt;Hirschman, L. &amp; Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.&lt;/ref&gt; After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text; thus, a ''document retrieval module'' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer, and a ''filter'' preselects small text fragments that contain strings of the same type as the expected answer.{{citation needed|date=January 2016}} For example, if the question is "Who invented
penicillin?", the filter returns text that contain names of people. Finally, an ''answer extraction'' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.{{citation needed|date=January 2016}}

A ''multiagent'' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta&#8211;agent controls the cooperation between question answering agents and chooses the most relevant answer(s).&lt;ref&gt;{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124 |doi=10.5210/fm.v10i1.1204
}}&lt;/ref&gt;

==Question answering methods==
QA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,&lt;ref&gt;Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).&lt;/ref&gt; leading to two benefits:
# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.
# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.

Question answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],&lt;ref&gt;{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=https://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}&lt;/ref&gt; a [[logic programming]] language associated with [[artificial intelligence]].

===Open domain question answering===
{{unreferenced section|date=January 2016}}
In [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.

The system takes a [[natural language]] question as an input rather than a set of keywords, for example, "When is the national day of China?" The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.

Keyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. "Who", "Where" or "How many", these words tell the system that the answers should be of type "Person", "Location", "Number" respectively. In the example above, the word "When" indicates that the answer should be of type "Date". POS (Part of Speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is "Chinese National Day", the predicate is "is" and the adverbial modifier is "when", therefore the answer type is "Date". Unfortunately, some interrogative words like "Which", "What" or "How" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.

Once the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as "Who" or "Where", a Named Entity Recogniser is used to find relevant "Person" and "Location" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.

A [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is "1st Oct."

==Issues==
In 2002, a group of researchers presented an unpublished and largely unsourced report as a funding support document, in which they describe a 5-year roadmap of research current to the state of the question answering filed at that time.&lt;ref&gt;Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R., date unknown, "Tasks and Program Structures to Roadmap Research in Question Answering (QA)," at [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues] [DRAFT DOCUMENT], accessed 1 January 2016.&lt;/ref&gt;&lt;ref&gt;Here is some content taken verbatim from that roadmap (see preceding citation): "[1] Question classes: Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}} [2] Question processing: The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification. [3] Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.) [4] Data sources for QA: Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained. [4] Answer extraction: Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}} [5] Answer formulation: The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents. [6] Real time question answering: There is need for developing Q&amp;A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question. [7] Multilingual (or cross-lingual) question answering: The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].) [8] Interactive QA: It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions. (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.) [9] Advanced reasoning for QA: More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system. [10] Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process. [11] User profiling for QA: The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}} [12] Deep Question Answering: Deep QA complement traditional Question Answering by adding some machine learning capabilities within a standard factoid question answering pipeline. The idea is to leverage curated data repositories or knowledge bases, which can be general ones such as Wikipedia, or domain-specific (e.g. molecular biology) in order to provide more accurate answers to the end-users.&lt;/ref&gt;
&lt;ref&gt;On the subject of interactive QA, see also Perera, R. and Nand, P. (2014). "Interaction History Based Answer Formulation for Question Answering," at [http://rivinduperera.com/publications/kesw2014.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}&lt;/ref&gt;{{full citation needed|date=January 2016}}
&lt;ref&gt;On the subject of information clustering for QA, see also Perera, R. (2012). "IPedagogy: Question Answering System Based on Web Information Clustering," at [http://rivinduperera.com/publications/t4e2012.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}&lt;/ref&gt;{{full citation needed|date=January 2016}}
&lt;ref&gt;On the subject of deep question answering, see the following citation.&lt;/ref&gt;&lt;ref&gt;{{cite journal | pmc = 4572360 | pmid=26384372 | doi=10.1093/database/bav081 | volume=2015 | title=Deep Question Answering for protein annotation | year=2015 | journal=Database (Oxford) |vauthors=Gobeill J, Gaudinat A, Pasche E, Vishnyakova D, Gaudet P, Bairoch A, Ruch P }}&lt;/ref&gt;
&lt;!-- 
Because much of the text in this section was copied and pasted from the "roadmap" document, which itself is a draft and unpublished document, the text was moved into a footnote. 
--&gt;

==Progress==
QA systems have been extended in recent years to encompass additional domains of knowledge&lt;ref&gt;Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.&lt;/ref&gt;  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:

* interactivity&#8212;clarification of questions or answers
* answer reuse or caching
* knowledge representation and reasoning
* social media analysis with QA systems
* [[sentiment analysis]]&lt;ref&gt;{{webarchive |url=https://web.archive.org/web/20121027153311/http://totalgood.com/bitcrawl/ |date=October 27, 2012 |title=BitCrawl by Hobson Lane }}&lt;/ref&gt;
* utilization of thematic roles&lt;ref&gt;Perera, R. and Perera, U. 2012. [http://rivinduperera.com/publications/qacd_coling2012.html Towards a thematic role based target identification model for question answering.]&lt;/ref&gt;
* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts&lt;ref&gt;{{cite conference |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430&#8211;437 | publisher=Springer Berlin Heidelberg | url=http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 | title=The impact of semantic class identification and semantic role labeling on natural language answer extraction}}&lt;/ref&gt;
* utilization of linguistic resources,&lt;ref&gt;{{cite journal |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes |title=The impact of frame semantic annotation levels, frame&#8208;alignment techniques, and fusion methods on factoid answer processing | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247&#8211;263 |year =2009 |url=http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&amp;userIsAuthenticated=false |doi=10.1002/asi.20989}}&lt;/ref&gt; such as [[WordNet]], [[FrameNet]], and the similar

IBM's question answering system, [[Watson (computer)|Watson]], defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.
&lt;ref&gt;http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0&lt;/ref&gt;

==References==
{{reflist}}

==Further reading==
{{citation style|section|date=January 2016}}
* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.
* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.
*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}
* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.

==External links==
* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]
* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]
* [http://nlp.uned.es/clef-qa/ Question Answering Evaluation at CLEF]
* [http://www.gyanibano.com Quiz Question Answers]

{{Computable knowledge}}
{{Natural Language Processing}}


[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval genres]]</text>
      <sha1>fc174yoongokvg7xj4js3qv2jeaylh8</sha1>
    </revision>
  </page>
  <page>
    <title>Expertise finding</title>
    <ns>0</ns>
    <id>20227676</id>
    <revision>
      <id>761687027</id>
      <parentid>761624067</parentid>
      <timestamp>2017-01-24T06:32:58Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Importance of expertise */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13620" xml:space="preserve">{{multiple issues|
{{original research|date=June 2015}}
{{refimprove|date=June 2015}}
{{cleanup|date=November 2010}}{{External links|date=January 2012}}
}}
'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.

== Importance of expertise ==

It can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and &#8220;licensing&#8221; expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.

Until very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one&#8217;s judgment about those individuals is justified and that their answers are thoughtful.

In the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed &#8220;expertise locating systems&#8221;.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|&#8220;recommender systems&#8221;]].

At the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.

Still other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed &#8220;gated objects&#8221;, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.

More recently, LinkedIn Expertise Search introduces a hybrid approach based on user-generated data (e.g., member profiles), community-based signals (e.g., recommendations and skill endorsements) and personalized signals (e.g., social connection between searcher and results).&lt;ref name=":0"&gt;{{Cite journal|last=Ha-Thuc|first=Viet|last2=Venkataraman|first2=Ganesh|last3=Rodriguez|first3=Mario|last4=Sinha|first4=Shakti|last5=Sundaram|first5=Senthil|last6=Guo|first6=Lin|date=2016-02-15|title=Personalized Expertise Search at LinkedIn|url=http://arxiv.org/abs/1602.04572|journal=arXiv:1602.04572 [cs]}}&lt;/ref&gt; Given required [[LinkedIn#Skills|skills]] and other types of information need like location and industries, the system allows recruiters to search for hiring candidates amongst more than 450 million LinkedIn members.

Examples of the systems outlined above are listed in Table 1.

'''Table 1: A classification of expertise location systems'''

{| class="wikitable" border="1"
|-
! Type
! Application domain
! Data source
! Examples
|-
| Social networking
| Professional networking
| User-generated and community-generated
|
* [[LinkedIn]] &lt;ref name=":0" /&gt;
|-
| [[Scientific literature]]
| Identifying publications with strongest research impact
| Third-party generated
|
* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]
|-
| [[Scientific literature]]
| Expertise search
| Software
|
* [[Arnetminer]][http://arnetminer.org]
|-
| Knowledge base
| Private expertise database
| User-Generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* Decisiv Search Matters &amp; Expertise ([[Recommind (software company)|Recommind]], Inc.)
* [http://www.profinda.com ProFinda] (ProFinda Ltd)
* [https://skillhive.com Skillhive] (Intunex)
* [[Tacit Software]] (Oracle Corporation)
* [http://www.guruscan.nl GuruScan] (GuruScan Social Expert Guide)
|-
| Knowledge base
| Publicly accessible expertise database
| User-generated
|
* [http://expertisefinder.com/ Expertise Finder]&lt;ref&gt;http://expertisefinder.com/&lt;/ref&gt;
* [[Community of Science]] Expertise [http://expertise.cos.com]
* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]
|-
| Knowledge base
| Private expertise database
| Third party-generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* MindServer Expertise ([[Recommind]], Inc.)
* Tacit Software
|-
| Knowledge base
| Publicly accessible expertise database
| Third party-generated
|
* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)
* [http://authoratory.com/ authoratory.com]
* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)
* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)
* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)
* [https://web.archive.org/web/20081120175851/http://www.researchcrossroads.org/ ResearchCrossroads.org]  (Innolyst, Inc.)
|-
| Blog [[search engine]]s
|
| Third party-generated
|
* [[Technorati]] [http://technorati.com/]
|}

== Technical problems ==
A number of interesting problems follow from the use of expertise finding systems:

* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.
* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).
* Finding ways to avoid &#8220;gaming&#8221; of the system to reap unjustified expertise [[credibility]].
* Infer expertise on implicit skills. Since users typically do not declare all of the skills they have, it is important to infer their implicit skills that are highly related their explicit ones. The inference step can significantly improve [[Precision and recall|recall]] in expertise finding.

== Expertise ranking ==

Means of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:

* How can expertise be assessed objectively? Is that even possible?
* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?
* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?
* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?
* How to make expertise ranking personalized to each individual searcher? This is particularly important for recruiting purpose since given the same skills, recruiters from different companies, industries, locations might have different preferences on candidates &lt;ref name=":0" /&gt;

== Sources of data for assessing expertise ==
Many types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.

Unfiltered data sources that have been used to assess expertise, in no particular ranking order:

* user recommendations
* help desk tickets: what the problem was and who fixed it
* e-mail traffic between users
* documents, whether private or on the web, particularly publications
* user-maintained web pages
* reports (technical, marketing, etc.)

Filtered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:

* [[patent]]s, particularly if issued
* scientific publications
* issued grants (failed grant proposals are rarely know beyond the authors)
* [[clinical trial]]s
* product launches
* pharmaceutical drugs

== Approaches for creating expertise content ==
* Manual, either by experts themselves (e.g., [https://skillhive.com Skillhive]) or by a curator ([http://expertisefinder.com/ Expertise Finder])
* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard] )
* In industrial expertise search engines (e.g., LinkedIn), there are many signals coming into the ranking functions, such as, user-generated content (e.g., profiles), community-generated content (e.g., recommendations and skills endorsements) and personalized signals (e.g., social connections). Moreover, user queries might contain many other aspects rather required expertise, such as, locations, industries or companies. Thus, traditional [[information retrieval]] features like text matching are also important. [[Learning to rank]] is typically used to combine all of these signals together into a ranking function &lt;ref name=":0" /&gt;

== Interesting expertise systems over the years ==
In no particular order...

* [http://www.guruscan.nl/ GuruScan]
* Autonomy's IDOL
* AskMe
* [http://expertisefinder.com/ Expertise Finder]
* Tacit Knowledge Systems' ActiveNet
* Triviumsoft's SEE-K
* MIT&#8217;s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)
* MITRE&#8217;s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]
* MITRE&#8217;s XpertNet
* Arnetminer (ref 2)
* Dataware II Knowledge Directory
* Thomson&#8217;s tool
* Hewlett-Packard&#8217;s CONNEX
* Microsoft&#8217;s SPUD project
* [http://www.profinda.com ProFinda]
* [http://www.xperscore.com Xperscore]
* [http://intunex.fi/skillhive/ Skillhive]
* LinkedIn&lt;ref name=":0" /&gt;

== Conferences ==
# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]

== References ==
{{Reflist}}

# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.
# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.
# Maybury, M., D&#8217;Amore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.
# Maybury, M., D&#8217;Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.
# Maybury, M., D&#8217;Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.
# Maybury, M., D&#8217;Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.
# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.
# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.
# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.

[[Category:Evaluation methods]]
[[Category:Metrics]]
[[Category:Analysis]]
[[Category:Impact assessment]]
[[Category:Knowledge sharing]]
[[Category:Library science]]
[[Category:Information retrieval genres]]
[[Category:Science studies]]</text>
      <sha1>5qwrh7ca3umvnhfygu2648br4addfdx</sha1>
    </revision>
  </page>
  <page>
    <title>Multimedia information retrieval</title>
    <ns>0</ns>
    <id>33407925</id>
    <revision>
      <id>756742240</id>
      <parentid>743226569</parentid>
      <timestamp>2016-12-26T15:12:45Z</timestamp>
      <contributor>
        <username>Saeidbk</username>
        <id>28099371</id>
      </contributor>
      <minor />
      <comment>fixed a typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6989" xml:space="preserve">{{COI|date=July 2014}}
{{Original research|date=July 2014}}
{{Use dmy dates|date=February 2012}}
'''Multimedia information retrieval''' ('''MMIR''' or '''MIR''') is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.&lt;ref name=Eidenberger&gt;H Eidenberger. ''Fundamental Media Understanding'', atpress, 2011, p. 1.&lt;/ref&gt;{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:

# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.
# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])
# Methods for the [[categorization]] of media descriptions into classes.

== Feature extraction methods ==

Feature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.&lt;ref name=Eidenberger/&gt;{{rp|2}}{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:

* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel-frequency cepstrum|mel-frequency cepstral coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms&lt;ref&gt;A Del Bimbo. ''Visual Information Retrieval'', Morgan Kaufmann, 1999.&lt;/ref&gt; such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.
* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,&lt;ref&gt;HG Kim, N Moreau, T Sikora.'' MPEG-7 Audio and Beyond", Wiley, 2005.&lt;/ref&gt; texture description in the visual domain and n-grams in text information retrieval.

== Merging and filtering methods ==

Multimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.&lt;ref&gt;MS Lew (Ed.). ''Principles of Visual Information Retrieval'', Springer, 2001.&lt;/ref&gt; Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions &#8211; as they frequently occur in motion description &#8211; have to be normalized to a fixed length first.

Frequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.

== Categorization methods ==

Generally, all forms of machine learning can be employed for the categorization of multimedia descriptions&lt;ref name=Eidenberger/&gt;{{rp|125}}{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[dynamic time warping]] &#8211; a semantically related method &#8211; is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:

* Metric approaches ([[Cluster analysis]], [[vector space model]], [[Minkowski]] distances, dynamic alignment)
* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-means, [[self-organizing map]])
* Risk Minimization (Support vector regression, [[support vector machine]], [[linear discriminant analysis]])
* Density-based Methods (Bayes nets, [[Markov process]]es, mixture models)
* Neural Networks ([[Perceptron]], associative memories, spiking nets)
* Heuristics ([[Decision trees]], random forests, etc.)

The selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.

== Open problems ==

The quality of MMIR Systems&lt;ref&gt;JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.&lt;/ref&gt; depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.&lt;ref&gt;H Eidenberger. ''Frontiers of Media Understanding'', atpress, 2012.&lt;/ref&gt; The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.

== Related areas ==

MMIR provides an overview over methods employed in the areas of information retrieval.&lt;ref&gt;H Eidenberger. ''Professional Media Understanding'', atpress, 2012.&lt;/ref&gt;&lt;ref&gt;{{cite journal |last=Raieli |first=Roberto ||date= |title=Introducing Multimedia Information Retrieval to libraries |url=http://leo.cineca.it/index.php/jlis/article/view/11530 |journal=JLIS.it |volume=7 |issue=3 |pages=9-42 |doi=10.4403/jlis.it-11530 |access-date=8 October 2016 }}&lt;/ref&gt; Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:

* [[Bioinformatics|Bioinformation analysis]]
* [[Biosignal|Biosignal processing]]
* [[Content-based image retrieval|Content-based image and video retrieval]]
* [[Facial recognition system|Face recognition]]
* [[Music information retrieval|Audio and music classification]]
* [[Speech recognition]]
* [[Technical analysis|Technical chart analysis]]
* [[Video Browsing|Video browsing]]
* [[Information retrieval|Text information retrieval]]

The ''Journal of Multimedia Information Retrieval''&lt;ref&gt;"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.&lt;/ref&gt; documents the development of MMIR as a research discipline that is independent of these areas. See also ''Handbook of Multimedia Information Retrieval''&lt;ref&gt;H Eidenberger. ''Handbook of Multimedia Information Retrieval'', atpress, 2012.&lt;/ref&gt; for a complete overview over this research discipline.

==References==
{{reflist}}

[[Category:Information retrieval genres]]</text>
      <sha1>5mt2f0afz0ehp4acixhfq0doivk4a29</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-document summarization</title>
    <ns>0</ns>
    <id>6870342</id>
    <revision>
      <id>752729896</id>
      <parentid>749532129</parentid>
      <timestamp>2016-12-02T23:06:57Z</timestamp>
      <contributor>
        <ip>148.240.2.63</ip>
      </contributor>
      <comment>/* Real-life systems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10323" xml:space="preserve">{{Refimprove|date=January 2016}}
'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].

==Key benefits==
Multi-[[document summarization]] creates information reports that are both concise and comprehensive.
With different opinions being put together &amp; outlined, every topic is described from multiple perspectives within a single document.
While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

==Technological challenges==
The multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,&lt;ref&gt;{{cite web|url=http://www-nlpir.nist.gov/projects/duc/index.html |title=Document Understanding Conferences |website=Nlpir.nist.gov |date=2014-09-09 |accessdate=2016-01-10}}&lt;/ref&gt; conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.

An ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:
*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections
*text within sections is divided into meaningful paragraphs
*gradual transition from more general to more specific thematic aspects
*good [[readability]]

The latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:
*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)
*no dangling references to what is not mentioned or explained in the overview
*no text breaks across a sentence
*no semantic [[Redundancy (information theory)|redundancy]].

==Real-life systems==
The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.
* Ultimate Research Assistant&lt;ref&gt;{{cite web|url=http://ultimate-research-assistant.com/ |title=Generate Research Report |publisher=Ultimate Research Assistant |date= |accessdate=2016-01-10}}&lt;/ref&gt; - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. 
* iResearch Reporter&lt;ref&gt;{{cite web|url=http://www.iresearch-reporter.com/ |title=iResearch Reporter service |website=Iresearch-reporter.com |date= |accessdate=2016-01-10}}&lt;/ref&gt; - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.
* Newsblaster&lt;ref&gt;[http://newsblaster.cs.columbia.edu]  {{webarchive |url=https://web.archive.org/web/20130416065538/http://newsblaster.cs.columbia.edu |date=April 16, 2013 }}&lt;/ref&gt; is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.
* NewsInEssence&lt;ref&gt;[http://www.newsinessence.com]  {{webarchive |url=https://web.archive.org/web/20110411005726/http://www.newsinessence.com |date=April 11, 2011 }}&lt;/ref&gt; may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.
* NewsFeed Researcher&lt;ref&gt;{{cite web|url=http://newsfeedresearcher.com |title=News Feed Researcher &amp;#124; General Stuff |website=Newsfeedresearcher.com |date= |accessdate=2016-01-10}}&lt;/ref&gt; is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.
* Scrape This&lt;ref&gt;[http://www.scrapethis.com]  {{webarchive |url=https://web.archive.org/web/20090919054723/http://www.scrapethis.com |date=September 19, 2009 }}&lt;/ref&gt; is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.
* JistWeb&lt;ref&gt;[http://www.jastatechnologies.com/productList.html]  {{webarchive |url=https://web.archive.org/web/20130529112318/http://www.jastatechnologies.com/productList.html |date=May 29, 2013 }}&lt;/ref&gt; is a query specific multiple document summariser.
* The Simplish Simplifying &amp; Summarizing tool&lt;ref&gt;{{cite web|url=http://simplish.org/ |title=Simplish Basic english Tool |publisher=The Goodwill Consortium |date= |accessdate=2016-02-12}}&lt;/ref&gt; - performs automatic multi-lingual multi-document summarization. This tool does not need training of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary). 

As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.

==Bibliography==
* G&#252;nes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]
* Dragomir R. Radev, Hongyan Jing, Malgorzata Sty&#347;, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919&#8211;938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]
* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74&#8211;82, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]
* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&amp;nbsp;457&#8211;464, 2002
*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR&#8217;05, Salvador, Brazil, August 15&#8211;19, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]
*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&amp;nbsp;35&#8211;55, 2002
*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9&#8211;10, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]
* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&amp;nbsp;724&#8211;728. Springer Berlin Heidelberg, 2009.

==See also==
* [[Automatic summarization]]
* [[Text mining]]
* [[News aggregators]]

==References==
{{reflist}}

==External links==
*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]
*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]
*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]

{{Natural Language Processing}}

{{DEFAULTSORT:Multi-Document Summarization}}
[[Category:Natural language processing]]
[[Category:Information retrieval genres]]</text>
      <sha1>4021eiugthu33xdnlkxwpfj0w9prxfy</sha1>
    </revision>
  </page>
  <page>
    <title>Dragomir R. Radev</title>
    <ns>0</ns>
    <id>31253847</id>
    <revision>
      <id>751100374</id>
      <parentid>719755646</parentid>
      <timestamp>2016-11-23T10:29:51Z</timestamp>
      <contributor>
        <ip>110.92.98.1</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5702" xml:space="preserve">'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]]. From January 2017 he will join [[Yale University]] as a professor of computer science.
He is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.

Radev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006&#8211;present) and associate editor of [http://www.jair.org JAIR].

== Awards ==
As [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].

In 2015 he was named a [[fellow]] of the [[Association for Computing Machinery]] "for contributions to natural language processing and computational linguistics."&lt;ref&gt;{{citation|url=http://www.acm.org/press-room/news-releases/2015/fellows-2015|title=ACM Fellows Named for Computing Innovations that Are Advancing Technology in the Digital Age|publisher=[[Association for Computing Machinery]]|year=2015|accessdate=2015-12-10}}.&lt;/ref&gt;

== IOL==
Radev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].

== Books ==
* Puzzles in Logic, Languages and Computation (2013) &lt;ref&gt;{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt;
* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']

== Selected Papers ==
* SIGIR 1995 Generating summaries of multiple news articles
* ANLP 1997 Building a generation knowledge source using internet-accessible newswire
* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources
* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities
* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation
* CIKM 2001 Mining the web for answers to natural language questions
* AAAI 2002 Towards CST-enhanced summarization
* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project
* Information Processing and Management 2004 Centroid-based summarization of multiple documents
* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization
* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web
* Communications of the ACM 2005 NewsInEssence: summarizing online news topics
* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing
* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network
* IEEE Intelligent Systems 2008 natural language processing and the web
* NAACL 2009 Generating surveys of scientific paradigms
* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways
* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts
* KDD 2010 Divrank: the interplay of prestige and diversity in information networks
* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs
* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language
* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology

==External links==
* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]
* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]
* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]
* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]
* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]

== References ==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*
*
*
*


{{DEFAULTSORT:Radev, Dragomir R.}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]

[[Category:Columbia School of Engineering and Applied Science alumni]]
[[Category:American computer scientists]]
[[Category:University of Michigan faculty]]
[[Category:Natural language processing]]
[[Category:Information retrieval researchers]]
[[Category:Fellows of the Association for Computing Machinery]]</text>
      <sha1>hhk5fper3t7ctndjuawin91u1cxj2ma</sha1>
    </revision>
  </page>
  <page>
    <title>Karen Sp&#228;rck Jones</title>
    <ns>0</ns>
    <id>17212387</id>
    <revision>
      <id>754982191</id>
      <parentid>736905046</parentid>
      <timestamp>2016-12-15T15:59:27Z</timestamp>
      <contributor>
        <ip>50.46.147.154</ip>
      </contributor>
      <comment>/* Karen Sp&#228;rck Jones Award */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9141" xml:space="preserve">{{Infobox scientist
| name = Karen Sp&#228;rck Jones
| image = Karen Sp&#228;rck.jpg
| caption = Karen Sp&#228;rck Jones in 2002
| birth_date = {{birth date|1935|8|26|df=y}}
| birth_place = [[Huddersfield]], [[Yorkshire]]
| death_date = {{death date and age|2007|4|4|1935|8|26|df=y}}
| death_place = [[Willingham, Cambridgeshire]]
| residence = United Kingdom
| nationality = British
| field = Computer science
| work_institution = [[University of Cambridge Computer Laboratory]]
| alma_mater = University of Cambridge
| doctoral_advisor = Richard Braithwaite&lt;ref name=odnb&gt;{{cite web|title=Jones, Karen Ida Boalth Sp&#228;rck (1935&#8211;2007), Computer Scientist|url=http://www.oxforddnb.com/view/article/98729|work=Oxford Dictionary of National Biography|publisher=Oxford University Press|accessdate=5 October 2014}}&lt;/ref&gt;
| thesis_title = Synonymy and Semantic Classi&#64257;cation
| thesis_year = 1964&lt;ref&gt;{{cite book|author=Karen Sp&#228;rck Jones|title=Synonymy and Semantic Classification (thesis published as a book)|publisher=Edinburgh University Press|series=Edinburgh Information Technology series|volume=1|year=1986}}&lt;/ref&gt;
| doctoral_students = 
| known_for  = work on information retrieval and natural language processing, in particular her probabilistic model of document and text retrieval
| prizes = ACL Lifetime Achievement Award, BCS Lovelace Medal, ACM-AAAI Allen Newell Award, ACM SIGIR Salton Award, American Society for Information Science and Technology&#8217;s Award of Merit
| religion = 
| spouse = [[Roger Needham]]
| website = {{URL|http://www.cl.cam.ac.uk/archive/ksj21}}
}}

'''Karen Sp&#228;rck Jones''' [[Fellow of the British Academy|FBA]] (26 August 1935 &#8211; 4 April 2007) was a [[United Kingdom|British]] computer scientist.&lt;ref&gt;{{Cite journal | last1 = Tait | first1 = J. I. | title = Karen Sp&#228;rck Jones | doi = 10.1162/coli.2007.33.3.289 | journal = Computational Linguistics | volume = 33 | issue = 3 | pages = 289&#8211;291 | year = 2007 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Robertson | first1 = S. | last2 = Tait | first2 = J. | doi = 10.1002/asi.20784 | title = Karen Sp&#228;rck Jones | journal = Journal of the American Society for Information Science and Technology | volume = 59 | issue = 5 | pages = 852 | year = 2008 | pmid =  | pmc = }}&lt;/ref&gt;

==Personal life==
Karen Ida Boalth Sp&#228;rck Jones was born in [[Huddersfield]], [[Yorkshire]], [[England]]. Her father was Owen Jones, a lecturer in chemistry, and her mother was Ida Sp&#228;rck, a [[Norway|Norwegian]] who moved to Britain during [[World War II]].  They left Norway on one of the last boats out after the German invasion in 1940.&lt;ref name="odnb" /&gt; Sp&#228;rck Jones was educated at a grammar school in Huddersfield and then [[Girton College, Cambridge]] from 1953 to 1956, reading History, with an additional final year in Moral Sciences (philosophy). She briefly became a school teacher, before moving into Computer Science.  During her career in Computer Science, she campaigned hard for more women to enter computing.&lt;ref name="odnb" /&gt;   She was married to fellow Cambridge computer scientist [[Roger Needham]] until his death in 2003. She died 4 April 2007 at [[Willingham, Cambridgeshire|Willingham]] in [[Cambridgeshire]].

==Career==
She worked at the Cambridge Language Research Unit from the late 1950s,&lt;ref&gt;{{cite web|url=http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/|title=Computer Laboratory obituary}}&lt;/ref&gt; then at [[University of Cambridge|Cambridge's]] [[Cambridge University Computer Laboratory|Computer Laboratory]] from 1974, and retired in 2002, holding the post of Professor of Computers and Information, which she was awarded in 1999.&lt;ref name="odnb" /&gt; She continued to work in the Computer Laboratory until shortly before her death. Her main research interests, since the late 1950s, were [[natural language processing]] and [[information retrieval]].&lt;ref name="doi10.1108/eb026526"&gt;{{Cite journal | last1 = Sp&#228;rck Jones | first1 = K. | authorlink1 = Karen Sp&#228;rck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11&#8211;21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | editor1-last = Tait | editor1-first = John I. | title = Charting a New Course: Natural Language Processing and Information Retrieval, Essays in Honour of Karen Sp&#228;rck Jones| doi = 10.1007/1-4020-3467-9 | series = The Kluwer International Series on Information Retrieval | volume = 16 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}&lt;/ref&gt; One of her most important contributions was the concept of [[inverse document frequency]] (IDF) weighting in information retrieval, which she introduced in a 1972 paper.&lt;ref name="doi10.1108/eb026526"/&gt;&lt;ref name="idf"&gt;{{Cite journal | last1 = Sp&#228;rck Jones | first1 = K. | authorlink1 = Karen Sp&#228;rck Jones| title = Index term weighting | doi = 10.1016/0020-0271(73)90043-0 | journal = Information Storage and Retrieval | volume = 9 | issue = 11 | pages = 619&#8211;633 | year = 1973 | pmid =  | pmc = }}&lt;/ref&gt; IDF is used in most search engines today, usually as part of the [[tf-idf]] weighting scheme.&lt;ref&gt;{{Cite book | last1 = Maybury | first1 = M. T. | chapter = Karen Sp&#228;rck Jones and Summarization | doi = 10.1007/1-4020-3467-9_7 | title = Charting a New Course: Natural Language Processing and Information Retrieval | series = The Kluwer International Series on Information Retrieval | volume = 16 | pages = 99&#8211;10 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}&lt;/ref&gt;

There is an annual [[British Computer Society|BCS]] lecture named in her honour.&lt;ref&gt;{{cite web|title=Karen Sp&#228;rck Jones lecture|url=http://academy.bcs.org/ksj|work=BCS Academy of Computing|publisher=British Computer Society|accessdate=3 October 2013}}&lt;/ref&gt;

===Honours===
* Fellow of the [[British Academy]], of which she was Vice-President in 2000&#8211;02
* Fellow of [[AAAI]]
* Fellow of [[ECCAI]]
* President of the [[Association for Computational Linguistics]] in 1994

===Awards===
* [[Gerard Salton Award]] (1988)
* [[ASIS&amp;T]] Award of Merit (2002)
* [[Association for Computational Linguistics|ACL]] Lifetime Achievement Award (2004) &lt;ref&gt;{{cite web|title=ACL Lifetime Achievement Award Recipients|url=http://aclweb.org/aclwiki/index.php?title=ACL_Lifetime_Achievement_Award_Recipients|website=ACL wiki|publisher=[[Association for Computational Linguistics|ACL]]|accessdate=16 August 2014}}&lt;/ref&gt;
* [[British Computer Society|BCS]] [[Lovelace Medal]] (2007)
* [[ACM - AAAI Allen Newell Award]] (2006)

==Karen Sp&#228;rck Jones Award==
To commemorate her achievements, the Karen Sp&#228;rck Jones Award was created in 2008 by the [[British Computer Society|BCS]] and its Information Retrieval Specialist Group (BCS IRSG), which is sponsored by [[Microsoft Research]].&lt;ref&gt;[http://irsg.bcs.org/ksjaward.php Microsoft BCS/BCS IRSG Karen Sp&#228;rck Jones Award An Award to Commemorate Karen Sp&#228;rck Jones]&lt;/ref&gt;

The recipients are:
* 2016, [[Jaime Teevan]]
* 2015, [[Jordan Boyd-Graber]], [[Emine Yilmaz]]
* 2014, [[Ryen White]]
* 2013, [[Eugene Agichtein]]
* 2012, [[Diane Kelly(computer scientist)]]
* 2011, No award was made
* 2010, [[Evgeniy Gabrilovich]]
* 2009, [[Mirella Lapata]]

==References==
{{reflist}}

==Further reading==
* [http://spectrum.ieee.org/may07/5063 Computer Science, A Woman's Work], IEEE Spectrum, May 2007

==External links==
*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/video/ Video: Natural Language and the Information Layer, Karen Sp&#228;rck Jones, March 2007]
*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/ University of Cambridge obituary]
*[http://news.independent.co.uk/people/obituaries/article2441969.ece Obituary], ''[[The Independent]]'', 12 April 2007 {{dead link|date=April 2014}}
*[http://www.telegraph.co.uk/news/main.jhtml?view=DETAILS&amp;grid=&amp;xml=/news/2007/04/12/db1201.xml  Obituary], ''[[The Daily Telegraph]]'', 12 April 2007 {{dead link|date=April 2014}}
*[http://www.timesonline.co.uk/tol/comment/obituaries/article1968942.ece Obituary], ''[[The Times]]'', 22 June 2007 {{subscription required}}

{{s-start}}
{{s-ach}}
{{succession box |
 before=[[Makoto Nagao]] |
 title=ACL Lifetime Achievement Award |
 after=[[Martin Kay]] |
 years=2004}}
{{s-end}}

{{Authority control}}

{{DEFAULTSORT:Sparck Jones, Karen}}
[[Category:1935 births]]
[[Category:2007 deaths]]
[[Category:Alumni of Girton College, Cambridge]]
[[Category:British computer scientists]]
[[Category:Women computer scientists]]
[[Category:Fellows of the British Academy]]
[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]
[[Category:Fellows of Newnham College, Cambridge]]
[[Category:Fellows of Wolfson College, Cambridge]]
[[Category:Members of the University of Cambridge Computer Laboratory]]
[[Category:People from Huddersfield]]
[[Category:Deaths from cancer in England]]
[[Category:Information retrieval researchers]]
[[Category:British women scientists]]
[[Category:Artificial intelligence researchers]]
[[Category:20th-century women scientists]]</text>
      <sha1>0q0ioemeyo5xj2f2cty45c1k78vmtqi</sha1>
    </revision>
  </page>
  <page>
    <title>Logic form</title>
    <ns>0</ns>
    <id>1936537</id>
    <revision>
      <id>750076051</id>
      <parentid>686866750</parentid>
      <timestamp>2016-11-17T18:05:36Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3180" xml:space="preserve">'''Logic forms''' are simple, [[first-order logic]] [[knowledge representation]]s of [[natural language]] sentences formed by the conjunction of concept predicates related through shared arguments. Each noun, verb, adjective, adverb, pronoun, preposition and conjunction generates a predicate. Logic forms can be decorated with [[word sense]]s to [[Word sense disambiguation|disambiguate]] the semantics of the word. There are two types of predicates: events are marked with ''e'', and entities are marked with ''x''. The shared arguments connect the subjects and objects of verbs and prepositions together. Example input/output might look like this:
 Input:  '''The Earth provides the food we eat every day.'''
 Output: '''Earth''':n_#1(&lt;span style="color:#008800;"&gt;x1&lt;/span&gt;) '''provide''':v_#2(&lt;span style="color:#888800;"&gt;e1&lt;/span&gt;, &lt;span style="color:#008800;"&gt;x1&lt;/span&gt;, &lt;span style="color:#880000;"&gt;x2&lt;/span&gt;) '''food''':n_#1(&lt;span style="color:#880000;"&gt;x2&lt;/span&gt;) '''we'''(&lt;span style="color:#000088;"&gt;x3&lt;/span&gt;) '''eat''':v_#1(&lt;span style="color:#880088;"&gt;e2&lt;/span&gt;, &lt;span style="color:#000088;"&gt;x3&lt;/span&gt;, &lt;span style="color:#880000;"&gt;x2&lt;/span&gt;; &lt;span style="color:#008888;"&gt;x4&lt;/span&gt;) '''day''':n_#1(&lt;span style="color:#008888;"&gt;x4&lt;/span&gt;)

Logic forms are used in some [[natural language processing]] techniques, such as [[question answering]], as well as in [[inference]] both for [[database]] systems and QA systems.

==Evaluations==
[http://www.senseval.org/ SENSEVAL-3] in 2004 introduced a {{webarchive |url=https://web.archive.org/web/20050902115653/site=http://www.cs.iusb.edu/~vasile/logic/indexLF.html |date=September 2, 2005 |title=Logic Form Identification task }}.

==References==
*{{cite book | author=Vasile Rus | title=Logic Form for WordNet Glosses  | url=http://www.engr.smu.edu/~vasile/rus02.PhDThesis.ps | publisher=Ph.D. thesis, Southern Methodist University | year=2002 }} &lt;!-- Most information in the article derived from Vasile's work --&gt;
*{{cite journal | author=Vasile Rus and Dan Moldovan | title=High performance logic form transformation | journal=International Journal for Tools with Artificial Intelligence. IEEE Computer Society, IEEE Press |date=September 2002 | volume=11| issue =  3 | pages=437&#8211;454 | url=http://www.worldscinet.com/ijait/11/1103/S0218213002000976.html}}
*{{cite conference | author=Dan Moldovan and Vasile Rus | url=http://engr.smu.edu/~vasile/acl2001.ps | title=Logic Form transformation of wordNet and its Applicability to question answering | booktitle=Proceedings of ACL 2001, Toulouse, France | year=2001 | pages=}}
*{{cite conference | author=Jerry R. Hobbs | title=Overview of the TACITUS project | booktitle=Computational Linguistics| year=1986 | pages=12(3)}}
*{{cite conference | author=Vasile Rus | url=http://acl.ldc.upenn.edu/acl2004/senseval/pdf/rus.pdf | title=A First Evaluation of Logic Form Identification Systems | booktitle=SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | year=2004 | pages=|format=PDF}}

[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Knowledge representation]]
{{ling-stub}}</text>
      <sha1>2tfugragxl0hvkmai4u5gj63vvpnlgv</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted classification</title>
    <ns>0</ns>
    <id>796657</id>
    <revision>
      <id>713819476</id>
      <parentid>709403952</parentid>
      <timestamp>2016-04-06T01:34:20Z</timestamp>
      <contributor>
        <ip>68.187.232.213</ip>
      </contributor>
      <comment>/* Art and Architecture Thesaurus (AAT) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13158" xml:space="preserve">{{mergefrom|Faceted search|date=January 2015}}
'''Faceted classification''' is a [[classification scheme]] used in organizing knowledge into a systematic order. A faceted classification uses semantic categories, either general or subject-specific, that are combined to create the full classification entry. Many library classification systems use a combination of a fixed, enumerative taxonomy of concepts with subordinate facets that further refine the topic.

== Definition ==

There are two primary types of classification used for information organization: enumerative and faceted. An enumerative classification contains a full set of entries for all concepts.&lt;ref name=lcsh&gt;{{Citation
 |publisher = Libraries Unlimited
 |isbn = 1591581540
 |publication-place = Westport, Conn
 |title = Library of Congress subject headings
 |url = http://openlibrary.org/books/OL3311856M/Library_of_Congress_subject_headings
 |author = Lois Mai Chan
 |publication-date = 2005
 |id = 1591581540
 }}&lt;/ref&gt; A faceted classification system uses a set of semantically cohesive categories that are combined as needed to create an expression of a concept. In this way, the faceted classification is not limited to already defined concepts. While this makes the classification quite flexible, it also makes the resulting expression of topics complex.&lt;ref name=sven&gt;{{Citation
        |publisher = MIT Press
        |isbn = 0262194333
        |publication-place = Cambridge, Mass
        |title = The intellectual foundation of information organization
        |url = http://openlibrary.org/books/OL44967M/The_intellectual_foundation_of_information_organization
        |author = Elaine Svenonius
        |publication-date = 2000
        |id = 0262194333
        }}&lt;/ref&gt; To the extent possible, facets represent "clearly defined, mutually exclusive, and collectively exhaustive aspects, properties or characteristics of a class or specific subject".&lt;ref name=taylor&gt;Taylor, A. G. (1992). Introduction to Cataloging and Classification. 8th ed. Englewood, Colorado: Libraries Unlimited.&lt;/ref&gt; Some commonly used general-purpose facets are time, place, and form.&lt;ref name=chan /&gt;

There are few purely faceted classifications; the best known of these is the [[Colon Classification]] of [[S. R. Ranganathan]], a general knowledge classification for libraries. Some other faceted classifications are specific to special topics, such as the Art and Architecture Thesaurus and the faceted classification of occupational safety and health topics created by D. J. Foskett for the International Labour Organization.&lt;ref name=coyle /&gt;

Many library classifications combine the enumerative and faceted classification techniques. The [[Dewey Decimal Classification]], the [[Library of Congress Classification]], and the [[Universal Decimal Classification]] all make use of facets at various points in their enumerated classification schedules. The allowed facets vary based on the subject area of the classification. These facets are recorded as tables that represent recurring types of subdivisions within subject areas. There are general facets that can be used wherever appropriate, such as geographic subdivisions of the topic. Other tables are applied only to specific areas of the schedules. Facets can be combined to create a complex subject statement.&lt;ref name=chan /&gt;

Arlene Taylor describes faceted classification using an analogy: &#8220;If one thinks of each of the faces of a cut and polished diamond as a facet for the whole diamond, one can picture a classification notation that has small notations  standing for subparts of the whole topic strung together to create a complete classification notation&#8221;.&lt;ref&gt;{{cite book|last1=Taylor|first1=Arlene G.|year=2004|title=The organization of information|location=Westport, CT|publisher=Libraries Unlimited}}&lt;/ref&gt;

Faceted classifications exhibit many of the same problems as classifications based on a hierarchy. In particular, some concepts could belong in more than one facet, so their placement in the classification may appear to be arbitrary to the classifier. It also tends to result in a complex notation because each facet must be distinguishable as recorded.&lt;ref name=sven /&gt;

== Retrieval ==

Search in systems with faceted classification can enable a user to navigate information along multiple paths corresponding to different orderings of the facets. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging.&lt;ref name="Star, S.L. 1998"&gt;Star, S.L. (1998, Fall). Grounded classification: grounded theory and faceted classification. [Electronic version]. Library Trends. 47.2, 218.&lt;/ref&gt;  It is also possible to use facets to filter search results to more quickly find desired results.

==Examples of Faceted Classifications==

=== Colon classification for library materials ===
The [[colon classification]] developed by [[S. R. Ranganathan]] is an example of general faceted classification designed to be applied to all library materials. In the Colon Classification system, a book is assigned a set of values from each independent facet.&lt;ref&gt;Garfield, E. (1984, February). A tribute to S.R. Ranganathan, the father of Indian library science. Essays of an Information Scientist, 7, 37-44.&lt;/ref&gt;  This facet formula uses punctuation marks and symbols placed between the facets to connect them. Colon classification was named after its use of the colon as the primary symbol in its notation.&lt;ref&gt;Chan, L.M. (1994). Cataloging and classification.  New York: McGraw-Hill, Inc.&lt;/ref&gt;&lt;ref&gt;[http://www.essessreference.com/servlet/esGetBiblio?bno=000374 Colon Classification (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Publications, Delhi, India.&lt;/ref&gt;

Ranganathan stated that hierarchical classification schemes like the Dewey Decimal Classification (DDC) or the Library of Congress Subject Headings are too limiting and finite to use for modern classification and that many items can pertain information to more than one subject.  He organized his classification scheme into 42 classes.  Each class can be categorized according to particular characteristics, that he called facets.  Ranganathan said that there are five fundamental categories that can be used to demonstrate the facets of a subject: personality, material, energy, space and time.  He called this the PMEST formula:&lt;ref&gt;Ranganathan, S. R (1987). Colon classification, 7th ed. revised and edited by M.A. Gopinath. Bangalore: Sarada Ranganathan Endowment for Library Science, 1987&lt;/ref&gt;  
*Personality is the most specific or focal subject.
*Matter is the substance, properties or materials of the subject.
*Energy includes the processes, operations and activities.
*Space relates to the geographic location of the subject.
*Time refers to the dates or seasons of the subject.

=== Universal Decimal Classification ===
Another example of a faceted classification scheme is the [[Universal Decimal Classification]] (UDC), the UDC is considered to be a complex multilingual classification that can be used in all fields of knowledge.&lt;ref&gt;About universal decimal classification and the udc consortium. (2006). Retrieved November 30, 2013, from http://www.udcc.org/about.htm&lt;/ref&gt;
The Universal Decimal Classification scheme was created at the end of the nineteenth century by Belgian bibliographers [[Paul Otlet]] and [[Henri la Fontaine]]. The goal of their system was to create an index that would be able to record knowledge even if it is stored in non-conventional ways including materials in notebooks and ephemera. They also wanted their index to organize material systematically instead of alphabetically.&lt;ref&gt;Batty, D. (2003). Universal decimal classification.  Encyclopedia of Library and Information Science.&lt;/ref&gt;

The UDC has an overall taxonomy of knowledge that is extended with a number of facets, such as language, form, place and time. Each facet has its own symbol in the notation, such as: "=" for language; "-02" for materials, "[...]" for subordinate concepts.&lt;ref name=chan&gt;{{Cite book     |publisher = The Scarecrow Press, Inc.     |isbn = 978-0-8108-5944-9     |title = Cataloging and classification     |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification     |last = Chan|first=Lois Mai|edition = Third     |publication-date = 2007 |page=321|id = 0810859440}}&lt;/ref&gt;

===Faceted Classification for Occupational Safety and Health===

[[Douglas John Foskett|D. J. Foskett]], a member of the [[Classification Research Group]] in London, developed classification of occupational safety and health materials for the library of the [[International Labour Organization]].&lt;ref name=coyle&gt;{{cite journal|last1=Coyle|first1=Karen|title=A Faceted Classification for Occupational Safety and Health|journal=Special Libraries|date=1975|volume=66|issue=5-6|pages=256&#8211;9}}&lt;/ref&gt;&lt;ref name=foskett&gt;{{cite book|last1=Foskett|first1=D. J.|title=Proceedings of the International Conference on Scientific Information|chapter=Construction of a Faceted Classification for a Special Subject|date=1959|publisher=National Science Foundation|isbn=0-309-57421-8|pages=867&#8211;888}}&lt;/ref&gt; After a study of the literature in the field, he created the classification with the following facets:

*Facet A: Occupational Safety and Health: General
*Facet B: Special Classes of Workers, Industries
*Facet C: Sources of Hazards: Fire, Machinery, etc.
*Facet D: Industrial Accidents and Diseases
*Facet E: Preventive Measures, Protection
*Facet F: Organisation, Administration

Notation was solely alphabetic, with the sub-facets organized hierarchically using extended codes, such as "g Industrial equipment and processes", "ge Machines".&lt;ref name=foskett /&gt;

===Art and Architecture Thesaurus (AAT)===

While not strictly a classification system, the [[Art and Architecture Thesaurus|AAT]] uses facets similar to those of Ranganathan's Colon Classification:

*Associated Concepts (e.g., philosophy)
*Physical Attributes
*Styles and Periods
*Agents (People/Organizations)
*Activities (similar to Ranganathan's Energy)
*Materials (similar to Ranganathan's Matter)
*Objects (similar to Ranganathan's Personality)&lt;ref name=denton&gt;{{cite web |url=https://www.miskatonic.org/library/facet-web-howto.html|author=William Denton|title=How to Make a Faceted Classification and Put it on the Web}}&lt;/ref&gt;

==Comparison between faceted and single hierarchical classification==
Hierarchical classification refers to the classification of objects using one ''single'' hierarchical taxonomy. Faceted classification may actually employ hierarchy in one or more of its facets, but allows for the use of more than one taxonomy to classify objects.

*Faceted classification systems allow the assignment of multiple classifications to an object, and enable those classifications to be applied by searchers in multiple ways, rather than in a single, predetermined order. Multiple facets may be used as a first step in a search process.&lt;ref name="Categories, Facets&#8212;and Browsable Facets?"&gt;Sirovich, Jaimie (2011). Categories, Facets&#8212;and Browsable Facets?, from http://www.uxmatters.com/mt/archives/2011/08/categories-facetsand-browsable-facets.php&lt;/ref&gt; For example, one may ''start'' from language or subject.
*Hierarchical classification systems are developed classes that are subdivided from the most general subjects to the most specific.&lt;ref&gt;Reitz, Joan M. (2004). Dictionary for library and information science. Westport, CT: Libraries Unlimited&lt;/ref&gt;
*Faceted classification systems allow for the combination of facets to [[Filter (software)|filter]] the set of objects rapidly. In addition, the facets can be used to address multiple classification criteria.&lt;ref&gt;Godert, Winfried. F. (1991). Facet classification in online retrieval. International Classification, 18, 98-109&lt;/ref&gt;
*A faceted system focuses on the important, essential or persistent characteristics of content objects, helping it to be useful for categorization of fine-grained rapidly changing repositories.
*In faceted classification systems one does not have to know the name of the category into which an object is placed a priori. A controlled vocabulary is presented with the number of documents matching each vocabulary term.
*New facets may be created at any time without disruption of a single hierarchy or reorganizing other facets.
*Faceted classification systems make few assumptions about the scope and organization of the domain. It is difficult to ''break'' a faceted classification schema.&lt;ref&gt;Adkisson, Hiedi P. (2005).  Use of faceted classification.  Retrieved December 1, 2013, from http://www.webdesignpractices.com/navigation/facets.html&lt;/ref&gt;

==See also==
* [[Classification Research Group]]
* [[Controlled vocabulary]]
* [[Findability]]
* [[Folksonomy]]
* [[Information architecture]]
* [[Tag (metadata)]]
* [[Universal Decimal Classification]]

==References==
{{Reflist|colwidth=35em}}

==External links==
* [http://eprints.soton.ac.uk/271488/ How to ''Reuse'' a Faceted Classification and Put It On the ''Semantic'' Web]

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>iidhs17tl23jnqykao6y0c0uqwfk2pc</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Classification systems</title>
    <ns>14</ns>
    <id>3615452</id>
    <revision>
      <id>756364608</id>
      <parentid>756364448</parentid>
      <timestamp>2016-12-23T18:50:51Z</timestamp>
      <contributor>
        <username>Allforrous</username>
        <id>12120664</id>
      </contributor>
      <comment>new key for [[Category:Classification]]: "systems" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="491" xml:space="preserve">'''Classification systems''' are [[system]]s with a distribution of classes created according to common relations or affinities.
{{Commons cat|Classification systems}}
See also: 
* [[Controlled vocabulary]]
* [[Scientific classification (disambiguation)]]
* [[Taxonomy (biology)|Taxonomy]]

[[Category:Classification|systems]]
[[Category:Conceptual systems]]
[[Category:Formal sciences]]
[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Information systems]]</text>
      <sha1>hwcwnbxb99w6bsq28uescubnjtwjx04</sha1>
    </revision>
  </page>
  <page>
    <title>John F. Sowa</title>
    <ns>0</ns>
    <id>102392</id>
    <revision>
      <id>761071592</id>
      <parentid>761071502</parentid>
      <timestamp>2017-01-20T18:31:57Z</timestamp>
      <contributor>
        <username>Jefferythomas</username>
        <id>13631841</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10745" xml:space="preserve">{{Infobox person
 | name             = John F. Sowa
 | image            =
 | image_size       = 
 | caption          = 
 | birth_name       = John Florian Sowa
 | birth_date       = {{Birth date and age|mf=yes|1940|1|1}}
 | birth_place      = 
 | death_date       = 
 | death_place      = 
 | death_cause      = 
 | resting_place    = 
 | residence        = [[Croton-on-Hudson, New York]]
 | nationality      = 
 | other_names      =
 | known_for        = [[Conceptual graph]]s
 | education        = [[Massachusetts Institute of Technology]] BS 1962, [[Harvard University]] MA 1966, [[Vrije Universiteit Brussel]] PhD 1999
 | alma_mater       = 
 | employer         = 
 | occupation       = Computer Scientist
 | boards           = 
 | religion         = 
 | spouse           = [[Cora Angier Sowa]]
 | children         = 
 | parents          = 
 | relations        =
 | callsign         = 
 | awards           = 
 | signature        =
 | website          = {{URL|http://www.jfsowa.com/|JFSowa.com}}
| 
}}
'''John Florian Sowa''' (born 1940) is an American [[computer scientist]], an expert in [[artificial intelligence]] and [[computer design]], and the inventor of [[conceptual graph]]s.&lt;ref&gt;[[Kecheng Liu]] (2000) ''Semiotics in Information Systems Engineering''. p.54 states: ''Conceptual graphs are devised as a language of knowledge representation by Sowa (1984), based on philosophy, psychology and linguistics. Knowledge in conceptual graph form is highly structured by modelling specialised facts that can be subjected to generalised reasoning.&lt;/ref&gt;&lt;ref&gt;Marite Kirikova (2002) ''Information Systems Development: Advances in Methodologies, Components, and Management''. p.194. states: ''The original theory of conceptual graphs was introduced by Sowa (Sowa, 1984 ). A conceptual graph is a finite, connected, bipartite graph. It includes notions of concepts, relations, and actors...''&lt;/ref&gt;

== Biography ==
Sowa received a BS in mathematics from [[Massachusetts Institute of Technology]] in 1962, an MA in applied mathematics from [[Harvard University]] in 1966, and a PhD in [[computer science]] from the [[Vrije Universiteit Brussel]] in 1999 on a dissertation titled "Knowledge Representation: Logical, Philosophical, and Computational Foundations".&lt;ref&gt;Andreas Tolk, Lakhmi C. Jain (2011) ''Intelligent-Based Systems Engineering''. p.xxi&lt;/ref&gt;

Sowa spent most of his professional career at [[International Business Machines|IBM]], which started in 1962 at IBM's applied mathematics group. Over the decades he has researched and developed emerging fields of [[computer science]] from compiler, programming languages, and system architecture&lt;ref name="SoZa92"&gt;John F. Sowa and [[John Zachman]] (1992). [http://www.research.ibm.com/journal/sj/313/sowa.pdf "Extending and Formalizing the Framework for Information Systems Architecture"] In: ''IBM Systems Journal'', Vol 31, no.3, 1992. p. 590-616.&lt;/ref&gt; to artificial intelligence and knowledge representation. In the 1990s Sowa was associated with IBM Educational Center in New York. Over the years he taught courses at the IBM Systems Research Institute, [[Binghamton University]], [[Stanford University]], [[Linguistic Society of America]] and [[Universit&#233; du Qu&#233;bec &#224; Montr&#233;al]]. He is a fellow of the [[Association for the Advancement of Artificial Intelligence]].

After early retirement at IBM Sowa in 2001 cofounded VivoMind Intelligence, Inc. with [[Arun K. Majumdar]]. With this company he was developing data-mining and database technology, more specific high-level "[[ontology|ontologies]]" for [[artificial intelligence]] and automated [[natural language understanding]]. Currently Sowa is working with [http://kyndi.com/ Kyndi Inc.], also founded by Majumdar. 

John Sowa is married to the philologist Cora Angier Sowa,&lt;ref&gt;Cora Angier Sowa (1984) ''Traditional themes and the Homeric hymns''. p.iv&lt;/ref&gt; and they live in [[Croton-on-Hudson, New York]].

== Work ==
Sowa's research interest since the 1970s were in the field of [[artificial intelligence]], [[expert systems]] and [[database query]] linked to natural languages.&lt;ref name="SoZa92"/&gt; In his work he combines ideas from numerous disciplines and eras modern and ancient, for example, applying ideas from [[Aristotle]], the medieval [[Scholastics]] to [[Alfred North Whitehead]] and including [[logical schema|database schema]] theory, and incorporating the model of analogy of Islamic scholar [[Ibn Taymiyyah]] in his works.&lt;ref&gt;[http://www.jfsowa.com/pubs/analog.htm Analogical Reasoning]&lt;/ref&gt;

=== Conceptual graph ===
{{main|Conceptual graph}}
Sowa invented conceptual graphs, a graphic notation for logic and natural language, based on the structures in [[semantic network]]s and on the [[existential graph]]s of [[Charles Sanders Peirce|Charles S. Peirce]]. He published the concept in the 1976 article "Conceptual graphs for a data base interface" in the ''IBM Journal of Research and Development''.&lt;ref&gt;{{cite journal |last=Sowa |authorlink = |first= John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336&#8211;357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}&lt;/ref&gt; He further explained in the 1983 book ''Conceptual structures: information processing in mind and machine''.

In the 1980s this theory has "been adopted by a number of research and development groups throughout the world.&lt;ref name="SoZa92"/&gt; International conferences on conceptual graphs have been held for over a decade since before 1992.{{citation needed|date=November 2012}}

==={{anchor|law of standards}}Sowa's law of standards===
In 1991, Sowa first stated his ''Law of Standards'': 
: "Whenever a major organization develops a new system as an official [[Technical standard|standard]] for X, the primary result is the widespread adoption of some simpler system as a [[de facto]] standard for X."&lt;ref&gt;[http://www.jfsowa.com/computer/standard.htm Law of Standards]&lt;/ref&gt; 
Like [[Gall's law]], The Law of Standards is essentially an argument in favour of underspecification. Examples include:

*The introduction of [[PL/I]] resulting in [[COBOL]] and [[FORTRAN]] becoming the de facto standards for scientific and business programming
*The introduction of [[Algol-68]] resulting in [[Pascal (programming language)|Pascal]] becoming the de facto standard for academic programming
*The introduction of the [[Ada (programming language)|Ada language]] resulting in [[C (programming language)|C]] becoming the de facto standard for [[United States Department of Defense|DoD]] programming
*The introduction of [[OS/2]] resulting in [[Microsoft Windows|Windows]] becoming the de facto standard for [[desktop OS]]
*The introduction of [[X.400]] resulting in [[SMTP]] becoming the de facto standard for [[electronic mail]]
*The introduction of [[X.500]] resulting in [[LDAP]] becoming the de facto standard for [[directory services]]

== Publications ==
* 1984. ''Conceptual Structures - Information Processing in Mind and Machine''. The Systems Programming Series, Addison-Wesley&lt;ref&gt;[http://conceptualstructures.org/ Conceptual Structures Home Page]. Retrieved Nov 23, 2012.&lt;/ref&gt;
* 1991. ''Principles of Semantic Networks''. Morgan Kaufmann.
* {{Cite journal| editor1-last = Mineau | editor1-first = Guy W| editor2-last = Moulin | editor2-first = Bernard| editor3-last = Sowa | editor3-first = John F | editor3-link = John F. Sowa| title = Conceptual Graphs for Knowledge Representation| doi = 10.1007/3-540-56979-0| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 699| year = 1993| isbn = 978-3-540-56979-4}}
* 1994. ''International Conference on Conceptual Structures (2nd : 1994 : College Park, Md.)	Conceptual structures, current practices : Second International Conference on Conceptual Structures, ICCS'94, College Park, Maryland, USA, August 16&#8211;20, 1994 : proceedings''. William M. Tepfenhart, Judith P. Dick, John F. Sowa, eds.
*{{Cite journal| editor1-last = Ellis | editor1-first = Gerard| editor2-last = Levinson | editor2-first = Robert| editor3-last = Rich | editor3-first = William| editor4-last = Sowa | editor4-first = John F | editor4-link = John F. Sowa| doi = 10.1007/3-540-60161-9| title = Conceptual Structures: Applications, Implementation and Theory| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 954| year = 1995| isbn = 978-3-540-60161-6}}
*{{Cite journal| editor1-last = Lukose | editor1-first = Dickson| editor2-last = Delugach | editor2-first = Harry| editor3-last = Keeler | editor3-first = Mary| editor4-last = Searle | editor4-first = Leroy| editor5-last = Sowa | editor5-first = John | editor5-link = John F. Sowa| doi = 10.1007/BFb0027865| title = Conceptual Structures: Fulfilling Peirce's Dream| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 1257| year = 1997| isbn = 3-540-63308-1}}
* 2000. ''Knowledge representation : logical, philosophical, and computational foundations'', Brooks Cole Publishing Co., Pacific Grove&lt;ref&gt;[http://www.jfsowa.com/krbook/ Knowledge Representation: Logical, Philosophical, and Computational Foundations] at jfsowa.com. Retrieved Nov 23, 2012.&lt;/ref&gt;

;Articles, a selection&lt;ref&gt;{{DBLP|name=John F. Sowa}}&lt;/ref&gt;
*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| title = Conceptual Graphs for a Data Base Interface| doi = 10.1147/rd.204.0336| journal = IBM Journal of Research and Development| volume = 20| issue = 4| pages = 336&#8211;357| date=July 1976 }}
*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| last2 = Zachman | first2 = J. A.| doi = 10.1147/sj.313.0590| title = Extending and formalizing the framework for information systems architecture| journal = IBM Systems Journal| volume = 31| issue = 3| pages = 590&#8211;616| year = 1992}}
* 1992. "[http://www.jfsowa.com/cg/cgif.htm Conceptual Graph Summary]"; In: T.E. Nagle et. al. (Eds.). ''Conceptual Structures: Current Research and Practice''. Chichester: Ellis Horwood. 
* 1995. "Top-level ontological categories." in: ''International journal of human-computer studies''. Vol. 43, Iss. 5&#8211;6, Nov. 1995, pp.&amp;nbsp;669&#8211;685
* 2006. "Semantic Networks". In: ''Encyclopedia of Cognitive Science.''. John Wiley &amp; Sons.

== References ==
{{reflist}}

== External links ==
{{Wikiquote}}
* [http://www.jfsowa.com/ John F. Sowa] homepage

{{Authority control}}

{{DEFAULTSORT:Sowa, John}}
[[Category:1940 births]]
[[Category:Artificial intelligence researchers]]
[[Category:Knowledge representation]]
[[Category:Living people]]
[[Category:People from Croton-on-Hudson, New York]]
[[Category:Harvard University alumni]]
[[Category:Binghamton University faculty]]</text>
      <sha1>ol0j0ajcaaqkhj2bulku86g7qudqbim</sha1>
    </revision>
  </page>
  <page>
    <title>Frame language</title>
    <ns>0</ns>
    <id>485226</id>
    <revision>
      <id>758150012</id>
      <parentid>753160858</parentid>
      <timestamp>2017-01-03T19:36:00Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22899" xml:space="preserve">{{Duplication|dupe=Frame (artificial intelligence)}}

A '''frame language''' is a technology used for [[knowledge representation]] in [[artificial intelligence]]. Frames are stored as [[Ontology (information science)|ontologies]] of [[Set theory|sets]] and subsets of the [[Frame_(artificial_intelligence)|frame concepts]]. They are similar to class hierarchies in [[object-oriented languages]] although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on [[Encapsulation (object-oriented programming)|encapsulation]] and [[information hiding]]. Frames originated in AI research and objects primarily in [[software engineering]]. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.

==Description==
Early work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations.&lt;ref&gt;{{cite book|last=Bartlett|first=F.C.|title=Remembering: A Study in Experimental and Social Psychology|year=1932|publisher=Cambridge University Press|location=Cambridge, England}}&lt;/ref&gt;  The term Frame was first used by [[Marvin Minsky]] as a paradigm to understand visual reasoning and natural language processing.&lt;ref&gt;{{cite book|last=Minsky|first=Marvin|title=The Psychology of Computer Vision|year=1975|publisher=McGraw Hill|location=New York|pages=211&#8211;277|editor=Pat Winston|chapter=A Framework for Representing Knowledge}}&lt;/ref&gt; In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things which seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them.

The initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant.&lt;ref&gt;{{cite book|last=Schank|first=Roger|title=Scripts, Plans, Goals, and Understanding|year=1977|publisher=Lawrence Erlbaum|location=Hillsdale, New Jersey|author2=R. P. Abelson}}&lt;/ref&gt;  These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object-oriented modeling and to relations in entity-relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use "triggers" (similar to the database concept of triggers) attached to slots. A trigger was simply procedural code that was attached to a slot. The trigger could fire either before and/or after a slot value was accessed or modified.

As with object classes, Frames were organized in [[Subsumption relation|subsumption]] hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonalds. A specialization (essentially a [[Subclass (computer science)|subclass]]) of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values (e.g., expected price range) for the specialized frame.&lt;ref&gt;{{cite book|last=Feigenbaum|first=Edward|title=The Handbook of Artificial Intelligence, Volume III|publisher=Addison-Wesley|isbn=0201118114|pages=216&#8211;222|url=https://archive.org/stream/handbookofartific01barr#page/156/mode/2up|author2=Avron Barr|date=September 1, 1986}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3&#8211;46|doi=10.1207/s15516709cog0101_2}}&lt;/ref&gt;

Much of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.

Similarly, in linguistics, [[Charles J. Fillmore]] in the mid-1970s started working on his theory of [[Frame semantics (linguistics)|frame semantics]], which later would lead to computational resources like [[FrameNet]].&lt;ref&gt;{{cite news|last=Lakoff|first=George|title=Charles Fillmore, Discoverer of Frame Semantics, Dies in SF at 84: He Figured Out How Framing Works|url=http://www.huffingtonpost.com/george-lakoff/charles-fillmore-discover_b_4807590.html|accessdate=7 March 2014|newspaper=The Huffington Post|date=18 February 2014}}&lt;/ref&gt; Frame semantics was motivated by reflections on human language and human cognition.

Researchers such as [[Ron Brachman]] on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic.  One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.

This evolution also illustrates a classic divide in AI research known as the "[[neats vs. scruffies]]". The "neats" were researchers who placed the most value on mathematical precision and formalism which could be achieved via [[First Order Logic]] and [[Set Theory]]. The "scruffies" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.&lt;ref&gt;{{cite book|last=Crevier|first=Daniel|title=AI: The Tumultuous Search for Artificial Intelligence|year=1993|publisher=Basic Books|location=New York|isbn=0-465-02997-3|page=168}}&lt;/ref&gt;

The most notable of the more formal approaches was the [[KL-ONE]] language.&lt;ref&gt;{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}&lt;/ref&gt; KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the [[Deductive classifier|classifier]]. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41&#8211;46}}&lt;/ref&gt;

This technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the [[Semantic Web]]. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web.&lt;ref&gt;{{cite journal|last=Berners-Lee |first=Tim |author2=James Hendler |author3=Ora Lassila |title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities |journal=Scientific American |date=May 17, 2001 |url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html |doi=10.1038/scientificamerican0501-34 |volume=284 |pages=34&#8211;43 |deadurl=yes |archiveurl=https://web.archive.org/web/20130424071228/http://www.cs.umd.edu/%7Egolbeck/LBSC690/SemanticWeb.html |archivedate=2013-04-24 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Horridge|first=Mathew|title=Prot&#233;g&#233; OWL Tutorial A step-by-step guide to modelling in OWL using the popular Prot&#233;g&#233; OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}&lt;/ref&gt; The "neats vs. scruffies" divide also emerged in Semantic Web research, culminating in the creation of the [[Linking Open Data]] community&#8212;their focus was on exposing data on the Web rather than modeling.

==Example==
A simple example of concepts modeled in a frame language is the [[FOAF (ontology)|Friend of A Friend (FOAF) ontology]] defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a ''Person''. Example slots are the person's ''email'', ''home page, phone,'' etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot ''knows'' links each person with other persons. Default values for a person's interests can be inferred by the web of people they are friends of.&lt;ref&gt;{{cite web|title=FOAF|url=http://semanticweb.org/wiki/FOAF|website=http://semanticweb.org|accessdate=7 June 2014}}&lt;/ref&gt;

==Implementations==
The earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re-used by other researchers. Just as with [[expert system]] [[inference engine]]s, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL.&lt;ref&gt;{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3&#8211;46|doi=10.1207/s15516709cog0101_2}}&lt;/ref&gt; One of the most influential early Frame languages was [[KL-ONE]]&lt;ref&gt;{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}&lt;/ref&gt; KL-ONE spawned several subsequent Frame languages. One of the most widely used successors to KL-ONE was the [[LOOM (ontology)|Loom language]] developed by Robert MacGregor at the [[Information Sciences Institute]].&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41&#8211;46}}&lt;/ref&gt;

In the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge-based systems. These early products were usually developed in Lisp and integrated constructs such as IF-THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge-base tools was the [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in [[Lisp (programming language)|Lisp]] on [[Lisp machine]] platforms but was eventually ported to PCs and Unix workstations.&lt;ref&gt;{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}&lt;/ref&gt;

The research agenda of the [[Semantic Web]] spawned a renewed interest in automatic classification and frame languages.  An example is the [[Web Ontology Language]] (OWL) standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications (e.g. Google) do today the web can be organized by concepts organized in an ontology.

The name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for "OWL" using the Internet today most of the pages retrieved would be on the bird [[Owl]] rather than the standard [[Web Ontology Language|OWL]]. With a Semantic Web it would be possible to specify the concept "Web Ontology Language" and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example.

In addition to OWL various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include [[Ontology Inference Layer|OIL]] and [[DARPA Agent Markup Language|DAML]].  The [[Prot&#233;g&#233; (software)|Protege]] Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier.&lt;ref&gt;{{cite web|last=Horridge|first=Mathew|title=Prot&#233;g&#233; OWL Tutorial A step-by-step guide to modelling in OWL using the popular Prot&#233;g&#233; OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}&lt;/ref&gt;

==Comparison of frames and objects==
Frame languages have a significant overlap with [[object-oriented]] languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types.

The following table illustrates the correlation between standard terminology from the object-oriented and frame language communities:

{| class="wikitable"
|-
! Frame Terminology !! OO Terminology
|-
| Frame || Object Class
|-
| Slot || Object property or attribute
|-
| Trigger || Accessor and Mutator methods
|-
| Method (e.g. Loom, KEE) || Method
|}

The primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object-oriented paradigm encapsulation was one of the if not the most critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object-oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object-oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information (called "facets" in some languages) again with the same type of constraint information.

The other main differeniator between frame and OO languages was multiple inheritance (allowing a frame or class to have two or more superclasses). For frame languages multiple inheritance was a requirement.  This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non-overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity.&lt;ref&gt;{{cite web|title=The Unified Modeling Language|url=http://www.essentialstrategies.com/publications/modeling/uml.htm|work=essentialstrategies.com|publisher=Essential Strategies Inc.|accessdate=10 December 2013|year=1999|quote=In your author&#8217;s experience, nearly all examples that appear to require multiple inheritance or multiple type hierarchies can be solved by attacking the model from a different direction.}}&lt;/ref&gt;

Although the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages (e.g. Loom, KEE) included the ability to define messages on Frames.&lt;ref&gt;{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}&lt;/ref&gt;

On the object-oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the [[Object Management Group]] has standardized specifications for capabilities such as associating test data and constraints with objects (analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom) and for integrating rule engines.&lt;ref&gt;{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=OMG Formal Specifications|url=http://www.omg.org/spec/|work=omg.org|publisher=Object Management Group|accessdate=10 December 2013}}&lt;/ref&gt;

==See also==
*[[Description logic]]
* [[Deductive classifier]]
*[[First-order logic]]
*[[Knowledge base]]
*[[Knowledge-based system]]
*[[Ontology language]]
*[[Semantic Networks]]

==References==
{{reflist}}

==Additional References==
* Marvin Minsky, [http://web.media.mit.edu/~minsky/papers/Frames/frames.html A Framework for Representing Knowledge], MIT-AI Laboratory Memo 306, June, 1974.
* Daniel G. Bobrow, Terry Winograd, [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/76/581/CS-TR-76-581.pdf An Overview of KRL, A Knowledge Representation Language],  Stanford Artificial Intelligence Laboratory Memo AIM 293, 1976.
* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-408.pdf The FRL Primer], 1977
* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-409.pdf The FRL Manual], 1977
* {{cite journal | last1 = Brachman | first1 = R. | last2 = Schmolze | first2 = J. | year = 1985 | title = An overview of the KL-ONE Knowledge Representation System | url = | journal = Cognitive science | volume = 9 | issue = | pages = 171&#8211;216 | doi=10.1016/s0364-0213(85)80014-8}}
* {{cite journal | last1 = Fikes | first1 = R. E. | last2 = Kehler | first2 = T. | year = 1985 | title = The role of frame-based representation in knowledge representation and reasoning | url = | journal = Communications of the ACM | volume = 28 | issue = 9| pages = 904&#8211;920 | doi=10.1145/4284.4285}}
* Peter Clark &amp; Bruce Porter:  KM - The Knowledge Machine 2.0: Users Manual,  http://www.cs.utexas.edu/users/mfkb/RKF/km.html.
* Peter D. Karp, [http://www.ai.sri.com/pub_list/236 The Design Space of Frame Knowledge Representation Systems], Technical Note 520. [[Artificial Intelligence Center]], [[SRI International]], 1992

==External links==
*[http://www.cs.umbc.edu/771/papers/nebel.html Frame-Based Systems]
*[http://www.ai.sri.com/~gfp/spec/paper/paper.html The Generic Frame Protocol]
*[http://protege.stanford.edu/ The Prot&#233;g&#233; Ontology Editor]
*[http://www.csee.umbc.edu/courses/771/current/presentations/frames.pdf Intro Presentation to Frame Languages]

[[Category:Artificial intelligence]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]</text>
      <sha1>bveu1rj08lz4c80puhgskjemzpcumhe</sha1>
    </revision>
  </page>
  <page>
    <title>Attempto Controlled English</title>
    <ns>0</ns>
    <id>6520028</id>
    <revision>
      <id>759248865</id>
      <parentid>756632136</parentid>
      <timestamp>2017-01-10T02:13:09Z</timestamp>
      <contributor>
        <username>Peterl</username>
        <id>266404</id>
      </contributor>
      <comment>/* top */ Updated version, added ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17071" xml:space="preserve">{{Refimprove|date=April 2016}}
'''Attempto Controlled English''' ('''ACE''') is a [[controlled natural language]], i.e. a subset of standard [[English grammar|English]] with a restricted syntax and restricted semantics described by a small set of construction and interpretation rules.&lt;ref&gt;{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Gerold Schneider | title = Attempto Controlled English Meets the Challenges of Knowledge Representation, Reasoning, Interoperability and User Interfaces | booktitle = FLAIRS 2006 | date = 2006 | url = http://attempto.ifi.uzh.ch/site/publications/papers/FLAIRS0601FuchsN.pdf | format = [[PDF]]}}&lt;/ref&gt; It has been under development at the [[University of Zurich]] since 1995. In 2013, ACE version 6.7 was announced.&lt;ref&gt;http://attempto.ifi.uzh.ch/site/news/&lt;/ref&gt;

ACE can serve as [[knowledge representation]], [[specification language|specification]], and [[query language]], and is intended for professionals who want to use formal notations and formal methods, but may not be familiar with them. Though ACE appears perfectly natural &#8211; it can be read and understood by any speaker of English &#8211; it is in fact a [[formal language]].

ACE and its related tools have been used in the fields of [[requirements analysis|software specifications]], [[theorem proving]], [[automatic summarization|text summaries]], [[ontologies]], rules, querying, [[health informatics|medical documentation]] and [[planning]].

Here are some simple examples:

# Every woman is a human.
# A woman is a human.
# A man tries-on a new tie. If the tie pleases his wife then the man buys it.

ACE construction rules require that each noun be introduced by a determiner (''a'', ''every'', ''no'', ''some'', ''at least 5'', ...). ACE interpretation rules decide that (1) is interpreted as [[Universal quantification|universally quantified]], while (2) is interpreted as [[Existential quantification|existentially quantified]]. Sentences like "Women are human" do not follow ACE syntax and are consequently not valid.

Interpretation rules resolve the [[Deixis#Anaphoric reference|anaphoric references]] in (3): ''the tie'' and ''it'' of the second sentence refer to ''a new tie'' of the first sentence, while ''his'' and ''the man'' of the second sentence refer to ''a man'' of the first sentence. Thus an ACE text is a coherent entity of anaphorically linked sentences.

The Attempto Parsing Engine (APE) translates ACE texts unambiguously into [[Discourse Representation Theory|discourse representation structures]] (DRS) that use a variant of the language of [[first-order logic]].&lt;ref&gt;{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Tobias Kuhn | title = Discourse Representation Structures for ACE 6.6 | booktitle = Technical Report ifi-2010.0010, Department of Informatics, University of Zurich | date = 2010 | url = http://attempto.ifi.uzh.ch/site/pubs/papers/drs_report_66.pdf| format = [[PDF]] }}&lt;/ref&gt; A DRS can be further translated into other [[formal languages]], for instance AceRules with various semantics,&lt;ref&gt;{{cite conference | author = Tobias Kuhn | title = AceRules: Executing Rules in Controlled Natural Language | booktitle = First International Conference on Web Reasoning and Rule Systems (RR 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/kuhn07acerules.pdf| format = [[PDF]]}}&lt;/ref&gt;   [[Web Ontology Language|OWL]],&lt;ref&gt;{{cite conference |author1=Kaarel Kaljurand |author2=Norbert E. Fuchs | title = Verbalizing OWL in Attempto Controlled English | booktitle = OWL: Experiences and Directions (OWLED 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/owled2007_kaljurand.pdf | format = [[PDF]]}}&lt;/ref&gt; and [[Semantic Web Rule Language|SWRL]]. Translating an ACE text into (a fragment of) first-order logic allows users to  [[inference|reason]] about the text, for instance to [[formal verification|verify]], to [[formal verification|validate]], and to [[Information retrieval|query]] it.

== ACE in a nutshell ==
{{unreferenced section|date=May 2013}}
As an overview of the current version 6.6 of ACE this section:

* Briefly describes the vocabulary
* Gives an account of the syntax
* Summarises the handling of ambiguity
* Explains the processing of anaphoric references.

=== Vocabulary ===

The vocabulary of ACE comprises:

* Predefined function words (e.g. determiners, conjunctions)
* Predefined phrases (e.g. "it is false that ...", "it is possible that ...")
* Content words (e.g. nouns, verbs, adjectives, adverbs).

=== Grammar ===

The grammar of ACE defines and constrains the form and the meaning of ACE sentences and texts. ACE's grammar is expressed as a set of  [http://attempto.ifi.uzh.ch/site/docs/ace_constructionrules.html  construction rules]. The meaning of sentences is described as a small set of [http://attempto.ifi.uzh.ch/site/docs/ace_interpretationrules.html  interpretation rules]. A [http://attempto.ifi.uzh.ch/site/docs/ace_troubleshooting.html Troubleshooting Guide] describes how to use ACE and how to avoid pitfalls.

==== ACE texts ====

An ACE text is a sequence of declarative sentences that can be anaphorically interrelated. Furthermore, ACE supports questions and commands.

==== Simple sentences ====

A simple sentence asserts that something is the case &#8212; a fact, an event, a state.

:The temperature is -2 &#176;C.
:A customer inserts 2 cards. 
:A card and a code are valid.

Simple ACE sentences have the following general structure:

:subject + verb + complements + adjuncts

Every sentence has a subject and a verb. Complements (direct and indirect objects) are necessary for transitive verbs (''insert something'') and ditransitive verbs (''give something to somebody''), whereas adjuncts (adverbs, prepositional phrases) are optional.

All elements of a simple sentence can be elaborated upon to describe the situation in more detail. To further specify the nouns ''customer'' and ''card'', we could add adjectives:

:A trusted customer inserts two valid cards.

possessive nouns and ''of''-prepositional phrases:

:John's customer inserts a card of Mary.

or variables as appositions:

:John inserts a card A.

Other modifications of nouns are possible through relative sentences:

:A customer who is trusted inserts a card that he owns.

which are described below since they make a sentence composite. We can also detail the insertion event, e.g. by adding an adverb:

:A customer inserts some cards manually.

or, equivalently:

:A customer manually inserts some cards.

or, by adding prepositional phrases:

:A customer inserts some cards into a slot.

We can combine all of these elaborations to arrive at:

:John's customer who is trusted inserts a valid card of Mary manually into a slot A.

==== Composite sentences ====

Composite sentences are recursively built from simpler sentences through [[coordination (linguistics)|coordination]], [[subordination (linguistics)|subordination]], [[Quantification (linguistics)|quantification]], and [[negation]]. Note that ACE composite sentences overlap with what linguists call compound sentences and complex sentences.

===== Coordination =====

Coordination by ''and'' is possible between sentences and between phrases of the same syntactic type.

:A customer inserts a card and the machine checks the code.
:There is a customer who inserts a card and who enters a code.
:A customer inserts a card and enters a code.
:An old and trusted customer enters a card and a code.

Note that the coordination of the noun phrases ''a card'' and ''a code'' represents a plural object.

Coordination by ''or'' is possible between sentences, verb phrases, and relative clauses.

:A customer inserts a card or the machine checks the code.
:A customer inserts a card or enters a code.
:A customer owns a card that is invalid or that is damaged.

Coordination by ''and'' and ''or'' is governed by the standard binding order of logic, i.e. ''and'' binds stronger than ''or''. Commas can be used to override the standard binding order. Thus the sentence:

:A customer inserts a VisaCard or inserts a MasterCard, and inserts a code.

means that the customer inserts a VisaCard and a code, or alternatively a MasterCard and a code.

===== Subordination =====

There are four constructs of subordination: relative sentences, ''if-then'' sentences, modality, and sentence subordination.

Relative sentences starting with ''who'', ''which'', and ''that'' allow to add detail to nouns:

:A customer who is trusted inserts a card that he owns.

With the help of ''if-then'' sentences we can specify conditional or hypothetical situations:

:If a card is valid then a customer inserts it.

Note the anaphoric reference via the pronoun ''it'' in the ''then''-part to the noun phrase ''a card'' in the ''if''-part.

Modality allows us to express possibility and necessity:

:A trusted customer can/must insert a card.
:It is possible/necessary that a trusted customer inserts a card.

Sentence subordination comes in various forms:

:It is true/false that a customer inserts a card.
:It is not provable that a customer inserts a card.
:A clerk believes that a customer inserts a card.

===== Quantification =====

Quantification allows us to speak about all objects of a certain class ([[universal quantification]]), or to denote explicitly the existence of at least one object of this class ([[existential quantification]]). The textual occurrence of a universal or existential quantifier opens its scope that extends to the end of the sentence, or in coordinations to the end of the respective coordinated sentence.

To express that all involved customers insert cards we can write

:Every customer inserts a card.

This sentence means that each customer inserts a card that may, or may not, be the same as the one inserted by another customer. To specify that all customers insert the same card &#8212; however unrealistic that situation seems &#8212; we can write:

:A card is inserted by every customer.

or, equivalently:

:There is a card that every customer inserts.

To state that every card is inserted by a customer we write:

:Every card is inserted by a customer.

or, somewhat indirectly:

:For every card there is a customer who inserts it.

===== Negation =====

Negation allows us to express that something is not the case:

:A customer does not insert a card.
:A card is not valid.

To negate something for all objects of a certain class one uses ''no'':

:No customer inserts more than 2 cards.

or, ''there is no'':

:There is no customer who inserts a card.

To negate a complete statement one uses sentence negation:

:It is false that a customer inserts a card.

These forms of negation are logical negations, i.e. they state that something is provably not the case. Negation as failure states that a state of affairs cannot be proved, i.e. there is no information whether the state of affairs is the case or not.

:It is not provable that a customer inserts a card.

==== Queries ====

ACE supports two forms of queries: ''yes/no''-queries and ''wh''-queries.

''Yes/no''-queries ask for the existence or non-existence of a specified situation. If we specified:

:A customer inserts a card.

then we can ask:

:Does a customer insert a card?

to get a positive answer. Note that interrogative sentences always end with a question mark.

With the help of ''wh''-queries, i.e. queries with query words, we can interrogate a text for details of the specified situation. If we specified:

:A trusted customer inserts a valid card manually in the morning in a bank.

we can ask for each element of the sentence with the exception of the verb.

:Who inserts a card?
:Which customer inserts a card?
:What does a customer insert?
:How does a customer insert a card?
:When does a customer enter a card?
:Where does a customer enter a card?

Queries can also be constructed by a sequence of declarative sentences followed by one interrogative sentence, for example:

:There is a customer and there is a card that the customer enters. Does a customer enter a card?

==== Commands ====

ACE also supports commands. Some examples:

:John, go to the bank!
:John and Mary, wait!
:Every dog, bark!
:A brother of John, give a book to Mary!

A command always consists of a noun phrase (the addressee), followed by a comma, followed by an uncoordinated verb phrase. Furthermore, a command has to end with an exclamation mark.

=== Constraining ambiguity ===

To constrain the ambiguity of full natural language ACE employs three simple means:

* Some ambiguous constructs are not part of the language; unambiguous alternatives are available in their place
* All remaining ambiguous constructs are interpreted deterministically on the basis of a small number of interpretation rules
* Users can either accept the assigned interpretation, or they must rephrase the input to obtain another one.

==== Avoidance of ambiguity ====

In natural language, relative sentences combined with coordinations can introduce ambiguity:

:A customer inserts a card that is valid and opens an account.

In ACE the sentence has the unequivocal meaning that the customer opens an account, as reflected by the paraphrase:

:A card is valid. A customer inserts the card. The customer opens an account.

To express the alternative &#8212; though not very realistic &#8212; meaning that the card opens an account, the relative pronoun ''that'' must be repeated, thus yielding a coordination of relative sentences:

:A customer inserts a card that is valid and that opens an account.

This sentence is unambiguously equivalent in meaning to the paraphrase:

:A card is valid. The card opens an account. A customer inserts the card.

==== Interpretation rules ====

Not all ambiguities can be safely removed from ACE without rendering it artificial. To deterministically interpret otherwise syntactically correct ACE sentences we use a small set of interpretation rules. For example, if we write:

:A customer inserts a card with a code.

then ''with a code'' attaches to the verb ''inserts'', but not to ''a card''. However, this is probably not what we meant to say. To express that ''the code'' is associated with ''the card'' we can employ the interpretation rule that a relative sentence always modifies the immediately preceding noun phrase, and rephrase the input as:

:A customer inserts a card that carries a code.

yielding the paraphrase:

:A card carries a code. A customer inserts the card.

or &#8212; to specify that the customer inserts a card and a code &#8212; as:

:A customer inserts a card and a code.

=== Anaphoric references ===

Usually ACE texts consist of more than one sentence:

:A customer enters a card and a code. If a code is valid then SimpleMat accepts a card.

To express that all occurrences of card and code should mean the same card and the same code, ACE provides anaphoric references via the definite article:

:A customer enters a card and a code. If the code is valid then SimpleMat accepts the card.

During the processing of the ACE text, all anaphoric references are replaced by the most recent and most specific accessible noun phrase that agrees in gender and number. As an example of "most recent and most specific", suppose an ACE parser is given the sentence:

:A customer enters a red card and a blue card.

Then:

:The card is correct.

refers to the second card, while:

:The red card is correct.

refers to the first card.

Noun phrases within ''if-then'' sentences, universally quantified sentences, negations, modality, and subordinated sentences cannot be referred to anaphorically from subsequent sentences, i.e. such noun phrases are not "accessible" from the following text. Thus for each of the sentences:

:If a customer owns a card, then they enter it.
:Every customer enters a card.
:A customer does not enter a card.
:A customer can enter a card.
:A clerk believes that a customer enters a card.

we cannot refer to ''a card'' with:

:The card is correct.

Anaphoric references are also possible via personal pronouns:

:A customer enters a card and a code. If it is valid then SimpleMat accepts the card.

or via variables:

:A customer enters a card X and a code Y. If Y is valid then SimpleMat accepts X.

Anaphoric references via definite articles and variables can be combined:

:A customer enters a card X and a code Y. If the code Y is valid then SimpleMat accepts the card X.

Note that proper names like ''SimpleMat'' always refer to the same object.

==See also==
*[[Gellish]]
*[[Natural Language Processing]]
*[[Knowledge Representation]]
*[[Natural language programming]]
*[[Structured English]]
**[[ClearTalk]], another machine-readable knowledge representation language
**[[Inform 7]], a programming language with English syntax

==References==
{{Reflist}}

==External links==
*[http://attempto.ifi.uzh.ch Project Attempto]

[[Category:Controlled English]]
[[Category:Knowledge representation]]
[[Category:Controlled natural languages]]
[[Category:Natural language processing]]
[[Category:Natural language parsing]]</text>
      <sha1>465gj40opcw6alzbradbry3wb03xf12</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Multi-agent systems</title>
    <ns>14</ns>
    <id>8050180</id>
    <revision>
      <id>547920225</id>
      <parentid>432040950</parentid>
      <timestamp>2013-03-31T02:46:51Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8645694]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="424" xml:space="preserve">This [[Wikipedia:category|category]] is about [[multi-agent system]]s, systems composed of several [[software agent]]s. 


[[Category:Computing platforms]]
[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Distributed computing architecture]]

== See also ==
*[[Cognitive architecture]]
*[[Intelligent agent]]
*[[Autonomous agent]]
*[[Internet bot]]
*[[Daemon (computer software)|Daemon]]</text>
      <sha1>hh9nxow18wd3uubje4ja7v5hzzgrre0</sha1>
    </revision>
  </page>
  <page>
    <title>Minimum Information Standards</title>
    <ns>0</ns>
    <id>7819348</id>
    <revision>
      <id>757657045</id>
      <parentid>721913703</parentid>
      <timestamp>2016-12-31T23:13:04Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor />
      <comment>/* top */ cite repair;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13369" xml:space="preserve">{{Multiple issues|
{{confusing|date=January 2010}}
{{essay-like|date=January 2010}}
{{lead rewrite|date=January 2010}}
{{external links|date=September 2012}}
{{more footnotes|date=January 2010}}
{{expert-subject|Computational Biology|date=January 2010}}
}}

The '''minimum information standard''' is a set of guidelines for [[data reporting|reporting]]  [[data]] derived by relevant methods in biosciences. If followed, it ensures that the data can be easily verified, analysed and clearly interpreted by the wider scientific community. Keeping with these recommendations also facilitates the foundation of structuralized databases, public repositories and development of data analysis tools.&lt;ref name="MIFlowCyt: The minimum information about a flow cytometry experiment"&gt;{{cite journal|last=Lee|first=Jamie A. |author2=Spidlen, Josef |author3=Boyce, Keith |author4=Cai, Jennifer |author5=Crosbie, Nicholas |author6=Dalphin, Mark |author7=Furlong, Jeff |author8=Gasparetto, Maura |author9=Goldberg, Michael |author10=Goralczyk, Elizabeth M. |author11=Hyun, Bill |author12=Jansen, Kirstin |author13=Kollmann, Tobias |author14=Kong, Megan |author15=Leif, Robert |author16=McWeeney, Shannon |author17=Moloshok, Thomas D. |author18=Moore, Wayne |author19=Nolan, Garry |author20=Nolan, John |author21=Nikolich-Zugich, Janko |author22=Parrish, David |author23=Purcell, Barclay |author24=Qian, Yu |author25=Selvaraj, Biruntha |author26=Smith, Clayton |author27=Tchuvatkina, Olga |author28=Wertheimer, Anne |author29=Wilkinson, Peter |author30=Wilson, Christopher |author31=Wood, James |author32=Zigon, Robert |author33=Scheuermann, Richard H. |author34=Brinkman, Ryan R. |title=MIFlowCyt: The minimum information about a flow cytometry experiment|journal=Cytometry Part A|date=1 October 2008|volume=73A|issue=10|pages=926&#8211;930|doi=10.1002/cyto.a.20623|pmid=18752282|pmc=2773297}}&lt;/ref&gt;&lt;ref name="Minimum information about a microarray experiment (MIAME)&#8212;toward standards for microarray data"&gt;{{cite journal|last=Brazma|first=Alvis |author2=Hingamp, Pascal |author3=Quackenbush, John |author4=Sherlock, Gavin |author5=Spellman, Paul |author6=Stoeckert, Chris |author7=Aach, John |author8=Ansorge, Wilhelm |author9=Ball, Catherine A. |author10=Causton, Helen C. |author11=Gaasterland, Terry |author12=Glenisson, Patrick |author13=Holstege, Frank C.P. |author14=Kim, Irene F. |author15=Markowitz, Victor |author16=Matese, John C. |author17=Parkinson, Helen |author18=Robinson, Alan |author19=Sarkans, Ugis |author20=Schulze-Kremer, Steffen |author21=Stewart, Jason |author22=Taylor, Ronald |author23=Vilo, Jaak |author24=Vingron, Martin |title=Minimum information about a microarray experiment (MIAME)&#8212;toward standards for microarray data|journal=Nature Genetics|date=30 November 2001|volume=29|issue=4|pages=365&#8211;371|doi=10.1038/ng1201-365|pmid=11726920}}&lt;/ref&gt;

The individual '''minimum information standards''' are brought by the communities of cross-disciplinary specialists focused on the problematic of the specific method used in experimental biology.  The standards then provide specifications what information about the experiments ([[metadata]]) is crucial and important to be reported together with the resultant data to make it comprehensive.&lt;ref name="MIFlowCyt: The minimum information about a flow cytometry experiment"/&gt;&lt;ref name="Minimum information about a microarray experiment (MIAME)&#8212;toward standards for microarray data"/&gt; The need for this standardization is largely driven by the development of high-throughput experimental methods that provide tremendous amounts of data.  The development of minimum information standards of different methods is since 2008 being harmonized by "Minimum Information about a Biomedical or Biological Investigation" (MIBBI) project.&lt;ref name="Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project"&gt;{{cite journal|last=Taylor|first=Chris F|title=Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project|journal=Nature Biotechnology|year=2008|volume=26|pages=889&#8211;896|doi=10.1038/nbt.1411}}&lt;/ref&gt;

==MI Standards==

===MIAME, gene expression microarray===
Minimum Information About a Microarray Experiment (MIAME) describes the Minimum Information About a Microarray Experiment that is needed to enable the interpretation of the results of the experiment unambiguously and potentially to reproduce the experiment and is aimed at facilitating the dissemination of data from microarray experiments.

MIAME contains a number of extensions to cover specific biological domains, including MIAME-env, MIAME-nut and MIAME-tox, covering environmental genomics, nutritional genomics and toxogenomics, respectively

===MINI: Minimum Information about a Neuroscience Investigation===

====MINI: Electrophysiology====
[[Electrophysiology]] is a technology used to study the electrical properties of biological cells and tissues. Electrophysiology typically involves the measurements of voltage change or electric current flow on a wide variety of scales from single ion channel
proteins to whole tissues. This document is a single module, as part of the Minimum Information about a Neuroscience investigation (MINI) family of reporting guideline
documents, produced by community consultation and continually available for public comment. A MINI module represents the minimum information that should be reported about a dataset to facilitate computational access and analysis to allow a reader to interpret and critically evaluate the processes performed and the conclusions reached, and to support their experimental corroboration. In practice a MINI module comprises a checklist of information that should be provided (for example about the protocols employed) when
a data set is described for publication. The full specification of the MINI module can be found here.&lt;ref&gt;Gibson, Frank, Overton, Paul, Smulders, Tom, Schultz, Simon, Eglen, Stephen, Ingram, Colin, Panzeri, Stefano, Bream, Phil, Sernagor, Evelyne, Cunningham, Mark, Adams, Christopher, Echtermeyer, Christoph, Simonotto, Jennifer, Kaiser, Marcus, Swan, Daniel, Fletcher, Marty, and Lord, Phillip. Minimum Information about a Neuroscience Investigation (MINI) Electrophysiology. Available from Nature Precedings &lt;http://hdl.handle.net/10101/npre.2008.1720.1&gt; (2008)&lt;/ref&gt;

===MIARE, RNAi experiment===
Minimum Information About an RNAi Experiment (MIARE) is a [[data reporting]] guideline which describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results.

===MIACA, cell based assay===
Advances in genomics and functional genomics have enabled large-scale analyses of gene and protein function by means of high-throughput cell biological analyses. Thereby, cells in culture can be perturbed in vitro and the induced effects recorded and analyzed. Perturbations can be triggered in several ways, for instance with molecules (siRNAs, expression constructs, small chemical compounds, ligands for receptors, etc.), through environmental stresses (such as temperature shift, serum starvation, oxygen deprivation, etc.), or combinations thereof. The cellular responses to such perturbations are analyzed in order to identify molecular events in the biological processes addressed and understand biological principles.
We propose the Minimum Information About a Cellular Assay (MIACA) for reporting a cellular assay, and CA-OM, the modular cellular assay object model, to facilitate exchange of data and accompanying information, and to compare and integrate data that originate from different, albeit complementary approaches, and to elucidate higher order principles. [http://sourceforge.net/project/showfiles.php?group_id=158121 Documents describing MIACA] are available and provide further information as well as the checklist of terms that should be reported.

===MIAPE, proteomic experiments===
The Minimum Information About a Proteomic Experiment documents describe information which should be given along with a proteomic experiment. The parent document describes the processes and principles underpinning the development of a series of domain specific documents which now cover all aspects of a MS-based proteomics workflow.
{{Details|Minimum Information About a Proteomics Experiment }}

===MIMIx, molecular interactions===
This document has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info) and describes the Minimum Information about a Molecular Interaction experiment.

===MIAPAR, protein affinity reagents===
The Minimum Information About a Protein Affinity Reagent has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info)in conjunction with the HUPO Antibody Initiative and a European consortium of binder producers and seeks to encourage users to improve their description of binding reagents, such as antibodies, used in the process of protein identification.

===MIABE, bioactive entities===
The Minimum Information About a Bioactive Entity was produced by representatives from both large pharma and academia who are looking to improve the description of usually small molecules which bind to, and potentially modulate the activity of, specific targets in a living organism. This document encompasses drug-like molecules as well as hebicides, pesticides and food additives. It is primarily maintained through the EMBL-EBI Industry program (www.ebi.ac.uk/industry).

===MIGS/MIMS, genome/metagenome sequences===
This specification is being developed by the [[Genomic Standards Consortium]]

===MIFlowCyt, flow cytometry===

====Minimum Information about a Flow Cytometry Experiment====
The fundamental tenet of any scientific research is that the published results of any study have to be open to independent validation or refutation. The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes the criteria to record information about the experimental overview, samples, instrumentation and data analysis. It promotes consistent annotation of clinical, biological and technical issues surrounding a flow cytometry experiment by specifying the requirements for data content and by providing a structured framework for capturing information.

More information can be found at:
* The Flow Informatics and Computational Cytometry Socienty (FICCS) [http://wiki.ficcs.org/ficcs/MIFlowCyt MIFlowCyt wiki] page.
* The Bioinformatics Standards for Flow Cytometry [http://flowcyt.sourceforge.net/miflowcyt/ MIFlowCyt web] page.

===MISFISHIE, In Situ Hybridization and Immunohistochemistry Experiments===
{{Emptysection|date=February 2013}}

===MIAPA, Phylogenetic Analysis===
Criteria for Minimum Information About a Phylogenetic Analysis were described in 2006. &lt;ref&gt; {{Cite journal | doi = 10.1089/omi.2006.10.231| title = Taking the First Steps towards a Standard for Reporting on Phylogenies: Minimum Information about a Phylogenetic Analysis (MIAPA)| journal = OMICS: A Journal of Integrative Biology| volume = 10| issue = 2| pages = 231| year = 2006| last1 = Leebens-Mack | first1 = J. | last2 = Vision | first2 = T. | last3 = Brenner | first3 = E. | last4 = Bowers | first4 = J. E. | last5 = Cannon | first5 = S. | last6 = Clement | first6 = M. J. | last7 = Cunningham | first7 = C. W. | last8 = Depamphilis | first8 = C. | last9 = Desalle | first9 = R. | last10 = Doyle | first10 = J. J. | last11 = Eisen | first11 = J. A. | last12 = Gu | first12 = X. | last13 = Harshman | first13 = J. | last14 = Jansen | first14 = R. K. | last15 = Kellogg | first15 = E. A. | last16 = Koonin | first16 = E. V. | last17 = Mishler | first17 = B. D. | last18 = Philippe | first18 = H. | last19 = Pires | first19 = J. C. | last20 = Qiu | first20 = Y. L. | last21 = Rhee | first21 = S. Y. | last22 = Sj&#246;lander | first22 = K. | last23 = Soltis | first23 = D. E. | last24 = Soltis | first24 = P. S. | authorlink24 = Pamela S. Soltis| last25 = Stevenson | first25 = D. W. | last26 = Wall | first26 = K. | last27 = Warnow | first27 = T. | last28 = Zmasek | first28 = C. }} &lt;/ref&gt;

===MIAO, ORF===
{{Emptysection|date=February 2013}}

===MIAMET, METabolomics experiment===
{{Emptysection|date=February 2013}}

===MIAFGE, Functional Genomics Experiment===
{{Emptysection|date=February 2013}}

===MIRIAM, Minimum Information Required in the Annotation of Models===
The Minimal Information Required In the Annotation of Models ([[MIRIAM]]), is a set of rules for the curation and annotation of quantitative models of biological systems.

===MIASE, Minimum Information About a Simulation Experiment===
The Minimum Information About a Simulation Experiment ([[MIASE]]) is an effort to standardize the description of simulation experiments in the field of systems biology.

===CIMR, Core Information for Metabolomics Reporting===

==External links==
* [http://mibbi.sourceforge.net/ MIBBI (Minimum Information for Biological and Biomedical Investigations)] A &#8216;one-stop shop&#8217; for exploring the range of extant projects, foster collaborative development and ultimately promote gradual integration.
* [http://www.biosharing.org BioSharing catalogue]

==References==
{{reflist}}

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]</text>
      <sha1>k8295hbjq6zvbdml4m81o3j9itnexqv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Microformats</title>
    <ns>14</ns>
    <id>10220551</id>
    <revision>
      <id>567179058</id>
      <parentid>548256812</parentid>
      <timestamp>2013-08-04T23:52:55Z</timestamp>
      <contributor>
        <username>Pasqui23</username>
        <id>9191315</id>
      </contributor>
      <comment>category on commons was deleted</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="237" xml:space="preserve">{{Cat main|Microformat}}



[[Category:Knowledge representation]]
[[Category:Metadata publishing]]
[[Category:Metadata]]
[[Category:Semantic HTML]]
[[Category:Semantic Web]]
[[Category:Domain-specific knowledge representation languages]]</text>
      <sha1>smys7h0335u6lk849ybghqyo7gr0bsl</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Argument mapping</title>
    <ns>14</ns>
    <id>11461295</id>
    <revision>
      <id>441884240</id>
      <parentid>133999634</parentid>
      <timestamp>2011-07-28T15:20:26Z</timestamp>
      <contributor>
        <username>Jm34harvey</username>
        <id>14284</id>
      </contributor>
      <comment>main article is Argument map</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="96" xml:space="preserve">{{Cat main|Argument map}}

[[Category:Knowledge representation]]
[[Category:Informal arguments]]</text>
      <sha1>keowc1rh3m0z24twk8tauwfgg8upx03</sha1>
    </revision>
  </page>
  <page>
    <title>Qualification problem</title>
    <ns>0</ns>
    <id>731287</id>
    <revision>
      <id>690772395</id>
      <parentid>614232524</parentid>
      <timestamp>2015-11-15T16:20:14Z</timestamp>
      <contributor>
        <username>RW Dutton</username>
        <id>7102439</id>
      </contributor>
      <comment>/* External links */ Add to Epistemology category (and move to Epistemology stubs) to match Ratification problem page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1400" xml:space="preserve">{{one source|date=July 2011}}
In [[philosophy]] and [[Artificial intelligence|AI]] (especially, knowledge based systems), the '''qualification problem''' is concerned with the impossibility of listing ''all'' the [[precondition]]s required for a real-world action to have its intended effect. It might be posed as ''how to deal with the things that prevent me from achieving my intended result''. It is strongly connected to, and opposite the [[ramification problem|ramification side]] of, the [[frame problem]]. John McCarthy gives the following motivating example, in which it is impossible to enumerate all the circumstances that may prevent a rowboat from performing its ordinary function:

:"[T]he successful use of a boat to cross a river requires, if the boat is a rowboat, that the oars and rowlocks be present and unbroken, and that they fit each other. Many other qualifications can be added, making the rules for using a rowboat almost impossible to apply, and yet anyone will still be able to think of additional requirements not yet stated."

==See also==
*[[Non-monotonic logic]]
*[[Circumscription (logic)|Circumscription]]

==External links==
* John McCarthy "[http://www-formal.stanford.edu/jmc/circumscription/node1.html Introduction: The Qualification Problem]" 

[[Category:Knowledge representation]]
[[Category:Logic programming]]
[[Category:Epistemology]]

{{epistemology-stub}}</text>
      <sha1>qepck1hjl6vn3r52v3h8hfaokc88qme</sha1>
    </revision>
  </page>
  <page>
    <title>Belief revision</title>
    <ns>0</ns>
    <id>1187311</id>
    <revision>
      <id>746894877</id>
      <parentid>745138546</parentid>
      <timestamp>2016-10-30T06:09:07Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.6)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="43581" xml:space="preserve">'''Belief revision''' is the process of changing beliefs to take into account a new piece of information. The [[formal logic|logical]] formalization of belief revision is researched in [[philosophy]], in [[databases]], and in artificial intelligence for the design of [[intelligent agent|rational agent]]s.

What makes belief revision non-trivial is that several different ways for performing this operation may be possible. For example, if the current knowledge includes the three facts "&lt;math&gt;A&lt;/math&gt; is true", "&lt;math&gt;B&lt;/math&gt; is true" and "if &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are true then &lt;math&gt;C&lt;/math&gt; is true", the introduction of the new information "&lt;math&gt;C&lt;/math&gt; is false" can be done preserving consistency only by removing at least one of the three facts. In this case, there are at least three different ways for performing revision. In general, there may be several different ways for changing knowledge.

==Revision and update==

Two kinds of changes are usually distinguished:

; update : the new information is about the situation at present, while the old beliefs refer to the past; update is the operation of changing the old beliefs to take into account the change;

; revision : both the old beliefs and the new information refer to the same situation; an inconsistency between the new and old information is explained by the possibility of old information being less reliable than the new one; revision is the process of inserting the new information into the set of old beliefs without generating an inconsistency.

The main assumption of belief revision is that of minimal change: the knowledge before and after the change should be as similar as possible. In the case of update, this principle formalizes the assumption of inertia. In the case of revision, this principle enforces as much information as possible to be preserved by the change.

===Example===

The following classical example shows that the operations to perform in the two settings of update and revision are not the same. The example is based on two different interpretations of the set of beliefs &lt;math&gt;\{a \vee b\}&lt;/math&gt; and the new piece of information &lt;math&gt;\neg a&lt;/math&gt;:

; update : in this scenario, two satellites, Unit A and Unit B, orbit around Mars; the satellites are programmed to land while transmitting their status to Earth; Earth has received a transmission from one of the satellites, communicating that it is still in orbit; however, due to interference, it is not known which satellite sent the signal; subsequently, Earth receives the communication that Unit A has landed; this scenario can be modeled in the following way; two [[propositional variable]]s &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; indicate that Unit A and Unit B, respectively, are still in orbit; the initial set of beliefs is &lt;math&gt;\{a \vee b\}&lt;/math&gt; (either one of the two satellites is still in orbit) and the new piece of information is &lt;math&gt;\neg a&lt;/math&gt; (Unit A has landed, and is therefore not in orbit); the only rational result of the update is &lt;math&gt;\neg a&lt;/math&gt;; since the initial information that one of the two satellites had not landed yet was possibly coming from the Unit A, the position of the Unit B is not known;

; revision : the play "Six Characters in Search of an Author" will be performed in one of the two local theatres; this information can be denoted by &lt;math&gt;\{a \vee b\}&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; indicates that the play will be performed at the first or at the second theatre, respectively; a further information that "Jesus Christ Superstar" will be performed at the first theatre indicates that &lt;math&gt;\neg a&lt;/math&gt; holds; in this case, the obvious conclusion is that "Six Characters in Search of an Author" will be performed at the second but not the first theatre, which is represented in logic by &lt;math&gt;\neg a \wedge b&lt;/math&gt;.

This example shows that revising the belief &lt;math&gt;a \vee b&lt;/math&gt; with the new information &lt;math&gt;\neg a&lt;/math&gt; produces two different results &lt;math&gt;\neg a &lt;/math&gt; and &lt;math&gt;\neg a \wedge b&lt;/math&gt; depending on whether the setting is that of update or revision.

==Contraction, expansion, revision, consolidation, and merging==

In the setting in which all beliefs refer to the same situation, a distinction between various operations that can be performed is made:

; contraction : removal of a belief;

; expansion : addition of a belief without checking consistency;

; revision : addition of a belief while maintaining consistency;

; consolidation : restoring consistency of a set of beliefs;

; merging : fusion of two or more sets of beliefs while maintaining consistency.

Revision and merging differ in that the first operation is done when the new belief to incorporate is considered more reliable than the old ones; therefore, consistency is maintained by removing some of the old beliefs. Merging is a more general operation, in that the priority among the belief sets may or may not be the same.

Revision can be performed by first incorporating the new fact and then restoring consistency via consolidation. This is actually a form of merging rather than revision, as the new information is not always treated as more reliable than the old knowledge.

==The AGM postulates==

The AGM postulates (named after the names of their proponents, [[Carlos Alchourr&#243;n|Alchourr&#243;n]], [[Peter G&#228;rdenfors|G&#228;rdenfors]], and [[David Makinson|Makinson]]) are properties that an operator that performs revision should satisfy in order for that operator to be considered rational. The considered setting is that of revision, that is, different pieces of information referring to the same situation. Three operations are considered: expansion (addition of a belief without a consistency check), revision (addition of a belief while maintaining consistency), and contraction (removal of a belief).

The first six postulates are called "the basic AGM postulates". In the settings considered by Alchourr&#243;n, G&#228;rdenfors, and Makinson, the current set of beliefs is represented by a [[Deductive closure|deductively closed]] set of logical formulae &lt;math&gt;K&lt;/math&gt; called belief base, the new piece of information is a logical formula &lt;math&gt;P&lt;/math&gt;, and revision is performed by a binary operator &lt;math&gt;*&lt;/math&gt; that takes as its operands the current beliefs and the new information and produces as a result a belief base representing the result of the revision. The &lt;math&gt;+&lt;/math&gt; operator denoted expansion: &lt;math&gt;K+P&lt;/math&gt; is the deductive closure of &lt;math&gt;K \cup \{P\}&lt;/math&gt;. The AGM postulates for revision are:

# Closure: &lt;math&gt;K*P&lt;/math&gt; is a belief base (i.e., a deductively closed set of formulae);
# Success: &lt;math&gt;P \in K*P&lt;/math&gt;
# Inclusion: &lt;math&gt;K*P \subseteq K+P&lt;/math&gt;
# Vacuity: &lt;math&gt;\text{If }(\neg P) \not \in K,\text{ then }K*P=K+P&lt;/math&gt;
# &lt;math&gt;K*P&lt;/math&gt; is [[inconsistent]] only if &lt;math&gt;P&lt;/math&gt; is inconsistent or &lt;math&gt;K&lt;/math&gt; is inconsistent
# Extensionality: &lt;math&gt;\text{If }P\text{ and }Q\text{ are logically equivalent, then }K*P=K*Q&lt;/math&gt; (see [[logical equivalence]])
# &lt;math&gt;K*(P \wedge Q) \subseteq (K*P)+Q&lt;/math&gt;
# &lt;math&gt;\text{If }(\neg Q) \not\in K*P\text{ then }(K*P)+Q \subseteq K*(P \wedge Q)&lt;/math&gt;

A revision operator that satisfies all eight postulates is the full meet revision, in which &lt;math&gt;K*P&lt;/math&gt; is equal to &lt;math&gt;K+P&lt;/math&gt; if consistent, and to the deductive closure of &lt;math&gt;P&lt;/math&gt; otherwise. While satisfying all AGM postulates, this revision operator has been considered to be too conservative, in that no information from the old knowledge base is maintained if the revising formula is inconsistent with it.{{Citation needed|date=November 2011}}

==Conditions equivalent to the AGM postulates==

The AGM postulates are equivalent to several different conditions on the revision operator; in particular, they are equivalent to the revision operator being definable in terms of structures known as selection functions, epistemic entrenchments, systems of spheres, and preference relations. The latter are [[reflexive relation|reflexive]], [[transitive relation|transitive]], and [[total relation]]s over the set of models.

Each revision operator &lt;math&gt;*&lt;/math&gt; satisfying the AGM postulates is associated to a set of preference relations &lt;math&gt;\leq_K&lt;/math&gt;, one for each possible belief base &lt;math&gt;K&lt;/math&gt;, such that the models of &lt;math&gt;K&lt;/math&gt; are exactly the minimal of all models according to &lt;math&gt;\leq_K&lt;/math&gt;. The revision operator and its associated family of orderings are related by the fact that &lt;math&gt;K*P&lt;/math&gt; is the set of formulae whose set of models contains all the minimal models of &lt;math&gt;P&lt;/math&gt; according to &lt;math&gt;\leq_K&lt;/math&gt;. This condition is equivalent to the set of models of &lt;math&gt;K*P&lt;/math&gt; being exactly the set of the minimal models of &lt;math&gt;P&lt;/math&gt; according to the ordering &lt;math&gt;\leq_K&lt;/math&gt;.

A preference ordering &lt;math&gt;\leq_K&lt;/math&gt; represents an order of implausibility among all situations, including those that are conceivable but yet currently considered false. The minimal models according to such an ordering are exactly the models of the knowledge base, which are the models that are currently considered the most likely. All other models are greater than these ones, and are indeed considered less plausible. In general, &lt;math&gt;I &lt;_K J&lt;/math&gt; indicates that the situation represented by the model &lt;math&gt;I&lt;/math&gt; is believed to be more plausible than the situation represented by &lt;math&gt;J&lt;/math&gt;. As a result, revising by a formula having &lt;math&gt;I&lt;/math&gt; and &lt;math&gt;J&lt;/math&gt; as models should select only &lt;math&gt;I&lt;/math&gt; to be a model of the revised knowledge base, as this model represent the most likely scenario among those supported by &lt;math&gt;P&lt;/math&gt;.

==Contraction==

Contraction is the operation of removing a belief &lt;math&gt;P&lt;/math&gt; from a knowledge base &lt;math&gt;K&lt;/math&gt;; the result of this operation is denoted by &lt;math&gt;K-P&lt;/math&gt;. The operators of revision and contractions are related by the Levi and Harper identities:

: &lt;math&gt;K*P=(K-\neg P)+P&lt;/math&gt;
: &lt;math&gt;K-P=K \cap (K*\neg P)&lt;/math&gt;

Eight postulates have been defined for contraction. Whenever a revision operator satisfies the eight postulates for revision, its corresponding contraction operator satisfies the eight postulates for contraction, and vice versa. If a contraction operator satisfies at least the first six postulates for contraction, translating it into a revision operator and then back into a contraction operator using the two identities above leads to the original contraction operator. The same holds starting from a revision operator.

One of the postulates for contraction has been longly discussed: the recovery postulate:

: &lt;math&gt;K=(K-P)+P&lt;/math&gt;

According to this postulate, the removal of a belief &lt;math&gt;P&lt;/math&gt; followed by the reintroduction of the same belief in the belief base should lead to the original belief base. There are some examples showing that such behavior is not always reasonable: in particular, the contraction by a general condition such as &lt;math&gt;a \vee b&lt;/math&gt; leads to the removal of more specific conditions such as &lt;math&gt;a&lt;/math&gt; from the belief base; it is then unclear why the reintroduction of &lt;math&gt;a \vee b&lt;/math&gt; should also lead to the reintroduction of the more specific condition &lt;math&gt;a&lt;/math&gt;. For example, if George was previously believed to have German citizenship, it was also believed to be European. Contracting this latter belief amounts to stop believing that George is European; therefore, that George has German citizenship is also retracted from the belief base. If George is later discovered to have Austrian citizenship, then the fact that he is European is also reintroduced. According to the recovery postulate, however, the belief that he also has German citizenship should also be reintroduced.

The correspondence between revision and contraction induced by the Levi and Harper identities is such that a contraction not satisfying the recovery postulate is translated into a revision satisfying all eight postulates, and that a revision satisfying all eight postulates is translated into a contraction satisfying all eight postulates, including recovery. As a result, if recovery is excluded from consideration, a number of contraction operators are translated into a single revision operator, which can be then translated back into exactly one contraction operator. This operator is the only one of the initial group of contraction operators that satisfies recovery; among this group, it is the operator that preserves as much information as possible.

==The Ramsey test==
&lt;!--[[Ramsey test]], [[Ramsey Test]], [[Ramsey's test]], [[Ramsey's Test]] redirect here.--&gt;

The evaluation of a [[counterfactual conditional]] &lt;math&gt;a &gt; b&lt;/math&gt; can be done, according to the '''Ramsey test''' (named for [[Frank P. Ramsey]]), to the hypothetical addition of &lt;math&gt;a&lt;/math&gt; to the set of current beliefs followed by a check for the truth of &lt;math&gt;b&lt;/math&gt;. If &lt;math&gt;K&lt;/math&gt; is the set of beliefs currently held, the Ramsey test is formalized by the following correspondence:

: &lt;math&gt;a &gt; b&lt;/math&gt; if and only if &lt;math&gt;b \in K * a&lt;/math&gt;

If the considered language of the formulae representing beliefs is propositional, the Ramsey test gives a consistent definition for counterfactual conditionals in terms of a belief revision operator. However, if the language of formulae representing beliefs itself includes the counterfactual conditional connective &lt;math&gt;&gt;&lt;/math&gt;, the Ramsey test leads to the Gardenfors triviality result: there is no non-trivial revision operator that satisfies both the AGM postulates for revision and the condition of the Ramsey test. This result holds in the assumption that counterfactual formulae like &lt;math&gt;a&gt;b&lt;/math&gt; can be present in belief bases and revising formulae. Several solutions to this problem have been proposed.

==Non-monotonic inference relation==

Given a fixed knowledge base &lt;math&gt;K&lt;/math&gt; and a revision operator &lt;math&gt;*&lt;/math&gt;, one can define a non-monotonic inference relation using the following definition: &lt;math&gt;P \vdash Q&lt;/math&gt; if and only if &lt;math&gt;K*P \models Q&lt;/math&gt;. In other words, a formula &lt;math&gt;P&lt;/math&gt; [[logical consequence|entails]] another formula &lt;math&gt;Q&lt;/math&gt; if the addition of the first formula to the current knowledge base leads to the derivation of &lt;math&gt;Q&lt;/math&gt;. This inference relation is non-monotonic.

The AGM postulates can be translated into a set of postulates for this inference relation. Each of these postulates is entailed by some previously considered set of postulates for non-monotonic inference relations. Vice versa, conditions that have been considered for non-monotonic inference relations can be translated into postulates for a revision operator. All these postulates are entailed by the AGM postulates.

==Foundational revision==

In the AGM framework, a belief set is represented by a deductively closed set of [[propositional formula]]e. While such sets are infinite, they can always be finitely representable. However, working with deductively closed sets of formulae leads to the implicit assumption that equivalent belief bases should be considered equal when revising. This is called the ''principle of irrelevance of syntax''.

This principle has been and is currently debated: while &lt;math&gt;\{a, b\}&lt;/math&gt; and &lt;math&gt;\{a \wedge b\}&lt;/math&gt; are two equivalent sets, revising by &lt;math&gt;\neg a&lt;/math&gt; should produce different results. In the first case, &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are two separate beliefs; therefore, revising by &lt;math&gt;\neg a&lt;/math&gt; should not produce any effect on &lt;math&gt;b&lt;/math&gt;, and the result of revision is &lt;math&gt;\{\neg a, b\}&lt;/math&gt;. In the second case, &lt;math&gt;a \wedge b&lt;/math&gt; is taken a single belief. The fact that &lt;math&gt;a&lt;/math&gt; is false contradicts this belief, which should therefore be removed from the belief base. The result of revision is therefore &lt;math&gt;\{\neg a\}&lt;/math&gt; in this case.

The problem of using deductively closed knowledge bases is that no distinction is made between pieces of knowledge that are known by themselves and pieces of knowledge that are merely consequences of them. This distinction is instead done by the ''foundational'' approach to belief revision, which is related to [[foundationalism]] in philosophy. According to this approach, retracting a non-derived piece of knowledge should lead to retracting all its consequences that are not otherwise supported (by other non-derived pieces of knowledge).  This approach can be realized by using knowledge bases that are not deductively closed and assuming that all formulae in the knowledge base represent self-standing beliefs, that is, they are not derived beliefs. In order to distinguish the foundational approach to belief revision to that based on deductively closed knowledge bases, the latter is called the ''coherentist'' approach. This name has been chosen because the coherentist approach aims at restoring the coherence
(consistency) among ''all'' beliefs, both self-standing and derived ones. This approach is related to [[coherentism]] in philosophy.

Foundationalist revision operators working on non-deductively closed belief bases typically select some subsets of &lt;math&gt;K&lt;/math&gt; that are consistent with &lt;math&gt;P&lt;/math&gt;, combined them in some way, and then conjoined them with &lt;math&gt;P&lt;/math&gt;. The following are two non-deductively closed base revision operators.

; WIDTIO : (When in Doubt, Throw it Out) the maximal subsets of &lt;math&gt;K&lt;/math&gt; that are consistent with &lt;math&gt;P&lt;/math&gt; are intersected, and &lt;math&gt;P&lt;/math&gt; is added to the resulting set; in other words, the result of revision is composed by &lt;math&gt;P&lt;/math&gt; and of all formulae of &lt;math&gt;K&lt;/math&gt; that are in all maximal subsets of &lt;math&gt;K&lt;/math&gt; that are consistent with &lt;math&gt;P&lt;/math&gt;;

; Ginsberg-Fagin-Ullman-Vardi : the maximal subsets of &lt;math&gt;K \cup \{P\}&lt;/math&gt; that are consistent and contain &lt;math&gt;P&lt;/math&gt; are combined by disjunction;

; Nebel : similar to the above, but a priority among formulae can be given, so that formulae with higher priority are less likely to being retracted than formulae with lower priority.

A different realization of the foundational approach to belief revision is based on explicitly declaring the dependences among beliefs. In the [[truth maintenance system]]s, dependence links among beliefs can be specified. In other worlds, one can explicitly declare that a given fact is believed because of one or more other facts; such a dependency is called a ''justification''. Beliefs not having any justifications play the role of non-derived beliefs in the non-deductively closed knowledge base approach.

==Model-based revision and update==

A number of proposals for revision and update based on the set of models of the involved formulae were developed independently of the AGM framework. The principle behind this approach is that a knowledge base is equivalent to a set of ''possible worlds'', that is, to a set of scenarios that are considered possible according to that knowledge base. Revision can therefore be performed on the sets of possible worlds rather than on the corresponding knowledge bases.

The revision and update operators based on models are usually identified by the name of their authors: [[Marianne Winslett|Winslett]], Forbus, Satoh, Dalal, Hegner, and Weber. According to the  first four of these proposal, the result of revising/updating a formula &lt;math&gt;K&lt;/math&gt; by another formula &lt;math&gt;P&lt;/math&gt; is characterized by the set of models of &lt;math&gt;P&lt;/math&gt; that are the closest to the models of &lt;math&gt;K&lt;/math&gt;. Different notions of closeness can be defined, leading to the difference among these proposals.

; Dalal : the models of &lt;math&gt;P&lt;/math&gt; having a minimal [[Hamming distance]] to models of &lt;math&gt;K&lt;/math&gt; are selected to be the models that result from the change;

; Satoh : similar to Dalal, but distance between two models is defined as the set of literals that are given different values by them; similarity between models is defined as set containment of these differences;

; Winslett : for each model of &lt;math&gt;K&lt;/math&gt;, the closest models of &lt;math&gt;P&lt;/math&gt; are selected; comparison is done using set containment of the difference;

; Borgida : equal to Winslett's if &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; are inconsistent; otherwise, the result of revision is &lt;math&gt;K \wedge P&lt;/math&gt;;

; [[Ken Forbus|Forbus]] : similar to Winslett, but the Hamming distance is used.

The revision operator defined by Hegner makes &lt;math&gt;K&lt;/math&gt; not to affect  the value of the variables that are mentioned in &lt;math&gt;P&lt;/math&gt;. What results from this operation is a formula &lt;math&gt;K'&lt;/math&gt; that is consistent with &lt;math&gt;P&lt;/math&gt;, and can therefore be conjoined with it. The revision operator by Weber is similar, but the literals that are removed from &lt;math&gt;K&lt;/math&gt; are not all literals of &lt;math&gt;P&lt;/math&gt;, but only the literals that are evaluated differently by a pair of closest models of &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; according to the Satoh measure of closeness.

==Iterated revision==

The AGM postulates are equivalent to a preference ordering (an ordering over models) to be associated to every knowledge base &lt;math&gt;K&lt;/math&gt;. However, they do not relate the orderings corresponding to two non-equivalent knowledge bases. In particular, the orderings associated to a knowledge base &lt;math&gt;K&lt;/math&gt; and its revised version &lt;math&gt;K*P&lt;/math&gt; can be completely different. This is a problem for performing a second revision, as the ordering associated with &lt;math&gt;K*P&lt;/math&gt; is necessary to calculate &lt;math&gt;K*P*Q&lt;/math&gt;.

Establishing a relation between the ordering associated with &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;K*P&lt;/math&gt; has been however recognized not to be the right solution to this problem. Indeed, the preference relation should depend on the previous history of revisions, rather than on the resulting knowledge base only. More generally, a preference relation gives more information about the state of mind of an agent than a simple knowledge base. Indeed, two states of mind might represent the same piece of knowledge &lt;math&gt;K&lt;/math&gt; while at the same time being different in the way a new piece of knowledge would be incorporated. For example, two people might have the same idea as to where to go on holiday, but yet they differ on how they would change this idea if they win a million-dollar lottery. Since the basic condition of the preference ordering is that their minimal models are exactly the models of their associated knowledge base, a knowledge base can be considered implicitly represented by a preference ordering (but not vice versa).

Given that a preference ordering allows deriving its associated knowledge base but also allows performing a single step of revision, studies on iterated revision have been concentrated on how a preference ordering should be changed in response of a revision. While single-step revision is about how a knowledge base &lt;math&gt;K&lt;/math&gt; has to be changed into a new knowledge base &lt;math&gt;K*P&lt;/math&gt;, iterated revision is about how a preference ordering (representing both the current knowledge and how much situations believed to be false are considered possible) should be turned into a new preference relation when &lt;math&gt;P&lt;/math&gt; is learned. A single step of iterated revision produces a new ordering that allows for further revisions.

Two kinds of preference ordering are usually considered: numerical and non-numerical. In the first case, the level of plausibility of a model is  representing by a non-negative integer number; the lower the rank, the more plausible the situation corresponding to the model. Non-numerical preference orderings correspond to the preference relations used in the AGM framework: a possibly total ordering over models. The non-numerical preference relation were initially considered unsuitable for iterated revision because of the impossibility of reverting a revision by a number of other revisions, which is instead possible in the numerical case.

Darwiche and [[Judea Pearl|Pearl]]&lt;ref name="darwiche-pearl"&gt;Darwiche, A. and Pearl, J. (1997) On the logic of iterated belief revision.  ''Artificial Intelligence'' '''89'''(1-2): 1-29.&lt;/ref&gt; formulated the following postulates for iterated revision.

# if &lt;math&gt;\alpha \models \mu&lt;/math&gt; then &lt;math&gt;(\psi * \mu) * \alpha \equiv \psi * \alpha&lt;/math&gt;;
# if &lt;math&gt;\alpha \models \neg \mu&lt;/math&gt;, then &lt;math&gt;(\psi * \mu) * \alpha \equiv \psi * \alpha&lt;/math&gt;;
# if &lt;math&gt;\psi * \alpha \models \mu&lt;/math&gt;, then &lt;math&gt;(\psi * \mu) * \alpha \models \mu&lt;/math&gt;;
# if &lt;math&gt;\psi * \alpha \not\models \neg \mu&lt;/math&gt;, then &lt;math&gt;(\psi * \mu) * \alpha \not\models \neg \mu&lt;/math&gt;.

Specific iterated revision operators have been proposed by Spohn, Boutilier, Williams, Lehmann, and others.

; Spohn rejected revision : this non-numerical proposal has been first considered by Spohn, who rejected it based on the fact that revisions can change some orderings in such a way the original ordering cannot be restored with a sequence of other revisions; this operator change a preference ordering in view of new information &lt;math&gt;P&lt;/math&gt; by making all models of &lt;math&gt;P&lt;/math&gt; being preferred over all other models; the original preference ordering is maintained when comparing two models that are both models of &lt;math&gt;P&lt;/math&gt; or both non-models of &lt;math&gt;P&lt;/math&gt;;

; Natural revision : while revising a preference ordering by a formula &lt;math&gt;P&lt;/math&gt;, all minimal models (according to the preference ordering) of &lt;math&gt;P&lt;/math&gt; are made more preferred by all other ones; the original ordering of models is preserved when comparing two models that are not minimal models of &lt;math&gt;P&lt;/math&gt;; this operator changes the ordering among models minimally while preserving the property that the models of the knowledge base after revising by &lt;math&gt;P&lt;/math&gt; are the minimal models of &lt;math&gt;P&lt;/math&gt; according to the preference ordering;

; Transmutations : these are two forms of revision, conditionalization and adjustment, which work on numerical preference orderings; revision requires not only a formula but also a number indicating its degree of plausibility; while the preference ordering is still inverted (the lower a model, the most plausible it is) the degree of plausibility of a revising formula is direct (the higher the degree, the most believed the formula is);

; Ranked revision : a ranked model, which is an assignment of non-negative integers to models, has to be specified at the beginning; this rank is similar to a preference ordering, but is not changed by revision; what is changed by a sequence of revisions are a current set of models (representing the current knowledge base) and a number called the rank of the sequence; since this number can only monotonically non-decrease, some sequences of revision lead to situations in which every further revision is performed as a full meet revision.

==Merging==

The assumption implicit in the revision operator is that the new piece of information &lt;math&gt;P&lt;/math&gt; is always to be considered more reliable than the old knowledge base &lt;math&gt;K&lt;/math&gt;. This is formalized by the second of the AGM postulates: &lt;math&gt;P&lt;/math&gt; is always believed after revising &lt;math&gt;K&lt;/math&gt; with &lt;math&gt;P&lt;/math&gt;. More generally, one can consider the process of merging several pieces of information (rather than just two) that might or might not have the same reliability. Revision becomes the particular instance of this process when a less reliable piece of information &lt;math&gt;K&lt;/math&gt; is merged with a more reliable &lt;math&gt;P&lt;/math&gt;.

While the input to the revision process is a pair of formulae &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt;, the input to merging is a [[multiset]] of formulae &lt;math&gt;K&lt;/math&gt;, &lt;math&gt;T&lt;/math&gt;, etc. The use of multisets is necessary as two sources to the merging process might be identical.

When merging a number of knowledge bases with the same degree of plausibility, a distinction is made between arbitration and majority. This distinction depends on the assumption that is made about the information and how it has to be put together.

; arbitration : the result of arbitrating two knowledge bases &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;T&lt;/math&gt; entails &lt;math&gt;K \vee T&lt;/math&gt;; this condition formalizes the assumption of maintaining as much as the old information as possible, as it is equivalent to imposing that every formula entailed by both knowledge bases is also entailed by the result of their arbitration; in a possible world view, the "real" world is assumed one of the worlds considered possible according to at least one of the two knowledge bases;

; majority : the result of merging a knowledge base &lt;math&gt;K&lt;/math&gt; with other knowledge bases can be forced to entail &lt;math&gt;K&lt;/math&gt; by adding a sufficient number of other knowledge bases equivalent to &lt;math&gt;K&lt;/math&gt;; this condition corresponds to a kind of vote-by-majority: a sufficiently large number of knowledge bases can always overcome the "opinion" of any other fixed set of knowledge bases.

The above is the original definition of arbitration. According to a newer definition, an arbitration operator is a merging operator that is insensitive to the number of equivalent knowledge bases to merge. This definition makes arbitration the exact opposite of majority.

Postulates for both arbitration and merging have been proposed. An example of an arbitration operator satisfying all postulates is the classical disjunction. An example of a majority operator satisfying all postulates is that selecting all models that have a minimal total Hamming distance to models of the knowledge bases to merge.

A merging operator can be expressed as a family of orderings over models, one for each possible multiset of knowledge bases to merge: the models of the result of merging a multiset of knowledge bases are the minimal models of the ordering associated to the multiset. A merging operator defined in this way satisfies the postulates for merging if and only if the family of orderings meets a given set of conditions. For the old definition of arbitration, the orderings are not on models but on pairs (or, in general, tuples) of models.

==Social choice theory==

Many revision proposals involve orderings over models representing the relative plausibility of the possible alternatives. The problem of merging amounts to combine a set of orderings into a single one expressing the combined  plausibility of the alternatives. This is similar with what is done in [[social choice theory]], which is the study of how the preferences of a group of agents can be combined in a rational way. Belief revision and social choice theory are similar in that they combine a set of orderings into one. They differ on how these orderings are interpreted: preferences in social choice theory; plausibility in belief revision. Another difference is that the alternatives are explicitly enumerated in social choice theory, while they are the propositional models over a given alphabet in belief revision.

==Complexity==

The problem about belief revision that is the most studied from the point of view of [[Computational complexity theory|computational complexity]] is that of query answering in the propositional case. This is the problem of establishing whether a formula follows from the result of a revision, that is, &lt;math&gt;K*P \models Q&lt;/math&gt;, where &lt;math&gt;K&lt;/math&gt;, &lt;math&gt;P&lt;/math&gt;, and &lt;math&gt;Q&lt;/math&gt; are propositional formulae. More generally, query answering is the problem of telling whether a formula is entailed by the result of a belief revision, which could be update, merging, revision, iterated revision, etc. Another problem that has received some attention is that of model checking, that is, checking whether a model satisfies the result of a belief revision. A related question is whether such result can be represented in space polynomial in that of its arguments.

Since a deductively closed knowledge base is infinite, complexity studies on belief revision operators working on deductively closed knowledge bases are done in the assumption that such deductively closed knowledge base are given in the form of an equivalent finite knowledge base.

A distinction is made among belief revision operators and belief revision schemes. While the former are simple mathematical operators mapping a pair of formulae into another formula, the latter depend on further information such as a preference relation. For example, the Dalal revision is an operator because, once two formulae &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; are given, no other information is needed to compute &lt;math&gt;K*P&lt;/math&gt;. On the other hand, revision based on a preference relation is a revision scheme, because &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; do not allow determining the result of revision if the family of preference orderings between models is not given. The complexity for revision schemes is determined in the assumption that the extra information needed to compute revision is given in some compact form. For example, a preference relation can be represented by a sequence of formulae whose models are increasingly preferred. Explicitly storing the relation as a set of pairs of models is instead not a compact representation of preference because the space required is exponential in the number of propositional letters.

The complexity of query answering and model checking in the propositional case is in the second level of the [[polynomial hierarchy]] for most belief revision operators and schemas. Most revision operators suffer from the problem of representational blow up: the result of revising two formulae is not necessarily representable in space polynomial in that of the two original formulae. In other words, revision may exponentially increase the size of the knowledge base.

==Implementations==

Systems specifically implementing belief revision are: [http://portal.acm.org/citation.cfm?id=122296.122301 Immortal], [https://web.archive.org/web/20051018054730/http://magic.it.uts.edu.au:80/systems/saten.html SATEN], and [http://www.dis.uniroma1.it/~liberato/brels/brels.html BReLS]. Two systems including a belief revision feature are [http://www.cse.buffalo.edu/sneps/ SNePS] and [[Cyc]]. [[Truth maintenance systems]] are used in [[Artificial Intelligence]] to implement belief revision.

==See also==

* [[Artificial intelligence]]
* [[Inquiry]]
* [[Knowledge representation]]
* [[Belief propagation]]
* [[Reason maintenance]]
* [[Epistemic closure]]
* [[Non-monotonic logic]]
* [[Defeasible reasoning]]
* [[Reasoning]]
* [[Philosophy of science]]
* [[Discursive dilemma]]

==Notes==
{{reflist}}

==References==
* C. E. Alchourr&#242;n, P. G&#228;rdenfors, and D. Makinson (1985). On the logic of theory change: Partial meet contraction and revision functions. ''Journal of Symbolic Logic'', 50:510&#8211;530.
* C. Boutilier (1993). Revision sequences and nested conditionals. In ''Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI'93)'', pages 519&#8211;525.
* C. Boutilier (1995). Generalized update: belief change in dynamic settings. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1550&#8211;1556.
* C. Boutilier (1996). Abduction to plausible causes: an event-based model of belief update. ''Artificial Intelligence'', 83:143&#8211;166.
* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (1999). The size of a revised knowledge base. ''Artificial Intelligence'', 115(1):25&#8211;64.
* T. Chou and [[Marianne Winslett|M. Winslett]] (1991). Immortal: A model-based belief revision system. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 99&#8211;110. Morgan Kaufmann Publishers.
* M. Dalal (1988). Investigations into a theory of knowledge base revision: Preliminary report. In ''Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI'88)'', pages 475&#8211;479.
* T. Eiter and G. Gottlob (1992). On the complexity of propositional knowledge base revision, updates and counterfactuals. ''Artificial Intelligence'', 57:227&#8211;270.
* T. Eiter and G. Gottlob (1996). The complexity of nested counterfactuals and iterated knowledge base revisions. ''Journal of Computer and System Sciences'', 53(3):497&#8211;512.
* R. Fagin, J. D. Ullman, and M. Y. Vardi (1983). On the semantics of updates in databases. In ''Proceedings of the Second ACM SIGACT SIGMOD Symposium on Principles of Database Systems (PODS'83)'', pages 352&#8211;365.
* M. A. Falappa, G. Kern-Isberner, G. R. Simari (2002): Explanations, belief revision and defeasible reasoning. ''Artificial Intelligence'', 141(1&#8211;2): 1&#8211;28.
* M. Freund and D. Lehmann (2002). Belief Revision and Rational Inference. [http://arxiv.org/abs/cs.AI/0204032 Arxiv preprint cs.AI/0204032].
* N. Friedman and J. Y. Halpern (1994). A knowledge-based framework for belief change, part II: Revision and update. In ''Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94)'', pages 190&#8211;200.
* A. Fuhrmann (1991). Theory contraction through base contraction. ''Journal of Philosophical Logic'', 20:175&#8211;203.
* D. Gabbay, G. Pigozzi, and J. Woods (2003). Controlled Revision&amp;nbsp;&#8211; An algorithmic approach for belief revision, ''Journal of Logic and Computation'', 13(1): 15&#8211;35.
* P. G&#228;rdenfors and D. Makinson (1988). Revision of knowledge systems using epistemic entrenchment. In ''Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge (TARK'88)'', pages 83&#8211;95.
* P. G&#228;rdenfors and H. Rott (1995). Belief revision. In ''Handbook of Logic in Artificial Intelligence and Logic Programming, Volume 4'', pages 35&#8211;132. Oxford University Press.
* G. Grahne and [[Alberto O. Mendelzon]] (1995). Updates and subjunctive queries. ''Information and Computation'', 2(116):241&#8211;252.
* G. Grahne, [[Alberto O. Mendelzon]], and P. Revesz (1992). Knowledge transformations. In ''Proceedings of the Eleventh ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'92)'', pages 246&#8211;260.
* S. O. Hansson (1999). ''A Textbook of Belief Dynamics''. Dordrecht: Kluwer Academic Publishers.
* A. Herzig (1996). The PMA revised. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 40&#8211;50.
* A. Herzig (1998). Logics for belief base updating. In D. Dubois, D. Gabbay, H. Prade, and P. Smets, editors, ''Handbook of defeasible reasoning and uncertainty management'', volume 3 &#8211; Belief Change, pages 189&#8211;231. Kluwer Academic Publishers.
* H. Katsuno and A. O. Mendelzon (1991). On the difference between updating a knowledge base and revising it. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 387&#8211;394.
* H. Katsuno and A. O. Mendelzon (1991). Propositional knowledge base revision and minimal change. ''Artificial Intelligence'', 52:263&#8211;294.
* S. Konieczny and R. Pino Perez (1998). On the logic of merging. In ''Proceedings of the Sixth International Conference on Principles of Knowledge Representation and Reasoning (KR'98)'', pages 488&#8211;498.
* D. Lehmann (1995). Belief revision, revised. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1534&#8211;1540.
* P. Liberatore (1997). The complexity of iterated belief revision. In ''Proceedings of the Sixth International Conference on Database Theory (ICDT'97)'', pages 276&#8211;290.
* P. Liberatore and M. Schaerf (1998). Arbitration (or how to merge knowledge bases). ''IEEE Transactions on Knowledge and Data Engineering'', 10(1):76&#8211;90.
* P. Liberatore and M. Schaerf (2000). BReLS: A system for the integration of knowledge bases. In ''Proceedings of the Seventh International Conference on Principles of Knowledge Representation and Reasoning (KR 2000)'', pages 145&#8211;152.
* D. Makinson (1985). How to give up: A survey of some formal aspects of the logic of theory change. ''Synthese'', 62:347&#8211;363.
* A. Perea (2003). ''Proper Rationalizability and Belief Revision in Dynamic Games''. Research Memoranda 048: METEOR, Maastricht Research School of Economics of Technology and Organization.
* B. Nebel (1991). Belief revision and default reasoning: Syntax-based approaches. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 417&#8211;428.
* B. Nebel (1994). Base revision operations and schemes: Semantics, representation and complexity. In ''Proceedings of the Eleventh European Conference on Artificial Intelligence (ECAI'94)'', pages 341&#8211;345.
* B. Nebel (1996). How hard is it to revise a knowledge base? Technical Report 83, Albert-Ludwigs-Universit&#228;t Freiburg, Institut f&#252;r Informatik.
* G. Pigozzi (2005). Two aggregation paradoxes in social decision making: the Ostrogorski paradox and the [[discursive dilemma]], ''Episteme: A Journal of Social Epistemology'', 2(2): 33&#8211;42.
* G. Pigozzi (2006). [http://pigozzi.org/Pigozzi_Judgment_Aggregation.pdf Belief merging and the discursive dilemma: an argument-based account to paradoxes of judgment aggregation]. ''Synthese'' 152(2): 285&#8211;298.
* P. Z. Revesz (1993). On the semantics of theory change: Arbitration between old and new information. In ''Proceedings of the Twelfth ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'93)'', pages 71&#8211;82.
* K. Satoh (1988). Nonmonotonic reasoning by minimal belief revision. In ''Proceedings of the International Conference on Fifth Generation Computer Systems (FGCS'88)'', pages 455&#8211;462.
* {{cite book | last1=Shoham | first1=Yoav | last2=Leyton-Brown | first2=Kevin | title=Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations | publisher=[[Cambridge University Press]] | isbn=978-0-521-89943-7 | url=http://www.masfoundations.org | year=2009 | location=New York}} See Section 14.2; [http://www.masfoundations.org/download.html downloadable free online].
* V. S. Subrahmanian (1994). Amalgamating knowledge bases. ''ACM Transactions on Database Systems'', 19(2):291&#8211;331.
* A. Weber (1986). Updating propositional formulas. In ''Proc. of First Conf. on Expert Database Systems'', pages 487&#8211;500.
* M. Williams (1994). Transmutations of knowledge systems. In ''Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94)'', pages 619&#8211;629.
* M. Winslett (1989). Sometimes updates are circumscription. In ''Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI'89)'', pages 859&#8211;863.
* M. Winslett (1990). ''Updating Logical Databases''. Cambridge University Press.
* Y. Zhang and N. Foo (1996). Updating knowledge bases with disjunctive information. In ''Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI'96)'', pages 562&#8211;568.

==External links==
* {{PhilPapers|category|belief-revision}}
* {{InPho|idea|1448|Logic of Belief Revision}}
* {{cite SEP |url-id=logic-belief-revision |title=Logic of Belief Revision}}
* [http://www.beliefrevision.org/ Beliefrevision.org]
* [http://plato.stanford.edu/entries/reasoning-defeasible/#4.3 Defeasible Reasoning: 4.3 Belief Revision Theory] at [[Stanford Encyclopedia of Philosophy]]

[[Category:Belief revision| ]]
[[Category:Belief]]
[[Category:Formal epistemology]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Logic programming]]</text>
      <sha1>s94nkucl2eowe8xhx9arbrigdbgnnfv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Mereology</title>
    <ns>14</ns>
    <id>14239157</id>
    <revision>
      <id>548024178</id>
      <parentid>511766336</parentid>
      <timestamp>2013-03-31T19:14:24Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 2 langlinks, now provided by Wikidata on [[d:Q9053255]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="162" xml:space="preserve">{{Cat main|Mereology}}
[[Category:Philosophical logic]]
[[Category:Ontology]]
[[Category:Knowledge representation]]
[[Category:Materialism]]
[[Category:Quantity]]</text>
      <sha1>2ogptqad7xopvfhf7xr5ua9qk0abgra</sha1>
    </revision>
  </page>
  <page>
    <title>Region connection calculus</title>
    <ns>0</ns>
    <id>8489018</id>
    <revision>
      <id>749442283</id>
      <parentid>749442220</parentid>
      <timestamp>2016-11-14T10:20:59Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <minor />
      <comment>/* top */ bold alt article name per MOS</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4849" xml:space="preserve">{{no footnotes|date=November 2016}}
The '''region connection calculus''' ('''RCC''') is intended to serve for qualitative spatial representation and [[Spatial-temporal reasoning|reasoning]]. RCC abstractly describes regions (in [[Euclidean space]], or in a [[topological space]]) by their possible relations to each other. RCC8 consists of 8 basic relations that are possible between two regions:
* disconnected (DC)
* externally connected (EC)
* equal (EQ)
* partially overlapping (PO)
* tangential proper part (TPP)
* tangential proper part inverse (TPPi)
* non-tangential proper part (NTPP)
* non-tangential proper part inverse (NTPPi)
From these basic relations, combinations can be built. For example, proper part (PP) is the union of TPP and NTPP.
[[Image:RCC8.jpg]]

==Composition table==
The composition table of RCC8 are as follows:

&lt;center&gt;
{| class="wikitable" style="text-align:center;" border="1"
! o
! DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ
|-
! DC
| * || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC || DC || DC
|-
! EC
| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPP,NTPP || EC,PO,TPP,NTPP || PO,TPP,NTPP || DC,EC || DC || EC
|-
! PO
| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || * || PO,TPP,NTPP || PO,TPP,NTPP || DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || PO
|-
! TPP
| DC || DC,EC || DC,EC,PO,TPP,NTPP || TPP,NTPP || NTPP || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPPi,NTPPi || TPP
|-
! NTPP
| DC || DC || DC,EC,PO,TPP,NTPP || NTPP || NTPP || DC,EC,PO,TPP,NTPP|| * || NTPP
|-
! TPPi
| DC,EC,PO,TPPi,NTPPi || EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,TPPi,EQ || PO,TPP,NTPP || TPPi,NTPPi || NTPPi || TPPi
|-
! NTPPi
| DC,EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,NTPP,TPPi,NTPPi,EQ|| NTPPi || NTPPi || NTPPi
|-
! EQ
|  DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ
|}
&lt;/center&gt;

*  "*" denotes the universal relation.

==Examples==

The RCC8 calculus is intended for reasoning about spatial configurations. Consider the following example: two houses are connected via a road. Each house is located on an own property. The first house possibly touches the boundary of the property; the second one surely does not. What can we infer about the relation of the second property to the road? 

The spatial configuration can be formalized in RCC8 as the following [[constraint network]]:

 house1 DC house2
 house1 {TPP, NTPP} property1
 house1 {DC, EC} property2
 house1 EC road
 house2 { DC, EC } property1
 house2 NTPP property2
 house2 EC road
 property1 { DC, EC } property2
 road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property1
 road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property2

Using the RCC8 [[composition table]] and the [[path-consistency algorithm]], we can refine the network in the following way:
 road { PO, EC } property1
 road { PO, TPP } property2

That is, the road either overlaps with the second property, or is even (tangential) part of it.

Other versions of the region connection calculus include RCC5 (with only five basic relations - the distinction whether two regions touch each other are ignored) and RCC23 (which allows reasoning about convexity).

==RCC8 use in GeoSPARQL==

RCC8 has been partially{{Clarify|date=January 2016}} implemented in [[GeoSPARQL]] as described below:
[[File:Region_Connection_Calculus_8_Relations_and_Open_Geospatial_Consortium_relations.svg|thumb|center|700px|alt=A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.|A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.]]

==References==
* Randell, D. A., Cui, Z. and Cohn, A. G.:  [http://wenxion.net/ac/randell92spatial.pdf A spatial logic based on regions and connection], Proc. 3rd Int. Conf. on Knowledge Representation and Reasoning, Morgan Kaufmann, San Mateo, pp. 165&#8211;176, 1992.
* Anthony G. Cohn, Brandon Bennett, John Gooday, Micholas Mark Gotts: Qualitative Spatial Representation and Reasoning with the Region Connection Calculus. GeoInformatica, 1, 275&#8211;316, 1997.
* J. Renz: [http://www.springerlink.com/content/d5g7fcjkd0q2/ Qualitative Spatial Reasoning with Topological Information]. Lecture Notes in Computer Science 2293, Springer Verlag, 2002.
* T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC&#8314;&#8314;]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352 

[[Category:Reasoning]]
[[Category:Knowledge representation]]
[[Category:Constraint programming]]
[[Category:Computational topology]]
[[Category:Logical calculi]]</text>
      <sha1>bxvwgbwl7cs8wlde1qcukmnwqff8jic</sha1>
    </revision>
  </page>
  <page>
    <title>Vivification</title>
    <ns>0</ns>
    <id>15440345</id>
    <revision>
      <id>618789411</id>
      <parentid>591607328</parentid>
      <timestamp>2014-07-28T08:48:15Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>redlink [[Least common subsumer]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2202" xml:space="preserve">'''Vivification''' is an operation on a [[description logic]] knowledge base to improve performance of a [[semantic reasoner]].  Vivification replaces a [[Logical disjunction|disjunction]] of concepts &lt;math&gt;C_1 \sqcup C_2 \ldots \sqcup C_n&lt;/math&gt; by the ''[[least common subsumer]]'' of the concepts &lt;math&gt;C_1,C_2,\ldots C_n&lt;/math&gt;.

The goal of this operation is to improve the performance of the reasoner by replacing a complex set of concepts with a single concept which subsumes the original concepts. 

For example, consider the example given in (Cohen 92):  Suppose we have the concept &lt;math&gt;\textrm{PIANIST(Jill)} \vee \textrm{ORGANIST(Jill)}&lt;/math&gt;.  This concept can be vivified into a simpler concept &lt;math&gt;\textrm{KEYBOARD-PLAYER(Jill)}&lt;/math&gt;.  This summarization leads to an approximation that may not be exactly equivalent to the original.

== An approximation ==

[[Knowledge base]] vivification is not necessarily exact. If the reasoner is operating under the [[open world assumption]] we may get surprising results.  In the previous example, if we replace the disjunction with the vivified concept,  we will arrive at a surprising results.  

First, we find that the reasoner will no longer classify Jill as either a pianist or an organist.  Even though &lt;math&gt;\textrm{ORGANIST}&lt;/math&gt; and &lt;math&gt;\textrm{PIANIST}&lt;/math&gt; are the only two sub-classes, under the OWA we can no longer classify Jill as playing one or the other.  The reason is that there may be another keyboard instrument (e.g. a harpsichord) that Jill plays but which does not have a specific subclass.   

== References ==
# Cohen, W.W., Borgida, A., Hirsh, H., Computing Least Common Subsumers in Description Logics, In: Proc. AAAI-92, AAAI Press/The MIT Press, 1992, pages 754--760. [http://citeseer.ist.psu.edu/cohen92computing.html CiteSeer]
# Baader, F., Kusters, R., Wolter F., Extensions to Description Logics. In F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P.F. Patel-Schneider, editors, The Description Logic Handbook: Theory, Implementation, and Applications. Cambridge University Press, 2003. http://citeseer.ist.psu.edu/baader03basic.html
{{Wikt|vivification}}

[[Category:Knowledge representation]]</text>
      <sha1>amp6jqukgzdgjel7w76r1m4az5kkcv7</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic knowledge management</title>
    <ns>0</ns>
    <id>20298912</id>
    <revision>
      <id>723876921</id>
      <parentid>721184068</parentid>
      <timestamp>2016-06-05T20:53:16Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* References */refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1429" xml:space="preserve">{{Orphan|date=February 2009}}
'''Semantic knowledge management''' is a set of practices that seeks to classify content so that the knowledge it contains may be immediately accessed and transformed for delivery to the desired audience, in the required format. This classification of content is semantic in its nature &amp;ndash; identifying content by its type or meaning within the content itself and via external, descriptive metadata &#8211; and is achieved by employing [[XML]] technologies.

The specific outcomes of these practices are:

* Maintain content for multiple audiences together in a single document 
* Transform content into various delivery formats without re-authoring  
* Search for content more effectively 
* Involve more [[subject-matter expert]]s in the creation of content without reducing quality 
* Reduce production costs for delivery formats 
* Reduce the manual administration of getting the right knowledge to the right people 
* Reduce the cost and time to localize content

==References==
{{refbegin}}
* {{cite book|title=Semantic Knowledge Management: Integrating Ontology Management, Knowledge Discovery, and Human Language Technologies|author1=John Davies |author2=Marko Grobelnik |author3=Dunja Mladenic |isbn=3-540-89164-1|year=2008}}
{{refend}}

== Notable semantic knowledge management systems ==
*Learn eXact
*Thinking Cap LCMS
*Thinking Cap LMS
*Xyleme LCMS

[[Category:Knowledge representation]]</text>
      <sha1>iaq3799265yhaugi453hvn3lj48dep4</sha1>
    </revision>
  </page>
  <page>
    <title>Upper ontology</title>
    <ns>0</ns>
    <id>3200382</id>
    <revision>
      <id>760467692</id>
      <parentid>758192259</parentid>
      <timestamp>2017-01-17T04:30:48Z</timestamp>
      <contributor>
        <username>RoyCullum</username>
        <id>7873718</id>
      </contributor>
      <minor />
      <comment>Changed "an" to "and"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="43954" xml:space="preserve">{{multiple issues|
{{more footnotes|date=February 2011}}
{{essay-like|date=October 2010}}
}}

In [[information science]], an '''upper ontology''' (also known as a '''top-level ontology''' or '''foundation ontology''') is an [[Ontology (computer science)|ontology]] (in the sense used in information science) which consists of very general terms (such as "object", "property", "relation") that are common across all domains.  An important function of an upper ontology is to support broad [[semantic interoperability]] among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked "under" the terms in the upper ontology, and the former stand to the latter in subclass relations.

A number of upper ontologies have been proposed, each with its own proponents. Each upper ontology can be considered as a computational implementation of [[natural philosophy]], which itself is a more empirical method for investigating the topics within the philosophical discipline of [[physical ontology]].

[[Library classification]] systems predate upper ontology systems. Though library classifications organize and categorize knowledge using general concepts that are the same across all knowledge domains, neither system is a replacement for the other.

==Development==

Any standard foundational ontology is likely to be contested among different groups, each with their own idea of "what exists". One factor exacerbating the failure to arrive at a common approach has been the lack of open-source applications that would permit the testing of different ontologies in the same computational environment.  The differences have thus been debated largely on theoretical grounds, or are merely the result of personal preferences. Foundational ontologies can however be compared on the basis of adoption for the purposes of supporting interoperability across domain ontologies.

No particular upper ontology has yet gained widespread acceptance as a [[de facto]] standard.  Different organizations have attempted to [[#Available ontologies|define standards]] for specific domains.  The '[[Process Specification Language]]' (PSL) created by the [[National Institute for Standards and Technology]] (NIST) is one example.

Another important factor leading to the absence of wide adoption of any existing upper ontology is the complexity. Some upper ontologies -- [[Cyc]] is often cited as an example in this regard -- are very large, ranging up to thousands of elements (classes, relations), with complex interactions among them and with a complexity similar to that of a human [[natural language]], and the learning process can be even longer than for a natural language because of the unfamiliar format and logical rules.  The motivation to overcome this learning barrier is largely absent because of the paucity of publicly accessible examples of use.  As a result, those building domain ontologies for local applications tend to create the simplest possible domain-specific ontology, not related to any upper ontology.  Such domain ontologies may function adequately for the local purpose, but they are very time-consuming to relate accurately to other domain ontologies. 

To solve this problem some genuinely top level ontologies have been developed, which are deliberately designed to have minimal overlap with any domain ontologies. Examples are [[Basic Formal Ontology]] and the [[Domain Ontology for Linguistic and Cognitive Engineering | DOLCE]] (see below).

===Arguments for the infeasibility of an upper ontology===
{{unreferenced section|date=December 2016}}
Historically, many attempts in many societies{{which?|date=December 2016}} have been made to impose or define a single set of concepts as more primal, basic, foundational, authoritative, true or rational than all others. A common objection{{By whom?|date=December 2016}} to such attempts points out that humans lack the sort of  transcendent perspective - or ''[[God's eye view]]'' - that would be required to achieve this goal. Humans are bound by language or culture, and so lack the sort of objective perspective from which to observe the whole terrain of concepts and derive any one standard.

Another objection is the problem of formulating definitions. Top level ontologies are designed to maximize support for interoperability across a large number of terms. Such ontologies must therefore consist of terms expressing very general concepts, but such concepts are so basic to our understanding that there is no way in which they can be defined, since the very process of definition implies that a less basic (and less well understood) concept is defined in terms of concepts that are more basic and so (ideally) more well understood. Very general concepts can often only be elucidated, for example by means of examples, or paraphrase. 

* There is no self-evident way of dividing the world up into [[concept]]s, and certainly no non-controversial one
* There is no neutral ground that can serve as a means of translating between specialized (or "lower" or "application-specific") ontologies
* Human [[language]] itself is already an arbitrary approximation of just one among many possible conceptual maps.  To draw any ''necessary correlation'' between [[English language|English]] words and any number of intellectual concepts we might like to represent in our ontologies is just asking for trouble. ([[WordNet]], for instance, is successful and useful precisely because it does not pretend to be a general-purpose upper ontology; rather, it is a tool for semantic / syntactic / linguistic disambiguation, which is richly embedded in the particulars and peculiarities of the English language.)
* Any hierarchical or topological representation of concepts must begin from some ontological, [[epistemology|epistemological]], linguistic, cultural, and ultimately pragmatic perspective.  Such pragmatism does not allow for the exclusion of politics between persons or groups, indeed it requires they be considered as perhaps more basic primitives than any that are represented.

Those{{who?|date=December 2016}} who doubt the feasibility of general purpose ontologies are more inclined to ask &#8220;what specific purpose do we have in mind for this conceptual map of entities and what practical difference will this ontology make?&#8221;  This pragmatic philosophical position surrenders all hope of devising the encoded ontology version of &#8220;everything that is the case,&#8221; ([[Wittgenstein]], [[Tractatus Logico-Philosophicus]]).

Finally there are objections similar to those against [[artificial intelligence]]{{From whom?|date=December 2016}}.  Technically, the complex concept acquisition and the social / linguistic interactions of human beings suggests any axiomatic foundation of "most basic" concepts must be cognitive, biological or otherwise difficult to characterize since we don't have axioms for such systems.  Ethically, any general-purpose ontology could quickly become an actual tyranny by recruiting adherents into a political program designed to propagate it and its funding means, and possibly defend it by violence.  Historically, inconsistent and irrational belief systems have proven capable of commanding obedience to the detriment or harm of persons both inside and outside a society that accepts them.  How much more harmful would a consistent rational one be, were it to contain even one or two basic assumptions incompatible with human life?

===Arguments for the feasibility of an upper ontology===
{{unreferenced section|date=November 2014}}
Many of those who doubt the possibility of developing wide agreement on a common upper ontology fall into one of two traps:
# they assert that there is no possibility of universal agreement on any conceptual scheme; but they argue that a practical common ontology does not need to have universal agreement, it only needs a large enough user community (as is the case for human languages) to make it profitable for developers to use it as a means to general interoperability, and for third-party developer to develop utilities to make it easier to use; and
# they point out that developers of data schemes find different representations congenial for their local purposes; but they do not demonstrate that these different representation are in fact logically inconsistent.

In fact, different representations of assertions about the real world (though not philosophical models), if they accurately reflect the world, must be logically consistent, even if they focus on different aspects of the same physical object or phenomenon.  If any two assertions about the real world are logically inconsistent, one or both must be wrong, and that is a topic for experimental investigation, not for ontological representation.  In practice, representations of the real world are created as and known to be approximations to the basic reality, and their use is circumscribed by the limits of error of measurements in any given practical application.  Ontologies are entirely capable of representing approximations, and are also capable of representing situations in which different approximations have different utility.  Objections based on the different ways people perceive things attack a simplistic, impoverished view of ontology.  The objection that there are logically incompatible models of the world are true, but in an upper ontology those different models can be represented as different theories, and the adherents of those theories can use them in preference to other theories, while preserving the logical consistency of the ''necessary'' assumptions of the upper ontology.  The ''necessary'' assumptions provide the logical vocabulary with which to specify the meanings of all of the incompatible models.  It has never been demonstrated that incompatible models cannot be properly specified with a common, more basic set of concepts, while there are examples of incompatible theories that can be logically specified with only a few basic concepts.

Many of the objections to upper ontology refer to the problems of life-critical decisions or non-axiomatized  problem areas such as law or medicine or politics that are difficult even for humans to understand.  Some of these objections do not apply to physical objects or standard abstractions that are defined into existence by human beings and closely controlled by them for mutual good, such as standards for electrical power system connections or the signals used in traffic lights.  No single general [[metaphysics]] is required to agree that some such standards are desirable.  For instance, while time and space can be represented many ways, some of these are already used in interoperable artifacts like maps or schedules.

Objections to the feasibility of a common upper ontology also do not take into account the possibility of forging agreement on an ontology that contains all of the ''primitive'' ontology elements that can be combined to create any number of more specialized concept representations.  Adopting this tactic permits effort to be focused on agreement only on a limited number of ontology elements. By agreeing on the meanings of that inventory of basic concepts, it becomes possible to create and then accurately and automatically interpret an infinite number of concept representations as combinations of the basic ontology elements.  Any domain ontology or database that uses the elements of such an upper ontology to specify the meanings of its terms will be automatically and accurately interoperable with other ontologies that use the upper ontology, even though they may each separately define a large number of domain elements not defined in other ontologies.  In such a case, proper interpretation will require that the logical descriptions of domain-specific elements be transmitted along with any data that is communicated; the data will then be automatically interpretable because the domain element descriptions, based on the upper ontology, will be properly interpretable by any system that can properly use the upper ontology.  In effect elements in different domain ontologies can be *translated* into each other using the common upper ontology. An upper ontology based on such a set of primitive elements can include alternative views, provided that they are logically compatible.  Logically incompatible models can be represented as alternative theories, or represented in a specialized extension to the upper ontology.  The proper use of alternative theories is a piece of knowledge that can itself be represented in an ontology.  Users that develop new domain ontologies and find that there are semantic primitives needed for their domain but missing from the existing common upper ontology can add those new primitives by the accepted procedure, expanding the common upper ontology as necessary.

Most proponents{{Who|date=April 2015}} of an upper ontology argue that several good ones may be created with perhaps different emphasis.  Very few are actually arguing to discover just one within natural language or even an academic field.  Most are simply standardizing some existing communication.  Another view advanced is that there is almost total overlap of the different ways that upper ontologies have been formalized, in the sense that different ontologies focus on a different aspect of the same entities, but the different views are complementary and not contradictory to each other; as a result, an internally consistent ontology that contains all the views, with means of translating the different views into the other, is feasible.  Such an ontology has not thus far been constructed, however, because it would require a large project to develop so as to include all of the alternative views in the separately developed upper ontologies, along with their translations.  The main barrier to construction of such an ontology is not the technical issues, but the reluctance of funding agencies to provide the funds for a large enough consortium of developers and users.

Several common arguments against upper ontology can be examined more clearly by separating issues of concept definition (ontology), language (lexicons), and facts (knowledge).  For instance, people have different terms and phrases for the same concept.  However, that does not necessarily mean that those people are referring to different concepts.  They may simply be using different language or idiom.  Formal ontologies typically use linguistic labels to refer to concepts, but the terms that label ontology elements mean no more and no less than what their axioms say they mean.  Labels are similar to variable names in software, evocative rather than definitive.  The proponents of a common upper ontology point out that the meanings of the elements (classes, relations, rules) in an ontology depend only on their [[logical form]], and not on the labels, which are usually chosen merely to make the ontologies more easily usable by their human developers.  In fact, the labels for elements in an ontology need not be words - they could be, for example, images of instances of a particular type, or videos of an action that is represented by a particular type.  It cannot be emphasized too strongly that words are *not* what are represented in an ontology, but entities in the real world, or abstract entities (concepts) in the minds of people.  Words are not equivalent to ontology elements, but words *label* ontology elements.  There can be many words that label a single concept, even in a single language (synonymy), and there can be many concepts labeled by a single word (ambiguity).  Creating the mappings between human language and the elements of an ontology is the province of Natural Language Understanding.  But the ontology itself stands independently as a logical and computational structure.  For this reason, finding agreement on the structure of an ontology is actually easier than developing a controlled vocabulary, because all different interpretations of a word can be included, each *mapped* to the same word in the different terminologies.

A second argument is that people believe different things, and therefore can't have the same ontology.  However, people can assign different truth values to a particular assertion while accepting the validity of certain underlying claims, facts, or way of expressing an argument with which they disagree. (Using, for instance, the issue/position/argument form.) This objection to upper ontologies ignores the fact that a single ontology can represent different belief systems, representing them as different belief systems, without taking a position on the validity of either.

Even arguments about the existence of a thing require a certain sharing of a concept, even though its existence in the real world may be disputed.  Separating belief from naming and definition also helps to clarify this issue, and show how concepts can be held in common, even in the face of differing belief.  For instance, [[wiki]] as a medium may permit such confusion but disciplined users can apply [[dispute resolution]] methods to sort out their conflicts.  It is also argued that most people share a common set of "semantic primitives", fundamental concepts, to which they refer when they are trying to explain unfamiliar terms to other people.  An ontology that includes representations of those semantic primitives could in such a case be used to create logical descriptions of any term that a person may wish to define logically.   That ontology would be one form of upper ontology, serving as a logical "interlingua" that can translate ideas in one terminology to its [[logical equivalence|logical equivalent]] in another terminology.

Advocates{{Who|date=April 2015}} argue that most disagreement about the viability of an upper ontology can be traced to the conflation of ontology, language and knowledge, or too-specialized areas of knowledge: many people, or agents or groups will have areas of their respective internal ontologies that do not overlap.  If they can cooperate and share a conceptual map at all, this may be so very useful that it outweighs any disadvantages that accrue from sharing.  To the degree it becomes harder to share concepts the deeper one probes, the more valuable such sharing tends to get.  If the problem is as basic as opponents of upper ontologies claim, then, it also applies to a group of humans trying to cooperate, who might need machine assistance to communicate easily.

If nothing else, such ontologies are implied by [[machine translation]], used when people cannot practically communicate.  Whether "upper" or not, these seem likely to proliferate.

==Available upper ontologies==

===Basic Formal Ontology (BFO)===
{{main|Basic Formal Ontology}}
The Basic Formal Ontology (BFO) framework developed by [[Barry Smith (academic and ontologist)|Barry Smith]] and his associates consists of a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: relating to continuant entities such as three-dimensional enduring objects, and occurrent entities (primarily) processes conceived as unfolding in successive phases through time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. A continuant domain ontology descending from BFO can be conceived as an inventory of entities existing at a time. Each occurrent ontology can be conceived as an inventory of processes unfolding through a given interval of time. Both BFO itself and each of its extension sub-ontologies can be conceived as a window on a certain portion of reality at a given level of granularity. More than [http://ifomis.uni-saarland.de/bfo/users 200 extension ontologies] of BFO have been created, applying the BFO architecture to different domains through the strategy of downward population. The Cell Ontology, for example, populates downward from BFO by importing the BFO branch terminating with object, and defining a cell as a subkind of object. Other examples of ontologies extending BFO are the [[Ontology for Biomedical Investigations]] (OBI) and the ontologies of the [[OBO Foundry|Open Biomedical Ontologies Foundry]]. In addition to these examples, BFO and extensions are increasingly being use in defense and security domains, for example in the [http://milportal.org AIRS framework]. BFO serves as the upper level of the Sustainable Development Goals (SDG) Interface Ontology developed by the [http://uneplive.unep.org/portal United Nations Environment Programme]. BFO has been documented in the textbook [http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology], published by MIT Press in 2015.

===BORO===
{{main|BORO}}
Business Objects Reference Ontology is an upper ontology designed for developing ontological or semantic models for large complex operational applications that consists of a top ontology as well as a process for constructing the ontology.  It is built upon a series of clear [[metaphysical choices]]  to provide a solid (metaphysical) foundation. A key choice was for an [[Extension (metaphysics)|extensional]] (and hence, [[Spacetime|four-dimensional]]) [[ontology]] which provides it a simple [[criteria of identity]]. Elements of it have appeared in a number of standards. For example, the ISO standard, [[ISO 15926]] &#8211; Industrial automation systems and integration &#8211; was heavily influenced by an early version. The [[IDEAS Group|IDEAS]] (International Defence Enterprise Architecture Specification for exchange) standard is based upon BORO, which in turn was used to develop [[DODAF]] 2.0.

===CIDOC Conceptual Reference Model===
{{main|CIDOC Conceptual Reference Model}}
Although "CIDOC object-oriented Conceptual Reference Model" (CRM) is a [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontology]], specialised to the purposes of representing cultural heritage, a subset called CRM Core is a generic upper ontology, including:&lt;ref&gt;{{cite web|title=Graphical Representation of core CRM form|url=http://www.cidoc-crm.org/cidoc_core_graphical_representation/graphical_representation.html|publisher=[[CIDOC]]}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Definition of the CIDOC Conceptual Reference Model, Version 5.0.4|url=http://www.cidoc-crm.org/html/5.0.4/cidoc-crm.html#_Toc310250785|publisher=[[CIDOC]]|date=November 2011}}&lt;/ref&gt;
* Space-Time &#8211; title/identifier, place, era/period, time-span, relationship to persistent items
* Events &#8211; title/identifier, beginning/ending of existence, participants (people, either individually or in groups), creation/modification of things (physical or conceptional), relationship to persistent items
* Material Things &#8211; title/identifier, place, the information object the material thing carries, part-of relationships, relationship to persistent items
* Immaterial Things &#8211; title/identifier, information objects (propositional or symbolic), conceptional things, part-of relationships

A persistent item is a physical or conceptional item that has a persistent identity recognized within the duration of its existence by its identification rather than by its continuity or by observation. A persistent item is comparable to an endurant.&lt;br/&gt;A propositional object is a set of statements about real or imaginary things.&lt;br/&gt;A symbolic object is a sign/symbol or an aggregation of signs or symbols.

===COSMO===
[[Common Semantic Model|COSMO]] (COmmon Semantic MOdel, available at http://micra.com/COSMO/COSMO.owl) is an ontology that was initiated as a project of the COSMO working group of the Ontology and taxonomy Coordinating Working Group, with the goal of developing a foundation ontology that can serve to enable broad general [[Semantic Interoperability]]. The current version is an OWL ontology, but a Common-Logic compliant version is anticipated in the future. The ontology and explanatory files are available at the COSMO site. The goal of the COSMO working group was to develop a foundation ontology by a collaborative process that will allow it to represent all of the basic ontology elements that all members feel are needed for their applications. The development of COSMO is fully open, and any comments or suggestions from any sources are welcome. After some discussion and input from members in 2006, the development of the COSMO has been continued primarily by Patrick Cassidy, the chairman of the COSMO Working Group. Contributions and suggestions from any interested party are still welcome and encouraged. Many of the types (OWL classes) in the current COSMO have been taken from the OpenCyc OWL version 0.78, and from the SUMO. Other elements were taken from other ontologies (such as BFO and DOLCE), or developed specifically for COSMO. Development of the COSMO initially focused on including representations of all of the words in the [[Longman Dictionary of Contemporary English]] (LDOCE) controlled [[defining vocabulary]] (2148 words). These words are sufficient to define (linguistically) all of the entries in the LDOCE. It is hypothesized that the ontological representations of the concepts represented by those terms will be sufficient to specify the meanings of any specialized ontology element, thereby serving as a basis for general [[Semantic Interoperability]].  Interoperability via COSMO is enabled by using the COSMO (or an ontology derived from it) as an interlingua by which other domain ontologies can be translated into each other's terms and thereby accurately communicate. As new domains are linked into COSMO, additional semantic primitives may be recognized and added to its structure.  The current (January 2016) OWL version of COSMO has over 8000 types (OWL classes), over 1000 relations, and over 3000 restrictions.  The COSMO itself (COSMO.owl) and other related and explanatory files can be obtained at http://micra.com/COSMO.

===Cyc===
{{main|Cyc}}
A well-known and quite comprehensive ontology available today is [[Cyc]], a proprietary system under development since 1986, consisting of a foundation ontology and several domain-specific ontologies (called ''microtheories''). A subset of that ontology has been released for free under the name [[Cyc#OpenCyc|OpenCyc]], and a more or less unabridged version is made available for free non-commercial use under the name [[Cyc#ResearchCyc|ResearchCyc]].

=== DOLCE ===
Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) is the first module of the WonderWeb foundational ontologies library,&lt;ref&gt;http://www.loa-cnr.it/old/Papers/D18.pdf&lt;/ref&gt; developed by Nicola Guarino and his associates at the Laboratory for Applied Ontology (LOA). As implied by its acronym, DOLCE has a clear ''cognitive bias'', in that it aims at capturing the ontological categories underlying [[natural language]] and human [[common sense]]. DOLCE, however, does not commit to a strictly [[referentialist]] metaphysics related to the intrinsic nature of the world. Rather, the categories it introduces are thought of as cognitive artifacts, which are ultimately depending on human perception, cultural imprints and social conventions. In this sense, they intend to be just ''descriptive'' (vs ''prescriptive'') notions, that assist in making already formed conceptualizations explicit.

===General Formal Ontology (GFO)===
{{main|General formal ontology}}
The general formal ontology (GFO), developed by Heinrich Herre and his colleagues of the research group Onto-Med in [[Leipzig]], is a realistic ontology integrating processes and objects. It attempts to include many aspects of recent philosophy, which is reflected both in its taxonomic tree and its axiomatizations. GFO allows for different axiomatizations of its categories (such as the existence of [[atomic time-interval]]s vs. [[dense time]]). The basic principles of GFO are published in the Onto-Med Report Nr. 8 and in "General Formal Ontology (GFO): A Foundational Ontology for Conceptual Modelling".&lt;ref&gt;http://www.onto-med.de/Archiv/ontomed2002/en/publications/scientific-reports/om-report-no8.pdf&lt;/ref&gt;&lt;ref&gt;http://www.onto-med.de/publications/2010/gfo-basic-principles.pdf&lt;/ref&gt;

Two GFO specialties, among others, are its account of persistence and its time model. Regarding persistence, the distinction between endurants (objects) and perdurants (processes) is made explicit within GFO by the introduction of a special category, a persistent&lt;!-- [sic]?, not persistAnt? --&gt;.&lt;ref&gt;http://www.onto-med.de/en/theories/gfo/part1/node20.html&lt;/ref&gt; A persistant&lt;!-- [sic], not persistEnt --&gt; is a special category with the intention that its instances "remain identical" (over time). With respect to time, time intervals are taken as primitive in GFO, and time-points (called "time boundaries") as derived. Moreover, time-points may coincide, which is convenient for modelling instantaneous changes.

===gist===

gist is developed and supported by Semantic Arts.  gist (not an acronym &#8211; it means to get the essence of) is a &#8220;minimalist upper ontology&#8221;.  gist is targeted at enterprise information systems, although it has been applied to healthcare delivery applications.  
The major attributes of gist are:
# it is small (there are 140 classes and 127 properties)
# it is comprehensive (most enterprises will not find the need to create additional primitive classes, but will find that most of their classes can be defined and derived from gist)
# it is robust &#8211; all the classes descend from 12 primitive classes, which are mostly mutually disjoint.  This aids a great deal in subsequent error detection.  There are 1342 axioms, and it uses almost all of the DL constructs (it is SROIQ(D) )
# it is concrete &#8211; most upper ontologies start with abstract philosophical concepts that users must commit to in order to use the ontology.  Gist starts with concrete classes that most people already do, or reasonably could agree with, such as Person, Organization, Document, Time, UnitOfMeasure and the like) 
# it is unambiguous &#8211; ambiguous terms (such as &#8220;term&#8221;) have been removed as they are often overloaded and confused.  Also terms that frequently have different definitions at different enterprises (such as customer and order) have been removed, also to reduce ambiguity.
# it is understandable &#8211; in addition to being built on concrete, generally understood primitives, it is extremely modular.  The 140 classes are implemented in 18 modular ontologies, each can easily be understood in its entirety, and each imports only the other modules that it needs. 
gist has been used to build Enterprise Ontologies for a number of major commercial and governmental agencies including:  Procter &amp; Gamble, Sentara Healthcare, Washington State Department of Labor &amp; Industries, LexisNexis, Sallie Mae and two major Financial Services firms.
gist is freely available with a Creative Commons share alike license.  There are 18 small ontologies that make up gist.  Gist can be downloaded all at once by loading or importing gistCore at gist7.   
gist is actively maintained, and has been in use for 10 years. As of May 2015 it is at version 7.1.1.&lt;ref&gt;{{cite web|url=http://semanticarts.com/gist|title=gist home page|author=Semantic Arts}}&lt;/ref&gt;

gist was the subject of a paper exploring how to bridge modeling differences between ontologies &lt;ref&gt;{{cite web|url=http://www.researchgate.net/profile/Anthony_Cohn/publication/221235042_Utility_Ontology_Development_with_Formal_Concept_Analysis/links/0912f50cbb29adba1f000000.pdf#page=163|title=Complexity of Reasoning with Expressive Ontology Mappings|author1=Chiara Ghidini |author2=Luciano Serafini |author3=Segio Tessaris }}&lt;/ref&gt;
In a paper describing the OQuaRE methodology for evaluating ontologies, the gist unit of measure ontology scored the highest in the manual evaluation against 10 other unit of measure ontologies,  and scored above average in the automated evaluation.  The authors stated "This ontology could easily be tested and validated, its knowledge could be effectively reused and adapted for different specified environments" &lt;ref&gt;{{cite web|url=http://www.acs.org.au/__data/assets/pdf_file/0015/14118/JRPIT43.2.159.pdf|title=OQuaRE: A SQuaRE-based Approach for Evaluating the Quality of Ontologies|author1=Astrid Duque-Ramos  |author2=Jesualdo Tomas Fernandez-Breis |lastauthoramp=yes }}&lt;/ref&gt;

===IDEAS===
The upper ontology developed by the [[IDEAS Group]] is [[higher-order]], [[extensional]] and [[4D ontology|4D]]. It was developed using the [[BORO Method]]. The IDEAS ontology is not intended for reasoning and inference purposes; its purpose is to be a precise model of business.

===ISO 15926===
{{main|ISO 15926}}
ISO 15926 is an International Standard for the representation of process plant life-cycle information. This representation is specified by a generic, conceptual data model that is suitable as the basis for implementation in a shared database or data warehouse. The data model is designed to be used in conjunction with reference data: standard instances that represent information common to a number of users, process plants, or both. The support for a specific life-cycle activity depends on the use of appropriate reference data in conjunction with the data model. To enable integration of life-cycle information the model excludes all information constraints that are appropriate only to particular applications within the scope.
ISO 15926-2 defines a generic model with 201 entity types. It has been prepared by Technical Committee ISO/TC 184, Industrial automation systems and integration, Subcommittee SC 4, Industrial data.

===MarineTLO===
MarineTLO is an upperontology for the marine domain (also applicable to the terrestrial domain), developed by the Information Systems Laboratory at the Institute of Computer Science,
Foundation for Research and Technology - Hellas ([[FORTH-ICS]]).
Its purpose is to tackle the need for having integrated sets of facts about marine species,
and thus to assist research about species and [[biodiversity]].
It provides a unified and coherent core model for schema mapping which enables formulating and
answering queries which cannot be answered by any individual source.&lt;ref&gt;{{cite web|url=http://www.ics.forth.gr/isl/MarineTLO|title=MarineTLO - A Top Level Ontology for the Marine/Biodiversity Domain|work=forth.gr|accessdate=22 April 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author=Tzitzikas, Y. and Alloca, C. and Bekiari, C. and Marketakis, Y. and Fafalios, P. and Doerr, M. and Minadakis, N. and Patkos, T. and Candela, L.|title=Integrating Heterogeneous and Distributed Information about Marine Species through a Top Level Ontology|url=http://link.springer.com/chapter/10.1007/978-3-319-03437-9_29|location=Institute of Computer Science, FORTH-ICS, Greece|publisher=Springer|year=2013|pages=289&#8211;301|doi=10.1007/978-3-319-03437-9_29|journal=Communications in Computer and Information Science}}&lt;/ref&gt;

===PROTON===
PROTON (PROTo ONtology) is a basic [[subsumption hierarchy]] which provides coverage of most of the upper-level concepts   necessary for semantic annotation, indexing, and retrieval.{{citation needed|date=November 2014}}

===SUMO (Suggested Upper Merged Ontology)===
{{main|Suggested Upper Merged Ontology}}
The [[Suggested Upper Merged Ontology]] (SUMO) is another comprehensive ontology project.  It includes an [[Standard upper ontology|upper ontology]], created by the [[IEEE]] working group P1600.1 (originally by [[Ian Niles]] and [[Adam Pease]]). It is extended with many domain ontologies and a complete set of links to WordNet. It is open source.

===UMBEL===
{{main|UMBEL}}
Upper Mapping and Binding Exchange Layer ([[UMBEL]]) is an ontology of 28,000 reference concepts that maps to a simplified subset of the [[OpenCyc]] ontology, that is intended to provide a way of linking the precise OpenCyc ontology with less formal ontologies.&lt;ref&gt;{{cite web|url=http://www.mkbergman.com/441/the-role-of-umbel-stuck-in-the-middle-with-you/|title=The Role of UMBEL: Stuck in the Middle with You . . .|author=Mike Bergman|accessdate=2010-10-26}}&lt;/ref&gt; It also has formal mappings to [[Wikipedia]], [[DBpedia]], [[PROTON Ontology|PROTON]] and [[GeoNames]]. It has been developed and maintained as [[open source]] by Structured Dynamics.

===UFO (Unified Foundational Ontology)===
The Unified Foundational Ontology (UFO), developed by Giancarlo Guizzardi and associates, incorporating developments from GFO, DOLCE and the Ontology of Universals underlying OntoClean in a single coherent foundational ontology. The core categories of UFO (UFO-A) have been completely formally characterized in Giancarlo Guizzardi's Ph.D. thesis and further extended at the Ontology and Conceptual Modelling Research Group (NEMO) in Brazil with cooperators from Brandenburg University of Technology (Gerd Wagner) and Laboratory for Applied Ontology (LOA). UFO-A has been employed to analyze structural conceptual modeling constructs such as object types and taxonomic relations, associations and relations between associations, roles, properties, datatypes and weak entities, and parthood relations among objects. More recent developments incorporate an ontology of events in UFO (UFO-B), as well as an ontology of social and intentional aspects (UFO-C). The combination of UFO-A, B and C has been used to analyze, redesign and integrate reference conceptual models in a number of complex domains such as, for instance, Enterprise Modeling, Software Engineering, Service Science, Petroleum and Gas, Telecommunications, and Bioinformatics. Another recent development aimed towards a clear account of services and service-related concepts, and provided for a commitment-based account of the notion of service (UFO-S),&lt;ref&gt;Nardi, J. C., Falbo, R. D. A., Almeida, J. P. A., Guizzardi, G., Pires, L. F., van Sinderen, M. J., &amp; Guarino, N. (2013, September). "Towards a commitment-based reference ontology for services". In Enterprise Distributed Object Computing Conference (EDOC), 2013 17th IEEE International (pp. 175-184). IEEE.&lt;/ref&gt;
UFO is the foundational ontology for [[OntoUML]], an ontology modeling language.

===WordNet===
{{main|WordNet}}
[[WordNet]], a freely available database originally designed as a [[semantic network]] based on [[psycholinguistic]] principles, was expanded by addition of definitions and is now also viewed as a [[dictionary]]. It qualifies as an upper ontology by including the most general concepts as well as more specialized concepts, related to each other not only by the [[subsumption relation]]s, but by other semantic relations as well, such as part-of and cause. However, unlike Cyc, it has not been formally axiomatized so as to make the logical relations between the concepts precise. It has been widely used in [[Natural language processing]] research.

===YAMATO (Yet Another More Advanced Top Ontology)===
YAMATO is developed by Riichiro Mizoguchi, formerly at the Institute of Scientific and Industrial Research of the [[University of Osaka]], and now at the [[Japan Advanced Institute of Science and Technology]]. Major features of YAMATO are:
# an advanced description of quality, attribute, property, and quantity,&lt;ref&gt;http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/YAMATO101216.pdf&lt;/ref&gt; 
# an ontology of representation,&lt;ref&gt;{{cite journal|url=http://link.springer.com/article/10.1007/BF03040960#page-1|title=Part 3: Advanced course of ontological engineering|work=springer.com|accessdate=22 April 2015|doi=10.1007/BF03040960|volume=22|pages=193&#8211;220}}&lt;/ref&gt; 
# an advanced description of processes and events,&lt;ref&gt;{{cite journal | doi = 10.3233/AO-2009-0067 }}&lt;/ref&gt; 
# the use of a theory of roles.&lt;ref&gt;{{cite journal | last1 = Mizoguchi | first1 = R. | last2 = Sunagawa | first2 = E. | last3 = Kozaki | first3 = K. | last4 = Kitamura | first4 = Y. | year = | title = A Model of Roles within an Ontology Development Tool: Hozo | url = http://iospress.metapress.com/content/w67u25284x0l206v/ | journal = J. of Applied Ontology | volume = 2 | issue = 2| pages = 159&#8211;179 }}&lt;/ref&gt;

YAMATO has been extensively used for developing other, more applied, ontologies such as a medical ontology,&lt;ref&gt;http://ceur-ws.org/Vol-833/paper9.pdf&lt;/ref&gt; an ontology of gene,&lt;ref&gt;http://ceur-ws.org/Vol-897/session1-paper05.pdf&lt;/ref&gt; an ontology of learning/instructional theories,&lt;ref&gt;{{cite web|url=http://edont.qee.jp/omnibus/doku.php|title=start    [OMNIBUS project]|date=6 December 2014|work=qee.jp|accessdate=22 April 2015}}&lt;/ref&gt; an ontology of sustainability science,&lt;ref&gt;{{cite web|url=http://link.springer.com/article/10.1007%2Fs11625-008-0063-z|title=Toward knowledge structuring of sustainability science based on ontology engineering|work=springer.com|accessdate=22 April 2015}}&lt;/ref&gt; and an ontology of the cultural domain.

== Upper/Foundational Ontology tools==

===ONSET===
{{unreferenced section|date=November 2014}}
ONSET, the foundational ontology selection and explanation tool, assists the domain ontology developer in selecting the most appropriate foundational ontology. The domain ontology developer provides the requirements/answers one or more questions, and ONSET computes the selection of the appropriate foundational ontology and explains why. The current version (v2 of 24 April 2013) includes DOLCE, BFO, GFO, SUMO, YAMATO and GIST.

===ROMULUS===
{{unreferenced section|date=November 2014}}
ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO. Features of ROMULUS include:
# It provides a high-level view of the foundational ontologies with only the most general concepts common to all implemented foundational ontologies. 
# Foundational ontologies in ROMULUS are modularised.
# Foundational ontology mediation has been performed. This includes alignment, mapping, merging, searchable metadata and an interchangeability method for foundational ontologies. 
# ROMULUS provides detailed taxonomies of each foundational ontology to allow easy browsing of foundational ontologies. 
# ROMULUS allows you to download each foundational ontology module including the integrated foundational ontologies. 
# Searchable metadata of each foundational ontology is available. 
# A comparison of the included foundational ontologies is available.

==See also==
* [[Authority control]]
* [[Formal ontology]]
* [[Foundations of mathematics]]
* [[Knowledge Organization Systems]]
* [[Library classification]]
* [[Ontology (information science)]]
* [[Physical ontology]]
* [[Process ontology]]
* [[Semantic interoperability]]
* [[Commonsense knowledge]]

==References==
{{Reflist}}

==External links==
{{External links|date=April 2015}}
* [http://www.onto-med.de/ontologies/gfo General Formal Ontology (GFO) homepage]
* [http://www.loa.istc.cnr.it/ Laboratory of Applied Ontology (LOA) homepage]
* [http://proton.semanticweb.org/ PROTON Ontology]
* [http://ontolog.cim3.net/cgi-bin/wiki.pl?UpperOntologySummit Upper Ontology Summit (March 2006)]
* [http://ontogenesis.knowledgeblog.org/740 What is an upper level ontology?] Knowledge Blog article, 2010.
* [http://www.ics.forth.gr/isl/MarineTLO/ The MarineTLO ontology] What, Why, Who, Current applications, How to exploit it, Documents and Publications, Provide feedback.
* [http://www.thezfiles.co.za/ROMULUS/Onset/webonset.html ONSET]
* [http://www.thezfiles.co.za/ROMULUS/ ROMULUS]
{{Computable knowledge}}

{{DEFAULTSORT:Upper Ontology (Information Science)}}
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]</text>
      <sha1>ds4gwz7ptxy53smetwh9ifeqx6liy1c</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Minimum Information Standards</title>
    <ns>14</ns>
    <id>23474509</id>
    <revision>
      <id>389242289</id>
      <parentid>376492840</parentid>
      <timestamp>2010-10-07T03:57:17Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Merging catmore1/catmore2 per [[Wikipedia:Templates for discussion/Log/2010 September 10|TFD]], and renaming catmore/catmore2 per [[Template talk:cat main|discussion]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="112" xml:space="preserve">{{Cat main|Minimum Information Standards}}

[[Category:Knowledge representation]]
[[Category:Standards by type]]</text>
      <sha1>ctsudpv127aivs1av1r0x9nfpnj2co9</sha1>
    </revision>
  </page>
  <page>
    <title>GNOWSYS</title>
    <ns>0</ns>
    <id>450307</id>
    <revision>
      <id>706276314</id>
      <parentid>706263530</parentid>
      <timestamp>2016-02-22T12:29:57Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <comment>Rescuing orphaned refs ("gnu" from rev 671823950)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5338" xml:space="preserve">{{Refimprove|date=June 2011}}
{{Infobox software
|  name = GNOWSYS
|  logo = [[Image:Gnowsys-logo.png|100px]]
|  developer = [[GNU|The GNU Project]]
|  latest_release_version = 1.0 rc1
|  operating_system = [[Cross-platform]]
|  genre = [[Semantic web|Semantic computing]]
|  license = [[GNU General Public License|GPL]]
|  website = [https://www.gnu.org/software/gnowsys/ www.gnu.org/software/gnowsys/]
}}
'''GNOWSYS''' (Gnowledge Networking and Organizing system) is a specification for a generic [[distributed network]] based memory/[[knowledge management]]. It is developed as an application for developing and maintaining [[semantic web]] content. It is written in [[Python (programming language)|Python]]. It is implemented as a [[Django (web framework)|Django]] app.

The memory of GNOWSYS is designed as a node-oriented space. A node is described by other nodes to which it has links. The nodes are organized and processed according to a complex data structure called the neighborhood.&lt;ref name="gnu"&gt;[https://www.gnu.org/software/gnowsys/] GNOWSYS: A Kernel for Semantic Computing.&lt;/ref&gt;

==Applications==

The application can be used for web-based knowledge representation and content management projects, for developing structured knowledge bases, as a collaborative authoring tool, suitable for making electronic glossaries, dictionaries and encyclopedias, for managing large web sites or links, developing an online catalogue for a library of any thing including books, to make ontologies, classifying and networking any objects, etc. This tool is also intended to be used for an on-line tutoring system with dependency management between various concepts or software packages.  For example, the dependency relations between [[Debian GNU/Linux]] packages have been represented by the [http://www.gnowledge.org/search_debmap?val=1 gnowledge portal].

==Component Classes==
The kernel is designed to provide support to persistently store highly granular nodes of knowledge representation like terms, predicates and very complex propositional systems like arguments, rules, axiomatic systems, loosely held paragraphs, and more complex structured and consistent compositions. All the component classes in GNOWSYS are classified according to complexity into three groups, where the first two groups are used to express all possible well formed formulae permissible in a first order logic.&lt;ref name="conceptPaper"&gt;[http://www.hbcse.tifr.res.in/gn/concept_paper.pdf GNOWSYS: A System for Semantic Computing ]&lt;/ref&gt;

===Terms===
&#8216;Object&#8217;, &#8216;Object Type&#8217; for declarative knowledge, &#8216;Event&#8217;, &#8216;Event Type&#8217;, for temporal objects, and &#8216;Meta Types&#8217; for expressing upper ontology. The
objects in this group are essentially any thing about which the [[knowledge engineer]] intends to express and store in the knowledge base, i.e., they are the objects of discourse. The instances of these component classes can be stored with or without expressing &#8216;instance of&#8217; or &#8216;sub-class of&#8217; relations among them.

===Predicates===
This group consists of &#8216;Relation&#8217;, and &#8216;Relation Type&#8217; for expressing declarative knowledge, and &#8216;Function&#8217; and &#8216;Function Type&#8217; for expressing procedural knowledge. This group is to express qualitative and quantitative relations among the various instances stored in the knowledge base. While instantiating the predicates can be characterized by their logical properties of relations, quantifiers and cardinality as monadic predicates
of these predicate objects.

===Structures===
&#8216;System&#8217;, &#8216;Encapsulated Class&#8217;, &#8216;Program&#8217;, and &#8216;Process&#8217;, are other base classes for complex structures, which can be combined iteratively to produce more complex systems. The component class &#8216;System&#8217; is to store in the knowledge base a set of propositions composed into ontologies, axiomatic systems, complex systems like say a human body, an artifact like a vehicle etc., with or without consistency check. An &#8216;Encapsulated Class&#8217; is to com-
pose declarative and behavioural objects in a flexible way to build classes. A &#8216;Program&#8217; is not only to store the logic of any complete program or a component class, composed from the already available behavioural instances in the knowledge base with built-in connectives (conditions, and loops), but also execute them as web services. A &#8216;Process&#8217; is to structure temporal objects with sequence, concurrency, synchronous or asynchronous specifications.

Every node in the database keeps the neighbourhood information, such as its super-class, sub-class, instance-of, and other relations, in which the object has a role, in the form of predicates. This feature makes computation of drawing graphs and inferences, on the one hand, and dependency and navigation paths on the other hand very easy.  All the data and metadata is indexed in a central catalogue making query and locating resources efficient.

==References==
{{Reflist}}

==External links==
{{Portal|Free software}}
* [http://www.gnowledge.org/ Welcome to Gnowledge!]
* [https://www.gnu.org/software/gnowsys/ GNOWSYS is part of the GNU project.]

{{GNU}}

{{DEFAULTSORT:Gnowsys}}
[[Category:Cross-platform free software]]
[[Category:Free network-related software]]
[[Category:GNU Project software]]
[[Category:Knowledge representation]]
[[Category:Semantic Web]]</text>
      <sha1>l5p2e8q32yapcqows945ig1uo0quf36</sha1>
    </revision>
  </page>
  <page>
    <title>Lumpers and splitters</title>
    <ns>0</ns>
    <id>558750</id>
    <revision>
      <id>751138146</id>
      <parentid>744553155</parentid>
      <timestamp>2016-11-23T16:35:03Z</timestamp>
      <contributor>
        <ip>163.1.120.19</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10482" xml:space="preserve">{{Refimprove|date=January 2014}}

'''Lumpers''' and '''splitters''' are opposing factions in any [[discipline]] that has to [[Categorization|place individual examples into rigorously defined categories]]. The lumper-splitter problem occurs when there is the need to create classifications and assign examples to them, for example schools of [[literature]], [[biology|biological]] [[taxon|taxa]] and so on. A "lumper" is an individual who takes a [[wikt:gestalt|gestalt]] view of a definition, and assigns examples broadly, assuming that differences are not as important as signature similarities. A "splitter" is an individual who takes precise definitions, and creates new categories to classify samples that differ in key ways.

==Origin of the terms==
The earliest use of these terms was by [[Charles Darwin]], in a letter to [[J. D. Hooker]] in 1857, "(Those who make many species are the 'splitters,' and those who make few are the 'lumpers.')" They were introduced more widely by [[George G. Simpson]] in his 1945 work "The Principles of Classification and a Classification of Mammals." As he put it, "splitters make very small units &#8211; their critics say that if they can tell two animals apart, they place them in different genera &#8230; and if they cannot tell them apart, they place them in different species. &#8230; Lumpers make large units &#8211; their critics say that if a [[Carnivora|carnivore]] is neither a dog nor a bear, they call it a cat."&lt;ref&gt;{{Cite journal|last=Simpson|first=George G.|title=The Principles of Classification and a Classification of Mammals|journal=Bulletin of the AMNH|volume=85|page=23|year=1945|publisher=American Museum of Natural History|location=New York}}&lt;/ref&gt;

Another early use can be found in the title of a 1969 paper by the medical geneticist [[Victor McKusick]]: "On lumpers and splitters, or the nosology of genetic disease."&lt;ref&gt;McKusick VA. On lumpers and splitters, or the nosology of genetic disease. Perspect Biol Med. 1969 Winter;12(2):298-312.&lt;/ref&gt;

Reference to lumpers and splitters also appeared in a debate in 1975 between [[J. H. Hexter]] and [[John Edward Christopher Hill|Christopher Hill]], in the ''[[Times Literary Supplement]]''. It followed from Hexter's detailed review of Hill's book ''Change and Continuity in Seventeenth Century England'', in which Hill developed [[Max Weber]]'s argument that the rise of capitalism was facilitated by [[Calvinist]] Puritanism. Hexter objected to Hill's 'mining' of sources to find evidence that supported his theories. Hexter argued that Hill plucked quotations from sources in a way that distorted their meaning. Hexter explained this as a mental habit that he called 'lumping'. According to him, lumpers rejected differences and chose to emphasize similarities. Any evidence that did not fit their arguments was ignored as aberrant. Splitters, by contrast, emphasised differences, and resisted simple schemes. While lumpers consistently tried to create coherent patterns, splitters preferred incoherent complexity.&lt;ref&gt;{{Cite journal|last=Chase|first=Bob|title=Upstart Antichrist|journal=History Workshop Journal|issue=60|date=Autumn 2005|pages=202-206}}&lt;/ref&gt;

==Usage in various fields==

===Biology===
{{anchor|Lumping and splitting in biology}}
{{main|Biological classification}}
The categorization and naming of a particular species should be regarded as a ''hypothesis'' about the [[evolution]]ary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual [[organism]]s later identified as the same species. When two named species are agreed to be of the same species, the older species name is almost always retained dropping the newer species name honoring a convention known as "priority of nomenclature".  This form of lumping is technically called synonymization. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognizing differences or commonalities between organisms.

=== History ===
{{main|Periodization}}

In history, lumpers are those who tend to create broad definitions that cover large periods of time and many disciplines, whereas splitters want to assign names to tight groups of inter-relationships. Lumping tends to create a more and more unwieldy definition, with members having less and less mutually in common. This can lead to definitions which are little more than conventionalities, or groups which join fundamentally different examples. Splitting often leads to "[[Distinction without a difference|distinctions without difference]]," ornate and fussy categories, and failure to see underlying similarities.

For example, in the arts, "[[Romanticism|Romantic]]" can refer specifically to a period of [[Germany|German]] poetry roughly from 1780&#8211;1810, but would exclude the later work of [[Goethe]], among other writers. In music it can mean every composer from [[Johann Nepomuk Hummel|Hummel]] through [[Sergei Rachmaninoff|Rachmaninoff]], plus many that came after.

=== Software modelling ===
[[Software engineering]] often proceeds by building models (sometimes known as [[model-driven architecture]]). A lumper is keen to generalize, and produces models with a small number of broadly defined objects. A splitter is reluctant to generalize, and produces models with a large number of narrowly defined objects. Conversion between the two styles is not necessarily symmetrical. For example, if error messages in two narrowly defined classes behave in the same way, the classes can be easily combined. But if some messages in a broad class behave differently, every object in the class must be examined before the class can be split. This illustrates the principle that "splits can be lumped more easily than lumps can be split."&lt;ref name=Pugh2005&gt;{{cite book|last1=Pugh|first1=Ken|title=Prefactoring|date=2005|publisher=O'Reilly Media|pages=14&#8211;15|url=https://books.google.com/books?id=8nykB7qerJYC&amp;pg=PA15#v=onepage&amp;q&amp;f=false|accessdate=2014-10-21}}&lt;/ref&gt;

=== Language classification ===
{{main|Language classification}}

There is no agreement among [[Historical linguistics|historical linguists]] about what amount of evidence is needed for two languages to be safely classified in the same [[language family]]. For this reason, many language families have had lumper&#8211;splitter controversies, including [[Altaic languages|Altaic]], [[Pama&#8211;Nyungan languages|Pama&#8211;Nyungan]], [[Nilo-Saharan]], and most of the larger [[Classification schemes for indigenous languages of the Americas|families of the Americas]]. At a completely different level, the splitting of a [[mutually intelligible]] [[dialect continuum]] into different languages, or lumping them into one, is also an issue that continually comes up, though the consensus in contemporary linguistics is that there is no completely objective way to settle the question.

Splitters regard the [[comparative method (linguistics)|comparative method]] (meaning not comparison in general, but only reconstruction of a common ancestor or [[protolanguage]]) as the only valid proof of kinship, and consider [[genetic (linguistics)|genetic]] relatedness to be the question of interest. American linguists of recent decades tend to be splitters.

Lumpers are more willing to admit techniques like [[mass lexical comparison]] or [[lexicostatistics]], and mass typological comparison, and to tolerate the uncertainty of whether relationships found by these methods are the result of [[linguistic divergence]] (descent from common ancestor) or [[language convergence]] (borrowing). Much long-range comparison work has been from Russian linguists like [[Vladislav Illich-Svitych]] and [[Sergei Starostin]]. In the US, [[Joseph Greenberg|Greenberg]]'s and [[Merritt Ruhlen|Ruhlen]]'s work has been met with little acceptance from linguists. Earlier American linguists like [[Morris Swadesh]] and [[Edward Sapir]] also pursued large-scale classifications like [[Classification schemes for indigenous languages of the Americas#Sapir .281929.29: Encyclop.C3.A6dia Britannica|Sapir's 1929 scheme for the Americas]], accompanied by controversy similar to that today.&lt;ref&gt;http://www.nostratic.ru/books/(137)ruhlen12.pdf [[Merritt Ruhlen]]: Is Algonquian Amerind?&lt;/ref&gt;

=== Liturgical studies ===
[[Paul F. Bradshaw]] suggests that the same principles of lumping and splitting apply to the study of early Christian [[liturgy]]. Lumpers, who tend to predominate, try to find a single line of texts from the apostolic age to the fourth century (and later). Splitters see many parallel and overlapping strands which intermingle and flow apart so that there is not a single coherent path in development of liturgical texts. Liturgical texts must not be taken solely at face value; often there are hidden agendas in texts.&lt;ref name="bradshaw"&gt;Bradshaw, Paul F., ''The Search for the Origins of Christian Worship'', Oxford University Press, 2002, p. ix. ISBN 0-19-521732-2&lt;/ref&gt;

The Hindu religion is essentially a lumper's concept, sometimes also known as [[Smartism]].  Hindu splitters, and individual adherents, often identify themselves as adherents of a religion such as [[Shaivism]], [[Vaishnavism]], or [[Shaktism]] according to which deity they believe to be the supreme creator of the universe.{{Citation needed|date=June 2012}}

===Philosophy===
[[Freeman Dyson]] has suggested that "observers of the philosophical scene" can be broadly, if over-simplistically, divided into splitters and lumpers, roughly corresponding to [[materialists]], who imagine the world as divided into atoms, and [[Platonists]], who regard the world as made up of ideas.

== See also ==
* [[Evolutionary biology]]
* [[Heterarchy]]
* [[Prototype theory]]
* [[Sorites paradox]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.users.globalnet.co.uk/~rxv/infomgt/abstraction.htm#lumpersplitter Abstraction: Lumpers and Splitters]
* [http://www.tvtropes.org/pmwiki/pmwiki.php/Main/LumperVsSplitter Lumper Vs. Splitter] on [[TV Tropes|TV Tropes, a wiki dedicated to recurring themes in fiction, metafiction, and real life]]

{{DEFAULTSORT:Lumpers And Splitters}}
[[Category:Knowledge representation]]
[[Category:Taxonomy]]</text>
      <sha1>gwgs0a68juiiqoell17o8657zi86siw</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Ontology (information science)</title>
    <ns>14</ns>
    <id>26259157</id>
    <revision>
      <id>644097920</id>
      <parentid>608480592</parentid>
      <timestamp>2015-01-25T13:12:39Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="176" xml:space="preserve">{{Cat main|Ontology (information science)}}
{{Commons category|Ontology}}

[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Ontology|Information science]]</text>
      <sha1>i6ccbiuwms726an8dfvzqsqnot9de04</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge value chain</title>
    <ns>0</ns>
    <id>5118075</id>
    <revision>
      <id>742241608</id>
      <parentid>588550636</parentid>
      <timestamp>2016-10-02T14:57:19Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Information, knowledge, and uncertainty]], this is a subfield of microeconomics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2302" xml:space="preserve">A '''[[knowledge value]] chain''' is a sequence of intellectual tasks by which [[knowledge workers]] build their employer's unique competitive advantage &lt;ref&gt;Carlucci, D., Marr, B. and Schiuma, G. (2004) 'The knowledge value chain: how intellectual capital impacts on business performance', ''Int J. Technology Management'', Vol. 27, Nos. 6/7, pp. 575&amp;ndash;690  [http://www.som.cranfield.ac.uk/som/dinamic-content/research/cbp/2004,%20Knowledge%20Value%20Chain%20(IJTM%2027,%206-7,%20Carlucci,%20Marr,%20Schiuma).pdf (pdf)]&lt;/ref&gt; and/or social and environmental benefit. As an example, the components of a research and development project form a knowledge value chain.

Productivity improvements in a knowledge value chain may come from [[knowledge integration]] in its original sense of data systems consolidation. Improvements also flow from the knowledge integration that occurs when [[knowledge management]] techniques are applied to the continuous improvement of a business process or processes.&lt;ref&gt;[http://www.edgeltd.com/performance-consultants-services/edge_service.php?service=3 Canada Edge Performance Consultants] - official page&lt;/ref&gt;

The term first started coming into common use around 1999, appearing in management-related talks and papers.&lt;ref&gt;[http://www.aurorawdc.com/kmworld99.htm 1999 KMWorld conference program], listing Powell's talk on "The Knowledge Value Chain - Aligning       Knowledge Workers with Competitive Strategy"&lt;/ref&gt;&lt;ref&gt;[http://www.ingentaconnect.com/content/mcb/026/2000/00000019/00000009/art00003 "Knowledge value chain"],''The Journal of Management Development'',            Volume 19, Number 9, 2000, pp. 783&amp;ndash;794(12)&lt;/ref&gt;&lt;ref&gt;Tim Powell, "Knowledge Value Chain", May 2001, Proceeding of 22nd National Online Meeting, Information Today ([http://www.knowledgeagency.com/pdf_center/Knowledge_Value_Chain.pdf pdf)]&lt;/ref&gt; It was registered as a trademark in 2004 by TW Powell Co., a [[Manhattan]] company.&lt;ref&gt;[http://www.knowledgeagency.com TW Powell Co. website]&lt;/ref&gt;&lt;ref&gt;U.S. Trademark, December 2004. 2,912,705&lt;/ref&gt;

'''Knowledge value chain processes'''
*Knowledge acquisition
*Knowledge storage
*Knowledge dissemination
*Knowledge application

==References==
{{reflist}}

{{DEFAULTSORT:Knowledge Value Chain}}
[[Category:Knowledge representation]]</text>
      <sha1>8zd225eevivv2s8qnx1tv4fp56yt96x</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Thesauri</title>
    <ns>14</ns>
    <id>31375917</id>
    <revision>
      <id>606439556</id>
      <parentid>589035962</parentid>
      <timestamp>2014-04-30T06:29:45Z</timestamp>
      <contributor>
        <username>Good Olfactory</username>
        <id>6454287</id>
      </contributor>
      <comment>cat main</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="186" xml:space="preserve">{{cat main|Thesaurus}}
{{Commons category|Thesauri}}
[[Category:Knowledge representation]]
[[Category:Reference works]]
[[Category:Dictionaries by type]]
[[Category:Information science]]</text>
      <sha1>aryv16kjjb3ecg8d80oj35wq3ch301f</sha1>
    </revision>
  </page>
  <page>
    <title>Document classification</title>
    <ns>0</ns>
    <id>1331441</id>
    <revision>
      <id>757804927</id>
      <parentid>754790580</parentid>
      <timestamp>2017-01-01T21:19:13Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2016 July 2|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11467" xml:space="preserve">'''Document classification''' or '''document categorization''' is a problem in [[library science]], [[information science]] and [[computer science]]. The task is to assign a [[document]] to one or more [[Class (philosophy)|classes]] or [[Categorization|categories]]. This may be done "manually" (or "intellectually") or [[algorithmically]]. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.

The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.

Documents may be classified according to their [[Subject (documents)|subjects]] or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.

=="Content-based" versus "request-based" classification==
'''Content-based classification''' is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned.&lt;ref&gt;Library of Congress (2008). The subject headings manual. Washington, DC.: Library of Congress, Policy and Standards Division. (Sheet H 180: "Assign headings only for topics that comprise at least 20% of the work.")&lt;/ref&gt; In automatic classification it could be the number of times given words appears in a document.

'''Request-oriented classification''' (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: &#8220;Under which descriptors should this entity be found?&#8221; and &#8220;think of all the possible queries and decide for which ones the entity at hand is relevant&#8221; (Soergel, 1985, p.&amp;nbsp;230&lt;ref&gt;Soergel, Dagobert (1985). Organizing information: Principles of data base and retrieval systems. Orlando, FL: Academic Press.&lt;/ref&gt;).

Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as ''policy-based classification'': The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.

==Classification versus indexing==
Sometimes a distinction is made between assigning documents to classes ("classification") versus assigning [[Subject (documents)|subjects]] to documents ("[[subject indexing]]") but as [[Frederick Wilfrid Lancaster]] has argued, this distinction is not fruitful. "These terminological distinctions,&#8221; he writes, &#8220;are quite meaningless and only serve to cause confusion&#8221; (Lancaster, 2003, p.&amp;nbsp;21&lt;ref&gt;Lancaster, F. W. (2003). Indexing and abstracting in theory and practice. Library Association, London.&lt;/ref&gt;). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a [[thesaurus]] and vice versa (cf., Aitchison, 1986,&lt;ref&gt;Aitchison, J. (1986). "A classification as a source for thesaurus: The Bibliographic Classification of H. E. Bliss as a source of thesaurus terms and structure." Journal of Documentation, Vol. 42 No. 3, pp. 160-181.&lt;/ref&gt; 2004;&lt;ref&gt;Aitchison, J. (2004). "Thesauri from BC2: Problems and possibilities revealed in an experimental thesaurus derived from the Bliss Music schedule." Bliss Classification Bulletin, Vol. 46, pp. 20-26.&lt;/ref&gt; Broughton, 2008;&lt;ref&gt;Broughton, V. (2008). "A faceted classification as the basis of a faceted terminology: Conversion of a classified structure to thesaurus format in the Bliss Bibliographic Classification (2nd Ed.)." Axiomathes, Vol. 18 No.2, pp. 193-210.&lt;/ref&gt; Riesthuis &amp; Bliedung, 1991&lt;ref&gt;Riesthuis, G. J. A., &amp; Bliedung, St. (1991). "Thesaurification of the UDC." Tools for knowledge organization and the human interface, Vol. 2, pp. 109-117. Index Verlag, Frankfurt.&lt;/ref&gt;). Therefore, is the act of labeling a document (say by assigning a term from a [[controlled vocabulary]] to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).

==Automatic document classification (ADC)==
Automatic document classification tasks can be divided into three sorts: '''supervised document classification''' where some external mechanism (such as human feedback) provides information on the correct classification for documents, '''unsupervised document classification''' (also known as [[document clustering]]), where the classification must be done entirely without reference to external information, and '''semi-supervised document classification''',&lt;ref&gt;
Rossi, R. G., Lopes, A. d. A., and Rezende, S. O. (2016). Optimization and label propagation in bipartite heterogeneous networks to improve transductive classification of texts.
Information Processing &amp; Management, 52(2):217&#8211;257.
&lt;/ref&gt; where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.&lt;ref&gt;[http://www.ling.ohio-state.edu/~kbaker/Automatic_Interactive_Document_Classification.pdf An Interactive Automatic Document Classification Prototype]&lt;/ref&gt;&lt;ref&gt;[https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An Interactive Automatic Document Classification Prototype] {{wayback|url=https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An |date=20150424122349 }}&lt;/ref&gt;&lt;ref&gt;[http://www.artsyltech.com/da_classification.htmlAutomatic Document Classification - Artsyl]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;&lt;ref&gt;[http://www.abbyy.com/ocr_sdk_windows/what_is_new/classification/ ABBYY FineReader Engine 11 for Windows]&lt;/ref&gt;

== Techniques ==
Automatic document classification techniques include:
* [[Expectation maximization]] (EM)
* [[Naive Bayes classifier]]
* [[tf&#8211;idf]]
* [[Instantaneously trained neural networks]]
* [[Latent semantic indexing]]
* [[Support vector machines]] (SVM)
* [[Artificial neural network]]
* [[k-nearest neighbor algorithm|K-nearest neighbour algorithms]]
* [[Decision tree learning|Decision trees]] such as [[ID3 algorithm|ID3]] or [[C4.5 algorithm|C4.5]]
* [[Concept Mining]]
* [[Rough set]]-based classifier
* [[Soft set]]-based classifier
* [[Multiple-instance learning]]
* [[Natural language processing]] approaches

== Applications ==
Classification techniques have been applied to
* [[spam filter]]ing, a process which tries to discern [[E-mail spam]] messages from legitimate emails
* email [[routing]], sending an email sent to a general address to a specific address or mailbox depending on topic&lt;ref&gt;Stephan Busemann, Sven Schmeier and Roman G. Arens (2000). Message classification in the call center. In Sergei Nirenburg, Douglas Appelt, Fabio Ciravegna and Robert Dale, eds., Proc. 6th Applied Natural Language Processing Conf. (ANLP'00), pp. 158-165, ACL.&lt;/ref&gt;
* [[language identification]], automatically determining the language of a text
* genre classification, automatically determining the genre of a text&lt;ref&gt;{{Citation|last = Santini| first = Marina | last2 = Rosso| first2 = Mark| title = Testing a Genre-Enabled Application: A Preliminary Assessment| url = http://www.bcs.org/upload/pdf/ewic_fd08_paper7.pdf| series = BCS IRSG Symposium: Future Directions in Information Access| place = London, UK | pages= 54&#8211;63| year = 2008 }}&lt;/ref&gt;
* [[Readability|readability assessment]], automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger [[text simplification]] system
* [[sentiment analysis]], determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.
* Article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.&lt;ref&gt;{{Cite journal
 | pmid = 18834495
| year = 2008
| author1 = Krallinger
| first1 = M
| title = Overview of the protein-protein interaction annotation extraction task of Bio ''Creative'' II
| journal = Genome Biology
| volume = 9 Suppl 2
| pages = S4
| last2 = Leitner
| first2 = F
| last3 = Rodriguez-Penagos
| first3 = C
| last4 = Valencia
| first4 = A
| doi = 10.1186/gb-2008-9-s2-s4
| pmc = 2559988
}}&lt;/ref&gt;

== See also ==
{{colbegin}}
* [[Categorization]]
* [[Classification (disambiguation)]]
* [[Compound term processing]]
* [[Concept-based image indexing]]
* [[Content-based image retrieval]]
* [[Document]]
* [[Supervised learning]], [[unsupervised learning]]
* [[Document retrieval]]
* [[Document clustering]]
* [[Information retrieval]]
* [[Knowledge organization]]
* [[Knowledge Organization System]]
* [[Library classification]]
* [[Machine learning]]
* [[Native Language Identification]]
* [[String metrics]]
* [[Subject (documents)]]
* [[Subject indexing]]
* [[Text mining]], [[web mining]], [[concept mining]]
{{colend}}

== Further reading ==
* Fabrizio Sebastiani. [http://arxiv.org/pdf/cs.ir/0110053 Machine learning in automated text categorization]. ACM Computing Surveys, 34(1):1&#8211;47, 2002.
* Stefan B&#252;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, 2010.

==References==
{{Reflist}}

== External links ==
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node11.html Introduction to document classification]
* [http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html Bibliography on Automated Text Categorization]
* [http://liinwww.ira.uka.de/bibliography/Ai/query-classification.html Bibliography on Query Classification]
* [http://www.gabormelli.com/RKB/Text_Classification_Task Text Classification] analysis page
* [http://www.nltk.org/book/ch06.html Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python] (available online)
* [http://techtc.cs.technion.ac.il TechTC - Technion Repository of Text Categorization Datasets]
* [http://www.daviddlewis.com/resources/testcollections/ David D. Lewis's Datasets]
* [http://www.biocreative.org/tasks/biocreative-iii/ppi/ BioCreative III ACT (article classification task) dataset]

[[Category:Information science]]
[[Category:Natural language processing]]
[[Category:Knowledge representation]]
[[Category:Data mining]]
[[Category:Machine learning]]</text>
      <sha1>t6n9nih5otbwrayqiruc73uhw9opbhw</sha1>
    </revision>
  </page>
  <page>
    <title>Social History and Industrial Classification</title>
    <ns>0</ns>
    <id>34821434</id>
    <revision>
      <id>704733684</id>
      <parentid>688105394</parentid>
      <timestamp>2016-02-13T07:17:20Z</timestamp>
      <contributor>
        <username>Jabberwoch</username>
        <id>6441698</id>
      </contributor>
      <comment>Adding/improving reference(s)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1986" xml:space="preserve">{{Underlinked|date=April 2014}}

'''Social History and Industrial Classification''' (SHIC) is a classification system used by many British museums for social history and industrial collections.
It was first published in 1983.&lt;ref&gt;{{cite web|title=SHIC Home|url=http://www.holm.demon.co.uk/shic/}}&lt;/ref&gt;

==Purpose==
SHIC was classifies materials (books, objects, recordings etc.) by their interaction with the people who used them. For example, a carpenter's hammer is classified with other tools of the carpenter, and not with a blacksmith's hammer.&lt;ref&gt;{{cite web|title=SHIC Section A|url=http://www.holm.demon.co.uk/shic/shicint.htm}}&lt;/ref&gt; In contrast other classification systems, for example the [[Dewey Decimal Classification]], might class all hammers together and close to the classification for other percussive tools. The specialist subject network, Social History Curator's Group (SHCG), obtained funding in 2012 to develop an on-line version, now on their website http://www.shcg.org.uk/&lt;ref&gt;{{cite web|title=Social History Curators' Group - SHCG|url=http://www.shcg.org.uk/About-SHIC|accessdate=29 October 2012}}&lt;/ref&gt;

==Scheme==
Materials are classified under four major category numbers:
#Community life
#Domestic and family life
#Personal life
#Working life
 
Further classification within a category is by the use of further numbers after the decimal point.&lt;ref&gt;{{cite web|title=SHIC Section B|url=http://www.holm.demon.co.uk/shic/shicint.htm}}&lt;/ref&gt; 

It is permissible to assign more than one classification in cases where the object had more than one use.&lt;ref&gt;{{cite web|title=SHIC Section F |url=http://www.holm.demon.co.uk/shic/shicint.htm}}&lt;/ref&gt;

==References==
{{reflist}}
*''Social history and industrial classification (SHIC), a subject classification for museum collections'', University of Sheffield, 1983

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Social history]]
[[Category:Industrial history]]</text>
      <sha1>4uyekv4xt100u6ri2mjy6w85780att7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Belief revision</title>
    <ns>14</ns>
    <id>36312376</id>
    <revision>
      <id>562888448</id>
      <parentid>519506014</parentid>
      <timestamp>2013-07-04T21:44:04Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2013 June 17|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="203" xml:space="preserve">{{Cat main}}
{{see also|WP:WikiProject Philosophy/Resources}}

[[Category:Belief]]
[[Category:Formal epistemology]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Logic programming]]</text>
      <sha1>qebihyvwur7tpreobbc3b9gri4n63fz</sha1>
    </revision>
  </page>
  <page>
    <title>BabelNet</title>
    <ns>0</ns>
    <id>37291130</id>
    <revision>
      <id>759335612</id>
      <parentid>749197036</parentid>
      <timestamp>2017-01-10T15:52:34Z</timestamp>
      <contributor>
        <username>Jona</username>
        <id>330665</id>
      </contributor>
      <minor />
      <comment>/* Applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7445" xml:space="preserve">{{Infobox software
 |name = BabelNet
 |logo = [[File:BabelNet_logo.svg|140px|BabelNet logo.]]
 |screenshot =
 |caption = Wikipedia Extraction
 |developer = 
 |released = 
 |latest_release_version = BabelNet 3.7
 |latest_release_date = August 2016
 |operating_system = {{Flatlist|
* [[Virtuoso Universal Server]]
* [[Lucene]]
}}
 |genre = {{Flatlist|
* [[Machine-readable dictionary|Multilingual encyclopedic dictionary]]
* [[Linked data]]
}}
 |programming language = 
 |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]
 |website = {{URL|babelnet.org}}
 |alexa   = 
}}

'''BabelNet''' is a [[Multilinguality|multilingual]] lexicalized [[semantic network]] and [[Ontology (information science)|ontology]] developed at the Linguistic Computing Laboratory in the Department of Computer Science of the [[Sapienza University of Rome]].&lt;ref name="NavigliPonzetto12"&gt;R. Navigli and S. P Ponzetto. 2012. [http://dx.doi.org/10.1016/j.artint.2012.07.001 BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network]. Artificial Intelligence, 193, Elsevier, pp. 217-250.&lt;/ref&gt;&lt;ref&gt;R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11&#8211;16, 2010, pp. 216&#8211;225.&lt;/ref&gt; BabelNet was automatically created by linking Wikipedia, to the most popular computational [[Dictionary|lexicon]] of the [[English language]], [[WordNet]]. The integration is performed by means of an automatic mapping and by filling in lexical gaps in resource-poor [[language]]s with the aid of [[statistical machine translation]]. The result is an "encyclopedic dictionary" 
that provides [[concept]]s and [[Named entity|named entities]] [[Lexicalization|lexicalized]] in many languages and connected with large amounts of [[Semantic relation#Relationships|semantic relations]]. Additional lexicalizations and definitions are added by linking to free-license wordnets, [[OmegaWiki]], the English [[Wiktionary]], [[Wikidata]], [[FrameNet]], [[VerbNet]] and others. Similarly to WordNet, BabelNet groups [[word]]s in different languages into sets of [[synonyms]], called ''Babel [[synsets]]''. For each Babel synset, BabelNet provides short definitions (called [[Definition|glosses]]) in many languages harvested from both WordNet and Wikipedia.

[[File:The BabelNet structure.png|thumb|600px|BabelNet is a multilingual semantic network obtained as an integration of WordNet and Wikipedia.]]

==Statistics of BabelNet==

{{As of|2016|08}}, BabelNet (version 3.7) covers 271 [[language]]s, including all European languages, most [[Asian language]]s, and [[Latin]]. BabelNet 3.7 contains almost 14 million synsets and about 746 million [[word sense]]s (regardless of their language). Each Babel synset contains 2 synonyms per language, i.e., word senses, on average. The semantic network includes all the lexico-semantic relations from WordNet ([[Hyponymy|hypernymy and hyponymy]], [[meronymy]] and [[holonymy]], [[antonymy]] and [[synonymy]], etc., totaling around 364,000 relation edges) as well as an underspecified relatedness relation from Wikipedia (totaling around 380 million relation edges).&lt;ref name="NavigliPonzetto12" /&gt; Version 3.7 also associates about 11 million images with Babel synsets and provides a Lemon [[Resource Description Framework|RDF]] encoding of the resource,&lt;ref&gt;M. Ehrmann, F. Cecconi, D. Vannella, J. McCrae, P. Cimiano, R. Navigli. [http://www.lrec-conf.org/proceedings/lrec2014/pdf/810_Paper.pdf Representing Multilingual Data as Linked Data: the Case of BabelNet 2.0]. Proc. of the 9th Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, 26&#8211;31 May 2014.&lt;/ref&gt; available via a [[SPARQL endpoint]]. 2.67 million synsets are assigned domain labels.

==Applications==

BabelNet has been shown to enable multilingual [[Natural Language Processing]] applications. The lexicalized [[knowledge]] available in BabelNet has been shown to obtain state-of-the-art results in:

* [[semantic relatedness]]&lt;ref&gt;R. Navigli and S. Ponzetto. 2012. [http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/download/5112/5126 BabelRelate! A Joint Multilingual Approach to Computing Semantic Relatedness]. Proc. of the 26th AAAI Conference on Artificial Intelligence (AAAI 2012), Toronto, Canada, pp. 108-114.&lt;/ref&gt;&lt;ref&gt;J. Camacho-Collados, M. T. Pilehvar and R. Navigli. [http://aclweb.org/anthology/N/N15/N15-1059.pdf NASARI: a Novel Approach to a Semantically-Aware Representation of Items]. Proc. of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2015), Denver, Colorado (US), 31 May-5 June 2015, pp. 567-577.&lt;/ref&gt;
* multilingual [[Word Sense Disambiguation]]&lt;ref&gt;R. Navigli and S. Ponzetto. [http://www.aclweb.org/anthology/D12-1128 Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation]. Proc. of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP 2012), Jeju, Korea, July 12&#8211;14, 2012, pp. 1399-1410.&lt;/ref&gt;
* multilingual Word Sense Disambiguation and [[Entity Linking]] with the [[Babelfy]] system&lt;ref&gt;A. Moro, A. Raganato, R. Navigli. [http://www.transacl.org/wp-content/uploads/2014/05/54.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.&lt;/ref&gt;
* [[game with a purpose|video games with a purpose]]&lt;ref&gt;D. Jurgens, R. Navigli. {{webarchive|url=http://web.archive.org/web/20150103085712/http://www.transacl.org/wp-content/uploads/2014/10/421-camera-ready.pdf |date=January 3, 2015 |title=It's All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation}}. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 449-464, 2014.&lt;/ref&gt;

==Prizes and acknowledgments==
BabelNet received the [http://www.meta-net.eu/meta-prize META prize] 2015 for "groundbreaking work in overcoming language barriers through a multilingual lexicalised semantic network and ontology making use of heterogeneous data sources". 

BabelNet featured prominently in a [[TIME magazine]]'s article&lt;ref&gt;Katy Steinmetz. [http://time.com/4327440/redefining-the-modern-dictionary/ Redefining the modern dictionary], TIME magazine, vol. 187, 23 maggio 2016, pp. 20-21.&lt;/ref&gt; about the new age of innovative and up-to-date lexical knowledge resources available on the Web. The article describes in some detail how BabelNet is playing a leading role in the 21st century scenario.

==See also==
* [[Babelfy]]
* [[EuroWordNet]]
* [[Knowledge acquisition]]
* [[Linguistic Linked Open Data]]
* [[OmegaWiki]]
* [[Semantic network]]
* [[Semantic relatedness]]
* [[Wikidata]]
* [[Wiktionary]]
* [[Word sense disambiguation]]
* [[Word sense induction]]
* [[UBY]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://babelnet.org}}
* [http://babelnet.org/sparql SPARQL endpoint]
* [http://www.meta-net.eu/meta-prize META prize]

[[Category:Lexical databases]]
[[Category:Knowledge bases]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Artificial intelligence]]
[[Category:Online dictionaries]]
[[Category:Multilingualism]]</text>
      <sha1>bpdwdc9uxj2vfrpe73qhdby58hkkunx</sha1>
    </revision>
  </page>
  <page>
    <title>Spatial&#8211;temporal reasoning</title>
    <ns>0</ns>
    <id>3342061</id>
    <revision>
      <id>748137965</id>
      <parentid>741649771</parentid>
      <timestamp>2016-11-06T15:22:28Z</timestamp>
      <contributor>
        <username>MaxEnt</username>
        <id>1190064</id>
      </contributor>
      <comment>fix lead; add unkind section headers</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5491" xml:space="preserve">{{about|spatial-temporal reasoning in information technology|spatial-temporal reasoning in psychology|Spatial visualization ability}}
{{technical|date=October 2012}}
'''Spatial&#8211;temporal reasoning''' is an area of [[Artificial Intelligence|artificial intelligence]] which draws from the fields of [[computer science]], [[cognitive science]], and [[cognitive psychology]]. The theoretic goal&#8212;on the cognitive side&#8212;involves representing and reasoning spatial-temporal knowledge in mind. The applied goal&#8212;on the computing side&#8212;involves developing high-level control systems of robots for navigating and understanding time and space. 

== Influence from cognitive psychology ==
A convergent result in cognitive psychology is that the connection relation is the first spatial relation that human babies acquire, followed by understanding orientation relations and distance relations. Internal relations among the three kinds of spatial relations can be computationally and systematically explained within the theory of cognitive prism as follows: (1) the connection relation is primitive; (2) an orientation relation is a distance comparison relation: you being in front of me can be interpreted as you are nearer to my front side than my other sides; (3) a distance relation is connection relations using a third object: you being one meter away from me can be interpreted as an object with the maximum extension of one meter can be connected with you and me simultaneously. 

== Fragmentary representations of temporal calculi ==
Without addressing internal relations among spatial relations, AI researchers contributed many fragmentary representations. Examples of temporal calculi include [[Allen's interval algebra]], and Vilain's &amp; Kautz's [[point algebra]]. The most prominent spatial calculi are [[Mereotopology|mereotopological calculi]], [[Andrew U. Frank|Frank]]'s [[cardinal direction calculus]], Freksa's double cross calculus, Egenhofer and Franzosa's [[9-intersection calculus|4- and 9-intersection calculi]], Ligozat's [[flip-flop calculus]], various [[region connection calculus|region connection calculi]] (RCC), and the [[Oriented Point Relation Algebra]]. Recently, spatio-temporal calculi have been designed that combine spatial and temporal information. For example, the [[spatiotemporal constraint calculus]] (STCC) by Gerevini and Nebel combines Allen's interval algebra with RCC-8. Moreover, the [[qualitative trajectory calculus]] (QTC) allows for reasoning about moving objects.

== Quantitative abstraction ==
An emphasis in the literature has been on [[Qualitative reasoning|qualitative]] spatial-temporal reasoning which is based on qualitative abstractions of temporal and spatial aspects of the common-sense background knowledge on which our human perspective of physical reality is based.  Methodologically, qualitative [[Constraint satisfaction|constraint]] calculi restrict the vocabulary of rich mathematical theories dealing with temporal or spatial entities such that specific aspects of these theories can be treated within [[Decidability (logic)|decidable]] fragments with simple qualitative (non-[[Metric (mathematics)|metric]]) languages. Contrary to mathematical or physical theories about space and time, qualitative constraint calculi allow for rather inexpensive reasoning about entities located in space and time.  For this reason, the limited expressiveness of qualitative representation formalism calculi is a benefit if such reasoning tasks need to be integrated in applications.  For example, some of these calculi may be implemented for handling spatial [[Geographic information system|GIS]] queries efficiently and some may be used for navigating, and communicating with, a mobile [[robot]].

== Relation algebra ==
Most of these calculi can be formalized as abstract [[relation algebra]]s, such that reasoning can be carried out at a symbolic level. For computing solutions of a [[constraint network]], the [[Local consistency#Path_consistency|path-consistency algorithm]] is an important tool.

== Software ==
* [http://www.sfbtr8.spatial-cognition.de/de/projekte/reasoning/r4-logospace/research-tools/gqr/ GQR], constraint network solver for calculi like RCC-5, RCC-8, Allen's interval algebra, point algebra, cardinal direction calculus, etc.

== See also ==
*[[Cerebral cortex]]
*[[Diagrammatic reasoning]]
*[[Temporal logic]]
*[[Visual thinking]]
*[[Spatial ability]]

== Notes ==
{{reflist}}

==References==
*J. Renz, B. Nebel, [http://users.rsise.anu.edu.au/~jrenz/papers/renz-nebel-los.pdf Qualitative Spatial Reasoning using Constraint Calculi], in: M. Aiello, I. Pratt-Hartmann, J. van Benthem (eds.): Handbook of Spatial Logics, Springer 2007.
*T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC&#8314;&#8314;]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352
*M. Vilain, H. Kautz, P. van Beek, [http://www.cs.rochester.edu/~kautz/papers/vilain-kautz-book.pdf Constraint propagation algorithms for temporal reasoning: A Revised Report], 1987.
*T. Dong. [http://www.springer.com/de/book/9783642240577 Recognizing Variable Environment -- The Theory of Cognitive Prism]. Studies in Computational Intelligence, Vol. 388, Springer-Verlag, Berlin Heidelberg, 2012.

{{DEFAULTSORT:Spatial-temporal reasoning}}
[[Category:Cognitive science]]
[[Category:Knowledge representation]]
[[Category:Educational psychology]]
[[Category:Logical calculi]]
[[Category:Reasoning]]</text>
      <sha1>gaewb4asl7rxrfihernjqd8ixvtexqw</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge representation and reasoning</title>
    <ns>0</ns>
    <id>16920</id>
    <revision>
      <id>758047669</id>
      <parentid>756355431</parentid>
      <timestamp>2017-01-03T04:45:23Z</timestamp>
      <contributor>
        <username>BrianPansky</username>
        <id>22359645</id>
      </contributor>
      <comment>/* Ontology engineering */ removed original research, personal essay type thing.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="31797" xml:space="preserve">'''Knowledge representation and reasoning''' ('''KR''') is the field of [[artificial intelligence]] (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a [[natural language]]. Knowledge representation incorporates findings from psychology{{citation needed|date=February 2016}} about how humans solve problems and represent knowledge in order to design [[Formalism (mathematics)|formalisms]] that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from [[logic]] to automate various kinds of ''reasoning'', such as the application of rules or the relations of [[Set theory|sets]] and [[subset]]s.

Examples of knowledge representation formalisms include [[Semantic network|semantic nets]], [[systems architecture]], [[Frame (artificial intelligence)|Frames]], Rules, and [[Ontology (information science)|ontologies]]. Examples of [[automated reasoning]] engines include [[inference engine]]s, [[automated theorem proving|theorem prover]]s, and classifiers.

== History ==

The earliest work in computerized knowledge representation was focused on general problem solvers such as the [[General Problem Solver]] (GPS) system developed by [[Allen Newell]] and [[Herbert A. Simon]] in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.

In these early days of AI, general search algorithms such as [[A*]] were also developed.  However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g. the "[[blocks world]]"). In order to tackle non-toy problems, AI researchers such as [[Ed Feigenbaum]] and [[Rick Hayes-Roth|Frederick Hayes-Roth]] realized that it was necessary to focus systems on more constrained problems.

It was the failure of these efforts that led to the [[cognitive revolution]] in psychology and to the phase of AI focused on knowledge representation that resulted in [[expert systems]] in the 1970s and 80s, [[Production system (computer science)|production systems]], [[frame language]]s, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.

Expert systems gave us the terminology still in use today where AI systems are divided into a Knowledge Base with facts about the world and rules and an inference engine that applies the rules to the [[knowledge base]] in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.&lt;ref&gt;{{cite book|last=Hayes-Roth|first=Frederick|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}&lt;/ref&gt;

In addition to expert systems, other researchers developed the concept of [[Frame language|Frame based languages]] in the mid 1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. [[natural language understanding|understanding natural language]] and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.

It wasn't long before the frame communities and the rule-based researchers realized that there was synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined Frames and Rules. One of the most powerful and well known was the 1983 [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE had a complete rule engine with [[forward chaining|forward]] and [[backward chaining]]. It also had a complete frame based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from [[Symbolics]], [[Xerox]], and [[Texas Instruments]].&lt;ref&gt;{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}&lt;/ref&gt;

The integration of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving.  One of the most influential languages in this research was the [[KL-ONE]] language of the mid 80's. KL-ONE was a [[frame language]] that had a rigorous semantics, formal definitions for concepts such as an [[Is-a|Is-A relation]].&lt;ref&gt;{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}&lt;/ref&gt; KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41&#8211;46}}&lt;/ref&gt;

Another area of knowledge representation research was the problem of common sense reasoning.  One of the first realizations from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent.  Basic principles of common sense physics, causality, intentions, etc. An example is the [[Frame problem]], that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world it is essential to represent this kind of knowledge. One of the most ambitious programs to tackle this problem was Doug Lenat's [[Cyc]] project. Cyc established its own Frame language and had large numbers of analysts document various areas of common sense reasoning in that language. The knowledge recorded in Cyc included common sense models of time, causality, physics, intentions, and many others.&lt;ref&gt;{{cite book|last=Lenat|first=Doug|title=Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project|publisher=Addison-Wesley|isbn=978-0201517521|author2=R. V. Guha |date=January 1990}}&lt;/ref&gt;

The starting point for knowledge representation is the ''knowledge representation hypothesis'' first formalized by [[Brian Cantwell Smith|Brian C. Smith]] in 1985:&lt;ref&gt;{{cite book|last=Smith|first=Brian C.|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=31&#8211;40|editor=Ronald Brachman and Hector J. Levesque|chapter=Prologue to Reflections and Semantics in a Procedural Language}}&lt;/ref&gt;

&lt;blockquote&gt;''Any mechanically embodied intelligent process will be {{sic|comprised |hide=y|of}} structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge.''&lt;/blockquote&gt;

Currently one of the most active areas of knowledge representation research are projects associated with the [[Semantic web]]. The semantic web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the semantic web creates large [[ontologies]] of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future semantic web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.

Recent projects funded primarily by the [[Defense Advanced Research Projects Agency]] (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The [[Resource Description Framework]] (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The [[Web Ontology Language]] (OWL) provides additional levels of semantics and enables integration with classification engines.&lt;ref&gt;{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34&#8211;43}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}&lt;/ref&gt;

== Overview ==
Knowledge-representation is the field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems. The justification for knowledge representation is that conventional [[procedural code]] is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in [[expert systems]].

For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.

Knowledge representation goes hand in hand with [[automated reasoning]] because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all [[knowledge representation language]]s have a reasoning or inference engine as part of the system.&lt;ref&gt;{{cite book|last=Hayes-Roth|first=Frederick|pages=6&#8211;7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}&lt;/ref&gt;

A key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is First Order Logic (FOL).  There is no more powerful formalism than that used by mathematicians to define general propositions about the world. However, FOL has two drawbacks as a knowledge representation formalism:  ease of use and practicality of implementation.  First order logic can be intimidating even for many software developers. Languages which do not have the complete formal power of FOL can still provide close to the same expressive power with a user interface that is more practical for the average developer to understand. The issue of practicality of implementation is that FOL in some ways is too expressive. With FOL it is possible to create statements (e.g. quantification over infinite sets) that would cause a system to never terminate if it attempted to verify them.

Thus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems. IF-THEN rules provide a subset of FOL but a very useful one that is also very intuitive.  The history of most of the early AI knowledge representation formalisms; from databases to semantic nets to theorem provers and production systems can be viewed as various design decisions on whether to emphasize expressive power or computability and efficiency.&lt;ref&gt;{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}&lt;/ref&gt;

In a key 1993 paper on the topic, Randall Davis of [[Massachusetts Institute of Technology|MIT]] outlined five distinct roles to analyze a knowledge representation framework:&lt;ref&gt;{{cite journal|last=Davis|first=Randall|author2=Howard Shrobe |author3=Peter Szolovits |title=What Is a Knowledge Representation?|journal=AI Magazine|date=Spring 1993|volume=14|issue=1|pages=17&#8211;33|url=http://www.aaai.org/ojs/index.php/aimagazine/article/view/1029/947}}&lt;/ref&gt; 
* A knowledge representation (KR) is most fundamentally a surrogate, a substitute for the thing itself, used to enable an entity to determine consequences by thinking rather than acting, i.e., by reasoning about the world rather than taking action in it.
* It is a set of ontological commitments, i.e., an answer to the question: In what terms should I think about the world?
* It is a fragmentary theory of intelligent reasoning, expressed in terms of three components: (i) the representation's fundamental conception of intelligent reasoning; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends.
* It is a medium for pragmatically efficient computation, i.e., the computational environment in which thinking is accomplished. One contribution to this pragmatic efficiency is supplied by the guidance a representation provides for organizing information so as to facilitate making the recommended inferences.
* It is a medium of human expression, i.e., a language in which we say things about the world."

Knowledge representation and reasoning are a key enabling technology for the [[Semantic web]]. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today it will be possible to define logical queries and find pages that map to those queries.&lt;ref&gt;{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34&#8211;43}}&lt;/ref&gt; The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the [[Subsumption relation|subsumption]] relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever changing and evolving information space of the Internet.&lt;ref&gt;{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}&lt;/ref&gt;

The Semantic web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The [[Resource Description Framework]] (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The [[Web Ontology Language]] (OWL) adds additional semantics and integrates with automatic classification reasoners.&lt;ref&gt;{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}&lt;/ref&gt;

== Characteristics ==
In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:&lt;ref&gt;{{cite book|last=Brachman|first=Ron|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=XVI-XVII|editor=Ronald Brachman and Hector J. Levesque|chapter=Introduction}}&lt;/ref&gt; 
*Primitives. What is the underlying framework used to represent knowledge? [[Semantic network]]s were one of the first knowledge representation primitives. Also, data structures and algorithms for general fast search. In this area there is a strong overlap with research in data structures and algorithms in computer science. In early systems the Lisp programming language which was modeled after the [[lambda calculus]] was often used as a form of functional knowledge representation. Frames and Rules were the next kind of primitive. Frame languages had various mechanisms for expressing and enforcing constraints on frame data. All data in frames are stored in slots. Slots are analogous to relations in entity-relation modeling and to object properties in object-oriented modeling. Another technique for primitives is to define languages that are modeled after [[First Order Logic]] (FOL). The most well known example is Prolog but there are also many special purpose theorem proving environments. These environments can validate logical models and can deduce new theories from existing models. Essentially they automate the process a logician would go through in analyzing a model. Theorem proving technology had some specific practical applications in the areas of software engineering. For example, it is possible to prove that a software program rigidly adheres to a formal logical specification.
*Meta-Representation.  This is also known as the issue of [[Reflection (computer programming)|reflection]] in computer science. It refers to the capability of a formalism to have access to information about its own state. An example would be the meta-object protocol in [[Smalltalk]] and [[CLOS]] that gives developers run time access to the class objects and enables them to dynamically redefine the structure of the knowledge base even at run time. Meta-representation means the knowledge representation language is itself expressed in that language. For example, in most Frame based environments all frames would be instances of a frame class. That class object can be inspected at run time so that the object can understand and even change its internal structure or the structure of other parts of the model. In rule-based environments the rules were also usually instances of rule classes. Part of the meta protocol for rules were the meta rules that prioritized rule firing. 
*[[Completeness (logic)|Incompleteness]]. Traditional logic requires additional axioms and constraints to deal with the real world as opposed to the world of mathematics. Also, it is often useful to associate degrees of confidence with a statement. I.e., not simply say "Socrates is Human" but rather "Socrates is Human with confidence 50%". This was one of the early innovations from [[expert system]]s research which migrated to some commercial tools, the ability to associate certainty factors with rules and conclusions. Later research in this area is known as [[Fuzzy Logic]].&lt;ref&gt;{{cite journal|last=Bih|first=Joseph|title=Paradigm Shift: An Introduction to Fuzzy Logic|journal=IEEE POTENTIALS|year=2006|url=http://www.cse.unr.edu/~bebis/CS365/Papers/FuzzyLogic.pdf|accessdate=24 December 2013}}&lt;/ref&gt; 
*Definitions and [[Universals]] vs. facts and defaults.  Universals are general statements about the world such as "All humans are mortal". Facts are specific examples of universals such as "Socrates is a human and therefore mortal". In logical terms definitions and universals are about universal quantification while facts and defaults are about existential quantifications. All forms of knowledge representation must deal with this aspect and most do so with some variant of set theory, modeling universals as sets and subsets and definitions as elements in those sets. 
*[[Non-monotonic logic|Non-Monotonic reasoning]]. Non-monotonic reasoning allows various kinds of hypothetical reasoning. The system associates facts asserted with the rules and facts used to justify them and as those facts change updates the dependent knowledge as well. In rule based systems this capability is known as a [[truth maintenance system]].&lt;ref&gt;{{cite journal|last=Zlatarva|first=Nellie|title=Truth Maintenance Systems and their Application for Verifying Expert System Knowledge Bases|journal=Artificial Intelligence Review|year=1992|volume=6|pages=67&#8211;110|url=http://link.springer.com/article/10.1007%2FBF00155580#page-2|accessdate=25 December 2013|doi=10.1007/bf00155580}}&lt;/ref&gt; 
*[[Functional completeness|Expressive Adequacy]]. The standard that Brachman and most AI researchers use to measure expressive adequacy is usually First Order Logic (FOL). Theoretical limitations mean that a full implementation of FOL is not practical. Researchers should be clear about how expressive (how much of full FOL expressive power) they intend their representation to be.&lt;ref&gt;{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages = 41&#8211;70|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning}}&lt;/ref&gt;
*Reasoning Efficiency. This refers to the run time efficiency of the system. The ability of the knowledge base to be updated and the reasoner to develop new inferences in a reasonable period of time. In some ways this is the flip side of expressive adequacy. In general the more powerful a representation, the more it has expressive adequacy, the less efficient its [[automated reasoning]] engine will be. Efficiency was often an issue, especially for early applications of knowledge representation technology. They were usually implemented in interpreted environments such as Lisp which were slow compared to more traditional platforms of the time.

== Ontology engineering ==
{{main article|Ontology engineering|Ontology language}}

In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic but medical diagnosis of certain kinds of diseases.

As knowledge-based technology scaled up the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common sense knowledge. In designing an artificial intelligence agent it was soon realized that representing common sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as [[CycL]].

After CycL, a number of [[ontology language]]s have been developed.  Most are [[declarative language]]s, and are either [[frame language]]s, or are based on [[first-order logic]]. Modularity&#8212;the ability to define boundaries around specific domains and problem spaces&#8212;is essential for these languages because as stated by Tom Gruber, "Every ontology is a treaty- a social agreement among people with common motive in sharing." There are always many competing and differing views that make any general purpose ontology impossible. A general purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.&lt;ref&gt;Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (3rd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, p. 437-439&lt;/ref&gt;

There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,&lt;ref&gt;Hayes P, Naive physics I: Ontology for liquids. University of Essex report, 1978, Essex, UK.&lt;/ref&gt; the lumped element model widely used in representing electronic circuits (e.g.,&lt;ref&gt;Davis R, Shrobe H E, Representing Structure and Behavior of Digital Hardware, IEEE Computer, Special Issue on Knowledge Representation, 16(10):75-82.&lt;/ref&gt;), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.
The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.

Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.

The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., [[MYCIN]]) looks substantially different from the same task viewed in terms of frames (e.g., [[INTERNIST]]). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.

== See also ==
* [[Chunking (psychology)]]
* [[Commonsense knowledge base]]
* [[Personal knowledge base]]
* [[Valuation-based system]]
* [[Conceptual Graph]]

== References ==

&lt;references/&gt;

== Further reading ==
* [[Ronald J. Brachman]]; [http://citeseer.nj.nec.com/context/177306/0 What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks]; IEEE Computer, 16 (10); October 1983
* [[Ronald J. Brachman]], [[Hector J. Levesque]] ''Knowledge Representation and Reasoning'', Morgan Kaufmann, 2004 ISBN 978-1-55860-932-7
* [[Ronald J. Brachman]], [[Hector J. Levesque]] (eds) ''Readings in Knowledge Representation'', Morgan Kaufmann, 1985, ISBN 0-934613-01-X
* Chein, M., Mugnier, M.-L. (2009),''[http://www.lirmm.fr/gbkrbook/ Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs]'', Springer, 2009,ISBN 978-1-84800-285-2.
* Randall Davis, Howard Shrobe, and Peter Szolovits; [http://citeseer.ist.psu.edu/davis93what.html What Is a Knowledge Representation?] AI Magazine, 14(1):17-33,1993
* [[Ronald Fagin]], [[Joseph Y. Halpern]], [[Yoram Moses]], [[Moshe Y. Vardi]] ''Reasoning About Knowledge'', MIT Press, 1995, ISBN 0-262-06162-7
* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57
* Hermann Helbig: ''Knowledge Representation and the Semantics of Natural Language'', Springer, Berlin, Heidelberg, New York 2006
* Arthur B. Markman: ''Knowledge Representation''  Lawrence Erlbaum Associates, 1998
* [[John F. Sowa]]: ''Knowledge Representation'': Logical, Philosophical, and Computational Foundations. Brooks/Cole: New York, 2000
* Adrian Walker, Michael McCord, [[John F. Sowa]], and Walter G. Wilson: ''Knowledge Systems and Prolog'', Second Edition, Addison-Wesley, 1990

== External links ==
* [http://medg.lcs.mit.edu/ftp/psz/k-rep.html What is a Knowledge Representation?] by Randall Davis and others
* [http://www.makhfi.com/KCM_intro.htm Introduction to Knowledge Modeling] by Pejman Makhfi
* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics course] by Enrico Franconi, Faculty of Computer Science, Free University of Bolzano, Italy
* [http://www.ccl.kuleuven.ac.be/LKR/html/datr.html DATR Lexical knowledge representation language]
* [http://www.isi.edu/isd/LOOM/LOOM-HOME.html Loom Project Home Page]
* [http://www.research.att.com/sw/tools/classic/tm/ijcai-95-with-scenario.html Description Logic in Practice: A CLASSIC Application]
* [http://www.dfki.uni-kl.de/ruleml/ The Rule Markup Initiative]
* [http://nelements.org Nelements KOS] - a non-free 3d knowledge representation system

{{Computer science}}
{{computable knowledge}}
{{Commons category|Knowledge representation}}

{{DEFAULTSORT:Knowledge Representation And Reasoning}}
[[Category:Knowledge representation| ]]
[[Category:Scientific modeling]]
[[Category:Programming paradigms]]
[[Category:Reasoning]]</text>
      <sha1>agiji1qopnxfbr9o82lq4yy4db64dyr</sha1>
    </revision>
  </page>
  <page>
    <title>ERIL</title>
    <ns>0</ns>
    <id>42325907</id>
    <revision>
      <id>725131878</id>
      <parentid>628302717</parentid>
      <timestamp>2016-06-13T19:35:42Z</timestamp>
      <contributor>
        <username>Cabayi</username>
        <id>6561336</id>
      </contributor>
      <minor />
      <comment>Added {{[[Template:COI|COI]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3945" xml:space="preserve">{{COI|date=June 2016}}
{{Orphan|date=April 2014}}

[[File:A simple example ERIL diagram.png|thumb|right|An example ERIL diagram with 3 classes and 3 one-to-many relationships.]]

'''ERIL''' ('''Entity-Relationship and Inheritance Language''') is a [[visual language]] for representing the data structure of a computer system.
As its name suggests, ERIL is based on [[Entity&#8211;relationship model|entity-relationship]] diagrams and [[class diagram]]s.
ERIL combines the [[Relational data model|relational]] and [[Object-oriented programming|object-oriented]] approaches to [[data model]]ing.

== Overview ==
ERIL can be seen as a set of guidelines aimed at improving the readability of structure diagrams.
These guidelines were borrowed from [[DRAKON]], a variant of [[flowchart]]s created within the Russian space program.
ERIL itself was developed by Stepan Mitkin.

The ERIL guidelines for drawing diagrams:
* Lines must be straight, either strictly vertical or horizontal.
* Vertical lines mean ownership ([[Object composition|composition]]).
* Horizontal lines mean peer relationships ([[Object composition#Aggregation|aggregation]]).
* Line intersections are not allowed.
* It is not recommended to fit the whole data model on a single diagram. Draw many simple diagrams instead.
* The same class (table) can appear several times on the same diagram.    
* Use the following standard symbols to indicate the type of the relationship.
** One-to-one: a simple line.
** One-to-many, two-way: a line with a "paw".
** One-to-many, one-way: an arrow.
** Many-to-many: a line with two "paws".    
* Do not lump together inheritance and data relationships.&lt;ref&gt;[http://drakon-editor.sourceforge.net/eril.html ERIL: a Visual Language for Data Modelling]&lt;/ref&gt;

== Indexes ==
A class (table) in ERIL can have several indexes.
Each index in ERIL can include one or more fields, similar to indexes in [[relational database]]s.
ERIL indexes are logical. They can optionally be implemented by real data structures.

== Links ==
Links between classes (tables) in ERIL are implemented by the so-called "link" fields.
Link fields can be of different types according to the link type:
* reference;
* collection of references.
    
Example: there is a one-to-many link between ''Documents'' and ''Lines''. One ''Document'' can have many ''Lines''. Then the ''Document.Lines'' field is a collection of references to the lines that belong to the document. ''Line.Document'' is a reference to the document that contains the line.

Link fields are also logical. They may or may not be implemented physically in the system.

== Usage ==

ERIL is supposed to model any kind of data regardless of the storage. 
The same ERIL diagram can represent data stored in a [[relational database]], in a [[Nosql|NoSQL]] database, [[Xml|XML]] file or in the memory.

ERIL diagrams serve two purposes.
The primary purpose is to explain the data structure of an existing or future system or component.
The secondary purpose is to automatically generate source code from the model.
Code that can be generated includes specialized collection classes, hash and comparison functions, data retrieval and modification procedures, [[Data definition language|SQL data-definition]] code, etc. Code generated from ERIL diagrams can ensure referential and uniqueness [[data integrity]].
Serialization code of different kinds can also be automatically generated.
In some ways ERIL can be compared to [[object-relational mapping]] frameworks.

== See also ==
* [[Model-driven engineering]]
* [[Unified Modeling Language|UML]]
* [[Entity&#8211;relationship model]]
* [[Flowchart]]s
* [[Class diagram]]
* [[DRAKON]]

== Notes ==
{{Reflist}}


[[Category:Architecture description language]]
[[Category:Data modeling languages]]
[[Category:Data modeling diagrams]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Specification languages]]
[[Category:Software modeling language]]</text>
      <sha1>h3ais36eetyq4dwd79i8utndn0v89k9</sha1>
    </revision>
  </page>
  <page>
    <title>Enactive interfaces</title>
    <ns>0</ns>
    <id>5510317</id>
    <revision>
      <id>757030560</id>
      <parentid>744779542</parentid>
      <timestamp>2016-12-28T09:50:14Z</timestamp>
      <contributor>
        <ip>82.158.187.111</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13424" xml:space="preserve">[[File:Enactive Human Machine Interface.png|thumb|300px|Enactive human-machine interface translating the aspects of a knowledge base into modalities of perception for a human operator. The auditory, visual, and tactile presentations by the system respond to tactile input from the operator, which user input in turn depends upon the auditory, visual, and tactile feedback from the system.&lt;ref name=Bordegoni/&gt;&lt;ref name=Fukuda/&gt;]]
'''Enactive interfaces''' are interactive systems that allow organization and transmission of knowledge obtained through action. Examples are interfaces that couple a human with a machine to do things usually done unaided, such as shaping a three-dimensional object using multiple modality interactions with a data base,&lt;ref name=Fukuda/&gt; or using interactive video to allow a student to visually engage with mathematics concepts.&lt;ref name=Held/&gt; Enactive interface design can be approached through the idea of raising awareness of [[affordances]], that is, optimization of the awareness of possible actions available to someone using the enactive interface.&lt;ref name=Stoffregen/&gt; This optimization involves visibility, affordance, and feedback.&lt;ref name=Stone/&gt;&lt;ref name=Zudilova/&gt;

The enactive interface in the figure interprets manual input and provides a response in perceptual terms in the form of images, sounds, and haptic (tactile) feedback. The system is called enactive because of the feedback loop in which the system response is decided by the user input, and the user input is driven by the perceived system responses.&lt;ref name=Bordegoni/&gt;

Enactive interfaces are new types of [[Human&#8211;computer interaction|human-computer interface]] that express and transmit the enactive knowledge by integrating different sensory aspects. The driving concept of enactive interfaces is then the fundamental role of motor action for storing and acquiring knowledge (action driven interfaces). Enactive interfaces are then capable of conveying and understanding gestures of the user, in order to provide an adequate response in perceptual terms. Enactive interfaces can be considered a new step in the development of the human-computer interaction because they are characterized by a closed loop between the natural gestures of the user (efferent component of the system) and the perceptual modalities activated (afferent component). Enactive interfaces can be conceived to exploit this direct loop and the capability of recognizing complex gestures.

The development of such interfaces requires the creation of a common vision between different research areas like [[computer vision]], [[Haptic perception|haptic]] and sound processing, giving more attention on the motor action aspect of interaction. An example of prototypical systems that are able to introduce enactive interfaces are reactive robots, robots that are always in contact with the human hand (like current play console controllers, [[Wii Remote]]) and are capable of interpreting the human movements and guiding the human for the completion of a manipulation task.

==Enactive knowledge==
Enactive knowledge is information gained through perception&#8211;action interaction in the environment. In many aspects the enactive knowledge is more natural than the other forms both in terms of the learning process and in the way it is applied in the world. Such knowledge is inherently [[multimodal interaction|multimodal]] because it requires the co-ordination of the various senses. Two key characteristics of enactive knowledge are that it is ''experential'': it relates to doing and depends on the user's experience, and it is ''cultural'': the way of doing is itself dependent upon social aspects, attitudes, values, practices, and legacy.&lt;ref name=Bordegoni/&gt;

Enactive interfaces are related to a fundamental interaction concept that often is not exploited by existing [[Human&#8211;computer interaction|human-computer interface]] technologies. As stated by cognitive psychologist [[Jerome Bruner]], the traditional interaction with the information mediated by a computer is mostly based on symbolic or iconic knowledge, and not on enactive knowledge.&lt;ref name=Slee/&gt; While in the symbolic way of learning knowledge is stored as words, mathematical symbols or other symbol systems, in the iconic stage knowledge is stored in the form of visual images, such as diagrams and illustrations that can accompany verbal information. On the other hand, enactive knowledge is a form of knowledge based on active participation, knowing by doing, by living rather than thinking.&lt;ref name=Slee2/&gt;
:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"&lt;ref name=Bruner/&gt;

A particular form of knowledge is a ''[[skill]]'', juggling being a simple example, and the acquisition of a skill is one area where enactive knowledge is evident. The sensorimotor and cognitive activities involved in acquiring skills are tabulated by the SKILLS FP6 European skills project.&lt;ref name=Bardy&gt;
{{cite journal |title=An enactive approach to perception-action and skill acquisition in virtual reality environments |author1=B Bardy |author2=D Deligni&#232;res |author3=J Lagarde |author4=D Mottet |author5=G Zelic |journal= Third International Conference on Applied Human Factors and Ergonomics |location=Miami |date=July 2010 |url=http://didier.delignieres.perso.sfr.fr/Colloques-docs/Bardy%20et%20al.%20%282010%29%20Skills%20apprentissage.pdf }}
&lt;/ref&gt;

==Multimodal interfaces==
Multimodal interfaces are a good candidate for the creation of ''Enactive interfaces'' because of their coordinated use of [[Haptic perception|haptic]], sound and vision. Such research is the main objective of the ENACTIVE [[Framework Programmes for Research and Technological Development|Network of Excellence]], a European consortium of more than 20 research laboratories that are joining their research effort for the definition, development and exploitation of enactive interfaces.

==ENACTIVE Network of Excellence==
The research on enactive knowledge and enactive interfaces is the objective of the ENACTIVE Network of Excellence. A Network of Excellence is a [[European Economic Community|European Community]] research instrument that provides fundings for the integration of the research activities of different research laboratories and institutions. The ENACTIVE NoE started in 2004 with more than 20 partners with the objective of ''the creation of a multidisciplinary research community with the aim of structuring the research on a new generation of human-computer interfaces called Enactive Interfaces.''. The aim of this NoE is not only the research on enactive interfaces by itself, but also the integration of the partners through a Virtual Laboratory and the spreading of the expertise and knowledge of the Network.

Since 2004, the partners, coordinated by the PERCRO laboratory, have improved both the theoretical aspects of enaction, through seminars and the creation of a [[lexicon]], and the technological aspects necessary for the creation of enactive interfaces. Every year the status of the ENACTIVE NoE is presented through an international conference.&lt;ref name=PERCRO/&gt;

== See also ==
* [[Enactivism]]

==References==
{{reflist|refs=

&lt;ref name=Bordegoni&gt;
{{cite book |title=Emotional Engineering: Service Development |editor=Shuichi Fukuda |url=https://books.google.com/books?id=ow-UFDj15rUC&amp;pg=PA76 |page=76 |chapter=&#167;4.4.2: PDP [Product Development Process] scenario based on user-centered design |author=Monica Bordegoni |isbn=9781849964234 |publisher=Springer |year=2010}}
&lt;/ref&gt;

&lt;ref name=Bruner&gt;
{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}}. Quoted in {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&amp;pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor &amp; Francis |isbn= 0415326982 |edition=Paperback}}
&lt;/ref&gt;

&lt;ref name=Fukuda&gt;
{{cite book |title=Emotional Engineering: Service Development |chapter=&#167;4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&amp;pg=PA78 |pages=78 ''ff'' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}
&lt;/ref&gt;

&lt;ref name=Held&gt;
{{cite book |title=Research on Technology and the Teaching and Learning of Mathematics  |editor1=Mary Kathleen Heid |editor2=Glendon W. Blume |url=https://books.google.com/books?id=RGqFJ9inaQQC&amp;pg=PA213 |pages=213 ''ff'' |chapter=Enactive control |authors=D Tall, D Smith, C Piez |isbn=9781931576192 |year=2008 |publisher=Information Age Publishing Inc }}
&lt;/ref&gt;

&lt;ref name=PERCRO&gt;
{{cite web |title=Research on haptic interfaces and virtual environments |url=http://www.percro.org/node/24 |publisher=PERCRO Perceptual Robotics Laboratory |accessdate=April 30, 2014}}
&lt;/ref&gt;

&lt;ref name=Slee&gt;
Bruner's list of six characteristics of iconic knowledge is found in {{cite book |chapter=Iconic representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&amp;pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}
&lt;/ref&gt;

&lt;ref name=Slee2&gt;
{{cite book |chapter=Enactive representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&amp;pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}
&lt;/ref&gt;

&lt;ref name=Stoffregen&gt;
{{cite journal |url=http://link.springer.com/article/10.1007/s10055-006-0025-7 |title=Affordances in the design of enactive systems |author1=TA Stoffregen |author2=BG Bardy |author3=B Mantel |journal=Virtual Reality |volume=10 |issue=1 |year=2006 |pages=4&#8211;10 |doi=10.1007/s10055-006-0025-7}}
&lt;/ref&gt;

&lt;ref name=Stone&gt;
{{cite book |author1=Debbie Stone |author2=Caroline Jarrett |author3=Mark Woodroffe |author4=Shailey Minocha Morgan Kaufmann |year=2005 |title=User Interface Design and Evaluation |publisher=Morgan Kaufmann |isbn=9780080520322 |url=https://books.google.com/books?id=VvSoyqPBPbMC&amp;pg=PA97 |pages=97 ''ff'' |chapter=Chapter 5; &#167;3: Three principles from experience: visibility, affordance, and feedback}}
&lt;/ref&gt;

&lt;ref name=Zudilova&gt;
{{cite book |title=Trends in Interactive Visualization: State-of-the-Art Survey |pages=166 ''ff'' |chapter=Perceptual and design principles for effective interactive visualizations |author1=Elena Zudilova-Seinstra |author2=Tony Adriaansen |author3=Robert van Liere |year=2008 |publisher=Springer |isbn=9781848002692 |url=https://books.google.com/books?id=mFtS7uN8ybsC&amp;pg=PA166}}
&lt;/ref&gt;

}}

==External links==
* [http://vimeo.com/79179138 Vimeo], video of a three-dimensional dynamic interactive graphical display allowing a human operator to visualize and manipulate data.

==Additional reading==
*{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&amp;pg=PA118&amp;lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118}} "The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela's sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself." 
*{{cite journal |title=The systemics of dialogism: On the prevalence of the self in HCI design |author=Colin T Schmidt |journal=Journal of the American society for information science |volume=48 |issue=11 |pages=1073&#8211;1081 |year=1997 |url=http://www.researchgate.net/publication/220433804_The_Systemics_of_Dialogism_On_the_Prevalence_of_the_Self_in_HCI_Design |doi=10.1002/(sici)1097-4571(199711)48:11&lt;1073::aid-asi9&gt;3.0.co;2-t}} Autopoiesic systems.
*{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-J&#246;rg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&amp;rct=j&amp;q=&amp;esrc=s&amp;sa=X&amp;ei=N-h_U6HtHIiEogSy_oHAAw&amp;ved=0CCcQgAMoADAA&amp;usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&amp;cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 ''ff'' |doi=10.1016/s0007-8506(07)62129-5}}

[[Category:Enactive cognition]]
[[Category:Knowledge representation]]
[[Category:Educational psychology]]
[[Category:Motor cognition]]
[[Category:User interface techniques]]</text>
      <sha1>08nw6zx0ynqhko9vgrf2u3qgg55et5c</sha1>
    </revision>
  </page>
  <page>
    <title>Open-world assumption</title>
    <ns>0</ns>
    <id>2692616</id>
    <revision>
      <id>626271390</id>
      <parentid>617470824</parentid>
      <timestamp>2014-09-19T23:05:16Z</timestamp>
      <contributor>
        <username>Lambiam</username>
        <id>745100</id>
      </contributor>
      <comment>/* top */ ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4199" xml:space="preserve">In a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], the '''open-world assumption''' is the assumption that the [[truth value]] of a [[statement (logic)|statement]] may be true irrespective of whether or not it is ''known'' to be true. It is the opposite of the [[closed-world assumption]], which holds that any statement that is true is also known to be true.

The open-world assumption (OWA) codifies the informal notion that in general no single agent or observer has complete knowledge, and therefore cannot make the closed-world assumption. The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true. In contrast, the closed world assumption allows an agent to infer, from its lack of knowledge of a statement being true, anything that [[Logical consequence|follows from]] that statement being false.

Heuristically, the open-world assumption applies when we represent knowledge within a system as we discover it, and where we cannot guarantee that we have discovered or will discover complete information. In the OWA, statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown, rather than wrong or false. 

[[Semantic Web]] languages such as [[Web Ontology Language|OWL]] make the open-world assumption. The absence of a particular statement within the web means, in principle, that the statement has not been made explicitly yet, irrespective of whether it would be true or not, and irrespective of whether we believe that it would be true or not. In essence, from the absence of a statement alone, a deductive reasoner cannot (and must not) infer that the statement is false.

Many [[procedural programming language]]s and [[database]]s make the closed-world assumption. For example, if a typical airline database does not contain a seat assignment for a traveler, it is assumed that the traveler has not checked in. The closed-world assumption typically applies when a system has complete control over information; this is the case with many database applications where the [[database transaction]] system acts as a central broker and arbiter of concurrent requests by multiple independent clients (e.g., airline booking agents). There are, however, many databases with incomplete information: for example, one cannot assume that because there is no mention on a patient's history of a particular allergy, that the patient does not suffer from that allergy.

'''Example'''
  Statement: "Mary" "is a citizen of" "France"

  Question: Is Paul a citizen of France?

  "Closed world" (for example SQL) answer: No.
  "Open world" answer: Unknown.

Under OWA, failure to derive a fact does not imply the opposite. For example, assume we only know that Mary is a citizen of France. From this information we can neither conclude that Paul is not a citizen of France, nor that he is. Therefore, we admit the fact that our knowledge of the world is incomplete. The open-world assumption is closely related to the [[Monotonicity of entailment|monotonic]] nature of [[first-order logic]]: adding new information never falsifies a previous conclusion. Namely, if we subsequently learn that Paul is also a citizen of France, this does not change any earlier positive or negative conclusions.

The language of logic programs with [[Stable_model_semantics#Strong_negation|strong negation]] allows us to postulate the closed-world assumption for some predicates and leave the other predicates in the realm of the open-world assumption.

==See also==
*[[Closed-world assumption]]

==References==
*{{cite book |last1=Russell |first1=Stuart J. |authorlink1=Stuart J. Russell |last2=Norvig |first2=Peter |authorlink2=Peter Norvig |title=Artificial Intelligence: A Modern Approach |year=2010 |publisher=Prentice Hall |location=Upper Saddle River |isbn=9780136042594 |url=http://www.pearsonhighered.com/educator/product/Artificial-Intelligence-A-Modern-Approach/9780136042594.page |edition=3rd}}

{{DEFAULTSORT:Open-world assumption}}
[[Category:Logic programming]]
[[Category:Knowledge representation]]</text>
      <sha1>kxeeien3b4vb6mmmyaqtovwhd91yenw</sha1>
    </revision>
  </page>
  <page>
    <title>Babelfy</title>
    <ns>0</ns>
    <id>43480298</id>
    <revision>
      <id>757595047</id>
      <parentid>739017631</parentid>
      <timestamp>2016-12-31T15:56:58Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>changed {{Notability}} to {{Notability|Products}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2767" xml:space="preserve">{{multiple issues|
{{context|date=August 2016}}
{{notability|Products|date=August 2016}}
}}
{{Infobox software
 |name = Babelfy
 |logo = [[File:Babelfy_logo.png|140px|Babelfy logo.]]
 |screenshot =
 |caption = Babelfy
 |developer = 
 |released = 
 |latest_release_version = Babelfy 1.0
 |latest_release_date = June 2014
 |genre = {{Flatlist|
* [[Word sense disambiguation]]
* [[Entity linking]]
}}
 |programming language = 
 |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]
 |website = {{URL|babelfy.org}}
 |alexa   = 
}}

'''Babelfy''' is an [[algorithm]] for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of [[Multilinguality|multilingual]] [[Word Sense Disambiguation]] (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and [[Entity Linking]] (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.).&lt;ref&gt;A. Moro, A. Raganato, R. Navigli. [http://wwwusers.di.uniroma1.it/~navigli/pubs/TACL_2014_Babelfy.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.&lt;/ref&gt; Babelfy is based on the [[BabelNet]] multilingual semantic network and performs disambiguation and entity linking in three steps:

* It associates with each [[Vertex (graph theory)|vertex]] of the BabelNet semantic network, i.e., either [[concept]] or [[named entity]], a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text.
* Given an input [[Written text|text]], it extracts all the linkable fragments from this text and, for each of them, lists the possible [[meaning (linguistics)|meanings]] according to the [[semantic network]].
* It creates a [[Graph (data structure)|graph-based]] semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures. It then extracts a dense [[Glossary of graph theory#Subgraphs|subgraph]] of this representation and selects the best candidate meaning for each fragment.

As a result, the text, written in any of the 271 [[language]]s supported by BabelNet, is output with possibly overlapping semantic annotations.

==See also==
* [[BabelNet]]
* [[Entity linking]]
* [[Multilinguality]]
* [[Word sense disambiguation]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://babelfy.org}}

[[Category:Lexical semantics]]
[[Category:Semantics]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Artificial intelligence]]
[[Category:Multilingualism]]


{{prog-lang-stub}}</text>
      <sha1>q5dslziv8z3z3x7qaiylxx2l9kly41y</sha1>
    </revision>
  </page>
  <page>
    <title>Flint toolkit</title>
    <ns>0</ns>
    <id>43763321</id>
    <revision>
      <id>684958210</id>
      <parentid>624928560</parentid>
      <timestamp>2015-10-09T21:37:27Z</timestamp>
      <contributor>
        <username>ASammourBot</username>
        <id>20828417</id>
      </contributor>
      <minor />
      <comment>bot: Test of replace stub category to stub template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1935" xml:space="preserve">{{COI|date=September 2014}}
Flint is a software toolkit which supports various modes of uncertainty handling, namely, [[Fuzzy_logic| Fuzzy]], [[Bayesian_inference| Bayesian]] and [[Expert_system#Certainty_factors| Certainty Theory]].

Along with [[Flex_expert_system| Flex]], Flint was licensed to the Open University as part of T396: 'Artificial intelligence for technology'.

Much of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.&lt;ref name = "AI Toolkit"&gt;{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists}} by Adrian Hopgood &lt;/ref&gt;

Flint is produced by [[Logic Programming Associates|LPA]] and runs on PCs and web servers.

==External links==
*[http://www.generation5.org/content/2003/lpainference.asp The Shape of Inference] by Clive Spenser &amp; Charles Langley, Generation5
*[http://www.generation5.org/content/2003/probmodulation.asp Probability Modulation and Non-linearity in Bayesian Networks] by Clive Spenser &amp; Charles Langley, Generation5
*[http://www.generation5.org/content/2004/defuzz.asp Defuzzification Options in Flex] by Clive Spenser &amp; Charles Langley, Generation5
*[http://www.degruyter.com/view/j/pomr.2007.14.issue-3/v10012-007-0012-2/v10012-007-0012-2.xml Application of fuzzy inference to assessment of degree of hazard to ship power plant operator] by Tomasz Kowalewski, Antoni Podsiad&#322;o &amp; Wies&#322;aw Tare&#322;ko
*[http://www.slaai.lk/proc/2007/2007.pdf#page=42 Computational Modeling in Conceptual Models: Widening Scope of Artificial Life] by Mendis, Asoka. S. Karunananda, Samaratunga
*[http://www.lpa.co.uk/fln.htm Flint Overview], LPA

== References ==
{{Reflist}}

[[Category:Expert_systems]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}</text>
      <sha1>lylypzekx98cjlrulwqdhe0dw152do1</sha1>
    </revision>
  </page>
  <page>
    <title>VisiRule</title>
    <ns>0</ns>
    <id>43833827</id>
    <revision>
      <id>650617219</id>
      <parentid>650617087</parentid>
      <timestamp>2015-03-09T15:17:33Z</timestamp>
      <contributor>
        <ip>91.142.235.80</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2453" xml:space="preserve">{{COI|date=September 2014}}

'''VisiRule''' is a graphical tool for non-programmers to develop and deliver rule-based and expert systems simply by drawing their decision logic.

VisiRule is designed for building regulatory compliance systems, financial and legal decision-making systems, and machine
diagnostic and medical systems.

VisiRule generates executable rules in the form of the [[Flex expert system|Flex expert system toolkit]] which was developed by [[Logic Programming Associates|LPA]] in 1989.

LPA set up a dedicated website for VisiRule in 2015 at http://www.visirule.co.uk

VisiRule is used as part of expert systems and decision support courses in Universities such as Uniten.&lt;ref name = "VisiRule Lab"&gt;{{citation |url=http://metalab.uniten.edu.my/~zaihisma/dss/lab/CISB434-module1.ppt | title=VisiRule Slides from Uniten}}&lt;/ref&gt;

==Academic Uses==
In RSA-Expert, VisiRule is used as a decision support tool, in which the rules are basically and precisely presented using a Logic Programming model. RSA-Expert aims to assist researchers in making a decision about utilizing suitable statistical data analysis in research.&lt;ref name = "RSA-Expert"&gt;{{citation |url=http://iiste.org/Journals/index.php/IKM/article/view/4740 | title=Use of a Rule Tool in Data Analysis Decision Making}}&lt;/ref&gt;

TPA-EXPERT is a legal expert system which deals with transfer of property act of Indian legal domain. TPA-EXPERT has a simple representation structure which combine time tested rule based and case based approach.&lt;ref name = "TPA-EXPERT"&gt;{{citation |url=http://www.ijcaonline.org/archives/volume33/number9/4045-5494 | title=TPA-EXPERT: A Hybrid Legal Knowledge Based System for Indian Legal domain}}&lt;/ref&gt;

==External links==
* [http://www.lpa.co.uk/vsr.htm VisiRule Overview, LPA website]
* [http://www.visirule.co.uk/ VisiRule website]
* [http://www.pcai.com/18.3_sample_issue/18.3%20sample%20PDF/PCAI_LPA-pg.29-30-Sample_Issue.pdf "The Visual Development of Rule-Based Systems", PCAI Magazine]
* [http://www.academicpub.com/map/items/2999654.html "Drawing on your Knowledge with VisiRule", C.Spenser, IEEE]

==See also==
* [[Expert system]]
* [[Inference engine]]
* [[Knowledge base]]
* [[Knowledge-based system]]
* [[Knowledge representation]]

== References ==
{{Reflist}}

[[Category:Artificial intelligence stubs]]
[[Category:Expert systems]]
[[Category:Rule engines]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]</text>
      <sha1>acu25fqb5nxa1ezg9iy6xg9v5n6vahu</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic network</title>
    <ns>0</ns>
    <id>29109</id>
    <revision>
      <id>757911417</id>
      <parentid>757910889</parentid>
      <timestamp>2017-01-02T12:21:55Z</timestamp>
      <contributor>
        <username>Krauss</username>
        <id>1222358</id>
      </contributor>
      <minor />
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12503" xml:space="preserve">{{Network Science}}

A '''semantic network''', or '''frame network''',  is a network that represents [[Semantics|semantic]] relations between [[concept]]s. This is often used as a form of [[Knowledge representation and reasoning|knowledge representation]]. It is a [[directed graph|directed]] or [[undirected graph]] consisting of [[vertex (graph theory)|vertices]], which represent [[concept]]s, and [[graph theory|edges]], which represent semantic relations between concepts.&lt;ref name = 'Sowa'/&gt;

Typical standardized semantic networks are expressed as [[semantic triple]]s.

== History ==
[[Image:Semantic Net.svg|thumb|320px|Example of a semantic network]]
"Semantic Nets" were first invented for [[computers]] by [[Richard H. Richens]] of the Cambridge Language Research Unit in 1956 as an "[[Pivot language|interlingua]]" for [[machine translation]] of [[natural language]]s.{{citation needed|date=October 2013}}

They were independently developed by Robert F. Simmons,&lt;ref name='Simmons1963'&gt;{{cite journal | title=Synthetic language behavior | journal=Data Processing Management | year=1963 | last=Robert F. Simmons |volume=5 |issue=12 |pages=11&#8211;18}}&lt;/ref&gt; Sheldon Klein, Karen McConologue, M. Ross Quillian&lt;ref name='Quillian1963'&gt;Quillian, R. A notation for representing conceptual information: An application to semantics and mechanical English para- phrasing. SP-1395, System Development Corporation, Santa Monica, 1963.&lt;/ref&gt; and others at [[System Development Corporation]] in the early 1960s as part of the SYNTHEX project. It later featured prominently in the work of [[Allan M. Collins]] and Quillian (e.g., Collins and Quillian;&lt;ref name='Collins1969'&gt;{{cite journal | title=Retrieval time from semantic memory | journal=Journal of verbal learning and verbal behavior | year=1969 | last1=Allan M. Collins |author2= M. R. Quillian |volume=8 |issue=2 |pages=240&#8211;247 |doi=10.1016/S0022-5371(69)80069-1  }}&lt;/ref&gt;&lt;ref name='Collins1970'&gt;{{cite journal |title=Does category size affect categorization time? |journal=Journal of verbal learning and verbal behavior |year=1970 |first= |last=Allan M. Collins
|author2=M. Ross Quillian  |volume=9 |issue=4 |pages=432&#8211;438 |doi=10.1016/S0022-5371(70)80084-6 }}&lt;/ref&gt; Collins and Loftus&lt;ref name='Collins1975'&gt;{{cite journal |title=A spreading-activation theory of semantic processing |journal=Psychological Review |year=1975 |last=Allan M. Collins |author2=Elizabeth F. Loftus |volume=82 | doi = 10.1037/0033-295x.82.6.407 |pages=407&#8211;428}}&lt;/ref&gt; Quillian&lt;ref&gt;Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12(5), 410-430.&lt;/ref&gt;&lt;ref&gt;Quillian, M. R. (1968). Semantic memory. Semantic information processing, 227&#8211;270.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Quillian | first1 = M. R. | year = 1969 | title = The teachable language comprehender: a simulation program and theory of language | url = | journal = Communications of the ACM | volume = 12 | issue = 8| pages = 459&#8211;476 | doi=10.1145/363196.363214}}&lt;/ref&gt;&lt;ref&gt;Quillian, R. Semantic Memory. Unpublished doctoral dissertation, Carnegie Institute of Technology, 1966.&lt;/ref&gt;)

In the late 1980s, two [[Netherlands]] universities, [[University of Groningen|Groningen]] and [[University of Twente|Twente]], jointly began a project called ''Knowledge Graphs'', which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.&lt;ref&gt;{{cite book |last=Van de Riet |first=R. P. |date=1992 |title=Linguistic Instruments in Knowledge Engineering |url=http://www.stokman.org/artikel/92Jame.KnowGraphs.LIKE.pdf |publisher=Elsevier Science Publishers |page=98 |isbn=0444883940}}&lt;/ref&gt; In the subsequent decades, the distinction between semantic networks and knowledge graphs was blurred.&lt;ref&gt;{{cite conference |url=https://books.google.com/books?id=15PDCgAAQBAJ&amp;pg=PA444 |title=Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation |last1=Hulpus |first1=Ioana |last2=Prangnawarat |first2=Narumol |date=2015 |publisher=Springer International Publishing |book-title=The Semantic Web - ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part 1 |pages=444 |conference=[[International Semantic Web Conference]] 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.authorea.com/users/6341/articles/107281 |title=What is a Knowledge Graph? |last1=McCusker |first1=James P. |last2=Chastain |first2=Katherine |date=April 2016 |website=authorea.com |access-date=15 June 2016 |quote="usage [of the term 'knowledge graph'] has evolved"}}&lt;/ref&gt; In 2012, [[Google]] gave their knowledge graph the name [[Knowledge Graph]].

== Basics of semantic networks ==
A semantic network is used when one has knowledge that is best understood as a set of concepts that are related to one another.

Most semantic networks are cognitively based. They also consist of arcs and nodes which can be organized into a taxonomic hierarchy. Semantic networks contributed ideas of [[spreading activation]], [[inheritance]], and nodes as proto-objects.

== Examples ==

=== Semantic Net in [[Lisp (programming language)|Lisp]] ===
Using an association list.
&lt;source lang="lisp"&gt;
(defun *database* ()
'((canary  (is-a bird)
           (color yellow)
           (size small))
  (penguin (is-a bird)
           (movement swim))
  (bird    (is-a vertebrate)
           (has-part wings)
           (reproduction egg-laying))))
&lt;/source&gt;

You would use the "assoc" function with a key of "canary" to extract all the information about the "canary" type.&lt;ref&gt;{{cite web|last=Swigger|first=Kathleen|title=Semantic.ppt|url=http://zeus.csci.unt.edu/swigger/csci3210/semantic.ppt|accessdate=23 March 2011}}&lt;/ref&gt;

=== WordNet ===
{{Main|WordNet}}
An example of a semantic network is [[WordNet]], a [[lexicon|lexical]] database of [[English language|English]]. It groups English words into sets of synonyms called [[synsets]], provides short, general definitions, and records the various semantic relations between these synonym sets. Some of the most common semantic relations defined are [[meronymy]] (A is part of B, i.e. B has A as a part of itself), [[holonymy]] (B is part of A, i.e. A has B as a part of itself), [[hyponym]]y (or [[troponymy]])  (A is subordinate of B; A is kind of B), [[hypernym]]y (A is superordinate of B), [[synonym]]y (A denotes the same as B) and [[antonym]]y (A denotes the opposite of B).

WordNet properties have been studied from a [[Graph theory|network theory]] perspective and compared to other semantic networks created from [[Roget's Thesaurus]] and [[word association]] tasks.  From this perspective the three of them are a [[Small-world network|small world structure]].&lt;ref name=Steyvers2005&gt;{{cite journal
 | author = Steyvers, M.
 |author2=Tenenbaum, J.B.
  | year = 2005
 | title = The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth
 | journal = Cognitive Science
 | volume = 29
 | issue = 1
 | pages = 41&#8211;78
 | doi = 10.1207/s15516709cog2901_3
}}&lt;/ref&gt;

=== Other examples ===
It is also possible to represent logical descriptions using semantic networks such as the [[existential graph]]s of [[Charles Sanders Peirce]] or the related [[conceptual graph]]s of [[John F. Sowa]].&lt;ref name='Sowa'&gt;{{cite encyclopedia
|author=John F. Sowa
|editor=Stuart C Shapiro
|encyclopedia=Encyclopedia of Artificial Intelligence
|title=Semantic Networks
|url=http://www.jfsowa.com/pubs/semnet.htm
|accessdate=2008-04-29
|year=1987
|authorlink=John F. Sowa}}&lt;/ref&gt; These have expressive power equal to or exceeding standard [[first-order predicate calculus|first-order predicate logic]].  Unlike WordNet or other lexical or browsing networks, semantic networks using these representations can be used for reliable automated logical deduction.  Some automated reasoners exploit the graph-theoretic features of the networks during processing.

Other examples of semantic networks are [[Gellish]] models. [[Gellish English]] with its [[Gellish English dictionary]], is a [[formal language]] that is defined as a network of relations between concepts and names of concepts. Gellish English is a formal subset of natural English, just as Gellish Dutch is a formal subset of Dutch, whereas multiple languages share the same concepts. Other Gellish networks consist of knowledge models and information models that are expressed in the Gellish language. A Gellish network is a network of (binary) relations between things. Each relation in the network is an expression of a fact that is classified by a relation type. Each relation type itself is a concept that is defined in the Gellish language dictionary. Each related thing is either a concept or an individual thing that is classified by a concept. The definitions of concepts are created in the form of definition models (definition networks) that together form a Gellish Dictionary. A Gellish network can be documented in a Gellish database and is computer interpretable.

[[SciCrunch]] is a collaboratively edited knowledge base for scientific resources. It provides unambiguous identifiers (Research Resource IDentifiers or RRIDs) for software, lab tools etc. and it also provides options to create links between RRIDs and from communities.

Another example of semantic networks, based on [[category theory]], is [[olog]]s. Here each type is an object, representing a set of things, and each arrow is a morphism, representing a function. [[Commutative diagrams]] also are prescribed to constrain the semantics.

In the social sciences people sometimes use the term semantic network to refer to [[co-occurrence networks]].&lt;ref name='Atteveldt'&gt;{{cite book
|author=Wouter Van Atteveldt
|title=Semantic Network Analysis: Techniques for Extracting, Representing, and Querying Media Content
|publisher=BookSurge Publishing
|year=2008}}&lt;/ref&gt; The basic idea is that words that co-occur in a unit of text, e.g. a sentence, are semantically related to one another. Ties based on co-occurrence can then be used to construct semantic networks.

== Software tools ==
There are also elaborate types of semantic networks connected with corresponding sets of software tools used for [[Lexicon|lexical]] [[knowledge engineering]], like the Semantic Network Processing System ([[SNePS]]) of Stuart C. Shapiro&lt;ref&gt;[http://www.cse.buffalo.edu/~shapiro/ Stuart C. Shapiro]&lt;/ref&gt; or the [[MultiNet]] paradigm of Hermann Helbig,&lt;ref&gt;[http://pi7.fernuni-hagen.de/helbig/index_en.html Hermann Helbig]&lt;/ref&gt; especially suited for the semantic representation of natural language expressions and used in several [[Natural language processing|NLP]] applications.

Semantic networks are used in specialized information retrieval tasks, such as [[plagiarism]] detection. They provide information on hierarchical relations in order to employ [[semantic compression]] to reduce language diversity and enable the system to match word meanings, independently from sets of words used.

== See also ==
{{Div col}}
* [[Abstract semantic graph]]
* [[Chunking (psychology)]]
* [[Network diagram]]
* [[Ontology (information science)]]
* [[Repertory grid]]
* [[Semantic lexicon]]
* [[Semantic neural network]]
* [[SemEval]] - an ongoing series of evaluations of [[Semantic analysis (computational)|computational semantic analysis]] systems
* [[Sparse distributed memory]]
* [[Taxonomy (general)]]
* [[Unified Medical Language System]] (UMLS)
* [[Word-sense disambiguation]] (WSD)
{{Div col end}}

=== Other examples ===
* [[Cognition Network Technology]]
* [[Lexipedia]]
* [[Open Mind Common Sense]] (OMCS)
* [[Schema.org]]
* [[SNOMED CT]]
* [[Universal Networking Language]] (UNL)
* [[Wikidata]]

== References ==
{{reflist|30em}}

== Further reading ==
* Allen, J. and A. Frisch (1982). "What's in a Semantic Network". In: ''Proceedings of the 20th. annual meeting of ACL'', Toronto, pp.&amp;nbsp;19&#8211;27.
* John F. Sowa, Alexander Borgida (1991). ''Principles of Semantic Networks: Explorations in the Representation of Knowledge''.

== External links ==
{{Commons category|Semantic networks}}
* [http://www.jfsowa.com/pubs/semnet.htm "Semantic Networks"] by John F. Sowa
* [http://www.knowledgegrid.net/~H.Zhuge/SLN.htm "Semantic Link Network" ] by Hai Zhuge

{{Semantic Web}}
{{Use dmy dates|date=August 2011}}

{{Authority control}}

[[Category:Knowledge representation]]
[[Category:Networks]]</text>
      <sha1>l9kyidky576ats53bz60sym9m52rm9i</sha1>
    </revision>
  </page>
  <page>
    <title>WordNet</title>
    <ns>0</ns>
    <id>33955</id>
    <revision>
      <id>763075132</id>
      <parentid>760466563</parentid>
      <timestamp>2017-02-01T05:10:15Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor />
      <comment>/* Knowledge structure */ {{tree list}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33042" xml:space="preserve">'''WordNet''' is a [[lexical database]] for the [[English language]].&lt;ref&gt;G. A. Miller, R. Beckwith, C. D. Fellbaum, D. Gross, K. Miller. 1990. WordNet: An online lexical database. Int. J. Lexicograph. 3, 4, pp. 235&#8211;244.&lt;/ref&gt; It groups English [[word]]s into sets of [[synonyms]] called ''[[synsets]]'', provides short definitions and usage examples, and records a number of relations among  these synonym sets or their members. WordNet can thus be seen as a combination of [[dictionary]] and [[thesaurus]]. While it is  accessible to human users via a [[web browser]],&lt;ref name="WordNet Search"&gt;{{cite web|url=http://wordnetweb.princeton.edu/perl/webwn|title=WordNet Search - 3.1}}&lt;/ref&gt; its primary use is in automatic [[natural language processing|text analysis]] and [[artificial intelligence]] applications. The [[database]] and [[software]] tools have been released under a [[BSD License|BSD style license]] and are freely available for download from the WordNet website. Both the lexicographic data (''lexicographer files'') and the compiler (''called grind'') for producing the distributed database are available.
[[File:WordNet.PNG|thumb|This is a snapshot of WordNet's definition of itself.]]

== History and team members ==
WordNet was created in the [[Cognitive Science]] Laboratory of [[Princeton University]] under the direction of  [[psychology]] [[professor]] [[George Armitage Miller]] starting in 1985 and has been directed in recent years by [[Christiane Fellbaum]]. The project received funding from government agencies including the [[National Science Foundation]], [[DARPA]], the [[Disruptive Technology Office]] (formerly the Advanced Research and Development Activity), and REFLEX. George Miller and Christiane Fellbaum were awarded the 2006 [[European Language Resources Association#Antonio Zampolli Prize|Antonio Zampolli Prize]] for their work with WordNet.

== Database contents ==
[[File:Hamburger WordNet.png|thumb|Example entry "Hamburger" in WordNet]]

As of November 2012 WordNet's latest Online-version is 3.1.&lt;ref&gt;{{cite web|url=http://wordnet.princeton.edu/wordnet/download/current-version/ |title=Current WordNet version |publisher=Wordnet.princeton.edu |date=2012-11-09 |accessdate=2014-03-11}}&lt;/ref&gt; The  database contains 155,287 words organized in 117,659 [[synsets]] for a total of 206,941 word-sense pairs; in [[data compression|compressed]] form, it is about 12 [[megabyte]]s in size.&lt;ref&gt;{{cite web|url=http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html |title=WordNet Statistics |publisher=Wordnet.princeton.edu |date= |accessdate=2014-03-11}}&lt;/ref&gt;

WordNet includes the lexical categories [[noun]]s, [[verb]]s, [[adjective]]s and [[adverb]]s but ignores [[preposition]]s, [[determiner (linguistics)|determiner]]s and other function words.

Words from the same lexical category that are roughly synonymous are grouped into [[synsets]]. Synsets include simplex words as well as [[collocation]]s like "eat out" and "car pool." The different senses of a [[polysemous]] word form are assigned to different synsets. The meaning of a synset is further clarified with a short defining ''gloss'' and one or more usage examples. An example adjective synset is:

: good, right, ripe &#8211; (most suitable or right for a particular purpose; "a good time to plant tomatoes"; "the right time to act"; "the time is ripe for great sociological changes")

All synsets are connected to other synsets by means of semantic relations. These relations, which are not all shared by all lexical categories, include:
* [[Noun]]s
**''[[hypernym]]s'': ''Y'' is a hypernym of ''X'' if every ''X'' is a (kind of) ''Y'' (''canine'' is a hypernym of ''[[dog]]'')
**''[[hyponym]]s'': ''Y'' is a hyponym of ''X'' if every ''Y'' is a (kind of) ''X'' (''dog'' is a hyponym of ''canine'')
**''coordinate terms'': ''Y'' is a coordinate term of ''X'' if ''X'' and ''Y'' share a hypernym (''wolf'' is a coordinate term of ''dog'', and ''dog'' is a coordinate term of ''wolf'')
**''[[meronymy|meronym]]'': ''Y'' is a meronym of ''X'' if ''Y'' is a part of ''X'' (''window'' is a meronym of ''building'')
**''[[holonymy|holonym]]'': ''Y'' is a holonym of ''X'' if ''X'' is a part of ''Y'' (''building'' is a holonym of ''window'')
* [[Verb]]s
**''hypernym'': the verb ''Y'' is a hypernym of the verb ''X'' if the activity ''X'' is a (kind of) ''Y'' (''to perceive'' is an hypernym of ''to listen'')
**''[[troponym]]'': the verb ''Y'' is a troponym of the verb ''X'' if the activity ''Y'' is doing ''X'' in some manner (''to lisp'' is a troponym of ''to talk'')
**''[[entailment]]'': the verb ''Y'' is entailed by ''X'' if by doing ''X'' you must be doing ''Y'' (''to sleep'' is entailed by ''to snore'')
**''coordinate terms'': those verbs sharing a common hypernym (''to lisp'' and ''to yell'')

These semantic relations hold among all members of the linked synsets. Individual synset members (words) can also be connected with  lexical relations. For example, (one sense of) the noun "director" is linked to (one sense of) the verb "direct" from which it is derived via a "morphosemantic" link.

The morphology functions of the software distributed with the database try to deduce the [[Lemma (morphology)|lemma]] or [[stem (linguistics)|stem]] form of a [[word]] from the user's input. Irregular forms are stored in a list, and looking up "ate" will return "eat," for example.

== Knowledge structure ==
Both nouns and verbs are organized into hierarchies, defined by [[hypernym]] or ''[[is-a|IS A]]'' relationships. For instance, one sense of the word ''dog'' is found following hypernym hierarchy; the words at the same level represent synset members.  Each set of synonyms has a unique index.
{{tree list}}
* {{Tree list/final branch}}dog, domestic dog, Canis familiaris
** {{Tree list/final branch}}canine, canid
*** {{Tree list/final branch}}carnivore
**** {{Tree list/final branch}}placental, placental mammal, eutherian, eutherian mammal
***** {{Tree list/final branch}}mammal
****** {{Tree list/final branch}}vertebrate, craniate
******* {{Tree list/final branch}}chordate
******** {{Tree list/final branch}}animal, animate being, beast, brute, creature, fauna
********* {{Tree list/final branch}}...
{{tree list/end}}
At the top level, these hierarchies are organized into 25 beginner "trees" for nouns and 15 for verbs (called ''lexicographic files'' at a maintenance level).  All are linked to a unique beginner synset, "entity."
Noun hierarchies are far deeper than verb hierarchies

Adjectives are not organized into hierarchical trees. Instead, two "central" antonyms such as "hot" and "cold" form binary poles, while 'satellite' synonyms such as "steaming" and "chilly" connect to their respective poles via a "similarity" relations. The adjectives can be visualized in this way as "dumbbells" rather than as "trees."

== Psycholinguistic aspects of WordNet ==

The initial goal of the WordNet project was to build a lexical database that would be consistent with theories of human semantic memory developed in the late 1960s.  Psychological experiments indicated that speakers organized their knowledge of concepts in an economic, hierarchical fashion. Retrieval time required to access conceptual knowledge seemed to be directly related to the number of hierarchies the speaker needed to "traverse" to access the knowledge. Thus, speakers could more quickly verify that ''canaries can sing'' because a canary is a songbird ("sing" is a property stored on the same level as "canary"), but required slightly more time to verify that ''canaries can fly'' (where they had to access the concept "bird" on the superordinate level) and even more time to verify ''canaries have skin'' (requiring look-up across multiple levels of hyponymy, up to "animal").&lt;ref&gt;Collins A., Quillian M. R. 1972. Experiments on Semantic Memory and Language Comprehension. In ''Cognition in Learning and Memory''. Wiley, New York.&lt;/ref&gt;
While such experiments and the underlying theories have been subject to criticism, some of WordNet's organization is consistent with experimental evidence. For example, [[anomic aphasia]] selectively affects speakers' ability to produce words from a specific semantic category, a WordNet hierarchy. Antonymous adjectives (WordNet's central adjectives in the dumbbell structure) are found to co-occur far more frequently than chance, a fact that has been found to hold for many languages.

== WordNet as a lexical ontology ==

WordNet is sometimes called an ontology, a persistent claim that its creators do not make. The hypernym/hyponym relationships among the noun synsets can be interpreted as specialization relations among conceptual categories. In other words, WordNet can be interpreted and used as a lexical [[ontology (computer science)|ontology]] in the [[computer science]] sense. However, such an ontology should normally be corrected before being used since it contains hundreds of basic semantic inconsistencies such as (i) the existence of common specializations for exclusive categories and (ii) redundancies in the specialization hierarchy. Furthermore, transforming WordNet into a lexical ontology usable for knowledge representation should normally also involve (i)&amp;nbsp;distinguishing the specialization relations into ''subtypeOf'' and ''instanceOf'' relations, and (ii)&amp;nbsp;associating intuitive unique identifiers to each category. Although such corrections and transformations have been performed and documented as part of the integration of WordNet&amp;nbsp;1.7 into the cooperatively updatable knowledge base of WebKB-2,&lt;ref&gt;{{cite web|author=http://www.phmartin.info |url=http://www.webkb.org/doc/wn/ |title=Integration of WordNet 1.7 in WebKB-2|publisher=Webkb.org |date= |accessdate=2014-03-11}}&lt;/ref&gt; most projects claiming to re-use WordNet for knowledge-based applications (typically, knowledge-oriented information retrieval) simply re-use it directly.

WordNet has also been converted to a formal specification, by means of a hybrid bottom-up top-down methodology to automatically extract association relations from WordNet, and interpret these associations in terms of a set of conceptual relations, formally defined in the [[Upper ontology (computer science)#DOLCE and DnS|DOLCE foundational ontology]].&lt;ref&gt;{{cite book |first1=A. |last1=Gangemi |first2=R. |last2=Navigli |first3=P. |last3=Velardi |url=http://www.w3.org/2001/sw/BestPractices/WNET/ODBASE-OWN.pdf |format=PDF |title=The OntoWordNet Project: Extension and Axiomatization of Conceptual Relations in WordNet |work= Proc. of International Conference on Ontologies, Databases and Applications of SEmantics (ODBASE 2003) |location=Catania, Sicily (Italy) |year=2003 |pages= 820&#8211;838}}&lt;/ref&gt;

In most works that claim to have integrated WordNet into ontologies, the content of WordNet has not simply been corrected when it seemed necessary; instead, WordNet has been heavily re-interpreted and updated whenever suitable. This was the case when, for example, the top-level ontology of WordNet was re-structured&lt;ref&gt;{{cite conference | first1 = A. | last1 = Oltramari | first2 = A. | last2 = Gangemi | first3 = N. | last3 = Guarino | first4 = C. | last4 = Masolo | date = 2002 | title = Restructuring WordNet's Top-Level: The OntoClean approach | citeseerx = 10.1.1.19.6574 | conference = OntoLex'2 Workshop, Ontologies and Lexical Knowledge Bases (LREC 2002) | location = Las Palmas, Spain | pages = 17&#8211;26 }}&lt;/ref&gt; according to the [[OntoClean]] based approach or when WordNet was used as a primary source for constructing the lower classes of the SENSUS ontology.

== Limitations ==

WordNet does not include information about the [[etymology]] or the pronunciation of words and it contains only limited information about usage.
WordNet aims to cover most of everyday English and does not include much domain-specific terminology.

WordNet is the most commonly used computational lexicon of English for [[word sense disambiguation]] (WSD), a task aimed to assigning the context-appropriate meanings (i.e. synset members) to words in a text.&lt;ref&gt;R. Navigli. [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ''ACM Computing Surveys'', 41(2), 2009, pp. 1&#8211;69&lt;/ref&gt; However, it has been argued that WordNet encodes sense distinctions that are too fine-grained. This issue prevents WSD systems from achieving a level of performance comparable to that of humans, who do not always agree when confronted with the task of selecting a sense from a dictionary that matches a word in a context. The granularity issue has been tackled by proposing [[cluster analysis|clustering]] methods that automatically group together similar senses of the same word.&lt;ref&gt;E. Agirre, O. Lopez. 2003.
Clustering WordNet Word Senses. In ''Proc. of the Conference on Recent Advances on Natural Language (RANLP&#8217;03)'', Borovetz, Bulgaria, pp. 121&#8211;130.&lt;/ref&gt;&lt;ref&gt;R. Navigli. [http://acl.ldc.upenn.edu/P/P06/P06-1014.pdf Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance], In ''Proc. of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL 2006)'', Sydney, Australia, July 17-21st, 2006, pp. 105&#8211;112.&lt;/ref&gt;&lt;ref&gt;R. Snow, S. Prakash, D. Jurafsky, A. Y. Ng. 2007. [http://www.aclweb.org/anthology/D/D07/D07-1107.pdf Learning to Merge Word Senses], ''In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)'', Prague, Czech Republic, pp. 1005&#8211;1014.&lt;/ref&gt;

=== Licensed vs. Open WordNets ===
Some wordnets were subsequently created for other languages. A 2012 survey lists the wordnets and their availability.&lt;ref&gt;Francis Bond and Kyonghee Paik 2012a. [http://web.mysites.ntu.edu.sg/fcbond/open/pubs/2012-gwc-wn-license.pdf A survey of wordnets and their licenses]. In Proceedings of the 6th Global WordNet Conference (GWC 2012). Matsue. 64&#8211;71&lt;/ref&gt; In an effort to propagate the usage of WordNets, the Global WordNet community had been slowly re-licensing their WordNets to an open domain where researchers and developers can easily access and use WordNets as language resources to provide [[ontology|ontological]] and [[lexicon|lexical]] knowledge in [[Natural Language Processing]] tasks.

The Open Multilingual WordNet&lt;ref&gt;http://compling.hss.ntu.edu.sg/omw/&lt;/ref&gt; provides access to [[Open-source license|open licensed]] wordnets in a variety of languages, all linked to the Princeton Wordnet of English (PWN). The goal is to make it easy to use wordnets in multiple languages.

== Applications ==

WordNet has been used for a number of different purposes in information systems, including [[word-sense disambiguation]], [[information retrieval]], [[Document classification|automatic text classification]], [[Automatic summarization|automatic text summarization]], [[machine translation]] and even automatic crossword puzzle generation.

A common use of WordNet is to determine the [[semantic similarity|similarity]] between words. Various algorithms have been proposed, and these include measuring the distance among the words and synsets in WordNet's graph structure, such as by counting the number of edges among synsets. The intuition is that the closer two words or synsets are, the closer their meaning. A number of WordNet-based word similarity algorithms are implemented in a [[Perl]] package called WordNet::Similarity,&lt;ref&gt;{{cite web|url=http://www.d.umn.edu/~tpederse/similarity.html |title=Ted Pedersen - WordNet::Similarity |publisher=D.umn.edu |date=2008-06-16 |accessdate=2014-03-11}}&lt;/ref&gt; and in a [[Python (programming language)|Python]] package called [[NLTK]].
Other more sophisticated WordNet-based similarity techniques include ADW,&lt;ref&gt;M. T. Pilehvar, D. Jurgens and R. Navigli.  [http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2013_Pilehvar_Jurgens_Navigli.pdf Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity.]. Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), Sofia, Bulgaria, August 4&#8211;9, 2013, pp. 1341-1351.&lt;/ref&gt; whose implementation is available in [[Java (programming language)|Java]]. WordNet can also be used to inter-link other vocabularies.&lt;ref&gt;{{cite journal  |vauthors=Ballatore A, etal |volume=20|issue=2| arxiv=1404.5372| journal=Annals of GIS |title=Linking geographic vocabularies through WordNet |publisher= |date=2014}}&lt;/ref&gt;

== Interfaces ==
Princeton maintains a list of related projects&lt;ref&gt;{{cite web|url=http://wordnet.princeton.edu/wordnet/related-projects/ |title=Related projects - WordNet - Related projects |publisher=Wordnet.princeton.edu |date=2014-01-06 |accessdate=2014-03-11}}&lt;/ref&gt; that includes links to some of the widely used [[application programming interface]]s available for accessing WordNet using various programming languages and environments.

== Related projects and extensions ==
WordNet is connected to several databases of the [[Semantic Web]]. WordNet is also commonly re-used via mappings between the WordNet synsets and the categories from ontologies. Most often, only the top-level categories of WordNet are mapped.

===Global WordNet Association===
The Global WordNet Association (GWA)&lt;ref&gt;{{cite web|author=The Global WordNet Association |url=http://www.globalwordnet.org/ |title=globalwordnet.org |publisher=globalwordnet.org |date=2010-02-04 |accessdate=2014-03-11}}&lt;/ref&gt; is a public and non-commercial organization that provides a platform for discussing, sharing and connecting wordnets for all languages in the world. The GWA also promotes the standardization of wordnets across different languages to ensure its uniformity in enumerating the different synsets in human languages. The GWA keeps a list of wordnets developed around the world.&lt;ref&gt;{{cite web|title=Wordnets in the World|url=http://www.globalwordnet.org/gwa/wordnet_table.html|archiveurl=https://web.archive.org/web/20111021114613/http://www.globalwordnet.org/gwa/wordnet_table.html |archivedate=2011-10-21}}&lt;/ref&gt;

===Other languages===
* [[Arabic WordNet]]:&lt;ref&gt;Black W., Elkateb S., Rodriguez H., Alkhalifa M., Vossen P., Pease A., Bertran M., Fellbaum C., (2006) The Arabic WordNet Project, Proceedings of LREC 2006&lt;/ref&gt;&lt;ref&gt;Lahsen Abouenour, Karim Bouzoubaa, Paolo Rosso (2013) On the evaluation and improvement of Arabic WordNet coverage and usability, Language Resources and Evaluation 47(3) pp 891&#8211;917&lt;/ref&gt; WordNet for Arabic language.
* [[Malayalam WordNet]] , developed by [[Cochin University of Science and Technology|Cochin University Of Science and Technology]]
* CWN (Chinese Wordnet or &#20013;&#25991;&#35422;&#24409;&#32178;&#36335;) supported by [[National Taiwan University]].&lt;ref&gt;[http://lope.linguistics.ntu.edu.tw/cwn/ Chinese Wordnet (&#20013;&#25991;&#35422;&#24409;&#32178;&#36335;) official page] at National Taiwan University&lt;/ref&gt;
* WOLF (WordNet Libre du Fran&#231;ais), a French version of WordNet.&lt;ref&gt;S. Beno&#238;t, F. Darja. 2008. [http://alpage.inria.fr/~sagot/pub/Ontolex08.pdf Building a free French wordnet from multilingual resources]. In ''Proc. of Ontolex 2008'', Marrakech, Maroc.&lt;/ref&gt;
* JAWS (Just Another WordNet Subset), another French version of WordNet&lt;ref&gt;C. Mouton, G. de Chalendar. 2010.[http://www.iro.umontreal.ca/~felipe/TALN2010/Xml/Papers/all/taln2010_submission_71.pdf JAWS : Just Another WordNet Subset]. In ''Proc. of TALN 2010''.&lt;/ref&gt; built using the Wiktionary and semantic spaces
* The [[IndoWordNet]]&lt;ref name="PushpakBhattacharyya"&gt;Pushpak Bhattacharyya, IndoWordNet, Lexical Resources Engineering Conference 2010 (LREC 2010), Malta, May, 2010.&lt;/ref&gt; is a linked lexical knowledge base of wordnets of 18 scheduled languages of India.
* The MultiWordNet project,&lt;ref&gt;E. Pianta, L. Bentivogli, C. Girardi. 2002. [http://multiwordnet.itc.it/paper/MWN-India-published.pdf MultiWordNet: Developing an aligned multilingual database]. In ''Proc. of the 1st International Conference on Global WordNet'', Mysore, India, pp. 21&#8211;25.&lt;/ref&gt; a multilingual WordNet aimed at producing an Italian WordNet strongly aligned with the Princeton WordNet.
* The [[EuroWordNet]] project&lt;ref&gt;P. Vossen, Ed. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer, Dordrecht, The Netherlands.&lt;/ref&gt; has produced WordNets for several European languages and linked them together; these are not freely available however. The Global Wordnet project attempts to coordinate the production and linking of "wordnets" for all languages.&lt;ref&gt;{{cite web|url=http://www.globalwordnet.org/ |title=The Global WordNet Association |publisher=Globalwordnet.org |date=2010-02-04 |accessdate=2014-01-05}}&lt;/ref&gt; [[Oxford University Press]], the publisher of the [[Oxford English Dictionary]], has voiced plans to produce their own online competitor to WordNet.{{Citation needed|date=May 2009}}
* The BalkaNet project&lt;ref&gt;D. Tufis, D. Cristea, S. Stamou. 2004. [http://www.racai.ro/~tufis/papers/Tufis-CS-ROMJIST2004.pdf Balkanet: Aims, methods, results and perspectives. A general overview]. ''Romanian J. Sci. Tech. Inform. (Special Issue on Balkanet)'', 7(1-2), pp. 9&#8211;43.&lt;/ref&gt; has produced WordNets for six European languages (Bulgarian, Czech, Greek, Romanian, Turkish and Serbian). For this project, a freely available XML-based WordNet editor was developed. This editor &#8211; VisDic &#8211; is not in active development anymore, but is still used for the creation of various WordNets. Its successor, DEBVisDic, is client-server application and is currently used for the editing of several WordNets (Dutch in Cornetto project, Polish, Hungarian, several African languages, Chinese).
* UWN is an automatically constructed multilingual lexical knowledge base extending WordNet to cover over a million words in many different languages.&lt;ref&gt;{{cite web|url=http://www.mpi-inf.mpg.de/yago-naga/uwn |title=UWN: Towards a Universal Multilingual Wordnet - D5: Databases and Information Systems (Max-Planck-Institut f&#252;r Informatik) |publisher=Mpi-inf.mpg.de |date=2011-08-14 |accessdate=2014-01-05}}&lt;/ref&gt;
* Such projects as BalkaNet and EuroWordNet made it feasible to create standalone wordnets linked to the original one. One of such projects is Russian WordNet patronized by [[Petersburg State University of Means of Communication]]&lt;ref&gt;{{cite web|url=http://www.pgups.ru/abitur/inostrancam/inter/ruwordnet/ |title=&#1056;&#1091;&#1089;&#1089;&#1082;&#1080;&#1081; WordNet |publisher=Pgups.ru |date= |accessdate=2014-01-05}}&lt;/ref&gt; or Russnet&lt;ref&gt;{{cite web|url=http://project.phil.spbu.ru/RussNet/index_ru.shtml |title=RussNet: &#1043;&#1083;&#1072;&#1074;&#1085;&#1072;&#1103; &#1089;&#1090;&#1088;&#1072;&#1085;&#1080;&#1094;&#1072; |publisher=Project.phil.spbu.ru |date= |accessdate=2014-03-11}}&lt;/ref&gt; by [[Saint Petersburg State University]]
* FinnWordNet is a Finnish version of the WordNet where all entries of the original English WordNet were translated.&lt;ref&gt;{{cite web|url=http://www.ling.helsinki.fi/en/lt/research/finnwordnet/ |title=FinnWordNet &#8211; The Finnish WordNet - Department of General Linguistics |publisher=Ling.helsinki.fi |date= |accessdate=2014-01-05}}&lt;/ref&gt;
* [[GermaNet]] is a German version of the WordNet developed by the University of T&#252;bingen.&lt;ref&gt;{{cite web|url=http://www.sfs.uni-tuebingen.de/lsd/index.shtml |title=GermaNet |publisher=Sfs.uni-tuebingen.de |date= |accessdate=2014-03-11}}&lt;/ref&gt;
* OpenWN-PT  is a Brazilian Portuguese version of the original WordNet freely available for download under CC-BY-SA license.&lt;ref&gt;{{cite web|url=https://github.com/arademaker/openWordnet-PT |title=arademaker/openWordnet-PT &#8212; GitHub |publisher=Github.com |date= |accessdate=2014-01-05}}&lt;/ref&gt;
* [[plWordNet]]&lt;ref&gt;http://plwordnet.pwr.wroc.pl/wordnet/ official webpage&lt;/ref&gt; is a Polish-language version of WordNet developed by [[Wroc&#322;aw University of Technology]].
* PolNet&lt;ref&gt;http://www.ltc.amu.edu.pl/polnet/ official webpage&lt;/ref&gt; is a Polish-language version of WordNet developed by [[Adam Mickiewicz University in Pozna&#324;]] (distributed under CC BY-NC-ND 3.0 license).
* [[BulNet]] is a Bulgarian version of the WordNet developed at the Department of Computational Linguistics of the [[Institute for Bulgarian Language]], Bulgarian Academy of Sciences.&lt;ref&gt;{{cite web|url=http://dcl.bas.bg/BulNet/general_en.html |title=BulNet |publisher=dcl.bas.bg |date= |accessdate=2015-05-07}}&lt;/ref&gt;

===Linked data===

* [[BabelNet]],&lt;ref&gt;R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11&#8211;16, 2010, pp. 216&#8211;225.&lt;/ref&gt; a very large multilingual [[semantic network]] with millions of concepts obtained from an integration of WordNet and Wikipedia based on an automatic mapping algorithm.
* The [[Suggested Upper Merged Ontology|SUMO]] ontology&lt;ref&gt;A. Pease, I. Niles, J. Li. 2002. [https://www.aaai.org/Papers/Workshops/2002/WS-02-11/WS02-11-011.pdf The suggested upper merged ontology: A large ontology for the Semantic Web and its applications]. In ''Proc. of the AAAI-2002 Workshop on Ontologies and the Semantic Web'', Edmonton, Canada.&lt;/ref&gt; has produced a mapping between all of the WordNet synsets, (including nouns, verbs, adjectives and adverbs), and [[SUMO class]]es.  The most recent addition of the mappings provides links to all of the more specific terms in the MId-Level Ontology (MILO), which extends SUMO.
* [[OpenCyc]],&lt;ref&gt;S. Reed and D. Lenat. 2002. [http://www.cyc.com/doc/white_papers/mapping-ontologies-into-cyc_v31.pdf Mapping Ontologies into Cyc]. In ''Proc. of AAAI 2002 Conference Workshop on Ontologies For The Semantic Web'', Edmonton, Canada, 2002&lt;/ref&gt; an open [[ontology (information science)|ontology]] and [[knowledge base]] of everyday common sense knowledge, has 12,000 terms linked to WordNet synonym sets.
* [[Descriptive Ontology for Linguistic and Cognitive Engineering|DOLCE]],&lt;ref&gt;Masolo, C., Borgo, S., Gangemi, A., Guarino, N., Oltramari, A., Schneider, L.S. 2002. [http://www.loa-cnr.it/Papers/WonderWebD17V2.0.pdf WonderWeb Deliverable D17. The WonderWeb Library of Foundational Ontologies and the DOLCE ontology]. Report (ver. 2.0, 15-08-2002)&lt;/ref&gt; is the first module of the WonderWeb Foundational Ontologies Library (WFOL). This upper-ontology has been developed in light of rigorous ontological principles inspired by the philosophical tradition, with a clear orientation toward language and cognition. OntoWordNet&lt;ref&gt;Gangemi, A., Guarino, N., Masolo, C., Oltramari, A. 2003 [http://www.loa-cnr.it/Papers/AIMag24-03-003.pdf Sweetening WordNet with DOLCE]. In AI Magazine 24(3): Fall 2003, pp. 13&#8211;24&lt;/ref&gt; is the result of an experimental effort to align WordNet's upper level with DOLCE. It is suggested that such alignment could lead to an "ontologically sweetened" WordNet, meant to be conceptually more rigorous, cognitively transparent, and efficiently exploitable in several applications.
* [[DBpedia]],&lt;ref&gt;C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, [http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/Bizer-etal-DBpedia-CrystallizationPoint-JWS-Preprint.pdf DBpedia &#8211; A crystallization point for the Web of Data]. Web Semantics, 7(3), 2009, pp. 154&#8211;165&lt;/ref&gt; a database of structured information, is also linked to WordNet.
* The [[eXtended WordNet]]&lt;ref&gt;S. M. Harabagiu, G. A. Miller, D. I. Moldovan. 1999. [http://www.ldc.upenn.edu/acl/W/W99/W99-0501.pdf WordNet 2 &#8211; A Morphologically and Semantically Enhanced Resource]. In ''Proc. of the ACL SIGLEX Workshop: Standardizing Lexical Resources'', pp. 1&#8211;8.&lt;/ref&gt; is a project at the [[University of Texas at Dallas]] which aims to improve WordNet by semantically parsing the glosses, thus making the information contained in these definitions available for automatic knowledge processing systems. It is also freely available under a license similar to WordNet's.
* The [[GCIDE]] project produced a dictionary by combining a [[public domain]] ''[[Webster's Dictionary]]'' from 1913 with some WordNet definitions and material provided by volunteers. It was released under the [[copyleft]] license [[GNU General Public License|GPL]].
* [[ImageNet]] is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.&lt;ref&gt;J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei. [https://nlpainter.googlecode.com/svn-history/r16/trunk/papers/ImageNet__cvpr09.pdf ImageNet: A Large-Scale Hierarchical Image Database]. In ''Proc. of 2009 IEEE Conference on Computer Vision and Pattern Recognition''&lt;/ref&gt; Currently it has an average of over five hundred images per node.
* BioWordnet, a biomedical extension of wordnet was abandoned due to issues about stability over versions.&lt;ref&gt;M. Poprat, E. Beisswanger, U. Hahn. 2008. [http://www.aclweb.org/anthology/W/W08/W08-0507.pdf Building a BIOWORDNET by Using WORDNET&#8217;s Data Formats and WORDNET&#8217;s Software Infrastructure &#8211; A Failure Story]. In ''Proc. of the Software Engineering, Testing, and Quality Assurance for Natural Language Processing Workshop'', pp. 31&#8211;39.&lt;/ref&gt;
* WikiTax2WordNet, a mapping between WordNet synsets and [[Wikipedia:Categorization|Wikipedia categories]].&lt;ref&gt;S. Ponzetto, R. Navigli. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia], In ''Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009)'', Pasadena, California, July 14-17th, 2009, pp. 2083&#8211;2088.&lt;/ref&gt;
* WordNet++, a resource including over millions of semantic edges harvested from Wikipedia and connecting pairs of WordNet synsets.&lt;ref&gt;S. P. Ponzetto, R. Navigli. [http://aclweb.org/anthology-new/P/P10/P10-1154.pdf Knowledge-rich Word Sense Disambiguation rivaling supervised systems]. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010, pp. 1522&#8211;1531.&lt;/ref&gt;
* SentiWordNet, a resource for supporting opinion mining applications obtained by tagging all the WordNet 3.0 synsets according to their estimated degrees of positivity, negativity, and neutrality.&lt;ref&gt;S. Baccianella, A. Esuli and F. Sebastiani. [http://nemis.isti.cnr.it/sebastiani/Publications/LREC10.pdf SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining]. In Proceedings of the 7th Conference on Language Resources and Evaluation (LREC'10), Valletta, MT, 2010, pp. 2200&#8211;2204.&lt;/ref&gt;
* ColorDict, is an Android application to mobiles phones that use Wordnet database and others, like Wikipedia.
* [[UBY-LMF]] a database of 10 resources including WordNet.

===Related projects===
* [[FrameNet]] is a lexical database that shares some similarities with, and refers to, WordNet. 
* [[Lexical markup framework]] (LMF) is an ISO standard specified within [[ISO/TC37]] in order to define a common standardized framework for the construction of lexicons, including WordNet. The subset of LMF for Wordnet is called Wordnet-LMF. An instantiation has been made within the KYOTO project.&lt;ref&gt;Piek Vossen, Claudia Soria, Monica Monachini: Wordnet-LMF: a standard representation for multilingual wordnets, in ''LMF Lexical Markup Framework'', edited by Gil Francopoulo ISTE / Wiley 2013 (ISBN 978-1-84821-430-9)&lt;/ref&gt;
* [[Universal Networking Language|UNL Programme]] is a project under the auspices of [[United Nations|UNO]] aimed to consolidate lexicosemantic data of many languages to be used in machine translation and information extraction systems.

==Distributions==
* [[Babylon (software)|Babylon]]&lt;ref&gt;{{cite web|url=http://www.babylon.com/free-dictionaries/reference/encyclopedias/WordNet-2.0/42406.html |title=Babylon WordNet |publisher=Babylon.com |date= |accessdate=2014-03-11}}&lt;/ref&gt;
WordNet Database is distributed as a dictionary package (usually a single file) for the following software:
* [[GoldenDict]]&lt;ref&gt;{{cite web|url=http://sourceforge.net/projects/goldendict/files/dictionaries |title=GoldenDict - Browse /dictionaries at Sourceforge.net |publisher=Sourceforge.net |date=2010-12-01 |accessdate=2014-01-05}}&lt;/ref&gt;
* [[Lingoes (program)|Lingoes]]&lt;ref&gt;{{cite web|url=http://www.lingoes.net/en/dictionary/dict_down.php?id=12D98EC3940843498672A92149455292 |title=Lingoes WordNet |publisher=Lingoes.net |date=2007-11-16 |accessdate=2014-03-11}}&lt;/ref&gt;

== See also ==
* [[Lexical Markup Framework]]
* [[Machine-readable dictionary]]
* [[Synonym Ring]]
* [[Taxonomy (general)|Taxonomy]]
* [[ThoughtTreasure]]

== References ==
{{Reflist|2}}

== External links ==
* {{Official website|http://wordnet.princeton.edu/ }}
{{Lexicography}}

{{Authority control}}

{{DEFAULTSORT:Wordnet}}
[[Category:English dictionaries]]
[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Online dictionaries]]
[[Category:Open data]]
[[Category:Thesauri]]</text>
      <sha1>to33p5kze2a2v6cvak8molnq88o88h3</sha1>
    </revision>
  </page>
  <page>
    <title>Information Coding Classification</title>
    <ns>0</ns>
    <id>47525166</id>
    <revision>
      <id>744377201</id>
      <parentid>739355021</parentid>
      <timestamp>2016-10-14T20:32:15Z</timestamp>
      <contributor>
        <username>Frietjes</username>
        <id>13791031</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18263" xml:space="preserve">The '''Information Coding Classification''' ('''ICC''') is a [[classification system]] covering almost all extant 6500 knowledge fields ([[knowledge domain]]s). Its [[conceptualization]] goes beyond the scope of the well known  library classification systems, such as [[Dewey Decimal Classification]] (DCC), [[Universal Decimal Classification]] (UDC), and [[Library of Congress Classification]] (LCC), by extending also to [[knowledge system]]s that so far have not afforded to  classify  [[literature]].  ICC actually presents a flexible universal ordering system for both literature and other kinds of [[information]], set out as knowledge fields. From a methodological point of view, ICC differs from the above-mentioned systems along the following three lines:
# Its main classes are not based on [[discipline]]s but on nine live stages of development, so-called [[ontic]]al levels.
# It breaks them roughly down into hierarchical steps by further nine [[Categorization|categories]] which makes decimal number coding possible.
# The contents of a knowledge field is earmarked via a digital position scheme, which makes the first hierarchical step refer to the nine ontical levels  (object areas as subject categories), and the second hierarchical step refer to nine functionally ordered form categories.
Respective knowledge fields permit to step down by the same principle to a third and  forth level, and even further to a fifth and sixth level. Finally, knowledge field subdivisions will have to conform to said digital position scheme.
Hence, for a given knowledge field identical codes will mark identical categories under respective numbers of the coding system. This  [[mnemotechnics|mnemotechnical]]  aspect of the  system helps memorizing and straightaway retrieving the whereabouts of respective [[interdisciplinary]] and [[transdisciplinary]] fields.

The first two hierarchical levels may be regarded as a top- or [[upper ontology]] for ontologies and other applications.

The terms of the first three hierarchical levels were set out in German and English in ''Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft'',&lt;ref name="WEAAZ2014"&gt;{{citation/core|Surname1=[[Ingetraut Dahlberg]]|EditorSurname1=Deutsche Sektion der Internationalen Gesellschaft f&#252;r Wissensorganisation e.V. (ISKO)|Periodical=Textbooks for Knowledge Organization|Title=Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft |Volume=Vol.3|Publisher=Ergon Verlag|PublicationPlace=W&#252;rzburg|Year=2014|At=pp.&amp;nbsp;1-175|ISBN=978-3-95650-065-7|Date=  2014|language=German|AccessDate=2015-07-16}}&lt;/ref&gt; on pp.&amp;nbsp;82 to 100. It was published in 2014 and available so far only in German. In the meantime, also the French terms of the knowlwdge fields have been collected.
Competence for maintenance and further development rests with the German Chapter of the  
[[International Society for Knowledge Organization]] (ISKO) e.V.

== Historical development ==
At the end of 1970, Prof. Alwin Diemer, Univ.of D&#252;sseldorf proposed to [[Ingetraut Dahlberg]] to undertake a philosophical [[dissertation]] on ''The universal classification system of knowledge, its ontological, epistemological, and information theoretical foundations''. Diemer had in mind an innovating ontological approach for such a system based on the whole spectrum of kinds of being and complying with [[epistemological]] requirements. The third requirement had already been taken up somehow in the Indian [[Colon Classification]], yet it still called for explanations and additions. In 1974, the dissertation was published in German entitled ''Grundlagen universaler Wissensordnung''.&lt;ref name="GuWo"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Deutsche Gesellschaft f&#252;r Dokumentation e.V.|Title=Grundlagen universaler Wissensordnung. Probleme und M&#246;glichkeiten eines universalen Klassifikationssystems des Wissens. : im Antiquariat noch erh&#228;ltlich sonst als Print on Demand bei deGruyter|Publisher=Verlag Dokumentation|PublicationPlace=Pullach bei M&#252;nchen|Year=1974|ISBN=978-3111412672|Date=  1974|language=German|AccessDate=2015-05-13}}&lt;/ref&gt; It started with conceptual clarifications, and why and how the term &#8222;universal&#8220; was linked to knowledge, including knowledge fields, such as commodity science, artefacts,  statistics, patents, standardization, communication, utility services et al. In chapter 3, six universal classification systems (DDC, UDC, LCC, BC, CC and BBK) were presented,  analyzed and compared.

While preparing the dissertation, Dahlberg started with elaborating the new universal system by first gleaning a lot of extant designations of knowledge fields from whatever available reference works. This was funded by the German Documentation Society (DGD) (1971-2) under the title of ''Order system of knowledge fields''. In addition, the [[syllabus]]es of German universities and polytechniques were explored for relevant terms and documented (1975).  Thereafter, it seemed necessary to add definitions from special dictionaries and encyclopediae; it soon appeared that the 12.500 terms included numerous synonyms, so that the whole collection boiled down to about 6.500 concept designations (Project Logstruktur, supported by the German Science Foundation (DFG) 1976-78).

The outcome of this work &lt;ref name="GuWo" /&gt; was the formulation of 30 theses which ended up in 12 principles for the new system, published 40 years later under.&lt;ref name="WEAAZ2014" /&gt; These principles refer not only to theoretical foundations but also to structure and other organizational aspects of the whole array of knowledge fields. In 1974, the digital position scheme for field subdivision had already been developed  to allow for classifying classification literature in the bibliographical section of the first issue of the Journal International Classification. In 1977, the entire ICC  was ready for presentation at a seminar in Bangalore, India.&lt;ref&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Sarada Ranganathan Endowment for Library Science|Title=Ontical Structures and Universal Classification |PublicationPlace=Bangalore|Year=1978|Date=  1978|language=German|AccessDate=2014-10-06}}&lt;/ref&gt; A publication of the first three hierarchical levels appeared however only in 1982.&lt;ref name="ICC-PSAP"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=International Classification|Title=ICC &#8211; Information Coding Classification. Principles, structure and application possibilities |Volume=2|Year=1982|At=pp.&amp;nbsp;98-103|Date=  1982|language=German}}&lt;/ref&gt; It was applied to the bibliography of classification systems and [[thesauri]] in vol.1 of the International Classification and Indexing Bibliography;&lt;ref&gt;{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=International Classification and Indexing Bibliography (ICIB 1): Classification systems and thesauri 1950-1982 |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1982|ISSN=0943-7444|Date=  1982|language=German|AccessDate=2015-05-13}}&lt;/ref&gt; it has been updated.&lt;ref name="WEAAZ2014" /&gt;

== Governing principles of ICC ==
These were published in full length in the book ''Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft''&lt;ref name="WEAAZ2014" /&gt; and the  article ''Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches'',&lt;ref name="ICC2010"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Marlies Ockenfeld|Periodical=Information, Wissenschaft &amp; Praxis|Title=Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches |Volume=61, Heft 8|Publisher=De Gruyter|Year=2010|At=pp.&amp;nbsp;449-454|ISSN=1619-4292|Date=  2010|language=German|AccessDate=2015-07-16}}&lt;/ref&gt; hence it suffices to just mention their topics with some necessary additions.

* Principle 1: Concept theoretical approaches. Concepts are the contents of ICC, they are understood as being units of knowledge. The &#8222;birth&#8220; of a concept. Where do the characteristics, the knowledge elements come from? How do conceptual relations arise?
* Principle 2: The four kinds of concept relations and their applications.
* Principle 3: Decimal numbers form the ICC codes as its universal language.
* Principle 4: The nine ontical levels of ICC. They were grouped under three captions: Prolegomena (1-3), life sciences (4-6) and human output (7-9):
:# Structure and form
:# Matter and energy
:# Cosmos and earth
:# Biosphere
:# Anthroposphere
:# Sociosphere
:# Material products (economics and technology)
:# Intellectual products (knowledge and information)
:# Spiritual products (products of mind and culture)

* Principle 5: Knowledge fields are structured by categories, based on the Aristotelian form-categories, under a digital position scheme, a kind of scaling rule for subdividing a given field as follows:
:# General area: problems, theories, principles (axiom and structure)
:# Object area: objects, kinds, parts, properties of objects
:# Activity area: methods, processes, activities
:# Field properties or first characterization   
:# Persons or secondary characterization
:# Societies or tertiary characterization
:# Influences from outside
:# Applications of the field to other fields
:# Field information and synthesizing tasks
:The digital position scheme, called Systematifier, has  also been used for structuring the entire system via the categories figuring on the upper zero level.

An example of its application is the structure of the classification system for knowledge organization literature [http://www.isko.org/scheme.php Gliederung der Klassifikationsliteratur]. (A simplified version with an additional introduction is given in,&lt;ref name="WEAAZ2014" /&gt; p.&amp;nbsp;71)

* Principle 6: The ontical levels outlined under principle 4 conform to the &#8222;integrative level theory&#8220; which means that every level is integrated in the following one. In addition, each knowledge area presumes the following one.
* Principle 7: The combination potential of knowledge fields (interdisciplinarity and transdisciplinarity)is determined by the digital position scheme. (Examples are given in,&lt;ref name="WEAAZ2014" /&gt; p.&amp;nbsp;103-4)
* Principle 8: The categories of the zero-level are general concepts, their possible subdivisions could once be used for classificatory statements. (These subdivisions still need elaboration)
* Principle 9 and 10: These relate to the combination potential of classificatory statements with space and time concepts. (Still to be elaborated)
* Principle 11: The system's mnemotechnical aspect relies on the fixed system position codes and on the 3x3 form- and subject-categories.
* Principle 12: The combination potential of system position 1, 8 and 9 make ICC to a self-networking system which complies with the present scientific development.

== ICC in Matrix Form ==

The first two levels of ICC can be represented by following matrix.

[[File:ICC as a Matrix.png|maxi|840px|ICC as a Matrix with the first two hierarchical levels]]

The first hierarchical level of the 9 subject categories results from the first vertical array under codes 1-9.  The second hierarchical level of subject categories is structured by the 9 functionally ordered form categories, listed in the first horizontal line under codes 01-09. Some exceptions are mentioned in principle 7.

== Research work with ICC ==

=== Exploration of automatic classification with ICC ===
For classifying web documents as conceived by Jens Hartmann, University of Karlsruhe, Prof.Walter Koch, University of Graz, has explored in his Institute for Applied Information Technology Research Society (AIT) the application of ICC to automatically classifying metadata of some 350.000 documents. This was facilitated by data generated within the framework of an EU-supported project [http://www.europeana-local.at "EuropeanaLocal"] . For this exploration, three ICC hierarchical levels have been used for some 5000 terms. The result is described in the report of Christoph Mak.&lt;ref&gt;{{citation/core|Surname1=Christian Mak|Periodical=Bericht des Instituts "Angewandte Informationstechnik Forschungsgesellschaft mbh" (AIT)|Title=Kategorisierung des Datenbestandes der EuropeanaLocal-&#214;sterreich anhand der ICC |PublicationPlace=Graz|Year=2011|Date=  2011|language=German}}&lt;/ref&gt; Prof.Koch regarded a classification degree of almost 50% as a good result, considering that only a shortened version of ICC had been used. In order to reach a better result one would have needed 1&#8211;2 years. Also an index of all terms with their codes could be achieved under these explorations.

=== Data Linkage with ICC ===
Motivated by the work of an Italian research Group in Trento on ''Revising the Wordnet Domains Hierarchy: semantics, coverage and balancing'',&lt;ref name="FGinTrnto"&gt;{{citation/core|Surname1=Luisa Bentivogli|Surname2=Pamela Forner|Surname3=Bernardo Magnini|Surname4=Emanuele Pianta|Periodical=Proceedings of COLING 2004 Workshop on "Multilingual Linguistic Resources|Title=[http://multiwordnet.fbk.eu/paper/coling04-ws-WDH.pdf Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing] |PublicationPlace=Geneva, Switzerland|Year=2004|At=pp.&amp;nbsp;101-108|Date=  2004|language=German}}&lt;/ref&gt; by which the DDC codes were used, Prof. Ernesto DeLuca et al. showed in a study that for such case the use of ICC could lead to essentially better results. This was shown in two contributions: ''Including knowledge domains from the ICC into the Multilingual Lexical Linked Data Cloud (LLD)'' &lt;ref name=" IKDfICC"&gt;{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Knowledge Organization in the 21st Century. Between Historical Patterns and Future Prospects. Proc.13th Int. ISKO Conf.|Title=Including Knowledge Domains from the ICC into the Multilingual Lexical Linked Data Cloud |PublicationPlace=Krakau, Polen|Year=2014|At=pp.&amp;nbsp;258-365|Date=  2014|language=German}}&lt;/ref&gt; and ''Die Multilingual Lexical Linked Data Cloud: Eine m&#246;gliche Zugangsoptimierung?'',&lt;ref name="MLLDC"&gt;{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Information, Wissenschaft &amp; Praxis|Title=Die Multilingual Lexical Linked Data Cloud: Eine m&#246;gliche Zugangsoptimierung? |Volume=65, Heft 4-5|Publisher=De Gruyter|Year=2014|At=pp.&amp;nbsp;279-287|ISSN=1619-4292|Date=  2014|language=German}}&lt;/ref&gt; in which the LLD was used in a meta-model which contains all ressources with the possibility of retrieval and navigation of data from different aspects. By this, the existing work about many thousand knowledge fields (of  ICC) can be combined with the Multilingual Lexical Linked Data Cloud, based on RDF/OWL representation of EuroWordNet and similar integrated lexical ressources (MultiWordNet, MEMODATA and the Hamburg Metapher BD).

=== Semantic Web structuring with ICC ===
In October 2013, the computer scientist Hermann Bense, Dortmund, explored the possibilities for structuring the Semanic Web with ICC codes. He developed two approaches for a pictorial presentation of knowledge fields with their possible subdivisions. A graphic representation of those knowledge fields pertaining to the first two levels can be found under [http://www.ontology4.us/english/Ontologies/Science%2520Ontology/index.html Ontology4]. The inclusion of the third hierarchical level has been envisaged as the next step.

== Some potential applications of ICC in its present form ==
# Possibility to roughly structure documents, especially bibliographies and reference works.
# Structuring personal repertories, e.g. a [[Who&#8217;s Who]] in ''Who&#8217;s Who in Classification and Indexing''&lt;ref name="whoiswho"&gt;{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=Who's Who in Classification and Indexing |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1983|Date=  1983|language=German}}&lt;/ref&gt;
# Supporting the recollection of statistics by knowledge fields, e.g. also concerning university professors, statistics of academies, of institutions, of teachers in special education
# Publishing houses could take up ICC codes for their products to help later sorting by knowledge fields.
# As a standard classification ICC may be used in many cases, especially in industry,  [[knowledge management]] and [[knowledge engineering]].
# With the definition of all its terms a lexicon of knowledge fields could be published. This could also be used for such lexica in other languages.&lt;ref name="lexikon"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=Knowledge Organisation 39, No.2|Title=A systematic new lexicon of all knowledge fields based on the Information Coding Classification. |Year=2012|At=pp.&amp;nbsp;142-150|Date=  2012|language=German}}&lt;/ref&gt;
# As an example, ICC could be used to compare ongoing scientific activities on a European or world-wide scale.
# ICC can also be an appropriate tool for switching between extant universal classification systems.&lt;ref name="switch"&gt;{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Green, R.|Periodical=Knowledge organization &amp; change. Proc.4th Int.ISKO Conf., Washington,D.C.|Title=Library catalogs and the Internet. Switching for future subject access. |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1996|At=pp.&amp;nbsp;155-165|Date=  1996|language=German}}&lt;/ref&gt;
# ICC can also be a suitable &#8222;hang-up system&#8220; for special classification systems, e.g. for special terminological concept systems.
# ICC in its three hierarchies and corresponding explanations might also be used in higher education to supply the youngsters with an overview of knowledge fields and an understanding of the relationships in the whole of human knowledge.
# Similar to the [[Unified Medical Language System]] (UMLS) for medicine such a Unified System of Knowledge Fields could be held available in many languages and thus reach a global understanding of knowledge fields.
# The alphabetical index to all knowledge field concepts could be used for comparisons with other such indexes to help in finding the missing fields in the different universal classification systems.   
 
== References ==
{{Reflist|30em}}

{{Authority control}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Ontology]]
[[Category:Data coding framework]]</text>
      <sha1>5gqt9kj4pivblmj56u1p8me5aykn2nv</sha1>
    </revision>
  </page>
  <page>
    <title>Group concept mapping</title>
    <ns>0</ns>
    <id>1539290</id>
    <revision>
      <id>732078977</id>
      <parentid>732040771</parentid>
      <timestamp>2016-07-29T13:23:54Z</timestamp>
      <contributor>
        <username>Pinkbeast</username>
        <id>11291690</id>
      </contributor>
      <comment>/* External links */ two spamlinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9778" xml:space="preserve">'''Group concept mapping''' is a structured methodology for organizing the ideas of a group on any topic of interest and representing those ideas visually in a series of interrelated maps.&lt;ref name="ref1"&gt;Kane M, Trochim WM (2007). Concept mapping for planning and evaluation. Thousand Oaks, CA: Sage Publications.&lt;/ref&gt;&lt;ref name="ref2"&gt;Trochim W (1989). An introduction to concept mapping for evaluation and planning. Evaluation and Program Planning, 12(1), 1-16.&lt;/ref&gt;  It is a type of integrative mixed method,&lt;ref&gt;Caracelli VW, Greene JC (1993). Data analysis strategies for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 15(2), 195-207.&lt;/ref&gt;&lt;ref&gt;Greene JC, Caracelli VJ, Graham WF (1989). Toward a conceptual framework for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 11, 255-274.&lt;/ref&gt; combining qualitative and quantitative approaches to [[data collection]] and [[analysis]].  Group concept mapping allows for a collaborative group process with groups of any size, including a broad and diverse array of participants.&lt;ref name="ref1" /&gt;  Since its development in the late 1980s by William M.K. Trochim at [[Cornell University]], it has been applied to various fields and contexts, including community and public health,&lt;ref&gt;Rao JK, Alongi J, Anderson LA, Jenkins L, Stokes GA, Kane M (2005). Development of public health priorities for end-of-life initiatives. American Journal of Preventive Medicine, 29(5), 453-460.&lt;/ref&gt;&lt;ref&gt;Risisky D, Hogan VK, Kane M, Burt B, Dove C, Payton M (2008). Concept mapping as a tool to engage a community in health disparity identification. Ethnicity &amp; Disease, 18, 77-83.&lt;/ref&gt;&lt;ref&gt;Trochim W, Milstein B, Wood B, Jackson S, Pressler V (2004). Setting objectives for community and systems change: an application of concept mapping for planning a statewide health improvement initiative. Health Promotion Practice, 5, 8&#8211;19.&lt;/ref&gt;&lt;ref&gt;Trochim WM, Cabrera DA, Milstein B, Gallagher RS, Leischow SJ (2006). Practical challenges of systems thinking and modeling in public health. American Journal of Public Health, 96(3), 538-546.&lt;/ref&gt; social work,&lt;ref&gt;Petrucci C, Quinlan KM (2007). Bridging the research-practice gap: concept mapping as a mixed-methods strategy in practice-based research and evaluation. Journal of Social Science Research, 34(2), 25-42.&lt;/ref&gt;&lt;ref&gt;Ridings JW, Powell DM, Johnson JE, Pullie CJ, Jones CM, Jones RL, Terrell KJ (2008). Using concept mapping to promote community building: the African American Initiative at Roseland. Journal of Community Practice, 16(1), 39-63.&lt;/ref&gt; health care,&lt;ref&gt;Trochim WM, Kane M (2005). Concept mapping: an introduction to structured conceptualization in health care. International Journal for Quality in Health Care, 7(3),187-191.&lt;/ref&gt; human services,&lt;ref&gt;Pammer W, Haney M, Wood BM, Brooks RG, Morse K, Hicks P, Handler EG, Rogers H, Jennett P (2001). Use of telehealth technology to extend child protection team services. Pediatrics, 108(3), 584-590.&lt;/ref&gt;&lt;ref&gt;Paulson BL, Truscott D, Stuart J (1999). Clients&#8217; perceptions of helpful experiences in counseling. Journal of Counseling Psychology, 46(3), 317-324.&lt;/ref&gt; and biomedical research and evaluation.&lt;ref&gt;Kagan JM, Kane M, Quinlan KM, Rosas S, Trochim WMK (2009). Developing a conceptual framework for an evaluation system for the NIAID HIV/AIDS clinical trials networks. Health Research Policy and Systems, 7, 12.&lt;/ref&gt;&lt;ref&gt;Robinson JM, Trochim WMK (2007). An examination of community members&#8217;, researchers&#8217; and health professionals&#8217; perceptions of barriers to minority participation in medical research: an application of concept mapping. Ethnicity &amp; Health, 12(5), 521-539.&lt;/ref&gt;&lt;ref&gt;Trochim WM, Markus SE, Masse LC, Moser RP, and Weld PC (2008). The evaluation of large research initiatives: a participatory integrative mixed-methods approach. American Journal of Evaluation, 29(1), 8&#8211;28.&lt;/ref&gt;

==Overview==
Group concept mapping integrates qualitative group processes with [[multivariate analysis]] to help a group organize and visually represent its ideas on any topic of interest through a series of related maps.&lt;ref name="ref1" /&gt;&lt;ref name="ref2" /&gt;  It combines the ideas of diverse participants to show what the group thinks and values in relation to the specific topic of interest. It is a type of structured conceptualization used by groups to develop a conceptual framework, often to help guide evaluation and planning efforts.&lt;ref name="ref2" /&gt;  Group concept mapping is participatory in nature, allowing participants to have an equal voice and to contribute through various methods.&lt;ref name="ref1" /&gt; A group concept map visually represents all the ideas of a group and how they relate to each other, and depending on the scale, which ideas are more relevant, important, or feasible.

==Process==

Group concept mapping involves a structured multi-step process, including [[brainstorming]], sorting and rating, [[multidimensional scaling]] and [[cluster analysis]], and the generation and interpretation of multiple maps.&lt;ref name="ref1" /&gt;&lt;ref name="ref2" /&gt;  The first step requires participants to brainstorm a large set of statements relevant to the topic of interest, usually in response to a focus prompt.  Participants are then asked to individually sort those statements into categories based on their perceived similarity and rate each statement on one or more scales, such as importance or feasibility.

The data is then analyzed using The Concept System&#174; software, which creates a series of interrelated maps using [[multidimensional scaling]] (MDS) of the sort data, [[hierarchical clustering]] of the MDS coordinates applying [[Ward's method|Ward&#8217;s method]], and the computation of average ratings for each statement and cluster of statements.&lt;ref name="ref3"&gt;Rosas SR, Camphausen (2007).  The use of concept mapping for scale development and validation in evaluation.  Evaluation and Program Planning, 30, 125-135.&lt;/ref&gt;  The resulting maps display the individual statements in two-dimensional space with more similar statements located closer to each other, and grouped into clusters that partition the space on the map.  The Concept System&#174; software also creates other maps that show the statements in each cluster rated on one or more scales, and absolute or relative cluster ratings between two cluster sets.  As a last step in the process, participants are led through a structured interpretation session to better understand and label all the maps.

==History==
Group concept mapping was developed as a methodology in the late 1980s by William M.K. Trochim at [[Cornell University]].  Trochim is considered to be a leading evaluation expert, and he has taught evaluation and research methods at Cornell since 1980.&lt;ref name="ref4"&gt;Cornell University, College of Human Ecology (2012).  William Trochim biographical statement. http://www.human.cornell.edu/bio.cfm?netid=wmt1.&lt;/ref&gt;  Originally called "concept mapping", the methodology has evolved since its inception with the maturation of the field and the continued advancement of the software, which is now a Web application.

==Uses==
Group concept mapping can be used with any group for any topic of interest.  It is often used by government agencies, academic institutions, national associations, not-for-profit and community-based organizations, and private businesses to help turn the ideas of the group into measurable actions.  This includes in the areas of organizational development, strategic planning, needs assessment, curriculum development, research, and evaluation.&lt;ref name="ref1" /&gt;  Group concept mapping is well-documented, well-established methodology, and it has been used in hundreds of published papers.

==Group concept mapping versus concept mapping and mind mapping==
Concept mapping is any process used for visually representing relationships between ideas in pictures or maps.&lt;ref name="ref1" /&gt;  The technique was originally developed in the 1970s by [[Joseph D. Novak]] at [[Cornell University]]. &lt;ref name="ref5"&gt;Novak JD, Gowin DB (1984). Learning How to Learn. Cambridge: Cambridge University Press.&lt;/ref&gt;  A [[concept map]] is typically a diagram of multiple ideas, often represented as boxes or circles, linked in a hierarchical structure through arrows and words where each idea is connected to each other and linked back to the original idea.&lt;ref name="ref5" /&gt;  Concept mapping tends to be more free form, and may involve an individual or group.  Unlike other forms of concept mapping, group concept mapping is purposefully designed to work with groups and has a more structured process for organizing and visually representing the ideas of a group through a series of specific steps.&lt;ref name="ref1" /&gt;

A [[mind map]] is a diagram used to visually represent information, centering on one word or idea with categories and sub-categories radiating off of it.&lt;ref&gt;Buzan T (2010).  The Mind Map Book: Unlock Your Creativity, Boost Your Memory, Change Your Life. Essex: British Broadcasting Company.&lt;/ref&gt;  Popularized by [[Tony Buzan]] in the 1970s, mind mapping is often a spontaneous exercise done by an individual or group to gather information about what they think around a single topic.  In contrast to mind mapping, group concept mapping represents multiple ideas and it has a less flexible and more structured process.  Group concept mapping is also specific to groups.

==See also==
* [[Knowledge representation and reasoning]]
* [[List of concept- and mind-mapping software]]

==References==
{{reflist}}

==External links==
* [http://www.socialresearchmethods.net/mapping/mapping.htm Concept mapping research guide]

[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Survey methodology]]</text>
      <sha1>osxf88ox9ikmlrrex5tgcq48ffe2w7l</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic triple</title>
    <ns>0</ns>
    <id>51445651</id>
    <revision>
      <id>738439464</id>
      <parentid>738439326</parentid>
      <timestamp>2016-09-08T23:48:38Z</timestamp>
      <contributor>
        <username>Waldir</username>
        <id>182472</id>
      </contributor>
      <minor />
      <comment>add {{Semantic Web}} navbox</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2417" xml:space="preserve">A '''semantic triple''', or simply '''triple''', is the atomic data entity in the [[Resource Description Framework]] (RDF) data model.&lt;ref&gt;http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework (RDF) Model and Syntax Specification"&lt;/ref&gt; As its name indicates, a triple is a [[tuple|set of three entities]] that codifies a [[statement (programming)|statement]] about [[Data model|semantic data]] in the form of subject&#8211;predicate&#8211;object expressions (e.g. "Bob is 35", or "Bob knows John").

This format enables [[Knowledge representation and reasoning|knowledge to be represented]] in a machine-readable way. Particularly, every part of an RDF triple is individually addressable via unique [[Uniform Resource Identifier|URIs]] &#8212; for example, the second statement above might be represented in RDF as &lt;code&gt;&lt;nowiki&gt;http://example.name#BobSmith12 http://xmlns.com/foaf/0.1/knows http://example.name#JohnDoe34&lt;/nowiki&gt;&lt;/code&gt;.
Given this precise representation, semantic data can be unambiguously [[Semantic query|queried]] and [[Semantic reasoner|reasoned]] about.

The components of a triple, such as the statement "The sky has the color blue", consist of a [[Subject (grammar)|subject]] ("the sky"), a [[Predicate (grammar)|predicate]] ("has the color"), and an [[Object (grammar)|object]] ("blue"). This is similar to the classical notation of an [[entity&#8211;attribute&#8211;value model]] within [[object-oriented design]], where this example would be expressed as an entity (sky), an attribute (color) and a value (blue). From this basic structure, triples can be composed into [[Semantic network|more complex models]], by using triples as objects or subjects of other triples &#8212; for example, &lt;code&gt;Mike &#8594; said &#8594; (triples &#8594; can be &#8594; objects)&lt;/code&gt;.

Given their particular, consistent structure, a collection of triples is often stored in purpose-built databases called [[Triplestore]]s.

== See also ==
* [[Named graph#Named graphs and quads]], an extension to semantic triples to also include a context node as a fourth element.

== References ==
{{reflist}}

== External links ==
* {{cite web |url = https://www.w3.org/TR/rdf11-primer/#section-triple |title = RDF 1.1 Primer &#167; Triples |publisher = [[World Wide Web Consortium|W3C]] }}

{{Semantic Web}}

[[Category:Semantic Web]]
[[Category:Data modeling]]
[[Category:Resource Description Framework]]
[[Category:Knowledge representation]]</text>
      <sha1>ewf0urdtisoxc9mwki02g6dsaujcbxd</sha1>
    </revision>
  </page>
  <page>
    <title>Plinian Core</title>
    <ns>0</ns>
    <id>49236889</id>
    <revision>
      <id>753853621</id>
      <parentid>745667548</parentid>
      <timestamp>2016-12-09T15:51:54Z</timestamp>
      <contributor>
        <username>Gaurav</username>
        <id>6379</id>
      </contributor>
      <comment>Added some internal links, removed some external links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7182" xml:space="preserve">{{Multiple issues|
{{Orphan|date=June 2016}}
{{primary sources|date=January 2016}}
{{Underlinked|date=October 2016}}
}}

'''Plinian Core''' is a set of vocabulary terms that can be used to describe different aspects of [[biodiversity informatics|biological species information]]. Under "biological species Information" all kinds of properties or traits related to taxa&#8212;biological and non-biological&#8212;are included. Thus, for instance, terms pertaining descriptions,  legal aspects, conservation, management, demographics, nomenclature, or related resources are incorporated.

== Description ==

The '''Plinian Core''' is aimed to facilitate the exchange of information about the species and upper taxa.
	
What is in scope?
* 	Species level catalogs of any kind of biological objects or data. 
* 	Terminology associated with biological collection data. 
* 	Striving for compatibility with other biodiversity-related standards. 
* 	Facilitating the addition of components and attributes of biological data. 
	What is not in scope? 
* 	Data interchange protocols. 
* 	Non-biodiversity-related data. 
* 	Occurrence level data. 
	This standard is named after Pliny the Elder, a very influential figure in the study of the biological species.
	
'''Plinian Core''' design requirements includes: ease of use, to be self-contained, able to support data integration from multiple databases, and ability to handle different levels of granularity. Core terms can be grouped in its current version as follows:
* 	[[Metadata]]
* 	Base Elements
* 	Record Metadata
* 	[[Nomenclature#Nomenclature.2C classification and identification|Nomenclature and Classification]]
* 	[[Taxonomic description]]
* 	[[Natural history]]
* 	[[Invasive species]]
* 	[[Habitat|Habitat and Distribution]]
* 	[[Demography]] and Threats
* 	Uses, Management and Conservation
* 	associatedParty, MeasurementOrFact, References, AncillaryData

== Background ==
Plinian Core started as a collaborative project between [[Instituto Nacional de Biodiversidad]] and [[GBIF]] Spain in 2005. A series of iterations in which elements were defined and implanted in different projects resulted in a "Plinian Core Flat" [deprecated].

As a result, a new development was impulse to overcome them in 2012. New formal requirements, additional input and a will to better support the standard and its documentation, as well as to align it with the processes of [[TDWG]], the world reference body for biodiversity information standards.

A new version, '''Plinian Core v3.1''' was defined. This provides more flexibility to fully represent the information of a species in a variety of scenarios. New elements to deal with aspects such as IPR, related resources, referenced, etc. were introduced, and elements already included were better-defined and documented.

Partner for the development of Plinian Core in this new phase incorporated the [[University of Granada]] (UG, Spain), the [[Alexander von Humboldt Institute]] (IAvH, Colombia), the [[National Commission for the Knowledge and Use of Biodiversity]] (Conabio, Mexico) and the [[University of S&#227;o Paulo]] (USP, Brazil).

A "Plinian Core Task Group" within TDWG "Interest Group on species Information"&lt;ref&gt;{{Cite web|title = TDWG: (TDWG) Species Information Interest Group - Charter|url = http://www.tdwg.org/activities/species-information/charter/|website = www.tdwg.org|access-date = 2016-01-29}}&lt;/ref&gt; in being proposed.

== Levels of the standard ==
Plinian Core is presented in to levels: the '''abstract model''' and the '''application profiles'''.

The abstract model (AM), comprising the abstract model schema(xsd)  and the terms' URIs, is the normative part. It is all comprehensive, and allows for different levels of granularity in describing species properties. The AM should be taken as a "menu" from which to choose terms and level of detail needed in any specific project.

The subsets of the abstract model intended to be implemented in specific projects are the "application profiles" (APs). Besides containing part of the elements of the AM, APs can impose additional specifications on the included elements, such as controlled vocabularies.  Some exampes of APs in use follow:
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_CONABIO.xsd CONABIO]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_INBIO.xsd INBIO]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_GBIF_ES.xsd GBIF.ES]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_MAGRAMA.xsd MAGRAMA]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_SIB-COLOMBIA.xsd SIB-COLOMBIA]

== Relation to other standards ==
Plinian incorporates a number of elements already defined by other standards. The following table summarizes these standards and the elements used in Plinian Core:
{| class="wikitable"
!Standard
!Elements
|-
|[[Darwin Core]]
|taxonConceptID, Hierarchy, MeasurementOrFact, ResourceRelationShip.
|-
|[[Ecological Metadata Language]]
|associatedParty, keywordSet, coverage, dataset
|-
|[[Encyclopedia of Life]] Schema
|AncillaryData: DataObjectBase
|-
|[[Global Invasive Species Network]]
|origin, presence, persistence, distribution, harmful, modified, startValidDate, endValidDate, countryCode, stateProvince, county, localityName, county, language, citation, abundance...
|-
|[http://www.tdwg.org/schemas/tcs/1.01 Taxon Concept Schema. TCS]
|scientificName
|}

== External Links ==
* [http://www.github.com/PlinianCore Main page]
* [http://tools.gbif.org/dwca-validator/extensions.do An Implementation of Plinian Core as GBIF's IPT Extensions]
* [https://github.com/PlinianCore/Documentation/wiki/PlinianCore_Terms Plinian Core Terms Quick Reference Guide]
* [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/abstract%20models/stable%20version/PlinianCore_AbstractModel_v3.2.1.xsd Plinian Core Abstract Model (xsd). Current version (3.2.1)]
* [http://www.tdwg.org/ Biodiversity Information Standards (TDWG)]
* [http://www.ingurumena.ejgv.euskadi.eus/r49-u95/es/contenidos/informacion/naturaeuskadi/es_def/adjuntos/plinian.pdf Sistema de informaci&#243;n de la naturaleza de euskadi. Aplicaci&#243;n del estandar Plinian Core]
* [http://www.magrama.gob.es/es/biodiversidad/servicios/banco-datos-naturaleza/informacion-disponible/BDN_Modelos_Datos.aspx Est&#225;ndar Plinian Core para la gesti&#243;n integrada de la informaci&#243;n sobre especies. Ministerio de Agricultura, Alimentaci&#243;n y Medio Ambiente de Espa&#241;a]
* [http://patrimonio.ambiente.gob.ec/bndv/modelo.php Modelo conceptual de la Base Nacional de Datos de Vegetaci&#243;n. Ministerio del Ambiente, Ecuador]

== References ==
&lt;references /&gt;

[[Category:Biodiversity informatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
[[Category:Metadata standards]]</text>
      <sha1>3gdxm7mdyv6e1tvpcs74klfttaffqzo</sha1>
    </revision>
  </page>
</mediawiki>