<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Information retrieval</title>
    <ns>0</ns>
    <id>15271</id>
    <revision>
      <id>744833846</id>
      <parentid>744800746</parentid>
      <timestamp>2016-10-17T18:44:14Z</timestamp>
      <contributor>
        <username>CAPTAIN RAJU</username>
        <id>25523690</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/61.3.77.128|61.3.77.128]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37974" xml:space="preserve">{{Information science}}

'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[Full text search|full-text]] or other content-based indexing.

Automated information retrieval systems are used to reduce what has been called "[[information overload]]". Many [[University|universities]] and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].

== Overview ==

An information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[Relevance (information retrieval)|relevancy]].

An object is an entity that is represented by information in a content collection or [[database]]. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.&lt;ref&gt;Jansen, B. J. and Rieh, S. (2010) [https://faculty.ist.psu.edu/jjansen/academic/jansen_theoretical_constructs.pdf The Seventeen Theoretical Constructs of Information Searching and Information Retrieval]. Journal of the American Society for Information Sciences and Technology. 61(8), 1517-1534.&lt;/ref&gt;

Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,&lt;ref name=goodron2000&gt;{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}&lt;/ref&gt; audio,&lt;ref name=Foote99&gt;{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}&lt;/ref&gt; [[mind maps]]&lt;ref name=Beel2009&gt;{{cite conference|first=J&#246;ran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |title=Information Retrieval On Mind Maps - What Could It Be Good For? |url=http://www.sciplore.org/publications_en.php |conference=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}&lt;/ref&gt; or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or [[metadata]].

Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.&lt;ref name="Frakes1992"&gt;{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures &amp; Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}&lt;/ref&gt;

== History ==
{{Rquote|right|there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, ca be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute| J. E. Holmstrom, 1948}}
The idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.&lt;ref name="Singhal2001"&gt;{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering|volume=24 |issue=4 |pages=35&#8211;43 |year =2001 |url=http://singhal.info/ieee2001.pdf }}&lt;/ref&gt; It would appear that Bush was inspired by patents for a 'statistical machine' - filed by [[Emanuel Goldberg]] in the 1920s and '30s - that searched for documents stored on film.&lt;ref name="Sanderson2012"&gt;{{cite journal |author=Mark Sanderson &amp; W. Bruce Croft |title=The History of Information Retrieval Research |journal=Proceedings of the IEEE |volume=100 |pages=1444&#8211;1451 |year =2012 |url=http://dx.doi.org/10.1109/JPROC.2012.2189916 |doi=10.1109/jproc.2012.2189916}}&lt;/ref&gt; The first description of a computer searching for information was described by Holmstrom in 1948,&lt;ref name="Holmstrom1948"&gt;{{cite journal |author=JE Holmstrom |title=&#8216;Section III. Opening Plenary Session |journal=The Royal Society Scientific Information Conference, 21 June-2 July 1948: report and papers submitted |pages=85|year =1948|url=https://books.google.com.au/books?ei=44VxVZrkGYqU8QX4wYPoBA&amp;id=M34lAAAAMAAJ&amp;dq=%E2%80%98Section+III.+Opening+Plenary+Session%22.+The+Royal+Society+Scientific+Information+Conference&amp;focus=searchwithinvolume&amp;q=univac}}&lt;/ref&gt; detailing an early mention of the [[Univac]] computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, [[Desk Set]]. In the 1960s, the first large information retrieval research group was formed by [[Gerard Salton]] at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).&lt;ref name="Singhal2001" /&gt; Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.

== Model types ==
[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsr&#252;ckgewinnung#Klassifikation von Modellen zur Repr&#228;sentation nat&#252;rlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&amp;lng=eng&amp;id= Dominik Kuropka]).]]
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.

=== First dimension: mathematical basis ===
* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
** [[Standard Boolean model]]
** [[Extended Boolean model]]
** [[Fuzzy retrieval]]
* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
** [[Vector space model]]
** [[Generalized vector space model]]
** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]
** [[Extended Boolean model]]
** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]
* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.
** [[Binary Independence Model]]
** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function
** [[Uncertain inference]]
** [[Language model]]s
** [[Divergence-from-randomness model]]
** [[Latent Dirichlet allocation]]
* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.

=== Second dimension: properties of the model ===
* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.
* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.
* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)

== Performance and correctness measures ==
{{further|Evaluation measures (information retrieval)}}

The '''evaluation of an information retrieval system''' is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a [[ground truth]] notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy.

Virtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.{{citation needed|date=June 2015}}

The mathematical symbols used in the formulas below mean:
* &lt;math&gt;X \cap Y&lt;/math&gt; - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in ''both'' sets X and Y
* &lt;math&gt;| X |&lt;/math&gt; - [[Cardinality]] - in this case, the number of documents in set X
* &lt;math&gt;\int&lt;/math&gt; - [[Integral]]
* &lt;math&gt;\sum&lt;/math&gt; - [[Summation]]
* &lt;math&gt;\Delta&lt;/math&gt; - [[Symmetric difference]]

=== Precision ===
{{main|Precision and recall}}

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:&lt;math&gt; \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} &lt;/math&gt;

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===
{{main|Precision and recall}}

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:&lt;math&gt;\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} &lt;/math&gt;

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:&lt;math&gt; \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} &lt;/math&gt;

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to &lt;math&gt;(1-\mbox{specificity})&lt;/math&gt;. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-score / F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:&lt;math&gt;F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}&lt;/math&gt;

This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

The general formula for non-negative real &lt;math&gt;\beta&lt;/math&gt; is:
:&lt;math&gt;F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,&lt;/math&gt;

Two other commonly used F measures are the &lt;math&gt;F_{2}&lt;/math&gt; measure, which weights recall twice as much as precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; "measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;.  Their relationship is:
:&lt;math&gt;F_\beta = 1 - E&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;

F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}

=== Average precision ===
&lt;!-- [[Average precision]] redirects here --&gt;
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision &lt;math&gt;p(r)&lt;/math&gt; as a function of recall &lt;math&gt;r&lt;/math&gt;. Average precision computes the average value of &lt;math&gt;p(r)&lt;/math&gt; over the interval from &lt;math&gt;r=0&lt;/math&gt; to &lt;math&gt;r=1&lt;/math&gt;:&lt;ref name="zhu2004"&gt;{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{AveP} = \int_0^1 p(r)dr&lt;/math&gt;
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:&lt;math&gt;\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)&lt;/math&gt;
where &lt;math&gt;k&lt;/math&gt; is the rank in the sequence of retrieved documents, &lt;math&gt;n&lt;/math&gt; is the number of retrieved documents, &lt;math&gt;P(k)&lt;/math&gt; is the precision at cut-off &lt;math&gt;k&lt;/math&gt; in the list, and &lt;math&gt;\Delta r(k)&lt;/math&gt; is the change in recall from items &lt;math&gt;k-1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt;.&lt;ref name="zhu2004" /&gt;

This finite sum is equivalent to:
:&lt;math&gt; \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!&lt;/math&gt;
where &lt;math&gt;\operatorname{rel}(k)&lt;/math&gt; is an indicator function equaling 1 if the item at rank &lt;math&gt;k&lt;/math&gt; is a relevant document, zero otherwise.&lt;ref name="Turpin2006"&gt;{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06&#8211;11, 2006) |publisher=ACM |location=New York, NY |pages=11&#8211;18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}&lt;/ref&gt; Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the &lt;math&gt;p(r)&lt;/math&gt; function to reduce the impact of "wiggles" in the curve.&lt;ref name=voc2010&gt;{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303&#8211;338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}&lt;/ref&gt;&lt;ref name="nlpbook"&gt;{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Sch&#252;tze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}&lt;/ref&gt; For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:&lt;ref name="voc2010" /&gt;&lt;ref name="nlpbook" /&gt;
:&lt;math&gt;\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)&lt;/math&gt;
where &lt;math&gt;p_{\operatorname{interp}}(r)&lt;/math&gt; is an interpolated precision that takes the maximum precision over all recalls greater than &lt;math&gt;r&lt;/math&gt;:
:&lt;math&gt;p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})&lt;/math&gt;.

An alternative is to derive an analytical &lt;math&gt;p(r)&lt;/math&gt; function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.&lt;ref&gt;K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.&lt;/ref&gt;

=== Precision at K ===

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.&lt;ref name="stanford" /&gt;  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.

=== R-Precision ===

R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, &lt;math&gt;R&lt;/math&gt;, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant &lt;math&gt;r&lt;/math&gt; turns that into a relevancy fraction: &lt;math&gt;r/R = r/15&lt;/math&gt;.&lt;ref name="trec15"/&gt;

Precision is equal to recall at the '''R'''-th position.&lt;ref name="stanford"&gt;{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch&#252;tze}}  Part of ''Introduction to Information Retrieval'' [http://nlp.stanford.edu/IR-book/]&lt;/ref&gt;

Empirically, this measure is often highly correlated to mean average precision.&lt;ref name="stanford" /&gt;

=== Mean average precision ===
&lt;!-- [[Mean average precision]] redirects here --&gt;
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:&lt;math&gt; \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!&lt;/math&gt;
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. &lt;/math&gt;

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (&lt;math&gt;IDCG_p&lt;/math&gt;), which normalizes the score:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. &lt;/math&gt;

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other measures ===
{{Confusion matrix terms}}
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]
* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents&lt;ref name="trec15"&gt;http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf&lt;/ref&gt;
* GMAP - geometric mean of (per-topic) average precision&lt;ref name="trec15" /&gt;
* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}

===Visualization===

Visualizations of information retrieval performance include:
* Graphs which chart precision on one axis and recall on the other&lt;ref name="trec15" /&gt;
* Histograms of average precision over various topics&lt;ref name="trec15" /&gt;
* [[Receiver operating characteristic]] (ROC curve)
* [[Confusion matrix]]

== Timeline ==

* Before the '''1900s'''
*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.
*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.
* '''1920s-1930s'''
*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine&#8221; a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.
* '''1940s&#8211;1950s'''
*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.
*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.
*: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems ([[Allen Kent]] ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).
*: '''1950''': The term "information retrieval" was coined by [[Calvin Mooers]].&lt;ref&gt;Mooers, Calvin N.; ''[https://babel.hathitrust.org/cgi/pt?id=mdp.39015034570591;view=1up;seq=3 The Theory of Digital Handling of Non-numerical Information and its Implications to Machine Economics]'' (Zator Technical Bulletin No. 48), cited in {{cite journal|last1=Fairthorne|first1=R. A.|title=Automatic Retrieval of Recorded Information|journal=The Computer Journal|date=1958|volume=1|issue=1|page=37|doi=10.1093/comjnl/1.1.36|url=http://comjnl.oxfordjournals.org/content/1/1/36.short}}&lt;/ref&gt;
*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].&lt;ref name="Doyle1975"&gt;{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}&lt;/ref&gt;
*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.&lt;ref&gt;{{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |doi=10.1002/asi.5090060411}}&lt;/ref&gt;
*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)
*: '''1959''': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."
* '''1960s''':
*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.
*: '''1960''': [[Melvin Earl Maron]] and John Lary&lt;!-- sic --&gt; Kuhns&lt;ref name="Maron2008"&gt;{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971&#8211;972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}&lt;/ref&gt; published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216&#8211;244, July 1960.
*: '''1962''':
*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.
*:* Kent published ''Information Analysis and Retrieval''.
*: '''1963''':
*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].
*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).
*: '''1964''':
*:* [[Karen Sp&#228;rck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.
*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.
*:'''mid-1960s''':
*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
*::* Project Intrex at MIT.
*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.
*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.
*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
*:: '''1968''':
*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.
*:* John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.
*:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.
* '''1970s'''
*: '''early 1970s''':
*::* First online systems&#8212;NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.
*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."&lt;ref&gt;{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217&#8211;240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}&lt;/ref&gt;
*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:
*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)
*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)
*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)
*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.
*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.
*: '''1979''': Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.&lt;ref&gt;Doszkocs, T.E. &amp; Rapp, B.A. (1979). "Searching MEDLINE in English: a Prototype User Inter-face with Natural Language Query, Ranked Output, and relevance feedback," In: Proceedings of the ASIS Annual Meeting, 16: 131-139.&lt;/ref&gt;
* '''1980s'''
*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.
*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.
*:: '''1985&#8211;1993''': Key papers on and experimental systems for visualization interfaces.
*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.
*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].
* '''1990s'''
*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.
*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''&lt;ref name="Korfhage1997"&gt;{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}&lt;/ref&gt; with emphasis on visualization and multi-reference point systems.
*: '''late 1990s''': [[Web search engine]]s implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.

== Awards in the field ==

* [[Tony Kent Strix award]]
* [[Gerard Salton Award]]

== Leading IR Research Groups ==
* [[Center for Intelligent Information Retrieval]] (CIIR) at the University of Massachusetts Amherst &lt;ref&gt;{{Cite web|url=http://ciir.cs.umass.edu|title=Center for Intelligent Information Retrieval {{!}} UMass Amherst|website=ciir.cs.umass.edu|access-date=2016-07-29}}&lt;/ref&gt;
* Information Retrieval Group at the University of Glasgow &lt;ref&gt;{{Cite web|url=http://www.gla.ac.uk/schools/computing/research/researchoverview/informationretrieval/|title=University of Glasgow - Schools - School of Computing Science - Research - Research overview - Information Retrieval|website=www.gla.ac.uk|access-date=2016-07-29}}&lt;/ref&gt;
* Information and Language Processing Systems (ILPS) at the University of Amsterdam &lt;ref&gt;{{Cite web|url=http://ilps.science.uva.nl/|title=ILPS - information and language processing systems|website=ILPS|language=en-US|access-date=2016-07-29}}&lt;/ref&gt;
* Language Technologies Institutes (LTI) at the Carnegie Mellon University
* Text Information Management and Analysis Group (TIMAN) at  the University of Illinois at Urbana-Champaign

==See also==

{{div col}}

* [[Adversarial information retrieval]]
* [[Collaborative information seeking]]
* [[Controlled vocabulary]]
* [[Cross-language information retrieval]]
* [[Data mining]]
* [[European Summer School in Information Retrieval]]
* [[Human&#8211;computer information retrieval]] (HCIR)
* [[Information extraction]]
* [[Information Retrieval Facility]]
* [[Knowledge visualization]]
* [[Multimedia information retrieval]]
* [[Personal information management]]
* [[Relevance (Information Retrieval)]]
* [[Relevance feedback]]
* [[Rocchio Classification]]
* [[Index (search engine)|Search index]]
* [[Social information seeking]]
* [[Special Interest Group on Information Retrieval]]
* [[Subject indexing]]
* [[Temporal information retrieval]]
* [[tf-idf]]
* [[XML-Retrieval]]

{{div col end}}

== References ==
{{reflist}}

==Further reading==
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&amp;uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.
*Stefan B&amp;uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

==External links==
{{wikiquote}}
* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]
* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]
* [http://trec.nist.gov Text Retrieval Conference (TREC)]
* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]
* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]
* [http://ir-facility.org/ Information Retrieval Facility]
* [http://www.nonrelevant.net Information Retrieval @ DUTH]
* [http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf TREC report on information retrieval evaluation techniques]
* [http://www.ebaytechblog.com/2010/11/10/measuring-search-relevance/ How eBay measures search relevance]
* [http://retrieval.ceti.gr Information retrieval performance evaluation tool @ Athena Research Centre]

{{Authority control}}

{{DEFAULTSORT:Information Retrieval}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Information retrieval| ]]
[[Category:Natural language processing]]</text>
      <sha1>6kxixhrxiaty7w8zd4znvgjcio79sds</sha1>
    </revision>
  </page>
  <page>
    <title>Evaluation measures (information retrieval)</title>
    <ns>0</ns>
    <id>50716473</id>
    <revision>
      <id>749401100</id>
      <parentid>730899398</parentid>
      <timestamp>2016-11-14T03:37:41Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16733" xml:space="preserve">{{Orphan|date=June 2016}}

The '''evaluation measures''' of an information retrieval system is the process of assessing how well the search results satisfied the user's query intent. The metrics are often split in to multiple categories. Online metrics measure actual users' interactions with the search system. Offline metrics measure the relevance of the search engine by having expert judges measure how likely each result (or the SERP page as a whole) is to meet the information needs of the user.

The mathematical symbols used in the formulas below mean:
* &lt;math&gt;X \cap Y&lt;/math&gt; - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in ''both'' sets X and Y
* &lt;math&gt;| X |&lt;/math&gt; - [[Cardinality]] - in this case, the number of documents in set X
* &lt;math&gt;\int&lt;/math&gt; - [[Integral]]
* &lt;math&gt;\sum&lt;/math&gt; - [[Summation]]
* &lt;math&gt;\Delta&lt;/math&gt; - [[Symmetric difference]]

== Online metrics ==
Online metrics are generally created from data mined from search logs. The metrics are often used to determine the success of an [[A/B testing|A/B test]].

=== Session abandonment rate ===
Session abandonment rate is a ratio of search session which do not result in a click.

=== Click-through rate ===
Click-through rate ('''CTR''') is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an [[online advertising]] campaign for a particular website as well as the effectiveness of email campaigns.&lt;ref name=AMA&gt;[[American Marketing Association]] Dictionary. http://www.marketingpower.com/_layouts/Dictionary.aspx.{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} Retrieved 2012-11-02. The [[Marketing Accountability Standards Board (MASB)]] endorses this definition as part of its ongoing [http://www.commonlanguage.wikispaces.net/ Common Language in Marketing Project].&lt;/ref&gt;

=== Session success rate ===
Session success rate measures the ratio of user sessions that lead to a success. Defining "success" is often dependent on context, but for search a successful result is often measured using [[Dwell time (information retrieval)|dwell time]] as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.

=== Zero result rate ===
Zero result rate ('''ZRR''') is the ratio of [[Search engine results page|SERPs]] which returned with zero results. The metric either indicates a [[Precision and recall|recall]] issue, or that the information being searched for is not in the index.

== Offline metrics ==
Offline metrics are generally created from relevance judgement sessions where the judges score the quality of the search results.  The judges often score each result of a query as either binary (good/bad), or on a multi-level scale of satisfying the needs of the searcher. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy. For instance there is ambiguity in the query "mars", the judge does not know if the user is search for [[Mars]] the planet, [[Mars (chocolate bar)|Mars]] the chocolate bar, or [[Bruno Mars]] the singer.

=== Precision ===
{{main|Precision and recall}}

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:&lt;math&gt; \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} &lt;/math&gt;

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===
{{main|Precision and recall}}

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:&lt;math&gt;\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} &lt;/math&gt;

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:&lt;math&gt; \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} &lt;/math&gt;

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to &lt;math&gt;(1-\mbox{specificity})&lt;/math&gt;. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-score / F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:&lt;math&gt;F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}&lt;/math&gt;

This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

The general formula for non-negative real &lt;math&gt;\beta&lt;/math&gt; is:
:&lt;math&gt;F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,&lt;/math&gt;

Two other commonly used F measures are the &lt;math&gt;F_{2}&lt;/math&gt; measure, which weights recall twice as much as precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; "measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;.  Their relationship is:
:&lt;math&gt;F_\beta = 1 - E&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;

F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}

=== Average precision ===
&lt;!-- [[Average precision]] redirects here --&gt;
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision &lt;math&gt;p(r)&lt;/math&gt; as a function of recall &lt;math&gt;r&lt;/math&gt;. Average precision computes the average value of &lt;math&gt;p(r)&lt;/math&gt; over the interval from &lt;math&gt;r=0&lt;/math&gt; to &lt;math&gt;r=1&lt;/math&gt;:&lt;ref name="zhu2004"&gt;{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{AveP} = \int_0^1 p(r)dr&lt;/math&gt;
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:&lt;math&gt;\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)&lt;/math&gt;
where &lt;math&gt;k&lt;/math&gt; is the rank in the sequence of retrieved documents, &lt;math&gt;n&lt;/math&gt; is the number of retrieved documents, &lt;math&gt;P(k)&lt;/math&gt; is the precision at cut-off &lt;math&gt;k&lt;/math&gt; in the list, and &lt;math&gt;\Delta r(k)&lt;/math&gt; is the change in recall from items &lt;math&gt;k-1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt;.&lt;ref name="zhu2004" /&gt;

This finite sum is equivalent to:
:&lt;math&gt; \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!&lt;/math&gt;
where &lt;math&gt;\operatorname{rel}(k)&lt;/math&gt; is an indicator function equaling 1 if the item at rank &lt;math&gt;k&lt;/math&gt; is a relevant document, zero otherwise.&lt;ref name="Turpin2006"&gt;{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06&#8211;11, 2006) |publisher=ACM |location=New York, NY |pages=11&#8211;18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}&lt;/ref&gt; Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the &lt;math&gt;p(r)&lt;/math&gt; function to reduce the impact of "wiggles" in the curve.&lt;ref name=voc2010&gt;{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303&#8211;338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}&lt;/ref&gt;&lt;ref name="nlpbook"&gt;{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Sch&#252;tze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}&lt;/ref&gt; For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:&lt;ref name="voc2010" /&gt;&lt;ref name="nlpbook" /&gt;
:&lt;math&gt;\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)&lt;/math&gt;
where &lt;math&gt;p_{\operatorname{interp}}(r)&lt;/math&gt; is an interpolated precision that takes the maximum precision over all recalls greater than &lt;math&gt;r&lt;/math&gt;:
:&lt;math&gt;p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})&lt;/math&gt;.

An alternative is to derive an analytical &lt;math&gt;p(r)&lt;/math&gt; function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.&lt;ref&gt;K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves] {{webarchive |url=https://web.archive.org/web/20121208201457/http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf |date=December 8, 2012 }}. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.&lt;/ref&gt;

=== Precision at K ===

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.&lt;ref name="stanford" /&gt;  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.

=== R-Precision ===

R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, &lt;math&gt;R&lt;/math&gt;, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant &lt;math&gt;r&lt;/math&gt; turns that into a relevancy fraction: &lt;math&gt;r/R = r/15&lt;/math&gt;.&lt;ref name="trec15"/&gt;

Precision is equal to recall at the '''R'''-th position.&lt;ref name="stanford"&gt;{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch&#252;tze}}  Part of ''Introduction to Information Retrieval'' [http://nlp.stanford.edu/IR-book/]&lt;/ref&gt;

Empirically, this measure is often highly correlated to mean average precision.&lt;ref name="stanford" /&gt;

=== Mean average precision ===
&lt;!-- [[Mean average precision]] redirects here --&gt;
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:&lt;math&gt; \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!&lt;/math&gt;
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. &lt;/math&gt;

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (&lt;math&gt;IDCG_p&lt;/math&gt;), which normalizes the score:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. &lt;/math&gt;

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other measures ===
{{Confusion matrix terms}}
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]
* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents&lt;ref name="trec15"&gt;http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf&lt;/ref&gt;
* GMAP - geometric mean of (per-topic) average precision&lt;ref name="trec15" /&gt;
* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}

===Visualization===

Visualizations of information retrieval performance include:
* Graphs which chart precision on one axis and recall on the other&lt;ref name="trec15" /&gt;
* Histograms of average precision over various topics&lt;ref name="trec15" /&gt;
* [[Receiver operating characteristic]] (ROC curve)
* [[Confusion matrix]]

== Non-metrics ==

=== Top queries list ===
Top queries is noting the most common queries over a fixed amount of time. The top queries list assists in knowing the style of queries entered by users.

== Non-relevance metrics ==

=== Queries per time ===
Measuring how many queries are performed on the search system per (month/day/hour/minute/sec) tracks the utilization of the search system. It can be used for diagnostics to indicate an unexpected spike in queries, or simply as a baseline when comparing with other metrics, like query latency. For example, a spike in query traffic, may be used to explain a spike in query latency.

==References==
&lt;references /&gt;

[[Category:Information retrieval]]
[[Category:Information retrieval evaluation]]
[[Category:Internet search engines]]</text>
      <sha1>7gm642zlzcutuefyvrtwsslvuv412p7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer file systems</title>
    <ns>14</ns>
    <id>754856</id>
    <revision>
      <id>732172789</id>
      <parentid>546483788</parentid>
      <timestamp>2016-07-30T03:39:47Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>see [[Category_talk:Computer_storage#Category:Computer_file_systems_and_subcategories_here]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="982" xml:space="preserve">{{catdiffuse}}
{{Cat main|File system}}
{{Commonscat|File systems}}
A '''[[file system]]''' in computing, is a method for storing and organizing [[computer file]]s and the data they contain to make it easy to find and access them. File systems may use a [[data storage device]] such as a [[hard disk]] or [[CD-ROM]] and involve maintaining the physical location of the files, or they may be virtual and exist only as an access method for virtual data or for data over a network (e.g. [[Network File System (protocol)|NFS]]).

More formally, a file system is a set of [[abstract data type]]s that are implemented for the storage, hierarchical organization, manipulation, navigation, access, and retrieval of [[data]].

== See also ==
* [[:Category:Computer storage]]

[[Category:Data management]]
[[Category:Operating system technology|File systems]]
&lt;!--[[Category:Computer systems|File systems]] deleted  "Computer systems" not an index of systems --&gt;
[[Category:Storage software]]</text>
      <sha1>7c7e34eyuo5deqb86lpzi9bvuxeh0lh</sha1>
    </revision>
  </page>
  <page>
    <title>Schedule (computer science)</title>
    <ns>0</ns>
    <id>400457</id>
    <revision>
      <id>753088132</id>
      <parentid>731512263</parentid>
      <timestamp>2016-12-05T03:18:02Z</timestamp>
      <contributor>
        <ip>68.68.88.19</ip>
      </contributor>
      <comment>/* Conflict equivalence */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14441" xml:space="preserve">{{refimprove|date=November 2012}}

In the fields of [[database]]s and [[transaction processing]] (transaction management), a '''schedule''' (or '''history''') of a system is an abstract model to describe execution of transactions running in the system. Often it is a ''list'' of operations (actions) ordered by time, performed by a set of [[Database transaction|transactions]] that are executed together in the system. If order in time between certain operations is not determined by the system, then a ''[[partial order]]'' is used. Examples of such operations are requesting a read operation, reading, writing, aborting, committing, requesting lock, locking, etc. Not all transaction operation types should be included in a schedule, and typically only selected operation types (e.g., data access operations) are included, as needed to reason about and describe certain phenomena. Schedules and schedule properties are fundamental concepts in database [[concurrency control]] theory.

==Formal description==

The following is an example of a schedule:

:&lt;math&gt;D = \begin{bmatrix}
T1 &amp; T2 &amp; T3 \\
R(X) &amp;  &amp;  \\
W(X) &amp;  &amp;  \\
Com. &amp;  &amp;  \\
 &amp; R(Y) &amp; \\
 &amp; W(Y) &amp; \\
 &amp; Com. &amp; \\
 &amp;&amp; R(Z) \\
 &amp;&amp; W(Z) \\
 &amp;&amp; Com. \end{bmatrix}&lt;/math&gt;

In this example, the horizontal axis represents the different transactions in the schedule D. The vertical axis represents time order of operations. Schedule D consists of three transactions T1, T2, T3.  The schedule describes the actions of the transactions as seen by the [[DBMS]].
First T1 Reads and Writes to object X, and then Commits. Then T2 Reads and Writes to object Y and Commits, and finally T3 Reads and Writes to object Z and Commits.  This is an example of a ''serial'' schedule, i.e., sequential with no overlap in time, because the actions of in all three transactions are sequential, and the transactions are not interleaved in time.

Representing the schedule D above by a table (rather than a list) is just for the convenience of identifying each transaction's operations in a glance. This notation is used throughout the article below. A more common way in the technical literature for representing such schedule is by a list:

:::D = R1(X) W1(X) Com1 R2(Y) W2(Y) Com2 R3(Z) W3(Z) Com3

Usually, for the purpose of reasoning about concurrency control in databases, an operation is modeled as ''[[Atomic operation|atomic]]'', occurring at a point in time, without duration. When this is not satisfactory start and end time-points and possibly other point events are specified (rarely). Real executed operations always have some duration and specified respective times of occurrence of events within them (e.g., "exact" times of beginning and completion), but for concurrency control reasoning usually only the precedence in time of the whole operations (without looking into the quite complex details of each operation) matters, i.e., which operation is before, or after another operation. Furthermore, in many cases the before/after relationships between two specific operations do not matter and should not be specified, while being specified for other pairs of operations.

In general operations of transactions in a schedule can interleave (i.e., transactions can be executed concurrently), while time orders between operations in each transaction remain unchanged as implied by the transaction's program. Since not always time orders between all operations of all transactions matter and need to be specified, a schedule is, in general, a ''[[partial order]]'' between operations rather than a ''[[total order]]'' (where order for each pair is determined, as in a list of operations). Also in the general case each transaction may consist of several processes, and itself be properly represented by a partial order of operations, rather than a total order. Thus in general a schedule is a partial order of operations, containing ([[embedding]]) the partial orders of all its transactions.

Time-order between two operations can be represented by an ''[[ordered pair]]'' of these operations (e.g., the existence of a pair (OP1,OP2) means that OP1 is always before OP2), and a schedule in the general case is a [[set (mathematics)|set]] of such ordered pairs. Such a set, a schedule, is a [[partial order]] which can be represented by an ''[[acyclic directed graph]]'' (or ''directed acyclic graph'', DAG) with operations as nodes and time-order as a [[directed edge]] (no cycles are allowed since a cycle means that a first (any) operation on a cycle can be both before and after (any) another second operation on the cycle, which contradicts our perception of [[Time]]). In many cases a graphical representation of such graph is used to demonstrate a schedule.

'''Comment:''' Since a list of operations (and the table notation used in this article) always represents a total order between operations, schedules that are not a total order cannot be represented by a list (but always can be represented by a DAG).

==Types of schedule==

===Serial===

The transactions are executed non-interleaved (see example above)
i.e., a serial schedule is one in which no transaction starts until a running transaction has ended.

===Serializable===&lt;!-- This section is linked from [[Concurrency control]] --&gt;

A schedule that is equivalent (in its outcome) to a serial schedule has the [[serializability]] property.

In schedule E, the order in which the actions of the transactions are executed is not the same as in D, but in the end, E gives the same result as D.
:&lt;math&gt;E = \begin{bmatrix}
T1 &amp; T2 &amp; T3 \\
R(X) &amp;  &amp;  \\
   &amp; R(Y) &amp; \\
 &amp;&amp; R(Z) \\

W(X) &amp;  &amp;  \\
 &amp; W(Y) &amp; \\
 &amp;&amp; W(Z) \\
Com. &amp; Com. &amp; Com. \end{bmatrix}&lt;/math&gt;

====Conflicting actions====

Two actions are said to be in conflict (conflicting pair) if: 

# The actions belong to different transactions.
# At least one of the actions is a write operation.
# The actions access the same object (read or write).

The following set of actions is conflicting: 
* R1(X), W2(X), W3(X) (3 conflicting pairs)

While the following sets of actions are not: 
* R1(X), R2(X), R3(X)
* R1(X), W2(Y), R3(X)

====Conflict equivalence====

The schedules S1 and S2 are said to be conflict-equivalent if the following two conditions are satisfied: 

# Both schedules S1 and S2 involve the same set of transactions (including ordering of actions within each transaction).
# Both schedules have same set of conflicting operations.

====Conflict-serializable====

A schedule is said to be conflict-serializable when the schedule is conflict-equivalent to one or more serial schedules. 

Another definition for conflict-serializability is that a schedule is conflict-serializable if and only if its [[precedence graph]]/serializability graph, when only committed transactions are considered, is acyclic (if the graph is defined to include also uncommitted transactions, then cycles involving uncommitted transactions may occur without conflict serializability violation).

:&lt;math&gt;G = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
 &amp; R(A) \\
W(B) &amp; \\
Com. &amp; \\
 &amp; W(A) \\
 &amp; Com. \\
 &amp;\end{bmatrix}&lt;/math&gt;

Which is conflict-equivalent to the serial schedule &lt;T1,T2&gt;, but not &lt;T2,T1&gt;.

====Commitment-ordered====
{{POV-section|Commitment ordering|date=November 2011}}
A schedule is said to be commitment-ordered (commit-ordered), or commitment-order-serializable, if it obeys the [[Commitment ordering]] (CO; also commit-ordering or commit-order-serializability) schedule property. This means that the order in time of transactions' commitment events is compatible with the precedence (partial) order of the respective transactions, as induced by their schedule's acyclic precedence graph (serializability graph, conflict graph). This implies that it is also conflict-serializable. The CO property is especially effective for achieving [[Global serializability]] in distributed systems.

'''Comment:''' [[Commitment ordering]], which was discovered in 1990, is obviously not mentioned in ([[#Bern1987|Bernstein et al. 1987]]). Its correct definition appears in ([[#Weikum2001|Weikum and Vossen 2001]]), however the description there of its related techniques and theory is partial, inaccurate, and misleading.{{Says who|date=December 2011}} For an extensive coverage of commitment ordering and its sources see ''[[Commitment ordering]]'' and ''[[The History of Commitment Ordering]]''.

====View equivalence====

Two schedules S1 and S2 are said to be view-equivalent when the following conditions are satisfied:

# If the transaction &lt;math&gt;T_i&lt;/math&gt; in S1 reads an initial value for object X, so does the transaction &lt;math&gt;T_i&lt;/math&gt; in S2. 
# If the transaction &lt;math&gt;T_i&lt;/math&gt; in S1 reads the value written by transaction &lt;math&gt;T_j&lt;/math&gt; in S1 for object X, so does the transaction &lt;math&gt;T_i&lt;/math&gt; in S2.
# If the transaction &lt;math&gt;T_i&lt;/math&gt; in S1 is the final transaction to write the value for an object X, so is the transaction &lt;math&gt;T_i&lt;/math&gt; in S2.

====View-serializable====

A schedule is said to be view-serializable if it is view-equivalent to some serial schedule. 
Note that by definition, all conflict-serializable schedules are view-serializable. 

:&lt;math&gt;G = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
 &amp; R(A) \\
W(B) &amp; \\
 \end{bmatrix}&lt;/math&gt;

Notice that the above example (which is the same as the example in the discussion of conflict-serializable) is both view-serializable and conflict-serializable at the same time.) There are however view-serializable schedules that are not conflict-serializable: those schedules with a transaction performing a [[blind write]]:
 
:&lt;math&gt;H = \begin{bmatrix}
T1 &amp; T2 &amp; T3 \\
R(A) &amp; &amp; \\
 &amp; W(A) &amp; \\
 &amp; Com. &amp; \\
W(A) &amp; &amp; \\
Com. &amp; &amp; \\
 &amp; &amp; W(A) \\
 &amp; &amp; Com. \\
 &amp; &amp; \end{bmatrix}&lt;/math&gt;

The above example is not conflict-serializable, but it is view-serializable since it has a view-equivalent serial schedule &lt;T1,&amp;nbsp;T2,&amp;nbsp;T3&gt;. 

Since determining whether a schedule is view-serializable is [[NP-complete]], view-serializability has little practical interest.{{citation needed|date=April 2015}}

===Recoverable===&lt;!-- This section is linked from [[Concurrency control]] --&gt;

Transactions commit only after all transactions whose changes they read, commit.

:&lt;math&gt;F = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Com. &amp; \\
 &amp; Com.\\
 &amp;\end{bmatrix} 
F2 = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Abort &amp;  \\
&amp; Abort \\
 &amp;\end{bmatrix}&lt;/math&gt;

These schedules are recoverable.  F is recoverable because T1 commits before T2, that makes the value read by T2 correct.  Then T2 can commit itself.  In F2, if T1 aborted, T2 has to abort because the value of A it read is incorrect.  In both cases, the database is left in a consistent state.

====Unrecoverable====

If a transaction T1 aborts, and a transaction T2 commits, but T2 relied on T1, we have an unrecoverable schedule.

:&lt;math&gt;G = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
 &amp; Com. \\
Abort &amp; \\
 &amp;\end{bmatrix}&lt;/math&gt;

In this example, G is unrecoverable, because T2 read the value of A written by T1, and committed.  T1 later aborted, therefore the value read by T2 is wrong, but since T2 committed, this schedule is unrecoverable.

====Avoids cascading aborts / rollbacks (ACA)====

Also named cascadeless. Avoids that a single transaction abort leads to a series of transaction rollbacks. A strategy to prevent cascading aborts is to disallow a transaction from reading uncommitted changes from another transaction in the same schedule. 

The following examples are the same as the ones in the discussion on recoverable: 

:&lt;math&gt;F = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Com. &amp; \\
 &amp; Com.\\
 &amp;\end{bmatrix} 
F2 = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Abort &amp;  \\
&amp; Abort \\
 &amp;\end{bmatrix}&lt;/math&gt;

In this example, although F2 is recoverable, it does not avoid 
cascading aborts. It can be seen that if T1 aborts, T2 will have to 
be aborted too in order to maintain the correctness of the schedule 
as T2 has already read the uncommitted value written by T1. 

The following is a recoverable schedule which avoids cascading abort. Note, however, that the update of A by T1 is always lost (since T1 is aborted).

:&lt;math&gt;F3 = \begin{bmatrix}
T1 &amp; T2 \\
 &amp; R(A) \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; W(A) \\
Abort &amp;  \\
&amp; Commit \\
 &amp;\end{bmatrix}&lt;/math&gt;
Note that this Schedule would not be serializable if T1 would be committed.
Cascading aborts avoidance is sufficient but not necessary for a schedule to be recoverable.

====Strict====

A schedule is strict - has the strictness property - if for any two transactions T1, T2, if a write operation of T1 precedes a ''conflicting'' operation of T2 (either read or write), then the commit or abort event of T1 also precedes that conflicting operation of T2.

Any strict schedule is cascadeless, but not the converse. Strictness allows efficient recovery of databases from failure.

==Hierarchical relationship between serializability classes==

The following expressions illustrate the hierarachical (containment) relationships between [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]] classes: 

* Serial &amp;sub; commitment-ordered &amp;sub; conflict-serializable &amp;sub; view-serializable &amp;sub; all schedules
* Serial &amp;sub; strict &amp;sub; avoids cascading aborts &amp;sub; recoverable &amp;sub; all schedules

The [[Venn diagram]] (below) illustrates the above clauses graphically. 

[[File:Schedule-serializability.png|frame|none|Venn diagram for serializability and recoverability classes]]

==Practical implementations==

In practice, most general purpose database systems employ conflict-serializable and recoverable (primarily strict) schedules.

==See also==
* [[schedule (project management)]]

==References==

*&lt;cite id=Bern1987&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman: [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5&lt;/cite&gt;
*&lt;cite id=Weikum2001&gt;[[Gerhard Weikum]], Gottfried Vossen: [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, 2001, ISBN 1-55860-508-8&lt;/cite&gt;

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>qzcrywnup7vhz0b5njlz99q07rcjgw8</sha1>
    </revision>
  </page>
  <page>
    <title>Nested transaction</title>
    <ns>0</ns>
    <id>1867103</id>
    <revision>
      <id>762483030</id>
      <parentid>733812145</parentid>
      <timestamp>2017-01-29T02:56:50Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>/* top */replaced: component based &#8594; component-based using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3169" xml:space="preserve">A '''nested transaction''' is a [[database transaction]] that is started by an instruction within the scope of an already started transaction.

Nested transactions are implemented differently in different databases. However, they have in common that the changes are not made visible to any unrelated transactions until the outermost transaction has committed. This means that a commit in an inner transaction does not necessarily persist updates to the database.

In some databases, changes made by the nested transaction are not seen by the 'host' transaction until the nested transaction is committed. According to some,{{Who|date=November 2009}} this follows from the isolation property of transactions.

The capability to handle nested transactions properly is a prerequisite for true component-based application architectures. In a component-based encapsulated architecture, nested transactions can occur without the programmer knowing it. A component function may or may not contain a database transaction (this is the encapsulated secret of the component. See [[Information hiding]]). If a call to such a component function is made inside a BEGIN - COMMIT bracket, nested transactions occur. Since popular databases like [[MySQL]]&lt;ref&gt;
{{cite web
 |url=http://dev.mysql.com/doc/refman/4.1/en/implicit-commit.html
 |title=Statements That Cause an Implicit Commit
 |author=
 |work=MySQL 4.1 Reference Manual
 |publisher=Oracle
 |accessdate=5 December 2010
}}
&lt;/ref&gt; do not allow nesting BEGIN - COMMIT brackets, a framework or a transaction monitor is needed to handle this. When we speak about nested transactions, it should be made clear that this feature is DBMS dependent and is not available for all databases.

Theory for nested transactions is similar to the theory for flat transactions.&lt;ref&gt;{{Cite journal
  | last = Resende | first = R.F. | last2 = El Abbadi | first2 = A.
  | title = On the serializability theorem for nested transactions
  | journal = Information Processing Letters | volume = 50 | issue = 4
  | pages = 177&#8211;183 | date = 1994-05-25 
  | doi = 10.1016/0020-0190(94)00033-6 }}&lt;/ref&gt;

The banking industry usually processes financial transactions using ''open nested transactions'',{{Citation needed|date=August 2015}} which is a looser variant of the nested transaction model that provides higher performance while accepting the accompanying trade-offs of inconsistency.&lt;ref&gt;{{cite journal
  | last = Weikum | first = Gerhard |author2=Hans-J. Schek
  | title = Concepts and Applications of Multilevel Transactions and Open Nested Transactions
  | journal = Database Transaction Models for Advanced Applications
  | pages = 515&#8211;553 | publisher = Morgan Kaufmann | year = 1992
  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7962 | isbn = 1-55860-214-3
  | accessdate = 2007-11-13 }}&lt;/ref&gt;

==Further reading==
* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8

==References==
{{reflist}}

[[Category:Data management]]
[[Category:Transaction processing]]

{{database-stub}}</text>
      <sha1>t656gl2qtd642vt29se0ofhx9cldvqt</sha1>
    </revision>
  </page>
  <page>
    <title>Online complex processing</title>
    <ns>0</ns>
    <id>7577329</id>
    <revision>
      <id>544571927</id>
      <parentid>244255920</parentid>
      <timestamp>2013-03-16T06:46:44Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q4335184]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="446" xml:space="preserve">'''Online complex processing''' ('''OLCP''') is a class of realtime data processing involving complex queries, lengthy queries and/or simultaneous reads and writes to the same records.
{{software-stub}}
==Sources==
*http://www.pcmag.com/encyclopedia_term/0,2542,t=online+complex+processing&amp;i=48345,00.asp

==See also==
*[[Online transaction processing]] 
*[[OLAP]]
*[[Transaction processing]]


[[Category:Data management]]
[[Category:Databases]]</text>
      <sha1>gdd1yy6oy0v7xzucep5jhm78m9rhz5t</sha1>
    </revision>
  </page>
  <page>
    <title>Single source publishing</title>
    <ns>0</ns>
    <id>1227094</id>
    <revision>
      <id>746466339</id>
      <parentid>746465537</parentid>
      <timestamp>2016-10-27T15:45:35Z</timestamp>
      <contributor>
        <ip>80.195.137.224</ip>
      </contributor>
      <comment>/* List of single-source publishing tools */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13928" xml:space="preserve">{{redirect|Single source}}
'''Single source publishing''', also known as '''single sourcing publishing''', is a [[content management]] method which allows the same source [[Content (media)|content]] to be used across different forms of [[Media (communication)|media]] and more than one time.&lt;ref&gt;Kay Ethier, ''XML and FrameMaker'', pg. 19. [[New York City|New York]]: [[Apress]], 2004. ISBN 9781430207191&lt;/ref&gt;&lt;ref&gt;Lucas Walsh, "The Application of Single-Source Publishing to E-Government." Taken from ''Encyclopedia of Digital Government'', pg. 64. Eds. Ari-Veikko Anttiroiko and Matti M&#228;lki&#228;. [[Hershey, Pennsylvania|Hershey]]: IGI Global, 2007. ISBN 9781591407904&lt;/ref&gt;&lt;ref&gt;[http://www.stylusstudio.com/single_source_publishing.html Single Source Publishing] at [[Stylus Studio]]. Copyright &#169; 2005-2013 [[Progress Software]]. Accessed June 11, 2013.&lt;/ref&gt;&lt;ref name=petra&gt;[http://www.writersua.com/articles/singlesource/ Single Source Publishing with Flare]. Copyright &#169; 2010 WritersUA. Published November 16, 2010; accessed June 11, 2013.&lt;/ref&gt; The labour-intensive and expensive work of [[Technical editing#Technical editing|editing]] need only be carried out once, on only one document;&lt;ref name=cms&gt;Barry Schaeffer, [http://www.cmswire.com/cms/information-management/single-source-publishing-creating-customized-output-015069.php Single Source Publishing: Creating Customized Output]. CMS Wire, 3 April 2012. Accessed 10 June 2013.&lt;/ref&gt; that source document can then be stored in one place and reused.&lt;ref&gt;[[Ann Rockley]] and Charles Cooper, [https://books.google.com/books?id=82X6jGY_dHMC&amp;pg=PT75&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CDoQ6AEwBg#v=onepage&amp;q=single%20source%20publishing&amp;f=false Managing Enterprise Content: A Unified Content Strategy], Chapter 5: Product content. 2nd ed. [[Berkeley, California|Berkeley]]: [[New Riders Press]], 2012. ISBN 9780132931649&lt;/ref&gt; This reduces the potential for error, as corrections are only made one time in the source document.&lt;ref&gt;Janet Mackenzie, ''The Editor's Companion'', pg. 92. [[Cambridge]]: [[Cambridge University Press]], 2011. ISBN 9781107402188&lt;/ref&gt;

The benefits of single source publishing primarily relate to the editor rather than the [[User (computing)|user]]. The user does benefit from consistent terminology and information, but this consistency is also a potential weakness of single source publishing if the content manager does not have an organized [[Conceptualization (information science)|conceptualization]].&lt;ref name=petra/&gt; Single-source publishing is sometimes used synonymously with '''multi-channel publishing''' though whether or not the two terms are synonymous is a matter of discussion.&lt;ref name=mek&gt;[http://www.mekon.com/index.php/pages/knowledge_zone/single-sourcing-multi-channel-publishing/technology_standards Single Source &amp; Multi-Channel Publishing]. &#169; 2013 Mekon, accessed 23 June 2013.&lt;/ref&gt;

==Definition==
While there is a general definition of single source publishing, there is no single official delineation between single source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delinieation. Single source publishing is most often understood as the creation of one source document in [[Microsoft Word]] or [[Adobe FrameMaker]] and converting that document into different [[file format]]s or human [[language]]s (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.&lt;ref name=mek/&gt;

==History==
The origins of single-source publishing lie, indirectly, with the release of [[Windows 3.0]] in 1990.&lt;ref name=bob162&gt;Bob Boiko, [https://books.google.com/books?id=p6nUDn3ZaBoC&amp;pg=PA162&amp;dq=Single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=CUiqU6LEDaLV0QXd44CQDw&amp;ved=0CCoQ6AEwAzgK#v=onepage&amp;q=Single%20source%20publishing&amp;f=false Content Management Bible], pg. 162. [[Hoboken, New Jersey|Hoboken]]: [[John Wiley &amp; Sons]], 2005. ISBN 9780764583643&lt;/ref&gt; With the eclipsing of [[MS-DOS]] by [[graphical user interface]]s, help files went from being unreadable text along the bottom of the screen to hypertext systems such as [[WinHelp]]. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of [[software documentation]] did not simply move from being writers of traditional bound books to writers of [[electronic publishing]], but rather they became authors of central documents which could be reused multiple times across multiple formats.&lt;ref name=bob162/&gt;

The first single-source publishing project was started in 1993 by Cornelia Hofmann at [[Schneider Electric]] in [[Seligenstadt]], using software based on [[Interleaf]] to automatically create paper documentation in multiple languages based on a single original source file.&lt;ref&gt;[https://books.google.com/books?id=inCeft4AkXcC&amp;pg=PA65&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CCoQ6AEwAw#v=onepage&amp;q=single%20source%20publishing&amp;f=false Translating Into Success: Cutting-edge Strategies for Going Multilingual in a Global Age], pg. 227. Eds. Robert C. Sprung and Simone Jaroniec. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9789027231871&lt;/ref&gt;

[[XML]], developed during the mid- to late-1990s, was also significant to the development of single source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.&lt;ref&gt;Doug Wallace and Anthony Levinson, "The XML e-Learning Revolution: Is Your Production Model Holding You Back?" Taken from [https://books.google.com/books?id=4RK7tJ-OO3cC&amp;pg=PA65&amp;dq=Single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=CUiqU6LEDaLV0QXd44CQDw&amp;ved=0CEgQ6AEwCDgK#v=onepage&amp;q=Single%20source%20publishing&amp;f=false Best of The eLearning Guild's Learning Solutions: Articles from the eMagazine's First Five Years], pg. 63. Ed. Bill Brandon. Hoboken: John Wiley &amp; Sons, 2008. ISBN 9780470277157&lt;/ref&gt;

In the mid-1990s, several firms began creating and using single source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt &amp; Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor.  The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.
Ford, for example, was able to tag its single owner's manual files so that 12 model years could be generated via a resolution script running on the single completed file.  Pratt &amp; Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single source files, calling out the desired version at publication time.  World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.

Starting from the early 2000s, single source publishing was used with an increasing frequency in the field of [[technical translation]]. It is still regarded as the most efficient method of publishing the same material in different languages.&lt;ref&gt;Bert Esselink, "Localisation and translation." Taken from [https://books.google.com/books?id=a4W7lWgCqYoC&amp;pg=PA73&amp;dq=Single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=CUiqU6LEDaLV0QXd44CQDw&amp;ved=0CCUQ6AEwAjgK#v=onepage&amp;q=Single%20source%20publishing&amp;f=false Computers and Translation: A Translator's Guide], pg. 73. Ed. H. L. Somers. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2003. ISBN 9789027216403&lt;/ref&gt; Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method.&lt;ref&gt;Burt Esselink, ''A Practical Guide to Localization'', pg. 228. Volume 4 of Language international world directory. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9781588110060&lt;/ref&gt; [[Metadata]] could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.&lt;ref&gt;Cornelia Hofmann and Thorsten Mehnert, "Multilingual Information Management at Schneider Automation." Taken from ''Translating Into Success'', pg. 67.&lt;/ref&gt;

Although single source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single source publishing or render one's operations obsolete.&lt;ref name=cms/&gt;

==Criticism==
Single-source publishing has been criticized due to the quality of work, being compared to as the "conveyor belt assembly" of content creation by its critics.&lt;ref&gt;Mick Hiatt, [http://mashstream.com/mashups/the-myth-of-single-source-authoring/ The Myth of Single-Source Authoring]. Mashstream, November 18, 2009.&lt;/ref&gt; 

While heavily used in technical translation, there are risks of error in regard to [[Index (publishing)|indexing]]. While two words might be [[synonym]]s in English, they may not be synonyms in another language. In a document produced via single sourcing, however, the index will be translated automatically and the two words will be rendered as synonyms because they are in the [[Source language (translation)|source language]], while in the [[Target language (translation)|target language]] they are not.&lt;ref&gt;Nancy Mulvany, [https://books.google.com/books?id=G0Eqm8FbiTMC&amp;pg=PA312&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CEcQ6AEwCA#v=onepage&amp;q=single%20source%20publishing&amp;f=false Indexing Books], pg. 154. 2nd ed. [[Chicago]]: [[University of Chicago Press]], 2009. ISBN 9780226550176&lt;/ref&gt;

==See also==
* [[Content management]]
* [[Darwin Information Typing Architecture]]
* [[EPUB]]
* [[Markup language]]

===List of single-source publishing tools=== 
* [[Adobe FrameMaker]]&lt;ref&gt;Sarah S. O'Keefe, Sheila A. Loring, Terry Smith and Lydia K. Wong, [https://books.google.com/books?id=b-yEKgcQmN8C&amp;pg=PA6&amp;dq=single+source+publishing&amp;hl=en&amp;sa=X&amp;ei=rRehU_TxFs2W0QXT6YCQAQ&amp;ved=0CE0Q6AEwCQ#v=onepage&amp;q=single%20source%20publishing&amp;f=false Publishing Fundamentals: Unstructured FrameMaker 8], pg. 6. Scriptorium Publishing, 2008. ISBN 9780970473349&lt;/ref&gt;
* [[Apache Cocoon]]
* [[Apache Forrest]]
* [[Altova]]
* [[Booktype]]
* [[DocBook XSL]]
* [[DITA Open Toolkit]]
* [[Help &amp; Manual]]
* [[MadCap Flare]]
* [[Oxygen XML Editor|Oxygen XML editor]]
* [[Scenari]]
* [[Sphinx (documentation generator)|Sphinx]]&lt;ref&gt;{{cite web | url = http://pythonic.pocoo.org/2008/3/21/sphinx-is-released | title = Sphinx is released! &amp;raquo; And now for something completely Pythonic... | publisher = Georg Brandl| work = And now for something completely Pythonic... | accessdate = 2011-04-03 }}&lt;/ref&gt;
* [[XPLM Publisher]]

==References==
{{reflist}}

==Further reading==
* {{cite book | last = Ament | first = Kurt | authorlink = | title = Single Sourcing: Building Modular Documentation | publisher = William Andrew | date = 2007-12-17 |  location = | pages = 245 | url = | doi = | id = | isbn = 0-8155-1491-3 }}
* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}
* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath | title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}
* {{cite book | last = Maler | first = Eve | authorlink = |author2=Jeanne El Andaloussi  | title = Developing SGML DTDs: From Text to Model to Markup | publisher = Prentice Hall PTR | date = 1995-12-15 | location = | pages = 560 | url = | doi = | id = | isbn = 0-13-309881-8 }} (the "bible" for Data Modeling)

==External links==
* [http://www.elkera.com/cms/articles/seminars_and_presentations/planning_a_single_source_publishing_application_for_business_documents/ Planning a Single Source Publishing Application for Business Documents] (A paper presented by Peter Meyer at OpenPublish, Sydney, on 29 July 2005)
* [https://www.tug.org/TUGboat/tb29-1/tb91sojka.pdf Single-source publishing in multiple formats for different output devices]
* [http://www.agilemodeling.com/essays/singleSourceInformation.htm Single Sourcing Information - An Agile Practice for Effective Documentation]
* [http://www.stcsig.org/ss Society for Technical Communication Single-sourcing Special Interest Group]
* [http://www.wisegeek.com/what-is-single-source-publishing.htm What Is Single Source Publishing?] at WiseGeek
* [http://www.technical-communication.org/topics/information-development.html tekom Europe] (Articles about Information Development and Single Source Publishing)

[[Category:Technical communication]]
[[Category:Computer file systems]]
[[Category:Data management]]</text>
      <sha1>lo7fy72f0vsj6ef57trf58lmzq5z09a</sha1>
    </revision>
  </page>
  <page>
    <title>Workflow engine</title>
    <ns>0</ns>
    <id>7711975</id>
    <revision>
      <id>742608756</id>
      <parentid>742608754</parentid>
      <timestamp>2016-10-04T18:17:06Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/144.160.5.102|144.160.5.102]] to version by 69.255.124.243. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2784062) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3015" xml:space="preserve">A '''workflow engine''' is a [[software application]] that manages business processes. It is a key component in [[workflow technology]] and typically makes use of a [[database server]].

A workflow engine manages and monitors the state of activities in a [[workflow]], such as the processing and approval of a loan application form, and determines which new activity to transition to according to defined processes (workflows).&lt;ref&gt;http://docs.oracle.com/cd/B13789_01/workflow.101/b10286/wfapi.htm&lt;/ref&gt; The actions may be anything from saving an application form in a [[document management system]] to sending a reminder e-mail to users or escalating overdue items to management. A workflow engine facilitates the flow of information, tasks, and events. Workflow engines may also be referred to as Workflow Orchestration Engines.&lt;ref&gt;http://pic.dhe.ibm.com/infocenter/tivihelp/v48r1/index.jsp?topic=%2Fcom.ibm.sco.doc_2.2%2Fenablement%2Fworkfloworchestration.html&lt;/ref&gt;

Workflow engines mainly have three functions:
*	Verification of the current status: Check whether the command is valid in executing a task.
*	Determine the authority of users: Check if the current user is permitted to execute the task.
*	Executing condition script: After passing the previous two steps, the workflow engine begins to evaluate the condition script in which the two processes are carried out, if the condition is true, workflow engine execute the task, and if the execution successfully completes, it returns the success, if not, it reports the error to trigger and roll back the change.&lt;ref&gt;The Workflow Engine Model. [http://msdn.microsoft.com/en-us/library/aa188337%28office.10%29.aspx  The Workflow Engine Model] Accessed 1 Dec. 2010.&lt;/ref&gt;

A workflow engine is a core technique for task allocation software, such as [[business process management]], in which the workflow engine allocates tasks to different executors while communicating data among participants. A workflow engine can execute any arbitrary sequence of steps, for example, a healthcare data analysis.&lt;ref name=hf2010&gt;{{Cite journal | last1 = Huser | first1 = V. | last2 = Rasmussen | first2 = L. V. | last3 = Oberg | first3 = R. | last4 = Starren | first4 = J. B. | title = Implementation of workflow engine technology to deliver basic clinical decision support functionality | doi = 10.1186/1471-2288-11-43 | journal = BMC Medical Research Methodology | volume = 11 | pages = 43 | year = 2011 | pmid = 21477364 | pmc = 3079703}}&lt;/ref&gt;

== See also ==
*[[Business rules engine]]
*[[Business rule management system]]
*[[Comparison of BPEL engines]]
*[[Inference engine]]
*[[Java Rules Engine API]]
*[[Rete algorithm]]
*[[Ripple down rules]]
*[[Semantic reasoner]]
*[[BPEL|Business Process Execution Language]]
*[[Production system (computer science)|Production system]]
*[[Workflow management system]]

== References ==
{{Reflist}}

[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Workflow technology]]
[[Category:Workflow software]]</text>
      <sha1>sy0rnkx6d7ot9whrbfyjik7xq5soa03</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Semantic desktop</title>
    <ns>14</ns>
    <id>12251078</id>
    <revision>
      <id>547930206</id>
      <parentid>474682861</parentid>
      <timestamp>2013-03-31T04:13:59Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 1 langlinks, now provided by Wikidata on [[d:Q8728629]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="97" xml:space="preserve">{{Cat main|Semantic desktop}}

[[Category:Knowledge representation]]
[[Category:Data management]]</text>
      <sha1>ru3intp4okx5skv7iqh2ud9q553vtys</sha1>
    </revision>
  </page>
  <page>
    <title>Tagsistant</title>
    <ns>0</ns>
    <id>12989031</id>
    <revision>
      <id>748259515</id>
      <parentid>677002446</parentid>
      <timestamp>2016-11-07T07:39:54Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* Main criticisms */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9171" xml:space="preserve">{{POV|date=October 2012}}

{{Infobox software
| name                   = Tagsistant
| logo                   = [[file:Tagsistant logo.png|300px]]
| developer              = Tx0 &lt;tx0@strumentiresistenti.org&gt;
| latest release version = 0.6
| frequently_updated     = yes&lt;!-- Release version update? Don't edit this page, just click on the version number! --&gt;
| programming language   = [[C (programming language)|C]]
| operating system       = Linux kernel
| language               = English
| genre                  = [[Semantic file system]]
| license                = [[GNU General Public License|GNU GPL]]
| website                = http://www.tagsistant.net/
}}
{{Infobox filesystem
| name = Tagsistant
| developer = Tx0
| full_name =
| introduction_date =
| introduction_os =
| partition_id =
| directory_struct =
| file_struct =
| bad_blocks_struct =
| max_file_size =
| max_files_no =
| max_filename_size =
| max_volume_size =
| dates_recorded =
| date_range =
| date_resolution =
| forks_streams =
| attributes =
| file_system_permissions =
| compression =
| encryption =
| OS =
}}
'''Tagsistant''' is a [[semantic file system]] for the [[Linux kernel]], written in [[C (programming language)|C]] and based on [[Filesystem in Userspace|FUSE]]. Unlike traditional [[file systems]] that use hierarchies of directories to locate objects, Tagsistant introduces the concept of [[Tag (metadata)|tags]].

==Design and differences with hierarchical file systems==

In computing, a [[file system]] is a type of data store which could be used to store, retrieve and update [[Computer file|files]]. Each file can be uniquely located by its [[Path (computing)|path]]. The user must know the path in advance to access a file and the path does not necessarily include any information about the content of the file.

Tagsistant uses a complementary approach based on [[Tag (metadata)|tags]]. The user can create a set of tags and apply those tags to files, [[File directory|directories]] and other objects ([[Device file|devices]], [[Named pipe|pipes]], ...). The user can then search all the objects that match a subset of tags, called a query. This kind of approach is well suited for managing user contents like pictures, audio recordings, movies and text documents but is incompatible with system files (like libraries, commands and configurations) where the univocity of the path is a [[Computer security|security]] requirement to prevent the access to a wrong content.

==The tags/ directory==

A Tagsistant file system features four main directories:

:archive/
:relations/
:stats/
:tags/

Tags are created as sub directories of the &lt;code&gt;tags/&lt;/code&gt; directory and can be used in queries complying to this syntax:

:&lt;code&gt;tags/subquery/[+/subquery/[+/subquery/]]/@/&lt;/code&gt;&lt;ref&gt;{{cite web|title=tags/ and relations/ directories|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&amp;start=3}}&lt;/ref&gt;

where a subquery is an arbitrarily long list of tags, concatenated as directories:

:&lt;code&gt;tag1/tag2/tag3/.../tagN/&lt;/code&gt;

The portion of a path delimited by &lt;code&gt;tags/&lt;/code&gt; and &lt;code&gt;@/&lt;/code&gt; is the actual query. The &lt;code&gt;+/&lt;/code&gt; operator joins the results of different sub-queries in one single list. The &lt;code&gt;@/&lt;/code&gt; operator ends the query.

To be returned as a result of the following query:

:&lt;code&gt;tags/t1/t2/+/t1/t4/@/&lt;/code&gt;

an object must be tagged as both &lt;code&gt;t1/&lt;/code&gt; and &lt;code&gt;t2/&lt;/code&gt; or as both &lt;code&gt;t1/&lt;/code&gt; and &lt;code&gt;t4/&lt;/code&gt;. Any object tagged as &lt;code&gt;t2/&lt;/code&gt; or &lt;code&gt;t4/&lt;/code&gt;, but not as &lt;code&gt;t1/&lt;/code&gt; will not be retrieved.

The query syntax deliberately violates the [[POSIX]] file system semantics by allowing a path token to be a descendant of itself, like in &lt;code&gt;tags/t1/t2/+/t1/t4/@&lt;/code&gt; where &lt;code&gt;t1/&lt;/code&gt; appears twice. As a consequence a recursive scan of a Tagsistant file system  will exit with an error or endlessly loop, as done by [[UNIX]] &lt;code&gt;[[Find|find]]&lt;/code&gt;:

&lt;syntaxhighlight lang="bash"&gt;
~/tagsistant_mountpoint$ find tags/
tags/
tags/document
tags/document/+
tags/document/+/document
tags/document/+/document/+
tags/document/+/document/+/document
tags/document/+/document/+/document/+
[...]
&lt;/syntaxhighlight&gt;

This drawback is balanced by the possibility to list the tags inside a query in any order. The query &lt;code&gt;tags/t1/t2/@/&lt;/code&gt; is completely equivalent to &lt;code&gt;tags/t2/t1/@/&lt;/code&gt; and &lt;code&gt;tags/t1/+/t2/t3/@/&lt;/code&gt; is equivalent to &lt;code&gt;tags/t2/t3/+/t1/@/&lt;/code&gt;.

The &lt;code&gt;@/&lt;/code&gt; element has the precise purpose of restoring the POSIX semantics: the path &lt;code&gt;tags/t1/@/directory/&lt;/code&gt; refers to a traditional directory and a recursive scan of this path will properly perform.

==The reasoner and the relations/ directory==

Tagsistant features a simple [[Semantic Reasoner|reasoner]] which expands the results of a query by including objects tagged with related tags. A relation between two tags can be established inside the &lt;code&gt;relations/&lt;/code&gt; directory following a three level pattern:

:&lt;code&gt;relations/tag1/rel/tag2/&lt;/code&gt;

The &lt;code&gt;rel&lt;/code&gt; element can be ''includes'' or ''is_equivalent''. To include the ''rock'' tag in the ''music'' tag, the UNIX command &lt;code&gt;mkdir&lt;/code&gt; can be used:

:&lt;code&gt;mkdir -p relations/music/includes/rock&lt;/code&gt;

The reasoner can recursively resolve relations, allowing the creation of complex structures:

:&lt;code&gt;mkdir -p relations/music/includes/rock&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/rock/includes/hard_rock&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/rock/includes/grunge&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/rock/includes/heavy_metal&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/heavy_metal/includes/speed_metal&lt;/code&gt;

The web of relations created inside the &lt;code&gt;relations/&lt;/code&gt; directory constitutes a basic form of [[Ontology (information science)|ontology]].

==Autotagging plugins==

Tagsistant features an autotagging plugin stack which gets called when a file or a symlink is written.&lt;ref&gt;{{cite web|title=How to write a plugin for Tagsistant?|url=http://www.tagsistant.net/documents-about-tagsistant/coding-and-debugging/7-how-to-write-a-plugin-for-tagsistant}}&lt;/ref&gt; Each plugin is called if its declared [[Mime type|MIME type]] matches

The list of working plugins released with Tagsistant 0.6 is limited to:

* text/html: tags the file with each word in &lt;code&gt;&lt;title&gt;&lt;/code&gt; and &lt;code&gt;&lt;keywords&gt;&lt;/code&gt; elements and with ''document'', ''webpage'' and ''html'' too
* image/jpeg: tags the file with each [[Exchangeable image file format|Exif]] tag

==The repository==

Each Tagsistant file system has a corresponding repository containing an &lt;code&gt;archive/&lt;/code&gt; directory where the objects are actually saved and a &lt;code&gt;tags.sql&lt;/code&gt; file holding tagging information as an [[SQLite]] database. If the [[MySQL]] database engine was specified with the &lt;code&gt;--db&lt;/code&gt; argument, the &lt;code&gt;tags.sql&lt;/code&gt; file will be empty. Another file named &lt;code&gt;repository.ini&lt;/code&gt; is a [[GLib]] ini store with the repository configuration.&lt;ref&gt;{{cite web|title=Key-value file parser|url=https://developer.gnome.org/glib/2.32/glib-Key-value-file-parser.html}}&lt;/ref&gt;

Tagsistant 0.6 is compatible with the MySQL and Sqlite dialects of SQL for tag reasoning and tagging resolution. While porting its logic to other SQL dialects is possible, differences in basic constructs (especially the INTERSECT SQL keyword) must be considered.

==The archive/ and stats/ directories==

The &lt;code&gt;archive/&lt;/code&gt; directory has been introduced to provide a quick way to access objects without using tags. Objects are listed with their inode number prefixed.&lt;ref&gt;{{cite web|title=Tagsistant 0.6 howto - Inodes|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&amp;start=6}}&lt;/ref&gt;

The &lt;code&gt;stats/&lt;/code&gt; directory features some read-only files containing usage statistics. A file &lt;code&gt;configuration&lt;/code&gt; holds both compile time information and current repository configuration.

==Main criticisms==

It has been highlighted that relying on an external database to store tags and tagging information could cause the complete loss of metadata if the database gets corrupted.&lt;ref&gt;{{cite web|title=Extended attributes and tag file systems|url=http://www.lesbonscomptes.com/pages/tagfs.html}}&lt;/ref&gt;

It has been highlighted that using a flat namespace tends to overcrowd the &lt;code&gt;tags/&lt;/code&gt; directory.&lt;ref&gt;{{cite web|title=The major problem with this approach is scalability|publisher=https://news.ycombinator.com/item?id=2573318}}&lt;/ref&gt; This could be mitigated introducing [[Tag (metadata)#Triple tags|triple tags]].

==See also==
{{Portal|Free software}}
[[Semantic file system]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.tagsistant.net/}}
* [https://aur.archlinux.org/packages.php?ID=54644 Arch Linux package]
* [https://news.ycombinator.com/item?id=2573318 Discussion on Hacker News]
* [http://www.lesbonscomptes.com/pages/tagfs.html Extended attributes and tag file systems]
* [http://lakm.us/logit/2010/03/tagsistant-on-production-2/ Tagsistant On Production]

[[Category:Computer file systems]]
[[Category:Data management]]
[[Category:Semantic file systems]]</text>
      <sha1>gkyvib83wpoyik8o5eco8mh75yuvnxh</sha1>
    </revision>
  </page>
  <page>
    <title>XLDB</title>
    <ns>0</ns>
    <id>14426202</id>
    <revision>
      <id>748738496</id>
      <parentid>738690280</parentid>
      <timestamp>2016-11-10T02:14:49Z</timestamp>
      <contributor>
        <username>Green Cardamom</username>
        <id>8931761</id>
      </contributor>
      <minor />
      <comment>Fix url error in cite template using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7605" xml:space="preserve">'''XLDB''' refers to '''eXtremely Large [[Database|Data Bases]]'''.  The definition of ''extremely large'' refers to data sets that are too big in terms of volume (too much), and/or velocity (too fast), and/of variety (too many places, too many formats) to be handled using conventional solutions.

== History ==

In October 2007 the XLDB experts gathered at [[SLAC]] for the [https://web.archive.org/web/20080417002612/http://www-conf.slac.stanford.edu/xldb07/ First Workshop on Extremely Large Databases]. As a result, the XLDB research community was formed. to meet rapidly growing demands, in addition to the original invitational workshop, an open conference, tutorials, and annual satellite events on different continents were added. The main event, held annually at Stanford gathers over 300 technically savvy attendees. XLDB is one of the premier database events catered towards both academic and industrial communities.

== Goals ==

The main goals of this community include:&lt;ref&gt;{{ cite web | url=http://www-conf.slac.stanford.edu/xldb09/docs/xldb09_welcomeTalk.ppt | year=2009 | last=Becla| first=Jacek | title=XLDB 3 Welcome | accessdate=2009-08-29 }}&lt;/ref&gt;

* Identify trends, commonalities and major roadblocks related to building extremely large databases
* Bridge the gap between users trying to build extremely large databases and database solution providers worldwide
* Facilitate development and growth of practical technologies for extremely large data stores

== XLDB Community ==
As of 2013, the community consisted of about a thousand members including:
# Scientists who develop, use, or plan to develop or use XLDB for their research, from laboratories.
# Commercial users of XLDB.
# Providers of database products, including commercial vendors and representatives from open source database communities.
# Academic database researchers.

== XLDB Conferences, Workshops and Tutorials ==
The community meets annually at [[Stanford]] where the main event is held each fall, usually in September. These who live too far from California to attend have the opportunity to attend satellite events, organized annually around May/June either in [[Asia]] or in [[Europe]].

A detailed report is produced after each workshop.

{| class="wikitable"
|-
! Year
! Place
! Link
! Report
! Comments
|-
| 2015
| [[Stanford]]
| [https://web.archive.org/web/20150521105100/http://www-conf.slac.stanford.edu/xldb2015/]
|
| 8th XLDB Conference
|-
| 2014
| [http://www.on.br/ Observat&#243;rio Nacional], [[Rio_de_Janeiro]]
| [https://web.archive.org/web/20150219081443/http://xldb-rio2014.linea.gov.br/]
|
| Satellite XLDB Workshop in South America
|-
| 2014
| [[Stony_Brook_University]]
| [https://web.archive.org/web/20150521052839/http://www3.cs.stonybrook.edu/~xldb/]
|
| XLDB-Healthcare Workshop
|-
| 2013
| [[Stanford]]
| [https://conf-slac.stanford.edu/xldb-2013/]
|
| 7th XLDB Conference
|-
| 2013
| [[CERN]], [[Geneva]]/[[Switzerland]]
| [http://xldb-europe-workshop-2013.web.cern.ch/]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
|
| Satellite XLDB Workshop in Europe
|-
| 2012
| [[Stanford]]
| [http://www-conf.slac.stanford.edu/xldb2012/]
| [http://www.jstage.jst.go.jp/article/dsj/12/0/12_12_023/_pdf]
| 6th XLDB Conference, Workshop &amp; Tutorials
|-
| 2012
| [[Beijing]], [[China]]
| [https://web.archive.org/web/20120708164351/http://idke.ruc.edu.cn/xldb/www.xldb-asia.org/home.html]
| [http://www.xldb.org/wp-content/uploads/2012/09/XLDBAsia2012Report.pdf]
| Satellite XLDB Conference in Asia
|-
| 2011
| [[SLAC]]
| [https://web.archive.org/web/20110426125951/http://www-conf.slac.stanford.edu/xldb2011/]
| [http://www.jstage.jst.go.jp/article/dsj/11/0/37/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 5th XLDB Conference and Workshop
|-
| 2011
| [[Edinburgh]], [[UK]]
| [https://web.archive.org/web/20160303221547/http://xldb.eu/xldb_europe_2011/]
| not available
| Satellite XLDB Workshop in Europe
|-
| 2010
| [[SLAC]]
| [https://web.archive.org/web/20110727234052/http://www-conf.slac.stanford.edu/xldb2010/]
| [http://www.jstage.jst.go.jp/article/dsj/9/0/9_MR1/_article]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 4th XLDB Conference and Workshop
|-
| 2009
| [[Lyon]], [[France]]
| [https://web.archive.org/web/20110727234623/http://www-conf.slac.stanford.edu/xldb2009/]
| [http://www.jstage.jst.go.jp/article/dsj/8/0/MR1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 3rd XLDB Workshop
|-
| 2008
| [[SLAC]]
| [https://web.archive.org/web/20110727234818/http://www-conf.slac.stanford.edu/xldb2008/]
| [http://www.jstage.jst.go.jp/article/dsj/7/0/196/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 2nd XLDB Workshop
|-
| 2007
| [[SLAC]]
| [https://web.archive.org/web/20110727235121/http://www-conf.slac.stanford.edu/xldb2007/]
| [http://www.jstage.jst.go.jp/article/dsj/7/0/1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 1st XLDB Workshop
|}

== Tangible results ==

The XLDB events led to initiating the effort of building a new open source, science database, [https://web.archive.org/web/20090220121225/http://scidb.org/ SciDB].&lt;ref&gt;{{ cite web | url=http://www.jstage.jst.go.jp/article/dsj/7/0/88/_pdf  | year=2008 | last=Becla| first=Jacek | title=Report from the SciDB Workshop |accessdate=2008-09-29}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

The XLDB organizers started defining a [http://www.xldb.org/science-benchmark/ science benchmark] for scientific data management systems called SS-DB.

At [http://xldb.org/2012|XLDB 2012]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} the XLDB organizers announced that two major databases that support arrays as first-class objects ([[MonetDB]] SciQL and [[SciDB]]) have formed a working group in conjunction with XLDB. This working group is proposing a common syntax (provisionally named &#8220;ArrayQL&#8221;) for manipulating arrays, including array creation and query.

== References ==
{{reflist}}

== Further reading ==
* Pavlo A., Paulson E., Rasin A., Abadi D. J., Dewitt D. J., Madden S., and Stonebraker M., ''A Comparison of Approaches to Large-Scale Data Analysis," Proceedings of the 2009 ACM SIGMOD, http://web.archive.org/web/20090611174944/http://database.cs.brown.edu:80/sigmod09/benchmarks-sigmod09.pdf
* Becla, J., et al. 2006, ''Designing a multi-petabyte database for LSST,'' http://arxiv.org/abs/cs/0604112
* Becla, J., &amp; Wang, D. L. 2005, ''Lessons Learned from Managing a Petabyte'', downloaded from http://web.archive.org/web/20110604223735/http://www.slac.stanford.edu/pubs/slacpubs/10750/slac-pub-10963.pdf on 2007-11-25.
* Bell, G., Gray, J., &amp; Szalay, A. 2005, ''Petascale computations systems: Balanced cyberinfrastructure in a data-centric world,'' http://arxiv.org/abs/cs/0701165
* Duellmann, D. 1999, ''Petabyte Databases'', ACM SIGMOD Record, vol. 28, p. 506, http://web.archive.org/web/20071012015357/http://www.sigmod.org/sigmod/record/issues/9906/index.html#TutorialSessions.
* Hanushevsky, A., &amp; Nowak, M. 1999, ''Pursuit of a Scalable High Performance Multi-Petabyte Database'', 16th IEEE Symposium on Mass Storage Systems, pp. 169&#8211;175, http://citeseer.ist.psu.edu/217883.html.
* Shiers, J., ''Building Very Large, Distributed Object Databases'', downloaded from http://web.archive.org/web/20070915101842/http://wwwasd.web.cern.ch:80/wwwasd/cernlib/rd45/papers/dbprog.html on 2007-11-25.

[[Category:Types of databases]]
[[Category:Data management]]</text>
      <sha1>nz60v4y81ouc3hcxgbesd2q0ejvpg8t</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data synchronization</title>
    <ns>14</ns>
    <id>7645825</id>
    <revision>
      <id>547448093</id>
      <parentid>498449680</parentid>
      <timestamp>2013-03-28T14:19:54Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363890]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="188" xml:space="preserve">[[Category:Synchronization]]
[[Category:Data management|Synchronization]]
[[Category:Distributed computing problems]]
[[Category:Fault-tolerant computer systems]]
[[Category:Data quality]]</text>
      <sha1>0o9p199s3p26nv88de1wvpq4e4hb25e</sha1>
    </revision>
  </page>
  <page>
    <title>Paper data storage</title>
    <ns>0</ns>
    <id>13756939</id>
    <revision>
      <id>752737302</id>
      <parentid>752736972</parentid>
      <timestamp>2016-12-03T00:06:27Z</timestamp>
      <contributor>
        <username>Riceissa</username>
        <id>20698937</id>
      </contributor>
      <comment>/* Limits */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5247" xml:space="preserve">{{refimprove|date=August 2012}}
'''Paper data storage''' refers to the use of [[paper]] as a [[data storage device]]. This includes [[writing]], [[illustrating]], and the use of data that can be interpreted by a machine or is the result of the functioning of a machine.  A defining feature of paper data storage is the ability of humans to produce it with only simple tools and interpret it visually.

Though this is now mostly obsolete, paper was once also an important form of [[computer data storage]].

==History==
Before paper was used for storing data, it had been used in several applications for storing instructions to specify a machine's operation.  The earliest use of paper to store instructions for a machine was the work of [[Basile Bouchon]] who, in 1725, used punched paper rolls to control textile looms.  This technology was later developed into the wildly successful [[Jacquard loom]].  The 19th century saw several other uses of paper for controlling machines.  In 1846, telegrams could be prerecorded on [[punched tape]] and rapidly transmitted using [[Alexander Bain (inventor)|Alexander Bain]]'s automatic telegraph.  Several inventors took the concept of a mechanical organ and used paper to represent the music.  

In the late 1880s [[Herman Hollerith]] invented the recording of data on a medium that could then be read by a machine.  Prior uses of machine readable media, above, had been for control ([[automaton]]s, [[piano roll]]s, [[Jacquard loom|looms]], ...), not data.  "After some initial trials with paper tape, he settled on [[punched card]]s..."&lt;ref&gt;[http://www.columbia.edu/acis/history/hollerith.html Columbia University Computing History - Herman Hollerith]&lt;/ref&gt;  Hollerith's method was used in the 1890 census.&lt;!-- The Census Bureau is not "an independent 3rd party" source - as required by Wikipedia - for Census Bureau performance claims. FOLLOWING CLAIM DELETED. --- and the completed results were "... finished months ahead of schedule and far under budget".&lt;ref&gt;[http://www.census.gov/history/www/technology/010873.html U.S. Census Bureau: Tabulation and Processing]&lt;/ref&gt;--&gt;  Hollerith's company eventually became the core of [[International Business Machines|IBM]]. 

Other technologies were also developed that allowed  machines to work with marks on paper instead of punched holes.  This technology was widely used for [[optical scan voting system|tabulating votes]] and grading [[scantron|standardized tests]].  [[Barcode]]s made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it. Banks used magnetic ink on checks, supporting MICR scanning.

In an early electronic computing device, the [[Atanasoff-Berry Computer]], electric sparks were used to singe small holes in paper cards to represent binary data.  The altered [[dielectric constant]] of the paper at the location of the holes could then be used to read the binary data back into the machine by means of electric sparks of lower voltage than the sparks used to create the holes.  This form of paper data storage was never made reliable and was not used in any subsequent machine.

As of 2014, [[Universal Product Code]] barcodes, first used in 1974, are ubiquitous.

Some people recommend a width of at least 3 pixels for each minimum-width gap and each minimum-width bar for 1D barcodes;
and a width of at least 4 pixels&#8212;e.g., a 4&amp;nbsp;&#215;&amp;nbsp;4 pixel = 16 pixel module for [[2D barcode]]s.&lt;ref&gt;
Accusoft.
[http://www.accusoft.com/whitepapers/barcodes/BarcodesinDocuments-BestPractices.pdf "Using Barcodes in Documents &#8211; Best Practices"].
2007.
Retrieved 2014-04-25.
&lt;/ref&gt;
For a typical black-and-white barcode scanned by a typical 300 dpi [[image scanner]],
and assuming roughly half the space is occupied by finder patterns, fiducial alignment patterns, and error detection and correction codes, that recommendation gives a maximum data density of roughly 50 bits per linear inch (about 2 bit/mm) for 1D barcodes, and roughly 2 800 bits per square inch (about 4.4 bit/mm&lt;sup&gt;2&lt;/sup&gt;).

==Limits==
The limits of data storage depend on the technology to write and read such data.  For example, an 8&#8243;&amp;nbsp;&#215;&amp;nbsp;10&#8243; (roughly A4 without margins) 300dpi 8-bit greyscale image map contains 7.2 megabytes of data&#8212;assuming a scanner can accurately reproduce the printed image to that resolution and [[color depth]], and a program can accurately interpret such an image.  A similarly sized image in 2400dpi 24-bit true color theoretically contains 1.38 gigabytes of information.

==See also==
{{div col|3}}
*[[Banknote]] read by a [[vending machine]]
*[[Book music]]
*[[Edge-notched card]]
*[[Index card]]
*[[Kimball tag]]
*[[Machine-readable medium]]
*[[Magnetic ink character recognition]]
*[[Mark sense]]
*[[Music roll]]
*[[Optical mark recognition]]
*[[Paper disc]]
*[[Perfin]]
*[[Perforation]]
*[[Punched tape]]
*[[Spindle (stationery)]]
*[[Stenotype]]
*[[Ticker tape]]
{{div col end}}

==References==
{{reflist}}

==External links==
* [http://www.microglyphs.com/english/html/dataglyphs.shtml DataGlyphs]
* [http://ollydbg.de/Paperbak/ PaperBack data storage]
{{Paper data storage media}}

[[Category:Data management]]
[[Category:Storage media]]</text>
      <sha1>add7p6ytvn1seizpp643nggtywawq39</sha1>
    </revision>
  </page>
  <page>
    <title>Holistic Data Management</title>
    <ns>0</ns>
    <id>17377283</id>
    <revision>
      <id>609004781</id>
      <parentid>450897010</parentid>
      <timestamp>2014-05-17T20:25:37Z</timestamp>
      <contributor>
        <username>Hebrides</username>
        <id>1264145</id>
      </contributor>
      <comment>clean up, remove orphan tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7548" xml:space="preserve">'''Holistic Data Management''' (HDM) framework is AHISDATA indigenous standard for implementing software implementations within an organization network. This framework extends the existing data management solutions such as [[data quality]], [[data governance]], [[data integration]], [[data processing]], [[master data management]] and [[data validation]] solutions.

The HDM framework specifies that:
*All data objects must exist as a child data object or a parent data object.
*Only one unique parent data object must exist within a data network scope (DNS).
*All child data objects must have a data-mapping link defined within a data network scope.
*A data object relationship must exist at least in one of the following four data management modules:
''Data mapping'', ''data validation'', ''[[data integration]]'',''[[data processing]]''

== HDM framework ==
The following entities are specified in the HDM framework.

*''Data network scope (DNS)''
The data network scope (DNS) is the logical boundary that a software application database system of record (SOR) exists within an enterprise network. There can be multiple DNS within an enterprise network.

*''Data network domain (DND)''
The data network domain (DND) is the logical boundary representing a collection of multiple data network scope (DNS). There can be multiple DND within an enterprise network.

*''System of record (SOR)''
A system of record applies to the master or principal database system that a parent data objects resides on.
There can only be one SOR within a data network scope.

*''Parent data object (PDO)''
A parent data object (PDO) is the system of record schema object name. Only one unique parent data object must exist within a data network scope.

*''Child data object (CDO)''
A child data object (CDO) is a schema object name that derives its data from one or more parent data object(s).

*''Data-mapping link (DML)''
A data-mapping link (DML) is the data requirement specification applied to the relationship between multiple database schema objects where one data object derives its data from one or more data objects. DML is only applicable for a data-mapping data management module.

*''Data&#8211;object relationship (DOR)''
The data&#8211;object relationship (DOR) is the data requirement, business rule, program function that applies to one or multiple data objects. DOR can be applied on data-mapping links for each data management modules. Only one DOR can exist on a DML within a data management module.

*''Data management modules (DMM)''
Data management modules are the common user interface (UI) programs that defines and manage the data object relationship(s) within a data network scope.

There are four data management modules:

''Data mapping'' &#8211; This is the base data management user interface module. The data-mapping module provide the functionalities for managing data-mapping links and data object relationships for all database schemas within a data network scope. A data network scope must have at least one data-mapping design defined.

''Data validation'' &#8211; This user interface module provides the functionalities for defining and managing validation events on data object relationships. Validation events include auditing, reporting, scheduler, logger, triggers and DNS health check. Data validation events requires a data-mapping design defined within a data network scope.

''[[Data integration]]'' &#8211; This user interface module provides the functionalities for defining and managing interface configurations on data object relationships. The interface configurations include scheduler, transmission mode, listener, interface API and reporting. The interface APIs would allow third-party systems to transfer data using the data object relationship defined within a data network scope. Data Integration interface configuration requires a Data Mapping design defined within a data network scope.

''[[Data processing]]'' &#8211; This user interface module provides the functionalities for defining and managing interface configurations and batch runtime engines on data object relationships. The interface configurations include scheduler, transmission mode, multi-batch transmission, user-defined DOR API and reporting. Data Processing interface configuration requires a data-mapping design defined within a data network scope.

== Implementing the HDM framework ==
The HDM framework presents a standard for software implementations within an organization. The objective is to shed visibility, increase efficiency and centralized management of all other software implementations within an organization.

The HDM framework should be implemented as a major organization project that is supervised by the project management office. This would require a project charter developed and a project manager assigned for managing the implementation process. There are several phases involve in implementing the HDM framework:

*''Choose a data management module (DMM)'' &#8211; This exercise requires the acquisition of a data management module software application to be used for implementing the rest of the HDM framework. AHISDATA iNTEGRITY software is an integrated solution that provides DMM functionalities.
*''Scrub (inventory of existing applications and data sources)'' &#8211; This exercise identifies all applications within an organization and the data sources that they are connected to.
*''Formation (applications and data schema relation)'' &#8211; This exercise is to align all applications in relation to the data schemas within the data sources. The applications are grouped in the order of the data schemas that they access.
*''First axe (applications eligible for decommission)'' &#8211; This exercise is to identify all applications that are rogue, obsolete and completely redundant. These applications are eligible for removal.
*''Second axe (application eligible for consolidation)'' &#8211; This exercise is to identify all applications that have some functional similarities and some uniqueness in the data requirement. These applications are eligible for consolidation. The functionalities that are similar are left intact on one application and turned off or disabled on the other(s).
*''Define data network domain (DND)'' &#8211; This exercise is to define the data network domain for all the approved applications within the enterprise network.
*''Define Data Network Scope (DNS)'' &#8211; This exercise is to define the data network scope(s) required for each DND.
*''Define system of records (SOR)'' &#8211; This exercise is to define the SOR for each DNS.
*''Define parent data objects (PDO)'' &#8211; This exercise is to define all PDOs in each DNS.
*''Define child data objects (CDO)'' &#8211; This exercise is to define all CDOs in each DNS.
*''Define data mapping links (DML)'' &#8211; This exercise is to define all data-mapping links and object relationship in all DNS.
*''Define data object relationships (DOR)'' &#8211; This exercise is to define the DOR requirement for each data management module implemented.

==See also==
*[[Reference data]]
*[[Master data]]
*[[Customer data integration]]
*[[Product information management]]
*[[Identity resolution]]

==External links==
* [http://www.ahisdata.com/eHDMS/AHISDATA_HDM_WhitePaper_v35.pdf AHISDATA HDM WhitePaper]
* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 The What, Why, and How of Master Data Management]

{{Data warehouse}}

[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing products]]
[[Category:Information technology management]]</text>
      <sha1>hf1jfndqe95k7gjtgciaco7xw52d62b</sha1>
    </revision>
  </page>
  <page>
    <title>Schema crosswalk</title>
    <ns>0</ns>
    <id>18048026</id>
    <revision>
      <id>760224308</id>
      <parentid>746499655</parentid>
      <timestamp>2017-01-15T18:34:36Z</timestamp>
      <contributor>
        <username>Capvespre</username>
        <id>25212216</id>
      </contributor>
      <minor />
      <comment>#1Lib1Ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7635" xml:space="preserve">A '''schema crosswalk''' is a table that shows equivalent elements (or "fields") in more than one [[database schema]]. It maps the elements in one schema to the equivalent elements in another schema.

Crosswalk tables are often employed within or in parallel to [[enterprise systems]], especially when multiple systems are interfaced or when the system includes [[legacy system]] data. In the context of Interfaces, they function as a sort of internal [[Extract, Transform, Load|ETL]] mechanism.

For example, this is a [[metadata]] crosswalk from [[MARC standards|MARC]] to [[Dublin Core]]:

&lt;center&gt;
{| class="wikitable"
|-
! MARC field
! 
! Dublin Core element
|-
| $260c (Date of publication, distribution, etc.)
| &#8594;
| Date.Created
|-
| 522 (Geographic Coverage Note)
| &#8594;
| Coverage.Spatial
|-
| $300a (Physical Description)
| &#8594;
| Format.Extent
|}
&lt;/center&gt;

Crosswalks show people where to put the data from one scheme into a different scheme. They are often used by libraries, archives, museums, and other cultural institutions to translate data to or from [[MARC standards|MARC]], [[Dublin Core]], [[Text Encoding Initiative|TEI]], and other metadata schemes.  For example, say an archive has a MARC record in their catalog describing a manuscript.  If the archive makes a digital copy of that manuscript and wants to display it on the web along with the information from the catalog, it will have to translate the data from the MARC catalog record into a different format such as [[Metadata Object Description Schema|MODS]] that is viewable in a webpage.  Because MARC has different fields than MODS, decisions must be made about where to put the data into MODS. This type of "translating" from one format to another is often called "metadata mapping" or "field mapping," and is related to "[[data mapping]]", and "[[Semantic mapper|semantic mapping]]".

Crosswalks also have several technical capabilities.  They help databases using different metadata schemes to share information. They help metadata harvesters create union catalogs.  They enable search engines to search multiple databases simultaneously with a single query.

== Challenges for crosswalks ==

One of the biggest challenges for crosswalks is that no two metadata schemes are 100% equivalent.  One scheme may have a field that doesn't exist in another scheme, or it may have a field that is split into two different fields in another scheme; this is why you often lose data when mapping from a complex scheme to a simpler one.  For example, when mapping from MARC to Simple Dublin Core, you lose the distinction between types of titles:

&lt;center&gt;
{| class="wikitable"
|-
! MARC field
! 
! Dublin Core element
|-
| 210 Abbreviated Title
| &#8594;
| Title
|-
| 222 Key Title
| &#8594;
| Title
|-
| 240 Uniform Title
| &#8594; 
| Title
|-
| 242 Translated Title
| &#8594;
| Title
|-
| 245 Title Statement
| &#8594;
| Title
|-
| 246 Variant Title
| &#8594;
| Title
|}
&lt;/center&gt;

Simple Dublin Core only has one single "Title" element so all of the different types of MARC titles get lumped together without any further distinctions.  This is called "many-to-one" mapping. This is also why, once you've translated these titles into Simple Dublin Core you can't translate them back into MARC.  Once they're Simple Dublin Core you've lost the MARC information about what types of titles they are so when you map from Simple Dublin Core back to MARC, all the data in the "Title" element maps to the basic MARC 245 Title Statement field.&lt;ref&gt;[http://www.loc.gov/marc/dccross.html "Dublin Core to MARC Crosswalk,"] Network Development and MARC Standards Office, Library of Congress&lt;/ref&gt;

&lt;center&gt;
{| class="wikitable"
|-
! Dublin Core element
!
! MARC field
|-
| Title
| &#8594;
| 245 Title Statement
|-
| Title
| &#8594;
| 245 Title Statement
|-
| Title
| &#8594;
| 245 Title Statement
|-
| Title
| &#8594;
| 245 Title Statement
|-
| Title
| &#8594;
| 245 Title Statement
|-
| Title
| &#8594;
| 245 Title Statement
|}
&lt;/center&gt;

This is why crosswalks are said to be "lateral" (one-way) mappings from one scheme to another.  Separate crosswalks would be required to map from scheme A to scheme B and from scheme B to scheme A.&lt;ref&gt;{{Cite book|title=Metadata fundamentals for all librarians|last=Caplan|first=Priscilla|publisher=American Library Association|year=2003|isbn=0838908470|location=Chicago|pages=39|quote=|via=}}&lt;/ref&gt;

===Difficulties in mapping===
Other mapping problems arise when:

*One scheme has one element that needs to be split up with different parts of it placed in multiple other elements in the second scheme ("one-to-many" mapping)
*One scheme allows an element to be repeated more than once while another only allows that element to appear once with multiple terms in it
*Schemes have different data formats (e.g. ''John Doe'' or ''Doe, John'')
*An element in one scheme is indexed but the equivalent element in the other scheme is not
*Schemes may use different controlled vocabularies
*Schemes change their standards over time

Some of these problems are simply not fixable. As Karen Coyle says in "''Crosswalking Citation Metadata: The University of California's Experience,''"

&lt;blockquote&gt;"The more metadata experience we have, the more it becomes clear that metadata perfection is not attainable, and anyone who attempts it will be sorely disappointed.  When metadata is crosswalked between two or more unrelated sources, there will be data elements that cannot be reconciled in an ideal manner.  The key to a successful metadata crosswalk is intelligent flexibility.  It is essential to focus on the important goals and be willing to compromise in order to reach a practical conclusion to projects."&lt;ref&gt;&lt;u&gt;in&lt;/u&gt; "Metadata in Practice" Diane I. Hillmann and Elaine L. Westbrooks, eds., American Library Association, Chicago, 2004, p. 91.&lt;/ref&gt;&lt;/blockquote&gt;

==Examples==

MARC to Dublin Core (Library of Congress) 
http://loc.gov/marc/marc2dc.html

Dublin Core to MARC21 (Library of Congress) 
http://www.loc.gov/marc/dccross.html

Dublin Core to UNIMARC (UKOLN)
http://www.ukoln.ac.uk/metadata/interoperability/dc_unimarc.html

TEI to and from MARC
http://purl.oclc.org/NET/teiinlibraries

FGDC to USMARC (Alexandria) 
http://www.alexandria.ucsb.edu/public-documents/metadata/fgdc2marc.html

ONIX to MARC21 (LC) 
http://www.loc.gov/marc/onix2marc.html

VRA to MARC (Indiana University) 
http://php.indiana.edu/%7Efryp/marcmap.html

Metadata Mappings (MIT Library)
http://web.archive.org/web/20080720134522/http://libraries.mit.edu/guides/subjects/metadata/mappings.html

Mapping Between Metadata formats (UKOLN) 
http://www.ukoln.ac.uk/metadata/interoperability/

International Metadata Standard Mappings (Academia Sinica) 
http://www.sinica.edu.tw/%7Emetadata/standard/mapping-foreign_eng.htm

JATS to MARC
http://webservices.itcs.umich.edu/mediawiki/jats/index.php/JATS-to-MARC_mapping

== See also ==
* [[Meta tag]]
* [[Metadata]]
* [[Database]]

==References==
 {{Reflist}}

==External links==
* [http://www.oclc.org/research/researchworks/schematrans/default.htm "Metadata Crosswalk Depository" (SchemaTrans)](OCLC)
* [http://www.ukoln.ac.uk/metadata/interoperability "Mapping Between Metadata Formats"] (UKOLN)
* [http://www.getty.edu/research/conducting_research/standards/intrometadata/path.html "Crosswalks the Path to Universal Access?"] (Getty)
* [http://www.dlib.org/dlib/june06/chan/06chan.html "Metadata Interoperability and Standardization - A Study of Methodology Part I"] (D-Lib)

[[Category:Data management]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata]]
[[Category:Technical communication]]</text>
      <sha1>sxjzi8iw00fcxqqojkeu14hl24y0flc</sha1>
    </revision>
  </page>
  <page>
    <title>Electronically stored information (Federal Rules of Civil Procedure)</title>
    <ns>0</ns>
    <id>19675044</id>
    <revision>
      <id>732728082</id>
      <parentid>729551330</parentid>
      <timestamp>2016-08-02T21:11:00Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>cap, bold, simplify heading</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4054" xml:space="preserve">'''Electronically stored information''' ('''ESI'''), for the purpose of the [[Federal Rules of Civil Procedure]] (FRCP) is information created, manipulated, communicated, stored, and best utilized in digital form, requiring the use of computer hardware and software.&lt;ref name="nwjtip"&gt;[http://www.law.northwestern.edu/journals/njtip/v4/n2/3 ''Electronically Stored Information: The December 2006 Amendments to the Federal Rules of Civil Procedure''], Kenneth J. Withers, Northwestern Journal of Technology and Intellectual Property, Vol.4 (2), 171&lt;/ref&gt;

ESI has become a legally defined phrase as the [[Federal government of the United States|U.S. government]] determined for the purposes of the FRCP rules of 2006 that promulgating procedures for maintenance and discovery for electronically stored information was necessary.  References to &#8220;electronically stored information&#8221; in the Federal Rules of Civil Procedure (FRCP) invoke an expansive approach to what may be discovered during the fact-finding stage of civil litigation.&lt;ref name="Federal Rules of Civil Procedure"&gt;{{cite web|title=Federal Rules of Civil Procedure (FRCP)|url=https://www.law.cornell.edu/rules/frcp/rule_34|website=Legal Information Institute [LII]|publisher=Cornell University Law School|accessdate=October 31, 2015|ref=Rule 34}}&lt;/ref&gt;

Rule 34(a) enables a party in a civil lawsuit to request another party to produce and permit the requesting party or its representative to inspect, copy, test, or sample the following items in the responding party's possession, custody, or control:

&lt;blockquote&gt;any designated documents or electronically stored information&#8212;including writings, drawings, graphs, charts, photographs, sound recordings, images, and other data or data compilations&#8212;stored in any medium from which information can be obtained either directly or, if necessary, after translation by the responding party into a reasonably usable form...Rule 34(a)(1) is intended to be broad enough to cover all current types of computer-based information, and flexible enough to encompass future changes and developments.&lt;/blockquote&gt;

==Types==

===Native files===
The term ''native files'' refers to user-created documents, which could be in [[Microsoft Office]] or [[Apache OpenOffice|Open Office]] document formats as well as other files stored on computer, but could include video surveillance footage saved on a computer hard drive, [[Computer-aided design]] files such as [[blueprint]]s or maps, [[digital photography|digital photographs]], scanned images, [[archive file]]s, e-mail, and [[digital audio]] files, among other data,

===Logical data===
A judge ruled that [[Random Access Memory|RAM]] is reasonably accessible and retainable for anticipation of litigation.{{Citation needed|date=November 2011}}

In Australia RAM, can be used in litigation post 1996.

==References==
{{reflist}}

==Further reading==
* {{cite book|chapter=Meet the New Rules|title=The Discovery Revolution|author1=George L. Paul |author2=Bruce H. Nearon |publisher=American Bar Association|year=2006|isbn=9781590316054}}
* {{cite book|title=Discovery of Electronically Stored Information|author=Ronald J. Hedges|publisher=BNA Books|year=2007|isbn=9781570186721}}
* {{cite book|title=The Sedona Principles 2007: Best Practices Recommendations &amp;amp; Principles for Addressing Electronic Document Production|author=Jonathan M. Redgrave|publisher=BNA Books|year=2007|isbn=9781570186776}}
* {{cite book|title=Cyber Forensics|author1=Albert J. Marcella |author2=Albert J. Marcella Jr. |author3=Doug Menendez |chapter=Electronically stored information and cyber forensics|publisher=CRC Press|year=2007|isbn=9780849383281}}
* {{cite book|title=Litigating With Electronically Stored Information|author1= Marian K. Riedy |author2=Suman Beros |author3= Kim Sperduto |publisher=Artech House Telecommunications Library|year=2007|isbn=9781596932203}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:United States discovery law]]
[[Category:Records management]]


{{US-law-stub}}</text>
      <sha1>eajijg8ucj17s6wiz2ajvb4znvjbi36</sha1>
    </revision>
  </page>
  <page>
    <title>Mobile content management system</title>
    <ns>0</ns>
    <id>22464607</id>
    <revision>
      <id>734719946</id>
      <parentid>732744245</parentid>
      <timestamp>2016-08-16T07:55:37Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>/* Examples of Mobile content management systems */ rm section as spam magnet</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4033" xml:space="preserve">A '''Mobile Content Management system''' ('''MCMs''') is a type of [[content management system]] (CMS) capable of storing and delivering content and services to mobile devices, such as mobile phones, smart phones, and PDAs. Mobile content management systems may be discrete systems, or may exist as features, modules or add-ons of larger content management systems capable of multi-channel content delivery. Mobile content delivery has unique, specific constraints including widely variable device capacities, small screen size, limited wireless bandwidth, small storage capacity, and comparatively weak device processors.&lt;ref&gt;[http://www.insight-corp.com/%5CExecSummaries%5Ccontent08ExecSum.pdf Content Management for Wireless Networks, 2008-2013 - Insight Research Report]&lt;/ref&gt;

Demand for mobile content management increased as mobile devices became increasingly ubiquitous and sophisticated. MCMS technology initially focused on the business to consumer (B2C) mobile market place with ringtones, games, text-messaging, news, and other related content. Since, mobile content management systems have also taken root in business-to-business (B2B) and business-to-employee (B2E) situations, allowing companies to provide more timely information and functionality to business partners and mobile workforces in an increasingly efficient manner. A 2008 estimate put global revenue for mobile content management at US$8 billion.&lt;ref&gt;[http://www.wirelessweek.com/Content-Management-Systems-Mobile-Embrace.aspx Content Management Systems&#8217; Mobile Embrace, By Evan Koblentz, WirelessWeek, August 28, 2008]&lt;/ref&gt;

==Key features==

===Multi-channel content delivery===
Multi-channel content delivery capabilities allow users to manage a central content repository while simultaneously delivering that content to mobile devices such as mobile phones, smartphones, tablets and other mobile devices. Content can be stored in a raw format (such as Microsoft Word, Excel, PowerPoint, PDF, Text, HTML etc.) to which device-specific presentation styles can be applied.&lt;ref&gt;[http://www.apoorv.info/2007/05/26/content-management-for-mobile-delivery/ Content Management for Mobile Delivery, Posted by Apoorv, PCM.Blog, May 26, 2007]&lt;/ref&gt;

===Content access control===
Access control includes authorization, authentication, access approval to each content.  In many cases the access control also includes download control, wipe-out for specific user, time specific access.  For the authentication, MCM shall have basic authentication which has user ID and password.  For higher security many MCM supports IP authentication and mobile device authentication.

===Specialized templating system===
While traditional web content management systems handle templates for only a handful of web browsers, mobile CMS templates must be adapted to the very wide range of target devices with different capacities and limitations. There are two approaches to adapting templates: multi-client and multi-site. The multi-client approach makes it possible to see all versions of a site at the same domain (e.g. sitename.com), and templates are presented based on the device client used for viewing. The multi-site approach displays the mobile site on a targeted sub-domain (e.g. mobile.sitename.com).

===Location-based content delivery===
Location-based content delivery provides targeted content, such as information, advertisements, maps, directions, and news, to mobile devices based on current physical location. Currently, GPS (global positioning system)  navigation systems offer the most popular [[location-based service]]s. Navigation systems are specialized systems, but incorporating mobile phone functionality makes greater exploitation of location-aware content delivery possible.

==See also==
*[[Mobile Web]]
*[[Content management]]
*[[Web content management system]]
*[[Enterprise content management]]
*[[Apache Mobile Filter]]

==References==
&lt;references/&gt;

[[Category:Content management systems]]
[[Category:Mobile Web]]
[[Category:Data management]]</text>
      <sha1>bcqzb9s6rh0ifwqulck51jkfkmifxj4</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic warehousing</title>
    <ns>0</ns>
    <id>20420236</id>
    <revision>
      <id>730312823</id>
      <parentid>679779238</parentid>
      <timestamp>2016-07-18T06:21:02Z</timestamp>
      <contributor>
        <username>DocWatson42</username>
        <id>38455</id>
      </contributor>
      <minor />
      <comment>Made two minor corrections.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6202" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=November 2008}}
{{confusing|date=August 2009}}
{{cleanup-rewrite|date=August 2009}}
}}

In [[data management]],  '''semantic warehousing''' is a methodology of digitalized text data using similar functions to [[Data warehousing]] (DW), such as ETL([[Extract, transform, load]]), ODS([[Operational data store]]), and MODEL. [[Value (computer science)|Key value]] operation is less useful for the digitalized text. Semantic warehousing is different from DW in that semantic information base from text(semantic) data.

Semantic warehousing is different from search engine in that semantic information base from text data is stored in the database.([[DBMS]])

Though data is most important word in computing era, it can not explain human knowledge well yet.
Data(numeric data) is key element of computing systems for certain organization (especially companies, enterprises), but no performance oriented organization needs something to gather and use knowledge or human feeling.
Semantic warehousing will be equally or more important than data warehousing in the future.

==Definition==
Semantic warehousing is a conceptual and functional term meaning to gather from a source, semantically defining and providing information from digitalized text type data.

==Background==

Data warehousing (DW) is popular these days. Gathering data from systems that generate transactions, data warehouses become a base of [[information]]. Key of data warehouse is a model (called [[datamart]]) and that model is made up of dimensions(key) and measures(value). Users get information from the models by doing certain operations. [[Online analytical processing]] (OLAP) is most the important operation for the users to get information from the DW models. Handling dimensions with pivoting, drilling, slice &amp; dice operations users get numeric values like sales amounts, growth rates, etc.
Various areas of this world defined and appeared on the World Wide Web(Internet), eager to present their contents in a semantic way. 
Briefly speaking semantic warehousing has datawarehousing boby and search head and ontology features.

Data warehousing contributed to companies' business values and lots of solutions and tools are commercially successful. Analysis of internal data delivers a certain level of business values, on the contrary to this Semantic warehousing environment has not yet matured. Capacity of social data is increasing rapidly and various efforts of finding value from that data are made widely known as Big data, etc. Semantic warehousing can be the mainstream of treat data and intelligence of social world in the future though it is defined with other keywords.

At the Big data era, semantic processing is going to become major IT process. Semantic warehousing is digital infra of Intelligence.

== Practices ==

'''&#9635; Medical area (Clinical Information)'''

Some hospital implement semantic warehousing for [[clinic]]al information (SWCI). Medical information is now knowledge network level. [[UMLS]] define semantic knowledge network of medical language. Currently medical information stored in database and not fully used for clinic. Semantic warehousing is next stage of digitalized medical information.

SWCI is a name of conceptual system of clinical information.&lt;br /&gt;
Named by Juhan Kim (SNUH, [[Seoul National University Hospital]]) and Bohyon Hwang, YongChan Keum in 2008.

Defined architecture on SWCI ;&lt;br /&gt;
1. Semantic-oriented cleansing&lt;br /&gt;
2. Semantic-oriented meta management&lt;br /&gt;
3. Clinical(Medical) knowledge basement&lt;br /&gt;
4. Semantic-oriented user intelligence

'''&#9635; Intelligence Area'''

At the point of Big data usage, intelligence reporting can be valuable results.

1. Source information&lt;br /&gt;
2. Manage intelligence &amp; Semantic data&lt;br /&gt;
3. Intelligence service &amp; use

http://www.globalintelligence.kr/gibigdata/

== Connected area ==

- [[Big data]] &lt;br /&gt;
- [[Semantic web]] &lt;br /&gt;
- [[Ontology]] &lt;br /&gt;
- [[Knowledge]] &lt;br /&gt;
- [[Medical]] and [[healthcare]] : EMR [[Electronic medical record|(Electronic Medical Record)]], EHR [[Electronic health record|(Electronic Health Record)]]&lt;br /&gt;
- [[Data warehouse]] &lt;br /&gt;
- AI ([[artificial intelligence]])

== References ==
*[http://www.snubi.org/ '''BI''' Laboratory of Seoul National University Hospital]
*Smith, Barry Kumar, Anand and Schulze-Kremer, Steffen (2004) [http://ontology.buffalo.edu/medo/UMLS_SN.pdf Revising the UMLS Semantic Network], in M. Fieschi, et al. (eds.), Medinfo 2004, Amsterdam: IOS Press, 1700.
* Foundations of Data Warehouse Quality :
  Data Quality article mentioning that semantically rich DW.
  http://www.cs.brown.edu/courses/cs227/Papers/Projects/iq97_dwq.pdf
* An Integrative and Uniform Model for Metadata Management in Data Warehousing Environment.
  Semantic metadata and technical metadata.
  http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-19/paper12.pdf

* Effective Query Expansion using Condensed UMLS Metathesaurus for Medical Information Retrieval
http://www.e-hir.org/journal/view.html?uid=201&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

* A Study of Effective Unified Medical Language System Concept Indexing in Radiology Reports
http://www.e-hir.org/journal/view.html?uid=226&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

* Developing a Reference Terminology Model for Health Care Using an Object-Oriented Approach
http://www.e-hir.org/journal/view.html?uid=311&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

* UMLS(Unified Medical Language System)&#51032; &#51613;&#49345;&#50857;&#50612;&#50752; &#44397;&#45236;&#51032;&#47924;&#44592;&#47197;&#50640;&#49436; &#49324;&#50857;&#46104;&#45716; &#51613;&#49345;&#50857;&#50612;&#50752;&#51032; &#48708;&#44368;&#50672;&#44396;
http://www.e-hir.org/journal/view.html?uid=922&amp;start=&amp;sort=&amp;scale=&amp;key=all&amp;oper=&amp;key_word=UMLS&amp;year1=&amp;year2=&amp;Vol=&amp;Num=&amp;PG=&amp;book=&amp;mod=vol&amp;sflag=&amp;sub_box=Y&amp;aut_box=Y&amp;sos_box=&amp;pub_box=Y&amp;key_box=&amp;abs_box=&amp;year=

[[Category:Data management]]</text>
      <sha1>4r2u156v40k16ohs9jx8aipmkx1yu2n</sha1>
    </revision>
  </page>
  <page>
    <title>Data dictionary</title>
    <ns>0</ns>
    <id>645139</id>
    <revision>
      <id>761240570</id>
      <parentid>761234121</parentid>
      <timestamp>2017-01-21T21:00:52Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <comment>rm promotional link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8421" xml:space="preserve">{{Distinguish|Dictionary (data structure)}}
{{Use dmy dates|date=July 2013}}
A '''data dictionary''', or [[metadata repository]], as defined in the ''IBM Dictionary of Computing'', is a "centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format."&lt;ref&gt;ACM, [http://portal.acm.org/citation.cfm?id=541721 IBM Dictionary of Computing], 10th edition, 1993&lt;/ref&gt; The term can have one of several closely related meanings pertaining to [[database]]s and [[database management system]]s (DBMS):

* A [[document]] describing a database or collection of databases
* An integral [[software component|component]] of a [[Database management system|DBMS]] that is required to determine its structure
* A piece of [[middleware]] that extends or supplants the native data dictionary of a DBMS

==Documentation==
The terms ''data dictionary'' and ''data repository'' indicate a more general software utility than a catalogue. A ''catalogue'' is closely coupled with the DBMS software. It provides the information stored in it to the user and the DBA, but it is mainly accessed by the various software modules of the DBMS itself, such as [[Data definition language|DDL]] and [[Data manipulation language|DML]] compilers, the query optimiser, the transaction processor, report generators, and the constraint enforcer. On the other hand, a ''data dictionary'' is a data structure that stores [[metadata]], i.e., (structured) data about information. The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. These systems maintain information on system hardware and software configuration, documentation, application and users as well as other information relevant to system administration.&lt;ref&gt;Ramez Elmasri, Shamkant B. Navathe: ''Fundamentals of Database Systems'', 3rd. ed. sect. 17.5, p. 582&lt;/ref&gt;

If a data dictionary system is used only by the designers, users, and administrators and not by the DBMS Software, it is called a ''passive data dictionary.'' Otherwise, it is called an ''active data dictionary'' or ''data dictionary.''  When a passive data dictionary is updated, it is done so manually and independently from any changes to a DBMS (database) structure. With an active data dictionary, the dictionary is updated first and changes occur in the DBMS automatically as a result.

Database [[User (computing)|users]] and [[Application software|application]] developers can benefit from an authoritative data dictionary document that catalogs the organization, contents, and conventions of one or more databases.&lt;ref&gt;TechTarget, ''SearchSOA'', [http://searchsoa.techtarget.com/sDefinition/0,,sid26_gci211896,00.html What is a data dictionary?]&lt;/ref&gt; This typically includes the names and descriptions of various [[Table (database)|tables]] (records or Entities) and their contents ([[Column (database)|fields]]) plus additional details, like the [[Data type|type]] and length of each [[data element]].  Another important piece of information that a data dictionary can provide is the relationship between Tables.  This is sometimes referred to in Entity-Relationship diagrams, or if using Set descriptors, identifying which Sets database Tables participate in.

In an active data dictionary constraints may be placed upon the underlying data.  For instance, a Range may be imposed on the value of numeric data in a data element (field), or a Record in a Table may be FORCED to participate in a set relationship with another Record-Type.  Additionally, a distributed DBMS may have certain location specifics described within its active data dictionary (e.g. where Tables are physically located).

The data dictionary consists of record types (tables) created in the database by systems generated command files, tailored for each supported back-end DBMS. Command files contain SQL Statements for CREATE TABLE, CREATE UNIQUE INDEX, ALTER TABLE (for referential integrity), etc., using the specific statement required by that type of database.

There is no universal standard as to the level of detail in such a document.

==Middleware==
In the construction of database applications, it can be useful to introduce an additional layer of data dictionary software, i.e. [[middleware]], which communicates with the underlying DBMS data dictionary. Such a "high-level" data dictionary may offer additional features and a degree of flexibility that goes beyond the limitations of the native "low-level" data dictionary, whose primary purpose is to support the basic functions of the DBMS, not the requirements of a typical application. For example, a high-level data dictionary can provide alternative [[entity-relationship model]]s tailored to suit different applications that share a common database.&lt;ref&gt;U.S. Patent 4774661, [http://www.freepatentsonline.com/4774661.html Database management system with active data dictionary], 19 November 1985, AT&amp;T&lt;/ref&gt; Extensions to the data dictionary also can assist in [[query optimization]] against [[distributed database]]s.&lt;ref&gt;U.S. Patent 4769772, [http://www.freepatentsonline.com/4769772.html Automated query optimization method using both global and parallel local optimizations for materialization access planning for distributed databases], 28 February 1985, Honeywell Bull&lt;/ref&gt;  Additionally, DBA functions are often automated using restructuring tools that are tightly coupled to an active data dictionary.

[[Software framework]]s aimed at [[rapid application development]] sometimes include high-level data dictionary facilities, which can substantially reduce the amount of programming required to build [[Menu (computing)|menus]], [[Form (programming)|forms]], reports, and other components of a database application, including the database itself. For example, PHPLens includes a [[PHP]] [[class library]] to automate the creation of tables, indexes, and [[foreign key]] constraints [[Portability (software)|portably]] for multiple databases.&lt;ref&gt;PHPLens, [http://phplens.com/lens/adodb/docs-datadict.htm ADOdb Data Dictionary Library for PHP]&lt;/ref&gt; Another PHP-based data dictionary, part of the RADICORE toolkit, automatically generates program [[Object (computer science)|objects]], [[Scripting language|scripts]], and SQL code for menus and forms with [[data validation]] and complex [[join (SQL)|joins]].&lt;ref&gt;RADICORE, [http://www.radicore.org/viewarticle.php?article_id=5 What is a Data Dictionary?]&lt;/ref&gt; For the [[ASP.NET]] environment, [[Base One International|Base One's]] data dictionary provides cross-DBMS facilities for automated database creation, data validation, performance enhancement ([[Cache (computing)|caching]] and index utilization), [[application security]], and extended [[data type]]s.&lt;ref&gt;Base One International Corp., [http://www.boic.com/b1ddic.htm Base One Data Dictionary]&lt;/ref&gt;  [[Visual DataFlex]] features&lt;ref&gt;VISUAL DATAFLEX,[http://www.visualdataflex.com/features.asp?pageid=1030 features]&lt;/ref&gt; provides the ability to use DataDictionaries as class files to form  middle layer between the user interface and the underlying database.   The intent is to create standardized rules to maintain data integrity and enforce business rules throughout one or more related applications.

==Platform-specific examples==
Developers use a ''data description specification'' (''DDS'') to describe data attributes in file descriptions that are external to the application program that processes the data, in the context of an [[IBM System i]].&lt;ref&gt;{{cite web |url=http://publib.boulder.ibm.com/infocenter/iseries/v5r3/topic/dds/rbafpddsmain.htm |title=DDS documentation for IBM System i V5R3}}&lt;/ref&gt;

==See also==
*[[Data hierarchy]]
*[[Data modeling]]
*[[Database schema]]
*[[ISO/IEC 11179]]
*[[Metadata registry]]
*[[Semantic spectrum]]
*[[Vocabulary OneSource]]
*[[Metadata repository]]

==References==
{{Reflist|30em}}

==External links==
{{Commons category|Data dictionary}}
*Yourdon, ''Structured Analysis Wiki'', [http://yourdon.com/strucanalysis/wiki/index.php?title=Chapter_10 Data Dictionaries]

{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Data Dictionary}}
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Knowledge representation]]
[[Category:Metadata]]</text>
      <sha1>92h9onkaqwf4ywyd2dggc90kjiema96</sha1>
    </revision>
  </page>
  <page>
    <title>Flat file database</title>
    <ns>0</ns>
    <id>573973</id>
    <revision>
      <id>753671249</id>
      <parentid>749961521</parentid>
      <timestamp>2016-12-08T15:47:12Z</timestamp>
      <contributor>
        <ip>165.225.80.120</ip>
      </contributor>
      <comment>/* Example database */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11669" xml:space="preserve">{{distinguish|Flat file system}}
{{refimprove|date=March 2015}}
{{originalresearch|date=March 2015}}

[[Image:Flat File Model.svg|thumb|280px|Example of a flat file model&lt;ref name="USDT01"&gt;[http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf Data Integration Glossary] {{webarchive |url=https://web.archive.org/web/20090320001015/http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf |date=March 20, 2009 }}, U.S. Department of Transportation, August 2001.&lt;/ref&gt;]]

A '''flat file database''' is a [[database]] which is stored on its host computer system as an ordinary unstructured file called a "flat file". To access the structure of the data and manipulate it, the file must be read in its entirety into the computer's memory. Upon completion of the database operations, the file is again written out in its entirety to the host's file system. In this stored mode the database is said to be "flat", meaning that it has no structure for indexing and there are usually no structural relationships between the records. A flat file can be a [[plain text]] file or a [[binary file]].

The term has generally implied a small, simple database. As computer memory has become cheaper, more sophisticated databases can now be entirely held in memory for faster access. These newer databases would not generally be referred to as flat-file databases.

==Overview==

Plain text files usually contain one [[Record (computer science)|record]] per line,&lt;ref&gt;{{Citation
 | last = Fowler
 | first = Glenn
 | year = 1994
 | title = cql: Flat file database query language
 | periodical = WTEC'94: Proceedings of the USENIX Winter 1994 Technical Conference on USENIX Winter 1994 Technical Conference
 | url = http://www.research.att.com/~astopen/publications/cql-1994.pdf
}}&lt;/ref&gt; There are different conventions for depicting data. In [[comma-separated values]] and [[delimiter-separated values]] files, [[field (computer science)|field]]s can be separated by [[delimiters]] such as [[Comma-separated values|comma]] or [[Tab separated values|tab]] characters. In other cases, each field may have a fixed length; short values may be padded with [[space character]]s. Extra formatting may be needed to avoid [[delimiter collision]]. More complex solutions are [[markup language]]s and [[programming language]]s.

Using delimiters incurs some [[Computational overhead|overhead]] in locating them every time they are processed (unlike fixed-width formatting), which may have [[Computer performance|performance]] implications. However, use of character delimiters (especially commas) is also a crude form of [[data compression]] which may assist overall performance by reducing data volumes&amp;nbsp;&#8212; especially for [[data transmission]] purposes. Use of character delimiters which include a length component ([[String literal#Declarative notation|Declarative notation]]) is comparatively rare but vastly reduces the overhead associated with locating the extent of each field.

Typical examples of flat files are &lt;code&gt;[[/etc/passwd]]&lt;/code&gt; and &lt;code&gt;[[/etc/group]]&lt;/code&gt; on [[Unix-like]] operating systems. Another example of a flat file is a name-and-address list with the fields ''Name'', ''Address'', and ''Phone Number''.

A list of names, addresses, and phone numbers written by hand on a sheet of paper is a flat file database. This can also be done with any [[typewriter]] or [[word processor]]. A [[spreadsheet]] or [[text editor]] program may be used to implement a flat file database, which may then be printed or used [[online]] for improved search capabilities.

==History==
[[Herman Hollerith]] conceived the idea that data could be represented by holes punched in paper cards then tabulated by machine. He implemented this concept for the [[United States Census Bureau|US Census Bureau]]; thus the [[1890 United States Census]] processing created the first database&#8212;consisting of thousands of boxes full of [[punched card]]s.

Hollerith's enterprise grew into the computer giant [[IBM]], which dominated the data processing market for most of the 20th century. IBM's fixed-length field, 80-column punch cards became the ubiquitous means of inputting  electronic data until the 1970s.

In the 1980s, configurable flat-file database [[computer application]]s were popular on [[DOS]] and the [[Apple Macintosh|Macintosh]]. These programs were designed to make it easy for individuals to design and use their own databases, and were almost on par with [[word processors]] and [[spreadsheet]]s in popularity.{{citation needed|date=September 2011}} Examples of flat-file database products were early versions of [[FileMaker]] and the [[shareware]] [[PC-File]].  Some of these, like [[dBase II]], offered limited [[relational database|relational]] capabilities, allowing some data to be shared between files.

In the 2010s flat file databases were used in [[content management system]]s. Instead of using a database, web developers were able to change the content directly in the file system or at the command line.

===Contemporary implementations===
FairCom's [[c-tree]] is an example of a modern enterprise-level solution, and [[spreadsheet]] software and [[text editor]]s can be used for this purpose.  [[WebDNA]] is a scripting language designed for the World Wide Web, with a hybrid flat file in-memory database system making it easy to build resilient database-driven websites. With the in-memory concept, WebDNA searches and database updates are almost realtime while the data is stored as text files within the website itself. Otherwise, flat file database is implemented in [[Microsoft Works]] and [[Apple Works]]. Over time, products like [[Borland]]'s Paradox, and [[Microsoft]]'s [[Microsoft Access|Access]] started offering some relational capabilities, as well as built-in programming languages.  Database Management Systems ([[DBMS]]) like [[MySQL]] or [[Oracle database|Oracle]] generally require programmers to build applications.

Faceless flat file database engines are used internally by [[Mac OS X]], [[Firefox]], and other computer software to store configuration data. Programs to manage collections of books or appointments and [[address book]] are essentially single-purpose flat file database applications, allowing users to store and retrieve information from flat files using a predefined set of fields.

==Data transfer operations==
Flat files are used not only as data storage tools in DB and [[Content_management_system|CMS]] systems, but also as data transfer tools to remote servers (in which case they become known as information streams).

In recent years, this latter implementation has been replaced with [[XML]] files, which not only contain but also describe the data.  Those still using flat files to transfer information are mainframes employing specific procedures which are too expensive to modify.

One criticism often raised against the XML format as a way to perform mass data transfer operations is that file size is significantly larger than that of flat files, which is generally reduced to the bare minimum.  The solution to this problem consists in XML file compression (a solution that applies equally well to flat files), which has nowadays gained [[Efficient XML Interchange|EXI]] standards (i.e., Efficient XML Interchange, which is often used by mobile devices).

It is advisable that transfer data be performed via EXI rather than flat files because defining the compression method is not required, because libraries reading the file contents are readily available, and because there is no need for the two communicating systems to preliminarily establish a protocol describing data properties such as position, alignment, type, and format. However, in those circumstances where the sheer mass of data and/or the inadequacy of legacy systems becomes a problem, the only viable solution remains the use of flat files.  In order to successfully handle those problems connected with data communication, format, validation, control and much else (be it a flat file or an XML file data source), it is advisable to adopt a [[Data Quality Firewall]].

==Terminology==
"Flat file database" may be defined very narrowly, or more broadly. The narrower interpretation is correct in [[Database|database theory]]; the broader covers the term as generally used.

Strictly, a flat file database should consist of nothing but data and, if records vary in length, delimiters. More broadly, the term refers to any database which exists in a single file in the form of rows and columns, with no relationships or links between records and fields except the table structure.

Terms used to describe different aspects of a database and its tools differ from one implementation to the next, but the concepts remain the same. FileMaker uses the term "Find", while MySQL uses the term "Query"; but the concept is the same. FileMaker "files", in version 7 and above, are equivalent to MySQL "databases", and so forth. To avoid confusing the reader, one consistent set of terms is used throughout this article.

However, the basic terms "record" and "field" are used in nearly every flat file database implementation.

==Example database==
The following example illustrates the basic elements of a flat-file database. The [[data]] arrangement consists of a series of columns and rows organized into a [[table (information)|tabular format]]. This specific example uses only one table.

The columns include: ''name'' (a person's name, second column); ''team'' (the name of an athletic team supported by the person, third column); and a numeric ''unique ID'', (used to uniquely identify records, first column).

Here is an example textual representation of the described data:

 id    name    team
 1     Amy     Blues
 2     Bob     Reds
 3     Chuck   Blues
 4     Richard Blues
 5     Ethel   Reds
 6     Fred    Blues
 7     Gilly   Blues
 8     Hank    Reds
 9     Hank    Blues

This type of data representation is quite standard for a flat-file database, although there are some additional considerations that are not readily apparent from the text:
* '''Data types:''' each column in a database table such as the one above is ordinarily restricted to a specific [[data type]]. Such restrictions are usually established by convention, but not formally indicated unless the data is transferred to a [[relational database]] system.
* '''Separated columns:''' In the above example, individual columns are separated using [[Whitespace (computer science)|whitespace]] characters. This is also called indentation or "fixed-width" data formatting. Another common convention is to separate columns using one or more [[delimiter]] characters. More complex solutions are markup and programming languages.
* '''Relational algebra:''' Each row or record in the above table meets the standard definition of a [[tuple]] under [[relational algebra]] (the above example depicts a series of 3-tuples). Additionally, the first row specifies the [[Tuple#Field_names|field names]] that are associated with the values of each row.
* '''Database management system:''' Since the formal operations possible with a text file are usually more limited than desired, the text in the above example would ordinarily represent an intermediary state of the data prior to being transferred into a [[database management system]].

==References==
{{Commons category|Flat file models}}
{{reflist}}

{{Database models}}

{{DEFAULTSORT:Flat File Database}}
[[Category:Data management]]
[[Category:Computer file formats]]
[[Category:Database models]]

[[it:Flat file]]</text>
      <sha1>nawn915xvjwnc5p7ebcbi2483pe1eu1</sha1>
    </revision>
  </page>
  <page>
    <title>Data center</title>
    <ns>0</ns>
    <id>579730</id>
    <revision>
      <id>762095565</id>
      <parentid>759809129</parentid>
      <timestamp>2017-01-26T16:40:40Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="70494" xml:space="preserve">{{Refimprove|date=July 2015}}
[[File:NetworkOperations.jpg|thumb|right|An operation engineer overseeing a network operations control room of a data center]]

A '''data center''' is a facility used to house computer systems and associated components, such as [[telecommunication]]s and [[computer data storage|storage systems]]. It generally includes redundant or backup [[power supply|power supplies]], redundant data communications connections, environmental controls (e.g., air conditioning, fire suppression) and various security devices. Large data centers are industrial scale operations using as much electricity as a small town.&lt;ref name=NYT92212&gt;{{cite news|title=Power, Pollution and the Internet|url=http://www.nytimes.com/2012/09/23/technology/data-centers-waste-vast-amounts-of-energy-belying-industry-image.html|accessdate=2012-09-25|newspaper=The New York Times|date=September 22, 2012|author=James Glanz}}&lt;/ref&gt;&lt;ref name="ReferenceDC2"&gt;"[http://www.academia.edu/6982393/Power_Management_Techniques_for_Data_Centers_A_Survey Power Management Techniques for Data Centers: A Survey]", 2014.&lt;/ref&gt;

==History==
[[File:Indiana University Data Center - P1100134.JPG|thumb|[[Indiana University]] Data Center. [[Bloomington, Indiana]]]]
{{Copypaste|section|url=http://www.rackspace.com/blog/datacenter-evolution-1960-to-2000/|date=August 2014}}
{{Unreferenced section|date=August 2014}}
Data centers have their roots in the huge computer rooms of the early ages{{when|date=September 2015}} of the computing industry. Early computer systems, complex to operate and maintain, required a special environment in which to operate. Many cables were necessary to connect all the components, and methods to accommodate and organize these were devised such as standard [[19-inch rack|racks]] to mount equipment, [[raised floor]]s, and [[cable tray]]s (installed overhead or under the elevated floor). A single mainframe required a great deal of power, and had to be cooled to avoid overheating. Security became important&amp;nbsp;&#8211; computers were expensive, and were often used for [[military]] purposes. Basic design-guidelines for controlling access to the computer room were therefore devised.

During the boom of the microcomputer industry, and especially during the 1980s, users started to deploy computers everywhere, in many cases with little or no care about operating requirements. However, as information technology (IT) operations started to grow in complexity, organizations grew aware of the need to control IT resources. The advent of [[Unix]] from the early 1970s led to the subsequent proliferation of freely available [[Linux]]-compatible [[personal computer|PC]] operating-systems during the 1990s. These were called "[[Server (computing)|servers]]", as [[timesharing]] operating systems like Unix rely heavily on the [[client-server model]] to facilitate sharing unique resources between multiple users. The availability of inexpensive [[Networking hardware|networking]] equipment, coupled with new standards for network [[structured cabling]], made it possible to use a hierarchical design that put the servers in a specific room inside the company. The use of the term "data center", as applied to specially designed computer rooms, started to gain popular recognition about this time.{{citation needed|date=September 2015}}

The boom of data centers came during the [[dot-com bubble]] of 1997&#8211;2000. [[Company|Companies]] needed fast Internet connectivity and non-stop operation to deploy systems and to establish a presence on the Internet. Installing such equipment was not viable for many smaller companies. Many companies started building very large facilities, called '''Internet data centers''' (IDCs), which provide [[customer|commercial client]]s with a range of solutions for systems deployment and operation. New technologies and practices were designed to handle the scale and the operational requirements of such large-scale operations. These practices eventually migrated toward the private data centers, and were adopted largely because of their practical results. Data centers for cloud computing are called '''cloud data centers''' (CDCs). But nowadays, the division of these terms has almost disappeared and they are being integrated into a term "data center".

With an increase in the uptake of [[cloud computing]], business and government organizations scrutinize data centers to a higher degree in areas such as security, availability, environmental impact and adherence to standards. Standards documents from accredited [[professional]] groups, such as the [[Telecommunications Industry Association]], specify the requirements for data-center design. Well-known operational metrics for [[data availability|data-center availability]] can serve to evaluate the [[Business Impact Analysis|commercial impact]] of a disruption. Development continues in operational practice, and also in environmentally-friendly data-center design. Data centers typically cost a lot to build and to maintain.{{citation needed|date=September 2015}}

==Requirements for modern data centers==
[[File:Datacenter-telecom.jpg|thumb|left|Racks of telecommunications equipment in part of a data center]]
{{Copypaste|section|url=https://global.ihs.com/doc_detail.cfm?&amp;rid=TIA&amp;input_doc_number=TIA-942&amp;item_s_key=00414811&amp;item_key_date=860905&amp;input_doc_number=TIA-942&amp;input_doc_title=#abstract|date=August 2014}}
IT operations are a crucial aspect of most organizational operations around the world. One of the main concerns is '''business continuity'''; companies rely on their information systems to run their operations. If a system becomes unavailable, company operations may be impaired or stopped completely. It is necessary to provide a reliable infrastructure for IT operations, in order to minimize any chance of disruption. Information security is also a concern, and for this reason a data center has to offer a secure environment which minimizes the chances of a security breach. A data center must therefore keep high standards for assuring the integrity and functionality of its hosted computer environment. This is accomplished through redundancy of mechanical cooling and power systems (including emergency backup power generators)serving the data center along with fiber optic cables.

The [[Telecommunications Industry Association]]'s Telecommunications Infrastructure Standard for Data Centers&lt;ref&gt;[http://www.tia-942.org TIA-942 Telecommunications Infrastructure Standard for Data Centers]&lt;/ref&gt; specifies the minimum requirements for telecommunications infrastructure of data centers and computer rooms including single tenant enterprise data centers and multi-tenant Internet hosting data centers. The topology proposed in this document is intended to be applicable to any size data center.&lt;ref&gt;{{cite web|url=http://www.tiaonline.org/standards/ |title=Archived copy |accessdate=2011-11-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20111106042758/http://www.tiaonline.org/standards/ |archivedate=2011-11-06 |df= }}&lt;/ref&gt;

Telcordia GR-3160, ''NEBS Requirements for Telecommunications Data Center Equipment and Spaces'',&lt;ref&gt;[http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&amp;DOCUMENT=GR-3160&amp; GR-3160, NEBS Requirements for Telecommunications Data Center Equipment and Spaces]&lt;/ref&gt; provides guidelines for data center spaces within telecommunications networks, and environmental requirements for the equipment intended for installation in those spaces. These criteria were developed jointly by Telcordia and industry representatives. They may be applied to data center spaces housing data processing or Information Technology (IT) equipment. The equipment may be used to:
* Operate and manage a carrier's telecommunication network
* Provide data center based applications directly to the carrier's customers
* Provide hosted applications for a third party to provide services to their customers
* Provide a combination of these and similar data center applications

Effective data center operation requires a balanced investment in both the facility and the housed equipment. The first step is to establish a baseline facility environment suitable for equipment installation. Standardization and modularity can yield savings and efficiencies in the design and construction of telecommunications data centers.

Standardization means integrated building and equipment engineering. Modularity has the benefits of scalability and easier growth, even when planning forecasts are less than optimal. For these reasons, telecommunications data centers should be planned in repetitive building blocks of equipment, and associated power and support (conditioning) equipment when practical. The use of dedicated centralized systems requires more accurate forecasts of future needs to prevent expensive over construction, or perhaps worse&amp;nbsp;&#8212; under construction that fails to meet future needs.

The "lights-out" data center, also known as a darkened or a [[dark data]] center, is a data center that, ideally, has all but eliminated the need for direct access by personnel, except under extraordinary circumstances. Because of the lack of need for staff to enter the data center, it can be operated without lighting. All of the devices are accessed and managed by remote systems, with automation programs used to perform unattended operations. In addition to the energy savings, reduction in staffing costs and the ability to locate the site further from population centers, implementing a lights-out data center reduces the threat of malicious attacks upon the infrastructure.&lt;ref&gt;{{cite book | first=Victor | last=Kasacavage | year=2002 | page=227 | title=Complete book of remote access: connectivity and security | series=The Auerbach Best Practices Series | publisher=CRC Press | isbn=0-8493-1253-1
}}&lt;/ref&gt;&lt;ref&gt;{{cite book |author1=Burkey, Roxanne E. |author2=Breakfield, Charles V. | year=2000 | title=Designing a total data solution: technology, implementation and deployment | page=24 | series=Auerbach Best Practices | publisher=CRC Press | isbn=0-8493-0893-3 }}&lt;/ref&gt;

There is a trend to modernize data centers in order to take advantage of the performance and energy efficiency increases of newer IT equipment and capabilities, such as [[cloud computing]]. This process is also known as data center transformation.&lt;ref name="mspmentor.net"&gt;Mukhar, Nicholas. "HP Updates Data Center Transformation Solutions," August 17, 2011 [http://www.mspmentor.net/2011/08/17/hp-updates-data-transformation-solutions/]&lt;/ref&gt;

Organizations are experiencing rapid IT growth but their data centers are aging. Industry research company [[International Data Corporation]] (IDC) puts the average age of a data center at nine years old.&lt;ref name="mspmentor.net"/&gt; [[Gartner]], another research company says data centers older than seven years are obsolete.&lt;ref&gt;{{cite web|url=http://www.forbes.com/2010/03/12/cloud-computing-ibm-technology-cio-network-data-centers.html |title=Sperling, Ed. "Next-Generation Data Centers," Forbes, March 15. 2010 |publisher=Forbes.com |date= |accessdate=2013-08-30}}&lt;/ref&gt;

In May 2011, data center research organization [[Uptime Institute]] reported that 36 percent of the large companies it surveyed expect to exhaust IT capacity within the next 18 months.&lt;ref&gt;Niccolai, James. "Data Centers Turn to Outsourcing to Meet Capacity Needs," CIO.com, May 10, 2011 [http://www.cio.com/article/681897/Data_Centers_Turn_to_Outsourcing_to_Meet_Capacity_Needs]&lt;/ref&gt;

Data center transformation takes a step-by-step approach through integrated projects carried out over time. This differs from a traditional method of data center upgrades that takes a serial and siloed approach.&lt;ref&gt;Tang, Helen. "Three Signs it's time to transform your data center," August 3, 2010, Data Center Knowledge [http://www.datacenterknowledge.com/archives/2010/08/03/three-signs-it%E2%80%99s-time-to-transform-your-data-center/]&lt;/ref&gt; The typical projects within a data center transformation initiative include standardization/consolidation, virtualization, [[automation]] and security.
* Standardization/consolidation: The purpose of this project is to reduce the number of data centers a large organization may have. This project also helps to reduce the number of hardware, software platforms, tools and processes within a data center. Organizations replace aging data center equipment with newer ones that provide increased capacity and performance. Computing, networking and management platforms are standardized so they are easier to manage.&lt;ref name="datacenterknowledge.com"&gt;Miller, Rich. "Complexity: Growing Data Center Challenge," Data Center Knowledge, May 16, 2007
[http://www.datacenterknowledge.com/archives/2007/05/16/complexity-growing-data-center-challenge/]&lt;/ref&gt;
* Virtualize: There is a trend to use IT virtualization technologies to replace or consolidate multiple data center equipment, such as servers. Virtualization helps to lower capital and operational expenses,&lt;ref&gt;Sims, David. "Carousel's Expert Walks Through Major Benefits of Virtualization," TMC Net, July 6, 2010
[http://virtualization.tmcnet.com/topics/virtualization/articles/193652-carousels-expert-walks-through-major-benefits-virtualization.htm]&lt;/ref&gt; and reduce energy consumption.&lt;ref&gt;Delahunty, Stephen. "The New urgency for Server Virtualization," InformationWeek, August 15, 2011. [http://www.informationweek.com/news/government/enterprise-architecture/231300585]&lt;/ref&gt; Virtualization technologies are also used to create virtual desktops, which can then be hosted in data centers and rented out on a subscription basis.&lt;ref&gt;{{cite web|title=HVD: the cloud's silver lining|url=http://www.intrinsictechnology.co.uk/FileUploads/HVD_Whitepaper.pdf|publisher=Intrinsic Technology|accessdate=2012-08-30}}&lt;/ref&gt;  Data released by investment bank Lazard Capital Markets reports that 48 percent of enterprise operations will be virtualized by 2012. Gartner views virtualization as a catalyst for modernization.&lt;ref&gt;Miller, Rich. "Gartner: Virtualization Disrupts Server Vendors," Data Center Knowledge, December 2, 2008 [http://www.datacenterknowledge.com/archives/2008/12/02/gartner-virtualization-disrupts-server-vendors/]&lt;/ref&gt;
* Automating: Data center automation involves automating tasks such as [[provisioning]], configuration, [[Patch (computing)|patching]], release management and compliance. As enterprises suffer from few skilled IT workers,&lt;ref name="datacenterknowledge.com"/&gt; automating tasks make data centers run more efficiently.
* Securing: In modern data centers, the security of data on virtual systems is integrated with existing security of physical infrastructures.&lt;ref&gt;Ritter, Ted. Nemertes Research, "Securing the Data-Center Transformation Aligning Security and Data-Center Dynamics," [http://lippisreport.com/2011/05/securing-the-data-center-transformation-aligning-security-and-data-center-dynamics/]&lt;/ref&gt; The security of a modern data center must take into account physical security, network security, and data and user security.

==Carrier neutrality==
Today many data centers are run by [[Internet service provider]]s solely for the purpose of hosting their own and third party [[Server (computing)|servers]].

However traditionally data centers were either built for the sole use of one large company, or as [[carrier hotel]]s or [[Network-neutral data center]]s.

These facilities enable interconnection of carriers and act as regional fiber hubs serving local business in addition to hosting content [[Server (computing)|servers]].

==Data center tiers==
&lt;!-- linked from [[data availability]] --&gt;
The [[Telecommunications Industry Association]] is a trade association accredited by ANSI (American National Standards Institute). In 2005 it published [http://global.ihs.com/doc_detail.cfm?currency_code=USD&amp;customer_id=2125452B2C0A&amp;oshid=2125452B2C0A&amp;shopping_cart_id=292558332C4A2020495A4D3B200A&amp;country_code=US&amp;lang_code=ENGL&amp;item_s_key=00414811&amp;item_key_date=940819&amp;input_doc_number=TIA-942&amp;input_doc_title= ANSI/TIA-942], Telecommunications Infrastructure Standard for Data Centers, which defined four levels (called tiers) of data centers in a thorough, quantifiable manner. TIA-942 was amended in 2008 and again in 2010. ''TIA-942:Data Center Standards Overview'' describes the requirements for the data center infrastructure. The simplest is a Tier 1 data center, which is basically a [[server room]], following basic guidelines for the installation of computer systems. The most stringent level is a Tier 4 data center, which is designed to host mission critical computer systems, with fully redundant subsystems and compartmentalized security zones controlled by [[biometric]] access controls methods. Another consideration is the placement of the data center in a subterranean context, for data security as well as environmental considerations such as cooling requirements.&lt;ref&gt;A ConnectKentucky article mentioning Stone Mountain Data Center Complex {{cite web|title=Global Data Corp. to Use Old Mine for Ultra-Secure Data Storage Facility|url=http://connectkentucky.org/_documents/connected_fall_FINAL.pdf|format=PDF|publisher=ConnectKentucky|accessdate=2007-11-01|date=2007-11-01}}&lt;/ref&gt;

The German Datacenter star audit program uses an auditing process to certify 5 levels of "gratification" that affect Data Center criticality.

Independent from the ANSI/TIA-942 standard, the [[Uptime Institute]], a think tank and professional-services organization based in [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]], has defined its own four levels. The levels describe the availability of data from the hardware at a location. The higher the tier, the greater the availability. The levels are:
&lt;ref&gt;A document from the Uptime Institute describing the different tiers (click through the download page) {{cite web
 |title=Data Center Site Infrastructure Tier Standard: Topology 
 |url=http://uptimeinstitute.org/index.php?option=com_docman&amp;task=doc_download&amp;gid=82 
 |format=PDF 
 |publisher=Uptime Institute 
 |accessdate=2010-02-13 
 |date=2010-02-13 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20100613072610/http://uptimeinstitute.org/index.php?option=com_docman&amp;task=doc_download&amp;gid=82 
 |archivedate=2010-06-13 
 |df= 
}}&lt;/ref&gt;
&lt;ref&gt;The rating guidelines from the Uptime Institute {{cite web
 |title=Data Center Site Infrastructure Tier Standard: Topology 
 |url=http://professionalservices.uptimeinstitute.com/UIPS_PDF/TierStandard.pdf 
 |format=PDF 
 |publisher=Uptime Institute 
 |accessdate=2010-02-13 
 |date=2010-02-13 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20091007121511/http://professionalservices.uptimeinstitute.com:80/UIPS_PDF/TierStandard.pdf 
 |archivedate=2009-10-07 
 |df= 
}}&lt;/ref&gt;

{| class="wikitable"
|-
! Tier Level
! Requirements
|-
! 1
|
* Single non-redundant distribution path serving the IT equipment
* Non-redundant capacity components
* Basic site infrastructure with expected availability of 99.671%
|-
! 2
|
* Meets or exceeds all Tier 1 requirements
* Redundant site infrastructure capacity components with expected availability of 99.741%
|-
! 3
|
* Meets or exceeds all Tier 2 requirements
* Multiple independent distribution paths serving the IT equipment
* All IT equipment must be dual-powered and fully compatible with the topology of a site's architecture
* Concurrently maintainable site infrastructure with expected availability of 99.982%
|-
! 4
|
* Meets or exceeds all Tier 3 requirements
* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems
* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%
|}

The difference between 99.671%, 99.741%, 99.982%, and 99.995%, while seemingly nominal, could be significant depending on the application.

Whilst no down-time is ideal, the tier system allows for unavailability of services as listed below over a period of one year (525,600 minutes):
* Tier 1 (99.671%) status would allow 1729.224 minutes or 28.817 hours
* Tier 2 (99.741%) status would allow 1361.304 minutes or 22.688 hours
* Tier 3 (99.982%) status would allow 94.608 minutes or 1.5768 hours
* Tier 4 (99.995%) status would allow 26.28 minutes or 0.438 hours

The Uptime Institute also classifies the tiers in different categories: design documents, constructed facility, operational sustainability&lt;ref name="uptimeinstitute"&gt;{{cite web|url=http://uptimeinstitute.com/TierCertification/|title=Uptime Institute - Tier Certification|publisher=uptimeinstitute.com|accessdate=2014-08-27}}&lt;/ref&gt;

==Design considerations==
[[File:Rack001.jpg|thumb|right|A typical server rack, commonly seen in [[colocation center|colocation]]]]
A data center can occupy one room of a building, one or more floors, or an entire building. Most of the equipment is often in the form of servers mounted in [[19 inch rack]] cabinets, which are usually placed in single rows forming corridors (so-called aisles) between them. This allows people access to the front and rear of each cabinet. Servers differ greatly in size from [[Rack unit|1U servers]] to large freestanding storage silos which occupy many square feet of floor space. Some equipment such as [[mainframe computer]]s and [[computer storage|storage]] devices are often as big as the racks themselves, and are placed alongside them. Very large data centers may use [[intermodal container|shipping containers]] packed with 1,000 or more servers each;&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=zRwPSFpLX8I|title=Google Container Datacenter Tour (video)}}&lt;/ref&gt; when repairs or upgrades are needed, whole containers are replaced (rather than repairing individual servers).&lt;ref&gt;{{cite web| title=Walking the talk: Microsoft builds first major container-based data center| url=http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9075519| archiveurl=https://web.archive.org/web/20080612193106/http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9075519| archivedate=2008-06-12| accessdate=2008-09-22}}&lt;/ref&gt;

Local building codes may govern the minimum ceiling heights.

===Design programming===
Design programming, also known as architectural programming, is the process of researching and making decisions to identify the scope of a design project.&lt;ref&gt;Cherry, Edith. "Architectural Programming: Introduction", Whole Building Design Guide, Sept. 2, 2009&lt;/ref&gt; Other than the architecture of the building itself there are three elements to design programming for data centers: facility topology design (space planning), engineering infrastructure design (mechanical systems such as cooling and electrical systems including power) and technology infrastructure design (cable plant). Each will be influenced by performance assessments and modelling to identify gaps pertaining to the owner's performance wishes of the facility over time.

Various vendors who provide data center design services define the steps of data center design slightly differently, but all address the same basic aspects as given below.

===Modeling criteria===
Modeling criteria are used to develop future-state scenarios for space, power, cooling, and costs in the data center.&lt;ref&gt;Mullins, Robert. "Romonet Offers Predictive Modelling Tool For Data Center Planning", Network Computing, June 29, 2011 [http://www.networkcomputing.com/data-center/231000669]&lt;/ref&gt; The aim is to create a master plan with parameters such as number, size, location, topology, IT floor system layouts, and power and cooling technology and configurations. The purpose of this is to allow for efficient use of the existing mechanical and electrical systems and also growth in the existing data center without the need for developing new buildings and further upgrading of incoming power supply.

===Design recommendations===
Design recommendations/plans generally follow the modelling criteria phase. The optimal technology infrastructure is identified and planning criteria are developed, such as critical power capacities, overall data center power requirements using an agreed upon PUE (power utilization efficiency), mechanical cooling capacities, kilowatts per cabinet, raised floor space, and the resiliency level for the facility.

===Conceptual design===
Conceptual designs embody the design recommendations or plans and should take into account "what-if" scenarios to ensure all operational outcomes are met in order to future-proof the facility. Conceptual floor layouts should be driven by IT performance requirements as well as lifecycle costs associated with IT demand, energy efficiency, cost efficiency and availability. Future-proofing will also include expansion capabilities, often provided in modern data centers through modular designs.  These allow for more raised floor space to be fitted out in the data center whilst utilising the existing major electrical plant of the facility.

===Detailed design===
Detailed design is undertaken once the appropriate conceptual design is determined, typically including a proof of concept. The detailed design phase should include the detailed architectural, structural, mechanical and electrical information and specification of the facility.  At this stage development of facility schematics and construction documents as well as schematics and performance specification and specific detailing of all technology infrastructure, detailed IT infrastructure design and IT infrastructure documentation are produced.

===Mechanical engineering infrastructure designs===
[[File:CRAC Cabinets 2.jpg|thumb|CRAC Air Handler]]
Mechanical engineering infrastructure design addresses mechanical systems involved in maintaining the interior environment of a data center, such as heating, ventilation and air conditioning (HVAC); humidification and dehumidification equipment; pressurization; and so on.&lt;ref name="nxtbook.com"&gt;Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today's Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 28. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]&lt;/ref&gt;
This stage of the design process should be aimed at saving space and costs, while ensuring business and reliability objectives are met as well as achieving PUE and green requirements.&lt;ref&gt;Data Center Energy Management: Best Practices Checklist: Mechanical, Lawrence Berkeley National Laboratory [http://hightech.lbl.gov/dctraining/strategies/mam.html]&lt;/ref&gt; Modern designs include modularizing and scaling IT loads, and making sure capital spending on the building construction is optimized.

===Electrical engineering infrastructure design===
Electrical Engineering infrastructure design is focused on designing electrical configurations that accommodate various reliability requirements and data center sizes. Aspects may include utility service planning; distribution, switching and bypass from power sources; uninterruptable power source (UPS) systems; and more.&lt;ref name="nxtbook.com"/&gt;

These designs should dovetail to energy standards and best practices while also meeting business objectives. Electrical configurations should be optimized and operationally compatible with the data center user's capabilities. Modern electrical design is modular and scalable,&lt;ref&gt;Clark, Jeff. "Hedging Your Data Center Power", The Data Center Journal, Oct. 5, 2011. [http://www.datacenterjournal.com/design/hedging-your-data-center-power/]&lt;/ref&gt; and is available for low and medium voltage requirements as well as DC (direct current).

===Technology infrastructure design===
[[File:Under Floor Cable Runs Tee.jpg|thumb|Under Floor Cable Runs]]
Technology infrastructure design addresses the telecommunications cabling systems that run throughout data centers. There are cabling systems for all data center environments, including horizontal cabling, voice, modem, and facsimile telecommunications services, premises switching equipment, computer and telecommunications management connections, keyboard/video/mouse connections and data communications.&lt;ref&gt;Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today's Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 30. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]&lt;/ref&gt; Wide area, local area, and storage area networks should link with other building signaling systems (e.g. fire, security, power, HVAC, EMS).

===Availability expectations===
The higher the availability needs of a data center, the higher the capital and operational costs of building and managing it. Business needs should dictate the level of availability required and should be evaluated based on characterization of the criticality of IT systems estimated cost analyses from modeled scenarios. In other words, how can an appropriate level of availability best be met by design criteria to avoid financial and operational risks as a result of downtime?
If the estimated cost of downtime within a specified time unit exceeds the amortized capital costs and operational expenses, a higher level of availability should be factored into the data center design. If the cost of avoiding downtime greatly exceeds the cost of downtime itself, a lower level of availability should be factored into the design.&lt;ref&gt;Clark, Jeffrey. "The Price of Data Center Availability&#8212;How much availability do you need?", Oct. 12, 2011, The Data Center Journal [http://www.datacenterjournal.com/home/news/languages/item/2792-the-price-of-data-center-availability]&lt;/ref&gt;

===Site selection===
Aspects such as proximity to available power grids, telecommunications infrastructure, networking services, transportation lines and emergency services can affect costs, risk, security and other factors to be taken into consideration for data center design.   Whilst a wide array of location factors are taken into account (e.g. flight paths, neighbouring uses, geological risks) access to suitable available power is often the longest lead time item. Location affects data center design also because the climatic conditions dictate what cooling technologies should be deployed. In turn this impacts uptime and the costs associated with cooling.&lt;ref&gt;Tucci, Linda. "Five tips on selecting a data center location", May 7, 2008, SearchCIO.com [http://searchcio.techtarget.com/news/1312614/Five-tips-on-selecting-a-data-center-location]&lt;/ref&gt; For example, the topology and the cost of managing a data center in a warm, humid climate will vary greatly from managing one in a cool, dry climate.

===Modularity and flexibility===
[[File:Cabinet Asile.jpg|thumb|Cabinet aisle in a data center]]
{{main article|Modular data center}}

Modularity and flexibility are key elements in allowing for a data center to grow and change over time. Data center modules are pre-engineered, standardized building blocks that can be easily configured and moved as needed.&lt;ref&gt;Niles, Susan. "Standardization and Modularity in Data Center Physical Infrastructure," 2011, Schneider Electric, page 4. [http://www.apcmedia.com/salestools/VAVR-626VPD_R1_EN.pdf]&lt;/ref&gt;

A modular data center may consist of data center equipment contained within shipping containers or similar portable containers.&lt;ref&gt;Pitchaikani, Bala. "Strategies for the Containerized Data Center," DataCenterKnowledge.com, Sept. 8, 2011. [http://www.datacenterknowledge.com/archives/2011/09/08/strategies-for-the-containerized-data-center/]&lt;/ref&gt; But it can also be described as a design style in which components of the data center are prefabricated and standardized so that they can be constructed, moved or added to quickly as needs change.&lt;ref&gt;Niccolai, James. "HP says prefab data center cuts costs in half," InfoWorld, July 27, 2010. [http://www.infoworld.com/d/green-it/hp-says-prefab-data-center-cuts-costs-in-half-837?page=0,0]&lt;/ref&gt;

===Environmental control===
{{main article|Data center environmental control}}
The physical environment of a data center is rigorously controlled.
[[Air conditioning]] is used to control the temperature and humidity in the data center. [[ASHRAE]]'s "Thermal Guidelines for Data Processing Environments"&lt;ref&gt;{{cite book|title=Thermal Guidelines for Data Processing Environments|year=2012|publisher=American Society of Heating, Refrigerating and Air-Conditioning Engineers|isbn=978-1936504-33-6|author=ASHRAE Technical Committee 9.9, Mission Critical Facilities, Technology Spaces and Electronic Equipment|edition=3}}&lt;/ref&gt; recommends a temperature range of {{convert|18|&#8211;|27|C|F}}, a dew point range of {{convert|5|&#8211;|15|C|F}}, and a relative humidity between 40% to 60% for data center environments.&lt;ref name=ServersCheck&gt;{{Cite web| title = Best Practices for data center monitoring and server room monitoring  | url=https://serverscheck.com/sensors/temperature_best_practices.asp | author = ServersCheck | accessdate = 2016-10-07}}&lt;/ref&gt;  The temperature in a data center will naturally rise because the electrical power used heats the air. Unless the heat is removed, the ambient temperature will rise, resulting in electronic equipment malfunction. By controlling the air temperature, the server components at the board level are kept within the manufacturer's specified temperature/humidity range. Air conditioning systems help control [[humidity]] by cooling the return space air below the [[dew point]]. Too much humidity, and water may begin to [[condensation|condense]] on internal components. In case of a dry atmosphere, ancillary humidification systems may add water vapor if the humidity is too low, which can result in [[electrostatics|static electricity]] discharge problems which may damage components. Subterranean data centers may keep computer equipment cool while expending less energy than conventional designs.

Modern data centers try to use economizer cooling, where they use outside air to keep the data center cool. At least one data center (located in [[Upstate New York]]) will cool servers using outside air during the winter. They do not use chillers/air conditioners, which creates potential energy savings in the millions.&lt;ref&gt;{{cite news| url=http://www.reuters.com/article/pressRelease/idUS141369+14-Sep-2009+PRN20090914 | work=Reuters | title=tw telecom and NYSERDA Announce Co-location Expansion | date=2009-09-14}}&lt;/ref&gt;  Increasingly [http://www.datacenterdynamics.com/focus/archive/2013/09/air-air-combat-indirect-air-cooling-wars-0 indirect air cooling] is being deployed in data centers globally which has the advantage of more efficient cooling which lowers power consumption costs in the data center.

Telcordia [http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&amp;DOCUMENT=GR-2930&amp; GR-2930, ''NEBS: Raised Floor Generic Requirements for Network and Data Centers''], presents generic engineering requirements for raised floors that fall within the strict NEBS guidelines.

There are many types of commercially available floors that offer a wide range of structural strength and loading capabilities, depending on component construction and the materials used. The general types of [[raised floor]]s include stringer, stringerless,  and structural platforms, all of which are discussed in detail in GR-2930 and summarized below.
* '''''Stringered raised floors''''' - This type of raised floor generally consists of a vertical array of steel pedestal assemblies (each assembly is made up of a steel base plate, tubular upright, and a head) uniformly spaced on two-foot centers and mechanically fastened to the concrete floor. The steel pedestal head has a stud that is inserted into the pedestal upright and the overall height is adjustable with a leveling nut on the welded stud of the pedestal head.
* '''''Stringerless raised floors''''' - One non-earthquake type of raised floor generally consists of an array of pedestals that provide the necessary height for routing cables and also serve to support each corner of the floor panels. With this type of floor, there may or may not be provisioning to mechanically fasten the floor panels to the pedestals. This stringerless type of system (having no mechanical attachments between the pedestal heads) provides maximum accessibility to the space under the floor. However, stringerless floors are significantly weaker than stringered raised floors in supporting lateral loads and are not recommended.
* '''''Structural platforms''''' - One type of structural platform consists of members constructed of steel angles or channels that are welded or bolted together to form an integrated platform for supporting equipment. This design permits equipment to be fastened directly to the platform without the need for toggle bars or supplemental bracing. Structural platforms may or may not contain panels or stringers.

Data centers typically have [[raised floor]]ing made up of {{convert|60|cm|ft|abbr=on|0}} removable square tiles. The trend is towards {{convert|80|-|100|cm|in|abbr=on}} void to cater for better and uniform air distribution. These provide a [[plenum space|plenum]] for air to circulate below the floor, as part of the air conditioning system, as well as providing space for power cabling.

====Metal whiskers====
Raised floors and other metal structures such as cable trays and ventilation ducts have caused many problems with [[zinc whiskers]] in the past, and likely are still present in many data centers. This happens when microscopic metallic filaments form on metals such as zinc or tin that protect many metal structures and electronic components from corrosion. Maintenance on a raised floor or installing of cable etc. can dislodge the whiskers, which enter the airflow and may short circuit server components or power supplies, sometimes through a high current metal vapor [[plasma arc]]. This phenomenon is not unique to data centers, and has also caused catastrophic failures of satellites and military hardware.&lt;ref&gt;{{cite web|title=NASA - metal whiskers research|url=http://nepp.nasa.gov/whisker/other_whisker/index.htm|publisher=NASA|accessdate=2011-08-01}}&lt;/ref&gt;

===Electrical power===

[[File:Datacenter Backup Batteries.jpg|thumb|right|A bank of batteries in a large data center, used to provide power until diesel generators can start]]

Backup power consists of one or more [[uninterruptible power supply|uninterruptible power supplies]], battery banks, and/or [[Diesel generator|diesel]] / [[gas turbine]] generators.&lt;ref&gt;Detailed explanation of UPS topologies {{cite web|url=http://www.emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |format=PDF |title=EVALUATING THE ECONOMIC IMPACT OF UPS TECHNOLOGY |deadurl=yes |archiveurl=https://web.archive.org/web/20101122074817/http://emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |archivedate=2010-11-22 |df= }}&lt;/ref&gt;

To prevent [[single point of failure|single points of failure]], all elements of the electrical systems, including backup systems, are typically fully duplicated, and critical servers are connected to both the "A-side" and "B-side" power feeds. This arrangement is often made to achieve [[N+1 redundancy]] in the systems. [[Transfer switch#Static transfer switch|Static transfer switches]] are sometimes used to ensure instantaneous switchover from one supply to the other in the event of a power failure.

===Low-voltage cable routing===
Data cabling is typically routed through overhead [[cable tray]]s in modern data centers. But some{{Who|date=May 2012}} are still recommending under raised floor cabling for security reasons and to consider the addition of cooling systems above the racks in case this enhancement is necessary. Smaller/less expensive data centers without raised flooring may use anti-static tiles for a flooring surface. Computer cabinets are often organized into a [[Data center environmental control#Aisle containment|hot aisle]] arrangement to maximize airflow efficiency.

===Fire protection===
[[File:FM200 Three.jpg|thumb|[[FM200]] Fire Suppression Tanks]]
Data centers feature [[fire protection]] systems, including [[passive fire protection|passive]] and [[Active Design]] elements, as well as implementation of [[fire prevention]] programs in operations. [[Smoke detectors]] are usually installed to provide early warning of a fire at its incipient stage. This allows investigation, interruption of power, and manual fire suppression using hand held fire extinguishers before the fire grows to a large size. An [[active fire protection]] system, such as a [[fire sprinkler system]] or a [[clean agent]] fire suppression gaseous system, is often provided to control a full scale fire if it develops. High sensitivity smoke detectors, such as [[aspirating smoke detector]]s, activating [[clean agent]] fire suppression gaseous systems activate earlier than fire sprinklers.

* Sprinklers = structure protection and building life safety.
* Clean agents = business continuity and asset protection.
* No water = no collateral damage or clean up.

Passive fire protection elements include the installation of [[Firewall (construction)|fire walls]] around the data center, so a fire can be restricted to a portion of the facility for a limited time in the event of the failure of the active fire protection systems. Fire wall penetrations into the server room, such as cable penetrations, coolant line penetrations and air ducts, must be provided with fire rated penetration assemblies, such as [[fire stop]]ping.

===Security===
Physical security also plays a large role with data centers. Physical access to the site is usually restricted to selected personnel, with controls including a layered security system often starting with fencing, [[bollard]]s and [[mantrap (access control)|mantraps]].&lt;ref&gt;{{cite web|author=Sarah D. Scalet |url=http://www.csoonline.com/article/220665 |title=19 Ways to Build Physical Security Into a Data Center |publisher=Csoonline.com |date=2005-11-01 |accessdate=2013-08-30}}&lt;/ref&gt; [[Video camera]] surveillance and permanent [[security guard]]s are almost always present if the data center is large or contains sensitive information on any of the systems within. The use of finger print recognition [[mantrap (snare)|mantrap]]s is starting to be commonplace.

==Energy use==
[[File:Google Data Center, The Dalles.jpg|thumb|[[Google Data Centers|Google Data Center]], [[The Dalles, Oregon]]]]
{{main article|IT energy management}}

Energy use is a central issue for data centers. Power draw for data centers ranges from a few kW for a rack of servers in a closet to several tens of MW for large facilities. Some facilities have power densities more than 100 times that of a typical office building.&lt;ref&gt;{{cite web|url=http://www1.eere.energy.gov/femp/program/dc_energy_consumption.html|title=Data Center Energy Consumption Trends|publisher=U.S. Department of Energy|accessdate=2010-06-10}}&lt;/ref&gt; For higher power density facilities, electricity costs are a dominant [[operating expense]] and account for over 10% of the [[total cost of ownership]] (TCO) of a data center.&lt;ref&gt;J Koomey, C. Belady, M. Patterson, A. Santos, K.D. Lange. [http://www.intel.com/assets/pdf/general/servertrendsreleasecomplete-v25.pdf Assessing Trends Over Time in Performance, Costs, and Energy Use for Servers] Released on the web August 17th, 2009.&lt;/ref&gt; By 2012 the cost of power for the data center is expected to exceed the cost of the original capital investment.&lt;ref&gt;{{cite web|url=http://www1.eere.energy.gov/femp/pdfs/data_center_qsguide.pdf |title=Quick Start Guide to Increase Data Center Energy Efficiency |publisher=U.S. Department of Energy |accessdate=2010-06-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20101122035456/http://www1.eere.energy.gov:80/femp/pdfs/data_center_qsguide.pdf |archivedate=2010-11-22 |df= }}&lt;/ref&gt;

===Greenhouse gas emissions===
In 2007 the entire [[information and communication technologies]] or ICT sector was estimated to be responsible for roughly 2% of global [[Greenhouse gas|carbon emissions]] with data centers accounting for 14% of the ICT footprint.&lt;ref name="smart1"&gt;{{cite web|url=http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |title=Smart 2020: Enabling the low carbon economy in the information age |publisher=The Climate Group for the Global e-Sustainability Initiative |accessdate=2008-05-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20110728032834/http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |archivedate=2011-07-28 |df= }}&lt;/ref&gt; The US EPA estimates that servers and data centers are responsible for up to 1.5% of the total US electricity consumption,&lt;ref name="energystar1"&gt;{{cite web|url=http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf|title=Report to Congress on Server and Data Center Energy Efficiency|publisher=U.S. Environmental Protection Agency ENERGY STAR Program}}&lt;/ref&gt; or roughly .5% of US GHG emissions,&lt;ref&gt;A calculation of data center electricity burden cited in the [http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf Report to Congress on Server and Data Center Energy Efficiency] and electricity generation contributions to green house gas emissions published by the EPA in the [http://epa.gov/climatechange/emissions/downloads10/US-GHG-Inventory-2010_ExecutiveSummary.pdf Greenhouse Gas Emissions Inventory Report]. Retrieved 2010-06-08.&lt;/ref&gt;  for 2007. Given a business as usual scenario greenhouse gas emissions from data centers is projected to more than double from 2007 levels by 2020.&lt;ref name="smart1"/&gt;

Siting is one of the factors that affect the energy consumption and environmental effects of a datacenter. In areas where climate favors cooling and lots of renewable electricity is available the environmental effects will be more moderate. Thus countries with favorable conditions, such as: Canada,&lt;ref&gt;[http://www.theglobeandmail.com/report-on-business/canada-called-prime-real-estate-for-massive-data-computers/article2071677/ Canada Called Prime Real Estate for Massive Data Computers - Globe &amp; Mail] Retrieved June 29, 2011.&lt;/ref&gt; Finland,&lt;ref&gt;[http://datacenter-siting.weebly.com/ Finland - First Choice for Siting Your Cloud Computing Data Center.]. Retrieved 4 August 2010.&lt;/ref&gt; Sweden,&lt;ref&gt;{{cite web|url=http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|title=Stockholm sets sights on data center customers|accessdate=4 August 2010|archiveurl=https://web.archive.org/web/20100819190918/http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|archivedate=19 August 2010}}&lt;/ref&gt; Norway &lt;ref&gt;[http://www.innovasjonnorge.no/en/start-page/invest-in-norway/industries/datacenters/ In a world of rapidly increasing carbon emissions from the ICT industry, Norway offers a sustainable solution] Retrieved 1 March 2016.&lt;/ref&gt; and Switzerland,&lt;ref&gt;[http://www.greenbiz.com/news/2010/06/30/swiss-carbon-neutral-servers-hit-cloud Swiss Carbon-Neutral Servers Hit the Cloud.]. Retrieved 4 August 2010.&lt;/ref&gt; are trying to attract cloud computing data centers.

In an 18-month investigation by scholars at Rice University's Baker Institute for Public Policy in Houston and the Institute for Sustainable and Applied Infodynamics in Singapore, data center-related emissions will more than triple by 2020.
&lt;ref&gt;{{Cite news
 |author=Katrice R. Jalbuena 
 |title=Green business news. 
 |quote= 
 |publisher=EcoSeed 
 |date=October 15, 2010 
 |pages= 
 |url=http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy 
 |accessdate=2010-11-11 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20160618081417/http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy 
 |archivedate=2016-06-18 
 |df= 
}}&lt;/ref&gt;

===Energy efficiency===
The most commonly used metric to determine the energy efficiency of a data center is [[power usage effectiveness]], or PUE. This simple ratio is the total power entering the data center divided by the power used by the IT equipment.

:&lt;math&gt; \mathrm{PUE}  =  {\mbox{Total Facility Power} \over \mbox{IT Equipment Power}} &lt;/math&gt;

Total facility power consists of power used by IT equipment plus any overhead power consumed by anything that is not considered a computing or data communication device (i.e. cooling, lighting, etc.). An ideal PUE is 1.0 for the hypothetical situation of zero overhead power. The average data center in the US has a PUE of 2.0,&lt;ref name="energystar1"/&gt; meaning that the facility uses two watts of total power (overhead + IT equipment) for every watt delivered to IT equipment. State-of-the-art data center energy efficiency is estimated to be roughly 1.2.&lt;ref&gt;{{cite web|url=https://microsite.accenture.com/svlgreport/Documents/pdf/SVLG_Report.pdf|title=Data Center Energy Forecast|publisher=Silicon Valley Leadership Group}}&lt;/ref&gt; Some large data center operators like [[Microsoft]] and [[Yahoo!]] have published projections of PUE for facilities in development; [[Google]] publishes quarterly actual efficiency performance from data centers in operation.&lt;ref&gt;{{cite web|url=https://www.google.com/about/datacenters/efficiency/internal/|title=Efficiency: How we do it &#8211; Data centers|publisher=Google|accessdate=2015-01-19}}&lt;/ref&gt;

The [[U.S. Environmental Protection Agency]] has an [[Energy Star]] rating for standalone or large data centers. To qualify for the ecolabel, a data center must be within the top quartile of energy efficiency of all reported facilities.&lt;ref&gt;Commentary on introduction of Energy Star for Data Centers {{cite web|title=Introducing EPA ENERGY STAR for Data Centers |url=http://www.emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |format=Web site |publisher=Jack Pouchet |accessdate=2010-09-27 |date=2010-09-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20100925210539/http://emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |archivedate=2010-09-25 |df= }}&lt;/ref&gt;

European Union also has a similar initiative: EU Code of Conduct for Data Centres&lt;ref&gt;{{cite web|url=http://iet.jrc.ec.europa.eu/energyefficiency/ict-codes-conduct/data-centres-energy-efficiency |title=EU Code of Conduct for Data Centres |publisher=iet.jrc.ec.europa.eu |date= |accessdate=2013-08-30 }}&lt;/ref&gt;

===Energy use analysis===
Often, the first step toward curbing energy use in a data center is to understand how energy is being used in the data center. Multiple types of analysis exist to measure data center energy use. Aspects measured include not just energy used by IT equipment itself, but also by the data center facility equipment, such as chillers and fans.&lt;ref&gt;Sweeney, Jim. "Reducing Data Center Power and Energy Consumption: Saving Money and 'Going Green,' " GTSI Solutions, pages 2&#8211;3. [http://www.gtsi.com/cms/documents/white-papers/green-it.pdf]&lt;/ref&gt;

===Power and cooling analysis===
Power is the largest recurring cost to the user of a data center.&lt;ref name=DRJ_Choosing&gt;{{Citation
 | title = Choosing a Data Center
 | url = http://www.atlantic.net/images/pdf/choosing_a_data_center.pdf
 | publisher = Disaster Recovery Journal
 | year = 2009
 | author = Cosmano, Joe
 | accessdate = 2012-07-21
}}&lt;/ref&gt;  A power and cooling analysis, also referred to as a thermal assessment, measures the relative temperatures in specific areas as well as the capacity of the cooling systems to handle specific ambient temperatures.&lt;ref&gt;Needle, David. "HP's Green Data Center Portfolio Keeps Growing," InternetNews, July 25, 2007. [http://www.internetnews.com/xSP/article.php/3690651/HPs+Green+Data+Center+Portfolio+Keeps+Growing.htm]&lt;/ref&gt; A power and cooling analysis can help to identify hot spots, over-cooled areas that can handle greater power use density, the breakpoint of equipment loading, the effectiveness of a raised-floor strategy, and optimal equipment positioning (such as AC units) to balance temperatures across the data center. Power cooling density is a measure of how much square footage the center can cool at maximum capacity.&lt;ref name=Inc_Howtochoose&gt;{{Citation
 | title = How to Choose a Data Center
 | url = http://www.inc.com/guides/2010/11/how-to-choose-a-data-center_pagen_2.html
 | year = 2010
 | author = Inc. staff
 | accessdate = 2012-07-21
}}&lt;/ref&gt;

===Energy efficiency analysis===
An energy efficiency analysis measures the energy use of data center IT and facilities equipment. A typical energy efficiency analysis measures factors such as a data center's power use effectiveness (PUE) against industry standards, identifies mechanical and electrical sources of inefficiency, and identifies air-management metrics.&lt;ref&gt;Siranosian, Kathryn. "HP Shows Companies How to Integrate Energy Management and Carbon Reduction," TriplePundit, April 5, 2011. [http://www.triplepundit.com/2011/04/hp-launches-program-companies-integrate-manage-energy-carbon-reduction-strategies/]&lt;/ref&gt;

===Computational fluid dynamics (CFD) analysis===
{{main article|Computational fluid dynamics}}

This type of analysis uses sophisticated tools and techniques to understand the unique thermal conditions present in each data center&#8212;predicting the temperature, airflow, and pressure behavior of a data center to assess performance and energy consumption, using numerical modeling.&lt;ref&gt;Bullock, Michael. "Computation Fluid Dynamics - Hot topic at Data Center World," Transitional Data Services, March 18, 2010. [http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software] {{webarchive |url=https://web.archive.org/web/20120103183406/http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software |date=January 3, 2012 }}&lt;/ref&gt; By predicting the effects of these environmental conditions, CFD analysis in the data center can be used to predict the impact of high-density racks mixed with low-density racks&lt;ref&gt;Bouley, Dennis (editor). "Impact of Virtualization on Data Center Physical Infrastructure," The Green grid, 2010. [http://www.thegreengrid.org/~/media/WhitePapers/White_Paper_27_Impact_of_Virtualization_Data_On_Center_Physical_Infrastructure_020210.pdf?lang=en]&lt;/ref&gt; and the onward impact on cooling resources, poor infrastructure management practices and AC failure of AC shutdown for scheduled maintenance.

===Thermal zone mapping===
Thermal zone mapping uses sensors and computer modeling to create a three-dimensional image of the hot and cool zones in a data center.&lt;ref&gt;Fontecchio, Mark. "HP Thermal Zone Mapping plots data center hot spots," SearchDataCenter, July 25, 2007. [http://searchdatacenter.techtarget.com/news/1265634/HP-Thermal-Zone-Mapping-plots-data-center-hot-spots]&lt;/ref&gt;

This information can help to identify optimal positioning of data center equipment. For example, critical servers might be placed in a cool zone that is serviced by redundant AC units.

===Green data centers===
[[File:Magazin Vauban E.jpg|thumb| This water-cooled data center in the [[Independent Port of Strasbourg|Port of Strasbourg]], France claims the attribute ''green''.]]
Data centers use a lot of power, consumed by two main usages: the power required to run the actual equipment and then the power required to cool the equipment. The first category is addressed by designing computers and storage systems that are increasingly power-efficient.&lt;ref name="ReferenceDC2"/&gt; To bring down cooling costs data center designers try to use natural ways to cool the equipment. Many data centers are located near good fiber connectivity, power grid connections and also people-concentrations to manage the equipment, but there are also circumstances where the data center can be miles away from the users and don't need a lot of local management. Examples of this are the 'mass' data centers like Google or Facebook: these DC's are built around many standardized servers and storage-arrays and the actual users of the systems are located all around the world. After the initial build of a data center staff numbers required to keep it running are often relatively low: especially data centers that provide mass-storage or computing power which don't need to be near population centers.Data centers in arctic locations where outside air provides all cooling are getting more popular as cooling and electricity are the two main variable cost components.&lt;ref&gt;{{cite web|url=http://www.gizmag.com/fjord-cooled-data-center/20938/|title=Fjord-cooled DC in Norway claims to be greenest|access-date=23 December 2011}}&lt;/ref&gt;

==Network infrastructure==
[[File:Paris servers DSC00190.jpg|thumb|left|An example of "rack mounted" servers]]
Communications in data centers today are most often based on [[computer network|networks]] running the [[Internet protocol|IP]] [[protocol (computing)|protocol]] suite. Data centers contain a set of [[Router (computing)|routers]] and [[Network switch|switches]] that transport traffic between the servers and to the outside world. [[Redundancy (engineering)|Redundancy]] of the Internet connection is often provided by using two or more upstream service providers (see [[Multihoming]]).

Some of the servers at the data center are used for running the basic Internet and [[intranet]] services needed by internal users in the organization, e.g., e-mail servers, [[proxy server]]s, and [[Domain Name System|DNS]] servers.

Network security elements are also usually deployed: [[firewall (networking)|firewalls]], [[VPN]] [[Gateway (computer networking)|gateways]], [[intrusion detection system]]s, etc. Also common are monitoring systems for the network and some of the applications. Additional off site monitoring systems are also typical, in case of a failure of communications inside the data center.

==Data center infrastructure management==
[[Data center infrastructure management]] (DCIM) is the integration of information technology (IT) and facility management disciplines to centralize monitoring, management and intelligent capacity planning of a data center's critical systems. Achieved through the implementation of specialized software, hardware and sensors, DCIM enables common, real-time monitoring and management platform for all interdependent systems across IT and facility infrastructures.

Depending on the type of implementation, DCIM products can help data center managers identify and eliminate sources of risk to increase availability of critical IT systems. DCIM products also can be used to identify interdependencies between facility and IT infrastructures to alert the facility manager to gaps in system redundancy, and provide dynamic, holistic benchmarks on power consumption and efficiency to measure the effectiveness of "green IT" initiatives.

It's important to measure and understand data center efficiency metrics.  A lot of the discussion in this area has focused on energy issues, but other metrics beyond the PUE can give a more detailed picture of the data center operations. Server, storage, and staff utilization metrics can contribute to a more complete view of an enterprise data center. In many cases, disc capacity goes unused and in many instances the organizations run their servers at 20% utilization or less.&lt;ref&gt;{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |title=Measuring Data Center Efficiency: Easier Said Than Done |publisher=Dell.com |accessdate=2012-06-25 |deadurl=yes |archiveurl=https://web.archive.org/web/20101027083349/http://content.dell.com:80/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |archivedate=2010-10-27 |df= }}&lt;/ref&gt; More effective automation tools can also improve the number of servers or virtual machines that a single admin can handle.

DCIM providers are increasingly linking with [[computational fluid dynamics]] providers to predict complex airflow patterns in the data center. The CFD component is necessary to quantify the impact of planned future changes on cooling resilience, capacity and efficiency.&lt;ref name="gartner"&gt;{{cite web|url=http://www.gartner.com/it-glossary/computational-fluid-dynamic-cfd-analysis|title=Computational-Fluid-Dynamic (CFD) Analysis &amp;#124; Gartner IT Glossary|publisher=gartner.com|accessdate=2014-08-27}}&lt;/ref&gt;

==Managing the capacity of a data center==
{{unreferenced section|date=August 2016}}
[[File:Capacity of a datacenter - Life Cycle.jpg|thumbnail|left|Capacity of a datacenter - Life Cycle]]
Several parameters may limit the capacity of a data center. For long term usage, the main limitations will be available area, then available power. In the first stage of its life cycle, a data center will see its occupied space growing more rapidly than consumed energy. With constant densification of new IT technologies, the need in energy is going to become dominant, equaling then overcoming the need in area (second then third phase of cycle). The development and multiplication of connected objects, the needs in storage and data treatment lead to the necessity of data centers to grow more and more rapidly. It is therefore important to define a data center strategy before being cornered. The decision, conception and building cycle lasts several years. Therefore, it is imperative to initiate this strategic consideration when the data center reaches about 50% of its power capacity. Maximum occupation of a data center needs to be stabilized around 85%, be it in power or occupied area. Resources thus managed will allow a rotation zone for managing hardware replacement and will allow temporary cohabitation of old and new generations. In the case where this limit would be overcrossed durably, it would not be possible to proceed to material replacements, which would invariably lead to smothering the information system. The data center is a resource in its own right of the information system, with its own constraints of time and management (life span of 25 years), it therefore needs to be taken into consideration in the framework of the SI midterm planning (between 3 and 5 years).

==Applications==
[[File:IBMPortableModularDataCenter.jpg|thumb|right|A 40-foot [[Portable Modular Data Center]]]]

The main purpose of a data center is running the IT systems applications that handle the core business and operational data of the organization. Such systems may be proprietary and developed internally by the organization, or bought from [[enterprise software]] vendors. Such common applications are [[Enterprise resource planning|ERP]] and [[Customer relationship management|CRM]] systems.

A data center may be concerned with just [[operations architecture]] or it may provide other services as well.

Often these applications will be composed of multiple hosts, each running a single component. Common components of such applications are [[database]]s, [[file server]]s, [[application server]]s, [[middleware]], and various others.

Data centers are also used for off site backups. Companies may subscribe to backup services provided by a data center. This is often used in conjunction with [[Tape drive|backup tapes]]. Backups can be taken off servers locally on to tapes. However, tapes stored on site pose a security threat and are also susceptible to fire and flooding. Larger companies may also send their backups off site for added security. This can be done by backing up to a data center. Encrypted backups can be sent over the Internet to another data center where they can be stored securely.

For quick deployment or [[disaster recovery]], several large hardware vendors have developed mobile/modular solutions that can be installed and made operational in very short time. Companies such as
[[File:Edge Night 02.jpg|thumb|A modular data center connected to the power grid at a utility substation]]
* [[Cisco Systems]],&lt;ref&gt;{{cite web|title=Info and video about Cisco's solution |url=http://www.datacenterknowledge.com/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |publisher=Datacentreknowledge |accessdate=2008-05-11 |date=May 15, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20080519213241/http://www.datacenterknowledge.com:80/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |archivedate=2008-05-19 |df= }}&lt;/ref&gt;
* [[Sun Microsystems]] ([[Sun Modular Datacenter]]),&lt;ref&gt;{{cite web|url=http://www.sun.com/products/sunmd/s20/specifications.jsp|archiveurl=https://web.archive.org/web/20080513090300/http://www.sun.com/products/sunmd/s20/specifications.jsp|archivedate=2008-05-13|title=Technical specs of Sun's Blackbox|accessdate=2008-05-11}}&lt;/ref&gt;&lt;ref&gt;And English Wiki article on [[Sun Modular Datacenter|Sun's modular datacentre]]&lt;/ref&gt;
* [[Groupe Bull|Bull]] (mobull),&lt;ref&gt;{{cite web|title=Mobull Plug and Boot Datacenter|url=http://www.bull.com/extreme-computing/mobull.html|publisher=Bull|first=Daniel|last=Kidger|accessdate=2011-05-24}}&lt;/ref&gt;
* [[IBM]] ([[Portable Modular Data Center]]),
* [[Schneider-Electric]] ([[Portable Modular Data Center]]),
* [[Hewlett-Packard|HP]] ([[HP Performance Optimized Datacenter|Performance Optimized Datacenter]]),&lt;ref&gt;{{cite web|url=http://h18004.www1.hp.com/products/servers/solutions/datacentersolutions/pod/index.html |title=HP Performance Optimized Datacenter (POD) 20c and 40c - Product Overview |publisher=H18004.www1.hp.com |date= |accessdate=2013-08-30}}&lt;/ref&gt;
* [[Huawei]] (Container Data Center Solution),&lt;ref&gt;{{cite web|title=Huawei's Container Data Center Solution|url=http://www.huawei.com/ilink/enenterprise/download/HW_143893|publisher=Huawei|accessdate=2014-05-17}}
&lt;/ref&gt; and
* [[Google]] ([[Google Modular Data Center]]) have developed systems that could be used for this purpose.&lt;ref&gt;{{cite web|url=http://www.crn.com/hardware/208403225 |publisher=ChannelWeb |accessdate=2008-05-11 |title=IBM's Project Big Green Takes Second Step |first=Brian |last=Kraemer |date=June 11, 2008 |deadurl=yes |archiveurl=https://web.archive.org/web/20080611114732/http://www.crn.com:80/hardware/208403225 |archivedate=2008-06-11 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |title=Modular/Container Data Centers Procurement Guide: Optimizing for Energy Efficiency and Quick Deployment |format=PDF |date= |accessdate=2013-08-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130531191212/http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |archivedate=2013-05-31 |df= }}&lt;/ref&gt;
* BASELAYER has a patent on the software defined modular data center.&lt;ref&gt;{{Citation|title = System and method of providing computer resources|url = http://www.google.com/patents/US8434804|date = May 7, 2013|accessdate = 2016-02-24|first = George|last = Slessman}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|title = Modular Data Center Firm IO to Split Into Two Companies|url = http://www.datacenterknowledge.com/archives/2014/12/02/modular-data-center-firm-io-to-split-into-two-companies/|website = Data Center Knowledge|access-date = 2016-02-24|language = en-US}}&lt;/ref&gt;

==US wholesale and retail colocation providers==
According to Synergy Research Group, "the scale of the wholesale colocation market in the United States is very significant relative to the retail market, with Q3 wholesale revenues reaching almost $700 million. [[Digital Realty]] Trust is the wholesale market leader, followed at a distance by [[DuPont Fabros]]." Synergy Research also describes the US colocation market as the most mature and well-developed in the world," based on revenue and the continued adoption of cloud infrastructure services.
* Contains estimates from Synergy Research Group.&lt;ref name="srgresearch"&gt;{{cite web|url=https://www.srgresearch.com/articles/mature-us-colocation-market-led-equinix-and-centurylink-savvis|title=Mature US Colocation Market Led by Equinix and CenturyLink-Savvis &amp;#124; Synergy Research Group|author=Synergy Research Group, Reno, NV|publisher=srgresearch.com|accessdate=2014-08-27}}&lt;/ref&gt;

{| class="wikitable sortable"
|-
!Rank !! Company Name !! US Market Share
|-
!1
| Various Providers || 34%
|-
!2
| [[Equinix]] || 18%
|-
!3
| [[CenturyLink-Savvis]] || 8%
|-
!4
| [[SunGard]] || 5%
|-
!5
| [[AT&amp;T]] || 5%
|-
!6
| [[Verizon]] || 5%
|-
!7
| Telx || 4%
|-
!8
| CyrusOne || 4%
|-
!9
| [[Level 3 Communications]] || 3%
|-
!10
| [[Internap]] || 2%
|}

==See also==
{{columns-list|colwidth=20em|
* [[Central apparatus room]]
* [[Colocation center]]
* [[Data center infrastructure management]]
* [[Disaster recovery]]
* [[Dynamic Infrastructure]]
* [[Electrical network]]
* [[HVAC]]
* [[Internet exchange point]]
* [[Internet hosting service]]
* [[Modular data center]]
* [[Neher&#8211;McGrath]]
* [[Network operations center]]
* [[Open Compute Project]], by [[Facebook]]
* [[Peering]]
* [[Server farm]]
* [[Server room]]
* [[Server Room Environment Monitoring System]]
* [[Server sprawl]]
* [[Sun Modular Datacenter]]
* [[Telecommunications network]]
* [[Utah Data Center]]
* [[Web hosting service]]
* [[Anderson Powerpole]] connector
}}

==References==
{{reflist|colwidth=30em}}

==External links==
{{Commons category|Data centers}}
{{wikibooks|The Design and Organization of Data Centers}}
{{wiktionary}}
* [http://hightech.lbl.gov/datacenters.html Lawrence Berkeley Lab] - Research, development, demonstration, and deployment of energy-efficient technologies and practices for data centers
* [http://hightech.lbl.gov/dc-powering/faq.html DC Power For Data Centers Of The Future] - FAQ: 380VDC testing and demonstration at a Sun data center.
* [http://www.dccompendium.com/ DC Compendium] - Repository and compendium of data centers globally.
* [http://media.wix.com/ugd/fb8983_e929404b24874e4fa7a8279f1cda58f8.pdf White Paper] - Property Taxes: The New Challenge for Data Centers

{{Authority control}}
{{Cloud computing}}

{{DEFAULTSORT:Data Center}}
[[Category:Computer networking]]
[[Category:Applications of distributed computing]]
[[Category:Cloud storage]]
[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
[[Category:Servers (computing)]]
[[Category:Data centers| ]]</text>
      <sha1>moov7q8axkagak4ued03d9agsv1wsjm</sha1>
    </revision>
  </page>
  <page>
    <title>Data conditioning</title>
    <ns>0</ns>
    <id>24540689</id>
    <revision>
      <id>677395227</id>
      <parentid>677395170</parentid>
      <timestamp>2015-08-23T00:05:46Z</timestamp>
      <contributor>
        <username>Twimoki</username>
        <id>7226010</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3078" xml:space="preserve">{{Multiple issues|
{{POV|date=August 2015}}
{{One source|date=August 2015}}
{{Notability|date=August 2015}}
}}

'''Data conditioning''' is the use of data management and optimization techniques which result in the intelligent routing, optimization and protection of data for storage or [[data movement]] in a computer system.  Data conditioning features enable enterprise and cloud [[data center]]s to dramatically improve system utilization and increase application performance lowering both [[capital expenditures]] and [[operating cost]]s.

Data conditioning technologies delivered through a Data Conditioning Platform optimize data as it moves through a computer&#8217;s I/O ([[Input/Output]]) path or I/O bus&#8212;the data path between the main processor complex and storage subsystems.  The functions of a Data Conditioning Platform typically reside on a storage controller add-in card inserted into the [[PCI-e]] slots of a server.  This enables easy integration of new features in a server or a whole data center.

Data conditioning features delivered via a Data Conditioning Platform are designed to simplify system integration, and minimize implementation risks associated with deploying new technologies by ensuring seamless compatibility with all leading server and  storage hardware, operating systems and applications, and meeting all current commercial/off-the-shelf (COTS) standards.  By delivering optimization features via a Data Conditioning Platform, data center managers can improve system efficiency and reduce cost with minimal disruption and avoid the need to modify existing applications or operating systems, and leverage existing hardware systems.

== Summary ==

Data conditioning builds on existing data storage functionality delivered in the I/O path including [[RAID]] (Redundant Arrays of Inexpensive Disks), intelligent I/O-based [http://www.adaptec.com/en-us/_common/greenpower?refURL=/greenpower/ power management], and [[Solid-state drive|SSD]] (Solid-State Drive) performance caching techniques.  Data conditioning is enabled both by advanced [[ASIC]] controller technology and intelligent software.  New data conditioning capabilities can be designed into and delivered via storage controllers in the I/O path  or to achieve the data center&#8217;s technical and business goals.

Data Conditioning strategies can also be applied to improving server and storage utilization and for better managing a wide range of hardware and system-level capabilities.

== Background and Purpose ==

Data conditioning principles can be applied to any demanding computing environment to create significant cost, performance and system utilization efficiencies, and are typically deployed by data center managers, system integrators, and storage and server OEMs seeking to optimize hardware and software utilization, simplified, non-intrusive technology integration, and minimal risks and performance hits traditionally associated with incorporating new data center technologies.

== References ==

[http://www.adaptec/maxIQ Adaptec MaxIQ]

[[Category:Data management]]</text>
      <sha1>gl31v81z2hv63njc5k63mm5bmosx4l1</sha1>
    </revision>
  </page>
  <page>
    <title>Concurrency control</title>
    <ns>0</ns>
    <id>217356</id>
    <revision>
      <id>730186339</id>
      <parentid>711854345</parentid>
      <timestamp>2016-07-17T09:23:28Z</timestamp>
      <contributor>
        <username>Ewen</username>
        <id>3610</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32811" xml:space="preserve">{{pp-pc1}}
In [[information technology]] and [[computer science]],  especially in the fields of [[computer programming]], [[operating systems]], [[multiprocessor]]s, and [[database]]s, '''concurrency control''' ensures that correct results for [[Concurrent computing|concurrent]] operations are generated, while getting those results as quickly as possible.

Computer systems, both [[software]] and [[computer hardware|hardware]], consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in [[Computer memory|memory]] or [[Computer data storage|storage]]), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and [[Scientific theory|theories]] to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a [[concurrent algorithm]] compared to the simpler [[sequential algorithm]].

For example, a failure in concurrency control can result in [[data corruption]] from [[Torn data-access operation|torn read or write operations]].

==Concurrency control in databases==
'''Comments:'''
# This section is applicable to all transactional systems, i.e., to all systems that use ''[[database transaction]]s'' (''atomic transactions''; e.g., transactional objects in [[Systems management]] and in networks of [[smartphone]]s which typically implement private, dedicated database systems), not only general-purpose [[database management system]]s (DBMSs).
# DBMSs need to deal also with concurrency control issues not typical just to database transactions but rather to operating systems in general. These issues (e.g., see ''[[Concurrency control#Concurrency control in operating systems|Concurrency control in operating systems]]'' below) are out of the scope of this section.

Concurrency control in [[Database management system]]s (DBMS; e.g., [[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]), other [[database transaction|transactional]] objects, and related distributed applications (e.g., [[Grid computing]] and [[Cloud computing]]) ensures that ''[[database transaction]]s'' are performed [[Concurrency (computer science)|concurrently]] without violating the [[data integrity]] of the respective [[database]]s. Thus concurrency control is an essential element for correctness in any system where two database transactions or more, executed with time overlap, can access the same data, e.g., virtually in any general-purpose database system. Consequently, a vast body of related research has been accumulated since database systems emerged in the early 1970s. A well established concurrency control [[Scientific theory|theory]] for database systems is outlined in the references mentioned above: [[Serializability|serializability theory]], which allows to effectively design and analyze concurrency control methods and mechanisms. An alternative theory for concurrency control of atomic transactions over [[abstract data type]]s is presented in ([[#Lynch1993|Lynch et al. 1993]]), and not utilized below. This theory is more refined, complex, with a wider scope, and has been less utilized in the Database literature than the classical theory above. Each theory has its pros and cons, emphasis and [[insight]]. To some extent they are complementary, and their merging may be useful.

To ensure correctness, a DBMS usually guarantees that only ''[[Serializability|serializable]]'' transaction [[Schedule (computer science)|schedule]]s are generated, unless ''serializability'' is [[Serializability#Relaxing serializability|intentionally relaxed]] to increase performance, but only in cases where application correctness is not harmed. For maintaining correctness in cases of failed (aborted) transactions (which can always happen for many reasons) schedules also need to have the ''[[Serializability#Correctness - recoverability|recoverability]]'' (from abort) property. A DBMS also guarantees that no effect of ''committed'' transactions is lost, and no effect of ''aborted'' ([[Rollback (data management)|rolled back]]) transactions remains in the related database. Overall transaction characterization is usually summarized by the [[ACID]] rules below. As databases have become [[Distributed database|distributed]], or needed to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990, and [[Cloud computing]] currently), the effective distribution of concurrency control mechanisms has received special attention.

===Database transaction and the ACID rules===
{{main|Database transaction|ACID}}
The concept of a ''database transaction'' (or ''atomic transaction'') has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time, and ''recovery'' from a crash to a well understood database state. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). Every database transaction obeys the following rules (by support in the database system; i.e., a database system is designed to guarantee them for the transactions it runs):
*'''[[Atomicity (database systems)|Atomicity]]''' - Either the effects of all or none of its operations remain ("all or nothing" semantics) when a [[database transaction|transaction]] is completed (''committed'' or ''aborted'' respectively). In other words, to the outside world a committed transaction appears (by its effects on the database) to be indivisible (atomic), and an aborted transaction does not affect the database at all.
*'''[[Consistency (database systems)|Consistency]]''' - Every transaction must leave the database in a consistent (correct) state, i.e., maintain the predetermined integrity rules of the database (constraints upon and among the database's objects). A transaction must transform a database from one consistent state to another consistent state (however, it is the responsibility of the transaction's programmer to make sure that the transaction itself is correct, i.e., performs correctly what it intends to perform (from the application's point of view) while the predefined integrity rules are enforced by the DBMS). Thus since a database can be normally changed only by transactions, all the database's states are consistent.
*'''[[Isolation (database systems)|Isolation]]''' - Transactions cannot interfere with each other (as an end result of their executions). Moreover, usually (depending on concurrency control method) the effects of an incomplete transaction are not even visible to another transaction. Providing isolation is the main goal of concurrency control.
*'''[[Durability (database systems)|Durability]]''' - Effects of successful (committed) transactions must persist through [[Crash (computing)|crash]]es (typically by recording the transaction's effects and its commit event in a [[non-volatile memory]]).

The concept of atomic transaction has been extended during the years to what has become [[Business transaction management|Business transactions]] which actually implement types of [[Workflow]] and are not atomic. However also such enhanced transactions typically utilize atomic transactions as components.

===Why is concurrency control needed?===
If transactions are executed ''serially'', i.e., sequentially with no overlap in time, no transaction concurrency exists. However, if concurrent transactions with interleaving operations are allowed in an uncontrolled manner, some unexpected, undesirable result may occur, such as:
# The lost update problem: A second transaction writes a second value of a data-item (datum) on top of a first value written by a first concurrent transaction, and the first value is lost to other transactions running concurrently which need, by their precedence, to read the first value. The transactions that have read the wrong value end with incorrect results.
# The dirty read problem: Transactions read a value written by a transaction that has been later aborted. This value disappears from the database upon abort, and should not have been read by any transaction ("dirty read"). The reading transactions end with incorrect results.
# The incorrect summary problem: While one transaction takes a summary over the values of all the instances of a repeated data-item, a second transaction updates some instances of that data-item. The resulting summary does not reflect a correct result for any (usually needed for correctness) precedence order between the two transactions (if one is executed before the other), but rather some random result, depending on the timing of the updates, and whether certain update results have been included in the summary or not.

Most high-performance transactional systems need to run transactions concurrently to meet their performance requirements. Thus, without concurrency control such systems can neither provide correct results nor maintain their databases consistent.

===Concurrency control mechanisms===

====Categories====
The main categories of concurrency control mechanisms are:
* '''[[Optimistic concurrency control|Optimistic]]''' - Delay the checking of whether a transaction meets the isolation and other integrity rules (e.g., [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]]) until its end, without blocking any of its (read, write) operations ("...and be optimistic about the rules being met..."), and then abort a transaction to prevent the violation, if the desired rules are to be violated upon its commit. An aborted transaction is immediately restarted and re-executed, which incurs an obvious overhead (versus executing it to the end only once). If not too many transactions are aborted, then being optimistic is usually a good strategy.
* '''Pessimistic''' - Block an operation of a transaction, if it may cause violation of the rules, until the possibility of violation disappears. Blocking operations is typically involved with performance reduction.
*'''Semi-optimistic''' - Block operations in some situations,  if they may cause violation of some rules, and do not block in other situations while delaying rules checking (if needed) to transaction's end, as done with optimistic.

Different categories provide different performance, i.e., different average transaction completion rates (''throughput''), depending on transaction types mix, computing level of parallelism, and other factors. If selection and knowledge about trade-offs are available, then category and method should be chosen to provide the highest performance.

The mutual blocking between two transactions (where each one blocks the other) or more results in a [[deadlock]], where the transactions involved are stalled and cannot reach completion. Most non-optimistic mechanisms (with blocking) are prone to deadlocks which are resolved by an intentional abort of a stalled transaction (which releases the other transactions in that deadlock), and its immediate restart and re-execution. The likelihood of a deadlock is typically low.

Blocking, deadlocks, and aborts all result in performance reduction, and hence the trade-offs between the categories.

====Methods====
Many methods for concurrency control exist. Most of them can be implemented within either main category above. The major methods,&lt;ref name=Bern2009&gt;[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (page 145)&lt;/ref&gt; which have each many variants, and in some cases may overlap or be combined, are:
#Locking (e.g., '''[[Two-phase locking]]''' - 2PL) - Controlling access to data by [[Lock (computer science)|locks]] assigned to the data. Access of a transaction to a data item (database object) locked by another transaction may be blocked (depending on lock type and access operation type) until lock release.
#'''Serialization [[Serializability#Testing conflict serializability|graph checking]]''' (also called Serializability, or Conflict, or Precedence graph checking) - Checking for [[Cycle (graph theory)|cycles]] in the schedule's [[Directed graph|graph]] and breaking them by aborts.
#'''[[Timestamp-based concurrency control|Timestamp ordering]]''' (TO) - Assigning timestamps to transactions, and controlling or checking access to data by timestamp order.
#'''[[Commitment ordering]]''' (or Commit ordering; CO) - Controlling or checking transactions' chronological order of commit events to be compatible with their respective [[Serializability#Testing conflict serializability|precedence order]].

Other major concurrency control types that are utilized in conjunction with the methods above include:

* '''[[Multiversion concurrency control]]''' (MVCC) - Increasing concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object) depending on scheduling method.
* '''[[Index locking|Index concurrency control]]''' - Synchronizing access operations to [[Index (database)|index]]es, rather than to user data. Specialized methods provide substantial performance gains.
* '''Private workspace model''' ('''Deferred update''') - Each transaction maintains a private workspace for its accessed data, and its changed data become visible outside the transaction only upon its commit (e.g., [[#Weikum01|Weikum and Vossen 2001]]). This model provides a different concurrency control behavior with benefits in many cases.

The most common mechanism type in database systems since their early days in the 1970s has been ''[[Two-phase locking|Strong strict Two-phase locking]]'' (SS2PL; also called ''Rigorous scheduling'' or ''Rigorous 2PL'') which is a special case (variant) of both [[Two-phase locking]] (2PL) and [[Commitment ordering]] (CO). It is pessimistic. In spite of its long name (for historical reasons) the idea of the '''SS2PL''' mechanism is simple: "Release all locks applied by a transaction only after the transaction has ended." SS2PL (or Rigorousness) is also the name of the set of all schedules that can be generated by this mechanism, i.e., these are SS2PL (or Rigorous) schedules, have the SS2PL (or Rigorousness) property.

===Major goals of concurrency control mechanisms===
Concurrency control mechanisms firstly need to operate correctly, i.e., to maintain each transaction's integrity rules (as related to concurrency; application-specific integrity rule are out of the scope here) while transactions are running concurrently, and thus the integrity of the entire transactional system. Correctness needs to be achieved with as good performance as possible. In addition, increasingly a need exists to operate effectively while transactions are [[Distributed transaction|distributed]] over [[Process (computing)|processes]], [[computer]]s, and [[computer network]]s. Other subjects that may affect concurrency control are [[Data recovery|recovery]] and [[Replication (computer science)|replication]].

====Correctness====

=====Serializability=====
{{Main|Serializability}}

For correctness, a common major goal of most concurrency control mechanisms is generating [[Schedule (computer science)|schedule]]s with the ''[[Serializability]]'' property. Without serializability undesirable phenomena may occur, e.g., money may disappear from accounts, or be generated from nowhere. '''Serializability''' of a schedule means equivalence (in the resulting database values) to some ''serial'' schedule with the same transactions (i.e., in which transactions are sequential with no overlap in time, and thus completely isolated from each other: No concurrent access by any two transactions to the same data is possible). Serializability is considered the highest level of [[isolation (database systems)|isolation]] among [[database transaction]]s, and the major correctness criterion for concurrent transactions. In some cases compromised, [[serializability#Relaxing serializability|relaxed forms]] of serializability are allowed for better performance (e.g., the popular ''[[Snapshot isolation]]'' mechanism) or to meet [[availability]] requirements in highly distributed systems (see ''[[Eventual consistency]]''), but only if application's correctness is not violated by the relaxation (e.g., no relaxation is allowed for [[money]] transactions, since by relaxation money can disappear, or appear from nowhere).

Almost all implemented concurrency control mechanisms achieve serializability by providing ''[[Serializability#View and conflict serializability|Conflict serializablity]]'', a broad special case of serializability (i.e., it covers, enables most serializable schedules, and does not impose significant additional delay-causing constraints) which can be implemented efficiently.

=====Recoverability=====
:See ''[[Serializability#Correctness - recoverability|Recoverability]]'' in ''[[Serializability]]''

'''Comment:''' While in the general area of systems the term "recoverability" may refer to the ability of a system to recover from failure or from an incorrect/forbidden state, within concurrency control of database systems this term has received a specific meaning.

Concurrency control typically also ensures the ''[[Serializability#Correctness - recoverability|Recoverability]]'' property of schedules for maintaining correctness in cases of aborted transactions (which can always happen for many reasons). '''Recoverability''' (from abort) means that no committed transaction in a schedule has read data written by an aborted transaction. Such data disappear from the database (upon the abort) and are parts of an incorrect database state. Reading such data violates the consistency rule of ACID. Unlike Serializability, Recoverability cannot be compromised, relaxed at any case, since any relaxation results in quick database integrity violation upon aborts. The major methods listed above provide serializability mechanisms. None of them in its general form automatically provides recoverability, and special considerations and mechanism enhancements are needed to support recoverability. A commonly utilized special case of recoverability is ''[[Schedule (computer science)#Strict|Strictness]]'', which allows efficient database recovery from failure (but excludes optimistic implementations; e.g., [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]] cannot have an optimistic implementation, but [[The History of Commitment Ordering#Semi-optimistic database scheduler|has semi-optimistic ones]]).

'''Comment:''' Note that the ''Recoverability'' property is needed even if no database failure occurs and no database ''recovery'' from failure is needed. It is rather needed to correctly automatically handle transaction aborts, which may be unrelated to database failure and recovery from it.

====Distribution====
With the fast technological development of computing the difference between local and distributed computing over low latency [[Computer network|networks]] or [[Bus (computing)|buses]] is blurring. Thus the quite effective utilization of local techniques in such distributed environments is common, e.g., in [[computer cluster]]s and [[multi-core processor]]s. However the local techniques have their limitations and use multi-processes (or threads) supported by multi-processors (or multi-cores) to scale. This often turns transactions into distributed ones, if they themselves need to span multi-processes. In these cases most local concurrency control techniques do not scale well.

=====Distributed serializability and Commitment ordering=====
{{POV-section|Commitment ordering|date=November 2011}}
:See ''[[Serializability#Distributed serializability|Distributed serializability]]'' in ''[[Serializability]]''
{{Main|Global serializability}} {{Main|Commitment ordering}}
As database systems have become [[Distributed database|distributed]], or started to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990s, and nowadays [[Grid computing]], [[Cloud computing]], and networks with [[smartphone]]s), some transactions have become distributed. A [[distributed transaction]] means that the transaction spans [[Process (computing)|processes]], and may span [[computer]]s and geographical sites. This generates a need in effective [[distributed concurrency control]] mechanisms. Achieving the Serializability property of a distributed system's schedule (see ''[[Serializability#Distributed serializability|Distributed serializability]]'' and ''[[Global serializability]]'' (''Modular serializability'')) effectively poses special challenges typically not met by most of the regular serializability mechanisms, originally designed to operate locally. This is especially due to a need in costly distribution of concurrency control information amid communication and computer [[latency (engineering)|latency]]. The only known general effective technique for distribution is Commitment ordering, which was disclosed publicly in 1991 (after being [[patent]]ed). '''[[Commitment ordering]]''' (Commit ordering, CO; [[#Raz92|Raz 1992]]) means that transactions' chronological order of commit events is kept compatible with their respective [[Serializability#Testing conflict serializability|precedence order]]. CO does not require the distribution of concurrency control information and provides a general effective solution ([[Reliability engineering|reliable]], high-performance, and [[Scalability|scalable]]) for both distributed and global serializability, also in a heterogeneous environment with database systems (or other transactional objects) with different (any) concurrency control mechanisms.&lt;ref name=Bern2009/&gt; CO is indifferent to which mechanism is utilized, since it does not interfere with any transaction operation scheduling (which most mechanisms control), and only determines the order of commit events. Thus, CO enables the efficient distribution of all other mechanisms, and also the distribution of a mix of different (any) local mechanisms, for achieving distributed and global serializability. The existence of such a solution has been considered "unlikely" until 1991, and by many experts also later, due to misunderstanding of the [[Commitment ordering#Summary|CO solution]] (see [[Global serializability#Quotations|Quotations]] in ''[[Global serializability]]''). An important side-benefit of CO is [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|automatic distributed deadlock resolution]]. Contrary to CO, virtually all other techniques (when not combined with CO) are prone to [[Deadlock#Distributed deadlock|distributed deadlocks]] (also called global deadlocks) which need special handling. CO is also the name of the resulting schedule property: A schedule has the CO property if the chronological order of its transactions' commit events is compatible with the respective transactions' [[Serializability#Testing conflict serializability|precedence (partial) order]].

[[Two-phase locking|SS2PL]] mentioned above is a variant (special case) of CO and thus also effective to achieve distributed and global serializability. It also provides automatic distributed deadlock resolution (a fact overlooked in the research literature even after CO's publication), as well as Strictness and thus Recoverability. Possessing these desired properties together with known efficient locking based implementations explains SS2PL's popularity. SS2PL has been utilized to efficiently achieve Distributed and Global serializability since the 1980, and has become the [[de facto standard]] for it. However, SS2PL is blocking and constraining (pessimistic), and with the proliferation of distribution and utilization of systems different from traditional database systems (e.g., as in [[Cloud computing]]), less constraining types of CO (e.g., [[Commitment ordering#Distributed optimistic CO (DOCO)|Optimistic CO]]) may be needed for better performance.

'''Comments:'''
# The ''Distributed conflict serializability'' property in its general form is difficult to achieve efficiently, but it is achieved efficiently via its special case ''Distributed CO'': Each local component (e.g., a local DBMS) needs both to provide some form of CO, and enforce a special ''vote ordering strategy'' for the ''[[Two-phase commit protocol]]'' (2PC: utilized to commit [[distributed transaction]]s). Differently from the general Distributed CO, ''Distributed SS2PL'' [[Commitment ordering#Strong strict two phase locking (SS2PL)|exists automatically when all local components are SS2PL based]] (in each component CO exists, implied, and the vote ordering strategy is now met automatically). This fact has been known and utilized since the 1980s (i.e., that SS2PL exists globally, without knowing about CO) for efficient Distributed SS2PL, which implies Distributed serializability and strictness (e.g., see [[#Raz92|Raz 1992]], page 293; it is also implied in [[#Bern87|Bernstein et al. 1987]], page 78). Less constrained Distributed serializability and strictness can be efficiently achieved by Distributed [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]], or by a mix of SS2PL based and SCO based local components.
# About the references and Commitment ordering: ([[#Bern87|Bernstein et al. 1987]]) was published before the discovery of CO in 1990. The CO schedule property is called ''[[The History of Commitment Ordering#Dynamic atomicity|Dynamic atomicity]]'' in ([[#Lynch1993|Lynch et al. 1993]], page 201). CO is described in ([[#Weikum2001|Weikum and Vossen 2001]], pages 102, 700), but the description is partial and misses [[Commitment ordering#Summary|CO's essence]]. ([[#Raz92|Raz 1992]]) was the first refereed and accepted for publication article about CO algorithms (however, publications about an equivalent Dynamic atomicity property can be traced to 1988). Other [[Commitment ordering#References|CO articles]] followed. (Bernstein and Newcomer 2009)&lt;ref name=Bern2009/&gt; note CO as one of the four major concurrency control methods, and CO's ability to provide interoperability among other methods.

=====Distributed recoverability=====
Unlike Serializability, ''Distributed recoverability'' and ''Distributed strictness'' can be achieved efficiently in a straightforward way, similarly to the way Distributed CO is achieved: In each database system they have to be applied locally, and employ a vote ordering strategy for the [[Two-phase commit protocol]] (2PC; [[#Raz92|Raz 1992]], page 307).

As has been mentioned above, Distributed [[Two-phase locking|SS2PL]], including Distributed strictness (recoverability) and Distributed [[commitment ordering]] (serializability), automatically employs the needed vote ordering strategy, and is achieved (globally) when employed locally in each (local) database system (as has been known and utilized for many years; as a matter of fact locality is defined by the boundary of a 2PC participant ([[#Raz92|Raz 1992]]) ).

====Other major subjects of attention====
The design of concurrency control mechanisms is often influenced by the following subjects:

=====Recovery=====
{{Main|Data recovery}}
All systems are prone to failures, and handling ''[[Data recovery|recovery]]'' from failure is a must. The properties of the generated schedules, which are dictated by the concurrency control mechanism, may affect the effectiveness and efficiency of recovery. For example, the Strictness property (mentioned in the section [[Concurrency control#Recoverability|Recoverability]] above) is often desirable for an efficient recovery.

=====Replication=====
{{Main|Replication (computer science)}}
For high availability database objects are often ''[[Replication (computer science)|replicated]]''. Updates of replicas of a same database object need to be kept synchronized. This may affect the way concurrency control is done (e.g., Gray et al. 1996&lt;ref name=Gray1996&gt;{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 |author2=Helland, P. |author3=[[Patrick O'Neil|O'Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | conference = The dangers of replication and a solution
 | title = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173&#8211;182
 | conference-url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}&lt;/ref&gt;).

=== See also ===
* [[Schedule (computer science)|Schedule]]
* [[Isolation (computer science)]]
* [[Distributed concurrency control]]
* [[Global concurrency control]]

===References===
*&lt;cite id=Bern87&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''] (free PDF download), Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 &lt;/cite&gt;
*&lt;cite id=Weikum01&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8 &lt;/cite&gt;
*&lt;cite id=Lynch1993&gt;[[Nancy Lynch]], Michael Merritt, William Weihl, Alan Fekete (1993): [http://www.elsevier.com/wps/find/bookdescription.cws_home/680521/description#description ''Atomic Transactions in Concurrent and Distributed Systems ''], Morgan Kauffman (Elsevier), August 1993, ISBN 978-1-55860-104-8, ISBN 1-55860-104-X &lt;/cite&gt;
*&lt;cite id=Raz92&gt;[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ([http://www.vldb.org/conf/1992/P292.PDF  PDF]), ''Proceedings of the Eighteenth International Conference on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) &lt;/cite&gt;

===Footnotes===
{{Reflist}}

== Concurrency control in operating systems ==
{{Expand section|date=December 2010}}
[[Computer multitasking|Multitasking]] operating systems, especially [[real-time operating system]]s, need to maintain the illusion that all tasks running on top of them are all running at the same time, even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on. Such multitasking is fairly simple when all tasks are independent from each other. However, when several tasks try to use the same resource, or when tasks try to share information, it can lead to confusion and inconsistency. The task of [[concurrent computing]] is to solve that problem. Some solutions involve "locks" similar to the locks used in databases, but they risk causing problems of their own such as [[deadlock]]. Other solutions are [[Non-blocking algorithm]]s and [[Read-copy-update]].

=== See also ===
* [[Linearizability]]
* [[Mutual exclusion]]
* [[Semaphore (programming)]]
* [[Lock (computer science)]]
* [[Software transactional memory]]
* [[Transactional Synchronization Extensions]]

===References===
*  Andrew S. Tanenbaum, Albert S Woodhull (2006): ''Operating Systems Design and Implementation, 3rd Edition'', [[Prentice Hall]], ISBN 0-13-142938-8
* {{cite book | last = Silberschatz | first = Avi |author2=Galvin, Peter |author3=Gagne, Greg | title = Operating Systems Concepts, 8th edition | publisher = [[John Wiley &amp; Sons]] | year = 2008 | isbn = 0-470-12872-0 }}

{{Databases}}

{{DEFAULTSORT:Concurrency Control}}
[[Category:Concurrency control| ]]
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]</text>
      <sha1>n6eq33kdj9uuwigzvrqtq2adn579g2r</sha1>
    </revision>
  </page>
  <page>
    <title>Database transaction</title>
    <ns>0</ns>
    <id>233953</id>
    <revision>
      <id>735112713</id>
      <parentid>735112652</parentid>
      <timestamp>2016-08-18T17:51:28Z</timestamp>
      <contributor>
        <username>ThePlatypusofDoom</username>
        <id>28032930</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/97.78.143.201|97.78.143.201]] ([[User talk:97.78.143.201|talk]]) to last revision by HakanIST. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9458" xml:space="preserve">{{Refimprove|date=August 2010}}

A '''transaction''' symbolizes a unit of work performed within a [[database management system]] (or similar system) against a database, and treated in a coherent and reliable way independent of other transactions. A transaction generally represents any change in database. Transactions in a database environment have two main purposes:

# To provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failure, when execution stops (completely or partially) and many operations upon a database remain uncompleted, with unclear status.
# To provide isolation between programs accessing a database concurrently. If this isolation is not provided, the programs' outcomes are possibly erroneous.

A database transaction, by definition, must be [[Atomicity (database systems)|atomic]], [[Consistency (database systems)|consistent]], [[Isolation (database systems)|isolated]] and [[Durability (database systems)|durable]].&lt;ref&gt;[http://msdn.microsoft.com/en-us/library/aa366402(VS.85).aspx A transaction is a group of operations that are atomic, consistent, isolated, and durable (ACID).]&lt;/ref&gt; Database practitioners often refer to these properties of database transactions using the acronym [[ACID]].

Transactions provide an "all-or-nothing" proposition, stating that each work-unit performed in a database must either complete in its entirety or have no effect whatsoever. Further, the system must isolate each transaction from other transactions, results must conform to existing constraints in the database, and transactions that complete successfully must get written to durable storage.

==Purpose==
[[Database]]s and other data stores which treat the [[data integrity|integrity]] of data as paramount often include the ability to handle transactions to maintain the integrity of data. A single transaction consists of one or more independent units of work, each reading and/or writing information to a database or other data store. When this happens it is often important to ensure that all such processing leaves the database or data store in a consistent state.

Examples from [[Double-entry bookkeeping system|double-entry accounting systems]] often illustrate the concept of transactions. In double-entry accounting every debit requires the recording of an associated credit. If one writes a check for $100 to buy groceries, a transactional double-entry accounting system must record the following two entries to cover the single transaction:

# Debit $100 to Groceries Expense Account
# Credit $100 to Checking Account

A transactional system would make both entries pass or both entries would fail. By treating the recording of multiple entries as an atomic transactional unit of work the system maintains the integrity of the data recorded. In other words, nobody ends up with a situation in which a debit is recorded but no associated credit is recorded, or vice versa.

==Transactional databases==
A '''transactional database''' is a [[DBMS]] where write transactions on the database are able to be rolled back if they are not completed properly (e.g. due to power or connectivity loss).

Most {{As of|2008|alt=modern}}  [[relational database management system]]s fall into the category of databases that support transactions.

In a database system a transaction might consist of one or more data-manipulation statements and queries, each reading and/or writing information in the database. Users of [[database system]]s consider [[Data consistency|consistency]] and [[data integrity|integrity]] of data as highly important. A simple transaction is usually issued to the database system in a language like [[Structured Query Language|SQL]] wrapped in a transaction, using a pattern similar to the following:

# Begin the transaction
# Execute a set of data manipulations and/or queries
# If no errors occur then commit the transaction and end it
# If errors occur then rollback the transaction and end it

If no errors occurred during the execution of the transaction then the system commits the transaction. A transaction commit operation applies all data manipulations within the scope of the transaction and persists the results to the database. If an error occurs during the transaction, or if the user specifies a [[Rollback (data management)|rollback]] operation, the data manipulations within the transaction are not persisted to the database. In no case can a partial transaction be committed to the database since that would leave the database in an inconsistent state.

Internally, multi-user databases store and process transactions, often by using a transaction [[identifier|ID]] or XID.

There are multiple varying ways for transactions to be implemented other than the simple way documented above. [[Nested transaction]]s, for example, are transactions which contain statements within them that start new transactions (i.e. sub-transactions). ''Multi-level transactions'' are a variant of nested transactions where the sub-transactions take place at different levels of a layered system architecture (e.g., with one operation at the database-engine level, one operation at the operating-system level) &lt;ref&gt;Beeri, C., Bernstein, P.A., and Goodman, N. A model for concurrency in nested transactions systems. Journal of the ACM, 36(1):230-269, 1989&lt;/ref&gt; Another type of transaction is the [[compensating transaction]].

===In SQL===
Transactions are available in most SQL database implementations, though with varying levels of robustness. (MySQL, for example, does not support transactions in the [[MyISAM]] storage engine, which was its default storage engine before version 5.5.)

A transaction is typically started using the command &lt;code&gt;BEGIN&lt;/code&gt; (although the SQL standard specifies &lt;code&gt;START TRANSACTION&lt;/code&gt;). When the system processes a &lt;code&gt;[[Commit (SQL)|COMMIT]]&lt;/code&gt; statement, the transaction ends with successful completion.  A &lt;code&gt;[[Rollback (data management)|ROLLBACK]]&lt;/code&gt; statement can also end the transaction, undoing any work performed since &lt;code&gt;BEGIN TRANSACTION&lt;/code&gt;. If [[autocommit]] was disabled using &lt;code&gt;START TRANSACTION&lt;/code&gt;, autocommit will also be re-enabled at the transaction's end.

One can set the [[Isolation (database systems)|isolation level]] for individual transactional operations as well as globally. At the READ COMMITTED level, the result of any work done after a transaction has commenced, but before it has ended, will remain invisible to other database-users until it has ended. At the lowest level (READ UNCOMMITTED), which may occasionally be used to ensure high concurrency, such changes will be visible.

==Object databases==
Relational databases traditionally comprise tables with fixed size fields and thus records. Object databases comprise variable sized [[Binary large object|blobs]] (possibly incorporating a [[mime-type]] or [[Serializable (databases)|serialized]]). The fundamental similarity though is the start and the [[Commit (data management)|commit]] or [[Rollback (data management)|rollback]].

After starting a transaction, database records or objects are locked, either read-only or read-write. Actual reads and writes can then occur. Once the user (and application) is happy, any changes are committed or rolled-back [[Atomicity (database systems)|atomically]], such that at the end of the transaction there is no [[Consistency (database systems)|inconsistency]].

==Distributed transactions==
Database systems implement [[distributed transaction]]s as transactions against multiple applications or hosts. A distributed transaction enforces the ACID properties over multiple systems or data stores, and might include systems such as databases, file systems, messaging systems, and other applications. In a distributed transaction a coordinating service ensures that all parts of the transaction are applied to all relevant systems. As with database and other transactions, if any part of the transaction fails, the entire transaction is rolled back across all affected systems.

==Transactional filesystems==
The [[Namesys]] [[Reiser4]] filesystem for [[Linux]]&lt;ref&gt;[http://namesys.com/v4/v4.html#committing namesys.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; supports transactions, and as of [[Microsoft]] [[Windows Vista]], the Microsoft [[NTFS]] filesystem&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/library/default.asp?url=/library/en-us/fileio/fs/portal.asp|title=MSDN Library|publisher=|accessdate=16 October 2014}} {{dead link|date=May 2014}}&lt;/ref&gt; supports [[distributed transaction]]s across networks.

==See also==
* [[Concurrency control]]

==References==
{{reflist}}

==Further reading==
* &lt;cite id=Bern2009&gt;[[Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4 &lt;/cite&gt;
* Gerhard Weikum, Gottfried Vossen (2001), ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, ISBN 1-55860-508-8

==External links==
* [[c2:TransactionProcessing]]
* https://docs.oracle.com/database/121/CNCPT/transact.htm#CNCPT016
* https://docs.oracle.com/cd/B28359_01/server.111/b28318/transact.htm

{{Databases}}

{{DEFAULTSORT:Database Transaction}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>mcyw5huf8xuouqlezj0t6hcrm0jgx62</sha1>
    </revision>
  </page>
  <page>
    <title>Durability (database systems)</title>
    <ns>0</ns>
    <id>245944</id>
    <revision>
      <id>617369164</id>
      <parentid>596543787</parentid>
      <timestamp>2014-07-17T20:53:29Z</timestamp>
      <contributor>
        <ip>73.183.64.164</ip>
      </contributor>
      <comment>change link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1240" xml:space="preserve">{{Unreferenced|date=December 2009}}
In [[database system]]s, '''durability''' is the [[ACID]] property which guarantees that [[database transaction|transaction]]s that have committed will survive permanently. 
For example, if a flight booking reports that a seat has successfully been booked, then the seat will remain booked even if the system crashes.

Durability can be achieved by flushing the transaction's log records to [[non-volatile storage]] before acknowledging commitment.

In [[distributed transaction]]s, all participating servers must coordinate before commit can be acknowledged. This is usually done by a [[two-phase commit protocol]].

Many DBMSs implement durability by writing transactions into a [[transaction log]] that can be reprocessed to recreate the system state right before any later failure. A transaction is deemed committed only after it is entered in the log.

==See also==
* [[Atomicity (database systems)|Atomicity]]
* [[Consistency (database systems)|Consistency]]
* [[Isolation (database systems)|Isolation]]
* [[Relational database management system]]

{{DEFAULTSORT:Durability (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]


{{Compu-sci-stub}}
{{Database-stub}}</text>
      <sha1>qw9bdp9zmxukd37b2t1h6xv6wfsxogi</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise Data Planning</title>
    <ns>0</ns>
    <id>25209669</id>
    <revision>
      <id>653212811</id>
      <parentid>653204355</parentid>
      <timestamp>2015-03-23T21:10:15Z</timestamp>
      <contributor>
        <username>Mdd</username>
        <id>113850</id>
      </contributor>
      <minor />
      <comment>+ Link(s)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4912" xml:space="preserve">{{Orphan|date=October 2010}}
{| style="width: 80%; margin: 0 0 0 10%; border-collapse: collapse; background: #FBFBFB; border: 1px solid #aaa; border-left: 10px solid #f28500;"
|-
| style="width: 52px; padding: 2px 0px 2px 0.5em; text-align: center;" | [[Image:Newspaper nicu buculei 01.svg|50px]]
| style="padding: 0.25em 0.5em;" | '''This article or section reads like an [[Wikipedia:What Wikipedia is not#Wikipedia is not a soapbox|advertisement]] for EDMworks.'''&lt;br/&gt;To meet Wikipedia's [[:Category:Wikipedia style guidelines|quality standards]] and comply with Wikipedia's [[Wikipedia:Neutral point of view|neutral point of view]] policy, it may require [[Wikipedia:Cleanup|cleanup]].
|}
'''Enterprise Data Planning''' is the starting point for enterprise wide change. It states the destination and describes how you will get there. It defines benefits, costs and potential risks.  It provides measures to be used along the way to judge progress and adjust the journey according to changing circumstances.

[[Data]] is fundamental to investment enterprises. Effective, economic management of data underpins operations and enables transformations needed to satisfy customer demands, competition and regulation. Data warehouse(s) and other aspects of the overall [[data architecture]] are critical to the enterprise.

EDMworks has created a strategic data planning approach for the Investment Sector.  It consists of a planning process, planning intranets, templates and training materials.

EDMworks planning process is based on the belief that extensive domain knowledge significantly shortens planning iterations and enables progressively higher quality plans to be produced and implemented.&lt;ref name=hull&gt;Introduction to Futures and Options Markets (John Hull) 1995&lt;/ref&gt;&lt;ref name=taylor&gt;Mastering Derivatives Markets (Francesca Taylor) 2007&lt;/ref&gt;  This approach drives the development of an effective and economic Enterprise Data Architecture.

Enterprise Data Planning is based on proven business disciplines.&lt;ref name=stutely&gt;The Definitive Business Plan (Richard Stutely) 2002&lt;/ref&gt; Key architectural layers for data and applications are then added in order to provide an enterprise wide understanding of the uses and interdependencies of data.&lt;ref name=tozer&gt;Planning for Effective Business Information Systems ([[Edwin E. Tozer]]) 1998&lt;/ref&gt; This enables the definition of the core components of the [[Enterprise data management|EDM]] plan:

* Industry structure and business objectives
* Assessment of systems and services
* Target architecture for applications, data and infrastructure
* Target organization structures
* Systems, database, infrastructure and organizational plans
* Business case, costs, benefits, results and risks.

EDMworks uses several components from the Open Systems Group [[TOGAF]] enterprise systems planning process. [[TOGAF]] acts as an extension to good business planning methods to provide a framework for the development of the systems and data architectural components.

==History==

[[James Martin (author)|James Martin]] was one of the pathfinders in data planning methodologies.  He was one of the first to identify data as being an enterprise wide asset that required management.  He developed a series of tools and methods to support that process.&lt;ref name=Martin&gt;Martin 1982&lt;/ref&gt;
 
Most of the large consulting firms developed their own methods to address the same basic issue.  Frequently, their approaches were incorporated into their own branded system development methodologies that encompassed the complete systems development life-cycle. 
 
Others, such as [[Edwin E. Tozer|Ed Tozer]], developed more focused offerings that dealt with the complexities of extracting key business needs from senior management and then defining relevant architectural visions for the specific enterprise.&lt;ref name=tozer/&gt;
 
From these various sources, the concepts of Business, Data, Applications and Technology Architectures emerged. 
 
The Open Group Architectural Framework (TOGAF) has taken this work forward and has established a sound method in TOGAF version 9.
 
EDMworks approach is to adopt these planning and architectural practices as a basis and then add two additional dimensions to the planning and implementation focus:
* Domain knowledge of the Investments sector.  Investments is a complex global industry with a common set of characteristics about clients, information vendors, competition and regulation.  Domain knowledge significantly improves the quality of the planning and implementation processes
* Development of people and teams.  Change is a major feature of in any Enterprise Data Management program and people and teams both need development in order to make EDM effective throughout an organization.

== References ==
{{reflist}}

== External links ==
* [http://www.edmworks.com Enterprise Data Management works]

[[Category:Data management]]</text>
      <sha1>6h9g13zdkfw2kxq11xxsx1zucydblvz</sha1>
    </revision>
  </page>
  <page>
    <title>Network transparency</title>
    <ns>0</ns>
    <id>1786529</id>
    <revision>
      <id>732099751</id>
      <parentid>705601954</parentid>
      <timestamp>2016-07-29T16:34:00Z</timestamp>
      <contributor>
        <username>Kvng</username>
        <id>910180</id>
      </contributor>
      <comment>separate section for X as discussed on talk page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3718" xml:space="preserve">{{Multiple issues|
{{Unreferenced|date=December 2009}}
{{Lead rewrite|date=July 2015}}
}}

'''Network transparency''' in its most general sense refers to the ability of a protocol to transmit data over the [[computer network|network]] in a manner which is [[Transparency (human&#8211;computer interaction)|transparent]] (invisible) to those using the applications that are using the protocol.

==X Window==
The term is often partially correctly applied in the context of the [[X Window System]], which is able to transmit graphical data over the network and integrate it seamlessly with applications running and displaying locally; however, certain extensions of the X Window System are not capable of working over the network.&lt;ref&gt;{{cite web |url=https://lwn.net/Articles/553415/ |title=The Wayland Situation: Facts About X vs. Wayland (Phoronix) |publisher=[[LWN.net]] |date=23 June 2013}}&lt;/ref&gt;

==Databases==
In a [[centralized database system]], the only available resource that needs to be shielded from the user is the data (that is, the [[storage system]]). In a [[Distributed Database Management System|distributed DBMS]], a second resource needs to be managed in much the same manner: the [[computer network|network]]. Preferably, the user should be protected from the network operational details. Then there would be no difference between database applications that would run on the centralized database and those that would run on a distributed one. This kind of transparency is referred to as '''network transparency''' or '''distribution transparency'''. From a [[database management system]] (DBMS) perspective, distribution transparency requires that users do not have to specify where data is located.

Some have separated distribution transparency into location transparency and naming transparency.

Location transparency in commands used to perform a task is independent both of the locations of the data, and of the system on which an operation is carried out.

Naming transparency means that a unique name is provided for each object in the database.

==Firewalls==
{{See also|Proxy server#Transparent proxy}}

Transparency in firewall technology can be defined at the networking (IP or [[Internet Layer|Internet layer]]) or at the [[Internet Layer|application layer]].

Transparency at the IP layer means the client targets the real IP address of the server. If a connection is non-transparent, then the client targets an intermediate host (address), which could be a proxy or a caching server. IP layer transparency could be also defined from the point of server's view. If the connection is transparent, the server sees the real client IP. If it is non-transparent, the server sees the IP of the intermediate host.

Transparency at the application layer means the client application uses the protocol in a different way. An example of a transparent HTTP request for a server:

&lt;syntaxhighlight lang="text"&gt;
GET / HTTP/1.1
Host: example.org
Connection: Keep-Alive
&lt;/syntaxhighlight&gt;

An example non-transparent HTTP request for a proxy (cache):

&lt;syntaxhighlight lang="text"&gt;
GET http://foo.bar/ HTTP/1.1
Proxy-Connection: Keep-Alive
&lt;/syntaxhighlight&gt;

Application layer transparency is symmetric when the same working mode is used on both the sides. The transparency is asymmetric when the firewall (usually a proxy) converts server type requests to proxy type or vice versa. 

Transparency at the IP layer does not mean automatically application layer transparency.

== See also ==
{{Portal|Computer networking}}

* [[Data independence]]
* [[Replication transparency]]

==References==
{{Reflist}}

{{DEFAULTSORT:Network Transparency}}
[[Category:Telecommunications]]
[[Category:Data management]]</text>
      <sha1>c3s8z1z7xf9eor96ujg8165t0gob27x</sha1>
    </revision>
  </page>
  <page>
    <title>Uniform data access</title>
    <ns>0</ns>
    <id>1610890</id>
    <revision>
      <id>578107650</id>
      <parentid>578107434</parentid>
      <timestamp>2013-10-21T12:57:02Z</timestamp>
      <contributor>
        <ip>142.240.200.7</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="765" xml:space="preserve">{{Unreferenced stub|auto=yes|date=December 2009}}
'''Uniform data access''' is a computational concept describing an even-ness of connectivity and controllability across numerous target data sources.  

Necessary to fields such as [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), it is most often used regarding analysis of disparate data types and data sources, which must be rendered into a [[uniform information representation]], and generally must appear [[wiktionary:Homogenous|homogenous]] to the analysis tools&#8212;when the data being analyzed is typically [[heterogeneous]] and widely varying in size, type, and original representation.{{DEFAULTSORT:Uniform Data Access}}
[[Category:Data management]]


{{Comp-sci-stub}}</text>
      <sha1>dqnsnfnf9ezgfpdbbhtb89g3e1fu5vu</sha1>
    </revision>
  </page>
  <page>
    <title>Data auditing</title>
    <ns>0</ns>
    <id>6890125</id>
    <revision>
      <id>746124190</id>
      <parentid>737583262</parentid>
      <timestamp>2016-10-25T11:34:13Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/73.254.190.33|73.254.190.33]] ([[User talk:73.254.190.33|talk]]) to last revision by Addbot. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="416" xml:space="preserve">{{Unreferenced|date=December 2009}}
'''Data auditing''' is the process of conducting a data audit to assess how company's data is fit for given purpose.  This involves [[data profiling|profiling]] the data and assessing the impact of [[data quality|poor quality data]] on the organization's performance and profits.

{{DEFAULTSORT:Data Auditing}}
{{Tech-stub}}

[[Category:Data management]]
[[Category:Data quality]]</text>
      <sha1>3rv7ibk1b3gv2peq2shcumpkl2s8b19</sha1>
    </revision>
  </page>
  <page>
    <title>Automated tiered storage</title>
    <ns>0</ns>
    <id>5732700</id>
    <revision>
      <id>755676226</id>
      <parentid>733446991</parentid>
      <timestamp>2016-12-19T14:51:23Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[data migration]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6859" xml:space="preserve">'''Automated tiered storage''' (also '''automated storage tiering''') is the automated progression or demotion of data across different tiers (types) of storage devices and media. The movement of data takes place in an automated way with the help of a software or embedded firmware and is assigned to the related media according to performance and capacity requirements. More advanced implementations include the ability to define rules and policies that dictate if and when data can be moved between the tiers, and in many cases provides the ability to pin data to tiers permanently or for specific periods of time. Implementations vary, but are classed into two broad categories: pure software based implementations that run on general purpose processors supporting most forms of general purpose storage media and embedded automated tiered storage controlled by firmware as part of a closed embedded storage system such as a SAN disk array. Software Defined Storage architectures commonly include a component of tiered storage as part of their primary functions.

In the most general definition, Automated Tiered Storage is a form of Hierarchical Storage Management. However, the term automated tiered storage has emerged to accommodate newer forms of real-time performance optimized [[data migration]] driven by the proliferation of solid state disks and storage class memory. Furthermore, where traditional HSM systems act on files and move data between storage tiers in a batch, scheduled like fashion, automated storage tiered systems are capable of operating at sub-file level both in batch and real-time modes. In the case of the latter, data is moved almost as soon as it enters the storage system or relocated based on its activity levels within seconds of data being accessed, whereas more traditional tiering tends to operate on an hourly, daily or even weekly schedule. Some more background on the relative differences between HSM, ILM and automated tiered storage is available at SNIA web site.&lt;ref&gt;http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf&lt;/ref&gt; A general comparison of different approaches can also be found in this 'comparison article on auto tiered storage'[http://searchstorage.techtarget.co.uk/feature/Automated-storage-tiering-product-comparison].

==OS and Software Based Automated Tiered Storage==
Most server oriented software automated tiered storage vendors offer tiering as a component of a general storage virtualization stack offering, an example being [[Microsoft]] with their Tiered Storage Spaces.&lt;ref&gt;https://redmondmag.com/articles/2013/08/30/windows-storage-tiering.aspx?m=1&lt;/ref&gt; However, automated tiering is now becoming a common part of industry standard operating systems such as Linux and Microsoft Windows, and in the case of consumer PCs, Apple OSX with its Fusion Drive.&lt;ref&gt;[http://www.apple.com/imac/performance/ "Apple iMac Performance Website"] October 24, 2012.&lt;/ref&gt; This solution allowed a single SSD and hard disk drive to be combined into a single automated tiered storage drive that ensured that the most frequently accessed data was stored on the SSD portion of the virtual disk. A more OS agnostic version was introduced by Enmotus which supports real-time tiering with its FuzeDrive product for Linux and Windows operating systems, extending support to storage class memory offerings such as NVDIMM and NVRAM devices.&lt;ref&gt;http://cdn2.hubspot.net/hub/486631/file-2586107985-pdf/PDFs/20111129_S2-102_Mills.pdf?t=1447892865729&lt;/ref&gt;

==SAN Based Tiered Storage==
An example of automated tiered storage in a hardware storage array is a feature called Data Progression from Compellent Technologies. Data Progression has the capability to transparently move blocks of data between different drive types and RAID groups such as RAID 10 and RAID 5. The blocks are part of the "same virtual volume even as they span different RAID groups and drive types.  Compellent can do this because they keep metadata about every block -- which allows them to keep track of each block and its associations.".&lt;ref&gt;[http://blogs.computerworld.com/compellent_ilm  Tony Asaro, Computerworld. "Compellent-Intelligent Tiered Storage."] January 19, 2009.&lt;/ref&gt; Another strong example of SAN based tiering is DotHill's Autonomous Tiered Storage which moves data between tiers of storage within the SAN disk array with decisions made every few seconds".&lt;ref&gt;[https://www.dothill.com/solutions/tiered-data-storage/ "Hybrid Data Storage Solution with SSD and HDD Tiers"]&lt;/ref&gt;

== Automated Tiered Storage vs. SSD Caching ==
While tiering solutions and caching may look the same on the surface, the fundamental differences lie in the way the faster storage is utilized and the algorithms used to detect and accelerate frequently accessed data. SSD caching operates much like SRAM-DRAM caches do i.e. they make a copy of frequently accessed blocks of data, for example in 4K cache page sizes, and store the copy in the SSD and use this copy instead of the original data source on the slower backend storage. Every time a storage IO occurs, the caching software look to see if a copy of this data already exists using a variety of algorithms and service the host request from the SSD if it is found. The SSD is used in this case as a lookaside device as it is not part of the primary storage. While some good caching algorithms can demonstrate native SSD performance on reads and short bursts of writes, caching typically operates well below the maximum sustainable rate of the underlying SSD devices as overhead CPU cycles are introduced during the host IO commands that increasingly impact performance as the amount of data cached grows. Tiering on the other hand operates very differently. Using the specific case of SSDs, once data is identified as frequently used, the identified blocks of data are moved in the background to the SSD and not copied as the SSD is being utilized as a primary storage tier, not a look aside copy area. When the data is subsequently accessed, the IOs occur at or near the native performance of the SSDs as there area are few if any CPU cycles needed to do the simpler virtual to physical addressing translations.&lt;ref&gt;[http://searchsolidstatestorage.techtarget.com/tip/Tiering-vs-caching-in-flash-based-storage-systems] "Tiering vs. caching in flash-based storage systems"&lt;/ref&gt;

== See also ==
* [[Hierarchical storage management]]
* [[Tiered storage]]

== References ==
* Russ Taddiken &#8211; Senior Storage Architect (2006). Automating Data Movement Between Storage Tiers. Retrieved from the UW Records Management Web site: http://www.compellent.com/
&lt;references/&gt;

== External links ==
* http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf

[[Category:Data management]]</text>
      <sha1>558tmm1uhtfsnglm1ynaxyh5q84guoq</sha1>
    </revision>
  </page>
  <page>
    <title>SQL programming tool</title>
    <ns>0</ns>
    <id>12821559</id>
    <revision>
      <id>693449642</id>
      <parentid>667028191</parentid>
      <timestamp>2015-12-02T17:25:30Z</timestamp>
      <contributor>
        <username>Lfstevens</username>
        <id>1686264</id>
      </contributor>
      <minor />
      <comment>/* Debugging */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3884" xml:space="preserve">{{unreferenced|date=April 2010}}
In the field of [[software]], '''[[SQL]] programming tools''' provide platforms for [[database administrator]]s (DBAs) and [[application software|application]] developers to perform daily tasks efficiently and accurately.

Database administrators and application developers often face constantly changing environments  which they rarely completely control. Many changes result from new development projects or from modifications to existing code, which, when deployed to production, do not always produce the expected result.

For organizations to better manage development projects and the [[team]]s that develop code, suppliers of SQL programming tools normally provide more than facility to the database administrator or application developer to aid in database management and in quality [[Software deployment|code-deployment]] practices.

==Features==

SQL programming tools may include the following features:

===SQL editing===

SQL editors allow users to edit and execute SQL statements. They may support the following features:

* cut, copy, paste, undo, redo, find (and replace), bookmarks
* block indent, print, save file, uppercase/lowercase
* keyword highlighting
* auto-completion
* access to frequently used files
* output of query result
* editing query-results
* committing and rolling-back transactions
* inside cut paper

===Object browsing===

Tools may display information about [[database object]]s relevant to developers or to database administrators. Users may:

* view object descriptions
* view object [[Data Definition Language|definition]]s (DDL)
* create database objects
* enable and disable [[database trigger|trigger]]s and [[database constraints|constraint]]s
* recompile valid or invalid objects
* query or edit [[Table (database)|table]]s and [[View (database)|view]]s

Some tools also provide features to display dependencies among objects, and allow users to expand these dependent objects recursively (for example: packages may reference views, views generally reference tables, super/subtypes, and so on).

===Session browsing===

Database administrators and application developers can use session browsing tools to view the current activities of each user in the database. They can check the resource-usage of individual users, statistics information, locked objects and the current running SQL of each individual session.

===User-security management===

DBAs can create, edit, delete, disable or enable user-accounts in the database using security-management tools. DBAs can also assign [[database role|role]]s, system [[Privilege (computing)|privilege]]s, object privileges, and [[database storage|storage]]-quotas to users.

===Debugging===

Some tools offer features for the debugging of [[stored procedure]]s: [[program animation|Step In]], Step Over, Step Out, Run Until Exception, [[Breakpoint]]s, View &amp; Set Variables, View Call Stack, and so on. Users can debug any program-unit without making any modification to it, including triggers and [[object type]]s.

===Performance monitoring===

Monitoring tools may show the database resources &#8212; usage summary, service time summary, recent activities, top sessions, session history or top SQL &#8212; in easy-to-read graphs. Database administrators can easily monitor the health of various components in the monitoring instance. Application developers may also make use of such tools to diagnose and correct application-performance problems as well as improve SQL server performance.

===Test Data===

Test data generation tools can populate the database by realistic test data for server or client side testing purposes. Also, this kind of software can upload sample BLOB files to database.

==See also==
* [[Comparison of database tools]]

{{DEFAULTSORT:Sql Programming Tool}}
[[Category:Data management]]
[[Category:Relational database management systems]]</text>
      <sha1>hc4fhvthkd8taqtnydbfe6vniepcyac</sha1>
    </revision>
  </page>
  <page>
    <title>Association rule learning</title>
    <ns>0</ns>
    <id>577053</id>
    <revision>
      <id>763087018</id>
      <parentid>761499698</parentid>
      <timestamp>2017-02-01T07:01:13Z</timestamp>
      <contributor>
        <username>Monitor333</username>
        <id>16957266</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33758" xml:space="preserve">{{Redirect|OneR|filmmaking technique|Long take}}
{{machine learning bar}}
'''Association rule learning''' is a [[rule-based machine learning]] method for discovering interesting relations between variables in large databases.  It is intended to identify strong rules discovered in databases using some measures of interestingness.&lt;ref name="piatetsky"&gt;Piatetsky-Shapiro, Gregory (1991), ''Discovery, analysis, and presentation of strong rules'', in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., ''Knowledge Discovery in Databases'', AAAI/MIT Press, Cambridge, MA.&lt;/ref&gt;  Based on the concept of strong rules, [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]], [[Tomasz Imieli&#324;ski]] and Arun Swami &lt;ref name="mining"&gt;{{Cite book | last1 = Agrawal | first1 = R. | last2 = Imieli&#324;ski | first2 = T. | last3 = Swami | first3 = A. | doi = 10.1145/170035.170072 | chapter = Mining association rules between sets of items in large databases | title = Proceedings of the 1993 ACM SIGMOD international conference on Management of data  - SIGMOD '93 | pages = 207 | year = 1993 | isbn = 0897915925 | pmid =  | pmc = }}&lt;/ref&gt; introduced association rules for discovering regularities between products in large-scale transaction data recorded by [[point-of-sale]] (POS) systems in supermarkets. For example, the rule &lt;math&gt;\{\mathrm{onions, potatoes}\} \Rightarrow \{\mathrm{burger}\}&lt;/math&gt; found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional [[pricing]] or [[product placement]]s. In addition to the above example from [[market basket analysis]] association rules are employed today in many application areas including [[Web usage mining]], [[intrusion detection]], [[Continuous production]], and [[bioinformatics]]. In contrast with [[sequence mining]], association rule learning typically does not consider the order of items either within a transaction or across transactions.

== Definition ==
{|class="wikitable" style="float: right; margin-left: 1em;"
|+ Example database with 5 transactions and 5 items
|-
! transaction ID !! milk !! bread !! butter !! beer !! diapers
|-
| 1 || 1 || 1 || 0 || 0 || 0
|-
| 2 || 0 || 0 || 1 || 0 || 0
|-
| 3 || 0 || 0 || 0 || 1 || 1
|-
| 4 || 1 || 1 || 1 || 0 || 0
|-
| 5 || 0 || 1 || 0 || 0 || 0
|-
|}

Following the original definition by Agrawal et al.&lt;ref name="mining" /&gt; the problem of association rule mining is defined as:

Let &lt;math&gt;I=\{i_1, i_2,\ldots,i_n\}&lt;/math&gt; be a set of &lt;math&gt;n&lt;/math&gt; binary attributes called ''items''.

Let &lt;math&gt;D = \{t_1, t_2, \ldots, t_m\}&lt;/math&gt; be a set of transactions called the ''database''.

Each ''transaction'' in &lt;math&gt;D&lt;/math&gt; has a unique transaction ID and contains a subset of the items in &lt;math&gt;I&lt;/math&gt;.

A ''rule'' is defined as an implication of the form:

&lt;math&gt;X \Rightarrow Y&lt;/math&gt;, where &lt;math&gt;X, Y \subseteq I&lt;/math&gt;.

In Agrawal et al.,&lt;ref name="mining" /&gt; a ''rule'' is defined only between a set and a single item, &lt;math&gt;X \Rightarrow i_j&lt;/math&gt; for &lt;math&gt;i_j \in I&lt;/math&gt;.

Every rule is composed by two different sets of items, also known as ''itemsets'', &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt;, where &lt;math&gt;X&lt;/math&gt; is called ''antecedent'' or left-hand-side (LHS) and &lt;math&gt;Y&lt;/math&gt; ''consequent'' or right-hand-side (RHS).

To illustrate the concepts, we use a small example from the supermarket domain. The set of items is &lt;math&gt;I= \{\mathrm{milk, bread, butter, beer, diapers}\}&lt;/math&gt; and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.

An example rule for the supermarket could be &lt;math&gt;\{\mathrm{butter, bread}\} \Rightarrow \{\mathrm{milk}\}&lt;/math&gt; meaning that if butter and bread are bought, customers also buy milk.

Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant{{Citation needed|date=February 2015}}, and datasets often contain thousands or millions of transactions.

== Useful Concepts ==
In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.

Let &lt;math&gt;X&lt;/math&gt; be an itemset, &lt;math&gt;X \Rightarrow Y&lt;/math&gt; an association rule and &lt;math&gt;T&lt;/math&gt; a set of transactions of a given database.

=== Support ===
Support is an indication of how frequently the itemset appears in the database.

The support of &lt;math&gt;X&lt;/math&gt; with respect to &lt;math&gt;T&lt;/math&gt; is defined as the proportion of transactions &lt;math&gt;t&lt;/math&gt; in the database which contains itemset &lt;math&gt;X&lt;/math&gt;.

&lt;math&gt;\mathrm{supp}(X) = \frac{|\{t \in T; X \subseteq t\}|}{|T|}&lt;/math&gt;

In the example database, the itemset &lt;math&gt;X=\{\mathrm{beer, diapers}\}&lt;/math&gt; has a support of &lt;math&gt;1/5=0.2&lt;/math&gt; since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of &lt;math&gt;\mathrm{supp}()&lt;/math&gt; is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).&lt;ref name=":0"&gt;{{Cite journal|last=Hahsler|first=Michael|date=2005|title=Introduction to arules &#8211; A computational environment for mining association rules and frequent item sets|url=https://mran.revolutionanalytics.com/web/packages/arules/vignettes/arules.pdf|journal=Journal of Statistical Software|doi=|pmid=|access-date=}}&lt;/ref&gt;

=== Confidence ===
Confidence is an indication of how often the rule has been found to be true.

The ''confidence'' value of a rule, &lt;math&gt;X \Rightarrow Y&lt;/math&gt; , with respect to a set of transactions &lt;math&gt;T&lt;/math&gt;, is the proportion of the transactions that contains &lt;math&gt;X&lt;/math&gt; which also contains &lt;math&gt;Y&lt;/math&gt;.

Confidence is defined as:

&lt;math&gt;\mathrm{conf}(X \Rightarrow Y) = \mathrm{supp}(X \cup Y) / \mathrm{supp}(X)&lt;/math&gt;.

For example, the rule &lt;math&gt;\{\mathrm{butter,  bread}\} \Rightarrow \{\mathrm{milk}\}&lt;/math&gt; has a confidence of &lt;math&gt;0.2/0.2=1.0&lt;/math&gt; in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).

Note that &lt;math&gt;\mathrm{supp}(X \cup Y)&lt;/math&gt; means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of [[Event (probability theory)|events]] and not sets of items. We can rewrite &lt;math&gt;\mathrm{supp}(X \cup Y)&lt;/math&gt; as the joint probability &lt;math&gt;P(E_X \cup E_Y)&lt;/math&gt;, where &lt;math&gt;E_X&lt;/math&gt; and &lt;math&gt;E_Y&lt;/math&gt; are the events that a transaction contains itemset &lt;math&gt;X&lt;/math&gt; or &lt;math&gt;Y&lt;/math&gt;, respectively.&lt;ref name="michael.hahsler.net"&gt;Michael Hahsler (2015).  A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules. http://michael.hahsler.net/research/association_rules/measures.html&lt;/ref&gt;

Thus confidence can be interpreted as an estimate of the conditional probability &lt;math&gt;P(E_Y | E_X)&lt;/math&gt;, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.&lt;ref name=":0" /&gt;&lt;ref name="hipp"&gt;{{Cite journal | last1 = Hipp | first1 = J. | last2 = G&#252;ntzer | first2 = U. | last3 = Nakhaeizadeh | first3 = G. | title = Algorithms for association rule mining --- a general survey and comparison | doi = 10.1145/360402.360421 | journal = ACM SIGKDD Explorations Newsletter | volume = 2 | pages = 58 | year = 2000 | pmid =  | pmc = }}&lt;/ref&gt;

=== Lift ===
The ''[[lift (data mining)|lift]]'' of a rule is defined as:

&lt;math&gt; \mathrm{lift}(X\Rightarrow Y) = \frac{ \mathrm{supp}(X \cup Y)}{ \mathrm{supp}(X) \times \mathrm{supp}(Y) } &lt;/math&gt;

or the ratio of the observed support to that expected if X and Y were [[Independence (probability theory)|independent]].{{citation needed|reason=I couldn't find this in 'Witten: Data Mining - Practical Machine Learning Tools and Techniques'|date=May 2016}}

For example, the rule &lt;math&gt;\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}&lt;/math&gt; has a lift of &lt;math&gt;\frac{0.2}{0.4 \times 0.4} = 1.25 &lt;/math&gt;.

If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.

If the lift is &gt; 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.

The value of lift is that it considers both the confidence of the rule and the overall data set.&lt;ref name=":0" /&gt;

=== Conviction ===
The ''conviction'' of a rule is defined as &lt;math&gt; \mathrm{conv}(X\Rightarrow Y) =\frac{ 1 - \mathrm{supp}(Y) }{ 1 - \mathrm{conf}(X\Rightarrow Y)}&lt;/math&gt;.

For example, the rule &lt;math&gt;\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}&lt;/math&gt; has a conviction of &lt;math&gt;\frac{1 - 0.4}{1 - 0.5} = 1.2 &lt;/math&gt;, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule &lt;math&gt;\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}&lt;/math&gt; would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.

== Process ==
[[File:FrequentItems.png|thumb|Frequent itemset lattice, where the color of the box indicates how many transactions contain the combination of items. Note that lower levels of the lattice can contain at most the minimum number of their parents' items; e.g. {ac} can have only at most &lt;math&gt;min(a,c)&lt;/math&gt; items. This is called the ''downward-closure property''.&lt;ref name="mining" /&gt;]] Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:
# A minimum support threshold is applied to find all ''frequent itemsets'' in a database.
# A minimum confidence constraint is applied to these frequent itemsets in order to form rules.
While the second step is straightforward, the first step needs more attention.

Finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations).  The set of possible itemsets is the [[power set]] over &lt;math&gt;I&lt;/math&gt; and has size &lt;math&gt;2^n-1&lt;/math&gt; (excluding the empty set which is not a valid itemset). Although the size of the power-set grows exponentially in the number of items &lt;math&gt;n&lt;/math&gt; in &lt;math&gt;I&lt;/math&gt;, efficient search is possible using the '''''downward-closure property''''' of support&lt;ref name="mining" /&gt;&lt;ref&gt;{{cite book |last1=Tan |first1=Pang-Ning |last2=Michael |first2=Steinbach |last3=Kumar |first3=Vipin |title=Introduction to Data Mining |publisher=[[Addison-Wesley]] |year=2005 |isbn=0-321-32136-7 |chapter=Chapter 6. Association Analysis: Basic Concepts and Algorithms |chapterurl=http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf }}&lt;/ref&gt; (also called ''anti-monotonicity''&lt;ref name="pei"&gt;Pei, Jian; Han, Jiawei; and Lakshmanan, Laks V. S.; ''Mining frequent itemsets with convertible constraints'', in ''Proceedings of the 17th International Conference on Data Engineering, April 2&#8211;6, 2001, Heidelberg, Germany'', 2001, pages 433-442&lt;/ref&gt;) which guarantees that for a frequent itemset, all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori&lt;ref name="apriori"&gt;Agrawal, Rakesh; and Srikant, Ramakrishnan; [http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf ''Fast algorithms for mining association rules in large databases''], in Bocca, Jorge B.; Jarke, Matthias; and Zaniolo, Carlo; editors, ''Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994'', pages 487-499&lt;/ref&gt; and Eclat&lt;ref name="eclat"&gt;{{Cite journal | last1 = Zaki | first1 = M. J. | title = Scalable algorithms for association mining | doi = 10.1109/69.846291 | journal = IEEE Transactions on Knowledge and Data Engineering | volume = 12 | issue = 3 | pages = 372&#8211;390 | year = 2000 | pmid =  | pmc = }}&lt;/ref&gt;) can find all frequent itemsets.

==History==
The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,&lt;ref name="mining" /&gt; which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, it is possible that what is now called "association rules" is similar to what appears in  the 1966 paper&lt;ref name="guha_oldest"&gt;H&#225;jek, Petr; Havel, Ivan; Chytil, Metod&#283;j; ''The GUHA method of automatic hypotheses determination'', Computing 1 (1966) 293-308&lt;/ref&gt; on GUHA, a general data mining method developed by [[Petr H&#225;jek]] et al.&lt;ref name="pospaper"&gt;H&#225;jek, Petr; Feglar, Tomas; Rauch, Jan; and Coufal, David; ''The GUHA method, data preprocessing and mining'', Database Support for Data Mining Applications, Springer, 2004, ISBN 978-3-540-22479-2&lt;/ref&gt;

An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with &lt;math&gt;\mathrm{supp}(X)&lt;/math&gt; and &lt;math&gt;\mathrm{conf}(X \Rightarrow Y)&lt;/math&gt; greater than user defined constraints.&lt;ref&gt;{{cite journal|last1=Webb|first1=Geoffrey|title=A Machine Learning Approach to Student Modelling|journal=Proceedings of the Third Australian Joint Conference on Artificial Intelligence (AI 89)|date=1989|pages=195&#8211;205}}&lt;/ref&gt;

== Alternative measures of interestingness ==
&lt;!-- would be nice to explain each measure --&gt;
In addition to confidence, other measures of ''interestingness'' for rules have been proposed. Some popular measures are:

*  All-confidence&lt;ref name="allconfidence"&gt;Omiecinski, Edward R.; ''Alternative interest measures for mining associations in databases'', IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003&lt;/ref&gt;
* Collective strength&lt;ref name="collectivestrength"&gt;Aggarwal, Charu C.; and Yu, Philip S.; ''A new framework for itemset generation'', in ''PODS 98, Symposium on Principles of Database Systems, Seattle, WA, USA, 1998'', pages 18-24&lt;/ref&gt;
*  Conviction&lt;ref name="brin-dynamic-itemset1"&gt;Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; ''Dynamic itemset counting and implication rules for market basket data'', in ''SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997'', pp. 255-264&lt;/ref&gt;
*  Leverage&lt;ref name="leverage"&gt;Piatetsky-Shapiro, Gregory; ''Discovery, analysis, and presentation of strong rules'', Knowledge Discovery in Databases, 1991, pp. 229-248&lt;/ref&gt;
*  Lift (originally called interest)&lt;ref name="brin-dynamic-itemset2"&gt;Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; ''Dynamic itemset counting and implication rules for market basket data'', in ''SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997'', pp. 265-276&lt;/ref&gt;

Several more measures are presented and compared by Tan et al.&lt;ref name="measurescomp"&gt;Tan, Pang-Ning; Kumar, Vipin; and Srivastava, Jaideep; ''Selecting the right objective measure for association analysis'', Information Systems, 29(4):293-313, 2004&lt;/ref&gt; and by Hahsler.&lt;ref name="michael.hahsler.net"/&gt; Looking for techniques that can model what the user has known  (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."

== Statistically sound associations ==

One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations.  These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance.  For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side.  There are approximately 1,000,000,000,000 such rules.  If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association.  If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules.  Statistically sound association discovery&lt;ref&gt;Webb, Geoffrey I. (2007); ''Discovering Significant Patterns'', Machine Learning 68(1), Netherlands: Springer, pp. 1-33 [http://link.springer.com/article/10.1007%2Fs10994-007-5006-x online access]&lt;/ref&gt;&lt;ref&gt;Gionis, Aristides; [[Heikki Mannila|Mannila, Heikki]]; Mielik&#228;inen, Taneli; and Tsaparas, Panayiotis; ''Assessing Data Mining Results via Swap Randomization'', ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 1, Issue 3 (December 2007), Article No. 14&lt;/ref&gt; controls this risk, in most cases reducing the risk of finding ''any'' spurious associations to a user-specified significance levels.

== Algorithms ==

Many algorithms for generating association rules were presented over time.

Some well-known algorithms are [[Apriori algorithm|Apriori]], Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.

=== Apriori algorithm ===
{{Main article|Apriori algorithm}}

Apriori&lt;ref name="apriori" /&gt; uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.

=== Eclat algorithm ===

Eclat&lt;ref name="eclat" /&gt; (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm using set intersection. It is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality-enhancing properties. It was first introduced by Zaki, Parthasarathy, Li and Ogihara in a series of papers written in 1997.

Mohammed Javeed Zaki, Srinivasan Parthasarathy, M. Ogihara, Wei Li:
New Algorithms for Fast Discovery of Association Rules. KDD 1997.

Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, Wei Li:
Parallel Algorithms for Discovery of Association Rules. Data Min. Knowl. Discov. 1(4): 343-373 (1997)

=== FP-growth algorithm ===

FP stands for frequent pattern.&lt;ref&gt;{{cite journal|last1=Han|title=Mining Frequent Patterns Without Candidate Generation|journal=Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data|date=2000|volume=SIGMOD '00|pages=1&#8211;12|doi=10.1145/342009.335372}}&lt;/ref&gt;

In the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances.
Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly.
Items in each instance that do not meet minimum coverage threshold are discarded.
If many instances share most frequent items, FP-tree provides high compression close to tree root.

Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database.
Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition.
New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts.
Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.

Once the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.&lt;ref&gt;Witten, Frank, Hall: Data mining practical machine learning tools and techniques, 3rd edition&lt;/ref&gt;

=== Others ===

==== AprioriDP ====
AprioriDP&lt;ref name="dharmesh2013" /&gt; utilizes [[Dynamic Programming]]  in Frequent itemset mining. The working principle is to eliminate the candidate generation like FP-tree, but it stores support count in specialized data structure instead of tree.

==== Context Based Association Rule Mining Algorithm ====
{{Main article|Context Based Association Rules}}

CBPNARM is an algorithm, developed in 2013, to mine association rules on the basis of context. It uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set.

==== Node-set-based algorithms ====
FIN,&lt;ref name="deng2014" /&gt; PrePost &lt;ref name="deng2012" /&gt; and PPV &lt;ref name="deng2010" /&gt; are three algorithms based on node sets. They use nodes in a coding FP-tree to represent itemsets, and employ a depth-first search strategy to discovery frequent itemsets using "intersection" of node sets.

==== GUHA procedure ASSOC ====

[[GUHA]] is a general method for exploratory data analysis that has theoretical foundations in [[observational calculi]].&lt;ref name="ObservationalCalculi"&gt;Rauch, Jan; ''Logical calculi for knowledge discovery in databases'', in ''Proceedings of the First European Symposium on Principles of Data Mining and Knowledge Discovery'', Springer, 1997, pp. 47-57&lt;/ref&gt;

The ASSOC procedure&lt;ref&gt;{{cite book |last=H&#225;jek |first=Petr |author2=Havr&#225;nek, Tom&#225;&#353; |title=Mechanizing Hypothesis Formation: Mathematical Foundations for a General Theory |publisher=Springer-Verlag |year=1978 |isbn=3-540-08738-9 |url=http://www.cs.cas.cz/hajek/guhabook/ }}&lt;/ref&gt; is a GUHA method which mines for generalized association rules using fast [[bitstring]]s operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.

==== OPUS search ====

OPUS is an efficient algorithm for rule discovery    that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.&lt;ref name=OPUS&gt;Webb, Geoffrey I. (1995); ''OPUS: An Efficient Admissible Algorithm for Unordered Search'', Journal of Artificial Intelligence Research 3, Menlo Park, CA: AAAI Press, pp. 431-465 [http://www.cs.washington.edu/research/jair/abstracts/webb95a.html online access]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; Initially used to find rules for a fixed consequent&lt;ref name="OPUS" /&gt;&lt;ref name="Bayardo"&gt;{{Cite journal |doi=10.1023/A:1009895914772 |last1=Bayardo |first1=Roberto J., Jr. |last2=Agrawal |first2=Rakesh |last3=Gunopulos |first3=Dimitrios |year=2000 |title=Constraint-based rule mining in large, dense databases |journal=Data Mining and Knowledge Discovery |volume=4 |issue=2 |pages=217&#8211;240 }}&lt;/ref&gt; it has subsequently been extended to find rules with any item as a consequent.&lt;ref name="webb"&gt;Webb, Geoffrey I. (2000); ''Efficient Search for Association Rules'', in Ramakrishnan, Raghu; and Stolfo, Sal; eds.; ''Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2000), Boston, MA'', New York, NY: The Association for Computing Machinery, pp. 99-107 [http://www.csse.monash.edu/~webb/Files/Webb00b.pdf online access]&lt;/ref&gt; OPUS search is the core technology in the popular Magnum Opus association discovery system.

== Lore ==
A famous story about association rule mining is the "beer and diaper" story.  A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.&lt;ref name="dss"&gt;http://www.dssresources.com/newsletters/66.php&lt;/ref&gt; Daniel Powers says:&lt;ref name="dss" /&gt;

&lt;blockquote&gt;In 1992, Thomas Blischok, manager of a retail consulting group at [[Teradata]], and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.&lt;/blockquote&gt;

== Other types of association mining ==

'''Multi-Relation Association Rules''': Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations ''live in'', ''nearby'' and ''humid'': &#8220;Those who ''live in'' a place which is ''near by'' a city with ''humid'' climate type and also are ''younger'' than 20 -&gt; their ''health condition'' is good&#8221;. Such association rules are extractable from RDBMS data or semantic web data.&lt;ref name="MRAR: Mining Multi-Relation Association Rules"&gt;Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; ''MRAR: Mining Multi-Relation Association Rules'', Journal of Computing and Security, 1, no. 2 (2014)&lt;/ref&gt;

'''[[Context Based Association Rules]]''' is a form of association rule. '''Context Based Association Rules''' claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables. For example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month.This might be because of abnormal context i.e. salary is drawn at the start of the month &lt;ref name="Context Based Positive and Negative Spatio Temporal Association Rule Mining"&gt;Shaheen, M; Shahbaz, M; and Guergachi, A; ''Context Based Positive and Negative Spatio Temporal Association Rule Mining'', Elsevier Knowledge-Based Systems, Jan 2013, pp. 261-273&lt;/ref&gt;

'''[[Contrast set learning]]''' is a form of associative learning. '''Contrast set learners''' use rules that differ meaningfully in their distribution across subsets.&lt;ref name="webb03"&gt;{{cite conference
 | author = GI Webb and S. Butler and D. Newlands
 | year = 2003
 | title = On Detecting Differences Between Groups
 | conference = KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
| url= http://portal.acm.org/citation.cfm?id=956781
 }}
&lt;/ref&gt;&lt;ref name="busy"&gt;Menzies, Tim; and Hu, Ying; ''Data Mining for Very Busy People'', IEEE Computer, October 2003, pp. 18-25&lt;/ref&gt;

'''Weighted class learning''' is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.

'''High-order pattern discovery''' facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.
&lt;ref name="discovere"&gt;{{cite journal |last=Wong |first=Andrew K.C. |author2=Wang, Yang |title=High-order pattern discovery from discrete-valued data |journal=IEEE Transactions on Knowledge and Data Engineering (TKDE) |year=1997 |pages=877&#8211;893 }}&lt;/ref&gt;

'''[[K-optimal pattern discovery]]''' provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.

'''Approximate Frequent Itemset''' mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.&lt;ref&gt;Jinze Liu, Susan Paulsen, Xing Sun, Wei Wang, Andrew Nobel, J. P. (2006). Mining approximate frequent itemsets in the presence of noise: Algorithm and analysis. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

'''Generalized Association Rules''' hierarchical taxonomy (concept hierarchy)

'''Quantitative Association Rules''' categorical and quantitative data
&lt;ref name="quantminer"&gt;{{cite journal |last=Salleb-Aouissi |first=Ansaf |author2=Vrain, Christel|author3= Nortet, Cyril |title=QuantMiner: A Genetic Algorithm for Mining Quantitative Association Rules |journal=International Joint Conference on Artificial Intelligence (IJCAI) |year=2007 |pages=1035&#8211;1040 }}&lt;/ref&gt;

'''Interval Data Association Rules''' e.g. partition the age into 5-year-increment ranged

'''Maximal Association Rules'''

'''Sequential pattern mining ''' discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.&lt;ref name="sequence"&gt;Zaki, Mohammed J. (2001); ''SPADE: An Efficient Algorithm for Mining Frequent Sequences'', Machine Learning Journal, 42, pp. 31&#8211;60&lt;/ref&gt;

'''Sequential Rules''' discovering relationships between items while considering the time ordering. It is generally applied on a sequence database. For example, a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and CD-Roms, later bought a webcam, with a given confidence and support.

'''Subspace Clustering''', a specific type of [[Clustering high-dimensional data]], is in many variants also based on the downward-closure property for specific clustering models.&lt;ref name="ZimekAssent2014"&gt;{{cite journal|last1=Zimek|first1=Arthur|last2=Assent|first2=Ira|last3=Vreeken|first3=Jilles|title=Frequent Pattern Mining Algorithms for Data Clustering|year=2014|pages=403&#8211;423|doi=10.1007/978-3-319-07821-2_16}}&lt;/ref&gt;

'''Warmr '''is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.&lt;ref&gt;{{cite journal | pmid = 11272703 | volume=15 | issue=2 | title=Warmr: a data mining tool for chemical data. | date=Feb 2001 | journal=J Comput Aided Mol Des | pages=173&#8211;81}}&lt;/ref&gt;

==See also==
* [[Sequence mining]]
* [[Production system (computer science)]]
* [[Learning classifier system]]
* [[Rule-based machine learning]]

==References==
{{reflist|3|refs=
&lt;ref name="deng2014"&gt;Z. H. Deng and S. L. Lv. Fast mining frequent itemsets using Nodesets.[http://www.sciencedirect.com/science/article/pii/S0957417414000463]. Expert Systems with Applications, 41(10): 4505&#8211;4512, 2014.&lt;/ref&gt;
&lt;ref name="deng2012"&gt;Z. H. Deng, Z. Wang&#65292;and J. Jiang. A New Algorithm for Fast Mining Frequent Itemsets Using N-Lists [http://info.scichina.com:8084/sciFe/EN/abstract/abstract508369.shtml]. SCIENCE CHINA Information Sciences, 55 (9): 2008 - 2030, 2012.&lt;/ref&gt;
&lt;ref name="deng2010"&gt;Z. H. Deng and Z. Wang.   A New Fast Vertical Method for Mining Frequent Patterns [http://www.tandfonline.com/doi/abs/10.1080/18756891.2010.9727736]. International Journal of Computational Intelligence Systems, 3(6): 733 - 744, 2010.&lt;/ref&gt;
&lt;ref name="dharmesh2013"&gt;D. Bhalodiya, K. M. Patel and C. Patel. An Efficient way to Find Frequent Pattern with Dynamic Programming Approach [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6780102&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6780102]. NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING, NUiCONE-2013, 28-30 NOVEMBER, 2013.&lt;/ref&gt;
}}

==External links==

===Bibliographies===
* [http://www.uco.es/grupos/kdis/ARMBibliography Extensive Bibliography on Association Rules] by J.M. Luna
* [http://michael.hahsler.net/research/bib/association_rules/ Annotated Bibliography on Association Rules] by M. Hahsler
* [http://www.statsoft.com/textbook/association-rules/ Statsoft Electronic Statistics Textbook: Association Rules] by [[Dell]] Software

{{Prone to spam|date=February 2016}}
{{Z148}}&lt;!--     {{No more links}}

       Please be cautious adding more external links.

Wikipedia is not a collection of links and should not be used for advertising.

     Excessive or inappropriate links will be removed.

 See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.

If there are already suitable links, propose additions or replacements on
the article's talk page, or submit your link to the relevant category at
DMOZ (dmoz.org) and link there using {{Dmoz}}.

--&gt;

{{DEFAULTSORT:Association Rule Learning}}
[[Category:Data management]]
[[Category:Data mining]]</text>
      <sha1>abkqiap84goeq7rscp0uefd8ygwja4u</sha1>
    </revision>
  </page>
  <page>
    <title>Novell File Reporter</title>
    <ns>0</ns>
    <id>28208309</id>
    <revision>
      <id>722194301</id>
      <parentid>721088348</parentid>
      <timestamp>2016-05-26T15:09:09Z</timestamp>
      <contributor>
        <ip>137.65.45.116</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3329" xml:space="preserve">{{Infobox software
|name                       = Novell File Reporter
|logo                       =
|screenshot                 =
|caption                    =
|collapsible                =
|author                     =
|developer                  = [[Novell]]
|released                   = {{Start date|2010|01}}
|discontinued               =
|latest release version     = 2.6.1
|latest release date        = {{Start date|2015|10|02}}
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       =
|operating system           =
|platform                   =
|size                       =
|language                   =
|status                     =
|genre                      = [[System Software]]
|license                    =
|website                    = [http://www.novell.com/products/file-reporter/ Novell File Reporter]
}}

'''Novell File Reporter''' (a.k.a. '''NFR''') is software that allows network administrators to identify files stored on the network and generates reports regarding the size of individual files, [[File format | file type]], when files were last accessed, and where duplicates exist. Additionally, the File Reporter tracks storage volume capacity and usage. It is a component of the [[Novell File Management Suite]].

==How It Works==

Novell File Reporter examines and reports on terabytes of data via a central reporting engine (NFR Engine) and distributed agents (NFR Agents). &lt;ref&gt;{{Citation| title = Novell File Reporter Reports on Terabytes of Data | url= http://www.novell.com/products/file-reporter/terabytes_data.html | accessdate = 31 July 2010}}&lt;/ref&gt; The NFR Engine schedules the scans of file instances conducted by NFR Agents, processes and compiles the scans for reporting purposes, and provides report information to the user interface. 

In addition to the standard reports &lt;ref&gt;{{Citation| title = Novell File Report Standard Reports | url= http://www.novell.com/products/file-reporter/standard_reports.html | accessdate = 31 July 2010}}&lt;/ref&gt; it can generate, the NFR Engine can also produce "trigger reports" in response to specific events (a server volume crossing a capacity threshold, for example). Accordingly, the NFR Engine monitors the data gathered by the NFR Agents in order to identify these "triggers."

The NFR Engine when working in either [[Novell eDirectory | eDirectory]] or [[Active Directory]] connects to the directory via a Directory Services Interface (DSI) and thus can monitor and check file permissions.&lt;ref&gt;{{Citation | last = Huber | first= Matthias | journal= Linux Magazine | title= Novell File Management Suite Optimizes Storage | date= 25 January 2010| url=http://www.linux-magazine.com/Online/News/Novell-File-Management-Suite-Optimizes-Storage | accessdate= 31 July 2010}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*[http://www.novell.com/products/file-reporter/technicalinfo/ Novell File Reporter: Product page] Overview, features, and technical information
*[http://www.novell.com/documentation/filereporter2/ Novell File Reporter: Documentation]
*[http://www.filereportersupport.com/nfr/ Novell File Reporter: Support]

{{Novell}}

[[Category:Novell]]
[[Category:Novell software]]
[[Category:Storage software]]
[[Category:Data management]]</text>
      <sha1>a8z2brbq58opkdwvulpism9emblrn19</sha1>
    </revision>
  </page>
  <page>
    <title>Signed overpunch</title>
    <ns>0</ns>
    <id>16284001</id>
    <revision>
      <id>688112677</id>
      <parentid>662925342</parentid>
      <timestamp>2015-10-29T18:43:29Z</timestamp>
      <contributor>
        <username>SoledadKabocha</username>
        <id>16861812</id>
      </contributor>
      <minor />
      <comment>/* top */ fix anchor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1553" xml:space="preserve">{{Refimprove|date=March 2008}}
A '''signed overpunch''' is a code used to store the [[Sign (mathematics)|sign]] of a number by changing the last digit. It is used in [[COBOL]], especially when using [[EBCDIC]]. Its purpose is to save a character that would otherwise be used by the sign digit.&lt;ref name="EncycArk"&gt;{{Cite web|url=http://www.3480-3590-data-conversion.com/article-signed-fields.html |title=Tech Talk, COBOL Tutorials, EBCDIC to ASCII Conversion of Signed Fields |accessdate=2008-03-15}}&lt;/ref&gt;  The code is derived from the [[punched card#IBM 80-column punched card formats and character codes|Hollerith Punched Card Code]], where both a digit and a sign can be entered in the same card column.

==The codes==
{| class="wikitable" style="text-align:center"
! Code !! Digit !! Sign
|-
| } || 0 || &amp;minus;
|-
| J || 1 || &amp;minus;
|-
| K || 2 || &amp;minus;
|-
| L || 3 || &amp;minus;
|-
| M || 4 || &amp;minus;
|-
| N || 5 || &amp;minus;
|-
| O || 6 || &amp;minus;
|-
| P || 7 || &amp;minus;
|-
| Q || 8 || &amp;minus;
|-
| R || 9 || &amp;minus;
|-
| { || 0 || +
|-
| A || 1 || +
|-
| B || 2 || +
|-
| C || 3 || +
|-
| D || 4 || +
|-
| E || 5 || +
|-
| F || 6 || +
|-
| G || 7 || +
|-
| H || 8 || +
|-
| I || 9 || +
|}

==Examples==
10} is -100&lt;BR&gt;
45A is 451

Decimal points are usually implied and not explicitly stated in the text. Using numbers with two decimal digits:

1000} is -100.00

==References==
{{Reflist}}

{{DEFAULTSORT:Signed Overpunch}}
[[Category:Computer programming]]
[[Category:Punched card]]
[[Category:Data management]]
[[Category:History of software]]</text>
      <sha1>9alf6rgcmb2q45vkbbggl49bj4i8jik</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise manufacturing intelligence</title>
    <ns>0</ns>
    <id>10612614</id>
    <revision>
      <id>681177748</id>
      <parentid>567398012</parentid>
      <timestamp>2015-09-15T17:07:15Z</timestamp>
      <contributor>
        <ip>50.177.205.50</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1731" xml:space="preserve">{{unreferenced|date=February 2013}}
'''Enterprise manufacturing intelligence (EMI),''' or simply manufacturing intelligence (MI), is a term which applies to software used to bring a corporation's manufacturing-related data together from many sources for the purposes of reporting, analysis, visual summaries, and passing data between enterprise-level and plant-floor systems. As data is combined from multiple sources, it can be given a new structure or context that will help users find what they need regardless of where it came from.  The primary goal is to turn large amounts of manufacturing data into real knowledge and drive business results based on that knowledge.{{reflist}}

== Core functions ==

[[AMR Research]] has identified five core functions every Enterprise Manufacturing Intelligence application should possess:

* '''Aggregation:''' Making available data from many sources, most often databases.
* '''Contextualization:''' Providing a structure, or model, for the data that will help users find what they need.  Usually a folder tree utilizing a hierarchy such as the [[ISA-95]] standard.
* '''Analysis:''' Enabling users to analyze data across sources and especially across production sites.  This often includes the ability for true ''ad hoc'' reporting.
* '''Visualization:''' Providing tools to create visual summaries of the data to alert decision makers and call attention to the most important information of the moment.  The most common visualization tool is the [[Dashboard (business)|dashboard.]]
* '''Propagation:''' Automating the transfer of data from the plant-floor up to enterprise-level systems or vice versa.

{{DEFAULTSORT:Enterprise Manufacturing Intelligence}}
[[Category:Data management]]</text>
      <sha1>cxmso4m7611javhijtave4kis8thei5</sha1>
    </revision>
  </page>
  <page>
    <title>Data custodian</title>
    <ns>0</ns>
    <id>28192799</id>
    <revision>
      <id>718091612</id>
      <parentid>705417314</parentid>
      <timestamp>2016-05-01T13:40:13Z</timestamp>
      <contributor>
        <username>Themightyquill</username>
        <id>1212157</id>
      </contributor>
      <comment>removed [[Category:Library science]]; added [[Category:Library occupations]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3014" xml:space="preserve">{{more footnotes|date=August 2010}}
{{merge|Data steward|date=February 2016}}
In [[Data governance|Data Governance]] groups, responsibilities for data management are increasingly divided between the business process owners and information technology (IT) departments.  Two functional titles commonly used for these roles are [[Data steward|Data Steward]] and Data Custodian. 

Data Stewards are commonly responsible for data content, context, and associated business rules. Data Custodians are responsible for the safe custody, transport, storage of the data and implementation of business rules.&lt;ref&gt;Carnegie Mellon - Information Security Roles and Responsibilities, http://www.cmu.edu/iso/governance/roles/data-custodian.html&lt;/ref&gt;&lt;ref&gt;''Policies, Regulations and Rules: Data Management Procedures - REG 08.00.3 - Information Technology'', , NC State University, http://www.ncsu.edu/policies/informationtechnology/REG08.00.3.php&lt;/ref&gt; Simply put, Data Stewards are responsible for what is stored in a data field, while Data Custodians are responsible for the technical environment and database structure. Common job titles for data custodians are Database Administrator (DBA), Data Modeler, and ETL Developer.

==Data Custodian Responsibilities==
A data custodian ensures:
# Access to the data is authorized and controlled
# Data stewards are identified for each data set
# Technical processes sustain data integrity
# Processes exist for data quality issue resolution in partnership with Data Stewards
# Technical controls safeguard data
# Data added to data sets are consistent with the common data model
# Versions of Master Data are maintained along with the history of changes
# Change management practices are applied in maintenance of the database
# Data content and changes can be audited

==See also==
* [[Data governance]]
* [[Data steward]]

==References==
&lt;references&gt;&lt;/references&gt;

==Related Links==
* ''Establishing data stewards'', by Jonathan G. Geiger, Teradata Magazine Online, September 2008, http://www.teradata.com/tdmo/v08n03/Features/EstablishingDataStewards.aspx
* '' A Rose By Any Other Name &#8211; Titles In Data Governance'', by Anne Marie Smith, Ph.D., EIMInstitute.ORG Archives, Volume 1, Issue 13, March 2008, http://www.eiminstitute.org/library/eimi-archives/volume-1-issue-13-march-2008-edition/a-rose-by-any-other-name-2013-titles-in-data-governance

{{DEFAULTSORT:Data Custodian}}
[[Category:Information technology governance]]
[[Category:Data management]]
[[Category:Knowledge representation]]
[[Category:Library occupations]]
[[Category:Metadata]]
[[Category:Technical communication]]

[[ar:&#1605;&#1610;&#1578;&#1575;&#1583;&#1575;&#1578;&#1575;]]
[[cs:Metadata]]
[[da:Metadata]]
[[de:Data Steward]]
[[et:Metaandmed]]
[[es:Metadato]]
[[eo:Meta-dateno]]
[[fr:M&#233;tadonn&#233;e]]
[[it:Metadata]]
[[lv:Metadati]]
[[hu:Metaadat]]
[[nl:Metadata]]
[[ja:&#12513;&#12479;&#12487;&#12540;&#12479;]]
[[no:Metadata]]
[[pl:Metadane]]
[[pt:Metadados]]
[[ru:&#1052;&#1077;&#1090;&#1072;&#1076;&#1072;&#1085;&#1085;&#1099;&#1077;]]
[[fi:Metatieto]]
[[sv:Metadata]]
[[th:&#3648;&#3617;&#3607;&#3634;&#3604;&#3634;&#3605;&#3634;]]
[[vi:Metadata]]</text>
      <sha1>c3km9wtkh88l53mzxmp6mlo8mvf4gi6</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Storage area networks</title>
    <ns>14</ns>
    <id>30304657</id>
    <revision>
      <id>734498895</id>
      <parentid>588991844</parentid>
      <timestamp>2016-08-14T19:27:50Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category Computer storage to [[:Category:Computer data storage]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="255" xml:space="preserve">{{Commons category|Storage area networks}}
{{See also category|Computer storage buses}}
{{cat main|Storage area network}}

[[Category:Computer data storage]]
[[Category:Local area networks]]
[[Category:Data management]]
[[Category:Storage virtualization]]</text>
      <sha1>q4bq0jgbdtwjasu3nufy7cov6mwuklc</sha1>
    </revision>
  </page>
  <page>
    <title>Operational historian</title>
    <ns>0</ns>
    <id>8829912</id>
    <revision>
      <id>761769230</id>
      <parentid>749378592</parentid>
      <timestamp>2017-01-24T18:41:35Z</timestamp>
      <contributor>
        <ip>106.220.132.175</ip>
      </contributor>
      <comment>/* Commercial */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7056" xml:space="preserve">{{Refimprove|date=October 2013}}
{{Use British English|date=March 2014}}
'''Operational historian''' refers to a database software application that logs or historizes time-based process data.&lt;ref&gt;{{cite web|title=A Practical Guide to Process Data Historians and Process Information Systems|url=http://www.tappi.org/Downloads/unsorted/UNTITLED---PCEI99339pdf.aspx|publisher=TAPPI|accessdate=14 September 2012|author=R. H. (Rick) Meeker, Jr.|format=PDF|date=13 January 1999}}&lt;/ref&gt; Historian software is used to record trends and historical information about industrial processes for future reference. It captures plant management information about production status, performance monitoring, quality assurance, tracking and genealogy, and product delivery with enhanced data capture, data compression, and data presentation capabilities.&lt;ref name=GlobalspecArticle  &gt;{{cite web |url= http://www.globalspec.com/learnmore/industrial_engineering_software/industrial_controls_software/trending_historian_software |title= Globalspec Historian Article| accessdate=12 Jul 2012}}&lt;/ref&gt;

Operational historians are like enterprise historians but differ in that they are used by engineers on the plant floor rather than by business processes.{{citation needed|date=February 2015}} They are typically cheaper, lighter in weight, and easier to use and reconfigure than enterprise historians. Having an operational historian enables "at the source" analysis of the historical data that is not typically possible with enterprise historians.
Typically, these applications offer two layers of data access: through a dedicated SDK(Standard Development Kit)(sometimes in two different flavours: full administration API(Application Programming Interface) and high-speed read/write API), as well as user front-end tools (for instance, administration panels, engineering consoles or portal-like web clients).

Because these applications are designed to fulfil specific operation time requirements, their marketing materials often indicate that these are real-time database systems.&lt;ref name="example"&gt;[http://software.schneider-electric.com/products/wonderware/production-information-management/historian/ Wonderware Historian - Example of naming the operational historian the real-time database]&lt;/ref&gt; However, since such performance measurements are often executed for atomic operations (especially write operations), not necessarily whole transactions, not all of the operational historians must be in fact real-time databases.

Usual challenges the operational historians must address are as follows:
* data collection from real-time external systems,
* storage and archiving of very large volumes of data,
* tag organisation (typically [[Time series database|time series]], where a single sample contains the information about the time stamp, the value and the sample quality),
* basic data limit monitoring (alarms) and user prompts (messages),
* performance of read and write operations.

== Data access ==
As opposed to enterprise historians, the data access layer in the operational historian is designed to offer sophisticated data fetching modes without complex information analysis facilities. The following settings are typically available for data access operations:
* Data scope (single point, history based on time range, history based on sample count),
* Request modes (raw data, last-known value, aggregation, interpolation),
* Sampling (single point, all points without sampling, all points with interval sampling),
* Data omission (based on the sample quality, based on the sample value, based on the count).

Even though the operational historians are rarely [[relational database management system]]s, they often offer [[SQL]]-based interfaces to query the database. In most of such implementations, the dialect does not follow the SQL standard in order to provide syntax for specifying data access operations parameters.

== Notable software ==
=== Commercial ===
* [[ABB]] Decathlon History
* [[Aspen Technology]] InfoPlus.21 &lt;ref&gt;{{Citation | url = http://www.aspentech.com/aspenONE_MES_Brochure.pdf | format = PDF | publisher = Aspen Technology, Inc. | title = aspenONE MES Brochure | page = 2 | year = 2014 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[ENEA_AB|Enea]]'s Polyhedra Historian, a module of [[Polyhedra DBMS]]&lt;ref&gt;{{Citation | url = http://developer.polyhedra.com/polyhedra-features/historian | publisher = Enea AB |  title = Handling time-series data in Polyhedra IMDB | date = 11 May 2012 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[GE Intelligent Platforms]] Proficy Historian&lt;ref&gt;{{Citation | url = http://www.ge-ip.com/account/prepsend/file/Proficy_Historian_5-5.pdf | format = PDF | publisher = GE Intelligent Platforms, Inc. | title = Datasheet: Proficy Historian 5.5 | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Honeywell]] Uniformance PHD&lt;ref&gt;{{Citation | url = http://www.honeywellprocess.com/library/marketing/notes/uniformance-phd-pin.pdf | publisher = Honeywell International Inc. | title = Uniformance PHD Product Information Note | format = PDF | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Iconics ]] [http://www.iconics.com/Home/Products/Historians/Hyper-Historian.aspx Hyper Historian]
* [[Inductive Automation]] [[Ignition SCADA#SQL Bridge| SQL Bridge]] module of [[Ignition SCADA]]&lt;ref&gt;{{Citation | url = http://inductiveautomation.com/scada-software/scada-modules/sqlbridge | publisher = Inductive Automation | title = High-Powered Data Acquisition | year = 2014 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[National Instruments]] Citadel, used in [[LabVIEW]] DSC and other products&lt;ref&gt;{{Citation | url = http://www.ni.com/white-paper/6579/en/ | publisher = National Instruments Corp. |  title = Logging Data with National Instruments Citadel | date = 19 July 2012 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[OSIsoft]] - PI System
* [[Schneider Electric]] InStep Software [http://www.instepsoftware.com/instep-software-products/edna-enterprise-data-historian eDNA Real-Time Historian]
* [[Schneider Electric]] Wonderware Historian&lt;ref&gt;{{Citation | url =  http://global.wonderware.com/EN/PDF%20Library/Datasheet_Wonderware_Historian.pdf | publisher = Invensys Systems, Inc. | title = Wonderware Historian Software Datasheet | format = PDF | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Yokogawa]] Exaquantum Historian&lt;ref&gt;{{Citation | url = http://www.yokogawa.com/eu/pims/pdf/BU%20GMSCS0102-02E%20Exaquantum%20Bulletin%2072ppi.pdf | publisher = Yokogawa Marex Limited |  title = Exaquantum delivers Production Excellence | format = PDF | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Jaaji Technologies]] inSis Historian&lt;ref&gt;{{Citation | url = http://www.jaajitech.com/Infoview | publisher = Jaaji Software Technologies Private Limited |  title = Find, View and Analyze your process data from everywhere | year = 2014 | accessdate = 30 July 2014 }}&lt;/ref&gt;

==See also==
* [[Time series database]]
* [[Relational database management system]]

== References ==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>nn6i2b58bfobkmfkg00n5tsziuarvov</sha1>
    </revision>
  </page>
  <page>
    <title>Bitmap index</title>
    <ns>0</ns>
    <id>2017214</id>
    <revision>
      <id>750350473</id>
      <parentid>747607300</parentid>
      <timestamp>2016-11-19T05:01:29Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21156" xml:space="preserve">A '''bitmap index''' is a special kind of [[Index (database)|database index]] that uses [[Bit array|bitmap]]s.

Bitmap indexes have traditionally been considered to work well for ''low-cardinality columns'', which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use [[bit array]]s (commonly called bitmaps) and answer queries by performing [[bitwise operation|bitwise logical operation]]s on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional [[B-tree]] indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for [[online transaction processing]] applications.

Some researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.&lt;ref name="sharma"&gt;[http://www.oracle.com/technetwork/articles/sharma-indexes-093638.html Bitmap Index vs. B-tree Index: Which and When?], Vivek Sharma, Oracle Technical Network.&lt;/ref&gt;

Bitmap indexes are also useful in [[data warehousing]] applications for joining a large [[fact table]] to smaller [[dimension table]]s such as those arranged in a [[star schema]].

Bitmap based representation can also be used for representing a data structure which is labeled and directed attributed multigraph, used for queries in [[graph databases]].&lt;code&gt;[http://www.researchgate.net/publication/236593640_Efficient_graph_management_based_on_bitmap_indices Efficient graph management based on bitmap indices]&lt;/code&gt; article shows how bitmap index representation can be used to manage large dataset(billions of data points) and answer queries related to graph efficiently.

==Example==
Continuing the internet access example, a bitmap index may be logically viewed as follows:
{| class="wikitable" style="text-align:center; float:left"
|-
!rowspan=2| Identifier
!rowspan=2| HasInternet
!colspan=2| Bitmaps
|-
!Y !! N
|-
|1 || Yes || 1|| 0
|-
|2 || No || 0 || 1
|-
|3 || No || 0 || 1
|-
|4 || Unspecified || 0 || 0
|-
|5 || Yes || 1 || 0
|}

On the left, [[Identifier]] refers to the unique number assigned to each resident, HasInternet is the data to be indexed, the content of the bitmap index is shown as two columns under the heading ''bitmaps''. Each column in the left illustration is a ''bitmap'' in the bitmap index. In this case, there are two such bitmaps, one for "has internet" ''Yes'' and one for "has internet" ''No''. It is easy to see that each bit in bitmap ''Y'' shows whether a particular row refers to a person who has internet access. This is the simplest form of bitmap index. Most columns will have more distinct values. For example, the sales amount is likely to have a much larger number of distinct values. Variations on the bitmap index can effectively index this data as well. We briefly review three such variations.

Note: Many of the references cited here are reviewed at ([[#JohnWu2007|John Wu (2007)]]).&lt;ref&gt;{{cite web|ref=JohnWu2007|author=John Wu |year=2007 |title=Annotated References on Bitmap Index |url=http://www.cs.umn.edu/~kewu/annotated.html}}&lt;/ref&gt; For those who might be interested in experimenting with some of the ideas mentioned here, many of them are implemented in open source software such as FastBit,&lt;ref&gt;[http://codeforge.lbl.gov/projects/fastbit/ FastBit]&lt;/ref&gt; the Lemur Bitmap Index C++ Library,&lt;ref&gt;[https://code.google.com/p/lemurbitmapindex/ Lemur Bitmap Index C++ Library]&lt;/ref&gt; the Roaring Bitmap Java library,&lt;ref&gt;[http://roaringbitmap.org/ Roaring bitmaps]&lt;/ref&gt;  the [[Apache Hive]] Data Warehouse system and [[LucidDB]].

{{Clear}}

==Compression==
Software can [[data compression|compress]] each bitmap in a bitmap index to save spaces.  There has been considerable amount of work on this subject.&lt;ref&gt;{{cite book |author=T. Johnson |editor1=Malcolm P. Atkinson |editor2=[[Maria Or&#322;owska|Maria E. Orlowska]] |editor3=Patrick Valduriez |editor4=Stanley B. Zdonik |editor5=Michael L. Brodie | title = VLDB'99, Proceedings of 25th International Conference on Very Large Data Bases, September 7&#8211;10, 1999, Edinburgh, Scotland, UK | publisher = Morgan Kaufmann | year = 1999 | isbn = 1-55860-615-7 | chapter =Performance Measurements of Compressed Bitmap Indices | pages=278&#8211;89 | url=http://www.vldb.org/conf/1999/P29.pdf }}&lt;/ref&gt;&lt;ref&gt;{{cite web |vauthors=Wu K, Otoo E, Shoshani A | title=On the performance of bitmap indices for high cardinality attributes | date=March 5, 2004 | url=http://www.osti.gov/energycitations/servlets/purl/822860-LOzkmz/native/822860.pdf }}&lt;/ref&gt;
Though there are exceptions such as Roaring bitmaps,&lt;ref name=roaring&gt;{{Cite journal | last1 = Chambi | first1 = S. | last2 = Lemire | first2 = D. | last3 = Kaser | first3 = O. | last4 = Godin | first4 = R.  | title = Better bitmap performance with Roaring bitmaps | doi = 10.1002/spe.2325 | journal = Software: Practice &amp; Experience | volume = 46 | pages = 5 | year = 2016 | pmid =  | pmc = }}&lt;/ref&gt; Bitmap compression algorithms typically employ [[run-length encoding]], such as the Byte-aligned Bitmap Code,&lt;ref&gt;{{US Patent|5363098|Byte aligned data compression}}&lt;/ref&gt; the Word-Aligned Hybrid code,&lt;ref&gt;{{US Patent|6831575|Word aligned bitmap compression method, data structure, and apparatus}}&lt;/ref&gt; the Partitioned Word-Aligned Hybrid (PWAH) compression,&lt;ref&gt;{{cite conference |url=http://dl.acm.org/citation.cfm?doid=1989323.1989419 |title=A memory efficient reachability data structure through bit vector compression | last1=van Schaik | first1=Sebastiaan |last2=de Moor |first2=Oege |year=2011 |publisher=ACM |booktitle=Proceedings of the 2011 international conference on Management of data |pages=913&#8211;924 |location=Athens, Greece |doi=10.1145/1989323.1989419 |conference=SIGMOD '11 |isbn=978-1-4503-0661-4 }}&lt;/ref&gt; the Position List Word Aligned Hybrid,&lt;ref name="doi_10.1145/1739041.1739071"&gt;{{cite book | chapter = Position list word aligned hybrid: optimizing space and performance for compressed bitmaps |vauthors=Deli&#232;ge F, Pedersen TB |editor1=Ioana Manolescu |editor2=Stefano Spaccapietra |editor3=Jens Teubner |editor4=Masaru Kitsuregawa |editor5=Alain Leger |editor6=Felix Naumann |editor7=Anastasia Ailamaki |editor8=Fatma Ozcan | title = EDBT '10, Proceedings of the 13th International Conference on Extending Database Technology | publisher = ACM | location = New York, NY, USA | year = 2010 | pages = 228&#8211;39 | isbn = 978-1-60558-945-9 | doi = 10.1145/1739041.1739071 | url = http://alpha.uhasselt.be/icdt/edbticdt2010proc/edbt/papers/p0228-Deliege.pdf }}&lt;/ref&gt; the Compressed Adaptive Index (COMPAX),&lt;ref name="autogenerated1382"&gt;{{cite journal|author1=F. Fusco |author2=M. Stoecklin |author3=M. Vlachos |title=NET-FLi: on-the-fly compression, archiving and indexing of streaming network traffic |date=September 2010 | volume = 3 | issue = 1&#8211;2 | pages = 1382&#8211;93 | journal=Proc. VLDB Endow | url=http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/I01.pdf }}&lt;/ref&gt; Enhanced Word-Aligned Hybrid (EWAH) &lt;ref name=ewah&gt;{{Cite journal | last1 = Lemire | first1 = D. | last2 = Kaser | first2 = O. | last3 = Aouiche | first3 = K. | title = Sorting improves word-aligned bitmap indexes | doi = 10.1016/j.datak.2009.08.006 | journal = Data &amp; Knowledge Engineering | volume = 69 | pages = 3 | year = 2010 | pmid =  | pmc = }}&lt;/ref&gt; and the COmpressed 'N' Composable Integer SEt.&lt;ref&gt;[http://ricerca.mat.uniroma3.it/users/colanton/concise.html Concise: Compressed 'n' Composable Integer Set] {{webarchive |url=https://web.archive.org/web/20110528033714/http://ricerca.mat.uniroma3.it/users/colanton/concise.html |date=May 28, 2011 }}&lt;/ref&gt;&lt;ref name="doi_10.1016/j.ipl.2010.05.018" /&gt; These compression methods require very little effort to compress and decompress. More importantly, bitmaps compressed with BBC, WAH, COMPAX, PLWAH, EWAH and CONCISE can directly participate in [[bitwise operation]]s without decompression. This gives them considerable advantages over generic compression techniques such as [[LZ77]]. BBC compression and its derivatives are used in a commercial [[database management system]]. BBC is effective in both reducing index sizes and maintaining [[database query|query]] performance. BBC encodes the bitmaps in [[bytes]], while WAH encodes in words, better matching current [[CPU]]s. "On both synthetic data and real application data, the new word aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC."&lt;ref&gt;{{cite book |vauthors=Wu K, Otoo EJ, Shoshani A |editor1=Henrique Paques |editor2=Ling Liu |editor3=David Grossman | chapter =A Performance comparison of bitmap indexes | year=2001 | title = CIKM '01 Proceedings of the tenth international conference on Information and knowledge management | publisher = ACM | location = New York, NY, USA | pages = 559&#8211;61 | isbn = 1-58113-436-3 | doi = 10.1145/502585.502689 | url = http://crd.lbl.gov/~kewu/ps/LBNL-48975.pdf }}&lt;/ref&gt; PLWAH bitmaps were reported to take 50% of the storage space consumed by WAH bitmaps and offer up to 20% faster performance on [[logical operation]]s.&lt;ref name="doi_10.1145/1739041.1739071" /&gt; Similar considerations can be done for CONCISE &lt;ref name="doi_10.1016/j.ipl.2010.05.018"&gt;{{cite journal |vauthors=Colantonio A, Di Pietro R | title=Concise: Compressed 'n' Composable Integer Set | journal = Information Processing Letters | volume = 110 | issue = 16 | date = 31 July 2010 | doi = 10.1016/j.ipl.2010.05.018 | url = http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf |pages=644&#8211;50 }}&lt;/ref&gt; and Enhanced Word-Aligned Hybrid.&lt;ref name="ewah"/&gt;

The performance of schemes such as BBC, WAH, PLWAH, EWAH, COMPAX and CONCISE is dependent on the order of the rows. A simple lexicographical sort can divide the index size by 9 and make indexes several times faster.&lt;ref&gt;{{cite journal|author1=D. Lemire |author2=O. Kaser |author3=K. Aouiche |title=Sorting improves word-aligned bitmap indexes |journal=Data &amp; Knowledge Engineering | volume=69 | issue=1 |date=January 2010 |arxiv=0901.3751 | doi = 10.1016/j.datak.2009.08.006|pages=3&#8211;28 }}&lt;/ref&gt; The larger the table, the more important it is to sort the rows. Reshuffling techniques have also been proposed to achieve the same results of sorting when indexing streaming data.&lt;ref name="autogenerated1382"/&gt;

==Encoding==
Basic bitmap indexes use one bitmap for each distinct value. It is possible to reduce the number of bitmaps used by using a different encoding method.&lt;ref name="autogenerated355"&gt;{{cite book |chapter=Bitmap index design and evaluation |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1998 | title = Proceedings of the 1998 ACM SIGMOD international conference on Management of data (SIGMOD '98) |editor1=Ashutosh Tiwary |editor2=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 355&#8211;6 | doi=10.1145/276304.276336 | url = http://www.comp.nus.edu.sg/~chancy/sigmod98.pdf }}&lt;/ref&gt;&lt;ref&gt;{{cite book |chapter=An efficient bitmap encoding scheme for selection queries |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1999 | title = Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD '99) | publisher = ACM | location = New York, NY, USA | pages = 215&#8211;26 | doi = 10.1145/304182.304201 | url = http://www.ist.temple.edu/~vucetic/cis616spring2005/papers/P4%20p215-chan.pdf }}&lt;/ref&gt; For example, it is possible to encode C distinct values using log(C) bitmaps with binary encoding.&lt;ref&gt;{{cite journal |author1=P. E. O'Neil  |author2=D. Quass |lastauthoramp=yes | chapter = Improved Query Performance with Variant Indexes | title = Proceedings of the 1997 ACM SIGMOD international conference on Management of data (SIGMOD '97) | year = 1997 |editor1=Joan M. Peckman |editor2=Sudha Ram |editor3=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 38&#8211;49| doi=10.1145/253260.253268 }}&lt;/ref&gt;

This reduces the number of bitmaps, further saving space, but to answer any query, most of the bitmaps have to be accessed. This makes it potentially not as effective as scanning a vertical projection of the base data, also known as a [[materialized view]] or projection index. Finding the optimal encoding method that balances (arbitrary) query performance, index size and index maintenance remains a challenge.

Without considering compression, Chan and Ioannidis analyzed a class of multi-component encoding methods and came to the conclusion that two-component encoding sits at the kink of the performance vs. index size curve and therefore represents the best trade-off between index size and query performance.&lt;ref name="autogenerated355"/&gt;

==Binning==
For high-cardinality columns, it is useful to bin the values, where each bin covers multiple values and build the bitmaps to represent the values in each bin. This approach reduces the number of bitmaps used regardless of encoding method.&lt;ref&gt;{{cite book |chapter = Space efficient bitmap indexing| title= Proceedings of the ninth international conference on Information and knowledge management (CIKM '00) | year=2000 | author =N. Koudas | publisher = ACM | location = New York, NY, USA | pages = 194&#8211;201 | doi=10.1145/354756.354819 }}&lt;/ref&gt; However, binned indexes can only answer some queries without examining the base data. For example, if a bin covers the range from 0.1 to 0.2, then when the user asks for all values less than 0.15, all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0.15. The process of checking the base data is known as the candidate check. In most cases, the time used by the candidate check is significantly longer than the time needed to work with the bitmap index. Therefore, binned indexes exhibit irregular performance. They can be very fast for some queries, but much slower if the query does not exactly match a bin.

==History==
The concept of bitmap index was first introduced by Professor Israel Spiegler and Rafi Maayan in their research "Storage and Retrieval Considerations of Binary Data Bases", published in 1985.&lt;ref&gt;{{cite journal | title = Storage and retrieval considerations of binary data bases | author1 = Spiegler I | author2 = Maayan R | journal = Information Processing and Management: an International Journal | volume = 21 | issue = 3 | year = 1985 | doi = 10.1016/0306-4573(85)90108-6 | pages = 233&#8211;54   }}&lt;/ref&gt; The first commercial database product to implement a bitmap index was Computer Corporation of America's [[Model 204]]. [[Patrick O'Neil]] published a paper about this implementation in 1987.&lt;ref name="model204"&gt;{{cite conference | last = O'Neil | first = Patrick | title = Model 204 Architecture and Performance | booktitle = Proceedings of the 2nd International Workshop on High Performance Transaction Systems | pages = 40&#8211;59 | publisher = Springer-Verlag | year = 1987 | location = London, UK | editor = Dieter Gawlick |editor2=Mark N. Haynie |editor3=Andreas Reuter (Eds.) }}&lt;/ref&gt; This implementation is a hybrid between the basic bitmap index (without compression) and the list of Row Identifiers (RID-list). Overall, the index is organized as a [[B+tree]]. When the column cardinality is low, each leaf node of the B-tree would contain long list of RIDs. In this case, it requires less space to represent the RID-lists as bitmaps. Since each bitmap represents one distinct value, this is the basic bitmap index. As the column cardinality increases, each bitmap becomes sparse and it may take more disk space to store the bitmaps than to store the same content as RID-lists. In this case, it switches to use the RID-lists, which makes it a [[B+tree]] index.&lt;ref&gt;{{cite conference | title=Bit-sliced index arithmetic | booktitle= Proceedings of the 2001 ACM SIGMOD international conference on Management of data (SIGMOD '01) | year = 2001 |author1=D. Rinfret, P. O'Neil  |author2=E. O'Neil  |lastauthoramp=yes | editor = Timos Sellis | publisher = ACM | location = New York, NY, USA | pages = 47&#8211;57 | doi = 10.1145/375663.375669 }}&lt;/ref&gt;&lt;ref&gt;{{cite conference |author1=E. O'Neil |author2=P. O'Neil |author3=K. Wu | title = Bitmap Index Design Choices and Their Performance Implications | booktitle = 11th International Database Engineering and Applications Symposium (IDEAS 2007) | year = 2007 | pages = 72&#8211;84 | url=http://crd.lbl.gov/~kewu/ps/LBNL-62756.pdf | isbn = 0-7695-2947-X | doi = 10.1109/IDEAS.2007.19 }}&lt;/ref&gt;

==In-memory bitmaps==
One of the strongest reasons for using bitmap indexes is that the intermediate results produced from them are also bitmaps and can be efficiently reused in further operations to answer more complex queries. Many programming languages support this as a bit array data structure. For example, Java has the &lt;code&gt;[http://download.oracle.com/javase/6/docs/api/java/util/BitSet.html BitSet]&lt;/code&gt; class.

Some database systems that do not offer persistent bitmap indexes use bitmaps internally to speed up query processing. For example, [[PostgreSQL]] versions 8.1 and later implement a "bitmap index scan" optimization to speed up arbitrarily complex [[logical operation]]s between available indexes on a single table.

For tables with many columns, the total number of distinct indexes to satisfy all possible queries (with equality filtering conditions on either of the fields) grows very fast, being defined by this formula:

:&lt;math&gt; \mathbf{C}_n^\left [ \frac{n}{2} \right ] \equiv \frac{n!}{\left(n-\left [ \frac{n}{2} \right ]\right)! \left [ \frac{n}{2} \right ]!}&lt;/math&gt;.&lt;ref&gt;{{cite web|author=Alex Bolenok|date=2009-05-09|title=Creating indexes|url=http://explainextended.com/2009/05/09/creating-indexes/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Egor Timoshenko|title=On minimal collections of indexes|url=http://explainextended.com/files/index-en.pdf }}&lt;/ref&gt;

A bitmap index scan combines expressions on different indexes, thus requiring only one index per column to support all possible queries on a table.

Applying this access strategy to B-tree indexes can also combine range queries on multiple columns. In this approach, a temporary in-memory bitmap is created with one [[bit]] for each row in the table (1 [[MiB]] can thus store over 8 million entries). Next, the results from each index are combined into the bitmap using [[bitwise operation]]s. After all conditions are evaluated, the bitmap contains a "1" for rows that matched the expression. Finally, the bitmap is traversed and matching rows are retrieved. In addition to efficiently combining indexes, this also improves [[locality of reference]] of table accesses, because all rows are fetched sequentially from the main table.&lt;ref&gt;{{cite web |author=Tom Lane |date=2005-12-26 |title=Re: Bitmap indexes etc. |publisher=PostgreSQL mailing lists |url=http://archives.postgresql.org/pgsql-performance/2005-12/msg00623.php |accessdate=2007-04-06 }}&lt;/ref&gt; The internal bitmap is discarded after the query. If there are too many rows in the table to use 1 bit per row, a "lossy" bitmap is created instead, with a single bit per disk page. In this case, the bitmap is just used to determine which pages to fetch; the filter criteria are then applied to all rows in matching pages.

==References==
;Notes
{{Reflist|30em}}

;Bibliography
*{{Cite journal|last=O'Connell|first=S.|year=2005|title=Advanced Databases Course Notes|location=[[Southampton]]|publisher=[[University of Southampton]]|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}
*{{Cite journal|last1=O'Neil|first1=P.|last2=O'Neil|first2=E.|year=2001|title=Database Principles, Programming, and Performance|location=[[San Francisco]]|publisher=[[Morgan Kaufmann Publishers]]|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}
*{{Cite journal|last1=Zaker|first1=M.|last2=Phon-Amnuaisuk|first2=S.|last3=Haw|first3=S.C.|year=2008|issue=2|volume=2|title=An Adequate Design for Large Data Warehouse Systems: Bitmap index versus B-tree index|journal=[[International Journal of Computers and Communications]]|url=http://www.universitypress.org.uk/journals/cc/cc-21.pdf|accessdate=2010-01-07|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}

{{DEFAULTSORT:Bitmap Index}}
[[Category:Bit data structures|Index]]
[[Category:Data management]]
[[Category:Database index techniques]]</text>
      <sha1>7wvz3zma0weqneiyc50cuqiphi2ghb4</sha1>
    </revision>
  </page>
  <page>
    <title>Very large database</title>
    <ns>0</ns>
    <id>30864622</id>
    <revision>
      <id>639584728</id>
      <parentid>624218067</parentid>
      <timestamp>2014-12-25T14:10:11Z</timestamp>
      <contributor>
        <username>Henrikdv</username>
        <id>3703541</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="731" xml:space="preserve">{{About|Large size databases|International Conference on Very Large Databases|VLDB}}

A '''very large database''', or '''VLDB''', is a database that contains an extremely high number of [[tuple]]s (database rows), or occupies an extremely large physical [[filesystem]] storage space. The most common definition of VLDB is a database that occupies more than 1 [[terabyte]] or contains several billion rows, although naturally this definition changes over time.{{Citation needed|date=March 2013}}

Very large databases are often, but not necessarily, a core component in [[big data]] analysis.

==References==
{{reflist}}

{{Database}}

{{DEFAULTSORT:Very Large Database}}
[[Category:Data management]]
[[Category:Types of databases]]</text>
      <sha1>ol46byttqtrrvj2w9q4o4towimegqjq</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise bus matrix</title>
    <ns>0</ns>
    <id>29723359</id>
    <revision>
      <id>756513003</id>
      <parentid>749962200</parentid>
      <timestamp>2016-12-24T20:25:36Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 1 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6846" xml:space="preserve">{{Multiple issues|
{{weasel|date=December 2010}}
{{orphan|date=February 2012}}
{{cleanup|date=December 2010}}
}}

The '''Enterprise Bus Matrix''' is a [[data Warehouse]] planning tool and model created by [[Ralph Kimball]], and is part of the Data Warehouse Bus Architecture. The Matrix is the logical definition of one of the core concepts of Kimball&#8217;s approach to Dimensional Modeling &#8211; Conformed dimensions.&lt;ref&gt;{{cite web|url=http://www.kimballgroup.com/2003/09/15/design-tip-49-off-the-bench/ |title=Design Tip #49: Off The Bench |publisher=Kimball Group |date=2003-09-15 |accessdate=2015-05-22 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

The Bus Matrix defines part of the Data Warehouse Bus Architecture and is an output of the Business Requirements phase in [[The Kimball Lifecycle]]. It is applied in the following phases of [[dimensional modeling]] and development of the Data Warehouse . The matrix can be categorized as a hybrid model, being part technical design tool, part project management tool and part communication tool&lt;ref name="Kimball"&gt;Kimball, Ralph &amp; Ross, Margy; The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling, 2nd Edition John Wiley &amp; Sons, 2002&lt;/ref&gt;

==Background==
The Enterprise Bus Matrix stems from the issue of how one goes about creating the overall Data Warehouse environment.  Historically there has been the structure of the centralized and planned approach and the more loosely defined, department specific, solutions developed in a more independent matter. Autonomous projects can result in a range of isolated stove pipe data marts. Naturally each approach has its issues; the overall visionary approach often struggles with long delivery cycles and lack of reaction time as the formalities and scope issues is evident. On the other hand, the development of isolated data marts, leading to [[Stovepipe system]]s that lacks synergy in development. Over time this approach will lead to a so-called data-mart-in-a-box architecture&lt;ref&gt;[http://www.mimno.com/avoiding-mistakes3.html#6]  {{webarchive |url=https://web.archive.org/web/20100704220014/http://www.mimno.com/avoiding-mistakes3.html#6 |date=July 4, 2010 }}&lt;/ref&gt; where [[interoperability]] and lack of cohesion is apparent, and can hinder the realization of an overall enterprise Data Warehouse. As an attempt to handle this matter [[Ralph Kimball]] introduced the enterprise bus.

==Bus matrix==
The bus matrix purpose is one of high abstraction and visionary planning on the Data Warehouse architectural level. By dictating coherency in the development and implementation of an overall Data Warehouse the Bus Architecture approach enables an overall vision of the broader enterprise integration and consistency while at the same time dividing the problem into more manageable parts&lt;ref name="Kimball" /&gt; &#8211; all in a technology and software independent manner .&lt;ref&gt;{{cite web|url=http://www.b-eye-network.com/view/713 |title=Data Warehouse: Ralph Kimball&#8217;s Vision by Katherine Drewek |publisher=Beyenetwork |date=2005-03-16 |accessdate=2015-05-22}}&lt;/ref&gt;

The bus matrix and architecture builds upon the concept of conformed dimensions -  creating a structure of common dimensions that ideally can be used across the enterprise by all business processes related to the DW and the corresponding fact tables from which they derive their context. According to Kimball and Margy Ross's article  &#8220;Differences of Opinion&#8221;&lt;ref&gt;{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=0OVJNEHMPRXGRQE1GHRSKH4ATMY32JVN?articleID=17800088 |title=Enterprise Software News, Analysis, &amp; Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}&lt;/ref&gt; "''The Enterprise Data warehouse built on the bus architecture &#8221;identifies and enforces the relationship between business process metrics (facts) and descriptive attributes (dimensions)''&#8221;.

The concept of a [[Bus (computing)|bus]] is well known in the language of [[Information Technology]], and is what reflects the conformed dimension concept in the Data Warehouse, creating the skeletal structure where all parts of a system connect, ensuring [[interoperability]] and consistency of data, and at the same time considers future expansion. This makes the conformed dimensions act as the integration &#8216;glue&#8217;, creating a robust backbone of the enterprise Data Warehouse.&lt;ref&gt;{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=GMS3H4SOBFQBBQE1GHOSKH4ATMY32JVN?articleID=17800088&amp;pgno=2 |title=Enterprise Software News, Analysis, &amp; Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}&lt;/ref&gt;

==Establishment and applicability==
Figure 1&lt;ref&gt;{{cite web|url=http://www.widama.us/Documents/Kimball-DimensionalModeling.PDF |format=PDF |title=Dimensional Modeling Overview |author=Bob Becker |publisher=Widama.is |accessdate=2015-05-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130322224742/http://www.widama.us:80/Documents/Kimball-DimensionalModeling.PDF |archivedate=2013-03-22 |df= }}&lt;/ref&gt; shows the base for a single document planning tool for the whole of the DW implementation - a graphical overview of the enterprises core business processes or events each correspond to a measurement table of facts, that typically is complemented by a major source system in the horizontal rows.  In the vertical columns the groups of contextual data is found as the common, conformed dimensions.

In this way the shared dimensions are defined, as each process indicates what dimensions it applies to through the cells figure 2.&lt;ref name="Kimball" /&gt; By this definition and coordination of conformed dimensions and processes the development of the overall data DW bus architecture is realized.&lt;ref name="Kimball" /&gt; The matrix identifies the shared dimensions related to processes and fact tables, and can be a tool for planning, prioritizing what needs to be approached, coordinating implementation and communicating the importance for conformed dimensions .

Kimball extends the matrix bus in detail as seen in figure 3&lt;ref name="Kimball" /&gt;  by introducing the other steps of the Datawarehouse Methodology; The Fact tables, Granularity, and at last the description of the needed facts.  description of the fact tables, granularity and fact instances of each process, structuring and specifying what is needed across the enterprise in a more specific matter, further exemplifying how the matrix can be used as a planning tool.

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise bus matrix}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]</text>
      <sha1>kt996n0ouc3d3kj8dopy46863djn271</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data warehousing</title>
    <ns>14</ns>
    <id>5611387</id>
    <revision>
      <id>548512320</id>
      <parentid>535192175</parentid>
      <timestamp>2013-04-03T16:55:39Z</timestamp>
      <contributor>
        <username>Chobot</username>
        <id>259798</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 6interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q8363893|Q8363893]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="122" xml:space="preserve">[[Category:Information technology management]]
[[Category:Business intelligence]]
[[Category:Data management|Warehousing]]</text>
      <sha1>mtus0ip5whglz3uxpwdlmx3ekvuh4fs</sha1>
    </revision>
  </page>
  <page>
    <title>Data management plan</title>
    <ns>0</ns>
    <id>31808302</id>
    <revision>
      <id>683497752</id>
      <parentid>683496667</parentid>
      <timestamp>2015-09-30T17:39:05Z</timestamp>
      <contributor>
        <username>Phoebe</username>
        <id>19217</id>
      </contributor>
      <comment>/* References */  added further reading section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11857" xml:space="preserve">A '''data management plan''' or '''DMP''' is a formal document that outlines how you will handle your [[data]] both during your research, and after the project is completed.&lt;ref&gt;http://www2.lib.virginia.edu/brown/data/plan.html&lt;/ref&gt; The goal of a data management plan is to consider the many aspects of [[data management]], [[metadata]] generation, data preservation, and analysis before the project begins; this ensures that data are well-managed in the present, and prepared for preservation in the future.

== Importance ==

Preparing a data management plan before data are collected ensures that data are in the correct format, organized well, and better annotated.&lt;ref&gt;http://libraries.mit.edu/data-management/plan/why/&lt;/ref&gt; This saves time in the long term because there is no need to re-organize, re-format, or try to remember details about data. It also increases research efficiency since both the data collector and other researchers will be able to understand and use well-annotated data in the future.  One component of a good data management plan is data archiving and preservation. By deciding on an archive ahead of time, the data collector can format data during collection to make its future submission to a database easier. If data are preserved, they are more relevant since they can be re-used by other researchers.  It also allows the data collector to direct requests for data to the database, rather than address requests individually.  Data that are preserved have the potential to lead to new, unanticipated discoveries, and they prevent duplication of scientific studies that have already been conducted. Data archiving also provides insurance against loss by the data collector.

Funding agencies are beginning to require data management plans as part of the proposal and evaluation process.&lt;ref&gt;http://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp&lt;/ref&gt;

== Major Components ==

=== Information about data &amp; data format ===

* Include a description of data to be produced by the project.&lt;ref&gt;{{Cite web|title = Elements of a Data Management Plan|url = http://www.icpsr.umich.edu/icpsrweb/content/datamanagement/dmp/elements.html|website = www.icpsr.umich.edu|accessdate = 2015-09-30}}&lt;/ref&gt; This might include (but is not limited to) data that are:
** Experimental
** Observational
** Raw or derived
** Physical collections
** Models
** Simulations
** Curriculum materials
** Software
** Images
* How will the data be acquired? When and where will they be acquired?
* After collection, how will the data be processed? Include information about
** Software used
** Algorithms
** [[workflow|Scientific workflows]]
* Describe the file formats that will be used, justify those formats, and describe the naming conventions used.
* Identify the quality assurance &amp; quality control measures that will be taken during sample collection, analysis, and processing.
* If existing data are used, what are their origins? How will the data collected be combined with existing data? What is the relationship between the data collected and existing data?
* How will the data be managed in the short-term? Consider the following:
** [[Version control]] for files
** Backing up data and data products
** Security &amp; protection of data and data products
** Who will be responsible for management

=== Metadata content and format ===

[[Metadata]] are the contextual details, including any information important for using data. This may include descriptions of temporal and spatial details, instruments, parameters, units, files, etc. Metadata is commonly referred to as &#8220;data about data&#8221;.&lt;ref&gt;Michener,WK and JW Brunt. 2000. ''Ecological Data: Design, Management and Processing''. Blackwell Science, 180p.&lt;/ref&gt; Consider the following:
* What metadata are needed? Include any details that make data meaningful.
* How will the metadata be created and/or captured? Examples include lab notebooks, GPS hand-held units, Auto-saved files on instruments, etc.
* What format will be used for the metadata? Consider the [[metadata standards]] commonly used in the scientific discipline that contains your work. There should be justification for the format chosen.

=== Policies for access, sharing, and re-use ===

* Describe any obligations that exist for sharing data collected. These may include obligations from funding agencies, institutions, other professional organizations, and legal requirements.
* Include information about how data will be shared, including when the data will be accessible, how long the data will be available, how access can be gained, and any rights that the data collector reserves for using data.
* Address any ethical or privacy issues with data sharing
* Address [[intellectual property]] &amp; [[copyright]] issues. Who owns the copyright? What are the institutional, publisher, and/or funding agency policies associated with intellectual property? Are there embargoes for political, commercial, or patent reasons?
* Describe the intended future uses/users for the data
* Indicate how the data should be cited by others. How will the issue of persistent citation be addressed? For example, if the data will be deposited in a public archive, will the dataset have a [[digital object identifier]] (doi) assigned to it?

=== Long-term storage and data management ===

* Researchers should identify an appropriate archive for long-term preservation of their data. By identifying the archive early in the project, the data can be formatted, transformed, and documented appropriately to meet the requirements of the archive. Researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database, and include a backup archive in their data management plan in case their first choice goes out of existence.
* Early in the project, the primary researcher should identify what data will be preserved in an archive. Usually, preserving the data in its most raw form is desirable, although data derivatives and products can also be preserved.
* An individual should be identified as the primary contact person for archived data, and ensure that contact information is always kept up-to-date in case there are requests for data or information about data.

=== Budget ===

Data management and preservation costs may be considerable, depending on the nature of the project. By anticipating costs ahead of time, researchers ensure that the data will be properly managed and archived. Potential expenses that should be considered are
* Personnel time for data preparation, management, documentation, and preservation
* Hardware and/or software needed for data management, backing up, security, documentation, and preservation
* Costs associated with submitting the data to an archive
The data management plan should include how these costs will be paid.

== NSF Data Management Plan ==

All grant proposals submitted to [[National Science Foundation|NSF]] must include a Data Management Plan that is no more than two pages.&lt;ref&gt;http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp&lt;/ref&gt; This is a supplement (not part of the 15 page proposal) and should describe how the proposal will conform to the Award and Administration Guide policy (see below). It may include the following:
# The types of data
# The standards to be used for data and metadata format and content
# Policies for access and sharing
# Policies and provisions for re-use
# Plans for archiving data

Policy summarized from the [[National Science Foundation|NSF]] Award and Administration Guide, Section 4 (Dissemination and Sharing of Research Results):&lt;ref&gt;http://www.nsf.gov/bfa/dias/policy/dmp.jsp&lt;/ref&gt;
# Promptly publish with appropriate authorship
# Share data, samples, physical collections, and supporting materials with others, within a reasonable time frame
# Share software and inventions
# Investigators can keep their legal rights over their intellectual property, but they still have to make their results, data, and collections available to others
# Policies will be implemented via
## Proposal review
## Award negotiations and conditions
## Support/incentives

== ESRC Data Management Plan ==

Since 1995, the UK's [[Economic and Social Research Council]] (ESRC) have had a research data policy in place. The current ESRC Research Data Policy states that research data created as a result of ESRC-funded research should be openly available to the scientific community to the maximum extent possible, through long-term preservation and high quality data management.&lt;ref&gt;[http://www.esrc.ac.uk/about-esrc/information/data-policy.aspx ESRC Research Data Policy 2010]&lt;/ref&gt;

ESRC requires a data management plan for all research award applications where new data are being created. Such plans are designed to promote a structured approach to data management throughout the data lifecycle, resulting in better quality data that is ready to archive for sharing and re-use. The [[UK Data Service]], the ESRC's flagship data service, provides practical guidance on research data management planning suitable for social science researchers in the UK and around the world.&lt;ref&gt;[http://ukdataservice.ac.uk/manage-data.aspx Prepare and manage data: Guidance from the UK Data Service]&lt;/ref&gt;&lt;ref&gt;[http://www.sagepub.com/books/Book240297 SAGE handbook: Managing and Sharing Data: A Guide to Good Practice]&lt;/ref&gt;

ESRC has a longstanding arrangement with the [[UK Data Archive]], based at the [[University of Essex]], as a place of deposit for research data, with award holders required to offer data resulting from their research grants via the UK Data Service.&lt;ref&gt;[http://www.data-archive.ac.uk/deposit/who UK Data Archive: Who can deposit data?]&lt;/ref&gt; The Archive enables data re-use by preserving data and making them available to the research and teaching communities.

== References ==
{{Reflist}}

== Further reading ==
{{Cite book|title = Delivering research data management services|last = Pryor|first = Graham|publisher = Facet Publishing|year = 2014|isbn = 9781856049337|location = |pages = }}

== External links ==
* [http://www.sagepub.com/books/Book240297?&amp;subject=B00&amp;sortBy=defaultPubDate%20desc&amp;fs=1 SAGE handbook]: Managing and Sharing Research Data: A Guide to Good Practice
* [http://dmp.cdlib.org DMPTool]: Guidance and resources for data management plans
* [http://www.cdlib.org/services/uc3/dmp/index.html California Digital Library], University of California Curation Center (UC3)
* [http://www.dataone.org/plans DataONE]
* [http://www2.lib.virginia.edu/brown/data/plan.html University of Virginia Library]
* [https://dmponline.dcc.ac.uk/ DMPonline]
* [http://www.dcc.ac.uk/resources/data-management-plans Digital Curation Centre]
* [http://www.lib.umich.edu/research-data-management-and-publishing-support/nsf-data-management-plans#directorate_guide University of Michigan Library]
* [http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp NSF Grant Proposal Guidelines]
* [http://www.icpsr.umich.edu/icpsrweb/ICPSR/dmp/index.jsp Inter-University Consortium for Political and Social Research]
* [http://lno.lternet.edu/node/269 LTER Blog: How to write a data management plan]
* [http://www.gesis.org/en/archive-and-data-management-training-and-information-center/research-data-management/data-management-plan/ More information about data management plans at [[GESIS &#8211; Leibniz Institute for the Social Sciences]]]
* [http://ukdataservice.ac.uk/manage-data.aspx UK Data Service]: Prepare and Manage Data: Guidance and tools for social science researchers
* [http://www.consorciomadrono.es/pagoda Plan de Gesti&#243;n de Datos PaGoDa]: DMP Toolkit of The Consortium of Universities of the Region of Madrid and the UNED for Library Cooperation (Madro&#241;o - Spain) 

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]</text>
      <sha1>1dip9zeham0b8vpglky4my08onu4j7v</sha1>
    </revision>
  </page>
  <page>
    <title>PureXML</title>
    <ns>0</ns>
    <id>19470961</id>
    <revision>
      <id>738923456</id>
      <parentid>615294716</parentid>
      <timestamp>2016-09-11T20:07:55Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7621" xml:space="preserve">{{Lowercase title}}
'''pureXML''' is the native [[XML]] storage feature in the [[IBM DB2]] data server.  pureXML provides [[query language]]s, storage technologies, indexing technologies, and other features to support XML data.  The word ''pure'' in pureXML was chosen to indicate that DB2 natively stores and natively processes XML data in its inherent hierarchical structure, as opposed to treating XML data as plain text or converting it into a relational format.&lt;ref&gt;http://www.ibm.com/developerworks/blogs/page/datastudioteam?entry=purexml_and_purequery_what_s&lt;/ref&gt;

== Technical information ==
DB2 includes two distinct storage mechanisms: one for efficiently managing traditional SQL data types, and another for managing XML data.  The underlying storage mechanism is transparent to users and applications; they simply use SQL (including SQL with XML extensions or [[SQL/XML]]) or [[XQuery]] to work with the data.

XML data is stored in columns of DB2 tables that have the XML data type.  XML data is stored in a parsed format that reflects the hierarchical nature of the original XML data.  As such, pureXML uses trees and nodes as its model for storing and processing XML data.  If you instruct DB2 to validate XML data against an XML schema prior to storage, DB2 annotates all nodes in the XML hierarchy with information about the schema types; otherwise, it will annotate the nodes with default type information.  Upon storage, DB2 preserves the internal structure of XML data, converting its tag names and other information into integer values. Doing so helps conserve disk space and also improves the performance of queries that use navigational expressions. However, users aren't aware of this internal representation.  Finally, DB2 automatically splits XML nodes across multiple database pages, as needed.

XML schemas specify which XML elements are valid, in what order these elements should appear in XML data, which XML data types are associated with each element, and so on.  pureXML allows you to validate the cells in a column of XML data against no schema, one schema, or multiple schemas.  pureXML also provides tools to support evolving XML schemas.

IBM has enhanced its [[programming language]] interfaces to support access to its XML data. These enhancements span [[Java (programming language)|Java]] ([[JDBC]]), [[C (programming language)|C]] (embedded SQL and call-level interface), [[COBOL]] (embedded SQL), [[PHP]], and [[Microsoft]]'s [[.NET Framework]] (through the DB2.NET provider).

== History ==
pureXML was first included in the DB2 9 for [[Linux]], [[Unix]], and [[Microsoft Windows]] release, which was codenamed Viper, in June 2006.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | title = IBM News room - 2006-06-08 IBM Transforms Database Market With Introduction of DB2 - United States | archiveurl= https://web.archive.org/web/20121011235127/http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | archivedate= 2012-10-11 }}&lt;/ref&gt;  It was available on DB2 9 for [[z/OS]] in March 2007.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | title = IBM News room - 2007-03-06 IBM Unveils DB2 Viper for the Mainframe - United States | archiveurl= https://web.archive.org/web/20121011235143/http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | archivedate= 2012-10-11 }}&lt;/ref&gt;  In October 2007, IBM released DB2 9.5 with improved XML data transaction performance and improved storage savings.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | title = IBM News room - 2007-10-15 IBM Extends Data Server Technology Lead With Introduction of DB2 &amp;quot;Viper 2&amp;quot; - United States | archiveurl= https://web.archive.org/web/20121011235149/http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | archivedate= 2012-10-11 }}&lt;/ref&gt; In June 2009, IBM released DB2 9.7 with XML supported for database-partitioned, range-partitioned, and multi-dimensionally clustered tables as well as compression of XML data and indices.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | title = IBM News room - 2009-04-22 IBM Database Software Improves Operational Efficiency and Cuts Storage Costs by Up to 75% - United States | archiveurl= https://web.archive.org/web/20121121014600/http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | archivedate= 2012-11-21 }}&lt;/ref&gt;

== Competition ==
{{See also|XML database}}
DB2 is a hybrid data server&#8212;it offers data management for traditional relational data, as well as providing native XML data management.  Other vendors that offer data management for both relational data and native XML storage include [[Oracle Corporation|Oracle]] with its [[Oracle Database|11g]] product and Microsoft with its [[Microsoft SQL Server|SQL Server]] product.

pureXML also competes with native XML databases like [[BaseX (database)|BaseX]], [[eXist]], [[MarkLogic]] or [[Sedna (database)|Sedna]].

== User groups ==
The International DB2 Users Group (IDUG) is an independent, not-for-profit association of IT professionals who use IBM DB2.  IDUG provides education, technical resources, peer networking opportunities, online resources and other programs for DB2 users.

== Books ==
IBM International Technical Support Organization (ITSO) has published the following books, which are available in print or as free e-books:
* [http://www.redbooks.ibm.com/abstracts/sg247298.html?Open DB2 9: pureXML Overview and Fast Start]
* [http://www.redbooks.ibm.com/abstracts/sg247315.html?Open DB2 9 pureXML Guide]

The following books are also available for purchase:
* [http://www.amazon.com/DB2-pureXML-Cookbook-Master-Hybrid/dp/0138150478/ DB2 pureXML Cookbook: Master the Power of IBM Hybrid Data Server]

== Education and training ==
The following pureXML classroom and online courses are available from IBM Education:
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&amp;courseCode=CG130 Query and Manage XML Data with DB2 9].  IBM course CG130.  Classroom.  Duration: 4 days.
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&amp;courseCode=CG100 Query XML Data with DB2 9].  IBM course CG100.  Classroom.  Duration: 2 days (first 2 days of CG130).
* Managing XML Data in DB2 9.  IBM course CG160.  Classroom.  Duration: 2 days (last 2 days of CG130).
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_search&amp;sortBy=5&amp;searchType=1&amp;sortDirection=9&amp;includeNotScheduled=15&amp;rowStart=0&amp;rowsToReturn=20&amp;maxSearchResults=200&amp;searchString=CT140&amp;language=en&amp;country=us DB2 pureXML].  IBM Course CT140.  Self-paced study plus Live Virtual Classroom.

== See also ==
* [[IBM DB2]]
* [[XML database]]

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.ibm.com/software/data/db2/xml}}
* [http://www.ibm.com/developerworks/wikis/display/db2xml/Home pureXML Wiki]
* [http://www.ibm.com/developerworks/forums/forum.jspa?forumID=1423 pureXML Forum]
* [http://www.ibm.com/developerworks/blogs/page/purexml pureXML Team Blog]
* [http://www.nativexmldatabase.com Native XML Database Blog]
* [http://blog.4loeser.net Blog with pureXML Topics]

=== Online communities ===
Online communities allow pureXML users to network with fellow professionals.
* [http://www.linkedin.com/groups?gid=129185 pureXML Group on LinkedIn]

{{IBM DB2 product family}}

[[Category:XML software]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:IBM DB2]]
[[Category:IBM software]]
[[Category:XML databases]]</text>
      <sha1>bvlgeblt5dlbnhx2wgo56lkwiep87yx</sha1>
    </revision>
  </page>
  <page>
    <title>Data deduplication</title>
    <ns>0</ns>
    <id>17174890</id>
    <revision>
      <id>759377096</id>
      <parentid>756132583</parentid>
      <timestamp>2017-01-10T20:20:41Z</timestamp>
      <contributor>
        <username>Ost316</username>
        <id>6289403</id>
      </contributor>
      <minor />
      <comment>[[WP:AWB]] cleanup, [[WP:AWB/T|typo(s) fixed]]: For example &#8594; For example, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20100" xml:space="preserve">{{multiple issues|
{{original research|date=February 2011}}
{{More footnotes|date=September 2009}}
}}

In [[computing]], '''data deduplication''' is a specialized [[data compression]] technique for eliminating duplicate copies of repeating data. Related and somewhat synonymous terms are '''intelligent (data) compression''' and '''[[single-instance storage|single-instance (data) storage]]'''. This technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent. In the deduplication process, unique chunks of data, or byte patterns, are identified and stored during a process of analysis. As the analysis continues, other chunks are compared to the stored copy and whenever a match occurs, the redundant chunk is replaced with a small reference that points to the stored chunk. Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced.&lt;ref&gt;"[http://www.druva.com/blog/2009/01/09/understanding-data-deduplication/ Understanding Data Deduplication]" Druva, 2009. Retrieved 2013-2-13&lt;/ref&gt;

This type of deduplication is different from that performed by standard file-compression tools, such as [[LZ77 and LZ78]]. Whereas these tools identify short repeated substrings inside individual files, the intent of storage-based data deduplication is to inspect large volumes of data and identify large sections &#8211; such as entire files or large sections of files &#8211; that are identical, in order to store only one copy of it. This copy may be additionally compressed by single-file compression techniques. For example, a typical email system might contain 100 instances of the same 1 MB ([[megabyte]]) file attachment. Each time the [[email]] platform is backed up, all 100 instances of the attachment are saved, requiring 100 MB storage space. With data deduplication, only one instance of the attachment is actually stored; the subsequent instances are referenced back to the saved copy for deduplication ratio of roughly 100 to 1.

==Benefits==
* Storage-based data deduplication reduces the amount of storage needed for a given set of files. It is most effective in applications where many copies of very similar or even identical data are stored on a single disk&#8212;a surprisingly common scenario. In the case of data backups, which routinely are performed to protect against data loss, most data in a given backup remain unchanged from the previous backup. Common backup systems try to exploit this by omitting (or [[hard link]]ing) files that haven't changed or storing [[Data differencing|differences]] between files.  Neither approach captures all redundancies, however. Hard-linking does not help with large files that have only changed in small ways, such as an email database;  differences only find redundancies in adjacent versions of a single file (consider a section that was deleted and later added in again, or a logo image included in many documents).
* Network data deduplication is used to reduce the number of bytes that must be transferred between endpoints, which can reduce the amount of bandwidth required. See [[WAN optimization]] for more information.
* Virtual servers benefit from deduplication because it allows nominally separate system files for each virtual server to be coalesced into a single storage space. At the same time, if a given server customizes a file, deduplication will not change the files on the other servers&#8212;something that alternatives like hard links or shared disks do not offer.  Backing up or making duplicate copies of virtual environments is similarly improved.

==Deduplication overview==
Deduplication may occur "in-line", as data is flowing, or "post-process" after it has been written.

===Post-process deduplication===
With post-process deduplication, new data is first stored on the storage device and then a process at a later time will [[analysis|analyze]] the data looking for duplication. The benefit is that there is no need to wait for the hash calculations and lookup to be completed before storing the data, thereby ensuring that store performance is not degraded. Implementations offering policy-based operation can give users the ability to defer optimization on "active" files, or to process files based on type and location. One potential drawback is that duplicate data may be unnecessarily stored for a short time, which can be problematic if the system is nearing full capacity.

===In-line deduplication===
Alternatively, deduplication hash calculations can be done in real-time as data enters the target device. If the storage system identifies a block which it has already stored, only a reference to the existing block is stored, rather than the whole new block.

The advantage of in-line deduplication over post-process deduplication is that it requires less storage, since duplicate data is never stored.  On the negative side, it is frequently argued{{by whom|date=August 2016}} that because hash calculations and lookups take so long, [[data ingestion]] can be slower, thereby reducing the backup throughput of the device.  However, certain vendors with in-line deduplication have demonstrated equipment with similar performance to their post-process deduplication counterparts{{according to whom|date=April 2015}}.

Data coming in is stored into "lining space" before it hits real storage blocks. On SSD disks lining space is provided using [[Non-volatile random-access memory|NVRAM]] which is not cost efficient{{according to whom|date=August 2016}}.

Post-process and in-line deduplication methods are often heavily debated.&lt;ref&gt;{{cite web|url=http://www.backupcentral.com/content/view/134/47/ |title=In-line or post-process de-duplication? (updated 6-08) |publisher=Backup Central |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091206035054/http://www.backupcentral.com:80/content/view/134/47 |archivedate=2009-12-06 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://searchdatabackup.techtarget.com/tip/0,289483,sid187_gci1315295,00.html |title=Inline vs. post-processing deduplication appliances |publisher=Searchdatabackup.techtarget.com |date= |accessdate=2009-10-16}}&lt;/ref&gt;

===Data formats===
[[SNIA Dictionary]] identifies two methods:
* content-agnostic data deduplication - a data deduplication method that does not require awareness of specific application data formats. 
* content-aware data deduplication - a data deduplication method that leverages knowledge of specific application data formats.

===Source versus target deduplication===
Another way to classify data deduplication methods is according to where they occur. Deduplication occurring close to where data is created, is often referred to{{according to whom|date=August 2016}} as "source deduplication". When it occurs near where the data is stored, it is commonly called "target deduplication".

* Source deduplication ensures that data on the data source is deduplicated.  This generally takes place directly within a file system.&lt;ref&gt;{{cite web|url=http://www.microsoft.com/windowsserver2008/en/us/WSS08/SIS.aspx |title=Windows Server 2008: Windows Storage Server 2008 |publisher=Microsoft.com |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091004073508/http://www.microsoft.com:80/windowsserver2008/en/us/WSS08/SIS.aspx |archivedate=2009-10-04 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.netapp.com/us/products/platform-os/dedupe.html |title=Products - Platform OS |publisher=NetApp |date= |accessdate=2009-10-16}}&lt;/ref&gt;  The file system will periodically scan new files creating hashes and compare them to hashes of existing files.   When files with same hashes are found then the file copy is removed and the new file points to the old file.  Unlike [[hard links]] however, duplicated files are considered to be separate entities and if one of the duplicated files is later modified, then using a system called [[copy-on-write]] a copy of that file or changed block is created.  The deduplication process is transparent to the users and backup applications.  Backing up a deduplicated file system will often cause duplication to occur resulting in the backups being bigger than the source data.
* Target deduplication is the process of removing duplicates when the data was not generated at that location.  Example of this would be a server connected to a SAN/NAS, The SAN/NAS would be a target for the server (Target deduplication).  The server is not aware of any deduplication, the server is also the point of data generation.

A second example would be backup. If you have a backup system with deduplication. Generally this will be a backup store such as a data repository or a [[virtual tape library]].

===Deduplication methods===
One of the most common forms of data deduplication implementations works by comparing chunks of data to detect duplicates. For that to happen, each chunk of data is assigned an identification, calculated by the software, typically using cryptographic hash functions. In many implementations, the assumption is made that if the identification is identical, the data is identical, even though this cannot be true in all cases due to the [[pigeonhole principle]]; other implementations do not assume that two blocks of data with the same identifier are identical, but actually verify that data with the same identification is identical.&lt;ref&gt;An example of an implementation that checks for identity rather than assuming it is described in [http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PG01&amp;s1=shnelvar&amp;OS=shnelvar&amp;RS=shnelvar "US Patent application # 20090307251"].&lt;/ref&gt; If the software either assumes that a given identification already exists in the deduplication namespace or actually verifies the identity of the two blocks of data, depending on the implementation, then it will replace that duplicate chunk with a link.

Once the data has been deduplicated, upon read back of the file, wherever a link is found, the system simply replaces that link with the referenced data chunk. The deduplication process is intended to be transparent to end users and applications.

Commercial deduplication implementations differ by their chunking methods and architectures.

* Chunking.  In some systems, chunks are defined by physical layer constraints (e.g. 4KB block size in [[Write Anywhere File Layout|WAFL]]). In some systems only complete files are compared, which is called [[single-instance storage]] or SIS. The most intelligent (but CPU intensive) method to chunking is generally considered to be sliding-block. In sliding block, a window is passed along the file stream to seek out more naturally occurring internal file boundaries.
* Client backup deduplication. This is the process where the deduplication hash calculations are initially created on the source (client) machines.  Files that have identical hashes to files already in the target device are not sent, the target device just creates appropriate internal links to reference the duplicated data.  The benefit of this is that it avoids data being unnecessarily sent across the network thereby reducing traffic load.
* Primary storage and secondary storage. By definition, primary storage systems are designed for optimal performance, rather than lowest possible cost.  The design criteria for these systems is to increase performance, at the expense of other considerations.  Moreover, primary storage systems are much less tolerant of any operation that can negatively impact performance.  Also by definition, secondary storage systems contain primarily duplicate, or secondary copies of data.  These copies of data are typically not used for actual production operations and as a result are more tolerant of some performance degradation, in exchange for increased efficiency.

To date, data deduplication has predominantly been used with secondary storage systems.  The reasons for this are two-fold.  First, data deduplication requires overhead to discover and remove the duplicate data.  In primary storage systems, this overhead may impact performance.  The second reason why deduplication is applied to secondary data, is that secondary data tends to have more duplicate data.  Backup application in particular commonly generate significant portions of duplicate data over time.

Data deduplication has been deployed successfully with primary storage in some cases where the system design does not require significant overhead, or impact performance.

==Drawbacks and concerns==
Whenever data is transformed, concerns arise about potential loss of data.  By definition, data deduplication systems store data differently from how it was written.  As a result, users are concerned with the integrity of their data.  The various methods of deduplicating data all employ slightly different techniques.  However, the integrity of the data will ultimately depend upon the design of the deduplicating system, and the quality used to implement the algorithms.  As the technology has matured over the past decade, the integrity of most of the major products has been well proven .{{citation needed|date=November 2012}}

One method for deduplicating data relies on the use of [[cryptographic hash function]]s to identify duplicate segments of data. If two different pieces of information generate the same hash value, this is known as a [[collision (computer science)|collision]].  The probability of a collision depends upon the hash function used, and although the probabilities are small, they are always non zero. Thus, the concern arises that [[data corruption]] can occur if a [[hash collision]] occurs, and additional means of verification are not used to verify whether there is a difference in data, or not. Both in-line and post-process architectures may offer bit-for-bit validation of original data for guaranteed data integrity.&lt;ref&gt;{{citation |url=http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ |title=Data Deduplication - Why, When, Where and How |publisher=Evaluator Group |accessdate=2011-07-05}}&lt;/ref&gt; The hash functions used include standards such as [[SHA-1]], [[SHA-256]] and others.

The computational resource intensity of the process can be a drawback of data deduplication.  However, this is rarely an issue for stand-alone devices or appliances, as the computation is completely offloaded from other systems.  This can be an issue when the deduplication is embedded within devices providing other services. To improve performance, many systems utilize both weak and strong hashes.  Weak hashes are much faster to calculate but there is a greater risk of a hash collision.  Systems that utilize weak hashes will subsequently calculate a strong hash and will use it as the determining factor to whether it is actually the same data or not. Note that the system overhead associated with calculating and looking up hash values is primarily a function of the deduplication workflow. The reconstitution of files does not require this processing and any incremental performance penalty associated with re-assembly of data chunks is unlikely to impact application performance.

Another area of concern with deduplication is the related effect on [[Snapshot (computer storage)|snapshots]], [[backup]], and [[archival]], especially where deduplication is applied against primary storage (for example inside a [[Network-attached storage|NAS]] filer).{{elucidate|date=December 2011}} Reading files out of a storage device causes full reconstitution of the files (also known as rehydration), so any secondary copy of the data set is likely to be larger than the primary copy. In terms of snapshots, if a file is snapshotted prior to deduplication, the post-deduplication snapshot will preserve the entire original file. This means that although storage capacity for primary file copies will shrink, capacity required for snapshots may expand dramatically.

Another concern is the effect of compression and encryption. Although deduplication is a version of compression, it works in tension with traditional compression. Deduplication achieves better efficiency against smaller data chunks, whereas compression achieves better efficiency against larger chunks. The goal of encryption is to eliminate any discernible patterns in the data. Thus encrypted data cannot be deduplicated, even though the underlying data may be redundant. Deduplication ultimately reduces redundancy.  If this was not expected and planned for, this may ruin the underlying reliability of the system.  (Compare this, for example, to the [[LOCKSS]] storage architecture that achieves reliability through multiple copies of data.)

Scaling has also been a challenge for deduplication systems because ideally, the scope of deduplication needs to be shared across storage devices. If there are multiple disk backup devices in an infrastructure with discrete deduplication, then space efficiency is adversely affected. A deduplication shared across devices preserves space efficiency, but is technically challenging from a reliability and performance perspective.{{citation needed|date=December 2011}}

Although not a shortcoming of data deduplication, there have been data breaches{{citation needed|date=August 2016}} when insufficient security and access validation procedures are used with large repositories of deduplicated data.  In some systems, as typical with cloud storage{{citation needed|date=August 2016}}, an attacker can retrieve data owned by others by knowing or guessing the hash value of the desired data.&lt;ref&gt;{{cite journal |title=A Cloud You Can Trust |publisher=[[IEEE]] |work=[[IEEE Spectrum]] |accessdate=2011-12-21 |url=http://spectrum.ieee.org/computing/networks/a-cloud-you-can-trust |author1=CHRISTIAN CACHIN |author2=MATTHIAS SCHUNTER |date=December 2011}}&lt;/ref&gt;

==See also==
* [[Capacity optimization]]
* [[Cloud storage]]
* [[Single-instance storage]]
* [[Content-addressable storage]]
* [[Delta encoding]]
* [[Linked data]]
* [[Pointer (computer programming)|Pointer]]
* [[Record linkage]]
* [[Identity resolution]]
* [[Convergent encryption]]

==References==
{{Reflist|30em}}

==External links==
* Biggar, Heidi(2007.12.11). [http://wayback.archive.org/web/20120325005645/http://www.infostor.com/webcast/display_webcast.cfm?ID=540 WebCast: The Data Deduplication Effect]
* Fellows, Russ(Evaluator Group, Inc.) [http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ Data Deduplication, why when where and how?]
* [http://wayback.archive.org/web/20120328022229/http://www.tacoma.washington.edu/tech/docs/research/gradresearch/MSpiz.pdf Using Latent Semantic Indexing for Data Deduplication].
* [http://www.forbes.com/2009/08/08/exagrid-storage-data-technology-cio-network-tape.html A Better Way to Store Data].
* [http://www.eweek.com/c/a/Database/What-Is-the-Difference-Between-Data-Deduplication-File-Deduplication-and-Data-Compression What Is the Difference Between Data Deduplication, File Deduplication, and Data Compression?] - Database from eWeek
* [http://www.snia.org/forums/dmf/programs/data_protect_init/ddsrsig/ SNIA DDSR SIG] * * [http://wayback.archive.org/web/20120322084240/http://www.snia.org/forums/dmf/knowledge/white_papers_and_reports/Understanding_Data_Deduplication_Ratios-20080718.pdf Understanding Data Deduplication Ratios]
* [http://public.dhe.ibm.com/common/ssi/ecm/en/tsu12345usen/TSU12345USEN.PDF Data Footprint Reduction Technology Whitepaper]
* [http://www.itnext.in/content/doing-more-less.html Doing More with Less by Jatinder Singh]
* [http://www.sersc.org/journals/IJSIA/vol7_no5_2013/38.pdf Byte Index Chunking Algorithm for Data Deduplication]

{{DEFAULTSORT:Data Deduplication}}
[[Category:Data management]]
[[Category:Data compression]]</text>
      <sha1>8hs8rkno35ofy3oln87wd031l04c08q</sha1>
    </revision>
  </page>
  <page>
    <title>Asset Description Metadata Schema</title>
    <ns>0</ns>
    <id>35676267</id>
    <revision>
      <id>745248841</id>
      <parentid>695226264</parentid>
      <timestamp>2016-10-20T02:47:20Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead. #IABot (v1.2.5)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6174" xml:space="preserve">[[File:ADMSmodelv1.png|thumb|300px|ADMS UML model version 1.00]]

The '''Asset Description Metadata Schema''' ('''ADMS''') is a common [[metadata]] vocabulary to describe standards, so-called interoperability assets, on the Web.

Used in concert with [[Web Syndication|web syndication technology]] ADMS helps people make sense of the complex multi-publisher environment around standards and in particular the ones which are semantic assets such as [[ontologies]], [[data model]]s, [[Data dictionary|data dictionaries]], code lists, [[XML]] and [[Resource Description Framework|RDF]] schemas. In spite of their importance, standards are not easily discoverable on the web via search engines because [[metadata]] about them is seldom available. Navigating on the websites of the different publishers of standards is not efficient either.

==Key terminology==
A '''semantic asset''' is a specific type of standard which involves:

 highly reusable metadata
 (e.g. xml schemata, generic data models)
 and/or reference data
 (e.g. code lists, taxonomies, dictionaries, vocabularies)

Organisations use semantic assets to share information and knowledge (within themselves and with others). Semantic assets are usually very valuable and reusable elements for the development of Information Systems, in particular, as part of machine-to-machine interfaces. As enablers to interoperable information exchange, semantic assets are usually created, published and maintained by standardisation bodies. Nonetheless, ICT projects and groups of experts also create such assets. There are therefore many publishers of semantic assets with different degrees of formalism.

==What is ADMS==
ADMS&lt;ref name="ADMS"&gt;[http://joinup.ec.europa.eu/asset/adms/home], ADMS homepage on Joinup&lt;/ref&gt; is a standardised metadata vocabulary created by the [[European Union|EU]]'s Interoperability Solutions for European Public Administrations (ISA) Programme&lt;ref name="ISA"&gt;[http://ec.europa.eu/isa/], Interoperability Solutions for European Public Administrations (ISA) Programme&lt;/ref&gt; of the [[European Commission]] to help publishers of standards document what their standards are about (their name, their status, theme, version, etc.) and where they can be found on the Web. ADMS descriptions can then be published on different websites while the standard itself remains on the website of its publisher (i.e. syndication of content). ADMS embraces the multi-publisher environment and, at the same time, it provides the means for the creation of aggregated catalogues of standards and single points of access to them based on ADMS descriptions. The Commission will offer a single point of access to standards described using ADMS via its collaborative platform, Joinup.&lt;ref name="Joinup"&gt;[https://joinup.ec.europa.eu/], Link to Joinup&lt;/ref&gt; The Federation&lt;ref name="Federation"&gt;[https://joinup.ec.europa.eu/elibrary/document/adms-enabled-federation-semantic-asset-repositories-brochure], Link to the brochure of the Federation of Semantic Asset Repositories&lt;/ref&gt; service will increase the visibility of standards described with ADMS on the web. This will also stimulate their reuse by Pan-European initiatives.

==ADMS Working Group==
More than 43 people of 20 EU Member States as well as from the US and Australia have participated in the [https://joinup.ec.europa.eu/asset/adms/document/adms-working-group ADMS Working Group]. Most of them were experts from standardisation bodies, research centres and the EU Commission. The working group used a methodology based on [[W3C]]&#8217;s processes and methods.&lt;ref name="CoreVocsPM"&gt;{{cite web|url=http://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |title=Archived copy |accessdate=2012-04-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130430164940/https://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |archivedate=2013-04-30 |df= }}, Link to ISA's Core Vocs methodology&lt;/ref&gt;

==How to download ADMS==
ADMS version 1 was officially released in April 2012.&lt;ref name="ADMSrelease"&gt;[https://joinup.ec.europa.eu/news/adms-v100-officially-released], ADMS V1 release announcement&lt;/ref&gt; Version 1.00 of ADMS is available for download on Joinup:&lt;ref name="Joinup"/&gt;
https://joinup.ec.europa.eu/asset/adms/release/100&lt;ref name="ADMS1"&gt;[https://joinup.ec.europa.eu/], Link to ADMS v1&lt;/ref&gt;

ADMS is offered under ISA's Open Metadata Licence v1.1&lt;ref name="OpenMetadataLicense"&gt;[https://joinup.ec.europa.eu/category/licence/isa-open-metadata-licence-v11], ISA Open Metadata Licence v1.1&lt;/ref&gt;

==Related work==
The ADMS specification reuses existing [[metadata]] vocabularies and core vocabularies including:
* The [[Dublin Core]] Metadata Element Set (DCMES)&lt;ref&gt;http://dublincore.org/documents/dces/&lt;/ref&gt;
* The [[Data Catalog Vocabulary]] (DCAT) &lt;ref&gt;http://www.w3.org/TR/vocab-dcat/&lt;/ref&gt;
* The [[FOAF (software)|Friend of a Friend (FOAF) Ontology]]
* The [[vCard]] Ontology &lt;ref&gt;[http://www.w3.org/TR/vcard-rdf/], Representing vCard Objects in RDF&lt;/ref&gt;

==The future of ADMS==
ADMS v1.00 will be contributed to&lt;ref name="ADMScontributed"&gt;[https://joinup.ec.europa.eu/asset/adms/topic/adms-public-review-key-specifications-interoperability-developed-eus-isa-programme-], Announcement that key specifications for interoperability developed by the EU's ISA Programme will become W3C standards&lt;/ref&gt; W3C&#8217;s Government Linked Data (GLD) Working Group.&lt;ref name="W3C_GLD"&gt;[http://www.w3.org/2011/gld/wiki/Main_Page], W3 Government Linked Data (GLD) Working Group&lt;/ref&gt; This means that ADMS will be published by the GLD Working Group as First Public Working Drafts for further consultation within the context of the typical W3C standardization process. The desired outcome of that process will be the publication of ADMS as a W3C Recommendation available under W3C's Royalty-Free License.

The ADMS RDFS Vocabulary already has a w3.org namespace: [http://www.w3.org/ns/adms http://www.w3.org/ns/adms#].

==References==
{{reflist|30em}}

[[Category:Data management]]
[[Category:Metadata]]
[[Category:Technical communication]]</text>
      <sha1>olprxnd5keb39hxtue8jzyxopq13j7l</sha1>
    </revision>
  </page>
  <page>
    <title>Clone (database)</title>
    <ns>0</ns>
    <id>8586147</id>
    <revision>
      <id>674652410</id>
      <parentid>643822482</parentid>
      <timestamp>2015-08-05T08:42:26Z</timestamp>
      <contributor>
        <ip>180.214.240.254</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="776" xml:space="preserve">{{Multiple issues|
{{unreferenced|date=December 2006}}
{{orphan|date=February 2009}}
}}

A '''database clone ''' is a complete and separate copy of a database system that includes the business data , the [[DBMS]] software and any other application  tiers that make up the environment. Cloning is a different kind of operation to [[Data replication|replication]] and [[backup]]s in that the cloned environment is both fully functional and separate in its own right. Additionally the cloned environment may be modified at its inception due to configuration changes or data subsetting.

The cloning refers to the replication of the server in order to have a backup, to upgrade the environment.

{{DEFAULTSORT:Clone (Database)}}
[[Category:Data management]]
[[Category:Databases]]</text>
      <sha1>002v73ur29c2req9sx9hsw6t5rv42xi</sha1>
    </revision>
  </page>
  <page>
    <title>Data flow diagram</title>
    <ns>0</ns>
    <id>1344164</id>
    <revision>
      <id>762717677</id>
      <parentid>762717389</parentid>
      <timestamp>2017-01-30T10:08:43Z</timestamp>
      <contributor>
        <username>Triptothecottage</username>
        <id>29005906</id>
      </contributor>
      <comment>/* Physical vs. logical DFD */ cleanup after merge</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6758" xml:space="preserve">{{context|date=July 2014}}
[[File:Data Flow Diagram Example.jpg|thumb|360px|Data flow diagram example.&lt;ref&gt;John Azzolini (2000). [http://ses.gsfc.nasa.gov/ses_data_2000/000712_Azzolini.ppt Introduction to Systems Engineering Practices]. July 2001.&lt;/ref&gt;]]

A '''data flow diagram''' ('''DFD''') is a graphical representation of the "flow" of data through an [[information system]], modelling its ''process'' aspects. A DFD is often used as a preliminary step to create an overview of the system, which can later be elaborated.&lt;ref&gt;Bruza, P. D., Van der Weide, Th. P., "The Semantics of Data Flow Diagrams", University of Nijmegen, 1993.&lt;/ref&gt; DFDs can also be used for the [[Data visualization|visualization]] of [[data processing]] (structured design).

A DFD shows what kind of information will be input to and output from the system, where the data will come from and go to, and where the data will be stored. It does not show information about the timing of process or information about whether processes will operate in sequence or in parallel (which is shown on a [[flowchart]]).

==History==
[[Larry Constantine]], the original developer of structured design,&lt;ref&gt;W. Stevens, G. Myers, L. Constantine, [http://domino.watson.ibm.com/tchjr/journalindex.nsf/d9f0a910ab8b637485256bc80066a393/a801ae3750be70ac85256bfa00685ded!OpenDocument "Structured Design"], IBM Systems Journal, 13 (2), 115-139, 1974.&lt;/ref&gt; based on Martin and Estrin's "Data Flow Graph"  model of computation.

Starting in the 1970s, data flow diagrams (DFD) became a popular way to visualize the major steps and data involved in software system processes. DFDs were usually used to show data flow in a computer system, although they could in theory be applied to [[business process modeling]]. DFD were useful to document the major data flows or to explore a new high-level design in terms of data flow.&lt;ref&gt;Craig Larman, "Applying UML and Patterns", Pearson Education, ISBN 978-81-7758-979-5&lt;/ref&gt;

==Theory==
[[File:DataFlowDiagram Example.png|thumb|360px|Data flow diagram example]]
[[File:Data-flow-diagram-notation.svg|thumb|160px|Data flow diagram - [[Edward Yourdon|Yourdon]]/[[Tom DeMarco|DeMarco]] notation]]

Data flow diagrams are also known as bubble charts.&lt;ref&gt;[http://www.orm.net/pdf/jcm13.pdf Introduced by Clive Finkelstein in Australia,  CACI in the UK, and later writers such as James Martin]&lt;/ref&gt; DFD is a designing tool used in the top-down approach to Systems Design. This context-level DFD is next "exploded", to produce a Level 1 DFD that shows some of the detail of the system being modeled. The Level 1 DFD shows how the system is divided into sub-systems (processes), each of which deals with one or more of the data flows to or from an external agent, and which together provide all of the functionality of the system as a whole. It also identifies internal data stores that must be present in order for the system to do its job, and shows the flow of data between the various parts of the system.

Data flow diagrams are one of the three essential perspectives of the structured-systems analysis and design method [[SSADM]]. The sponsor of a project and the end users will need to be briefed and consulted throughout all stages of a system's evolution.  With a data flow diagram, users are able to visualize how the system will operate, what the system will accomplish, and how the system will be implemented.  The old system's dataflow diagrams can be drawn up and compared with the new system's data flow diagrams to draw comparisons to implement a more efficient system. Data flow diagrams can be used to provide the end user with a physical idea of where the data they input ultimately has an effect upon the structure of the whole system from order to dispatch to report. How any system is developed can be determined through a data flow diagram model. 

In the course of developing a set of ''levelled'' data flow diagrams the analyst/designer is forced to address how the system may be decomposed into component sub-systems, and to identify the [[transaction data]] in the [[data model]].

Data flow diagrams can be used in both Analysis and Design phase of the [[Systems development life cycle|SDLC]].

There are  different notations to draw data flow diagrams (Yourdon &amp; Coad and [[Chris Gane (computer scientist)|Gane]] &amp; [[Trish Sarson|Sarson]]&lt;ref&gt;[[Chris Gane (computer scientist)|Chris Gane]] and [[Trish Sarson]]. ''Structured Systems Analysis: Tools and Techniques.'' McDonnell Douglas Systems Integration Company, 1977&lt;/ref&gt;), defining different visual representations for processes, data stores, data flow, and external entities.&lt;ref&gt;[http://www.smartdraw.com/tutorials/software/dfd/tutorial_01.htm How to draw Data Flow Diagrams]&lt;/ref&gt;

===Physical vs. logical DFD=== 
A logical DFD captures the data flows that are necessary for a system to operate. It describes the processes that are undertaken, the data required and produced by each process, and the stores needed to hold the data. On the other hand, a physical DFD shows how the system is actually implemented, either at the moment (Current Physical DFD), or how the designer intends it to be in the future (Required Physical DFD). Thus, a Physical DFD may be used to describe the set of data items that appear on each piece of paper that move around an office, and the fact that a particular set of pieces of paper are stored together in a filing cabinet. It is quite possible that a Physical DFD will include references to data that are duplicated, or redundant, and that the data stores, if implemented as a set of [[database table]]s, would constitute an un-normalised (or de-normalised) relational database. In contrast, a Logical DFD attempts to capture the data flow aspects of a system in a form that has neither redundancy nor duplication.

==See also==
* [[Activity diagram]]
* [[Business Process Model and Notation]]
* [[Control flow diagram]]
* [[Data island]]
* [[Dataflow]]
* [[Directed acyclic graph]]
* [[DRAKON|Drakon-chart]]
* [[Functional flow block diagram]]
* [[Function model]]
* [[IDEF0]]
* [[Logical Data Flow]]
* [[Pipeline (software)|Pipeline]]
* [[Structured Analysis and Design Technique]]
* [[Structure chart]]
* [[System context diagram]]
* [[Value stream mapping]]
* [[Workflow]]

==References==
{{Reflist}}

==Further reading==
*[[Scott W. Ambler]]. [http://www.agilemodeling.com/artifacts/dataFlowDiagram.htm The Object Primer 3rd Edition Agile Model Driven Development with UML 2]

==External links==
*{{Commons-inline}}

{{Data model}}
{{Authority control}}

{{DEFAULTSORT:Data Flow Diagram1}}
[[Category:Information systems]]
[[Category:Data management]]
[[Category:Diagrams]]
[[Category:Visualization (graphic)]]
[[Category:Systems analysis]]</text>
      <sha1>3hks82uqqw2psvpr5hfmptomozxhicz</sha1>
    </revision>
  </page>
  <page>
    <title>Write&#8211;write conflict</title>
    <ns>0</ns>
    <id>217748</id>
    <revision>
      <id>731667758</id>
      <parentid>664943200</parentid>
      <timestamp>2016-07-26T19:39:12Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>cap, punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1284" xml:space="preserve">In [[computer science]], in the field of [[database]]s, '''write&#8211;write conflict''', also known as '''overwriting [[commit (data management)|uncommitted]] data''' is a computational anomaly associated with interleaved execution of [[Database transaction|transactions]].

Given a [[Schedule (computer science)|schedule]] S

&lt;math&gt;S = \begin{bmatrix}
T1 &amp; T2 \\
W(A) &amp; \\
 &amp; W(B) \\
W(B) &amp; \\
Com. &amp; \\
 &amp; W(A)\\
 &amp; Com. \end{bmatrix}&lt;/math&gt;

note that there is no read in this schedule. The writes are called '''''blind writes'''''.

We have a '''''lost update'''''.  Any attempts to make this schedule serial would give off two different results (either T1's version of A and B is shown, or T2's version of A and B is shown), and would not be the same as the above schedule.  This schedule would not be [[Serializability|serializable]].

[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T1 out from B.  Unfortunately, [[deadlock]]s are something Strict 2PL does not overcome all the time.

== See also ==

* [[Concurrency control]]
* [[Read&#8211;write conflict]]
* [[Write&#8211;read conflict]]

==References==
{{reflist}}
{{Unreferenced|date=August 2009}}

{{DEFAULTSORT:Write-write conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>j4999757pqsyb3seqq7ty4tz35f56g1</sha1>
    </revision>
  </page>
  <page>
    <title>Data room</title>
    <ns>0</ns>
    <id>1216068</id>
    <revision>
      <id>755124661</id>
      <parentid>755124485</parentid>
      <timestamp>2016-12-16T10:27:39Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <minor />
      <comment>Refspam</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2918" xml:space="preserve">{{Refimprove|date=March 2015}}
'''Data rooms''' are spaces used for housing data, usually of a secure or privileged nature. They can be physical data rooms, [[virtual data room]]s, or [[data centers]].&lt;ref&gt;{{cite web|title=Data Room (entry)|url=http://financial-dictionary.thefreedictionary.com/Data+room|website=Financial Dictionary, The Free Dictionary by Farlex}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Data Room (entry)|url=http://www.nasdaq.com/investing/glossary/d/data-room|website=Nasdaq}}&lt;/ref&gt; They are used for a variety of purposes, including data storage, document exchange, file sharing, financial transactions, legal transactions, and more.

In mergers and acquisitions, the traditional data room will literally be a physically secure continually monitored room, normally in the vendor&#8217;s offices (or those of their lawyers), which the bidders and their advisers will visit in order to inspect and report on the various documents and other data made available. Often only one bidder at a time will be allowed to enter and if new documents or new versions of documents are required these will have to be brought in by [[courier]] as [[hardcopy]]. Teams involved in large [[due diligence]] processes will typically have to be flown in from many regions or countries and remain available throughout the process.  Such teams often comprise a number of experts in different fields and so the overall cost of keeping such groups on call near to the data room is often extremely high.  Combating the significant cost of physical datarooms is the [[virtual data room]], which provides for the secure, online dissemination of confidential information.

A [[virtual data room]] (VDR) is essentially a website with limited controlled access (using a secure log-on supplied by the vendor/authority which can be disabled at any time by the vendor/authority if a bidder withdraws) to which the bidders and their advisers are given access. Much of the information released will be confidential and restrictions should be applied to the viewers' ability to release this to third parties by forwarding, copying or printing. [[Digital rights management]] is sometimes applied to control information.

Detailed auditing must be provided for legal reasons so that a record is kept of who has seen which version of each document.

Data rooms are commonly used by [[legal]], [[accounting]], [[investment banking]] and [[private equity]] companies performing [[mergers and acquisitions]], [[fundraising]], [[insolvency]], [[corporate restructuring]], and joint ventures including bio-technology and tender processes.

==References==
{{Reflist}}
* [http://www.imaa-institute.org/docs/kummer-sliskovic_do%20virtual%20data%20rooms%20add%20value%20to%20the%20mergers%20and%20acquisitions%20process.pdf A report about the advantages and disadvantages of virtual vs. physical data rooms]

{{DEFAULTSORT:Data Room}}
[[Category:Data management]]</text>
      <sha1>otte3oxkfxd6vf4onb4jxb34ny6f274</sha1>
    </revision>
  </page>
  <page>
    <title>PhUSE</title>
    <ns>0</ns>
    <id>43000868</id>
    <revision>
      <id>760307557</id>
      <parentid>751624586</parentid>
      <timestamp>2017-01-16T05:10:27Z</timestamp>
      <contributor>
        <username>Syced</username>
        <id>223682</id>
      </contributor>
      <comment>illustration</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1538" xml:space="preserve">{{unreferenced|date=June 2014}}
[[File:PhUSE Computational Science Symposium 2016 (26133831630).jpg|thumb|PhUSE Computational Science Symposium 2016]]
'''PhUSE''', or '''Pharmaceutical Users Software Exchange''' is an independent, [[not-for-profit]] organization, that started in Europe, but now which serves as forum and global platform for [[clinical data management]], [[biostatistics]], and [[eClinical]] [[information technology]] professionals. It provides three collaboration platforms for members, a set online suits which implements worldwide collaboration tools, a [[wiki]], a repository of videos (PhUSE Tube), a [[blog]], a [[webforum]] and an [[archive]].

PhUSE also publishes '''[[Pharmaceutical Programming]]''',  an [[academic journal]] focusing on programming for [[Drug regulation|drug regulation environments]] of the [[pharmaceutical]] industry, a quarterly [[newsletter]], '''PhUSE News'''. In addition, it organizes an annual [[Meeting|conference]].

==External links==

* [http://www.phuse.eu/ Official web site]
* [http://www.phusewiki.org/wiki/index.php?title=PhUSE_Wiki PhUSE Wiki]
* [http://www.phuse.eu/blog/blog.aspx Blog]
* [http://www.phuse.eu/Society-Newsletters.aspx Newsletter]
* [http://www.phuse.eu/forum.aspx Forum]
* [http://www.phuse.eu/archive.aspx Archive]
* [http://www.phuse.eu/phusetube.aspx PhUSE Tube]
* [http://www.phuse.eu/publications.aspx Publications]

[[Category:Pharmacy organizations]]
[[Category:Regulation|Therapeutic goods]]
[[Category:Biostatistics]]
[[Category:Data management]]</text>
      <sha1>ntpzz02eu5tii83k85te9i2ebb8f8rd</sha1>
    </revision>
  </page>
  <page>
    <title>Computer-aided software engineering</title>
    <ns>0</ns>
    <id>627071</id>
    <revision>
      <id>761279235</id>
      <parentid>758196872</parentid>
      <timestamp>2017-01-22T01:54:42Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* Environments */Cleaning up..., [[WP:AWB/T|typo(s) fixed]]: For example &#8594; For example, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16889" xml:space="preserve">[[File:Umbrello 1.png|320px|thumb|Example of a CASE tool.]]
'''Computer-aided software engineering''' ('''CASE''') is the domain of software tools used to design and implement applications. CASE tools are similar to and were partly inspired by [[computer-aided design]] (CAD) tools used for designing hardware products.  CASE tools are used for developing high-quality, defect-free, and maintainable software.&lt;ref&gt;Kuhn, D.L (1989). "Selecting and effectively using a computer aided software engineering tool". Annual Westinghouse computer symposium; 6&#8211;7 Nov 1989; Pittsburgh, PA (U.S.); DOE Project.&lt;/ref&gt; CASE software is often associated with methods for the development of [[information system]]s together with automated tools that can be used in the [[software development process]].&lt;ref&gt;P. Loucopoulos and V. Karakostas (1995). ''System Requirements Engineerinuality software which will perform effectively.&lt;/ref&gt;

== History ==
The Information System Design and Optimization System (ISDOS) project, started in 1968 at the University of Michigan, initiated a great deal of interest in the whole concept of using computer systems to help analysts in the very difficult process of analysing requirements and developing systems. Several papers by Daniel Teichroew fired a whole generation of enthusiasts with the potential of automated systems development. His Problem Statement Language / Problem Statement Analyzer (PSL/PSA) tool was a CASE tool although it predated the term.&lt;ref&gt;{{cite journal|last1=Teichroew|first1=Daniel|last2=Hershey|first2=Ernest Allen|title=PSL/PSA a computer-aided technique for structured documentation and analysis of information processing systems|journal=Proceeding ICSE '76 Proceedings of the 2nd international conference on Software engineering|date=1976|url=http://dl.acm.org/citation.cfm?id=807641|publisher=IEEE Computer Society Press}}&lt;/ref&gt;

Another major thread emerged as a logical extension to the [[data dictionary]] of a [[database]]. By extending the range of [[metadata]] held, the attributes of an application could be held within a dictionary and used at runtime. This "active dictionary" became the precursor to the more modern [[model-driven engineering]] capability. However, the active dictionary did not provide a graphical representation of any of the metadata. It was the linking of the concept of a dictionary holding analysts' metadata, as derived from the use of an integrated set of techniques, together with the graphical representation of such data that gave rise to the earlier versions of CASE.&lt;ref&gt;{{cite book|last1=Coronel|first1=Carlos|last2=Morris|first2=Steven|title=Database Systems: Design, Implementation, &amp; Management|date=February 4, 2014|publisher=Cengage Learning|isbn=1285196147|pages=695&#8211;700|url=https://books.google.com/books?id=QWLpAgAAQBAJ&amp;pg=PA698&amp;lpg=PA698&amp;dq=case+tools+data+dictionary&amp;source=bl&amp;ots=eJt_GWYHGx&amp;sig=MZEMesWkJrGdczKSEZ_6bnqdNAY&amp;hl=en&amp;sa=X&amp;ei=HNF0VOvWDu3xigK5FQ&amp;ved=0CFIQ6AEwCQ#v=onepage&amp;q=case%20tools%20data%20dictionary&amp;f=false|accessdate=25 November 2014}}&lt;/ref&gt;

The term was originally coined by software company Nastec Corporation of Southfield, Michigan in 1982 with their original integrated graphics and text editor GraphiText, which also was the first microcomputer-based system to use hyperlinks to cross-reference text strings in documents&amp;mdash;an early forerunner of today's web page link.  GraphiText's successor product, DesignAid, was the first microprocessor-based tool to logically and semantically evaluate software and system design diagrams and build a data dictionary.

Under the direction of [[Albert F. Case, Jr.]] vice president for product management and consulting, and Vaughn Frick, director of product management, the DesignAid product suite was expanded to support analysis of a wide range of [[Structured Analysis and Design Technique|structured analysis and design methodologies]], including those of [[Ed Yourdon]] and [[Tom DeMarco]], [[Chris Gane (computer scientist)|Chris Gane]] &amp; [[Trish Sarson]], Ward-Mellor (real-time) SA/SD and [[Warnier-Orr]] (data driven).&lt;ref&gt;{{cite journal|last1=Case|first1=Albert|title=Computer-aided software engineering (CASE): technology for improving software development productivity|journal=ACM SIGMIS Database|date=Fall 1985|volume=17|issue=1|pages=35&#8211;43|url=http://dl.acm.org/citation.cfm?id=1040698}}&lt;/ref&gt;

The next entrant into the market was Excelerator from Index Technology in Cambridge, Mass.  While DesignAid ran on Convergent Technologies and later Burroughs Ngen networked microcomputers, Index launched Excelerator on the IBM PC/AT platform. While, at the time of launch, and for several years, the IBM platform did not support networking or a centralized database as did the Convergent Technologies or Burroughs machines, the allure of IBM was strong, and Excelerator came to prominence. Hot on the heels of Excelerator were a rash of offerings from companies such as Knowledgeware (James Martin, [[Fran Tarkenton]] and Don Addington), Texas Instrument's [[Information Engineering Facility|IEF]] and [[Andersen Consulting|Andersen Consulting's]] FOUNDATION toolset (DESIGN/1, INSTALL/1, FCP).

CASE tools were at their peak in the early 1990s.&lt;ref&gt;{{cite news|last1=Yourdon|first1=Ed|title=Can XP Projects Grow?|url=https://books.google.com/books?id=_faqtO2fEbkC&amp;pg=PA28&amp;lpg=PA28&amp;dq=CASE+tools+most+interest+90%27s&amp;source=bl&amp;ots=9WNDAYPU89&amp;sig=vC_s1JtRyOwcHcCvyDici5H9Z7w&amp;hl=en&amp;sa=X&amp;ei=lNd0VPr1De2rjAK6o4D4DA&amp;ved=0CDIQ6AEwAw#v=onepage&amp;q=CASE%20tools%20most%20interest%2090%27s&amp;f=false|accessdate=25 November 2014|publisher=Computerworld|date=Jul 23, 2001}}&lt;/ref&gt;  At the time [[IBM]] had proposed AD/Cycle, which was an alliance of software vendors centered on IBM's [[Software repository]] using [[IBM DB2]] in [[Mainframe computer|mainframe]] and [[OS/2]]:

:''The application development tools can be from several sources: from IBM, from vendors, and from the customers themselves. IBM has entered into relationships with [[Bachman Information Systems]], Index Technology Corporation, and [[KnowledgeWare|Knowledgeware]] wherein selected products from these vendors will be marketed through an IBM complementary marketing program to provide offerings that will help to achieve complete life-cycle coverage''.&lt;ref name="ADC_SAaA"&gt;"AD/Cycle strategy and architecture", IBM Systems Journal, Vol 29, NO 2, 1990; p. 172.&lt;/ref&gt;

With the decline of the mainframe, AD/Cycle and the Big CASE tools died off, opening the market for the mainstream CASE tools of today. Many of the leaders of the CASE market of the early 1990s ended up being purchased by [[Computer Associates]], including IEW, IEF, ADW, Cayenne, and Learmonth &amp; Burchett Management Systems (LBMS).  The other trend that led to the evolution of CASE tools was the rise of object-oriented methods and tools. Most of the various tool vendors added some support for object-oriented methods and tools.  In addition new products arose that were designed from the bottom up to support the object-oriented approach. Andersen developed its project Eagle as an alternative to Foundation. Several of the thought leaders in object-oriented development each developed their own methodology and CASE tool set: Jacobsen, Rumbaugh, [[Grady Booch|Booch]], etc. Eventually, these diverse tool sets and methods were consolidated via standards led by the [[Object Management Group]] (OMG). The OMG's [[Unified Modelling Language]] (UML) is currently widely accepted as the industry standard for object-oriented modeling.

== CASE software ==
A. Fuggetta classified CASE software into 3 categories:&lt;ref name="AF_93"&gt;{{cite journal
| author      = Alfonso Fuggetta
|date=December 1993
| title       = A classification of CASE technology
| journal     = Computer
| volume      = 26
| issue       = 12
| pages       = 25&#8211;38
| doi         = 10.1109/2.247645
| url         = http://www2.computer.org/portal/web/csdl/abs/mags/co/1993/12/rz025abs.htm
| accessdate  = 2009-03-14
}}
&lt;/ref&gt;
# ''Tools'' support specific tasks in the [[software life-cycle]].
# ''Workbenches'' combine two or more tools focused on a specific part of the software life-cycle.
# ''Environments'' combine two or more tools or workbenches and support the complete software life-cycle.

=== Tools ===
CASE tools supports specific tasks in the software development life-cycle. They can be divided into the following categories:
# Business and Analysis modeling. Graphical modeling tools. E.g., E/R modeling, object modeling, etc.
# Development. Design and construction phases of the life-cycle. Debugging environments. E.g., [[GNU Debugger]].
# Verification and validation. Analyze code and specifications for correctness, performance, etc.  
# Configuration management. Control the check-in and check-out of repository objects and files. E.g., [[Source Code Control System|SCCS]], CMS.
# Metrics and measurement. Analyze code for complexity, modularity (e.g., no "go to's"), performance, etc. 
# Project management. Manage project plans, task assignments, scheduling.
Another common way to distinguish CASE tools is the distinction between Upper CASE and Lower CASE. Upper CASE Tools support business and analysis modeling. They support traditional diagrammatic languages such as [[ER diagram]]s, [[Data flow diagram]], [[Structure chart]]s, [[Decision Tree]]s, [[Decision table]]s, etc. Lower CASE Tools support development activities, such as physical design, debugging, construction, testing, component integration, maintenance, and reverse engineering. All other activities span the entire life-cycle and apply equally to upper and lower CASE.&lt;ref&gt;Software Engineering: Tools, Principles and Techniques by Sangeeta Sabharwal, Umesh Publications&lt;/ref&gt;

=== Workbenches ===
Workbenches integrate two or more CASE tools and support specific software-process activities. Hence they achieve:
*a homogeneous and consistent interface (presentation integration).
*seamless integration of tools and tool chains (control and data integration).

An example workbench is Microsoft's [[Visual Basic]] programming environment. It incorporates several development tools: a GUI builder, smart code editor, debugger, etc. Most commercial CASE products tended to be such workbenches that seamlessly integrated two or more tools. Workbenches also can be classified in the same manner as tools; as focusing on Analysis, Development, Verification, etc. as well as being focused on upper case, lower case, or processes such as configuration management that span the complete life-cycle.

=== Environments ===
An environment is a collection of CASE tools or workbenches that attempts to support the complete software process. This contrasts with tools that focus on one specific task or a specific part of the life-cycle. CASE environments are classified by Fuggetta as follows:&lt;ref name="AF_93" /&gt;
#Toolkits. Loosely coupled collections of tools. These typically build on operating system workbenches such as the Unix Programmer's Workbench or the VMS VAX set. They typically perform integration via piping or some other basic mechanism to share data and pass control. The strength of easy integration is also one of the drawbacks. Simple passing of parameters via technologies such as shell scripting can't provide the kind of sophisticated integration that a common repository database can. 
#Fourth generation. These environments are also known as 4GL standing for fourth generation language environments due to the fact that the early environments were designed around specific languages such as Visual Basic. They were the first environments to provide deep integration of multiple tools. Typically these environments were focused on specific types of applications. For example, user-interface driven applications that did standard atomic transactions to a relational database. Examples are Informix 4GL, and Focus.
#Language-centered. Environments based on a single often object-oriented language such as the Symbolics Lisp Genera environment or VisualWorks Smalltalk from Parcplace. In these environments all the operating system resources were objects in the object-oriented language. This provides powerful debugging and graphical opportunities but the code developed is mostly limited to the specific language. For this reason, these environments were mostly a niche within CASE. Their use was mostly for prototyping and R&amp;D projects. A common core idea for these environments was the [[model-view-controller]] user interface that facilitated keeping multiple presentations of the same design consistent with the underlying model. The MVC architecture was adopted by the other types of CASE environments as well as many of the applications that were built with them. 
#Integrated. These environments are an example of what most IT people tend to think of first when they think of CASE. Environments such as IBM's AD/Cycle, Andersen Consulting's FOUNDATION, the ICL [[CADES]] system, and DEC Cohesion. These environments attempt to cover the complete life-cycle from analysis to maintenance and provide an integrated database repository for storing all artifacts of the software process. The integrated software repository was the defining feature for these kinds of tools. They provided multiple different design models as well as support for code in heterogenous languages. One of the main goals for these types of environments was "round trip engineering": being able to make changes at the design level and have those automatically be reflected in the code and vice versa. These environments were also typically associated with a particular methodology for software development. For example, the FOUNDATION CASE suite from Andersen was closely tied to the Andersen Method/1 methodology.
#Process-centered. This is the most ambitious type of integration. These environments attempt to not just formally specify the analysis and design objects of the software process but the actual process itself and to use that formal process to control and guide software projects. Examples are East, Enterprise II, Process Wise, Process Weaver, and Arcadia. These environments were by definition tied to some methodology since the software process itself is part of the environment and can control many aspects of tool invocation.

In practice, the distinction between workbenches and environments was flexible. Visual Basic for example was a programming workbench but was also considered a 4GL environment by many. The features that distinguished workbenches from environments were deep integration via a shared repository or common language and some kind of methodology (integrated and process-centered environments) or domain (4GL) specificity.&lt;ref name="AF_93" /&gt;

== Major CASE Risk Factors ==
Some of the most significant risk factors for organizations adopting CASE technology include: 
* Inadequate standardization. Organizations usually have to tailor and adopt methodologies and tools to their specific requirements. Doing so may require significant effort to integrate both divergent technologies as well as divergent methods. For example, before the adoption of the UML standard the diagram conventions and methods for designing object-oriented models were vastly different among followers of Jacobsen, Booch, Rumbaugh, 
* Unrealistic expectations. The proponents of CASE technology&#8212;especially vendors marketing expensive tool sets&#8212;often hype expectations that the new approach will be a silver bullet that solves all problems. In reality no such technology can do that and if organizations approach CASE with unrealistic expectations they will inevitably be disappointed. 
* Inadequate training. As with any new technology, CASE requires time to train people in how to use the tools and to get up to speed with them. CASE projects can fail if practitioners are not given adequate time for training or if the first project attempted with the new technology is itself highly mission critical and fraught with risk. 
* Inadequate process control. CASE provides significant new capabilities to utilize new types of tools in innovative ways. Without the proper process guidance and controls these new capabilities can cause significant new problems as well.&lt;ref&gt;[http://ithandbook.ffiec.gov/it-booklets/development-and-acquisition/development-procedures/software-development-techniques/computer-aided-software-engineering.aspx Computer Aided Software Engineering]. In: ''FFIEC IT Examination Handbook InfoBase''. Retrieved 3 Mar 2012.&lt;/ref&gt;

== See also ==
* [[Data modeling]]
* [[Domain-specific modeling]]
* [[Method engineering]]
* [[Model-driven architecture]]
* [[Modeling language]]
* [[Rapid application development]]
* [[Model-based architecture]]

== References ==
{{reflist}}

{{Authority control}}

[[Category:Computer-aided software engineering tools|*]]
[[Category:Data management]]</text>
      <sha1>dfcbvnmyv9w4gihg3l0k73r0ip5fxof</sha1>
    </revision>
  </page>
  <page>
    <title>NCSA Brown Dog</title>
    <ns>0</ns>
    <id>43444201</id>
    <revision>
      <id>731682627</id>
      <parentid>698306369</parentid>
      <timestamp>2016-07-26T21:15:11Z</timestamp>
      <contributor>
        <username>Sbrad77</username>
        <id>28839370</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11863" xml:space="preserve">
'''NCSA Brown Dog''' is a research project to develop a method for easily accessing historic research data stored in order to maintain the long-term viability of large bodies of scientific research. It is supported by the [[National Center for Supercomputing Applications]] (NCSA) that is funded by the [[National Science Foundation]] (NSF).&lt;ref name=bdweb&gt;{{cite web|title=Brown Dog|url=http://browndog.ncsa.illinois.edu|website=NCSA Brown Dog|accessdate=31 July 2014}}&lt;/ref&gt; 

==History==
Brown Dog is part of the [[Datanet|DataNet]] partners program funded by NSF in 2008. DataNet was conceived to address the increasingly digital and data-intensive nature of science, engineering and education. Brown Dog is part of a follow-on effort called [[Data Infrastructure Building Blocks (DIBBs)]], focused on building software to support DataNet. The project was proposed by researchers at NCSA and the [[University of Illinois Urbana-Champaign]] as well as researchers from [[Boston University]] and the [[University of North Carolina at Chapel Hill]].

==Unstructured, uncurated, long tail data==
Much scientific data is smaller, [[Unstructured data|unstructured]] and uncurated and thus not easily shared. Such data is sometimes referred to as "long tail" data. This borrows a term from statistics and refers to the tail of the distribution of project sizes. The majority of smaller projects lack the resources to properly steward the data they produce. This so-called &#8220;long tail&#8221; data, both past and present, has the potential to inform future research in many study areas. Much of this data has become inaccessible due to obsolete software and file formats. The resulting impossibility of reviewing data from older research disrupts the overall scientific research project.&lt;ref&gt;{{cite web|title=DataUp&#8212;Data Curation for the Long Tail of Science|url=http://blogs.msdn.com/b/msr_er/archive/2012/10/02/dataup-data-curation-for-the-long-tail-of-science.aspx|website=Microsoft Research Connections Blog|publisher=Microsoft Research Connections Team|accessdate=7 August 2014}}&lt;/ref&gt;

==Approach==
Brown Dog describes itself as the &#8220;super mutt&#8221; of software&lt;ref&gt;{{cite web|last1=Woodie|first1=Alex|title=NCSA Project Aims to Create a DNS-Like Service for Data|url=http://www.datanami.com/2014/01/06/ncsa_project_aims_to_create_a_dns-like_service_for_data/|website=datanami|accessdate=7 August 2014}}&lt;/ref&gt; (thus the name &#8220;Brown Dog&#8221;), serving as a low-level data infrastructure to interface digital data content across the internet.  Its approach is to use every possible source of automated help (i.e., software) in existence in a robust and provenance-preserving manner to create a service that can deal with as much of this data as possible.&lt;ref&gt;{{cite web|last1=Pletz|first1=John|title=U of I researchers get millions for 'super mutt' to sniff out big-data trends|url=http://www.chicagobusiness.com/article/20131202/blogs11/131129794/u-of-i-researchers-get-millions-for-super-mutt-to-sniff-out-big-data-trends|website=Chicago Business|publisher=Crain Communications, Inc.|accessdate=7 August 2014}}&lt;/ref&gt; The project sees the broader impact of its work in its potential to serve the general public as a sort of &#8220;DNS for data&#8221;, with the goal of making all data and all file formats as accessible as webpages are today.

==Technology==
Brown Dog seeks to address problems involving the use of uncurated and unstructured data collections through the development of two services: the Data Access Proxy (DAP) to aid in the conversion of file formats and the Data Tilling Services (DTS) for the automatic extraction of metadata from file contents. Once developed, researchers and general public users will be able to download browser plugins and other tools from the Brown Dog tool catalog.&lt;ref name="bdweb" /&gt;&lt;ref&gt;{{cite web|last1=Jewett|first1=Barbara|title=DATA SET FREE|url=http://www.ncsa.illinois.edu/news/stories/KentonMcHenry/|website=NCSA Access Magazine|publisher=NCSA|accessdate=7 August 2014}}&lt;/ref&gt;

===Data Tilling Service===
Data Tilling Service (DTS) will allow users to search data collections using an existing file to discover other similar files in a collection. A DTS search field will be appended to configured browsers where example files can be dropped. This tells DTS to search all the files under a given [[URL]] for files similar to the dropped file. For example, while browsing an online image collection, a user could drop an image of three people into the search field, and the DTS would return all images in the collection that also contain three people. If DTS encounters a foreign file format, it will utilize DAP to make the file accessible. DTS also indexes the data and extract and appends metadata to files and collections enabling users to gain some sense of the type of data they are encountering.

This service runs on port 9443.

===Data Access Proxy===
Data Access Proxy (DAP) allows users to access data files that would otherwise be unreadable. Similar to an internet gateway or [[Domain Name System|Domain Name Service]], the DAP configuration would be entered into a user&#8217;s machine and browser settings. Data requests over [[HTTP]] would first be examined by DAP to determine if the native file format is readable on the client device. If not, DAP converts the file into the best available format readable by the client machine.  Alternatively, the user could specify the desired format themselves.

This service runs on port 8184.

==Use cases==
Brown Dog targets three [[use cases]] proposed by groups within the [http://earthcube.org EarthCube] research communities. Developers and researchers from these communities will work together on use cases that span [[geoscience]], [[engineering]], [[biology]] and [[social science]].

===Long tail vegetation data in ecology and global change biology===
This use case is led by [http://people.bu.edu/dietze/ '''Michael Dietze'''], [http://www.bu.edu/ Boston University]

&lt;blockquote&gt;Data on the abundance, species composition, and size structure of vegetation is critically important for a wide array of sub-disciplines in ecology, conservation, natural resource management, and global change biology. However, addressing many of the pressing questions in these disciplines will require that terrestrial biosphere and hydrologic models are able to assimilate the large amount of long-tail data that exists but is largely inaccessible. The Brown Dog team in cooperation with researches from Dietze's lab will facilitate the capture of a huge body of smaller research-oriented vegetation data sets collected over many decades and historical vegetation data embedded in Public Land Survey data dating back to 1785. This data will be used as initial conditions for models, to make sense of other large data sets and for model calibration and validation.&lt;ref name=bdweb /&gt;&lt;ref name=newswise&gt;{{cite web|title=BU Scientist, Collaborators Get $10.5 Million Grant to Develop Software for un-Curated Data|url=http://www.newswise.com/articles/bu-scientist-collaborators-get-10-5-million-grant-to-develop-software-for-un-curated-data|website=www.newswise.com|publisher=Boston University College of Arts and Sciences|accessdate=7 August 2014}}&lt;/ref&gt;&lt;/blockquote&gt;

===Designing green infrastructure considering storm water and human requirements===
This use case is led by [http://eisa.ncsa.illinois.edu/ Barbara Minsker], [http://illinois.edu University of Illinois at Urbana-Champaign];  [http://willsull.net/research/ William Sullivan], University of Illinois at Urbana-Champaign; [http://cee.illinois.edu/faculty/arthurschmidt Arthur Schmidt], University of Illinois at Urbana-Champaign

&lt;blockquote&gt;This case study involves developing novel green infrastructure design criteria and models that integrate requirements for storm water management and ecosystem and human health and well being. To address the scientific and social problems associated with the design of green spaces, data accessibility and availability is a major challenge.  This study will focus on identified areas of the Green Healthy Neighborhood Planning region within the City of Chicago where existing local sewer performance is most deficient and where changes in impervious area through green infrastructure would be beneficial to under served neighborhoods. Brown Dog will be used to extract long-tail experimental data on human landscape preferences and health impacts. This data will be used to develop a human health impacts model that will then be linked together with a terrestrial biosphere model and a storm water model using Brown Dog technology.&lt;ref name=bdweb /&gt;&lt;/blockquote&gt;

===Development and application for critical zone studies===
This use case is led by [http://hydrocomplexity.net/index.html Praveen Kumar], University of Illinois at Urbana-Champaign

&lt;blockquote&gt;[[Earth's Critical Zone|Critical Zone]] (CZ) is the &#8220;skin&#8221; of the earth that extends from the treetops to the bedrock that is created by life processes working at scales from microbes to biomes. The Critical Zone supports all terrestrial living systems. Its upper part is the bio-mantle. This is where terrestrial biota live, reproduce, use and expend energy, and where their wastes and remains accumulate and decompose. It encompasses the soil, which acts as a geomembrane through which water and solutes, energy, gases, solids, and organisms interact with the atmosphere, biosphere, hydrosphere, and lithosphere. A variety of drivers affect this bio-dynamic zone, ranging from climate and deforestation to agriculture, grazing and human development. Understanding and predicting these effects is central to managing and sustaining vital ecosystem services such as soil fertility, water purification, and production of food resources, and, at larger scales, global carbon cycling and carbon sequestration.
The CZ provides a unifying framework for integrating terrestrial surface and near-surface environments, and reflects an intricate web of biological and chemical processes and human impacts occurring at vastly different temporal and spatial scales. The nature of these data create significant challenges for inter-disciplinary studies of the CZ because integration of the variety and number of data products and models has been a barrier. On the other hand, CZ data provides an excellent opportunity for defining, testing and implementing Brown Dog technologies. In this context &#8220;unstructured&#8221; data is viewed broadly as consisting of a collection of heterogeneous data with formats that reflect temporal and disciplinary legacies, data from emerging low cost open hardware based sensors and embedded sensor networks that lack well defined metadata and sensor characteristics, as well as data that are available as maps, images and text.&lt;ref name=bdweb /&gt;&lt;/blockquote&gt;

==NSF Award==
CIF21 DIBBs: Brown Dog was awarded in the winter of 2013 with a start date of October 1, 2013. Estimated expiration date is September 30, 2018.&lt;ref&gt;{{cite web|title=Award#1261582 - CIF21 DIBBs: Brown Dog|url=http://www.nsf.gov/awardsearch/showAward?AWD_ID=1261582&amp;HistoricalAwards=false|website=nsf.gov|accessdate=31 July 2014}}&lt;/ref&gt;

The award amount was $10,519,716.00, the largest DIBB award. The principal investigator is Kenton McHenry of NCSA at the University of Illinois at Urbana-Champaign. Coleaders are Jong Lee NCSA/UIUC; Barbara Minsker, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Praveen Kumar, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Michael Dietze, Department of Earth and Environment, Boston University.

==References==
{{reflist|30em}}

==External links==
* {{official website|http://browndog.ncsa.illinois.edu}}

[[Category:Data management]]
[[Category:National Science Foundation]]
[[Category:Research projects]]</text>
      <sha1>rsom5tobksrht6c33gpez3hmoo7hpku</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic heterogeneity</title>
    <ns>0</ns>
    <id>43972057</id>
    <revision>
      <id>757144058</id>
      <parentid>734610485</parentid>
      <timestamp>2016-12-29T01:38:16Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* Classification of semantic heterogeneities */ simplify heading, tweak italics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13832" xml:space="preserve">{{About|semantic differences in data|other uses|Heterogeneity (disambiguation)}}

'''Semantic heterogeneity''' is when [[database schema]] or [[Data set|datasets]] for the same domain are developed by independent parties, resulting in differences in meaning and interpretation of data values.&lt;ref&gt;{{cite journal |title=Why your data won't mix |author=Alon Halevy|journal=Queue |volume=3 |issue=8 |year=2005 |url=http://queue.acm.org/detail.cfm?id=1103836}}&lt;/ref&gt; Beyond [[Data structure|structured data]], the problem of semantic heterogeneity is compounded due to the flexibility of [[semi-structured data]] and various [[Tag (metadata)|tagging]] methods applied to documents or [[unstructured data]]. Semantic heterogeneity is one of the more important sources of differences in [[Heterogeneous database system|heterogeneous datasets]].

Yet, for multiple data sources to [[Interoperability|interoperate]] with one another, it is essential to reconcile these [[Semantics|semantic]] differences. Decomposing the various sources of semantic heterogeneities provides a basis for understanding how to map and transform data to overcome these differences.

== Classification ==

One of the first known classification schemes applied to [[Semantic data model|data semantics]] is from William Kent more than two decades ago.&lt;ref&gt;{{cite conference |title=The many forms of a single fact |author=William Kent |conference=Proceedings of the IEEE COMPCON |date=February 27 &#8211; March 3, 1989 |location=San Francisco |number=HPL-SAL-88-8, Hewlett-Packard Laboratories, Oct. 21, 1988 | at=13 pp. |url=http://www.bkent.net/Doc/manyform.htm}}&lt;/ref&gt; Kent's approach dealt more with structural [[Data mapping|mapping]] issues than differences in meaning, which he pointed to [[Data dictionary|data dictionaries]] as potentially solving.

One of the most comprehensive classifications is from Pluempitiwiriyawej and Hammer, "Classification Scheme for Semantic and Schematic Heterogeneities in XML Data Sources".&lt;ref&gt;{{cite news |title=A classification scheme for semantic and schematic heterogeneities in XML data sources |author=Charnyote Pluempitiwiriyawej and Joachim Hammer |publisher=University of Florida |at=Technical Report TR00-004 |location=Gainesville, Florida |date=September 2000 |url=https://cise.ufl.edu/tr/DOC/REP-2000-396.pdf}}&lt;/ref&gt; They classify heterogeneities into three broad classes:

* ''[[Data structure|Structural]]'' conflicts arise when the schema of the sources representing related or overlapping data exhibit discrepancies. Structural conflicts can be detected when comparing the underlying schema. The class of structural conflicts includes generalization conflicts, aggregation conflicts, internal path discrepancy, missing items, element ordering, constraint and type mismatch, and naming conflicts between the element types and attribute names.
* ''[[Data domain|Domain]]'' conflicts arise when the semantics of the data sources that will be integrated exhibit discrepancies. Domain conflicts can be detected by looking at the information contained in the schema and using knowledge about the underlying data domains. The class of domain conflicts includes schematic discrepancy, scale or unit, precision, and data representation conflicts.
* ''[[Data]]'' conflicts refer to discrepancies among similar or related data values across multiple sources. Data conflicts can only be detected by comparing the underlying sources. The class of data conflicts includes ID-value, missing data, incorrect spelling, and naming conflicts between the element contents and the attribute values.

Moreover, mismatches or conflicts can occur between set elements (a "population" mismatch) or attributes (a "description" mismatch).

Michael Bergman expanded upon this schema by adding a fourth major explicit category of language, and also added some examples of each kind of semantic heterogeneity, resulting in about 40 distinct potential categories &lt;ref&gt;{{cite web |title=Sources and classification of semantic heterogeneities |author=M.K. Bergman |website=AI3:::Adaptive Information |date=6 June 2006 |accessdate=28 September 2014 |url=http://www.mkbergman.com/232/sources-and-classification-of-semantic-heterogeneities/}}&lt;/ref&gt;
.&lt;ref&gt;{{cite web |title=Big structure and data interoperability |author=M.K. Bergman |website=AI3:::Adaptive Information |date=12 August 2014 |accessdate=28 September 2014 |url=http://www.mkbergman.com/1782/big-structure-and-data-interoperability/}}&lt;/ref&gt; This table shows the combined 40 possible sources of semantic heterogeneities across sources:

{|  style="text-align: left; width: 100%;" border="1" cellpadding="3" cellspacing="0"
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Class
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Category
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Subcategory
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Examples
|-
| rowspan="8" colspan="1" |
'''[[Language]]'''
| rowspan="4" colspan="1" |
[[Character encoding|Encoding]]
| Ingest Encoding Mismatch
|
For example, [[US-ASCII|ASCII]] ''v'' [[UTF-8]]
|-
| Ingest Encoding Lacking
| Mis-recognition of tokens because not being parsed with the proper encoding
|-
| Query Encoding Mismatch
| For example, ASCII ''v'' UTF-8 in search
|-
| Query Encoding Lacking
| Mis-recognition of search tokens because not being parsed with the proper encoding
|-
| rowspan="4" colspan="1" | Languages
| Script Mismatch
| Variations in how parsers handle, say, stemming, white spaces or hyphens
|-
| Parsing / Morphological Analysis Errors (many)
| Arabic languages (right-to-left) ''v'' Romance languages (left-to-right)
|-
| Syntactical Errors (many)
|
Ambiguous sentence references, such as ''I'm glad I'm a man, and so is Lola'' ([[Lola (song)|Lola]] by [[Ray Davies]] and the [[Kinks]])
|-
| Semantics Errors (many)
| River ''bank'' ''v'' money ''bank'' ''v'' billiards ''bank'' shot
|-
| rowspan="17" colspan="1" | '''Conceptual'''
| rowspan="5" colspan="1" | Naming
| Case Sensitivity
| Uppercase ''v'' lower case ''v'' Camel case
|-
|
[[Synonym]]s
| United States ''v'' USA ''v'' America ''v'' Uncle Sam ''v'' Great Satan
|-
|
[[Acronym]]s
| United States ''v'' USA ''v'' US
|-
|
[[Homonym]]s
| Such as when the same name refers to more than one concept, such as Name referring to a person ''v'' Name referring to a book
|-
| Misspellings
| As stated
|-
| rowspan="1" colspan="2" | Generalization / Specialization
| When single items in one schema are related to multiple items in another schema, or vice versa. For example, one schema may refer to "phone" but the other schema has multiple elements such as "home phone", "work phone" and "cell phone"
|-
| rowspan="2" colspan="1" | Aggregation
| Intra-aggregation
| When the same population is divided differently (such as, Census ''v'' Federal regions for states, England ''v'' Great Britain ''v'' United Kingdom, or full person names ''v'' first-middle-last)
|-
| Inter-aggregation
| May occur when sums or counts are included as set members
|-
| rowspan="1" colspan="2" | Internal Path Discrepancy
| Can arise from different source-target retrieval paths in two different schemas (for example, hierarchical structures where the elements are different levels of remove)
|-
| rowspan="4" colspan="1" | Missing Item
| Content Discrepancy
| Differences in set enumerations or including items or not (say, US territories) in a listing of US states
|-
| Missing Content
| Differences in scope coverage between two or more datasets for the same concept
|-
| Attribute List Discrepancy
| Differences in attribute completeness between two or more datasets
|-
| Missing Attribute
| Differences in scope coverage between two or more datasets for the same attribute
|-
| rowspan="2" colspan="2" | Item Equivalence
|
When two types (classes or sets) are asserted as being the same when the scope and reference are not (for example, [[Berlin]] the city ''v''  [[States of Germany#Subdivisions|Berlin]] the official city-state)
|-
|
When two individuals are asserted as being the same when they are actually distinct (for example, [[John F. Kennedy]] the president ''v''  [[USS John F. Kennedy (CV-67)|''John F. Kennedy'']] the aircraft carrier)
|-
| rowspan="1" colspan="2" | Type Mismatch
| When the same item is characterized by different types, such as a person being typed as an animal ''v'' human being ''v'' person
|-
| rowspan="1" colspan="2" | Constraint Mismatch
| When attributes referring to the same thing have different cardinalities or disjointedness assertions
|-
| rowspan="9" colspan="1" |
'''[[Domain of discourse|Domain]]'''
| rowspan="4" colspan="1" | Schematic Discrepancy
| Element-value to Element-label Mapping
| rowspan="4" colspan="1" | One of four errors that may occur when attribute names (say, Hair ''v'' Fur) may refer to the same attribute, or when same attribute names (say, Hair ''v'' Hair) may refer to different attribute scopes (say, Hair ''v'' Fur) or where values for these attributes may be the same but refer to different actual attributes or where values may differ but be for the same attribute and putative value. &lt;br /&gt;&lt;br /&gt; Many of the other semantic heterogeneities herein also contribute to schema discrepancies
|-
| Attribute-value to Element-label Mapping
|-
| Element-value to Attribute-label Mapping
|-
| Attribute-value to Attribute-label Mapping
|-
| rowspan="2" colspan="1" | Scale or Units
| Measurement Type
| Differences, say, in the metric ''v'' English measurement systems, or currencies
|-
| Units
| Differences, say, in meters ''v'' centimeters ''v'' millimeters
|-
| rowspan="1" colspan="2" | Precision
| For example, a value of 4.1 inches in one dataset ''v'' 4.106 in another dataset
|-
| rowspan="2" colspan="1" |
[[Data representation]]
| Primitive Data Type
|
Confusion often arises in the use of literals ''v'' [[Uniform resource identifier|URIs]] ''v'' object types
|-
| Data Format
| Delimiting decimals by period ''v'' commas; various date formats; using exponents or aggregate units (such as thousands or millions)
|-
| rowspan="8" colspan="1" |
'''[[Data]]'''
| rowspan="5" colspan="1" | Naming
| Case Sensitivity
| Uppercase ''v'' lower case ''v'' Camel case
|-
| Synonyms
| For example, centimeters ''v'' cm
|-
| Acronyms
| For example, currency symbols ''v'' currency names
|-
| Homonyms
| Such as when the same name refers to more than one attribute, such as Name referring to a person ''v'' Name referring to a book
|-
| Misspellings
| As stated
|-
| rowspan="1" colspan="2" | ID Mismatch or Missing ID
| URIs can be a particular problem here, due to actual mismatches but also use of name spaces or not and truncated URIs
|-
| rowspan="1" colspan="2" | Missing Data
|
A common problem, more acute with closed world approaches than with [[Open world assumption|open world ones]]
|-
| rowspan="1" colspan="2" | Element Ordering
| Set members can be ordered or unordered, and if ordered, the sequences of individual members or values can differ
|}

A different approach toward classifying semantics and integration approaches is taken by [[Amit Sheth|Sheth]] et al.&lt;ref&gt;{{cite journal |title=Semantics for the semantic Web: the implicit, the formal and the powerful | author1=Amit P. Sheth|author2=Cartic Ramakrishnan|author3=Christopher Thomas|journal=Int&#8217;l Journal on Semantic Web &amp; Information Systems |volume=1 |issue=1 |pages=1&#8211;18 |date=2005 |url=http://www.informatik.uni-trier.de/~ley/db/journals/ijswis/ijswis1.html}}&lt;/ref&gt; Under their concept, they split semantics into three forms: implicit, formal and powerful. Implicit semantics are what is either largely present or can easily be extracted; formal languages, though relatively scarce, occur in the form of [[Ontology (information science)|ontologies]] or other [[description logic]]s; and powerful (soft) semantics are fuzzy and not limited to rigid set-based assignments. Sheth et al.'s main point is that [[first-order logic]] (FOL) or description logic is inadequate alone to properly capture the needed semantics.

== Relevant applications ==

Besides data interoperabiity, relevant areas in [[information technology]] that depend on reconciling semantic heterogeneities include [[data mapping]], [[semantic integration]], and [[enterprise information integration]], among many others. From the conceptual to actual data, there are differences in perspective, vocabularies, measures and conventions once any two data sources are brought together. Explicit attention to these semantic heterogeneities is one means to get the information to integrate or interoperate.

A mere twenty years ago, information technology systems expressed and stored data in a multitude of formats and systems. The Internet and Web protocols have done much to overcome these sources of differences. While there is a large number of categories of semantic heterogeneity, these categories are also patterned and can be anticipated and corrected. These patterned sources inform what kind of work must be done to overcome semantic differences where they still reside.

==See also==
* [[Data integration]]
* [[Data mapping]]
* [[Enterprise information integration]]
* [[Heterogeneous database system]]
* [[Interoperability]]
* [[Ontology-based data integration]]
* [[Schema matching]]
* [[Semantic integration]]
* [[Semantic matching]]
* [[Semantics]]

==References==
&lt;references/&gt;

==Further reading==
* [http://wiki.opensemanticframework.org/index.php/Classification_of_Semantic_Heterogeneity Classification of semantic heterogeneity]

[[Category:Data management]]
[[Category:Interoperability]]
[[Category:Knowledge management]]
[[Category:Semantics]]</text>
      <sha1>g5zzdt1wsg86kg07m7degw1yq19ymqq</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-model database</title>
    <ns>0</ns>
    <id>44971098</id>
    <revision>
      <id>763089810</id>
      <parentid>762923464</parentid>
      <timestamp>2017-02-01T07:29:22Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Background */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8405" xml:space="preserve">Most database management systems are organized around a single [[Database model|data model]] that determines how data can be organized, stored, and manipulated. In contrast, a '''multi-model database''' is designed to support multiple data models against a single, integrated backend.&lt;ref name="neither"&gt;[http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]&lt;/ref&gt; Document, graph, relational, and key-value models are examples of data models that may be supported by a multi-model database.

== Background ==

The [[relational model|relational]] data model became popular after its publication by [[Edgar F. Codd]] in 1970. Due to increasing requirements for [[Scalability#Horizontal and vertical scaling|horizontal scalability]] and [[fault tolerance]], [[NoSQL]] databases became prominent after 2009. NoSQL databases use a variety of data models, with [[Document-oriented database|document]], [[Graph database|graph]], and key-value models being popular.&lt;ref name="rise"&gt;[http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]&lt;/ref&gt;

A Multi-model database is a database that can store, index and query data in more than one model. For some time, databases have primarily supported only one model, such as: [[Relational database]], [[Document-oriented database]], [[Graph database]] or [[Triplestore]]. A database that combines many of these is multi-model.

For some time, it was all but forgotten (or considered irrelevant) that there were any other database models besides Relational. The Relational model and notion of [[Third normal form]] were the de facto standard for all data storage. However, prior to the dominance of Relational data modeling from about 1980 to 2005 the [[Hierarchical database model]] was commonly used, and since 2000 or 2010, many [[NoSQL]] models that are non-relational including Documents, triples, key-value stores and graphs are popular. Arguably, geospatial data, temporal data and text data are also separate models, though indexed, queryable text data is generally termed a "[[search engine]]" rather than a database.

The first time the word "Multi-Model" has been associated to the databases was on May 30, 2012 in Cologne, Germany, during the Luca Garulli's key note "''NoSQL Adoption &#8211; What&#8217;s the Next Step?''".&lt;ref&gt;{{Cite journal|date=2012-06-01|title=Multi-Model storage 1/2 one product,|url=http://www.slideshare.net/lvca/no-sql-matters2012keynote/47-MultiModel_storage_12_one_product}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://2012.nosql-matters.org/cgn/index.html?p=1202.html#luca_garulli_keynote|title=Nosql Matters Conference 2012 {{!}} NoSQL Matters CGN 2012|website=2012.nosql-matters.org|access-date=2017-01-12}}&lt;/ref&gt; Luca Garulli envisioned the evolution of the 1st generation NoSQL products into new products with more features able to be used by multiple use cases.

A Multi-model database is most directly a response to the "[[Polyglot Persistence]]" approach of knitting together multiple database products, each handing a different model, to achieve a multi-model capability as described by Martin Fowler.&lt;ref name="polyglot"&gt;[http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]&lt;/ref&gt; This strategy has two major disadvantages: it leads to a significant increase in operational complexity, and there is no support for maintaining data consistency across the separate data stores, so Multi-model databases have begun to fill in this gap.

Some stories of overcomplicated systems from un-necessary "frankenbeast" database integrations are found on the web.&lt;ref name="frankenbeast"&gt;[http://www.marklogic.com/blog/polyglot-persistence-done-right/ MarkLogic, "Avoiding the Frankenbeast"]&lt;/ref&gt;&lt;ref name="boring"&gt;[http://mcfunley.com/choose-boring-technology McKinley, "Choose Boring Technology"]&lt;/ref&gt;

Multi-model databases are intended to offer the data modeling advantages of [[Polyglot Persistence]],&lt;ref name="polyglot"/&gt; without its disadvantages. Operational complexity, in particular, is reduced through the use of a single data store.&lt;ref name="rise"/&gt;

== Databases ==
Multi-model databases include (in alphabetic order):
* [[ArangoDB]] - Document (JSON), Graph, Key-value
* [[CouchBase]] - Relational (SQL), Document
* [[CrateDB]] - Relational (SQL), Document (Lucene)
* [[MarkLogic]] - Document (XML and JSON), Graph (RDF with OWL/RDFS), text, geospatial, binary, SQL
* [[OrientDB]] - Document (JSON), Graph, Key-value, Text, Geospatial, Binary, SQL, Reactive

Note that the level of support for the various models varies widely, including the ability to query across models, fully index the internal structure of a model, transactional support, and optimization or query planning across models.
The first multi-model database was [[OrientDB]], created in 2010 as an answer to the fragmented NoSQL environment, with the goal of providing one product to replace multiple NoSQL databases.

== Architecture ==

The main difference between the available multi-model databases is related to their architectures. Multi-model databases can support different models either within the engine or via different layers on top of the engine. Some products may provide an engine which supports documents and graphs while others provide layers on top of a key-key store.&lt;ref&gt;[http://blog.foundationdb.com/7-things-that-make-google-f1-and-the-foundationdb-sql-layer-so-strikingly-similar, "layer"]&lt;/ref&gt; With a layered architecture, each data model is provided via its own [[Component-based software engineering|component]].

== User-defined data models ==

In addition to offering multiple data models in a single data store, some databases allow developers to easily define custom data models. This capability is enabled by ACID transactions with high performance and scalability. In order for a custom data model to support concurrent updates, the database must be able to synchronize updates across multiple keys. ACID transactions, if they are sufficiently performant,  allow such synchronization.&lt;ref name="multiple"&gt;[http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]&lt;/ref&gt; JSON documents, graphs, and relational tables can all be implemented in a manner that inherits the horizontal scalability and fault-tolerance of the underlying data store.

== See also ==
&lt;!-- please do not list specific implementations here --&gt;
* [[Comparison of multi-model databases]]
* [[ACID]]
* [[NoSQL]]
* [[Comparison of structured storage software]]
* [[Database transaction]]
* [[Distributed database]]
* [[Distributed transaction]]
* [[Document-oriented database]]
* [[Graph database]]
* [[Relational model]]

== References ==
{{Reflist|2}}

== External links ==
* [http://www.orientechnologies.com/docs/last/orientdb.wiki/Tutorial-Document-and-graph-model.html OrientDB Document and Graph Model]
* [https://www.arangodb.com/key-features ArangoDB Key Features]
* [https://foundationdb.com/try/multi-model FoundationDB Multi-Model Architecture]
* [http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]
* [http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]
* [http://www.odbms.org/blog/2013/10/on-multi-model-databases-interview-with-martin-schonert-and-frank-celler/ ODBMS, "On Multi-Model Databases. Interview with Martin Sch&#246;nert and Frank Celler."]
* [http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]
* [http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]
* [https://crate.io/docs/reference/storage_consistency.html, Crate.IO Storage and Consistency]
* [http://www.marklogic.com/blog/tag/multi-model-database/, MarkLogic on Multi-model databases]

{{DEFAULTSORT:Multi-model Database}}
[[Category:Applications of distributed computing]]
[[Category:Databases]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
[[Category:Distributed data stores]]
[[Category:NoSQL]]
[[Category:Structured storage]]
[[Category:Transaction processing]]</text>
      <sha1>smn3zf81lfkiey4voug5gpdsmiayms5</sha1>
    </revision>
  </page>
  <page>
    <title>MaPS S.A.</title>
    <ns>0</ns>
    <id>46489271</id>
    <revision>
      <id>743007109</id>
      <parentid>719527623</parentid>
      <timestamp>2016-10-07T05:45:42Z</timestamp>
      <contributor>
        <username>Cnwilliams</username>
        <id>10190671</id>
      </contributor>
      <minor />
      <comment>Disambiguated: [[PLC]] &#8594; [[Public limited company]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2927" xml:space="preserve">{{Orphan|date=July 2015}}

{{Infobox company
| name             = MaPS System 
| image            = [[File:MaPS-System Logo.png|250px]]
| industry         = [[Public limited company|PLC]]
| founder          = Thierry Muller
| headquarters     = [[Luxembourg]]
| area_served      = [[France]], [[Luxembourg]], [[Germany]], [[Switzerland]], [[Belgium]]
| products         = [[Product Information Management]], [[Digital Asset Management]], [[Master Data Management]] and [[Business Process Management]]
| homepage         = http://www.maps-system.com/
}}

'''MaPS S.A.''' is a software editor founded in 2011 by Thierry Muller which is headquartered in [[Luxembourg]]. Its platform, called MaPS System, provides [[Data management|Data Management]] solutions for [[Multichannel Marketing]].

==History and Funding==

In 1999, the founder, had realized that certain challenges arose with several tools for [[Customer relationship management|Customer Relationship Management]], [[Public relations|Public Relations]] and in particular [[Marketing]] tools set in place. Complex data structures developed difficulties for organizations who lost focus of their dispersed data when wanting to operate and sell in an international and [[Multichannel Marketing|Multichannel]] environment.

The founder drafted his initial ideas on the topic of [[Multichannel Marketing]] and developed his first version of MaPS System under the agency Prem1um S.A. in 2005, which in combination with the [[Data Management]] solution also provided various Multimedia &amp; Marketing activities.

In 2011, after being successful, Prem1um S.A. decided to enable the software MaPS System to operate independently under MaPS S.A., as a separate company and editor of the software. The first financial supports were provided by Malta ICI, a Venture Capital firm, and the local partner Chameleon Invest, a seed-capital fund led by Business Angels, who invested &#8364;900.000. In a second investment round in 2014 led by Newion Investments, a Venture Capital firm, &#8364;1.4 Million were raised, thus amounting to total assets of &#8364;2.2 Million.

==Products==
The services included in MaPS System range from the data centralization, [[Data Governance]] to an optimized [[Multichannel Marketing]]. The software today features more than 35 modules for [[Master Data Management]], [[Product Information Management]], [[Digital Asset Management]], [[Business Process Management]] including catalogue [[Publishing]] features.

==References==
* {{Official website | http://www.maps-system.com/ MaPS S.A.}}
* Newion invests in MaPS System | http://www.newion-investments.com/news/newion-invests-in-maps-system/1]
* Introducing MaPS System | http://www.siliconluxembourg.lu/introducing-maps-system-a-centralized-information-management-solution/]

[[Category:Data management software]]
[[Category:Data management]]
[[Category:Software companies]]
[[Category:Companies of Luxembourg]]</text>
      <sha1>ixv68bp67cxl0siqe0ja3evuxkxtw1b</sha1>
    </revision>
  </page>
  <page>
    <title>Chunked transfer encoding</title>
    <ns>0</ns>
    <id>7061159</id>
    <revision>
      <id>756992059</id>
      <parentid>756988961</parentid>
      <timestamp>2016-12-28T03:16:29Z</timestamp>
      <contributor>
        <username>StrokeOfMidnight</username>
        <id>28125651</id>
      </contributor>
      <comment>Undid revision 756988961 by [[Special:Contributions/2600:8805:D880:844:4462:6DA1:7D2:E25A|2600:8805:D880:844:4462:6DA1:7D2:E25A]] ([[User talk:2600:8805:D880:844:4462:6DA1:7D2:E25A|talk]]): not a typo; chunk size is in hex format</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7142" xml:space="preserve">{{refimprove|date=June 2014}}
'''Chunked transfer encoding''' is a data transfer mechanism in version 1.1 of the [[Hypertext Transfer Protocol]] (HTTP) in which data is sent in a series of "chunks". It uses the [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]] HTTP header in place of the [[Content-Length]] header, which the earlier version of the protocol would otherwise require.&lt;ref&gt;http://tools.ietf.org/html/rfc1945#section-7.2&lt;/ref&gt; Because the Content-Length header is not used, the sender does not need to know the length of the content before it starts transmitting a response to the receiver. Senders can begin transmitting dynamically-generated content before knowing the total size of that content.

The size of each chunk is sent right before the chunk itself so that the receiver can tell when it has finished receiving data for that chunk. The data transfer is terminated by a final chunk of length zero.

An early form of the chunked encoding was proposed in 1994.&lt;ref&gt;{{cite web|last=Connolly|first=Daniel|title=Content-Transfer-Encoding: packets for HTTP|url=http://1997.webhistory.org/www.lists/www-talk.1994q3/1147.html|accessdate=13 September 2013|date=27 Sep 1994|id=&amp;lt;9409271503.AA27488@austin2.hal.com&amp;gt;}}&lt;/ref&gt; Later it was standardized in HTTP 1.1.

==Rationale==
The introduction of chunked encoding provided various benefits:

* Chunked transfer encoding allows a server to maintain an [[HTTP persistent connection]] for dynamically generated content. In this case, the HTTP Content-Length header cannot be used to delimit the content and the next HTTP request/response, as the content size is as yet unknown. Chunked encoding has the benefit that it is not necessary to generate the full content before writing the header, as it allows streaming of content as chunks and explicitly signaling the end of the content, making the connection available for the next HTTP request/response.
* Chunked encoding allows the sender to send additional header fields after the message body. This is important in cases where values of a field cannot be known until the content has been produced, such as when the content of the message must be digitally signed. Without chunked encoding, the sender would have to buffer the content until it was complete in order to calculate a field value and send it before the content.

==Applicability==
For version 1.1 of the HTTP protocol, the chunked transfer mechanism is considered to be always and anyways acceptable, even if not listed in the [[List of HTTP header fields#te-request-header|TE]] (transfer encoding) request header field, and when used with other transfer mechanisms, should always be applied last to the transferred data and never more than one time. This transfer coding method also allows additional entity header fields to be sent after the last chunk if the client specified the "trailers" parameter as an argument of the TE field. The origin server of the response can also decide to send additional entity trailers even if the client did not specify the "trailers" option in the TE request field, but only if the metadata is optional (i.e. the client can use the received entity without them). Whenever the trailers are used, the server should list their names in the Trailer header field; 3 header field types are specifically prohibited from appearing as a trailer field:  [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]], [[List of HTTP header fields#content-length-response-header|Content-Length]] and [[List of HTTP header fields#trailer-response-header|Trailer]].

==Format==
If a &lt;tt&gt;Transfer-Encoding&lt;/tt&gt; field with a value of "&lt;tt&gt;chunked&lt;/tt&gt;" is specified in an HTTP message (either a request sent by a client or the response from the server), the body of the message consists of an unspecified number of chunks, a terminating chunk, trailer, and a final CRLF sequence (i.e. [[carriage return]] followed by [[line feed]]).

Each chunk starts with the number of [[Octet (computing)|octets]] of the data it embeds expressed as a [[hexadecimal]] number in [[ASCII]] followed by optional parameters (''chunk extension'') and a terminating CRLF sequence, followed by the chunk data. The chunk is terminated by CRLF.

If chunk extensions are provided, the chunk size is terminated by a semicolon and followed by the parameters, each also delimited by semicolons. Each parameter is encoded as an extension name followed by an optional equal sign and value. These parameters could be used for a running [[message digest]] or [[digital signature]], or to indicate an estimated transfer progress, for instance.

The terminating chunk is a regular chunk, with the exception that its length is zero. It is followed by the trailer, which consists of a (possibly empty) sequence of entity header fields. Normally, such header fields would be sent in the message's header; however, it may be more efficient to determine them after processing the entire message entity. In that case, it is useful to send those headers in the trailer.

Header fields that regulate the use of trailers are ''TE'' (used in requests), and ''Trailers'' (used in responses).

==Use with compression==

HTTP servers often use [[data compression|compression]] to optimize transmission, for example with &lt;tt&gt;Content-Encoding: [[gzip]]&lt;/tt&gt; or &lt;tt&gt;Content-Encoding: [[deflate]]&lt;/tt&gt;. If both compression and chunked encoding are enabled, then the content stream is first compressed, then chunked; so the chunk encoding itself is not compressed, and the data in each chunk is not compressed individually. The remote endpoint then decodes the stream by concatenating the chunks and uncompressing the result.

==Example==

===Encoded data===
In the following example, three chunks of length 4, 5 and 14 are shown. The chunk size is transferred as a hexadecimal number followed by \r\n as a line separator, followed by a chunk of data of the given size.

&lt;pre&gt;
4\r\n
Wiki\r\n
5\r\n
pedia\r\n
E\r\n
 in\r\n
\r\n
chunks.\r\n
0\r\n
\r\n
&lt;/pre&gt;

Note: the chunk size indicates the size of the chunk data and excludes the trailing CRLF ("\r\n").&lt;ref&gt;http://skrb.org/ietf/http_errata.html&lt;/ref&gt; In this particular example, the CRLF following "in" is counted toward the chunk size of 0xE (14). The CRLF in its own line is also counted toward the chunk size.
The period character at the end of "chunks" is the 14th character, so it is the
last data character in that chunk. The CRLF following the period is
the trailing CRLF, so it is not counted toward the chunk size of 0xE (14).

===Decoded data===
&lt;pre&gt;
Wikipedia in

chunks.
&lt;/pre&gt;

==See also==
* [[List of HTTP header fields]]

==References==
{{Reflist}}
{{Refbegin}}
* See [http://tools.ietf.org/html/rfc7230#section-4.1 RFC 7230 section 4.1] for further details of chunked encoding.
* The previous (obsoleted) version is at [https://tools.ietf.org/html/rfc2616#section-3.6.1 RFC 2616 section 3.6.1].
{{Refend}}

{{DEFAULTSORT:Chunked Transfer Encoding}}
[[Category:Data management]]
[[Category:Hypertext Transfer Protocol]]
[[Category:Hypertext Transfer Protocol headers]]</text>
      <sha1>el48fz3d726vzdy68dkbp2m8cuemjpn</sha1>
    </revision>
  </page>
  <page>
    <title>Copyright</title>
    <ns>0</ns>
    <id>5278</id>
    <revision>
      <id>762384948</id>
      <parentid>762384891</parentid>
      <timestamp>2017-01-28T13:39:30Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/Souda007|Souda007]] to version by Zzuuzz. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2913989) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="62606" xml:space="preserve">{{Redirect2|Copyrighting|Copyrights|the use of words to promote or advertise|Copywriting|the Wikipedia policy about copyright issues|Wikipedia:Copyrights}}
{{pp-move-indef|small=yes}}
{{Intellectual property}}
{{Capitalism|Concepts}}
{{Use dmy dates|date=January 2011}}
{{Use American English|date=January 2014}}

'''Copyright''' is a [[Natural and legal rights|legal right]] created by the law of a country that grants the creator of an original work [[exclusive right]]s for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by [[limitations and exceptions to copyright]] law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.&lt;ref&gt;{{cite web|url=http://www.bitlaw.com/copyright/unprotected.html#ideas|title=Works Unprotected by Copyright Law|publisher=Bitlaw|author=Daniel A. Tysver}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://digital-law-online.info/lpdi1.0/treatise9.html |title=Legal Protection of Digital Information |page=''Chapter 1: An Overview of Copyright'', Section II.E. Ideas Versus Expression.|author=Lee A. Hollaar}}&lt;/ref&gt;

Copyright is a form of [[intellectual property]], applicable to certain forms of creative work. Some, but not all jurisdictions require "fixing" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders.&lt;ref&gt;{{Citation|title=Copyright|publisher=University of California|year=2014|url=http://copyright.universityofcalifornia.edu/ownership/joint-works.html|accessdate=2014-12-15}}&lt;/ref&gt;&lt;ref&gt;http://www.jetlaw.org/publish/journal-conventions/&lt;/ref&gt;&lt;ref&gt;https://books.google.de/books?id=kz1F6uAHtaEC&amp;pg=PA81&amp;dq=%22rights+holder%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIHDAA#v=onepage&amp;q=%22rights%20holder%22&amp;f=false&lt;/ref&gt;&lt;ref&gt;https://books.google.de/books?id=xD_iBwAAQBAJ&amp;pg=PT465&amp;dq=%22rights+holder%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIKDAC#v=onepage&amp;q=%22rights%20holder%22&amp;f=false&lt;/ref&gt; These rights frequently include reproduction, control over [[derivative work]]s, distribution, [[Performing rights|public performance]], and "[[moral rights]]" such as attribution.&lt;ref&gt;{{Citation|title=17 U.S.C. &#167; 106|publisher=United States of America|year=2011|url=http://www.copyright.gov/title17/92chap1.html#106|accessdate=2014-12-15}}&lt;/ref&gt;

Copyrights are considered ''territorial rights'', which means that they do not extend beyond the territory of a specific jurisdiction.  While many aspects of national copyright laws have been standardized through [[international copyright agreements]], copyright laws vary by country.&lt;ref name="International Copyright Law Survey"&gt;{{cite web|url=http://worldcopyrightlaw.com/copyrightsurvey|title=International Copyright Law Survey|publisher=Mincov Law Corporation}}&lt;/ref&gt;

Typically, the [[Copyright term|duration of a copyright]] spans the author's life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction).  Some countries require certain copyright [[copyright formalities|formalities]] to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a [[civil law (common law)|civil]] matter, though some jurisdictions do apply [[criminal law|criminal]] sanctions.

Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.

==History==
{{main|History of copyright law}}
===Background===
Copyright came about with the invention of [[Printing press|the printing press]] and with wider literacy. As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th&amp;nbsp;century. [[Charles II of England]] was concerned by the unregulated [[copying]] of books and passed the [[Licensing of the Press Act 1662]] by Act of Parliament,&lt;ref&gt;''Copyright in Historical Perspective'', p. 136-137, Patterson, 1968, Vanderbilt Univ. Press&lt;/ref&gt; which established a register of licensed books and required a copy to be deposited with the [[Worshipful Company of Stationers and Newspaper Makers|Stationers' Company]], essentially continuing the licensing of material that had long been in effect.

Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in [[Europe]] and not, for example, in Asia. In the [[Middle Ages]] in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which [[capitalism]] led to the [[commodification]] of many aspects of social life that earlier had no monetary or economic value per&amp;nbsp;se.&lt;ref&gt;Bettig, Ronald V. (1996). ''Copyrighting Culture: The Political Economy of Intellectual Property. Westview Press''. p. 9&#8211;17. ISBN 0-8133-1385-6.&lt;/ref&gt;

Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as [[Sound recording and reproduction|sound recordings]], films, photographs, software, and architectural works.

===National copyrights===
{{See also|Statute of Anne|History of US Copyright Law}}
[[File:Statute of anne.jpg|thumb|left|The [[Statute of Anne]] (the Copyright Act 1709) came into force in 1710.]]
Often seen as the first real copyright law, the 1709 British [[Statute of Anne]] gave the publishers rights for a fixed period, after which the copyright expired.&lt;ref name="Rethinking copyright: history, theory, language"&gt;{{cite book|url=https://www.google.com/books?id=dMYXq9V1JBQC&amp;dq=statute+of+anne+copyright&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s |title=Rethinking copyright: history, theory, language |page=13 |author=Ronan, Deazley |isbn=978-1-84542-282-0 |year=2006 |publisher=Edward Elgar Publishing. |deadurl=yes |archiveurl=https://web.archive.org/web/20111119042246/https://www.google.com/books?id=dMYXq9V1JBQC&amp;dq=statute+of+anne+copyright&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s |archivedate=19 November 2011 }}&lt;/ref&gt;
The act also alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing... Books, and other Writings, without the Consent of the Authors... to their very great Detriment, and too often to the Ruin of them and their Families:".&lt;ref&gt;{{cite web|url=http://www.copyrighthistory.com/anne.html |title=Statute of Anne |publisher=Copyrighthistory.com |accessdate=2012-06-08}}&lt;/ref&gt; A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.

The [[Copyright Clause]] of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.

The original length of copyright in the United States was 14&amp;nbsp;years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14&#8209;year monopoly grant, but after that the work entered the [[public domain]], so it could be used and built upon by others.

Copyright law was enacted rather [[Copyright in Germany|late in German states]], and the historian Eckhard H&#246;ffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.&lt;ref name="thad"&gt;{{cite web |url=http://www.spiegel.de/international/zeitgeist/0,1518,710976,00.html | author=Frank Thadeusz |title=No Copyright Law: The Real Reason for Germany's Industrial Expansion? |publisher=[[Der Spiegel]] |date= 18 August 2010 |accessdate= 11 April 2015}}&lt;/ref&gt;

===International copyright treaties===
{{See also|International copyright agreements|List of parties to international copyright agreements}}
[[File:Joseph Ferdinand Keppler - The Pirate Publisher - Puck Magazine - Restoration by Adam Cuerden.jpg|'' The Pirate Publisher&#8212;An International Burlesque that has the Longest Run on Record'', from ''[[Puck (magazine)|Puck]]'', 1886, satirizes the then-existing situation where a publisher could profit by simply stealing newly published works from one country, and publishing them in another, and vice versa.|thumb|300px]]
The 1886 [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] first established recognition of copyrights among [[Sovereignty|sovereign nations]], rather than merely bilaterally. Under the Berne Convention, copyrights for [[creative works]] do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention.&lt;ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5"&gt;{{cite web |url=http://www.wipo.int/treaties/en/ip/berne/trtdocs_wo001.html#P109_16834 |title=Berne Convention for the Protection of Literary and Artistic Works Article 5 |accessdate=2011-11-18 |publisher=World Intellectual Property Organization}}&lt;/ref&gt; As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100&amp;nbsp;years later with the passage of the ''Copyright, Designs and Patents Act of 1988''. The United States did not sign the Berne Convention until 1989.&lt;ref&gt;Garfinkle, Ann M; Fries, Janet; Lopez, Daniel; Possessky, Laura (1997). "Art conservation and the legal obligation to preserve artistic intent". [[JAIC]]   36 (2): 165&#8211;179.&lt;/ref&gt;

The United States and most [[Latin America]]n countries instead entered into the [[Buenos Aires Convention]] in 1910, which required a copyright notice on the work (such as ''[[all rights reserved]]''), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms.&lt;ref&gt;[http://www.copyright.gov/circs/circ38a.pdf "International Copyright Relations of the United States"], U.S.&amp;nbsp;Copyright Office Circular No.&amp;nbsp;38a, August&amp;nbsp;2003.&lt;/ref&gt;&lt;ref&gt;[http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf Parties to the Geneva Act of the Universal Copyright Convention] as of 2000-01-01: the dates given in the document are dates of ratification, not dates of coming into force. The Geneva Act came into force on 16 September 1955, for the first twelve to have ratified (which included four non-members of the Berne Union as required by Art.&amp;nbsp;9.1), or three months after ratification for other countries. {{webarchive |url=https://web.archive.org/web/20080625003242/http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf |date=25 June 2008 }}&lt;/ref&gt;&lt;ref&gt;[http://www.copyright.ht/en 165&amp;nbsp;Parties to the Berne Convention for the Protection of Literary and Artistic Works] as of May 2012.&lt;/ref&gt; The [[Universal Copyright Convention]] was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the [[Soviet Union]] and developing nations.

The regulations of the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] are incorporated into the [[World Trade Organization]]'s [[Agreement on Trade-Related Aspects of Intellectual Property Rights|TRIPS]] agreement (1995), thus giving the Berne Convention effectively near-global application.&lt;ref name="Contemporary Intellectual Property: Law and Policy"&gt;{{cite book |title=Contemporary Intellectual Property: Law and Policy|url= https://www.google.com/books?id=_Iwcn4pT0OoC&amp;dq=contemporary+intellectual+property&amp;source=gbs_navlinks_s |page=39 |author1=MacQueen, Hector L |author2=Charlotte Waelde |author3=Graeme T Laurie |isbn=978-0-19-926339-4 |year=2007 |publisher=Oxford University Press}}&lt;/ref&gt; 

In 1961, the [[United International Bureaux for the Protection of Intellectual Property]] signed the [[Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations]]. In 1996, this organization was succeeded by the founding of the [[World Intellectual Property Organization]], which launched the 1996 [[WIPO Performances and Phonograms Treaty]] and the 2002 [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]], which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The [[Trans-Pacific Partnership]] includes [[Trans-Pacific Partnership Intellectual Property Provisions|intellectual Property Provisions]] relating to copyright.

Copyright laws are standardized somewhat through these international conventions such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] and [[Universal Copyright Convention]]. These multilateral treaties have been ratified by nearly all countries, and [[international organizations]] such as the [[European Union]] or [[World Trade Organization]] require their member states to comply with them.

==Obtaining protection==
===Ownership===
The original holder of the copyright may be the employer of the author rather than the author himself, if the work is a "[[work for hire]]".&lt;ref&gt;17 U.S.C. &#167; 201(b); Cmty. for Creative Non-Violence v. Reid, 490 U.S. 730 (1989)&lt;/ref&gt; For example, in [[English law]] the ''Copyright, Designs and Patents Act'' 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire."

===Eligible works===
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by [[jurisdiction]], but these can include [[poem]]s, [[theses]], [[drama|plays]] and other [[book|literary works]], [[film|motion pictures]], [[choreography]], [[music|musical compositions]], [[sound recording]]s, [[painting]]s, [[drawing]]s, [[sculpture]]s, [[photography|photographs]], [[computer software]], [[radio]] and [[television]] [[Broadcasting|broadcasts]], and [[industrial design]]s. Graphic [[designs]] and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.&lt;ref name="Intellectual Property and Information Wealth: Copyright and related rights"&gt;{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights|url=https://www.google.com/books?id=tgK9BzcF5WgC&amp;dq=statute+of+anne+copyright&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s|page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.wipo.int/freepublications/en/intproperty/909/wipo_pub_909.pdf |format=PDF| last = World Intellectual Property Organization | title= Understanding Copyright and Related Rights|publisher=WIPO|accessdate=11 August 2016|page=8}}&lt;/ref&gt;

Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed.&lt;ref name="Art and copyright"&gt;{{cite book |title=Art and copyright|url=https://www.google.com/books?id=h-XBqKIryaQC&amp;dq=idea-expression+dichotomy&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s|pages=48&#8211;49 |author=Simon, Stokes |isbn=978-1-84113-225-9 |year=2001 |publisher=Hart Publishing }}&lt;/ref&gt; For example, the copyright to a [[Mickey Mouse]] cartoon restricts others from making copies of the cartoon or creating [[derivative work]]s based on [[The Walt Disney Company|Disney's]] particular [[anthropomorphic]] mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's.&lt;ref name="Art and copyright"/&gt; Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, [[Steamboat Willie]] is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.

===Originality===
{{main|Threshold of originality}}
Typically, a work must meet [[Threshold of originality|minimal standards of originality]] in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the [[United Kingdom]] there has to be some "skill, labour, and judgment" that has gone into it.&lt;ref&gt;''Express Newspaper Plc v News (UK) Plc'', F.S.R. 36 (1991)&lt;/ref&gt; In [[Australia]] and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a [[trademark]] instead.

Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.

===Registration===
{{main|Copyright registration}}
[[File:Fermat Last Theorem "proof" registered by Ukraine officials.jpg|thumb|right|A copyright certificate for proof of the Fermat theorem, issued by the State Department of Intellectual Property of Ukraine.]]
In all countries where the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights.&lt;ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5"/&gt; However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as ''[[prima facie]]'' evidence of a valid copyright and enables the copyright holder to seek [[statutory damages for copyright infringement|statutory damages]] and attorney's fees.&lt;ref&gt;{{cite web|title=Subject Matter and Scope of Copyright|url=http://copyright.gov/title17/92chap1.pdf|website=copyright.gov|accessdate=4 June 2015}}&lt;/ref&gt; (In the USA, registering after an infringement only enables one to receive actual damages and lost profits.)

A widely circulated strategy to avoid the cost of copyright registration is referred to as the [[poor man's copyright]]. It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the [[postmark]] to establish the date. This technique has not been recognized in any published opinions of the United States courts. &lt;!-- Note to editors: The previously-worded statement, "This technique has not been recognized by any United States court" is overbroad because not all such cases are reported, and it is impossible to know whether this is correct.--&gt; The United States Copyright Office says the technique is not a substitute for actual registration.&lt;ref&gt;{{cite web|title=Copyright in General (FAQ)|url=http://www.copyright.gov/help/faq/faq-general.html#poorman|publisher=U.S Copyright Office|accessdate=11 Aug 2016}}&lt;/ref&gt; The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is.&lt;ref&gt;[http://www.ipo.gov.uk/copy/c-claim/c-register.htm "Copyright Registers"], United Kingdom Intellectual Property Office&lt;/ref&gt;&lt;ref&gt;[http://www.ipo.gov.uk/types/copy/c-about/c-auto.htm "Automatic right"], United Kingdom Intellectual Property Office&lt;/ref&gt;  &lt;!-- Note to editors: The previously-worded statement, "The United Kingdom Intellectual Property Office discusses the technique but does not recommend its use." overstates the UK IPO position; the IPO does NOT recommend against the PMC approach.--&gt;

===Fixing===
The [[Berne Convention]] allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection.&lt;ref name="cyber.law.harvard.edu"&gt;See Harvard Law School, [http://cyber.law.harvard.edu/copyrightforlibrarians/Module_3:_The_Scope_of_Copyright_Law#Fixation ''Module 3: The Scope of Copyright Law'']. See also Tyler T. Ochoa, [http://digitalcommons.law.scu.edu/chtlj/vol20/iss4/5 ''Copyright, Derivative Works and Fixation: Is Galoob a Mirage, or Does the Form(GEN) of the Alleged Derivative Work Matter?''], 20 {{smallcaps|Santa Clara High Tech.}} L.J. 991, 999&#8211;1002 (2003) ("Thus, both the text of the Act and its legislative history demonstrate that Congress intended that a derivative work does not need to be fixed in order to infringe."). The legislative history of the 1976 Copyright Act says this difference was intended to address transitory works such as ballets, pantomimes, improvised performances, dumb shows, mime performances, and dancing.&lt;/ref&gt; U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance."&lt;ref name="cyber.law.harvard.edu"/&gt;

===Copyright notice===
{{main|Copyright notice}}
[[File:Copyright.svg|thumb|upright|A copyright symbol used in copyright notice.]]
Before 1989, United States law required the use of a copyright notice, consisting of the [[copyright symbol]] (&#169;, the letter '''C''' inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder.&lt;ref&gt;Copyright Act of 1976, {{USPL|94|553}}, 90 Stat. 2541, &#167; 401(a) (19 October 1976)&lt;/ref&gt;&lt;ref&gt;The Berne Convention Implementation Act of 1988 (BCIA), {{USPL|100|568}}, 102 Stat. 2853, 2857. One of the changes introduced by the BCIA was to section&amp;nbsp;401, which governs copyright notices on published copies, specifying that notices "may be placed on" such copies; prior to the BCIA, the statute read that notices "shall be placed on all" such copies. An analogous change was made in section&amp;nbsp;402, dealing with copyright notices on phonorecords.&lt;/ref&gt; Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a [[sound recording copyright symbol]] (&#8471;, the letter&amp;nbsp;'''P''' inside a circle), which indicates a sound recording copyright, with the letter&amp;nbsp;'''P''' indicating a "phonorecord". In addition, the phrase ''[[All rights reserved]]'' was once required to assert copyright, but that phrase is now legally obsolete.

In 1989 the United States enacted the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] Implementation Act, amending the 1976&amp;nbsp;Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic.&lt;ref&gt;{{cite web|url=http://www.copyright.gov/circs/circ03.pdf |title=U.S. Copyright Office &#8211; Information Circular |format=PDF |accessdate=2012-07-07}}&lt;/ref&gt; However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit&amp;nbsp;&#8211; using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.&lt;ref&gt;[[17 U.S.C.]]{{UnitedStatesCodeSec|17|401(d)}}&lt;/ref&gt;

==Enforcement==
Copyrights are generally enforced by the holder in a [[Civil law (private law)|civil law]] court, but there are also criminal infringement statutes in some jurisdictions. While [[copyright registry|central registries]] are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily [[legal proof|prove]] that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the [[RIAA]] are increasingly targeting the [[file sharing]] home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: [[Legal aspects of file sharing]])

In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative and or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.

===Copyright infringement===
{{main|Copyright infringement}}
For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws and/or adheres to a bilateral treaty or established international convention such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] or [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]]. Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.&lt;ref&gt;{{Cite journal | last1 = Owen | first1 = L. | doi = 10.1087/09531510125100313 | title = Piracy | journal = Learned Publishing | volume = 14 | pages = 67&#8211;70 | year = 2001 | pmid =  | pmc = }}&lt;/ref&gt;

Copyright infringement most often occurs to software, film and music. However, infringement upon books and other text works remains common, especially for educational reasons. Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available.&lt;ref&gt;Butler, S. Piracy Losses "Billboard" 199(36)&lt;/ref&gt; Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect.&lt;ref&gt;{{cite web|url=http://www.ejpd.admin.ch/content/ejpd/de/home/dokumentation/mi/2011/2011-11-30.html |title=Urheberrechtsverletzungen im Internet: Der bestehende rechtliche Rahmen gen&#252;gt |publisher=Ejpd.admin.ch}}&lt;/ref&gt; In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.&lt;ref&gt;{{cite journal|publisher=Social Science Electronic Publishing|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2425386|title=Video Killed the Radio Star? Online Music Videos and Digital Music Sales|ISSN=2042-2695|year=2014|authors=Tobias Kretschmer &amp; Christian Peukert}}&lt;/ref&gt;

==Rights granted==
===Exclusive rights===
Several exclusive rights typically attach to the holder of a copyright:
* to produce copies or reproductions of the work and to sell those copies (including, typically, electronic copies)
* to import or export the work
* to create [[derivative work]]s (works that adapt the original work)
* to perform or display the work publicly
* to sell or cede these rights to others
* to transmit or display by radio or video.&lt;ref name=autogenerated1&gt;{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights |page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}&lt;/ref&gt;

The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the [[unregistered design right]] in [[English law]] and [[European law]]. The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a [[Philosophy of copyright|philosophical interpretation of copyright law]] that is not universally shared. There is also debate on whether copyright should be considered a [[property right]] or a [[Moral rights (copyright law)|moral right]].&lt;ref&gt;Tom G. Palmer, [http://www.tomgpalmer.com/wp-content/uploads/papers/morallyjustified.pdf "Are Patents and Copyrights Morally Justified?"] Accessed 5 February 2013.&lt;/ref&gt;

If a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted.&lt;ref name="U.S Copyright Office - Copyright Law: Chapter 1"&gt;{{cite web |url=http://www.copyright.gov/title17/92chap1.pdf |title=U.S Copyright Office &#8211; Copyright Law: Chapter 1 |accessdate=2012-06-27}}&lt;/ref&gt;

===Duration===&lt;!-- This section is linked from [[Little Nemo]] --&gt;
{{main|Copyright term|List of countries' copyright length}}
[[File:Tom Bell's graph showing extension of U.S. copyright term over time.svg|thumb|300px|Expansion of U.S. copyright law (currently based on the date of creation or publication).]]
Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been [[Publication|published]], and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States&lt;ref&gt;{{usc|17|305}}&lt;/ref&gt; and the United Kingdom&lt;ref&gt;The Duration of Copyright and Rights in Performances Regulations 1995, [http://www.opsi.gov.uk/si/si1995/Uksi_19953297_en_3.htm part II], Amendments of the UK Copyright, Designs and Patents Act 1988&lt;/ref&gt;), copyrights expire at the end of the calendar year in question.

The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.&lt;ref&gt;{{cite book
|title=Copyright: Sacred Text, Technology, and the DMCA|last=Nimmer|first=David |publisher=Kluwer Law International|year=2003|isbn=  978-90-411-8876-2|oclc=50606064|page=63|url= https://books.google.com/books?id=RYfRCNxgPO4C}}&lt;/ref&gt;

In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain.&lt;ref&gt;"[http://copyright.cornell.edu/resources/publicdomain.cfm Copyright Term and the Public Domain in the United States].", ''[[Cornell University]]''.&lt;/ref&gt; In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain.&lt;ref&gt;See Peter B. Hirtle, "Copyright Term and the Public Domain in the United States 1 January 2015" [https://copyright.cornell.edu/resources/publicdomain.cfm online at footnote 8]&lt;/ref&gt;  Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.

But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.

In 1998, the length of a copyright in the United States was increased by 20 years under the [[Copyright Term Extension Act]]. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.&lt;ref&gt;Lawrence Lessig, ''Copyright's First Amendment'', 48 UCLA L. Rev. 1057, 1065 (2001)&lt;/ref&gt;

{{globalize/US|date=September 2016}}

==Limitations and exceptions==
{{main|Limitations and exceptions to copyright|Traditional safety valves}}

In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas).&lt;ref&gt;[http://copyright.gov/circs/circ34.pdf (2012) ''Copyright Protection Not Available for Names, Titles, or Short Phrases'' U.S. Copyright Office]&lt;/ref&gt; However, there are protections available for those areas copyright does not cover &#8211; such as [[trademark]]s and [[patent]]s.

There are some exceptions to what copyright will protect. Copyright will not protect:
* Names of products
* Names of businesses, organizations, or groups
* Pseudonyms of individuals
* Titles of works
* Catchwords, catchphrases, mottoes, slogans, or short advertising expressions
* Listings of ingredients in recipes, labels, and formulas, though the directions can be copyrighted

===Idea&#8211;expression dichotomy and the merger doctrine===
{{main|Idea&#8211;expression divide}}

The idea&#8211;expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of [[Baker v. Selden]], has since been codified by the [[Copyright Act of 1976]] at 17 U.S.C. &#167; 102(b).

===The first-sale doctrine and exhaustion of rights===

{{main|First-sale doctrine|Exhaustion of rights}}
Copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or [[compact disc|CD]]. In the United States this is known as the [[first-sale doctrine]], and was established by the [[court]]s to clarify the legality of reselling books in second-hand [[bookstore]]s.

Some countries may have [[parallel importation]] restrictions that allow the copyright holder to control the [[aftermarket (merchandise)|aftermarket]]. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as [[exhaustion of rights]] in other countries and is a principle which also applies, though somewhat differently, to [[patent]] and [[trademark]] rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.

In ''[[Kirtsaeng v. John Wiley &amp; Sons, Inc.]]'',&lt;ref&gt;{{cite web|title=John Wiley &amp; Sons Inc. v. Kirtsaeng |url= http://www.supremecourt.gov/opinions/12pdf/11-697_d1o2.pdf}}&lt;/ref&gt; in 2013, the [[United States Supreme Court]] held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission.  The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.

In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement [[Moral rights (copyright law)|moral rights]], a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.

===Fair use and fair dealing===
{{main|Fair use|Fair dealing}}
Copyright does not prohibit all copying or replication. In the United States, the [[Fair use|fair use doctrine]], codified by the [[United States Copyright Act of 1976|Copyright Act of 1976]] as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:
# the purpose and character of one's use
# the nature of the copyrighted work
# what amount and proportion of the whole work was taken, and
# the effect of the use upon the potential market for or value of the copyrighted work.&lt;ref&gt;{{cite web|url=http://www4.law.cornell.edu/uscode/17/107.html |title=US CODE: Title 17,107. Limitations on exclusive rights: Fair use |publisher=.law.cornell.edu |date=2009-05-20 |accessdate=2009-06-16}}&lt;/ref&gt;

In the [[United Kingdom]] and many other [[Commonwealth of Nations|Commonwealth]] countries, a similar notion of fair dealing was established by the [[court]]s or through [[legislation]]. The concept is sometimes not well defined; however in [[Canada]], private copying for personal use has been expressly permitted by statute since 1999. In ''[[Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)]]'', 2012 SCC 37, the [[Supreme Court of Canada]] concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the ''Copyright Act 1968'' (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. [[legal advice]]). Under current [[Law of Australia|Australian law]], although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to &#8220;format shift&#8221; that work from one medium to another for personal, private use, or to &#8220;time shift&#8221; a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.

In the United States the AHRA ([[Audio Home Recording Act]] Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.

:''Section 1008. Prohibition on certain infringement actions''

:''No action may be brought under this title alleging infringement of copyright based on the manufacture, importation, or distribution of a digital audio recording device, a digital audio recording medium, an analog recording device, or an analog recording medium, or based on the noncommercial use by a consumer of such a device or medium for making digital musical recordings or analog musical recordings.''

Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The [[Digital Millennium Copyright Act]] prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner.&lt;ref name="Intellectual Property and Information Wealth: Copyright and related rights"/&gt; An appellate court has held that fair use is not a defense to engaging in such distribution.

The [[copyright directive]] allows EU member states to implement a set of exceptions to copyright. Examples of those exceptions are:
*photographic reproductions on paper or any similar medium of works (excluding sheet music) provided that the rightholders receives fair compensation,
*reproduction made by libraries, educational establishments, museums or archives, which are non-commercial
*archival reproductions of broadcasts,
*uses for the benefit of people with a disability,
*for demonstration or repair of equipment,
*for non-commercial research or private study
*when used in [[parody]]

===Accessible copies===
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.&lt;ref&gt;[http://www.copyright.gov/title17/92chap1.html#121 Copyright Law of the USA, Chapter 1 Section 121]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.rnib.org.uk/xpedio/groups/public/documents/publicwebsite/public_cvipsact2002.hcsp|title=Copyright (Visually Impaired Persons) Act 2002 comes into force|publisher=Royal National Institute of Blind People|date=1 January 2011|accessdate=11 Aug 2016}}&lt;/ref&gt;

=={{anchor|Transfer and licensing, and assignment}} Transfer, assignment and licensing==
{{see also|Collective rights management|extended collective licensing|Compulsory license|Copyright transfer agreement}}
[[File:All rights reserved.jpg|thumb|right|300px|DVD: [[All Rights Reserved]].]]
A copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another.&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights"&gt;{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&amp;dq=copyright+transfer+and+licensing&amp;as_brr=3&amp;source=gbs_navlinks_s|isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization|page=15}}&lt;/ref&gt; For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the [[Internet]]; however, the [[record industry]] attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy and/or distribute the work in a particular region or for a specified period of time.

A transfer or licence may have to meet particular formal requirements in order to be effective,&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights(2)"&gt;{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&amp;dq=copyright+transfer+and+licensing&amp;as_brr=3&amp;source=gbs_navlinks_s|page=8 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}&lt;/ref&gt; for example under the Australian [[Copyright law of Australia#Copyright Act 1968|Copyright Act 1968]] the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under [[Law of the United States|U.S. law]]. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a [[real estate]] transaction.

Copyright may also be [[license]]d.&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights"/&gt; Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed [[statutory license]] (e.g. musical works in the United States used for radio broadcast or performance). This is also called a [[compulsory license]], because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made.&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights(3)"&gt;{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&amp;dq=copyright+transfer+and+licensing&amp;as_brr=3&amp;source=gbs_navlinks_s|page=16 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}&lt;/ref&gt; Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, [[copyright collective]]s or [[collecting societies]] and [[performance rights organisation|performing rights organizations]] (such as [[ASCAP]], [[Broadcast Music Incorporated|BMI]], and [[SESAC]]) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.

===Free licences===
Copyright licenses known as ''open'' or [[free license]]s seek to grant several rights to licensees, either for a fee or not, to an effect inspired by the [[public domain]]. ''Free'' in this context isn't much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the [[Free Software Definition]], the [[Debian Free Software Guidelines]], the [[Open Source Definition]] and the [[Definition of Free Cultural Works]]. Further refinements to these licenses have resulted in categories such as [[copyleft]] and [[permissive license|permissive]]. Common examples of free licences are the [[GNU General Public License]], [[BSD license]]s and some [[Creative Commons licenses]].

Founded in 2001 by [[James Boyle (academic)|James Boyle]], [[Lawrence Lessig]], and [[Hal Abelson]], the [[Creative Commons]] (CC) is a non-profit organization&lt;ref name="CC"&gt;{{cite web|url=http://creativecommons.org/ |title=Creative Commons Website|website=creativecommons.org|accessdate=24 October 2011}}&lt;/ref&gt; which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, [[gratis versus libre|gratis]]. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.&lt;ref name="CC" /&gt;

Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them aren't properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work.&lt;ref name="Rubin"&gt;Rubin, R. E. (2010) 'Foundations of Library and Information Science: Third Edition', Neal-Schuman Publishers, Inc., New York, p. 341&lt;/ref&gt; {{As of|2009}} approximately 130 million individuals had received such licenses.&lt;ref name="Rubin" /&gt;

==Criticism==
Some sources are critical of particular aspects of the copyright system. This is known as a debate over [[copynorms]]. Particularly on the internet, there is discussion about the [[copyright aspects of downloading and streaming]], the [[copyright aspects of hyperlinking and framing]]. Such concerns are often couched in the language of [[digital rights]] and [[database right]]s. Discussions include ''[[Free Culture (book)|Free Culture]]'', a 2004 book by [[Lawrence Lessig]]. Lessig coined the term [[permission culture]] to describe a worst-case system. ''[[Good Copy Bad Copy]]'' (documentary) and [[RiP!: A Remix Manifesto]], discuss copyright. Some suggest an [[alternative compensation system]]. 

Some groups reject copyright altogether, taking an [[anti-copyright]] stance. The perceived inability to enforce copyright online leads some to advocate [[Crypto-anarchism|ignoring legal statutes when on the web]].

==Public domain==
{{main|Public domain}}
Copyright, like other [[intellectual property rights]], is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a [[common law copyright]]. Public domain works should not be confused with works that are publicly available. Works posted in the [[internet]], for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.

==See also==
{{Portal|Social and political philosophy|Law}}
{{colbegin|colwidth=15em}}
* [[Adelphi Charter]]
* [[Artificial scarcity]]
* [[Conflict of laws]]
* [[Copyright Alliance]]
* [[Copyright in architecture in the United States]]
* [[Copyright on the content of patents and in the context of patent prosecution]]
* [[Copyright for Creativity]]
* [[Copyright infringement of software]]
* [[Copyright on religious works]]
* [[Creative Barcode]]
* [[Digital rights management]]
* [[Digital watermarking]]
* [[Entertainment law]]
* [[Freedom of panorama]]
* [[Intellectual property education]]
* [[Intellectual property protection of typefaces]]
* [[List of Copyright Acts]]
* [[List of copyright case law]]
* [[Model release]]
* [[Paracopyright]]
* [[Photography and the law]]
* [[Pirate Party]]
* [[Private copying levy]]
* [[Production music]]
* [[Rent-seeking]]
* [[Reproduction fees]]
* [[Samizdat]]
* [[Software copyright]]
* [[Threshold pledge system]]
{{colend}}

==References==
{{Reflist|30em}}

==Further reading==
{{refbegin|30em}}
* {{Cite book
  |author=Dowd, Raymond J.
  |title=Copyright Litigation Handbook
  |publisher=Thomson West |edition=1st |year=2006
  |isbn=0-314-96279-4
  |ref=Dowd, Litigation handbook
}}
* Ellis, Sara R. ''Copyrighting Couture: An Examination of Fashion Design Protection and Why the DPPA and IDPPPA are a Step Towards the Solution to Counterfeit Chic'', 78 Tenn. L. Rev. 163 (2010), ''available at'' http://ssrn.com/abstract=1735745.
* {{Cite book
  |author1=Gantz, John  |author2=Rochester, Jack B.
  |title=Pirates of the Digital Millennium
  |publisher=Financial Times Prentice Hall
  |year=2005
  |isbn=0-13-146315-2
  |ref=Gantz, Pirates
}}
* [[Shuman Ghosemajumder|Ghosemajumder, Shuman]]. ''[http://dspace.mit.edu/handle/1721.1/8438 Advanced Peer-Based Technology Business Models]''. [[MIT Sloan School of Management]], 2002.
* [[Bruce Lehman|Lehman, Bruce]]: ''Intellectual Property and the National Information Infrastructure'' (Report of the Working Group on Intellectual Property Rights, 1995)
* Lindsey, Marc: ''Copyright Law on Campus.'' [[Washington State University]] Press, 2003. ISBN 978-0-87422-264-7.
* Mazzone, Jason. ''[[Copyfraud]]''. [http://ssrn.com/abstract=787244 SSRN]
* McDonagh, Luke. ''Is Creative use of Musical Works without a licence acceptable under Copyright?'' International Review of Intellectual Property and Competition Law (IIC) 4 (2012) 401-426, available at [http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2521081 SSRN]
* {{Cite book
  | last = Nimmer | first = Melville |authorlink=Melville Nimmer |author2=David Nimmer | title = [[Nimmer on Copyright]] | publisher = Matthew Bender | year=1997| isbn = 0-8205-1465-9
}}
* {{Cite book
  |title=Copyright in Historical Perspective
  |author=Patterson, Lyman Ray
  |year=1968|publisher=Vanderbilt University Press
  |isbn=0-8265-1373-5
  |version=Online Version
}}
* Rife, by Martine Courant. ''Convention, Copyright, and Digital Writing''  (Southern Illinois University Press; 2013) 222 pages; Examines legal, pedagogical, and other aspects of online authorship.
* {{cite book | last = Rosen | first = Ronald | title = Music and Copyright | publisher = Oxford University Press | location = Oxford Oxfordshire | year = 2008 | isbn = 0-19-533836-7 }}
* Shipley, David E. [http://ssrn.com/abstract=1076789 Thin But Not Anorexic: Copyright Protection for Compilations and Other Fact Works] UGA Legal Studies Research Paper No. 08-001; Journal of Intellectual Property Law, Vol. 15, No. 1, 2007.
* Silverthorne, Sean. ''[http://hbswk.hbs.edu/item.jhtml?id=4206&amp;t=innovation Music Downloads: Pirates- or Customers?]''. [[Harvard Business School|Harvard Business School Working Knowledge]], 2004.
* Sorce Keller, Marcello. "Originality, Authenticity and Copyright", ''Sonus'', VII(2007), no. 2, pp.&amp;nbsp;77&#8211;85.
* {{Cite book
  |author1=Steinberg, S.H.  |author2=Trevitt, John
  |title=Five Hundred Years of Printing
  |location=London and New Castle |publisher=The British Library and Oak Knoll Press
  |edition=4th |year=1996
  |isbn=1-884718-19-1
  |ref=Steinberg, Five hundred years
}}
* {{Cite book
  |title=The Copy/South Dossier: Issues in the Economics, Politics and Ideology of Copyright in the Global South 
  |url=http://copysouth.org/en/documents/csdossier.pdf
  |editor1=Story, Alan |editor2=Darch, Colin |editor3=Halbert, Deborah |year=2006|publisher=Copy/South Research Group
  |isbn=978-0-9553140-1-8
}}
* [http://whynotaskme.org/ WhyNotAskMe.org]: ''Organization demanding democratic participation in copyright legislation and a moratorium on secret and fast-tracked copyright negotiations''
{{refend}}

==External links==
{{Wikisource1911Enc|Copyright}}
{{Wikisource|Wikisource:Copyright law|Copyright law}}
{{Spoken Wikipedia|En-Copyright.ogg|2008-12-30}}
{{Library resources box}}
* {{Wikiquote-inline}}
* {{Commons-inline|Copyright}}
* {{dmoz|Society/Law/Legal_Information/Intellectual_Property/Copyrights}}
* [http://www.wipo.int/clea/en/ Collection of laws for electronic access] from [[WIPO]] &#8211; intellectual property laws of many countries
* [http://purl.fdlp.gov/GPO/gpo55676 Compendium of Copyright Practices] (3rd ed.) [[United States Copyright Office]]
* [http://ucblibraries.colorado.edu/govpubs/us/copyrite.htm Copyright] from ''UCB Libraries GovPubs''
* [http://www.ipo.gov.uk/types/copy.htm About Copyright] at the UK Intellectual Property Office
* [http://www.lawtech.jus.unitn.it/index.php/copyright-history/bibliography A Bibliography on the Origins of Copyright and Droit d'Auteur]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ 6.912 Introduction to Copyright Law] taught by Keith Winstein, MIT OpenCourseWare January IAP 2006
* [http://www.wipo.int/treaties/en/ShowResults.jsp?country_id=ALL&amp;start_year=ANY&amp;end_year=ANY&amp;search_what=C&amp;treaty_id=15 Copyright Berne Convention: Country List]  List of the 164 members of the Berne Convention for the protection of literary and artistic works
* [http://www.copyrightservice.co.uk/copyright/p01_uk_copyright_law UK Copyright Law fact sheet] (April 2000) a concise introduction to UK Copyright legislation
* [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance/reports/ipr.aspx IPR Toolkit &#8211; An Overview, Key Issues and Toolkit Elements] (September 2009) by Professor Charles Oppenheim and Naomi Korn at the [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance.aspx Strategic Content Alliance]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ MIT OpenCourseWare 6.912 Introduction to Copyright Law] Free self-study course with video lectures as offered during the January 2006, Independent Activities Period (IAP)
* [http://www.loc.gov/rr/rarebook/coll/067.html Early Copyright Records] From the [http://www.loc.gov/rr/rarebook/ Rare Book and Special Collections Division at the Library of Congress]
* [http://copyright.gov/title17/ Copyright Law of the United States Documents], US Government
{{Copyright law by country}}
{{Intellectual property activism}}

{{Authority control}}

[[Category:Copyright law| ]]
[[Category:Data management]]
[[Category:Intellectual property law]]
[[Category:Metadata]]
[[Category:Monopoly (economics)]]
[[Category:Public records]]</text>
      <sha1>p642694jelqd413wy5o0hm74fb8r50c</sha1>
    </revision>
  </page>
  <page>
    <title>Couchbase Server</title>
    <ns>0</ns>
    <id>28366048</id>
    <revision>
      <id>754348871</id>
      <parentid>749524126</parentid>
      <timestamp>2016-12-12T04:48:30Z</timestamp>
      <contributor>
        <username>Rkeshav.murthy</username>
        <id>29873708</id>
      </contributor>
      <minor />
      <comment>Expanded the description of N1QL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12158" xml:space="preserve">{{Infobox software
| name                   = Couchbase Server
| logo                   = [[File:CouchbaseLogo.svg|224px]]
| screenshot             = Couchbase Server Screenshot.jpg
| caption                =
| developer              = [[Couchbase, Inc.]]
| released               = {{Start date|2010|08}}
| latest release version = 4.5
| latest release date    = {{release date|2016|06|22}}
| status                 = active
| programming language   = [[C++]], [[Erlang (programming language)|Erlang]], [[C (programming language)|C]],&lt;ref&gt;{{cite web |author= Damien Katz|url=http://damienkatz.net/2013/01/the_unreasonable_effectiveness_of_c.html |title=The Unreasonable Effectiveness of C |date= January 8, 2013 |accessdate= September 30, 2016 }}&lt;/ref&gt; [[Go (programming language)|Go]]
| operating system       = [[Cross-platform]]
| genre                  = [[Multi-model database]] / [[Key-value database|Distributed Key-Value]] / [[Document-oriented database]]
| license                = [[Apache License]] (Open Source edition), [[Proprietary software|Proprietary]] (Free Community edition and Paid Enterprise edition)
| website                = {{URL|http://www.couchbase.com/}}
| frequently updated     = yes
}}

'''Couchbase Server''', originally known as '''Membase''', is an [[open-source]], distributed ([[shared-nothing architecture]]) [[Multi-model database|multi-model]] [[NoSQL]] [[document-oriented database]] software package that is optimized for interactive applications. These applications may serve many [[concurrent user]]s by creating, storing, retrieving, aggregating, manipulating and presenting data. In support of these kinds of application needs, Couchbase Server is designed to provide easy-to-scale key-value or JSON document access with low latency and high sustained throughput. It is designed to be [[Cluster (computing)|clustered]] from a single machine to very large-scale deployments spanning many machines.
A version originally called '''Couchbase Lite''' was later marketed as Couchbase Mobile combined with other software.

Couchbase Server provided client protocol compatibility with [[memcached]],&lt;ref&gt;{{cite web|url=http://code.google.com/p/memcached/wiki/NewProtocols |title=NewProtocols - memcached - Klingon - Memcached - Google Project Hosting |publisher=Code.google.com |date=2011-08-22 |accessdate=2013-06-04}}&lt;/ref&gt; but added disk [[Persistence (computer science)|persistence]], [[data replication]], live cluster reconfiguration, rebalancing and [[multitenancy]] with [[Partition (database)|data partitioning]].

==Product history==
Membase was developed by several leaders of the [[memcached]] project, who had founded a company, NorthScale, to develop a [[key-value store]] with the simplicity, speed, and scalability of memcached, but also the storage, persistence and querying capabilities of a database. The original membase source code was contributed by NorthScale, and project co-sponsors [[Zynga]] and [[Naver Corporation]] (then known as NHN) to a new project on membase.org in June 2010.&lt;ref&gt;{{Cite book |title= Professional NoSQL |author= Shashank Tiwari  |publisher= John Wiley &amp; Sons |pages= 15&#8211;16 |isbn= 9781118167809 }}&lt;/ref&gt;

On February 8, 2011, the Membase project founders and Membase, Inc. announced a merger with CouchOne (a company with many of the principal players behind [[CouchDB]]) with an associated project merger. The merged company was called [[Couchbase, Inc.]] In January 2012, Couchbase released Couchbase Server 1.8. 
In September, 2012, [[Orbitz]] said it had changed some of its systems to use Couchbase.&lt;ref&gt;{{cite web |url= http://gigaom.com/cloud/balancing-oracle-and-open-source-at-orbitz/ |title= Balancing Oracle and open source at Orbitz |publisher=[[GigaOM]] |date= September 21, 2012 |accessdate= September 19, 2016 }}&lt;/ref&gt;
On December 2012, 
Couchbase Server 2.0 (announced in July 2011) was released and included a new [[JSON]] document store, indexing and querying, incremental [[MapReduce]] and [[Replication (computing)|replication]] across [[data center]]s.&lt;ref name="zd2"&gt;{{cite web |url= http://www.zdnet.com/couchbase-2-0-released-implements-json-document-store-7000008649/ |title= Couchbase 2.0 released; implements JSON document store |publisher= [[ZDNet]] |author=  Andrew Brust |date= December 12, 2012}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title= Couchbase goes 2.0, pushes SQL for NoSQL |author= Derrick Harris |date= July 29, 2011 |work= GigaOm |url= https://gigaom.com/2011/07/29/couchbase-2-0-unql-sql-nosql/ |accessdate= September 19, 2016 }}&lt;/ref&gt;

==Architecture==
Every Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.
In the parlance of Eric Brewer&#8217;s [[CAP theorem]], Couchbase is normally a CP type system meaning it provides [[Consistency (database systems)|consistency]] and [[Network partitioning|partition tolerance]], or it can be set up as an AP system with multiple clusters.

===Cluster manager===
The cluster manager supervises the configuration and behavior of all the servers in a Couchbase cluster. It configures and supervises inter-node behavior like managing replication streams and re-balancing operations. It also provides metric aggregation and consensus functions for the cluster, and a [[REST]]ful cluster management interface. The cluster manager uses the [[Erlang (programming language)|Erlang programming language]] and the [[Open Telecom Platform]].

====Replication and fail-over====
[[Data replication]] within the nodes of a cluster can be controlled with several parameters.
In December 2012, replication was also supported between different [[data center]]s.&lt;ref name="zd2" /&gt;

===Data manager===
The data manager stores and retries documents in response to data operations from applications.
It asynchronously writes data to disk after acknowledging to the client.  In version 1.7 and later, applications can optionally ensure data is written to more than one server or to disk before acknowledging a write to the client.
Parameters define item ages that affect when data is persisted, and how max memory and migration from main-memory to disk is handled.
It supports working sets greater than a memory quota per "node" or "bucket".
External systems can subscribe to filtered data streams, supporting, for example, [[full text search]] indexing, [[data analytics]] or archiving.&lt;ref&gt;{{Cite web |url= http://blog.couchbase.com/want-know-what-your-memcached-servers-are-doing-tap-them |title= Want to know what your memcached servers are doing? Tap them |author= Trond Norbye |work= Couchbase blog |date= March 15, 2010}}&lt;/ref&gt;

====Data format====
A document is the most basic unit of data manipulation in Couchbase Server. Documents are stored in JSON document format with no predefined schemas.

====Object-managed cache====
Couchbase Server includes a built-in multi-threaded object-managed cache that implements memcached compatible APIs such as get, set, delete, append, prepend etc.

====Storage engine ====
Couchbase Server has a tail-append storage design that is immune to data corruption, [[OOM killer]]s or sudden loss of power.  Data is written to the data file in an append-only manner, which enables Couchbase to do mostly sequential writes for update, and provide an optimized access patterns for disk I/O.

=== Performance ===
A performance benchmark done by [[Altoros]] in 2012, compared Couchbase Server with other technologies.&lt;ref&gt;{{cite web |url= http://www.couchbase.com/nosql-resources/presentations/benchmarking-couchbase%5B2%5D.html |title= Benchmarking Couchbase |author= Frank Weigel |publisher=Couchbase |date= October 30, 2012 |accessdate= September 30, 2016 }}&lt;/ref&gt;
[[Cisco Systems]] published a benchmark that measured the latency and throughput of Couchbase Server with a mixed workload in 2012.&lt;ref&gt;{{cite web |url= http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |title=Cisco and Solarflare Achieve Dramatic Latency Reduction for Interactive Web Applications with Couchbase, a NoSQL Database |publisher=[[Cisco Systems]] |date= June 18, 2012 |archivedate=  August 13, 2012 |archiveurl= https://web.archive.org/web/20120813162214/http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |accessdate= October 7, 2016 }}&lt;/ref&gt;

== Licensing and support ==
Couchbase Server is a packaged version of Couchbase's [[open source software]] technology and is available in a community edition without recent bug fixes with Apache 2.0 license.&lt;ref&gt;{{cite web |url= http://developer.couchbase.com/open-source-projects |title=Couchbase Open Source Projects |work= Couchbase web site |accessdate= October 7, 2016 }}&lt;/ref&gt; and an edition for commercial use.&lt;ref&gt;{{cite web|url=http://www.couchbase.com/couchbase-server/editions|title=Couchbase Server Editions|publisher= Couchbase }}&lt;/ref&gt; 
Couchbase Server builds are available for Ubuntu, Debian, Red Hat, SUSE, Oracle Linux, [[Microsoft Windows]] and Mac OS X operating systems.

Couchbase has supported software developers' kits for the programming languages [[.NET Framework|.Net]], [[PHP]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[C (programming language)|C]], [[Node.js]], [[Java (programming language)|Java]], and [[Go (programming language)|Go]].

==N1QL==
A [[query language]] called the non-first normal form query language, N1QL (pronounced nickel), is used for manipulating the JSON data in Couchbase, just like SQL manipulates data in RDBMS. It has SELECT, INSERT, UPDATE, DELETE, MERGE statements to operate on JSON data.
It was announced in March 2015 as "SQL for documents".&lt;ref&gt;{{Cite web |title= Ssssh!  don&#8217;t tell anyone but Couchbase is a serious contender: Couchbase Live Europe 2015 |author= Andrew Slater |date= March 24, 2015 |accessdate= September 19, 2016 }}&lt;/ref&gt;

The N1QL [[data model]] is [[Database normalization#Non-first normal form .28NF.C2.B2 or N1NF.29|non-first normal form]] (N1NF) with support for nested attributes and domain-oriented [[Database normalization|normalization]].  The N1QL data model is also a proper superset and generalization of the [[relational model]].

===Example===
&lt;source lang="json"&gt;
{
  "email":"testme@gmail.com",
  "friends":[
            {"name":"rick"},
            {"name":"cate"}
           ]
}
&lt;/source&gt;

;Like Query: {{code|2=sql|SELECT * FROM `bucket` WHERE LIKE "%@gmail.com";}}

;Array Query: {{code|2=sql|1=SELECT * FROM `bucket` WHERE ANY x IN friends SATISFIES x.name = "cate" END;}}

==Bibliography==
* {{cite book |last=Brown |first=MC |editor-first=|editor-last=|title=Getting Started with Couchbase Server (1st edition) |publisher=O'Reilly Media |date=June 22, 2012 |page=88 | isbn=978-1449331061}}
* {{citation
| first1    = David
| last1     = Ostrovsky
| first2   = Mohammed
| last2   = Haji
| first3  = Yaniv
| last3   = Rodenski
| date      = November 26, 2015
| title     = Pro Couchbase Server 2nd ed.
| edition   = 2nd
| publisher = [[Apress]]
| page     = 349
| isbn      = 978-1484211861
}}
* {{citation
| first1    = Henry
| last1     = Potsangbam
| date      = November 23, 2015
| title     = Learning Couchbase
| edition   = 1st
| publisher = [[Packt]]
| page     = 202
| isbn      = 978-1785288593
}}
* {{citation
| first1    = Deepak
| last1     = Vohra
| date      = August 3, 2015
| title     = Pro Couchbase Development: A NoSQL Platform for the Enterprise
| edition   = 1st
| publisher = [[Apress]]
| page     = 331
| isbn      = 978-1484214350
}}

==References==
{{Reflist}}

==External links==
*{{Official website}}

[[Category:Free database management systems]]
[[Category:Distributed computing architecture]]
[[Category:NoSQL]]
[[Category:Cross-platform software]]
[[Category:Structured storage]]
[[Category:Client-server database management systems]]
[[Category:Database-related software for Linux]]
[[Category:Applications of distributed computing]]
[[Category:Databases]]
[[Category:Data management]]
[[Category:Distributed data stores]]</text>
      <sha1>90d8mq5ts1ar436d5ir0uel4ynvae23</sha1>
    </revision>
  </page>
  <page>
    <title>National Data Repository</title>
    <ns>0</ns>
    <id>30966530</id>
    <revision>
      <id>760848524</id>
      <parentid>754608711</parentid>
      <timestamp>2017-01-19T12:50:15Z</timestamp>
      <contributor>
        <ip>81.29.45.182</ip>
      </contributor>
      <comment>Updated figures for Norwegian NDR</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37149" xml:space="preserve">A '''National Data Repository''' ('''NDR''') is a data bank that seeks to preserve and promote a country&#8217;s natural resources data, particularly data related to the petroleum [[exploration and production]] (E&amp;P) sector.

A National Data Repository is normally established by an entity that governs, controls and supports the exchange, capture, transference and distribution of E&amp;P information, with the final target to provide the State with the tools and information to assure the growth, govern-ability, control, independence and sovereignty of the industry.

The two fundamental reasons for a country to establish an NDR are to '''preserve''' data generated inside the country by the industry, and to '''promote''' investments in the country by utilizing data to reduce the exploration, production, and transportation business risks.

Countries take different approaches towards preserving and promoting their natural resources data. The approach varies according to a country&#8217;s natural resources policies, level of openness, and its attitude towards foreign investment.

==Data types==
NDRs store a vast array of data related to a country&#8217;s natural resources. This includes wells, [[Well logging|well log data]], well reports, [[core sample]]s, [[seismic]] surveys, [[Seismic inversion#Post-stack seismic resolution inversion|post-stack seismic]], field data/tapes, seismic (acquisition/processing) reports, [[Oil production|production]] data, [[geological map]]s and reports, license data and [[geologic modeling|geological models]].

==Funding models==
Some NDRs are financed entirely by a country&#8217;s government. Others are industry-funded. Still some are hybrid systems, funded in part by industry and government.
NDRs typically charge fees for data requests and for data loading. The cost differs significantly between countries. In some cases an annual membership is charged to oil companies to store and access the data in the NDR.

==Standards body==
[[Energistics]] is the global energy standards resource center for the upstream oil and gas industry.

Energistics National Data Repository Work Group:
The standards body is Energistics.&lt;ref&gt;[http://energistics.org/energistics-standards-directory Energistics]&lt;/ref&gt;

===Energistics-standards-directory===
Global regulators of upstream oil and natural gas information, including seismic, drilling, production and reservoir data, formed the National Data Repository (NDR) Work Group in 2008 to collaborate on the development of data management standards and to assist emerging nations with hydrocarbon reserves to better collect, maintain and deliver oil and gas data to the public and to the industry.

Ten countries, led by the [[Netherlands]], [[Norway]] and the [[United Kingdom]], formed NDR to share best practices and to formalize the development and deployment of data management standards for regulatory agencies. The other countries involved in the NDR Work Group&#8217;s formation are [[Australia]], [[Canada]], [[India]], [[Kenya]], [[New Zealand]], [[South Africa]] and the [[United States]].

Annual NDR Conference: Approximately every 18 months Energistics organizes a National Data Repository Conference. The purpose is to provide government and regulatory agencies from around the world an opportunity to attend a series of workshops dedicated to developing data exchange standards, improving communications with the oil and gas industry and learning data management techniques for natural resources information.&lt;ref&gt;[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group/ndr-meetings NDR Conference page on the Energistics website]&lt;/ref&gt;

===Society of Exploration Geophysicists and The International Oil and Gas Producers Association===
The SEG is the custodian of the SEG standards which are used for the exchange, retention and release of seismic data.  They are commonly used by National Data Repositories with the SEGD and SEGY being the field and processed exchange standards respectively.

==NDRs around the world==

[https://www.google.com/maps/d/viewer?mid=1by9vDDoWwnZD0f8vNt_le2TThTU Click here to see a map of the NDRs around the world]

{| class="wikitable sortable"
|-
! Country  !!  Name  !!  Agency  !!  Scope  !! Status !!   Purposes   !! Data types/volumes !! Standards used !! Funding !! Website
|-
| {{flagcountry|Algeria}} || Banque de Donn&#233;es Nationale "BDN" || Agence Nationale pour la Valorisation des Ressources en Hydrocarbures (ALNAFT) || Onshore and Offshore Algeria || Ongoing project - agency created by new law in 2005 || Custodian of all E&amp;P data of the country || Cultural, Seismic 2D &amp; 3D, Wells, Data Wells, Wells report, Production, Facilities, Economical and Fiscality, Interpretation, Physical assets index, Data drilling, Transcription, Vectorisation, digitalization || ASCII, SEGY, UKOOA, LAS, DLIS, LIS, PDS, BIT, RODE, PDF, TIF....etc || Government funding/Agency revenue || http://www.alnaft.gov.dz/
|-
| {{flagcountry|Colombia}} || EPIS || Agencia Nacional de Hidrocarburos (ANH) || Onshore and offshore Colombia || Created originally for Ecopetrol and transferred to ANH when it was established in 2003. New system launched December 2009 || Promote and preserve all the technical E&amp;P information assets of the country || wells, surveys, licenses, seismic sections, well reports, maps || REST Web services || Government funding ||http://www.epis.com.co
|-
|  {{flagcountry|Canada}} || CNSOPB || Nova Scotia Offshore Petroleum Board &#8211; Geoscience Research Centre- Digital Data Management Centre (DMC) || Offshore Nova Scotia, Canada ||  Operational since 2007 || To provide an effective &amp; efficient system for the management of digital petroleum data, assist explorers in easily obtaining access to large volumes of data via the web, Data Preservation and Data Distribution ||  Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports || LAS, DLIS, SEGY || Funded 50/50 by the Federal and Provincial Governments with some funds from industry through cost recovery  || http://www.cnsopb.ns.ca/
|-
| {{flagcountry|Australia}} || PIMS ||Geoscience Australia || ||Active  || Various online and web based systems exist for E &amp;P, geosciences
||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports ||  ||  ||
http://dbforms.ga.gov.au/pls/www/npm.pims_web.search
|-
| {{flagcountry|Western Australia}} || WAPIMS ||Government of Western Australia   || || Active || WAPIMS is a petroleum, geothermal and minerals exploration database ||Contains data on titles, wells, geophysical surveys and other petroleum exploration and production data submitted to DMP by the petroleum industry.
  ||  ||  || http://dmp.wa.gov.au
|-
| {{flagcountry|New South Wales}} ||  ||Government of New South Wales ||  || Active  || Various online geoscience databases to assist New South Wales including DIGS  ||  ||  ||  ||
http://www.dpi.nsw.gov.au/minerals
http://digsopen.minerals.nsw.gov.au/
|-
| {{flagcountry|Northern Territory}} ||  || Government of Northern Territory  ||  || Active  || Various online geoscience databases to assist Northern Territories ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
http://www.nt.gov.au/d/Minerals_Energy/index.cfm?header=Petroleum	
|-
| {{flagcountry|Queensland}} ||  || Government of Queensland ||  || Active  || Various online geoscience databases to assist Queensland including Q-DEX ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
https://www.dnrm.qld.gov.au/
|-
| {{flagcountry|South Australia}} || SARIG || Government of South Australia  ||  || Active  || Various online geoscience databases to assist South Australia such as PEP-SA  || ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
http://petroleum.statedevelopment.sa.gov.au/data_and_publications/sarig
https://sarig.pir.sa.gov.au/
|-
| {{flagcountry|Tasmania}} ||  ||  ||  ||   ||Various online geoscience databases to assist Tasmania   || || Active ||  ||
http://www.mrt.tas.gov.au/portal/page?_pageid=35,1&amp;_dad=portal&amp;_schema=PORTAL		
|-
| {{flagcountry|China }} || CNPC  || Chinese National Petroleum Corporation  ||  ||   || Various oil companies in China with CNPC the largest and parent of Petrochina ||  ||  ||  ||
http://www.cnpc.com.cn/en/
http://www.petrochina.com.cn/ptr/
http://www.cnooc.com.cn/
http://english.sinopec.com/index.shtml
|-
| {{flagcountry|Russia}} ||  ||  Sakhalin, DIGC RDC ||  || Various oil companies in Russia the largest being Rosneft which is state owned  ||  ||  ||  ||  ||
http://www.rosneft.com
http://www.lukoil.com
http://www.tnk-bp.com/en/
http://www.surgutneftegas.ru/
http://www.gazprom-neft.com/
http://www.tatneft.ru/wps/wcm/connect/tatneft/portal_rus/homepage/
|-
| {{flagcountry|Indonesia}} || Indonesia's National Data Centre (NDC) for petroleum, energy and minerals data || Agency for Research and Development in the Ministry of Energy and Mineral Resources of the Republic of Indonesia||Onshore &amp; Offshore || In 1997 Indonesia established Migas Data Management (MDM) operated by PT. Patra Nusa Data (PND)  || PND manages and promotes petroleum investment opportunities by compiling and value adding available petroleum data and information. ||  ||  ||  || http://www.patranusa.com/
|-
|  {{flagcountry|New Zealand}} || New Zealand Online Exploration Database || New Zealand Petroleum &amp; Minerals, Ministry of Business Innovation &amp; Employment || New Zealand onshore and offshore out to the outer continental shelf.  || Opened to public in April 2007. || Data preservation, Investment facilitation, aid in monitoring regulatory compliance, maximise the return to the nation by informing public policy and business strategy. || Wells, well log curves, petroleum reports (includes wells and surveys), mineral reports, coal reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes, seismic acquisition/processing reports, geophysical and geochemical data acquired in mineral and coal exploration (incorporated as enclosures to reports), VSP (incorporated as enclosures to reports), Seismic survey observer logs. GIS data and projects (minerals and coal). Estimated total NDR Size: 2.5 TB loaded, 3.0 TB staged for loading, 40 TB field data offline.  || Closely follow Australian digital reporting standards. No naming standards for wells and surveys.  || 50% Government funding, 50% third party permit (license) fees paid by exploration companies. || https://data.nzpam.govt.nz
|-
| {{flagcountry|Jordan}} ||NRA  || Jordan Natural Resources Authority (NRA) ||Onshore || Active    || Online data room allows users to browse and select large data set quickly in a controlled and secure environment ||Reserves land records, field data, maps, engineering, seismic data, geological studies and well files.  ||  ||  || http://www.jordan.gov.jo
|-
| {{flagcountry|Angola}} ||  || Sonangol || Offshore Angola || Active || Promotion, Organisation &amp; Management of all Exploration &amp; Production (E&amp;P) Data of Angola || Wells, surveys, licenses, seismic sections, well reports, maps ||  || Norad/OfD and NPD assistance || http://www.sonangol.co.ao
|-
| {{flagcountry|France}} ||BEPH ||  || French Territory || || Interactive maps of French territory of oil data are available to Internet users which includes: Permits for petroleum exploration, seismic exploration, oil drilling  (data, documents available) || Wells, Surveys, Licenses, Seismic Sections, Well Reports, Maps ||  ||  || http://www.beph.net/
|-
| {{flagcountry|S&#227;o Tom&#233; and Pr&#237;ncipe }} || ANP-STP  || National Petroleum Agency of S&#227;o Tom&#233; &amp; Principe (ANP-STP)  || Offshore || ||  ||  ||  ||Norad/OfD and NPD assistance  || http://www.anp-stp.gov.st
|-
| {{flagcountry|Tanzania}} || TPDC  || Tanzania Petroleum Development Corp  ||  || Began in the early 1990s with Norwegian assistance || An E &amp; P data archive centre || Geophysical survey data, Geological studies, Well drilling and completion reports, Cores and drill steam data ||  || Norad/OfD and NPD assistance || http://www.tpdc-tz.com
|-
| {{flagcountry|Oman}} || OGDR || Department of Petroleum Concession, Ministry of Oil and Gas || Onshore &amp; Offshore || Operational, tendering OGDR as a managed service (fully outsourced) June 2015 || Preservation of E&amp;P data, support concession promotion. || Well-related Data: Header, deviation, tops, field and processed logs, well documents. Seismic-related Data: Field and processed 2D/3D, Gravimag, VSP. || OGDR Data Submission Standard that uses industry standards where possible i.e. DLIS, SEG, UKOOA. || Government &amp; concession holders.||  http://www.mog.gov.om/english/tabid/309/Default.aspx
|-
| {{flagcountry|Netherlands}} || DINO || The Geological Survey of the Netherlands, a division of [[Netherlands Organisation for Applied Scientific Research|TNO]] || The Netherlands including offshore waters ||Started in 2004. Currently BRO is being planned to succeed DINO. || To archive subsurface data of the Netherlands in one repository and provide easy access to the data to encourage multiple use of data.||  || WMS web services. DINO uses own naming conventions || 100% Government funding || http://www.nlog.nl/en/home/NLOGPortal.html
|-
| {{flagcountry|India}} ||DGH  || Directorate General of Hydrocarbons (DGH) ||  || Active - scheduled operation by April 2015
 ||  Establishing national data archival, improving data quality and access for quality exploration covering large area under exploration and providing basis for long term energy policy formulation as well as support OALP  ||Wells, Well Logs, Cores, Scanned core images, Seismic, Reports, production, Technical Reports
  ||  ||Government of India || http://www.dghindia.org/DataManagement.aspx#
|-
| {{flagcountry|Sri Lanka}} || PRDS || Ministry of Petroleum and Petroleum Resources Development ||  || Active since 2009 ||   The PRDS developed a website to disseminate petroleum data and information to public and to investors to assist promotion of offshore areas to attract investors for petroleum exploration ||Wells, surveys, licenses, seismic sections, well reports, maps. Data historic and current, archived on different media (paper, mylar, magnetic tape) || ||  || http://www.prds-srilanka.com/data/onlineData.faces
|-
| {{flagcountry|Argentina}} || ENARSA || Energia Argentina SA  || || Established in 2006  ||  ||  ||  ||  ||  http://www.enarsa.com.ar http://energia.mecon.gov.ar/upstream/US_Pterminados.asp
|-
| {{flagcountry|Peru}} || PeruPetro ||  ||  || Active ||  ||  || ||  || http://www.perupetro.com.pe
|-
| {{flagcountry|Kazakhstan}} ||  || Ministry of Energy and Mineral Resources of the Republic of Kazakhstan (MEMR) ||  || Active ||  ||  ||  ||  || http://www.petrodata.kz
|-
| {{flagcountry|Pakistan}} || PPEPDR || Directorate General Petroleum Concessions (DGPC)  ||  || Active since 2001 ||  || Repository contains more than 10 terabytes of secure petrotechnical data ||  ||  || http://www.ppepdr.net/
|-
| {{flagcountry|Nigeria}} ||Department of Petroleum Resources  ||  ||  || Active since December 2003.
 || Preserve, maintain the integrity and promote the National E&amp;P data assets with improved quality, efficiency and accessibility in the most rapid, secure and reliable manner|| || International and PetroBank data management standards || Funded by Establishment Costs - one-off funding by Government and Running Costs - Subscription &amp;  Transaction Fees by Operators ||http://ndr.dprnigeria.com/
|-
| {{flagcountry|Turkey}} || PetroBank MDS  || Turkish Petroleum Corporation (TPAO). It is NOC of Turkey. || 36&#730;-42&#730; northern parallel and the 26-45&#730; eastern meridian.  || Operational since 2007 || Data assets preservation, easy access to assets, assets access controlling and auditing, consolidation of assets, national archive, central management of all assets, standardization of assets according to international standards and naming conventions, working with the most convenient assets. || Wells, Well log curves, well reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes and seismic acquisition/processing reports. || International and PetroBank data management standards || Funded fully by the Turkish Petroleum Corporation. Service usage is free of charge.  || http://www.tpao.gov.tr
|-
| {{flagcountry|Norway}} || DISKOS- Norwegian National Data Repository || Norwegian Petroleum Directorate (NPD) and DISKOS Group of oil companies || Norwegian continental shelf || Started in 1995 || To ensure compliance with NPD reporting regulations for digital E&amp;P data. To reduce data redundancy. To ensure that data is made generally available to the oil and gas industry and to society as a whole Long term preservation of data.
 || Wells, Well Log Curves, Seismic Surveys, field, pre-stack &amp;  post-stack seismic, seismic reports, production data (monthly allocated).Size of NDR estimated at more than 3 Petabytes.
 || SEG-D for seismic field data, SEG-Y for pre-stack and post-stack seismic data (currently only limited amounts of field and pre-stack data) All relevant well data standards such as LIS, DLIS, LAS, SPLA, SCAL etc. PDF and TIF are also used. || Costs are shared equally between all participating oil companies (around 50) in the Diskos consortium, including the NPD. In addition reporting companies pay to submit and download data. All Norwegian universities have free access to public data in Diskos. Non-oil companies can apply for Associated Membership, there are currently around 25 such members. ||http://www.diskos.no/ http://www.npd.no
|-
| {{flagcountry|United Kingdom}} || CDA || CDA Common Data Access Ltd  || UK Offshore Waters || Wells went live in 1995. Infrastructure started operations in 2000. Seismic went live in 2009. Estimated NDR size: 6 Terabytes||Save costs for licenses,Improve access to data,Comply with regulations || Well log curves, Well reports, Post-stack seismic, Seismic reports, VSP, deviation and test data. Estimated NDR size: 6 Terabytes || CDA has adopted DECC&#8217;s naming standards for wells and surveys and continues to work closely with DECC and industry to identify a range of standards (see the CDA and DECC websites for more on this) || Owned by the UK oil and gas industry ||
http://www.ukoilandgasdata.com
http://www.gov.uk/oil-and-gas-petroleum-operations-notices
http://www.cdal.com
|-
| {{flagcountry|United Kingdom}} || UKOGL || UK Onshore Geophysical Library || UK onshore || In operation since 1994. Managed and operated by Lynx Information Systems Ltd on behalf of UKOGL. || Custodian of all UK onshore seismic data || Seismic, well tops, logs, cultural. Current archive size approx 6TB || SEGY, UKOOA, LAS, DLIS || Self-funded through data sales ||
http://www.ukogl.org.uk
http://maps.lynxinfo.co.uk/UKOGL_LIVE/map.html
|-
| {{flagcountry|Brazil}} || ANP || Ag&#234;ncia Nacional do Petr&#243;leo (ANP)||  || BDEP formed in May 2000
 || ||Stores seismic, well log, post stack and pre-stack seismic data and potential field data(Grav/Mag)  || ANP standards in place || Funded by Members || http://www.bdep.gov.br
|-
| {{flagcountry|Mexico}} || Ditep || Pemex ||  ||Established in 2002  || || Promotes and preserve all the technical E&amp;P information assets of the country ||  || ||http://www.pep.pemex.com/index.html
|-
|  {{flagcountry|Israel}} ||  || The Ministry of National Infrastructures  || || Exploratory ||  ||  ||  ||  || http://www.mni.gov.il/mni/en-US/NaturalResources/OilandgasExploration/OilMaps/
|-
| {{flagcountry|Cyprus}} || MCIT || Ministry of Commerce, Industry and Tourism-Energy Service || Offshore  || Promotional || Responsible for granting licences for prospecting, exploration and exploitation of hydrocarbons ||  ||  ||  || http://www.mcit.gov.cy/mcit/mcit.nsf/dmlhexploration_en/dmlhexploration_en?OpenDocument
|-
| {{flagcountry|South Africa}} ||   || Petroleum Agency of South Africa ||  || Active || Seismic data, Well data, Samples, reports and diagrams ||  ||Standards: Formats &#8211; SEGD, SEGY, LIS, LAS, PDF and TIFF, Media &#8211; 3480, 3590, DLT, 8mm Exabyte, DAT   || From 2010 funded by Government || http://www.petroleumagencysa.com
|-
| {{flagcountry|Kenya}} || National Data Center (NDC) || National Oil Corporation of Kenya || Offshore and Onshore || Began in 2007, system implemented in 2010.  || Digital data preservation, National archive, to implement integrated data management systems, provide easy access to quality-controlled data for internal and external customers, attract oil and gas exploration investment and to reduce data management costs. || Wells, well log curves, well reports, post-stack seismic, field data/tapes, seismic acquisition/processing reports, interpretive maps and reports. || Seismic data- SEGY. 3590 or 3592 data cartridges.  || 100% Government Funded || http://www.nockenya.co.ke/
|-
| {{flagcountry|United States}} || BOEMRE || [[Bureau of Ocean Energy Management, Regulation and Enforcement|Bureau of Ocean Energy, Management, Regulation and Enforcement (BOEMRE)]] || Gulf of Mexico || Has replaced the former Minerals Management Service (MMS) ||  ||  ||  ||  || http://www.gomr.boemre.gov/homepg/data_center.html
|-
| {{flagcountry|United States}} || NGRDS || National Geoscience Data Repository System (NGDRS) ||  ||  || NGRDS is a system of geoscience data repositories, providing information about their respective holdings accessible through a web-based supercatalog. ||  || geologic, geophysical, and engineering data, maps, well logs, and samples || DOE has provided funds for the NGDRS since 1993  ||
http://www.agiweb.org/ngdrs/index.html
http://www.energy.gov/
http://www.agiweb.org/index.html

List of Repositories in US listed also as directory
http://www.agiweb.org/ngdrs/overview/datadirectory.html
|-
| {{flagcountry|Cambodia}} ||CNPA ||Cambodia National Petroleum Authority  || Onshore &amp; Offshore  ||  ||Promotion and preservation of technical E&amp;P information assets of the country  || ||  ||Norad/OfD and NPD assistance || http://www.cnpa-cambodia.com/
|-
| {{flagcountry|Afghanistan}} ||MOM ||Ministry of Mines of the Islamic Republic of Afghanistan (MoM)
|| Onshore ||  ||Promotion and preservation of technical E&amp;P information assets of the country  || ||  ||Norad/OfD and NPD assistance ||http://mom.gov.af/en/news/1637
|-
| {{flagcountry|Bangladesh}} || MOEMR  ||Hydrocarbon Unit, Ministry of Power, Energy and Mineral Resources (MOEMR)|| Onshore &amp; Offshore || Active and ongoing via HCU unit since 2005
|| A mini-data bank has established in the HCU to handle Production data, Resource data by using Database &amp; GIS Software 2005 and promotion of technical E&amp;P information assets of the country  || ||Funded assistance    ||Norad/OfD and NPD assistance ||
http://www.hcu.org.bd/
http://www.petrobangla.org.bd
http://www.bapex.com.bd
|-
| {{flagcountry|Ethiopia}} || MOME  ||Ministry of Mines and Energy Ethiopia  ||  || Active and ongoing ||Promotion of technical E&amp;P information assets of the country   || ||  || ||
http://www.mome.gov.et/petroleum.html
|-
| {{flagcountry|Cameroon}} ||SNH ||SNH Cameroon   ||  || Active &amp; Ongoing || Preservation and promotion of technical E&amp;P information assets of the country  || ||  || || http://www.snh.cm
|-
| {{flagcountry|Malaysia}} ||PIRI  ||Petronas  ||  ||Yet to establish full NDR  || Promotion and preservation of technical E&amp;P information assets of the country  || ||  || || http://www.petronas.com.my
|-
| {{flagcountry|Spain}} || ATH  ||  ||  ||  ||Online GIS databases to geophyscial information SIGEOF and ATH (Archivo de Hydrocarbures) || ||  ||  ||
http://www.mityc.es/energia/petroleo/Exploracion/Paginas/Estadisticas.aspx
http://hidrocarburos.mityc.es/ath/
http://www.igme.es/internet/sigeof/INICIOsiGEOF.htm
|-
| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.onhym.com
|-
| {{flagcountry|Madagascar}} ||OMNIS  ||  ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||Norad OfD and NPD assistance    ||
|-
| {{flagcountry|Sudan}} || PIC (Petroleum Information Center) || Ministry of Oil and Gas ||  || Active since 2000  || Preserve and promote E&amp;P data,managing Oil Museum || Wells, well log, well reports, cores and samples,seismic (acquisition/processing) reports, production data,GIS ||  ||  ||
http://www.spc.gov.sd
|-
| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.onhym.com
|-
| {{flagcountry|Nicaragua}} ||MEM || ||  ||  Active &amp; ongoing ||  || ||  || Norad OfD and NPD assistance   ||
http://www.ine.gob.ni
http://www.mem.gob.ni
|-
| {{flagcountry|Iraq}} ||MoO ||  Ministry of Oil Republic of Iraq||  || Active and Ongoing since 2005  || MoO establishing a centralized data base and NDR for Iraqi petroleum data and to ensure that data &amp; information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Norad/OfD and NPD assistance || http://www.oil.gov.iq
|-
| {{flagcountry|Latvia}} ||LEGMC  ||Latvian Environment, Geology and Meteorology Centre    || Offshore &amp; Onshore || || An E &amp; P data archive centre which provides data available for purchase  || Geological (well and seismic data, maps, reports etc.)  ||  ||  ||  http://mapx.map.vgd.gov.lv/geo3/VGD_OIL_PAGE/index.htm
|-
|  {{flagcountry|Albania}} || AKBN  || National Agency of Natural Resources ||  ||  || Generates and promotes exploration opportunities in Albania, maintains archive of E &amp; P data. ||  ||  ||  || http://www.akbn.gov.al/index.php?ak=details&amp;cid=5&amp;lng=en
|-
| {{flagcountry|Uganda}} ||PEPD|| Petroleum Exploration &amp; Production Dept (PEPD) || Onshore || || || Technical E &amp; P data archive and information ||  ||Norad/Ofd assistance || http://www.statehouse.go.ug/government.php?catId=10 http://www.energyandminerals.go.ug
|-
| {{flagcountry|Zambia}} |||| Ministry of Mines and Minerals Development, Geological Survey Department (GSD)
 || Onshore ||Active and ongoing || || Technical E &amp; P data archive and information - Technical Records Unit||  ||Norad/Ofd &amp; NPD assistance ||
http://www.zambiageosurvey.gov.zm/
|-
| {{flagcountry|Ivory Coast}} ||MME|| Ministry of Mines &amp; Energy || Onshore &amp; Offshore || || || ||  ||Norad/Ofd and NPD assistance ||
http://www.cotedivoirepr.ci/
http://www.petroci.ci/index.php?numlien=31
|-
| {{flagcountry|Romania}} || || National Agency for Mineral Resources  ||  || ||Promotion and preservation of technical E&amp;P information assets of the country  || ||  ||  || http://www.namr.ro/main_en.htm
|-
| {{flagcountry|Fiji}} ||SOPAC ||Mineral Resources Dept Fiji     |||| Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || In part externally managed ||
http://www.mrd.gov.fj/gfiji/
http://www.mrd.gov.fj/gfiji/petroleum/petroleum.html
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/fiji-islands
|-
| {{flagcountry|Papua New Guinea}} ||SOPAC||Department of Petroleum and Energy ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.petroleum.gov.pg
http://www.petrominpng.com.pg/about.html
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/papua-new-guinea
|-
| {{flagcountry|Solomon Islands}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations  || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/solomon-islands
|-
| {{flagcountry|Tonga}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia    || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/tonga
|-
| {{flagcountry|Vanuatu}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/vanuatu
|-
| {{flagcountry|Guyana}} ||GGMC  ||Guyana Geology and Mines Commission  ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.ggmc.gov.gy
|-
| {{flagcountry|Syria}} ||SPC  || Syrian Petroleum Company  ||  ||  || || ||  ||  || http://www.spc-sy.com/en/aboutus/aboutus1_en.php
|-
| {{flagcountry|Liberia}} ||NOCAL || National Oil Company of Liberia ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.nocal-lr.com/
|-
| {{flagcountry|Chile}} ||ENAP || National Oil Company of Chile ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.enap.cl
|-
| {{flagcountry|Thailand}} ||PTTEP ||PTT Exploration and Production Public Company Ltd  ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.pttep.com/
http://www.pttep.com/en/index.aspx
|-
| {{flagcountry|Venezuela}} ||PDVSA ||Petroleos de Venezuela  || Onshore &amp; Offshore ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.pdvsa.com/
|-
| {{flagcountry|Trinidad}} || || Trinidad Ministry of Energy and Energy Affairs ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.energy.gov.tt/energy_industry.php?mid=31
http://www.petrotrin.com/Petrotrin2007/UpstreamBusiness.htm
|-
| {{flagcountry|Mozambique}} || NAPD ||  ||  || Established in 1999 under NORAD support  || To ensure that data &amp; information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || National Budget and INP funds || http://www.inp.gov.mz
|-
| {{flagcountry|Denmark}} || || Danish Energy Agency  ||  ||   || Online GIS service for wells and license data  || ||  ||  ||
http://www.ens.dk/EN-US/OILANDGAS/Sider/Oilandgas.aspx
|-
| {{flagcountry|Dominican Republic}} || || Directorate of Hydrocarbons  ||  ||   ||  ||  ||  ||  || http://www.dgm.gov.do/sdhidrocarburo/index.html
|-
| {{flagcountry|Equatorial Guinea}} || ||   ||  ||   ||   Exploration databank for Equatorial Guinea||  ||  || ||
http://www.equatorialoil.com
http://www.equatorialoil.com/database.html
|-
| {{flagcountry|Faroe Islands}} || Jardfeingi || Jardfeingi Faorese Earth and Energy Directorate  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || || http://www.jardfeingi.fo
|-
| {{flagcountry|Philippines}} || PNOC || Philippine National Oil Company  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || ||
http://www.pnoc.com.ph
http://www.pnoc-ec.com.ph/business.php?id=2
|-
| {{flagcountry|Greenland}} || GreenPetroData || MMR- Ministry of Mineral Resources  ||  ||  ||  Web and GIS system providing access to all Released Well and Geophysical data. ||  ||  || ||
https://www.greenpetrodata.gl/
http://govmin.gl/
|-
| {{flagcountry|Iceland}} || Iceland Continental Shelf Portal (ICSP)  || Orkustofnunn - National Energy Authority   ||Offshore ||   The Iceland Continental Shelf Portal (ICSP)|| Provides access to information about data pertaining to the Icelandic Continental Shelf, in particular initially to the northern Dreki Area to assist with licensing round promotion || ||  ||  ||
http://www.os.is
http://www.nea.is/oil-and-gas-exploration/
|-
| {{flagcountry|Myanmar}} || MOGE || Myanmar Oil &amp; Gas Enterprise   ||  ||   ||    ||  ||  || ||
http://www.energy.gov.mm/upstreampetroleumsubsector.htm
|-
| {{flagcountry|Yemen}} || PEPA || Petroleum Exploration and Production Authority (PEPA)
   ||  ||   ||    ||  ||  || ||
http://www.pepa.com.ye/
|-
| {{flagcountry|Tunisia}} || ETAP || Enterprise Tunisienne D&#8217;Activities Petrolieres   ||  ||   ||  Promotion and preservation of technical E&amp;P information assets of the country    ||  ||  || ||
http://www.etap.com.tn
|-
| {{flagcountry|Gabon}} || DGH || Direction Generale des Hydrocarbures (DGH)||  ||   ||  ||  ||  || ||
http://www.gabon-industriel.com/les-actions/energie/petrole
|-
| {{flagcountry|Republic of the Congo  }} || SNPC || Soci&#233;t&#233; Nationale des p&#233;troles du Congo ||  ||   ||    ||  ||  || ||
|-
| {{flagcountry|Mali}} || Aurep || {{Not a typo|Autorite}} pour la Promotion de la Recherce des Petroliere au Mali
||  ||   ||   Databank service managing the geological and geophysical data relative to petroleum research. ||  ||  || ||
http://www.aurep.org
http://www.aurep.org/htmlpages/mali.html
|-
| {{flagcountry|Guatemala}} || MEM || Direcci&#243;n General de Hidrocarbures||  ||   ||  Online maps and images of wells, seismic, licenses, protected areas, exploration and production ||  ||  || ||
http://www.mem.gob.gt/Portal/home.aspx
http://www.mem.gob.gt/Portal/Home.aspx?secid=25
|-
| {{flagcountry|Iran}} || NIOC || National Oil Company of Iran ||  ||   ||   ||  ||  || ||
http://www.nioc.ir
|-
| {{flagcountry|Libya}} || NOC || NOC Libya ||  ||   ||  Virtual data room in place for promotion of exploration and exploitation of hydrocarbons ||  ||  || ||
|-
| {{flagcountry|United Arab Emirates}} || ADNOC || Abu Dhabi National Oil Company ||  ||   ||  ||  ||  ||  ||
http://www.adnoc.ae
|-
| {{flagcountry|Qatar}} ||  || Qatar Petroleum ||  ||   ||  ||  ||  ||  ||
http://www.qp.com.qa
|-
| {{flagcountry|South Korea}} || KNOC || Korea National Petroleum Corporation ||  ||   ||  ||  ||  ||  ||
http://www.knoc.co.kr
|-
| {{flagcountry|Seychelles}} || SNOC || Seychelles National Oil Company ||  ||   ||  ||  ||  ||  ||
|-
| {{flagcountry|Saudi Arabia}} ||  || Saudi Aramco ||  ||   ||  ||  ||  ||  ||
http://www.saudiaramco.com
|-
| {{flagcountry|Belarus}} || ||  ||  ||   ||  ||  ||  ||  ||
http://geologiya.org/index.php?categoryid=14
http://minpriroda.by/ru/napravlenia/minsyrbaza
|-
| {{flagcountry|East Timor}} || LAFAEK || Autoridade Nacional do Petr&#243;leo ||  || || Online GIS with wells and licences ||  ||  || Norad/OfD assistance  || http://www.anp-tl.org/webs/anptlweb.nsf/pgMaps
|}

==See also==
*[[Norwegian Petroleum Directorate]]
*[[Energistics]]
*[[Professional Petroleum Data Management Association (PPDM)]]
*[[Oil and gas industry in the United Kingdom]]
*[[Petroleum exploration in Guyana]]

==Notes==
{{Reflist}}

==External links==
*[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group Energistics: National Data Repository Work Group]
*[http://www.kadme.com/wp-content/uploads/KADME-Oil-and-Gas-Technology-Jan2011.pdf National Data Repositories: the case for open data in the oil and gas industry]
*[http://www.seg.org/ts Society of Exploration Geophysicists]

[[Category:Data management]]
[[Category:Open standards]]
[[Category:Hydrocarbons]]
[[Category:Geophysics organizations]]</text>
      <sha1>rhbih2gcj6413ezp1v39iedwdhiyu5s</sha1>
    </revision>
  </page>
  <page>
    <title>Altitude3.Net</title>
    <ns>0</ns>
    <id>47288071</id>
    <revision>
      <id>717345805</id>
      <parentid>717345798</parentid>
      <timestamp>2016-04-27T04:43:45Z</timestamp>
      <contributor>
        <username>DaltonCastle</username>
        <id>16866178</id>
      </contributor>
      <comment>added [[Category:Technical communication]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5346" xml:space="preserve">{{Infobox software
| name                   = Altitude3.Net
| logo                   = Altitude-en.png
| screenshot             = 
| caption                = Altitude3.Net Dashboard
| developer              = Nm&#233;dia Solutions
| status                 = Active
| released               = {{start date and age|2000|06|01}}
| operating system       = 
| platform               = [[.NET Framework|.Net]]
| genre                  = [[Content Management System]], [[Content Management Framework]]
| alexa                  =
| website                = {{URL|http://altitude3.net}}
}}

'''Altitude3.Net''' is an electronic business development platform that allows to create web and mobile solutions along with interactive communication strategies. The platform has the same functionalities &lt;ref&gt;{{cite web|language=fr|title = Pinpoint|url = https://pinpoint.microsoft.com/fr-CA/Companies/4296539037/Services|publisher = pinpoint.microsoft.com|accessdate = 2015-06-22}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Altitude3.Net|url=http://www.cmsmatrix.org/matrix/cms-matrix/altitude-3.net|website=CMS Matrix|accessdate=27 July 2015}}&lt;/ref&gt; than a content management system (CMS) and communicates with other systems (accounting systems, manufacturing management software (MRP), business management software (enterprise resource planning (ERP)), database, Excel files, XML, CSV or all other kinds of structural data).

Nm&#233;dia solutions developed Altitude3.Net in 2001 using Microsoft's .NET Framework technology. The platform is currently using the 4.5 version of Microsoft&#8217;s Framework.

== History ==
In 2001, Nm&#233;dia solutions created the content management system Altitude&lt;sup&gt;mc&lt;/sup&gt;.&lt;ref&gt;{{cite web|title=About|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}&lt;/ref&gt; As it went on, many versions were developed:
* Altitude Moto and Altitude Auto (2001 to 2006);
* Altitude 2 (2006);
* Altitude3.Net (2010).

== List of main functionalities ==
The Altitude3.Net platform is structured in many modules:&lt;ref&gt;{{cite web|title=Altitude advantages|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}&lt;/ref&gt;
* Content management
* contact management and mass-emailing
* Control of advanced SEO parameters
* Microsoft flexibility &amp; computability
* Security &amp; access management
* Security &amp; permission management
* E-commerce solutions: Centralized Product Management (CPM) services. This module includes several functionalities: interface for mass product modification, centralized coupon management, custom management by product group, inventory by store location, shopping cart, price &amp; currency management, catalog management, centralized database, supplier management, product by media, product comparison tool (based on common characteristics), syncing accounting software inventory with Altitude3.Net
* A [[Microsoft Azure]] solution (cloud computing)
* Omnichannel marketing
* Other functionalities : On-site search engines for meta data and documents (text, Word, Excel and PDF); HTML5 video player with descending compatibility; Integrated functions enabling an entire site to be generated in Hypertext Markup Language (HTML) or enabling to export all its data (DATA) and import it in any other CMS

== Awards and recognitions ==
* In 2010: Altitude3.Net is finalist in the IT category (software application) of the M&#233;rites du fran&#231;ais during the Francof&#234;te.
* In 2011: Nm&#233;dia solutions wins the title of Web Development Partner of the Year awarded by the Microsoft Partner Network.&lt;ref&gt;{{cite news|title=Microsoft honore deux entreprises de la r&#233;gion|url=http://www.lapresse.ca/la-tribune/economie-et-innovation/201112/19/01-4479237-microsoft-honore-deux-entreprises-de-la-region.php|accessdate=27 July 2015|publisher=[[La Presse (Canadian newspaper)]]|date=19 December 2011|language=fr}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Drummondville triomphe &#224; Toronto|url=http://www.journalexpress.ca/Actualites/Economie/2011-12-05/article-2825943/Drummondville-triomphe-a-Toronto/1|accessdate=27 July 2015|publisher=Journal l'Express|date=5 December 2011|language=fr}}&lt;/ref&gt;
* In 2012: Altitude3.Net wins the Prix Franco awarded by the Drummondville Young Chamber of Commerce during its annual gala.&lt;ref&gt;{{cite web|title=Prix Franco 2012 Nm&#233;dia Solutions Inc.|url=http://www.jccd.ca/Concours-Prix-Franco/Prix-Franco-2012/NMedia-Solutions-Inc.aspx|website=Jeune Chambre de Commerce de Drummondville|accessdate=27 July 2015|language=fr}}&lt;/ref&gt;
* In 2015: The CPM module of Altitude3 is finalist in the Web Solutions category at the Octas.&lt;ref&gt;{{cite web|title=Les laur&#233;ats du concours des Octas 2015|url=http://www.actionti.com/microsites/octas/gagnants/nos-gagnants|website=R&#233;seau Action TI|accessdate=27 July 2015|language=fr}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Nm&#233;dia en lice aux Octas|url=http://www.journalexpress.ca/Actualites/2015-05-12/article-4144312/Nmedias-en-lice-aux-Octas/1|accessdate=27 July 2015|publisher=Journal l'Express|date=12 May 2015|language=fr}}&lt;/ref&gt;

== See also ==
*[[List of content management systems]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://altitude3.net Altitude3.Net's website]

[[:Category:Content management systems]]
[[:Category:Website management]]



[[Category:Content management systems]]
[[Category:Data management]]
[[Category:Technical communication]]</text>
      <sha1>eivll36nj7w5zfkcw4718as2gbujesz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Clinical data management</title>
    <ns>14</ns>
    <id>34875542</id>
    <revision>
      <id>724141820</id>
      <parentid>478676895</parentid>
      <timestamp>2016-06-07T11:33:33Z</timestamp>
      <contributor>
        <username>Edgar181</username>
        <id>491706</id>
      </contributor>
      <comment>added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="96" xml:space="preserve">[[Category:Clinical research]]
[[Category:Pharmaceutical industry]]
[[Category:Data management]]</text>
      <sha1>03mm3kr2ro7sh2qbh6bnbwp22gapom4</sha1>
    </revision>
  </page>
  <page>
    <title>Research data archiving</title>
    <ns>0</ns>
    <id>10022970</id>
    <revision>
      <id>751090225</id>
      <parentid>751090168</parentid>
      <timestamp>2016-11-23T09:00:18Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>rm EL tag as we now dont have an EL section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15615" xml:space="preserve">'''Research data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of scholarly research [[data]], including  the natural sciences, social sciences, and life sciences. The various [[academic journals]] have differing policies regarding how much of their data and methods researchers are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.

Data archiving is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.

The requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].&lt;ref&gt;&#8221;Policy on Referencing Data in and Archiving Data for AGU Publications&#8221; [http://www.agu.org/pubs/authors/policies/data_policy.shtml]&lt;/ref&gt; This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.

Prior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The academic community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.

The need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.&lt;ref&gt;"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]&lt;/ref&gt;&lt;ref&gt;[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]&lt;/ref&gt;

==Selected policies by journals==

===''The American Naturalist''===
{{quote|''[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR&lt;ref&gt;[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]&lt;/ref&gt;}}

===''Journal of Heredity''===
{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.

The American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org&lt;ref&gt;[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]&lt;/ref&gt;}}

===''Molecular Ecology''===
{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley&lt;ref&gt;[http://www.wiley.com/bw/submit.asp?ref=0962-1083&amp;site=1 Policy on data archiving]&lt;/ref&gt;}}

===''Nature''===
{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.&lt;ref&gt;[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]&lt;/ref&gt;

''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]&lt;ref&gt;{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}&lt;/ref&gt;}}

===''Science''===
{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.&lt;ref&gt;[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]&lt;/ref&gt;

"Materials and methods" &#8211; ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]&lt;ref&gt;[http://www.sciencemag.org/about/authors/prep/prep_online.dtl &#8221;Preparing Your Supporting Online Material&#8221;]&lt;/ref&gt;}}

===Royal Society===
{{quote|To allow others to verify and build on the work published in [[Royal Society]] journals, it is a condition of publication that authors make available the data, code and research materials supporting the results in the article.
Datasets and code should be deposited in an appropriate, recognised, publicly available repository. Where no data-specific repository exists, authors should deposit their datasets in a general repository such as [[Dryad (repository)]] or [[Figshare]].
|[[Royal Society]]&lt;ref&gt;[https://royalsociety.org/journals/ethics-policies/data-sharing-mining/ "Data sharing and mining"]&lt;/ref&gt;}}

==Policies by funding agencies==
In the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.&lt;ref&gt;[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html &#8221;NSF to Ask Every Grant Applicant for Data Management Plan&#8221;]&lt;/ref&gt;

The NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.

==Data archives==

===Natural sciences===
The following list refers to scientific data archives.
* [[CISL Research Data Archive]]
* [[Dryad (repository)|Dryad]]
* [[ESO/ST-ECF Science Archive Facility]]
* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]
* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]
* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]
* [[National Archive of Computerized Data on Aging]]
* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]
* [[National Climatic Data Center]]
* [[National Geophysical Data Center]]
* [[National Snow and Ice Data Center]]
* [[National Oceanographic Data Center]]
* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]
* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth &amp; Environmental Science]]
* [[World Data Center]]
* [[DataONE]]

===Social sciences===
{{cleanup merge|Data archives|date=April 2016}}

'''Data archives''' are professional institutions for the acquisition, preparation, preservation, and dissemination of social and behavioral data. The term is also sometimes used about natural science institutions (e.g., [[CISL Research Data Archive]], see [[Scientific data archiving]] and Borgman, 2007, p.&amp;nbsp;18&lt;ref&gt;Borgman, Christine L. (2007).''Scholarship in the digital age: information, infrastructure and the internet''. Cambridge, MA: The MIT Press.&lt;/ref&gt;), but here seems '''data centers''' to be the most used term. Data archives in the social sciences evolved in the 1950s and has been perceived as an international movement: 

&lt;blockquote&gt;By 1964 the International Social Science Council (ISSC) had sponsored a second conference on Social Science Data Archives and had a standing Committee on Social Science Data, both of which stimulated the data archives movement. By the beginning of the twenty-first century, most developed countries and some developing countries had organized formal and well-functioning national data archives. In addition, college and university campuses often have `data libraries' that make data available to their faculty, staff, and students; most of these bear minimal archival responsibility, relying for that function on a national institution (Rockwell, 2001, p. 3227).&lt;ref&gt;Rockwell, R. C. (2001). Data Archives: International. IN: Smelser, N. J. &amp; Baltes, P. B. (eds.) ''International Encyclopedia of the Social and Behavioral Sciences'' (vol. 5, pp. 3225- 3230). Amsterdam: Elsevier&lt;/ref&gt;&lt;/blockquote&gt;

* [[Registry of Research Data Repositories | re3data.org]] is a global registry of research data repository indexing data archives from all disciplines: http://www.re3data.org
* CESSDA Members are data archives and other organisations that archive social science data and provide data for secondary use: http://www.cessda.net/about/members.html
* Consortium of European Social Science Data Archives: http://www.cessda.org/
* The Danish Data Archives: http://www.sa.dk/content/us/about_us ; specific page (only in Danish): http://www.sa.dk/dda/default.htm
* Inter-university Consortium for Political and Social Research: http://www.icpsr.umich.edu/
* The Roper Center for Public Opinion Research: http://www.ropercenter.uconn.edu
* The Social Science Data Archive: http://dataarchives.ss.ucla.edu/
* The NCAR Research Data Archive:  http://rda.ucar.edu

===Life sciences===
{{stub section|date=April 2016}}

==See also==
*[[Data archive]]

==References==
{{Reflist}}

==Notes==
* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]
* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]
* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]
* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]
* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]
* Data sharing and replication &#8211; Gary King website [http://gking.harvard.edu/replication.shtml]
* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]
* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]
* &#8220;How to encourage the right behaviour&#8221; An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]
* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]
* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]
* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]


[[Category:Computer archives]]
[[Category:Data management]]
[[Category:Data publishing]]
[[Category:Digital preservation]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Structured storage]]</text>
      <sha1>qxb64zeibon54urxs0gfo8c4nfcwl7g</sha1>
    </revision>
  </page>
  <page>
    <title>Intelligence Engine</title>
    <ns>0</ns>
    <id>51136320</id>
    <revision>
      <id>741695582</id>
      <parentid>739178254</parentid>
      <timestamp>2016-09-29T03:14:31Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>/* top */[[WP:AWB/GF|General fixes]], removed orphan tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7489" xml:space="preserve">'''Intelligence Engines''' are a type of [[enterprise information management]] that combine [[Business rule management system|business rule management]], [[predictive analytics|predictive]] and [[prescriptive analytics]] to form a unified information access platform that provides real-time intelligence through [[Web search engine|search technologies]], [[Dashboard (management information systems)|dashboards]] and/or existing business infrastructure.  Intelligence Engines are process and/or business problem specific, resulting in industry and/or function-specific [[marketing]] [[trademark]]s associated with them.  They can be differentiated from [[enterprise resource planning]] (ERP) software in that intelligence engines include organization-level business rules and proactive [[decision management]] functionality.

==History==
The first intelligence engine application appears to have been introduced in 2001 by [[Sonus Networks|Sonus Networks, Inc.]] in their patent US6961334 B1.&lt;ref name="sonus"&gt;{{cite patent | country = US | number = 6961334 | status = patent | title = Intelligence engine | gdate = 2005-11-01 | invent1 = Kaczmarczyk, Casimer M}}&lt;/ref&gt;  Applied to the field of telecommunications systems, the intelligence engine was composed of a database queried by a data distributor layer, received by a telephony management layer and acted upon by a facility management command &amp; control layer.&lt;ref name="sonus" /&gt;  This combined standalone business intelligence tools like a [[data warehouse]], reporting and querying software and a [[decision support system]].

The concept was reinforced in 2002 in patent application US20030236689 A1&lt;ref name="2002patent"&gt;{{cite patent | country = US | number = 20030236689 | status = application | title = Analyzing decision points in  business processes | pubdate = 2003-12-25 | invent1 = Casati, Fabio | invent2 = Sayal, Mehmet | invent3 = Guadalupe Castellanos, Maria | invent4 = Gunopulos, Dimitrios | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2003236689&amp;KC=A1&amp;FT=E&amp;locale=en_EP}}&lt;/ref&gt; which applied predictive quantitative models to data and used rules to correlate context data at different stages of the business process with business process outcomes to be presented to end users.&lt;ref name="2002patent" /&gt;

[[LogRhythm|LogRhythm Inc.]] advanced the concept in 2010 by adding event managers to the end of the intelligence engine's process to determine reporting, remediation and other outcomes.&lt;ref name="LogRhythm"&gt;{{cite patent | country = US | number = 2012131185 | status = application | title = Advanced Intelligence Engine | pubdate = 2012-05-24 | invent1 = Petersen , Chris | invent2 = Villella, Phillip | invent3 = Aisa, Brad | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=2012131185A1&amp;KC=A1&amp;FT=D}}&lt;/ref&gt;

In 2016, professional service company [[KPMG]] continued to advance the concept by commercializing intelligence engines with the introduction of Third Party Intelligence, which is differentiated from past intelligence engines in its increased use of embedded intellectual property, diversity of global data inputs and focus on predictive analytics to mitigate risk and yield cost savings.&lt;ref name="CIOReview"&gt;{{cite web|url=http://www.cioreview.com/news/kmpg-launches-third-party-intelligence-intelligence-engine-to-anticipate-thirdparty-disruptions-nid-14407-cid-78.html |title=KMPG Launches Third Party Intelligence: Intelligence Engine to Anticipate Third-party Disruptions|accessdate=2016-07-22}}&lt;/ref&gt;

==Traits==
As a system that combines human intelligence, data inputs, automated decision-making and unified information access, intelligence engines are an advancement in business intelligence tools because they: 
* integrate structured data and unstructured content in a single index&lt;ref name="itbiz"&gt;{{cite web|url=http://www.itbusinessedge.com/blogs/it-unmasked/attivio-applies-predictive-analytics-to-indexed-data.html |title=Attivio Applies Predictive Analytics to Indexed Data |accessdate=2016-07-22}}&lt;/ref&gt;
* provide advanced workflow automation that can trigger multiple business processes&lt;ref name="salesforce"&gt;{{cite press release|url=http://www.salesforce.com/company/news-press/press-releases/2015/03/150309.jsp |title=Salesforce Unveils Service Cloud Intelligence Engine&#8212;Fueling Smarter Customer Service for the Connected World |publisher=Salesforce.com|accessdate=2016-07-22}}&lt;/ref&gt;
* project future impact of data&lt;ref name="inboundlog"&gt;{{cite web|url=http://resources.inboundlogistics.com/digital/issues/il_digital_may2016.pdf |title=Inbound Logistics Magazine May 2016|accessdate=2016-07-22}}&lt;/ref&gt; such as supply chain threats &lt;ref name="3pie"&gt;{{cite web|url=https://www.kpmgspectrum.com/3pie/index.html |title=KPMG Spectrum |accessdate=2016-07-22}}&lt;/ref&gt;
* recommend best actions&lt;ref name="custmatrix"&gt;{{cite web|url=http://www.customermatrix.com/news-and-press-releases/press-releases/145-customermatrix-unveils-first-ever-cognitive-intelligence-engine-for-crm-2 |title=CustomerMatrix Unveils First-Ever Cognitive Intelligence Engine for CRM |accessdate=2016-07-22}}&lt;/ref&gt; / highlight opportunities for process improvement&lt;ref name="parasoft"&gt;{{cite web|url=https://www.parasoft.com/capability/process-intelligence-engine/ |title=Process Intelligence Engine (PIE) |accessdate=2016-07-22}}&lt;/ref&gt;
* leverage business intelligence from a variety of experts&lt;ref name="inboundlog" /&gt; 
* combine human expertise with the power of technology to deliver actionable intelligence&lt;ref name="CIOReview" /&gt;
* scale data visualization capabilities with the number of users&lt;ref name="armanta"&gt;{{cite web|url=http://www.armanta.com/product/technology/intelligence-engine/ |title=A Big Data User Experience |accessdate=2016-07-22}}&lt;/ref&gt;

==Applications==
* Attivio Active Intelligence Engine&#174;&lt;ref name="attivo-pr"&gt;{{cite web|url=http://info.attivio.com/rs/attivio/images/Attivio-Customer-Succes-Story-General-Electric.pdf |title=Active Intelligence Engine&amp;reg; (AIE&amp;reg;) Case Study: General Electric |accessdate=2016-07-22}}&lt;/ref&gt;
* [[KPMG]] Spectrum Intelligence Engine(s)&lt;ref name="KPMGIE"&gt;{{cite web|url=https://www.kpmgspectrum.com/3pie/about.html |title=KPMG Spectrum: Action through Intelligence |accessdate=2016-07-22}}&lt;/ref&gt;
* [[Salesforce.com|Salesforce]] Service Cloud Intelligence Engine&lt;ref name="salesforce" /&gt; 
* [[FireEye]] Threat Intelligence Engine&lt;ref name="fireeye"&gt;{{cite web|url=https://www.fireeye.com/products/dynamic-threat-intelligence/threat-intelligence-engine.html |title=FIREEYE THREAT INTELLIGENCE ENGINE |accessdate=2016-07-22}}&lt;/ref&gt;
* [[Factiva]] Intelligence Engine&lt;ref name="factiva"&gt;{{cite web|url=http://solutions.dowjones.com/collateral/files/dj-factivacom-brochure-F-3465.pdf |title=Factiva&amp;reg; - The Intelligence Engine |accessdate=2016-07-22}}&lt;/ref&gt;
* [[Parasoft]] Process Intelligence Engine&lt;ref name="parasoft" /&gt;

==See also==
* [[Business Intelligence]] (BI)
* [[Business intelligence tools]] 
* [[Business Rule Management System]]
* [[Enterprise Information Management]] 
* [[Predictive Analytics]] 
* [[Prescriptive analytics]]
* [[Decision Management]] 
* [[Data Science]]
* [[Data Mining]]

==References==

{{Reflist}}

{{DEFAULTSORT:Intelligence Engine}}
[[Category:Data management]]
[[Category:Information management]]
[[Category:Big data]]
[[Category:Business terms]]
[[Category:Business intelligence]]
[[Category:Information systems]]
[[Category:Supply chain management terms]]</text>
      <sha1>0qs2h97071mmikjalai5n7l7sb18r7r</sha1>
    </revision>
  </page>
  <page>
    <title>Lambda architecture</title>
    <ns>0</ns>
    <id>43539426</id>
    <revision>
      <id>751624124</id>
      <parentid>749778644</parentid>
      <timestamp>2016-11-26T21:52:40Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9474" xml:space="preserve">[[File:Diagram of Lambda Architecture (generic).png|thumb|Flow of data through the processing and serving layers of a generic lambda architecture]]
'''Lambda architecture''' is a [[data processing|data-processing]] architecture designed to handle massive quantities of data by taking advantage of both [[batch processing|batch]]- and [[stream processing|stream-processing]] methods. This approach to architecture attempts to balance [[latency (engineering)|latency]], [[throughput]], and [[fault-tolerance]] by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of [[big data]], real-time analytics, and the drive to mitigate the latencies of [[map-reduce]].&lt;ref&gt;{{cite web|last1=Schuster|first1=Werner|title=Nathan Marz on Storm, Immutability in the Lambda Architecture, Clojure|url=http://www.infoq.com/interviews/marz-lambda-architecture|website=www.infoq.com}} Interview with Nathan Marz, 6 April 2014&lt;/ref&gt;

Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record.&lt;ref name=bijnens-slide&gt;Bijnens, Nathan. [http://lambda-architecture.net/architecture/2013-12-11-a-real-time-architecture-using-hadoop-and-storm-devoxx/ "A real-time architecture using Hadoop and Storm"]. 11 December 2013.&lt;/ref&gt;{{rp|32}} It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.

==Overview==
Lambda architecture describes a system consisting of three layers: batch processing, speed (or real-time) processing, and a serving layer for responding to queries.&lt;ref name=big-data&gt;Marz, Nathan; Warren, James. ''Big Data: Principles and best practices of scalable realtime data systems''. Manning Publications, 2013.&lt;/ref&gt;{{rp|13}} The processing layers ingest from an immutable master copy of the entire data set.

===Batch layer===
The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. The batch layer aims at perfect accuracy by being able to process ''all'' available data when generating views. This means it can fix any errors by recomputing based on the complete data set, then updating existing views. Output is typically stored in a read-only database, with updates completely replacing existing precomputed views.&lt;ref name=big-data /&gt;{{rp|18}}

[[Hadoop|Apache Hadoop]] is the de facto standard batch-processing system used in most high-throughput architectures.&lt;ref&gt;Kar, Saroj. [http://cloudtimes.org/2014/05/28/hadoop-sector-will-have-annual-growth-of-58-for-2013-2020/ "Hadoop Sector will Have Annual Growth of 58% for 2013-2020"], 28 May 2014. ''Cloud Times''.&lt;/ref&gt;

===Speed layer===
[[File:Diagram of Lambda Architecture (named components).png|thumb|Diagram showing the flow of data through the processing and serving layers of lambda architecture. Example named components are shown.]]
The speed layer processes data streams in real time and without the requirements of fix-ups or completeness. This layer sacrifices throughput as it aims to minimize latency by providing real-time views into the most recent data. Essentially, the speed layer is responsible for filling the "gap" caused by the batch layer's lag in providing views based on the most recent data. This layer's views may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received, and can be replaced when the batch layer's views for the same data become available.&lt;ref name=big-data /&gt;{{rp|203}}

Stream-processing technologies typically used in this layer include [[Storm (event processor)|Apache Storm]], [[Sqlstream|SQLstream]] and [[Apache Spark]]. Output is typically stored on fast NoSQL databases.&lt;ref name=kinley&gt;Kinley, James. [http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting "The Lambda architecture: principles for architecting realtime Big Data systems"], retrieved 26 August 2014.&lt;/ref&gt;&lt;ref&gt;Ferrera Bertran, Pere. [http://www.datasalt.com/2014/01/lambda-architecture-a-state-of-the-art/ "Lambda Architecture: A state-of-the-art"]. 17 January 2014, Datasalt.&lt;/ref&gt;

===Serving layer===
[[File:Diagram of Lambda Architecture (Druid data store).png|thumb|Diagram showing a lambda architecture with a Druid data store.]]
Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.

Examples of technologies used in the serving layer include [[Druid (open-source data store)|Druid]], which provides a single cluster to handle output from both layers.&lt;ref name=metamarkets-lambda&gt;Yang, Fangjin, and Merlino, Gian. [https://speakerdeck.com/druidio/real-time-analytics-with-open-source-technologies-1 "Real-time Analytics with Open Source Technologies"]. 30 July 2014.&lt;/ref&gt; Dedicated stores used in the serving layer include [[Apache Cassandra]] or [[Apache HBase]] for speed-layer output, and [https://github.com/nathanmarz/elephantdb Elephant DB] or [[Cloudera Impala]] for batch-layer output.&lt;ref name=bijnens-slide /&gt;{{rp|45}}&lt;ref name=kinley /&gt;

==Optimizations==
To optimize the data set and improve query efficiency, various rollup and aggregation techniques are executed on raw data,&lt;ref name=metamarkets-lambda /&gt;{{rp|23}} while estimation techniques are employed to further reduce computation costs.&lt;ref&gt;Ray, Nelson. [https://metamarkets.com/2013/histograms/ "The Art of Approximating Distributions: Histograms and Quantiles at Scale"]. 12 September 2013. Metamarkets.&lt;/ref&gt; And while expensive full recomputation is required for fault tolerance, incremental computation algorithms may be selectively added to increase efficiency, and techniques such as ''partial computation'' and resource-usage optimizations can effectively help lower latency.&lt;ref name=big-data /&gt;{{rp|93,287,293}}

==Lambda architecture in use==
Metamarkets, which provides analytics for companies in the programmatic advertising space, employs a version of the lambda architecture that uses [[Druid (open-source data store)|Druid]] for storing and serving both the streamed and batch-processed data.&lt;ref name=metamarkets-lambda /&gt;{{rp|42}}

For running analytics on its advertising data warehouse, [[Yahoo]] has taken a similar approach, also using [[Storm (event processor)|Apache Storm]], [[Hadoop|Apache Hadoop]], and [[Druid (open-source data store)|Druid]].&lt;ref name=yahoo-lambda&gt;Rao, Supreeth; Gupta, Sunil. [http://www.slideshare.net/Hadoop_Summit/interactive-analytics-in-human-time?next_slideshow=1 "Interactive Analytics in Human Time"]. 17 June 2014&lt;/ref&gt;{{rp|9,16}}

The [[Netflix]] Suro project has separate processing paths for data, but does not strictly follow lambda architecture since the paths may be intended to serve different purposes and not necessarily to provide the same type of views.&lt;ref name=netflix&gt;Bae, Jae Hyeon; Yuan, Danny; Tonse, Sudhir. [http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html "Announcing Suro: Backbone of Netflix's Data Pipeline"], ''[[Netflix]]'', 9 December 2013&lt;/ref&gt; Nevertheless, the overall idea is to make selected real-time event data available to queries with very low latency, while the entire data set is also processed via a batch pipeline. The latter is intended for applications that are less sensitive to latency and require a map-reduce type of processing.

==Criticism==
Criticism of lambda architecture has focused on its inherent complexity and its limiting influence. The batch and streaming sides each require a different code base that must be maintained and kept in sync so that processed data produces the same result from both paths. Yet attempting to abstract the code bases into a single framework puts many of the specialized tools in the batch and real-time ecosystems out of reach.&lt;ref&gt;{{cite web|last1=Kreps|first1=Jay|title=Questioning the Lambda Architecture|url=http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html|website=radar.oreilly.com|publisher=Oreilly|accessdate=15 August 2014|ref=kreps}}&lt;/ref&gt;

In a technical discussion over the merits of employing a pure streaming approach, it was noted that using a flexible streaming framework such as [[Apache Samza]] could provide some of the same benefits as batch processing without the latency.&lt;ref&gt;[https://news.ycombinator.com/item?id=7976785 Hacker News] retrieved 20 August 2014&lt;/ref&gt; Such a streaming framework could allow for collecting and processing arbitrarily large windows of data, accommodate blocking, and handle state.

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://lambda-architecture.net/ Repository of Information on Lambda of Architecture]

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data processing]]
[[Category:Big data]]
[[Category:Data management]]
[[Category:Free software projects]]
[[Category:Software architecture]]</text>
      <sha1>t781nt95sbnlymxlf5mxb49ypph7n72</sha1>
    </revision>
  </page>
  <page>
    <title>Critical data studies</title>
    <ns>0</ns>
    <id>51578025</id>
    <revision>
      <id>755904168</id>
      <parentid>755904057</parentid>
      <timestamp>2016-12-20T21:05:31Z</timestamp>
      <contributor>
        <username>Slashme</username>
        <id>451287</id>
      </contributor>
      <comment>Added {{[[Template:notability|notability]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5594" xml:space="preserve">{{notability|date=December 2016}}
{{Orphan|date=December 2016}}

'''Critical data studies''' is the systematic study of data and its criticisms.&lt;ref&gt;Dalton, Craig, and Jim Thatcher, 2014.&lt;/ref&gt; The field was named by scholars [[Craig Dalton]] and [[Jim Thatcher]] in their 2015 article titled "What does a critical data studies look like, and why do we care?" Interest has developed in this domain as a response to the emergence and reliance on '[[big data]]' in contemporary society.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Some of the other key scholars in this discipline include [[Rob Kitchen]] and [[Tracey P. Lauriault]].&lt;ref&gt;Kitchin, Rob, and Tracey P. Lauriault, 2014&lt;/ref&gt;&lt;ref&gt;Kitchin, Rob, 2014&lt;/ref&gt; Scholars have attempted to make sense of data through different theoretical frameworks, some of which include analyzing data technically, ethically, politically/economically, temporally/spatially, and philosophically.&lt;ref&gt;Kitchin, Rob, 2014&lt;/ref&gt; Some of the key academic journals related to critical data studies include the ''[[Journal of Big Data]]'' and ''[[Big Data and Society]]''.

==Why is a critical approach to data needed?==

In their article in which they coin the term 'critical data studies,' Dalton and Thatcher also provide several justifications as to why data studies is a discipline worthy of a critical approach.&lt;ref&gt;Dalton, Craig, and Jim Thatcher, 2014.&lt;/ref&gt; Firstly, 'big data' is an important aspect of twenty-first century society, and the analysis of 'big data' allows for a deeper understanding of what is happening and for what reasons.&lt;ref&gt;Dalton, Craig, and Jim Thatcher, 2014.&lt;/ref&gt; Furthermore, big data as a technological tool and the information that it yields are not neutral, according to Dalton and Thatcher,&lt;ref&gt;''Ibid.''&lt;/ref&gt; making it worthy of critical analysis in order to identify and address its biases. Building off this idea, another justification for a critical approach is that the relationship between big data and society is an important one, and therefore worthy of study.&lt;ref&gt;''Ibid.''&lt;/ref&gt;  Dalton and Thatcher stress how the relationship is not an example of [[technological determinism]], but rather how big data can shape the lives of individuals. Big data technology can cause significant changes in society's structure and in the everyday lives of people,&lt;ref&gt;''Ibid.''&lt;/ref&gt; and being a product of society, big data technology is worthy of sociological investigation.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Moreover, data sets are almost never completely raw, that is to say without any influences. Dalton and Thatcher describe how data are shaped by the vision or goals of a research team, and during the data collection process, certain things are quantified, stored, sorted and even discarded by the research team.&lt;ref&gt;''Ibid.''&lt;/ref&gt; A critical approach is thus necessary in order to understand and reveal the intent behind the information being presented. Dalton and Thatcher also argue how data alone cannot speak for itself; in order to possess any concrete meaning, data must be accompanied by theoretical insight or be accompanied by alternative quantitative or qualitative research measures.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Dalton and Thatcher argue that if one were to only think of data in terms of its exploitative power, there is no possibility of using data for revolutionary, liberatory purposes.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Finally, Dalton and Thatcher propose that a critical approach in studying data allows for 'big data' to be combined with older, 'small data,' and thus create more thorough research, opening up more opportunities, questions and topics to be explored.&lt;ref&gt;''Ibid.''&lt;/ref&gt;

==Issues and Concerns for Critical Data Scholars==

The use of data in modern society brings about new ways of understanding and measuring the world, but also brings with it certain concerns or issues.&lt;ref&gt;Kitchin, Rob, 2014&lt;/ref&gt; Data scholars attempt to bring some of these issues to light in their quest to be critical of data. Rob Kitchin identifies both technical and organizational issues of data, as well as some normative and ethical questions.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Technical and organization issues concerning data range from the scope of datasets, access to the data, the quality of the data, the integration of the data, the application of analytics and ecological fallacies, as well as the skills and organizational capabilities of the research team.&lt;ref&gt;''Ibid.''&lt;/ref&gt; Some of the normative and ethical concerns addressed by Kitchin include surveillance through one's data (dataveillance), the privacy of one's data, the ownership of one's data, the security of one's data, anticipatory or corporate governance, and finally profiling individuals by their data.&lt;ref&gt;''Ibid.''&lt;/ref&gt; All of these concerns must be taken into account by scholars of data in their objective to be critical.

==References==
{{reflist|24em}}

==Sources==
* Dalton, Craig, and Jim Thatcher. "What does a critical data studies look like, and why do we care? Seven points for a critical approach to &#8216;big data&#8217;." ''Society and Space open site'' (2014). Retrieved October 23, 2016.
* Elkins, James R. "The Critical Thinking Movement: Alternating Currents in One Teacher's Thinking". ''myweb.wvnet.edu''(1999). Retrieved 29 November 2016.
* Kitchin, Rob. ''The data revolution: Big data, open data, data infrastructures and their consequences.'' Sage, 2014. Retrieved October 23, 2016.
* Kitchin, Rob, and Tracey P. Lauriault. "Towards critical data studies: Charting and unpacking data assemblages and their work." (2014). Retrieved October 23, 2016.

[[Category:Data management]]</text>
      <sha1>mna8keu8k6j7g0k2ctyoetabq4zxq13</sha1>
    </revision>
  </page>
  <page>
    <title>Data recovery</title>
    <ns>0</ns>
    <id>2160183</id>
    <revision>
      <id>762889004</id>
      <parentid>762887218</parentid>
      <timestamp>2017-01-31T07:17:15Z</timestamp>
      <contributor>
        <username>Harshitgpt89</username>
        <id>29513190</id>
      </contributor>
      <comment>/* Logical damage */  added other forms of logical damages</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17631" xml:space="preserve">{{Use dmy dates|date=June 2016}}
{{Multiple issues|
{{Refimprove|date=February 2012}}
{{Manual|date=April 2016}}
}}

In [[computing]], '''data recovery''' is a process of salvaging (retrieving) inaccessible, lost, corrupted, damaged or formatted data from [[secondary storage]], [[removable media]] or [[Computer file|files]], when the data stored in them cannot be accessed in a normal way. The data is most often salvaged from storage media such as internal or external [[hard disk drive]]s (HDDs), [[solid-state drive]]s (SSDs), [[USB flash drive]]s, [[Magnetic tape data storage|magnetic tapes]], [[CD]]s, [[DVD]]s, [[RAID]] subsystems, and other [[electronic devices]]. Recovery may be required due to physical damage to the storage devices or logical damage to the [[file system]] that prevents it from being [[Mount (computing)|mounted]] by the host [[operating system]] (OS).

The most common data recovery scenario involves an operating system failure, malfunction of a storage device, logical failure of storage devices, accidental damage or deletion, etc. (typically, on a single-drive, single-[[disk partition|partition]], single-OS system), in which case the ultimate goal is simply to copy all important files from the damaged media to another new drive. This can be easily accomplished using a [[Live CD]], many of which provide a means to [[Mount (computing)|mount]] the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a [[file manager]] or [[optical disc authoring software]]. Such cases can often be mitigated by [[disk partition]]ing and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.

Another scenario involves a drive-level failure, such as a compromised [[file system]] or drive partition, or a [[hard disk drive failure]]. In any of these cases, the data is not easily read from the media devices. Depending on the situation, solutions involve repairing the logical file system, partition table or [[master boot record]],or updating the firmware or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive's "firmware"), to hardware replacement on a physically damaged drive which involves changes the parts of the damaged drive to make the data in a readable form and can be copied to a new drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.

In a third scenario, files have been accidentally "[[file deletion|deleted]]" from a storage medium by the users. Typically, the contents of deleted files are not removed immediately from the physical drive; instead, references to them in the directory structure are removed, and thereafter space they deleted data occupy is made available for later data overwriting. In the mind of [[end user]]s, deleted files cannot be discoverable through a standard file manager, but the deleted data still technically exists on the physical drive. In the meantime, the original file contents remain, often in a number of disconnected [[File system fragmentation|fragments]], and may be recoverable if not overwritten by other data files.

The term "data recovery" is also used in the context of [[Computer forensics|forensic]] applications or [[espionage]], where data which have been [[Encryption|encrypted]] or hidden, rather than damaged, are recovered. Sometimes data present in the computer gets encrypted or hidden due to reasons like virus attack which can only be recovered by some computer forensic experts. 

==Physical damage==
{{See also|Data recovery hardware}}

A wide variety of failures can cause physical damage to storage media, which may result from human errors and natural disasters. [[CD-ROM]]s can have their metallic substrate or dye layer scratched off; hard disks can suffer any of several mechanical failures, such as [[head crash]]es and failed motors; [[tape drive|tapes]] can simply break. Physical damage always causes at least some data loss, and in many cases the logical structures of the file system are damaged as well. Any logical damage must be dealt with before files can be salvaged from the failed media.

Most physical damage cannot be repaired by end users. For example, opening a hard disk drive in a normal environment can allow airborne dust to settle on the platter and become caught between the platter and the [[read/write head]], causing new head crashes that further damage the platter and thus compromise the recovery process. Furthermore, end users generally do not have the hardware or technical expertise required to make these repairs. Consequently, data recovery companies are often employed to salvage important data with the more reputable ones using [[Cleanroom#Cleanroom classifications|class 100]] dust- and static-free [[cleanroom]]s.&lt;ref&gt;{{cite web|last=Vasconcelos|first=Pedro|title=DIY data recovery could mean "bye-bye"|url=http://blog.ontrackdatarecovery.co.uk/data-recovery-realities/diy-data-recovery-could-mean-bye-bye/|work=The Ontrack Data Recovery Blog|publisher=Kroll Ontrack UK|accessdate=23 May 2013}}&lt;/ref&gt;

===Recovery techniques===
Recovering data from physically damaged hardware can involve multiple techniques. Some damage can be repaired by replacing parts in the hard disk. This alone may make the disk usable, but there may still be logical damage. A specialized disk-imaging procedure is used to recover every readable bit from the surface. Once this image is acquired and saved on a reliable medium, the image can be safely analyzed for logical damage and will possibly allow much of the original file system to be reconstructed.

==== {{Anchor|SERVICE-AREA}}Hardware repair ====
[[File:HD with toasty PCB.jpg|thumb|right|250px|Media that has suffered a catastrophic electronic failure requires data recovery in order to salvage its contents.]]

A common misconception is that a damaged [[printed circuit board]] (PCB) may be simply replaced during recovery procedures by an identical PCB from a healthy drive. While this may work in rare circumstances on hard disk drives manufactured before 2003, it will not work on newer drives.  Electronics boards of modern drives usually contain drive-specific [[adaptation data]] required for accessing their system areas, so the related componentry needs to be either reprogrammed (if possible) or unsoldered and transferred between two electronics boards.&lt;ref&gt;{{cite web
 | url = http://www.donordrives.com/pcb-replacement-guide
 | title = Hard Drive Circuit Board Replacement Guide or How To Swap HDD PCB
 | accessdate = 27 May 2015
 | website = donordrives.com
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap
 | archiveurl = https://web.archive.org/web/20130329021847/http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap
 | title = Firmware Adaptation Service - ROM Swap
 | accessdate = 27 May 2015 | archivedate = 29 March 2013
 | website = pcb4you.com
}}&lt;/ref&gt;

Each hard disk drive has what is called a ''system area'' or ''service area''; this portion of the drive, which is not directly accessible to the [[end user]], usually contains drive's [[firmware]] and adaptive data that helps the drive operate within normal parameters.&lt;ref&gt;{{cite web
 | url = http://www.recover.co.il/SA-cover/SA-cover.pdf
 | title = Hiding Data in Hard Drive's Service Areas
 | date = 14 February 2013 | accessdate = 23 January 2015
 | author = Ariel Berkman | website = recover.co.il
 | format = PDF
}}&lt;/ref&gt;  One function of the system area is to log defective sectors within the drive; essentially telling the drive where it can and cannot write data.

The sector lists are also stored on various chips attached to the PCB, and they are unique to each hard disk drive. If the data on the PCB do not match what is stored on the platter, then the drive will not calibrate properly.&lt;ref&gt;[https://web.archive.org/web/20130416232748/http://datarecoveryreport.com/#Swapping_PCB_Logic_Board#Swapping_PCB_Logic_Board Swapping Data Recovery Report]&lt;/ref&gt; In most cases the drive heads will click because they are unable to find the data matching what is stored on the PCB.

==Logical damage==
{{See also|List of data recovery software}}
[[Image:Data loss of image file.JPG|thumb|Result of a failed data recovery from a hard disk drive.]]

The term "logical damage" refers to situations in which the error is not a problem in the hardware and requires software-level solutions.

===Corrupt partitions and file systems, media errors===
In some cases, data on a hard disk drive can be unreadable due to damage to the [[partition table]] or [[file system]], or to (intermittent) media errors. In the majority of these cases, at least a portion of the original data can be recovered by repairing the damaged partition table or file system using specialized data recovery software such as [[Testdisk]]; software like [[dd rescue]] can image media despite intermittent errors, and image raw data when there is partition table or file system damage. This type of data recovery can be performed by people without expertise in drive hardware, as it requires no special physical equipment or access to platters.

Sometimes data can be recovered using relatively simple methods and tools;&lt;ref&gt;[http://www.recover-computerdata.com/ Data Recovery Software]&lt;/ref&gt; more serious cases can require expert intervention, particularly if parts of files are irrecoverable. [[File carving|Data carving]] is the recovery of parts of damaged files using knowledge of their structure.

===Overwritten data===
{{See also|Data erasure}}

After data has been physically overwritten on a hard disk drive, it is generally assumed that the previous data are no longer possible to recover. In 1996, [[Peter Gutmann (computer scientist)|Peter Gutmann]], a computer scientist, presented a paper that suggested overwritten data could be recovered through the use of [[magnetic force microscope]].&lt;ref&gt;[http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html ''Secure Deletion of Data from Magnetic and Solid-State Memory''], Peter Gutmann, Department of Computer Science, University of Auckland&lt;/ref&gt; In 2001, he presented another paper on a similar topic.&lt;ref&gt;[http://www.cypherpunks.to/~peter/usenix01.pdf ''Data Remanence in Semiconductor Devices''], Peter Gutmann, IBM T.J. Watson Research Center&lt;/ref&gt;  To guard against this type of data recovery, Gutmann and Colin Plumb designed a method of irreversibly scrubbing data, known as the [[Gutmann method]] and used by several disk-scrubbing software packages.

Substantial criticism has followed, primarily dealing with the lack of any concrete examples of significant amounts of overwritten data being recovered.&lt;ref&gt;{{cite web | last = Feenberg | first = Daniel | title = Can Intelligence Agencies Read Overwritten Data? A response to Gutmann. | publisher = National Bureau of Economic Research | date = 14 May 2004 | url = http://www.nber.org/sys-admin/overwritten-data-guttman.html | accessdate = 21 May 2008}}&lt;/ref&gt; Although Gutmann's theory may be correct, there is no practical evidence that overwritten data can be recovered, while research has shown to support that overwritten data cannot be recovered.{{specify|date=June 2013}}&lt;ref&gt;{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough/ |title=Disk Wiping &#8211;  One Pass is Enough |date=17 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20120902011743/http://www.anti-forensics.com:80/disk-wiping-one-pass-is-enough |archivedate=2 September 2012 |df=dmy }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |title=Disk Wiping &#8211;  One Pass is Enough &#8211;  Part 2 (this time with screenshots) |date=18 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20121127130830/https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |archivedate=27 November 2012 |df=dmy }}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://blogs.sans.org/computer-forensics/2009/01/15/overwriting-hard-drive-data/
 | title = Overwriting Hard Drive Data
 | date = 15 January 2009
 | first = Dr. Craig | last = Wright
}}&lt;/ref&gt;

[[Solid-state drive]]s (SSD) overwrite data differently from [[hard disk drive]]s (HDD) which makes at least some of their data easier to recover. Most SSDs use [[flash memory]] to store data in pages and blocks, referenced by logical block addresses (LBA) which are managed by the flash translation layer (FTL). When the FTL modifies a sector it writes the new data to another location and updates the map so the new data appear at the target LBA. This leaves the pre-modification data in place, with possibly many generations, and recoverable by data recovery software.


===Lost, Deleted &amp; Formatted Data ===

Sometimes, data present in the physical drives (Internal/External Hard disk, Pen Drive, etc) gets lost, deleted and formatted due to circumstances like virus attack, accidental deletion or accidental use of SHIFT+DELETE. In these cases, data recovery software are used to recover/restore the data files. 

===Logical Bad Sector ===

In the list of logical failures of Hard disk, Logical bad sector is the most common in which data files can't be retrieved from particular sector of the media drives. To resolve this, software are incorporated to correct the logical sectors of media drive and is this is not enough, then there is need for replacement of hardware parts to make the logical bad sectors to be OK.

==Remote data recovery==
Recovery experts do not always need to have physical access to the damaged hardware.  When the lost data can be recovered by software techniques, they can often perform the recovery using remote access software over the Internet, LAN or other connection to the physical location of the damaged media.  The process is essentially no different from what the end user could perform by themselves.&lt;ref&gt;{{Cite web|url = http://datarecovery-overinternet.datarecoverydigest.com/|title = Data Recovery Over the Internet|date = 17 December 2012|accessdate = 29 April 2015|website = Data Recovery Digest|publisher = |last = Barton|first = Andre}}&lt;/ref&gt;

Remote recovery requires a stable connection with an adequate bandwidth. However, it is not applicable where access to the hardware is required, as in cases of physical damage.

==Four phases of data recovery==
Usually, there are four phases when it comes to successful data recovery, though that can vary depending on the type of data corruption and recovery required.&lt;ref&gt;{{cite web
 | url = http://www.dolphindatalab.com/the-four-phases-of-data-recovery/
 | title = [Infographic] Four Phases Of Data Recovery
 | date = 28 December 2012 | accessdate = 23 March 2015
 | author = Stanley Morgan | website = dolphindatalab.com
}}&lt;/ref&gt;

; Phase 1: Repair the hard disk drive
: Repair the hard disk drive so it is running in some form, or at least in a state suitable for reading the data from it. For example, if heads are bad they need to be changed; if the PCB is faulty then it needs to be fixed or replaced; if the spindle motor is bad the platters and heads should be moved to a new drive.

; Phase 2: Image the drive to a new drive or a disk image file
: When a hard disk drive fails, the importance of getting the data off the drive is the top priority. The longer a faulty drive is used, the more likely further data loss is to occur. Creating an image of the drive will ensure that there is a secondary copy of the data on another device, on which it is safe to perform testing and recovery procedures without harming the source.

; Phase 3: Logical recovery of files, partition, MBR and filesystem structures
: After the drive has been cloned to a new drive, it is suitable to attempt the retrieval of lost data. If the drive has failed logically, there are a number of reasons for that. Using the clone it may be possible to repair the partition table or [[master boot record]] (MBR) in order to read the file system's data structure and retrieve stored data.

; Phase 4: Repair damaged files that were retrieved
: Data damage can be caused when, for example, a file is written to a sector on the drive that has been damaged. This is the most common cause in a failing drive, meaning that data needs to be reconstructed to become readable. Corrupted documents can be recovered by several software methods or by manually reconstructing the document using a hex editor.

== See also ==
{{Portal|Computer security|Computing}}

{{Div col||20em}}
* [[Backup]]
* [[Cleanroom]]
* [[Comparison of file systems]]
* [[Computer forensics]]
* [[Continuous data protection]]
* [[Data archaeology]]
* [[Data loss]]
* [[Error detection and correction]]
* [[File carving]]
* [[Hidden file and hidden directory]]
* [[Undeletion]]
{{Div col end}}

== References ==
{{Reflist|30em}}

==Further reading==
* Tanenbaum, A. &amp; Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}


{{Data erasure}}

{{DEFAULTSORT:Data recovery}}
[[Category:Data recovery| ]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Transaction processing]]
[[Category:Hard disk software|*]]
[[Category:Backup|Recovery]]</text>
      <sha1>qcw1v405k7sgjwvxeip6mm2b5l6r9tn</sha1>
    </revision>
  </page>
  <page>
    <title>Rubrik</title>
    <ns>0</ns>
    <id>52955620</id>
    <revision>
      <id>762349594</id>
      <parentid>761580008</parentid>
      <timestamp>2017-01-28T06:35:01Z</timestamp>
      <contributor>
        <username>Majora</username>
        <id>25977978</id>
      </contributor>
      <comment>+logo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5235" xml:space="preserve">'''Rubrik''' is a privately held cloud data management company headquartered in [[Palo Alto, California|Palo Alto, CA]]. Rubrik offers a data management platform for enterprises in private, public, and hybrid [[Cloud computing|cloud environments]].&lt;ref&gt;{{Cite web|url=http://social.techcrunch.com/2016/08/16/rubrik-snares-61-million-series-c-led-by-khosla-ventures/|title=Rubrik snares $61 million Series C led by Khosla Ventures|last=Miller|first=Ron|website=TechCrunch|access-date=2017-01-23}}&lt;/ref&gt;

{{Infobox company
| name = Rubrik
| type = Private
| logo = Rubrik logo.png
| industry = Cloud Data Management
| founder =  Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap
| hq_location_city = Palo Alto, CA
| hq_location_country = United States
| products = Rubrik Cloud Data Management platform
| website = {{URL|www.rubrik.com/}}
}}

== History ==
Rubrik was founded in 2014 by Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap.&lt;ref&gt;{{Cite news|url=http://www.forbes.com/sites/benkepes/2015/03/24/with-an-a-grade-founding-team-and-a-grade-investors-rubrik-launches/#219bf1ea2b6e|title=With An A-Grade Founding Team And A-Grade Investors, Rubrik Launches|last=Kepes|first=Ben|newspaper=Forbes|access-date=2017-01-23}}&lt;/ref&gt; It raised $10 million in March 2015,&lt;ref&gt;{{Cite news|url=http://blogs.wsj.com/venturecapital/2015/03/24/rubrik-emerges-with-10-million-for-software-defined-backup/|title=Rubrik Emerges With $10 Million for Software-Defined Backup|last=Gage|first=Deborah|newspaper=WSJ|language=en-US|access-date=2017-01-23}}&lt;/ref&gt; followed by a $41 million round that same May. In August 2016, Rubrik raised an additional $61 million in funding led by [[Khosla Ventures]].&lt;ref&gt;{{Cite news|url=http://www.businessinsider.com/rubrik-raises-61-million-khosla-ventures-2016-8|title=This investor turned founder scoffs at funding slowdown: 'real businesses' can still get money|newspaper=Business Insider|language=en|access-date=2017-01-23}}&lt;/ref&gt; Sinha, who is Founding Investor at [[Nutanix]] and a partner at [[Lightspeed Venture Partners]], currently serves as CEO.

In 2015, Rubrik launched with its r300 series appliances, called "Briks", in two configurations for VMware environments. The appliance includes a console from which users can manage and monitor their data.&lt;ref&gt;{{Cite web|url=http://cormachogan.com/2015/05/26/a-closer-look-at-rubrik/|title=A closer look at Rubrik|date=2015-05-26|website=CormacHogan.com|access-date=2017-01-23}}&lt;/ref&gt; In April 2016, it launched its r528 Brik, which is [[FIPS 140-2]] Level 2 certified.&lt;ref&gt;{{Cite web|url=http://www.thepaypers.com/default/rubrik-r528-provides-storage-encryption-and-security/764067-0|title=Rubrik r528 provides storage encryption and security|website=www.thepaypers.com|access-date=2017-01-23}}&lt;/ref&gt;

In April 2016, the company released their Cloud Data Management platform with Rubrik Firefly, which extended capabilities to physical [[SQL]] and [[Linux]] environments, and with Rubrik Edge, a software appliance that extended protection to remote and branch offices. The release also included a software upgrade to utilize [[Erasure code|erasure coding]].&lt;ref&gt;{{Cite web|url=http://www.theregister.co.uk/2016/08/16/rubriks_extra_funding_as_firefly_data_management_flies_out_of_the_coop/|title=Rubrik's extra funding as Firefly extended data management flies out of the coop|last=18:13|first=16 Aug 2016 at|last2=tweet_btn()|first2=Chris Mellor|access-date=2017-01-23}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://blog.mwpreston.net/2016/08/16/rubrik-firefly-released/|title=Rubrik Firefly - Now with physical, edge, and moar cloud!|date=2016-08-16|website=mwpreston.net|access-date=2017-01-23}}&lt;/ref&gt;

== Awards ==
Rubrik was a winner of the 2015 Virtualization Review Editor's Choice Awards.&lt;ref&gt;{{Cite web|url=https://virtualizationreview.com/articles/2015/12/01/editors-choice-awards.aspx|title=The 2015 Virtualization Review Editor's Choice Awards -|last=Ward|first=By Keith|last2=01/04/2016|website=Virtualization Review|access-date=2017-01-23}}&lt;/ref&gt; In 2016, Rubrik was named a [[Gartner]] Cool Vendor in Storage Technologies.&lt;ref&gt;{{Cite web|url=https://www.gartner.com/doc/3290518/cool-vendors-storage-technologies-|title=Cool Vendors in Storage Technologies, 2016|website=www.gartner.com|access-date=2017-01-23}}&lt;/ref&gt; Rubrik won &#8220;Best security or data protection project&#8221; for Best of VMworld Europe User Awards 2016.&lt;ref&gt;{{Cite news|url=http://www.techtarget.com/press-release/techtargets-searchservervirtualization-com-announces-best-vmworld-2016-award-winners/|title=TechTarget&#8217;s SearchServerVirtualization.com Announces &#8220;Best of VMworld&#8221; 2016 Award Winners - TechTarget|newspaper=TechTarget|language=en-US|access-date=2017-01-23}}&lt;/ref&gt; In October 2016, Rubrik was selected as one of 25 &#8220;Next Billion-Dollar Startups&#8221; by [[Forbes]].&lt;ref&gt;{{Cite news|url=http://www.forbes.com/sites/amyfeldman/2016/10/19/next-billion-dollar-startups-2016/#1221eca0554e|title=Next Billion-Dollar Startups 2016|last=Feldman|first=Amy|newspaper=Forbes|access-date=2017-01-23}}&lt;/ref&gt;

== References ==
{{Reflist}}

[[Category:Data management]]
[[Category:Cloud computing providers]]
[[Category:Companies based in California]]</text>
      <sha1>9ufu6w39csishpj99q1y1c7w1fl1o6f</sha1>
    </revision>
  </page>
  <page>
    <title>Document retrieval</title>
    <ns>0</ns>
    <id>731640</id>
    <revision>
      <id>722724171</id>
      <parentid>701537816</parentid>
      <timestamp>2016-05-29T20:18:34Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5819" xml:space="preserve">'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, '''text retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.

==Description==
Document retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.

A document retrieval system has two main tasks:
# Find relevant documents to user queries
# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].

Internet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.

==Variations==
There are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.

===Form based===
Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.

===Content based===
The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.

A ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.

==Example: PubMed==
The [[PubMed]]&lt;ref&gt;{{cite journal |vauthors=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319&#8211;23 |year=2001 |pmid=11825203 |pmc=2243528 }}
&lt;/ref&gt; form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.&lt;ref&gt;{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}&lt;/ref&gt;

== See also ==

* [[Compound term processing]]
* [[Document classification]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Latent semantic indexing]]
* [[Search engine]]

== References ==

&lt;references/&gt;

==Further reading==
* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems|volume=2|issue=4|year=1984|pages=267&#8211;288|doi=10.1145/2275.357411}}
* {{cite journal|author1=Justin Zobel |author2=Alistair Moffat |author3=Kotagiri Ramamohanarao |title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems|volume=23|issue=4|year=1998|pages= 453&#8211;490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}
* {{cite journal|author1=Ben Carterette |author2=Fazli Can |title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613&#8211;633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}

== External links ==
* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College

[[Category:Information retrieval genres]]
[[Category:Electronic documents]]
[[Category:Substring indices]]
[[Category:Search engine software]]

[[zh:&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;]]</text>
      <sha1>iu7olurw00fk6emu4f3rcvs4xa7ym58</sha1>
    </revision>
  </page>
  <page>
    <title>Transaction document</title>
    <ns>0</ns>
    <id>17788286</id>
    <revision>
      <id>639807736</id>
      <parentid>586807892</parentid>
      <timestamp>2014-12-27T13:30:13Z</timestamp>
      <contributor>
        <username>Eumolpo</username>
        <id>7953109</id>
      </contributor>
      <minor />
      <comment>orthographic</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1257" xml:space="preserve">'''Transaction documents'''  refers to legally relevant [[documents]] that are either printed, inserted and mailed, or electronically presented.&lt;ref&gt;[http://www.outputlinks.com/html/news/acadami_xplor_best_practices_progam_canada_043008.shtml Transaction documents]&lt;blockquote&gt;"...The course focuses on the concepts, technologies, and best practices associated with automated transaction document production."&lt;/blockquote&gt;&lt;/ref&gt; They consist of a mixture of fixed and variable data. 

These [[documents]] are usually created by organizations through their financial computing system and then delivered to other parties (such as clients) through the [[post office]] or through an [[electronic billing]] system. The printed transaction documents, once delivered to the [[post office]], conform to the [[mail box rule]]. 

Common examples of transaction documents are:
* bills
* [[bank statements]] (and credit card, financial services, etc.)
* insurance policies
* notices
* other legally relevant correspondence, etc.

[[Xplor international]] is a technical association that focuses on the best practices and technologies associated with these documents.

==References==
{{Reflist}}

[[Category:Electronic documents]]
[[Category:Contract law]]


{{law-stub}}</text>
      <sha1>sklsp5tlg6vr1kll4elchztrhex4wm6</sha1>
    </revision>
  </page>
  <page>
    <title>SAFE-BioPharma Association</title>
    <ns>0</ns>
    <id>2321249</id>
    <revision>
      <id>715659724</id>
      <parentid>715574250</parentid>
      <timestamp>2016-04-17T06:03:33Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>Removed invisible unicode characters + other fixes, removed: &#8206; using [[Project:AWB|AWB]] (12002)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5445" xml:space="preserve">{{primary sources|date=February 2010}}
'''SAFE-BioPharma Association''' is the non-profit association that created and manages the SAFE-BioPharma [[digital identity]] and [[digital signature]] standard for the global [[pharmaceutical]], [[biotech]] and [[healthcare]] industries. SAFE stands for "Signatures &amp; Authentication For Everyone" (but originally stood for "Secure Access For Everyone"&lt;ref&gt;{{cite web|title=CSI page on SAFE describing original goal and name|url=http://www.csirochester.com/safe.htm}}&lt;/ref&gt;).  It was originally created as an initiative of the [[Pharmaceutical Research and Manufacturers of America]] (PhRMA) association to encourage the use of a common digital identity and digital signature standard for the pharmaceutical industry, but is now an independent non-profit association offering such standards services to the government and the entire healthcare industry.

The SAFE-BioPharma industry standard is used to establish and manage digital identities and to issue and apply digital signatures. It mitigates legal, regulatory and business risk associated with business-to-business and business-to-regulator electronic transactions. It also facilitates interoperability by providing a secure, enforceable, and regulatory-compliant way to verify identities of parties involved in electronic transactions.

SAFE-BioPharma&#8217;s vision is to be a catalyst in transforming the biopharmaceutical and healthcare communities to a fully electronic business environment by 2012.

== Certificate authority ==
The SAFE-BioPharma digital identity and signature standard operates one of the nation&#8217;s leading [[public key infrastructure]] (PKI) certificate authority bridges. These PKI certificate bridges establish an infrastructure for the trusted exchange of confidential information and the reliable authentication of identities over the Internet. By providing a highly secure way to validate, trust, and manage identities of unknown participants in an Internet transaction, the SAFE-BioPharma Bridge [[certificate authority|Certificate Authority]] (SBCA) is essential to helping achieve the speed, efficiency and cost-savings inherent in use of the Internet for business transactions. This technology also improves interoperability across many different systems.

== Cross-certification ==
The SAFE-BioPharma Bridge Certificate Authority is cross-certified with the [http://www.cio.gov/fpkia/ Federal Bridge Public Key Infrastructure Architecture] (FPKIA), facilitating the ability of SAFE-BioPharma member companies that meet certain security, technical and operational criteria to leverage the identity credentials of any and all bridge members in the exchange of sensitive and confidential information. In essence, it allows officials in [[Health and Human Services]], the [[Food and Drug Administration]], [[United States Department of Defense|Department of Defense]], and other government agencies to trust the origins of electronic documents received from corporate managers, physicians, clinical researchers, etc. who are credentialed to digitally sign documents with SAFE-BioPharma digital signatures. Additionally, the identities are trusted for authentication access control for sites requiring strong authentication.

== Regulatory acceptance ==
SAFE-BioPharma has worked closely with the US Food and Drug Administration (FDA), the [[European Medicines Agency]] (EMA) and other global healthcare and regulatory agencies to ensure the digital signatures generated using SAFE-BioPharma certificates meet regulatory requirements and are accepted by these agencies when used on documents that are part of electronic submissions. It thus allows voluminous paper documents used for regulatory compliance to be digitally signed and submitted in electronic form. This improves accuracy, reduces costs, enables electronic search and retrieval and saves energy and natural resources.

== Member organizations ==
SAFE-BioPharma Association members include [[Abbott Laboratories]] (NYSE: ABT), [[Amgen]] (NASDAQ: AMGN), [[AstraZeneca]] (NYSE: AZN), [[Bristol-Myers Squibb]] (NYSE:BMY), [[GlaxoSmithKline]] (NYSE: GSK), [[Johnson &amp; Johnson]] (NYSE: JNJ), [[Eli Lilly and Company|Eli Lilly]] (NYSE: LLY), [[Merck &amp; Co.|Merck]] (NYSE:MRK), [[National Notary Association]], [[Pfizer]] (NYSE: PFE), Premier Inc.,  and [[Sanofi-Aventis]] (NYSE:SNY).

SAFE-BioPharma is a trademark of the SAFE-BioPharma Association. Any use of this trademark requires approval from the SAFE-BioPharma Association.

==See also==
* [[Electronic lab notebook]]
* [[Public key infrastructure|PKI]]
* [[Title 21 CFR Part 11]]
* [[Digital signature]]
* [[Electronic Signatures in Global and National Commerce Act]] (ESIGN, USA)
* [[European Medicines Agency]] (EMEA)
* [[Food and Drug Administration]] (FDA)
* [[Pharmaceutical company]]
* [[Japan Pharmaceutical Manufacturers Association]] (JPMA)
* [[Digital signature]]
* [[Data management]]

==External links==
* [http://www.safe-biopharma.org SAFE-BioPharma Organization Website]

==References==

{{Reflist}}

{{DEFAULTSORT:Safe-Biopharma Association}}
[[Category:Public-key cryptography]]
[[Category:Cryptography companies]]
[[Category:Electronic documents]]
[[Category:Non-profit organizations based in New Jersey]]
[[Category:Pharmaceutical industry trade groups]]
[[Category:International medical and health organizations]]
[[Category:Medical and health organizations based in the United States]]</text>
      <sha1>2r1ccl8ppeloj0t0xw0zya8ochoo0gu</sha1>
    </revision>
  </page>
  <page>
    <title>Digital object identifier</title>
    <ns>0</ns>
    <id>422994</id>
    <revision>
      <id>763040353</id>
      <parentid>763023012</parentid>
      <timestamp>2017-02-01T01:00:32Z</timestamp>
      <contributor>
        <ip>73.253.110.94</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27416" xml:space="preserve">{{Selfref|For the use of digital object identifiers on Wikipedia, see [[Wikipedia:Digital Object Identifier]].}}
{{Use dmy dates|date=February 2011}}
{{Infobox identifier
| name          = Digital object identifier
| image         = DOI logo.svg
| image_size    = 130px
| image_caption = 
| image_alt     = 
| image_border  = no
| full_name     = 
| acronym       = DOI
| number        = 
| start_date    = {{Start date|2000}}
| organisation  = International DOI Foundation
| digits        = 
| check_digit   = 
| example       = 
| website       = {{URL|doi.org}}
}}
In computing, a '''Digital Object Identifier''' or '''DOI''' is a [[persistent identifier]] or [[handle (computing)|handle]] used to uniquely identify objects. An implementation of the [[Handle System]]&lt;ref&gt;{{cite web|url= http://handle.net/|title=The Handle System}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.doi.org/factsheets.html |title=Factsheets}}&lt;/ref&gt; and standardized by the [[ISO]], DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they can be used to identify other objects, such as commercial videos.

DOI means "digital identifier of an object" rather than "identifier of a digital object".&lt;ref name = "iso"&gt;{{cite web | url = https://www.iso.org/obp/ui/#iso:std:iso:26324:ed-1:v1:en | title = ISO 26324:2012(en), Information and documentation &#8212; Digital object identifier system | publisher = [[ISO]] | date = | accessdate = 2016-04-20 | quote = DOI is an acronym for 'digital object identifier', meaning a 'digital identifier of an object' rather than an 'identifier of a digital object'.}} "Introduction", paragraph 2.&lt;/ref&gt; Thus ''DOI'' stands for "digital object-identifier" rather than "digital-object identifier".

[[Metadata]] about the object is stored in association with the DOI name. It may include a location, such as a [[URL]], indicating where the object can be found. The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.&lt;ref&gt;{{cite book|author= Witten, Ian H.|author2= David Bainbridge|author3= David M. Nichols|last-author-amp= yes |date= 2010|title= How to Build a Digital Library|edition= 2nd|location= Amsterdam; Boston|publisher= Morgan Kaufmann|pages= 352&#8211;253|isbn= 978-0-12-374857-7}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|first1= Marc|last1= Langston|first2= James|last2= Tyler|title= Linking to journal articles in an online teaching environment: The persistent link, DOI, and OpenURL|journal= The Internet and Higher Education|volume= 7|issue= 1|date= 2004|pages= 51&#8211;58|doi= 10.1016/j.iheduc.2003.11.004}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal |url=http://www.bloomberg.com/bw/stories/2001-07-22/online-extra-how-the-digital-object-identifier-works |title= How the 'Digital Object Identifier' works |date= 23 July 2001 |work= BusinessWeek |accessdate= 20 April 2010 |quote= Assuming the publishers do their job of maintaining the databases, these centralized references, unlike current Web links, should never become outdated or broken. |publisher= [[BusinessWeek]]}}&lt;/ref&gt;

A DOI name differs from standard identifier registries such as the [[ISBN]] and [[ISRC]]. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable.

The DOI system began in 2000 and is managed by the International DOI Foundation.&lt;ref&gt;{{citation|last= Paskin|first= Norman|chapter= Digital Object Identifier (DOI&#174;) System|title= Encyclopedia of Library and Information Sciences|date= 2010|publisher= Taylor and Francis|pages= 1586&#8211;1592|edition= 3rd}}&lt;/ref&gt; Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.&lt;ref name="dd"&gt;{{Cite journal|title= Digital Object Identifiers: Promise and problems for scholarly publishing|first1= Lloyd A.|last1= Davidson|first2= Kimberly|last2= Douglas|date= December 1998|journal= Journal of Electronic Publishing|volume= 4|issue= 2|doi= 10.3998/3336451.0004.203}}&lt;/ref&gt; The DOI system is implemented through a federation of registration agencies coordinated by the International DOI Foundation,&lt;ref&gt;{{cite web|url= https://doi.org/ |title= Welcome to the DOI System |publisher= Doi.org |date= 28 June 2010 |accessdate= 7 August 2010}}&lt;/ref&gt; which developed and controls the system. The DOI system has been developed and implemented in a range of publishing applications since 2000; by late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations.&lt;ref&gt;{{Cite web|url= https://doi.org/news/DOINewsApr11.html#1 |title= DOI&#174; News, April 2011: 1. DOI System exceeds 50 million assigned identifiers |publisher= Doi.org |date= 20 April 2011 |accessdate= 3 July 2011}}&lt;/ref&gt; By April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.

==Nomenclature==
A DOI name takes the form of a [[character string]] divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the name, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal [[Unicode]] characters are allowed in these strings, which are interpreted in a [[case-insensitive]] manner. The prefix usually takes the form &lt;code&gt;10.NNNN&lt;/code&gt;, where &lt;code&gt;NNNN&lt;/code&gt; is a series of at least 4 numbers greater than or equal to &lt;code&gt;1000&lt;/code&gt;, whose limit depends only on the total number of registrants.&lt;ref name="CrossRefDOI"&gt;{{cite web |url=http://www.crossref.org/01company/15doi_info.html |access-date=10 June 2016 |title=doi info &amp; guidelines |website=CrossRef.org |publisher=Publishers International Linking Association, Inc. |date=2013 |quote=All DOI prefixes begin with "10" to distinguish the DOI from other implementations of the Handle System followed by a four-digit number or string (the prefix can be longer if necessary).}}&lt;/ref&gt;&lt;ref name="DOIKeyFacts"&gt;{{cite web |url=https://doi.org/factsheets/DOIKeyFacts.html |access-date=10 June 2016 |title=Factsheet&#8212;Key Facts on Digital Object Identifier System |website=doi.org |publisher=International DOI Foundation |date=June 6, 2016 |quote=Over 18,000 DOI name prefixes within the DOI System}}&lt;/ref&gt; The prefix may be further subdivided with periods, like &lt;code&gt;10.NNNN.N&lt;/code&gt;.&lt;ref name="2.2.2"&gt;{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.2.2 |access-date=10 June 2016 |title=DOI Handbook&#8212;2 Numbering |website=doi.org |publisher=International DOI Foundation |date=February 1, 2016 |quote=The registrant code may be further divided into sub-elements for administrative convenience if desired. Each sub-element of the registrant code shall be preceded by a full stop.}}&lt;/ref&gt;

For example, in the DOI name &lt;code&gt;10.1000/182&lt;/code&gt;, the prefix is &lt;code&gt;10.1000&lt;/code&gt; and the suffix is &lt;code&gt;182&lt;/code&gt;. The "10." part of the prefix identifies the DOI registry,{{efn-ua|Other registries are identified by other strings at the start of the prefix. Handle names that begin with "100." are also in use, as for example in the following citation: {{cite journal|hdl=100.2/ADA013939 |url=http://handle.dtic.mil/100.2/ADA013939 |title=Development of a Transmission Error Model and an Error Control Model l |journal=&lt;!-- --&gt; |volume=&lt;!-- --&gt; | date=May 1975 |last1=Hammond |first1=Joseph L., Jr. |last2=Brown |first2=James E. |last3=Liu |first3=Shyan-Shiang S. |bibcode=1975STIN...7615344H|publisher=Rome Air Development Center|series=Technical Report RADC-TR-75-138}}}} and the characters &lt;code&gt;1000&lt;/code&gt; in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. &lt;code&gt;182&lt;/code&gt; is the suffix, or item ID, identifying a single object (in this case, the latest version of the ''DOI Handbook'').

DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, [[performance]]s, and abstract works&lt;ref name=doifaq2&gt;{{Cite journal |url=https://doi.org/faq.html#1 |title=Frequently asked questions about the DOI system: 2. What can be identified by a DOI name? | accessdate = 23 April 2010 | date = 17 February 2010|origyear=update of earlier version |publisher=International DOI Foundation}}
&lt;/ref&gt; such as licenses, parties to a transaction, etc.

The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the [[indecs Content Model]].

===Display===
The official ''DOI Handbook'' explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".&lt;ref name="C4WDefault-2811140"&gt;{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.6.1 |title=DOI Handbook &#8211; Numbering |date=13 February 2014 |accessdate=30 June 2014 |work=doi.org |author1=&lt;!--Staff writer(s); no by-line.--&gt; |archiveurl=https://web.archive.org/web/20140630181440/http://www.doi.org/doi_handbook/2_Numbering.html |archivedate=30 June 2014 |deadurl=no |at=Section 2.6.1 Screen and print presentation}}&lt;/ref&gt; Contrary to the ''DOI Handbook'', [[CrossRef]], a major DOI registration agency, recommends displaying a URL (for example, &lt;code&gt;&lt;nowiki&gt;https://doi.org/10.1000/182&lt;/nowiki&gt;&lt;/code&gt;) instead of the officially specified format (for example, &lt;code&gt;[https://doi.org/10.1000/182 doi:10.1000/182]&lt;/code&gt;)&lt;ref&gt;{{Cite web| title=DOI Display Guidelines|url=http://www.crossref.org/02publishers/doi_display_guidelines.html}}&lt;/ref&gt;&lt;ref&gt;{{Cite web| title=New Crossref DOI display guidelines are on the way|url=http://blog.crossref.org/2016/09/new-crossref-doi-display-guidelines.html}}&lt;/ref&gt; This URL provides the location of an [[HTTP proxy]] server which will redirect web accesses to the correct online location of the linked item.&lt;ref name="dd"/&gt;&lt;ref&gt;{{Cite journal | first=Andy |last=Powell |title=Resolving DOI Based URNs Using Squid: An Experimental System at UKOLN |journal = D-Lib Magazine|date=June 1998|url=http://www.dlib.org/dlib/june98/06powell.html| issn=1082-9873}}&lt;/ref&gt; This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL &#8211; the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.

==Applications==
Major applications of the DOI system currently include:
* [[Scientific literature|scholarly materials]] (journal articles, books, ebooks, etc.) through [[CrossRef]], a consortium of around 3,000 publishers;
* research datasets through [[DataCite]], a consortium of leading research libraries, technical information providers, and scientific data centers;
* [[European Union]] official publications through the [[Publications Office (European Union)|EU publications office]];
* Permanent global identifiers for commercial video content through the Entertainment ID Registry, commonly known as [[EIDR]].

In the [[Organisation for Economic Co-operation and Development]]'s publication service [[OECD iLibrary]], each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.&lt;ref&gt;{{cite journal|doi=10.1787/603233448430 |title=We Need Publishing Standards for Datasets and Data Tables |date=2009|journal=Research Information |last=Green|first=T.}}&lt;/ref&gt;

A multilingual European DOI registration agency activity, [http://www.mEDRA.org ''m''EDRA], Traditional Chinese content thru [http://doi.airiti.com/ Airiti Inc.] and a Chinese registration agency, [http://www.wanfangdata.com/ Wanfang Data], are active in non-English language markets. Expansion to other sectors is planned by the International DOI Foundation.{{Citation needed|date=May 2010}}

==Features and benefits==
The DOI system was designed to provide a form of [[Persistent identifier|persistent identification]], in which each DOI name permanently and unambiguously identifies the object to which it is associated. And, it associates [[metadata]] with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the [[Handle System]] and the [[indecs Content Model]] with a social infrastructure.

The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the [[Uniform Resource Identifier|URI]] specification. The DOI name resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on [[open architecture]]s, incorporates [[Computational trust|trust mechanisms]], and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.&lt;ref&gt;{{cite web|url=http://arstechnica.com/science/2010/03/dois-and-their-discontents-1/|title=DOIs and their discontents|last=Timmer|first=John|date=6 March 2010|work=[[Ars Technica]]|accessdate=5 March 2013}}&lt;/ref&gt; DOI name resolution may be used with [[OpenURL]] to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.&lt;ref&gt;{{Cite journal|first1=Susanne|last1=DeRisi|first2=Rebecca|last2=Kennison|first3=Nick|last3=Twyman|title=Editorial: The what and whys of DOIs|journal=[[PLoS Biology]]|volume=1|issue=2|page=e57|date=2003|doi=10.1371/journal.pbio.0000057|pmid=14624257|pmc=261894}} {{open access}}&lt;/ref&gt; However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.&lt;ref&gt;{{Cite book|contribution=Open access to scientific and technical information: the state of the art|first=Jack|last=Franklin|title=Open access to scientific and technical information: state of the art and future trends|editor1-first=Herbert|editor1-last=Gr&#252;ttemeier|editor2-first=Barry|editor2-last=Mahon|publisher=IOS Press|date=2003|page=74|url=https://books.google.com/?id=2X3gW1lUvN4C&amp;pg=PA74#v=onepage&amp;q|isbn=978-1-58603-377-4}}&lt;/ref&gt;

The [[indecs Content Model]] is used within the DOI system to associate metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.

The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as [[GS1]] and [[ISBN]].

==Comparison with other identifier schemes==
A DOI name differs from commonly used Internet pointers to material, such as the [[Uniform Resource Locator]] (URL), in that it identifies an object itself as a [[First class (computing)|first-class entity]], rather than the specific place where the object is located at a certain time. It implements the [[Uniform Resource Identifier]] ([[Uniform Resource Name]]) concept and adds to it a data model and social infrastructure.&lt;ref&gt;{{cite web|url=https://doi.org/factsheets/DOIIdentifierSpecs.html |title=DOI System and Internet Identifier Specifications |publisher=Doi.org |date=18 May 2010 |accessdate=7 August 2010}}&lt;/ref&gt;

A DOI name also differs from standard identifier registries such as the [[International Standard Book Number|ISBN]], [[International Standard Recording Code|ISRC]], etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.&lt;ref&gt;{{cite web|url=https://doi.org/factsheets/DOIIdentifiers.html |title=DOI System and standard identifier registries |publisher=Doi.org |accessdate=7 August 2010}}&lt;/ref&gt;

The DOI system offers persistent, [[Semantic interoperability|semantically-interoperable]] resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include [[Persistent Uniform Resource Locator]] (PURL), URLs, [[Globally Unique Identifier]]s (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., [[Archival Resource Key|ARK]]).

A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.

==Resolution==
DOI name resolution is provided through the [[Handle System]], developed by [[Corporation for National Research Initiatives]], and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its &lt;code&gt;&lt;type&gt;&lt;/code&gt; field, which defines the syntax and semantics of its data.

To resolve a DOI name, it may be input to a DOI resolver (e.g. [https://doi.org/ doi.org]) or may be represented as an HTTP string by preceding the DOI name by the string &lt;code&gt;&lt;nowiki&gt;https://doi.org/&lt;/nowiki&gt;&lt;/code&gt; (preferred)&lt;ref&gt;{{cite web|author1=International DOI Foundation|title=Resolution|url=https://doi.org/doi_handbook/3_Resolution.html#3.7.3|website=DOI Handbook|accessdate=19 March 2015|date=2014-08-07}}&lt;/ref&gt; or &lt;code&gt;&lt;nowiki&gt;https://dx.doi.org/&lt;/nowiki&gt;&lt;/code&gt;. For example, the DOI name &lt;code&gt;10.1000/182&lt;/code&gt; can be resolved at the address "&lt;nowiki&gt;https://doi.org/10.1000/182&lt;/nowiki&gt;". Web pages or other hypertext documents can include hypertext links in this form. Some browsers allow the direct resolution of a DOI (or other handles) with an add-on, e.g., [http://www.handle.net/hs-tools/extensions/firefox_hdlclient.html CNRI Handle Extension for Firefox]. The CNRI Handle Extension for Firefox enables the browser to access handle or DOI URIs like hdl:4263537/4000 or doi:10.1000/1 using the native Handle System protocol. It will even replace references to web-to-handle proxy servers with native resolution.

Alternative DOI resolvers include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/ and http://doai.io. The last is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.&lt;ref&gt;{{cite web|url=http://doai.io/|title=DOAI|publisher=CAPSH (Committee for the Accessibility of Publications in Sciences and Humanities)|accessdate=6 August 2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite web| last = Schonfeld| first = Roger C.| title = Co-opting 'Official' Channels through Infrastructures for Openness |work = The Scholarly Kitchen| accessdate = 2016-10-17| date = 2016-03-03| url = https://scholarlykitchen.sspnet.org/2016/03/03/coopting-official-channels/}}&lt;/ref&gt;

==Organizational structure==
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.&lt;ref&gt;{{cite book|url=https://doi.org/doi_handbook/7_IDF.html#7.5 |title=DOI Handbook |chapter=Chapter 7: The International DOI Foundation |publisher=Doi.org |accessdate=8 July 2015}}&lt;/ref&gt; It safeguards all [[intellectual property|intellectual property rights]] relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.

The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.

Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation.

Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a [[not-for-profit]] cost recovery basis.

==Standardization==
The DOI system is an international standard developed by the [[International Organization for Standardization]] in its technical committee on identification and description, TC46/SC9.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/pressrelease.htm?refid=Ref1561 |title=Digital object identifier (DOI) becomes an ISO standard |publisher=iso.org |date=10 May 2012 |accessdate=10 May 2012}}&lt;/ref&gt; The Draft International Standard ISO/DIS 26324, Information and documentation &#8211; Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,&lt;ref&gt;{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=about_the_doi.html DOI Standards and Specifications |publisher=Doi.org |date=28 June 2010 |accessdate=7 August 2010}}&lt;/ref&gt; which was approved by 100% of those voting in a ballot closing on 15 November 2010.&lt;ref&gt;{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=Overviews &amp; Standards &#8211; Standards and Specifications: 1. ISO TC46/SC9 Standards |publisher=Doi.org |date=18 November 2010 |accessdate=3 July 2011}}&lt;/ref&gt; The final standard was published on 23 April 2012.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=43506 |title=ISO 26324:2012 |publisher=iso.org |date=23 April 2012 |accessdate=10 May 2012}}&lt;/ref&gt;

DOI is a registered URI under the [[info URI scheme]] specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.&lt;ref&gt;{{cite web|url=http://info-uri.info/registry/docs/misc/faq.html#which_namespaces |title=About "info" URIs &#8211; Frequently Asked Questions |publisher=Info-uri.info |accessdate=7 August 2010}}&lt;/ref&gt;

The DOI syntax is a [[NISO]] standard, first standardised in 2000, ANSI/NISO Z39.84{{hyphen}}2005 Syntax for the Digital Object Identifier.&lt;ref&gt;{{cite web|url=http://www.techstreet.com/standards/niso-z39-84-2005-r2010?product_id=1262088 |title=ANSI/NISO Z39.84{{hyphen}}2000 Syntax for the Digital Object Identifier |publisher=Techstreet.com |accessdate=7 August 2010}}&lt;/ref&gt;

==See also==
{{columns-list|3|
* [[Bibcode]]
* [[Digital identity]]
* [[Metadata standards]]
* [[Object identifier]]
* [[ORCID]]
* [[PubMed#PubMed identifier|PMID]]
* [[Publisher Item Identifier]] (PII)
* [[Permalink]]
* [[Scientific literature]]
* [[Universally Unique Identifier]] (UUID)
}}

==Notes==
{{notelist-ua}}

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Wikidata property|P356}}
* {{official website|https://doi.org/}}
* [http://shortdoi.org Short DOI] &#8211; DOI Foundation service for converting long DOIs to shorter equivalents
* [https://doi.org/factsheets/DOIIdentifierSpecs.html Factsheet: DOI System and Internet Identifier Specifications]
* [http://search.crossref.org/ CrossRef DOI lookup]

{{Audiovisual works|state=uncollapsed}}
{{ISO standards}}
{{Authority control}}

{{DEFAULTSORT:Digital Object Identifier}}
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]</text>
      <sha1>0z6p5zss5o63hfzd6lsol85zyx3a2r2</sha1>
    </revision>
  </page>
  <page>
    <title>Aperture card</title>
    <ns>0</ns>
    <id>8403499</id>
    <revision>
      <id>749306895</id>
      <parentid>744606694</parentid>
      <timestamp>2016-11-13T17:40:05Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6058" xml:space="preserve">[[Image:Aperture card.JPG|400px|right]]
An '''aperture card''' is a type of '''[[punched card]]''' with a cut-out window into which a chip of '''[[microform|microfilm]]''' is mounted.  Such a card is used for [[archive|archiving]] or for making multiple inexpensive copies of a document for ease of distribution.  The card is typically punched with machine-readable [[metadata]] associated with the microfilm image, and printed across the top of the card for visual identification.  The microfilm chip is most commonly 35mm in height, and contains an [[optics|optically reduced]] image, usually of some type of reference document, such as an [[engineering drawing]], that is the focus of the archiving process.  Aperture cards have several advantages and disadvantages when compared to digital systems.  Machinery exists to automatically store, retrieve, sort, duplicate, create, and digitize cards with a high level of automation.  While many aperture cards still play an important role in archiving, their role is gradually being replaced by digital systems.

== Usage ==
Aperture cards are used for engineering drawings from all engineering disciplines.  The [[U.S. Department of Defense]] once made extensive use of aperture cards, and some are still in use, but most data is now digital.&lt;ref&gt;[https://web.archive.org/web/20060530111716id_/http://federalvoice.dscc.dla.mil/federalvoice/030924/tech.html Federal use of aperture cards (Archived Copy)]&lt;/ref&gt;

Information about the drawing, for example the drawing number, could be both punched and printed on the remainder of the card.  With the proper machinery, this allows for automated handling.  In the absence of such machinery, the cards can still be read by a human with a lens and a light source.

=== Advantages ===
Aperture cards have, for archival purposes, some advantages over digital systems.  They have a 500-year lifetime, they are human readable, and there is no expense or risk in converting from one digital format to the next when computer systems become obsolete.&lt;ref&gt;{{cite journal|first=Ed |last=LoTurco |title=The Engineering Aperture Card: Still Active, Still Vital |publisher=EDM Consultants |date=January 2004 |url=http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |accessdate=October 10, 2007 |archiveurl=https://web.archive.org/web/20071128162738/http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |archivedate=November 28, 2007 |deadurl=no |df= }}&lt;/ref&gt;

=== Disadvantages ===
{{unreferenced section|date=February 2015}}
Most of the disadvantages are related to the well established differences in analog and digital technology. In particular, searching for given strings within content is considerably slower.  Handling physical cards requires proprietary machinery and processing optical film takes significant time.

The very nature of microfilm cameras and the high contrast properties of microfilm stock itself also impose limits on the amount of detail that can be resolved particularly at the higher reduction ratios (36x or greater) needed to film larger drawings. Faded drawings or those of low or uneven contrast do not reproduce well and significant detail or annotations may be lost.

In common with other forms of microfilm mis-filing cards after use, particularly in large archives, results in the card being for all intents and purposes lost forever unless it's later found by accident.

Aperture cards created from 35mm roll film mounted on to blank cards have to be treated with great care. Bending the card can cause the film to detach and excessive pressure to a stack of cards can cause the mounting glue to ooze creating clumps of cards which will feed through duplicators and other machinery either poorly or not at all. Feeding a de-laminated card through machinery not only risks destroying the image it also risks jamming or damaging the machinery.

== Machinery ==
A set of cards could be rapidly sorted by drawing number or other punched data using a [[IBM 80 series Card Sorters|card sorter]].  Machines are now available that [[Image scanner|scan]] aperture cards and produce a digital version.&lt;ref&gt;For example, this aperture card scanner from  [http://www.oceusa.com/main/product_detail.jsp?FOLDER%3C%3Efolder_id=1408474395186237&amp;PRODUCT%3C%3Eprd_id=845524441761057 Oce']&lt;/ref&gt;  Aperture card plotters are machines that use a laser to create the image on the film.&lt;ref&gt;For example, this aperture card plotter from [http://www.wwl.co.uk/apertureplotters.htm Wicks &amp; Wilson] {{webarchive |url=https://web.archive.org/web/20060627060408/http://www.wwl.co.uk/apertureplotters.htm |date=June 27, 2006 }}&lt;/ref&gt;

== Conversion ==
Aperture cards can be converted to digital documents using scanning equipment and software. The scan software we use allows for significant image cleanup and enhancement. Often, the digital image produced is better than the visual quality available prescan. A variety of output image types can be generated, most notably, Group 4 TIFF and PDF.&lt;ref&gt;{{cite web|last1=Bryant|first1=Joe|title=Aperture Card Scanning|url=http://www.microcomseattle.com/solutions/document-scanning/aperture-card/|website=Micro Com Seattle|accessdate=17 March 2015}}&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://stinet.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0232960 1959 Defense Technical Information Center report] on the technology and its use for submitting engineering plans to the military.
* [http://www.wipo.int/export/sites/www/scit/en/standards/pdf/03-07-a.pdf Detailed description of a particular format]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of Aperture cards from [[WIPO]].
* [https://web.archive.org/web/20090327011959/http://www.green-sheet.net/tutorial2.6.htm Detailed information regarding duplicating microforms and aperture cards] (select and highlight to read black on black text)

[[Category:Archival science]]
[[Category:History of computing]]
[[Category:Infographics]]
[[Category:Technical drawing]]
[[Category:Electronic documents]]</text>
      <sha1>a92bnuc4unheomcofdyhq6hibigpfvo</sha1>
    </revision>
  </page>
  <page>
    <title>Quickstart guide</title>
    <ns>0</ns>
    <id>12918613</id>
    <revision>
      <id>407353675</id>
      <parentid>383762117</parentid>
      <timestamp>2011-01-11T22:08:34Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="538" xml:space="preserve">A '''quickstart guide''' is a short, simple introductory guide to a piece of equipment for many consumer electronics products (e.g. [[television]]) or recently, [[automobile]]s, [[mobile phone]]s, computers connection. 

With the increase in complexity and functions with electronics products quickstart guides are created to get users quickly accustomed to the basic operations of the product. Complex or detailed operations are usually left in the full-length [[owner's manual]].

{{Electronics-stub}}

[[Category:Electronic documents]]</text>
      <sha1>949y2weqpt0efoxg7dui0iznnzk10z6</sha1>
    </revision>
  </page>
  <page>
    <title>Registry of Research Data Repositories</title>
    <ns>0</ns>
    <id>41872647</id>
    <revision>
      <id>754916806</id>
      <parentid>739907976</parentid>
      <timestamp>2016-12-15T05:56:30Z</timestamp>
      <contributor>
        <username>JFG</username>
        <id>168812</id>
      </contributor>
      <minor />
      <comment>Target page renamed (via JWB)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7616" xml:space="preserve">{{Infobox website
| name = re3data.org
| logo = Re3data Logo RGB 72dpi.png
| logocaption = The logo of re3data.org, the online Registry of Research Data Repositories
| registration = none
| language = English
| type = Online registry
| owner = [[Karlsruhe Institute of Technology]], [[GFZ German Research Centre for Geosciences]], [[Berlin School of Library and Information Science]]
| commercial = no
| launch date = {{Start date and years ago|mf=yes|2013|05|28}}
| current status  = Online
| content license = Website: [[Creative Commons licenses|CC-BY]], Database: [[Creative Commons licenses|CC0]] 
| url = {{URL|http://www.re3data.org/}}
}}

The '''Registry of Research Data Repositories''' ('''re3data.org''') is an [[Open Science]] tool that offers researchers, funding organizations, libraries and publishers an overview of existing international [[disciplinary repository|repositories]] for [[research data]].

== Background ==

re3data.org is a global registry of research data repositories from all academic disciplines. It provides an overview of existing research data repositories in order to help researchers to identify a suitable repository for their data and thus comply with requirements set out in data policies.&lt;ref name=Pampel2013&gt;{{cite journal|last=Pampel|first=Heinz|author2=Vierkant, Paul |author3=Scholze, Frank |author4=Bertelmann, Roland |author5=Kindling, Maxi |title=Making Research Data Repositories Visible: The re3data.org Registry|journal=PLoS ONE|date=4 November 2013|volume=8|issue=11|pages=e78080|doi=10.1371/journal.pone.0078080|pmid=24223762|url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078080|accessdate=7 February 2014|bibcode=2013PLoSO...878080P|pmc=3817176|display-authors=etal}}&lt;/ref&gt;
The registry was officially launched in May 2013.&lt;ref name=Wellander2013&gt;{{cite web | url=http://sparceurope.org/registry-of-research-data-repositories-launched-re3data-org/ | title=Registry of Research Data Repositories launched &#8211; re3data.org | author=Wellander, Janna | date= 4 June 2013 | work=SPARC Europe | accessdate= 5 February 2014}}&lt;/ref&gt;

== Content ==

In March 2014 the registry lists 634 research data repositories from around the world covering all academic disciplines. 586 of these are described in detail using the re3data.org schema.&lt;ref&gt;{{cite web|title=re3data.org &#8211; from Funding to Growing|url=http://www.re3data.org/2014/03/re3data-org-from-funding-to-growing/|work=re3data.org|accessdate=21 March 2014|date=19 March 2014}}&lt;/ref&gt;
The project makes all metadata in the registry available for open use under the Creative Commons deed CC0.&lt;ref&gt;{{cite web|title=DataCite, re3data.org, and Databib Announce Collaboration|url=http://www.re3data.org/2014/03/datacite-re3data-org-databib-collaboration/|work=re3data|accessdate=25 March 2014}}&lt;/ref&gt;
[[File:DRYAD entry in re3data.org 2014-03-21.png|thumb|A screenshot of the DataDryad entry in re3data.org.]]

== Features ==

The majority of the listed research data repositories are described in detail by a comprehensive schema, namely the re3data.org Schema for the Description of Research Data Repositories.&lt;ref name=Vierkant&gt;{{cite web|last=Vierkant|first=Paul|title=Schema for the description of research data repositories|work=GFZ Helmholtz-Zentrum Potsdam|accessdate=7 February 2014|author2=Spier, Shaked |author3=R&#252;cknagel, Jessika |author4= et. al. |url=http://gfzpublic.gfz-potsdam.de/pubman/faces/viewItemFullPage.jsp?itemId=escidoc:301641|doi=10.2312/re3.004}}&lt;/ref&gt;
Information icons support researchers to identify an adequate repository for the storage and reuse of their data.&lt;ref name=Wellander2013 /&gt;&lt;ref&gt;{{cite web|last=Fenner|first=Martin|title=registry of research data repositories launched|url=http://blogs.plos.org/mfenner/2013/06/01/re3data-org-registry-of-research-data-repositories-launched/|work=PLOS Blog|publisher=Gobbledygook|accessdate=5 February 2014}}&lt;/ref&gt;
[[File:Journal.pone.0078080.g001.png|thumb|Aspects of a Research Data Repository with the corresponding icons used in re3data.org.]]

== Inclusion criteria ==

A repository is indexed when the minimum requirements for inclusion in re3data.org are met: the repository has to be run by a legal entity, such as a sustainable institution (e.g. library, university) and clearly state access conditions to the data and repository as well as the terms of use. Additionally, an English [[graphical user interface]] (GUI) plus a focus on research [[data]] is needed.&lt;ref name=Vierkant /&gt;

== Partners and Cooperation ==

re3data.org is a joint project of the [[Berlin School of Library and Information Science]], the [[GFZ German Research Centre for Geosciences]] and the Library of the [[Karlsruhe Institute of Technology]] (KIT). The project is funded by the [[Deutsche Forschungsgemeinschaft|German Research Foundation]] (DFG).&lt;ref name=Pampel2013 /&gt;
The project cooperates with other Open Science initiatives like Databib,&lt;ref&gt;{{cite web|last=Kratz|first=John|title=Finding Disciplinary Data Repositories with DataBib and re3data|url=http://datapub.cdlib.org/2014/03/03/finding-disciplinary-data-repositories-with-databib-and-re3data/|work=Data Pub|accessdate=21 March 2014|author2=Nicholls, Natsuko |date=3 March 2014}}&lt;/ref&gt; BioSharing,&lt;ref&gt;{{cite web|title=Databases|url=http://www.biosharing.org/biodbcore|work=biosharing|accessdate=5 February 2014}}&lt;/ref&gt; [[DataCite]]&lt;ref&gt;{{cite web|title=Resources|url=http://www.datacite.org/resources|work=DataCite|accessdate=5 February 2014}}&lt;/ref&gt; and OpenAIRE.&lt;ref&gt;{{cite web|title=re3data.org and OpenAIRE sign Memorandum of Understanding|url=https://www.openaire.eu/it/component/content/article/481-re3data-and-openaire-sign-memorandum-of-understanding|work=OpenAIRE|accessdate=5 February 2014|date=21 October 2013}}&lt;/ref&gt; Several publishers, research institutions and funders refer to re3data.org in their Editorial Policies and guidelines as a tool for the identification of suitable data repositories, e.g. [[Nature (journal)|Nature]],&lt;ref&gt;{{cite web|title=The paper trail|url=http://www.nature.com/news/the-paper-trail-1.13123|work=Nature|accessdate=5 February 2014|date=4 June 2013}}&lt;/ref&gt; [[Springer Science+Business Media|Springer]]&lt;ref&gt;{{cite web|title=About SpringerPlus - Editorial policies|url=http://www.springerplus.com/about#editorialpolicies|work=SpringerPlus|accessdate=5 February 2014}}&lt;/ref&gt; and the [[European Commission]].&lt;ref&gt;{{cite web|title=Guidelines on Open Access to Scientific Publications and Research Data in Horizon 2020|url=http://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-pilot-guide_en.pdf|publisher=European Commission|accessdate=20 March 2014}}&lt;/ref&gt;

== See also ==

*[[Scientific data archiving]]
*[[Data sharing]]
*[[Data archive]]
*[[Data library]]
*[[Data curation]]

== External links ==
* [http://www.re3data.org/ Official website]

== References ==

{{reflist}}

&lt;!-- Just press the "Save page" button below without changing anything! Doing so will submit your article submission for review. Once you have saved this page you will find a new yellow 'Review waiting' box at the bottom of your submission page. If you have submitted your page previously, either the old pink 'Submission declined' template or the old grey 'Draft' template will still appear at the top of your submission page, but you should ignore it. Again, please don't change anything in this text box. Just press the "Save page" button below. --&gt;


[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
[[Category:Open science]]</text>
      <sha1>jxt4uc6vn4wotduwsa11hnjhmlm3mii</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Eprint archives</title>
    <ns>14</ns>
    <id>43511230</id>
    <revision>
      <id>738754911</id>
      <parentid>679712778</parentid>
      <timestamp>2016-09-10T21:42:45Z</timestamp>
      <contributor>
        <ip>98.230.192.179</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="578" xml:space="preserve">An [[eprint]] archive contains electronic copies of [[Academic journal|research paper]]s (which can be closed or open).

{{cat see also|Open-access archives|Bibliographic databases and indexes}}

== External links ==
[http://roar.eprints.org/ Registry of Open Access Repositories] (ROAR)

{{DEFAULTSORT:Eprint archives}}
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Lists of publications in science]]
[[Category:Online archives]]
[[Category:Digital libraries]]
[[Category:Full text scholarly online databases]]</text>
      <sha1>8l24eyzmoorrzoowbebj7a86t57g1l7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Open-access archives</title>
    <ns>14</ns>
    <id>43515750</id>
    <revision>
      <id>668257984</id>
      <parentid>643620719</parentid>
      <timestamp>2015-06-23T08:09:47Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="284" xml:space="preserve">{{cat main|Open archive}}
{{cat see also|Eprint archives}}
 
[[Category:Open access (publishing)|Archives]]
[[Category:Online archives]]
[[Category:Academic publishing]]
[[Category:Digital libraries]]
[[Category:Electronic documents]]
[[Category:Full text scholarly online databases]]</text>
      <sha1>cln4ycknrm2ld5dnxbjpfnax8hp1gib</sha1>
    </revision>
  </page>
  <page>
    <title>DjVu</title>
    <ns>0</ns>
    <id>610868</id>
    <revision>
      <id>762498297</id>
      <parentid>751569210</parentid>
      <timestamp>2017-01-29T05:33:03Z</timestamp>
      <contributor>
        <ip>70.184.214.35</ip>
      </contributor>
      <comment>alphabetized the categories</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11205" xml:space="preserve">{{About|a computer file format|a computer-assisted translation software tool|D&#233;j&#224; Vu (software)}}

{{ infobox file format
| icon = Djvu icon.svg
| logo = [[File:DjVu-logo.svg|frameless]]
| screenshot =
| caption =
| extension = .djvu, .djv
| mime = image/vnd.djvu, image/x-djvu
| type code = DJVU
| uniform type =
| magic =
| owner = [[AT&amp;T Labs|AT&amp;T Labs &#8211; Research]]
| released = {{start date and age|1998}}
| latest release version = Version 26&lt;ref name=djvuvers&gt;[http://www.djvu.org/forum/phpbb/viewtopic.php?p=952&amp;sid=33819a3f9de6fe1db7870159f47c4dd5 DjVu File Format Version], By Jim Rile, Posted: Fri Feb 23, 2007 1:08 am, PlanetDjVu&lt;/ref&gt;
| latest release date = {{start date and age|2006|06}}
| genre = [[Image file formats]]
| container for =
| contained by =
| extended from =
| extended to =
| standard =
| free = GNU GPLv2 for DjVu Reference Library and DjVuLibre-3.5;&lt;br&gt;License grants under the GNU GPL for several patents that cover aspects of the library&lt;ref&gt;{{cite web |title=DjVu Licensing |work=DjVu Sourceforge page |publisher=Sourceforge.net |date=2011-08-17 |url=http://djvu.sourceforge.net/licensing.html |accessdate=2011-09-21}}&lt;/ref&gt;
| url = {{url|http://www.djvu.org/}}
}}
'''DjVu''' ({{IPAc-en|&#716;|d|e&#618;|&#658;|&#593;&#720;|&#712;|v|u&#720;}} {{respell|DAY|zhah|VOO|'}},&lt;ref name="DjVuOverview"&gt;{{Cite web|url=http://www.cuminas.jp/en/technology/?src=technology_djvu.aspx|title=DjVu Technology|publisher=Cuminas|accessdate=2014-02-12}}&lt;/ref&gt; like {{lang-fr|[[d&#233;j&#224; vu]]}} {{IPA-fr|de&#658;avy|}}) is a [[computer]] [[file format]] designed primarily to store [[image scanner|scanned documents]], especially those containing a combination of text, line drawings, indexed color images, and photographs. It uses technologies such as image layer separation of text and background/images, [[Interlacing (bitmaps)|progressive loading]], [[arithmetic coding]], and [[lossy compression]] for bitonal ([[monochrome]]) images. This allows high-quality, readable images to be stored in a minimum of space, so that they can be made available on the [[World Wide Web|web]].

DjVu has been promoted as an alternative to [[Portable Document Format|PDF]], promising smaller files than PDF for most scanned documents.&lt;ref name="DjVu"&gt;{{Cite web|url=http://djvu.org/resources/whatisdjvu.php|title=What is DjVu &#8211; DjVu.org|publisher=DjVu.org|accessdate=2009-03-05}}&lt;/ref&gt; The DjVu developers report that color magazine pages compress to 40&#8211;70&amp;nbsp;kB, black-and-white technical papers compress to 15&#8211;40&amp;nbsp;kB, and ancient manuscripts compress to around 100&amp;nbsp;kB; a satisfactory [[JPEG]] image typically requires 500&amp;nbsp;kB.&lt;ref name=djvupaper&gt;{{cite journal |author1=L&#233;on Bottou |author2=Patrick Haffner |author3=Paul G. Howard |author4=Patrice Simard |author5=Yoshua Bengio |author6=Yann Le Cun |title=High Quality Document Image Compression with DjVu, 7(3):410&#8211;425|publisher=Journal of Electronic Imaging |year=1998 |url=http://leon.bottou.org/publications/pdf/jei-1998.pdf}}&lt;/ref&gt; Like PDF, DjVu can contain an [[Optical character recognition|OCR]] text layer, making it easy to perform [[copy and paste]] and text search operations.

Free browser plug-ins and desktop viewers from different developers are available from the djvu.org website. DjVu is supported by a number of multi-format document viewers and e-book reader software on Linux ([[Okular]], [[Evince]]) and Windows ([[SumatraPDF]]).

== History ==
The DjVu technology was originally developed&lt;ref name=djvupaper/&gt; by [[Yann LeCun]], [[L&#233;on Bottou]], Patrick Haffner, and Paul G. Howard at [[AT&amp;T Labs]] from 1996 to 2001.

Due to its declared higher compression ratio (and thus smaller file size) and the ease of converting large volumes of text into DjVu format, and because it is an [[open file format]], some independent technologists (such as [[Brewster Kahle]]&lt;ref name="Kahle2005"&gt;{{Cite web|url=http://itc.conversationsnetwork.org/shows/detail400.html|author=Brewster Kahle|title=Universal Access to All Knowledge|format=Audio; Speech at 1h:31m:20s|publisher=Conversations Network|date=December 16, 2004}}&lt;/ref&gt;) have historically considered it superior to [[PDF]].

The DjVu library distributed as part of the open-source package ''DjVuLibre'' has become the reference implementation for the DjVu format. DjVuLibre has been maintained and updated by the original developers of DjVu since 2002.&lt;ref&gt;http://djvu.sourceforge.net/&lt;/ref&gt;

The DjVu file format specification has gone through a number of revisions:

{| class="wikitable sortable"
|+ Revision history
! Support status
! Version
! Release date
! Notes
|-
| Unsupported
| 1&#8211;19&lt;ref name=djvuvers /&gt;
| 1996&#8211;1999
| Developmental versions by AT&amp;T labs preceding the sale of the format to [[LizardTech]].
|-
| Unsupported
| Version 20&lt;ref name=djvuvers /&gt;&lt;!--keep the word ''version'' as it is part of the name--&gt;
| April 1999
| DjVu version 3. DjVu changed from a single-page format to a multipage format.
|-
| Older, still supported
| Version 21&lt;ref name=djvuvers /&gt;
| September 1999
| Indirect storage format replaced. The searchable text layer was added.
|-
| Older, still supported
| Version 22&lt;ref name=djvuvers /&gt;
| April 2001
| Page orientation, color JB2
|-
| Unsupported
| Version 23&lt;ref name=djvuvers /&gt;
| July 2002
| CID chunk
|-
| Unsupported
| Version 24&lt;ref name=djvuvers /&gt;
| February 2003
| LTAnno chunk
|-
| Older, still supported
| Version 25&lt;ref name=djvuvers /&gt;
| May 2003
| NAVM chunk. Support for DjVu bookmarks (outlines) was added. Changes made by Versions 23 and 24 were made obsolete.
|-
| Current
| Version 26&lt;ref name=djvuvers /&gt;
| April 2005
| Text/line annotations
|-
|}

== Technical overview ==

=== File structure ===
The DjVu file format is based on the [[Interchange File Format]] and is composed of hierarchically organized chunks. The IFF structure is preceded by a 4-byte &lt;code&gt;AT&amp;T&lt;/code&gt; [[magic number (programming)|magic number]]. Following is a single &lt;code&gt;FORM&lt;/code&gt; chunk with a secondary identifier of either &lt;code&gt;DJVU&lt;/code&gt; or &lt;code&gt;DJVM&lt;/code&gt; for a single-page or a multi-page document, respectively.

==== Chunk types ====
{| class="wikitable"
|-
! Chunk identifier !! Contained by !! Description
|-
| FORM:DJVU || FORM:DJVM || Describes a single page. Can either be at the root of a document and be a single-page document or referred to from a &lt;code&gt;DIRM&lt;/code&gt; chunk. 
|-
| FORM:DJVM || {{n/a}} || Describes a multi-page document. Is the document's root chunk.
|-
| FORM:DJVI || FORM:DJVM || Contains data shared by multiple pages.
|-
| FORM:THUM || FORM:DJVM || Contains thumbnails.
|-
| INFO || FORM:DJVU || Must be the first chunk. Describes the page width, height, format version, DPI, gamma, and rotation.
|-
| DIRM || FORM:DJVM || Must be the first chunk. References other &lt;code&gt;FORM&lt;/code&gt; chunks. These chunks can either follow this chunk inside the &lt;code&gt;FORM:DJVM&lt;/code&gt; chunk or be contained in external files. These types of documents are referred to as ''bundled'' or ''indirect'', respectively.
|-
| NAVM || FORM:DJVM || If present, must immediately follow the &lt;code&gt;DIRM&lt;/code&gt; chunk. Contains a BZZ-compressed outline of the document.
|}

=== Compression ===
DjVu divides a single image into many different images, then compresses them separately. To create a DjVu file, the initial image is first separated into three images: a background image, a foreground image, and a mask image. The background and foreground images are typically lower-resolution color images (e.g., 100 dpi); the mask image is a high-resolution bilevel image (e.g., 300 dpi) and is typically where the text is stored. The background and foreground images are then compressed using a [[Wavelet compression#Wavelet compression|wavelet-based compression]] algorithm named IW44.&lt;ref name=djvupaper/&gt; The mask image is compressed using a method called JB2 (similar to [[JBIG2]]). The JB2 encoding method identifies nearly identical shapes on the page, such as multiple occurrences of a particular character in a given font, style, and size. It compresses the bitmap of each unique shape separately, and then encodes the locations where each shape appears on the page. Thus, instead of compressing a letter "e" in a given font multiple times, it compresses the letter "e" once (as a compressed bit image) and then records every place on the page it occurs.

Optionally, these shapes may be mapped to [[UTF-8]] codes (either by hand or potentially by a [[Text recognition|text recognition system]]), and stored in the DjVu file. If this mapping exists, it is possible to select and copy text.

Since JBIG2 was based on JB2, both compression methods have the same problems when performing lossy compression.  Numbers may be substituted with similar looking numbers (such as replacing 6 with 8) if the text was scanned at a low DPI prior to lossy compression.

== Format licensing ==
DjVu is an [[open file format]] with patents.&lt;ref name="DjVu"/&gt; The file format specification is published, as well as source code for the reference library.&lt;ref name="DjVu"/&gt; The original authors distribute an [[Open-source software|open-source]] implementation named "''DjVuLibre''" under the [[GNU General Public License]]. The rights to the commercial development of the encoding software have been transferred to different companies over the years, including [[AT&amp;T Corporation]], [[LizardTech]], ''Celartem'' and ''Cuminas''.

==Support==
[[SumatraPDF]] (Windows) among others can manipulate DjVu files.

In 2002, the DjVu file format was chosen by the [[Internet Archive]] as a format in which its ''[[Million Book Project]]'' provides scanned [[public domain]] books online (along with [[TIFF]] and PDF).&lt;ref&gt;{{Cite web|url=http://wiki.laptop.org/go/DJVU |title=Image file formats &#8211; OLPC |publisher=Wiki.laptop.org |date= |accessdate=2008-09-09}}&lt;/ref&gt;

[[Wikimedia Commons]], a media repository used by [[Wikipedia]] among others, conditionally permits PDF and DjVu media files.&lt;ref&gt;[[commons:Commons:Scope#PDF and DjVu formats|PDF and DjVu]]&lt;/ref&gt;

== See also ==
* [[JBIG2]]
* [[Comparison of e-book formats]]

== References ==
{{reflist}}

== External links ==
{{Commons category|DjVu}}
* [http://djvu.org/ "The premier menu for DjVu resources"] (status of the site, which is maintained by an anonymous webmaster, is unclear)
* [http://djvu.sourceforge.net/ DjVuLibre site]
* [http://jwilk.net/software/ Jakub Wilk's pdf2djvu and other DjVu tools]
* [https://bitbucket.org/jsbien/ndt/wiki/wyniki Poliqarp for DjVu search engine and other DjVu tools]
* [http://djvu.org/forum/phpbb/viewtopic.php?t=146 Why won't Google index DjVu files after all this time?] &#8211; topic on  PlanetDjVu
* [http://any2djvu.djvuzone.org Any2Djvu Server - online document converter]
* [https://www.cuminas.jp/en/downloads Cuminas Software Downloads]
* [http://www.djvu-soft.narod.ru/soft/ Table of Djvu Programmes (Russian)]

{{Office document file formats}}
{{Graphics file formats}}

[[Category:1998 introductions]]
[[Category:Computer file formats]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Filename extensions]]
[[Category:Graphics file formats]]
[[Category:Office document file formats]]
[[Category:Open formats]]</text>
      <sha1>54o36uudjldyv8hd3h8ev1xdou6v5vg</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Customer communications management</title>
    <ns>14</ns>
    <id>47769116</id>
    <revision>
      <id>747002157</id>
      <parentid>747002110</parentid>
      <timestamp>2016-10-30T21:50:03Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Customer relationship management software]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="254" xml:space="preserve">[[Category:Software]]
[[Category:Document management systems]]
[[Category:Electronic documents]]
[[Category:Customer relationship management software]]
[[Category:Information technology management]]
{{Commons category|Customer communications management}}</text>
      <sha1>mvkrnvnmrv65hb8cr8ypljlbvdd94c2</sha1>
    </revision>
  </page>
  <page>
    <title>Email</title>
    <ns>0</ns>
    <id>9738</id>
    <revision>
      <id>763013937</id>
      <parentid>763013632</parentid>
      <timestamp>2017-01-31T22:14:06Z</timestamp>
      <contributor>
        <username>Sterilized by cupcake</username>
        <id>30215987</id>
      </contributor>
      <minor />
      <comment>/* Host-based mail systems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="81455" xml:space="preserve">{{about|the communications medium|the former manufacturing conglomerate|Email Limited}}
{{redirect|Inbox|the Google product|Inbox by Gmail}}
&lt;!--Before adding {{lowercase title}} again, please see talk page.--&gt;
[[File:2016-03-22-trojita-home.png|thumb|right|400px|This screenshot shows the "Inbox" page of an email system, where users can see new emails and take actions, such as reading, deleting, saving, or responding to these messages]]
[[File:(at).svg|thumb|100px|The [[at sign]], a part of every SMTP [[email address]]&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5321#section-2.3.11|title=RFC 5321 &#8211; Simple Mail Transfer Protocol|accessdate=19 January 2015|work=Network Working Group }}&lt;/ref&gt;]]
'''Electronic mail''', or '''email''', is a method of exchanging digital messages between people using digital devices such as computers, tablets and mobile phones. Email first entered substantial use in the 1960s and by the mid-1970s had taken the form now recognized as email. Email operates across [[computer network]]s, which in the 2010s is primarily the [[Internet]]. Some early email systems required the author and the recipient to both be [[Online and offline|online]] at the same time, in common with [[instant messaging]]. Today's email systems are based on a [[store-and-forward]] model. Email [[Server (computing)|servers]] accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect only briefly, typically to a [[Message transfer agent|mail server]] or a [[webmail]] interface, for as long as it takes to send or receive messages.

Originally an [[ASCII]] text-only communications medium, Internet email was extended by [[Multipurpose Internet Mail Extensions]] (MIME) to carry text in other character sets and multimedia content attachments. [[International email]], with internationalized email addresses using [[UTF-8]], has been standardized, but as of 2016 it has not been widely adopted.{{citation needed|date=March 2016}}

The history of modern Internet email services reaches back to the early [[ARPANET]], with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s looks very similar to a basic email sent today. Email played an important part in creating the Internet,&lt;ref&gt;{{Harv|Partridge|2008}}&lt;/ref&gt; and the conversion from ARPANET to the Internet in the early 1980s produced the core of the current services.

==Terminology==
Historically, the term ''electronic mail'' was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe [[fax]] document transmission.&lt;ref&gt;Ron Brown, Fax invades the mail market, [https://books.google.com/books?id=Ry64sjvOmLkC&amp;pg=PA218 New Scientist], Vol. 56, No. 817 (Oct., 26, 1972), pages 218&#8211;221.&lt;/ref&gt;&lt;ref&gt;Herbert P. Luckett, What's News: Electronic-mail delivery gets started, [https://books.google.com/books?id=cKSqa8u3EIoC&amp;pg=PA85 Popular Science], Vol. 202, No. 3 (March 1973); page 85&lt;/ref&gt; As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.

Electronic mail has been most commonly called ''email'' or ''e-mail'' since around 1993,&lt;ref&gt;{{cite book|url=https://books.google.com/ngrams/graph?content=electronic+mail%2Ce-mail&amp;year_start=1980&amp;year_end=1995&amp;corpus=15&amp;smoothing=0&amp;share= |title=Google Ngram Viewer |publisher=Books.google.com |accessdate=2013-04-21}}&lt;/ref&gt; but variations of the [[spelling]] have been used:

* ''email'' is the most common form used online, and is required by [[IETF]] [[Request for Comments|Requests for Comments]] (RFC) and working groups&lt;ref&gt;{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt|publisher=IETF|title=RFC Editor Terms List}} This is suggested by the [https://www.rfc-editor.org/rfc-style-guide/rfc-style-manual-08.txt RFC Document Style Guide]&lt;/ref&gt; and increasingly by [[style guide]]s.&lt;ref&gt;{{cite web|url=http://styleguide.yahoo.com/word-list/e |title=Yahoo style guide |publisher=Styleguide.yahoo.com |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref name="aces2011"&gt;[http://www.huffingtonpost.com/2011/03/18/ap-removes-hyphen-from-em_n_837833.html "AP Removes Hyphen From &#8216;Email&#8217; In Style Guide"], 18 March 2011, huffingtonpost.com&lt;/ref&gt; This spelling also appears in most dictionaries.&lt;ref name="AskOxford Language Query team"&gt;{{cite web | url=http://www.askoxford.com/asktheexperts/faq/aboutspelling/email | title=What is the correct way to spell 'e' words such as 'email', 'ecommerce', 'egovernment'? | publisher=[[Oxford University Press]] | work=FAQ | accessdate=4 September 2009 | author=AskOxford Language Query team | archiveurl=https://web.archive.org/web/20080701194047/http://www.askoxford.com/asktheexperts/faq/aboutspelling/email?view=uk | quote=We recommend email, as this is now by far the most common form | archivedate=July 1, 2008}}&lt;/ref&gt;&lt;ref name="Reference.com"&gt;{{cite web|url=http://dictionary.reference.com/browse/email |title=Reference.com |publisher=Dictionary.reference.com |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref name="ReferenceA"&gt;Random House Unabridged Dictionary, 2006&lt;/ref&gt;&lt;ref name="ReferenceB"&gt;The American Heritage Dictionary of the English Language, Fourth Edition&lt;/ref&gt;&lt;ref name="Princeton University WordNet 3.0"&gt;Princeton University WordNet 3.0&lt;/ref&gt;&lt;ref name="ReferenceC"&gt;The American Heritage Science Dictionary, 2002&lt;/ref&gt;&lt;ref name="Merriam-Webster Dictionary"&gt;{{cite web|title=Merriam-Webster Dictionary|url=http://www.merriam-webster.com/dictionary/email|publisher=Merriam-Webster|accessdate=9 May 2014}}&lt;/ref&gt;
* ''e-mail'' is the format that sometimes appears in edited, published American English and British English writing as reflected in the [[Corpus of Contemporary American English]] data,&lt;ref&gt;{{cite web|url=http://english.stackexchange.com/questions/1925/email-or-e-mail|title= "Email" or "e-mail"|work=English Language &amp; Usage &#8211; Stack Exchange|date=August 25, 2010|accessdate=September 26, 2010}}&lt;/ref&gt; but is falling out of favor in style guides.&lt;ref name="aces2011" /&gt;&lt;ref name="ap"&gt;{{cite web|title=AP changes e-mail to email|url=http://www.aces2011.org/sessions/18/the-ap-stylebook-editors-visit-aces-2011/|work=15th National Conference of the American Copy Editors Society (2011, Phoenix)|publisher=ACES|accessdate=23 March 2011|author=Gerri Berendzen|authorlink=AP Stylebook editors share big changes|author2=Daniel Hunt}}&lt;/ref&gt;
* ''mail'' was the form used in the original protocol standard, ''RFC&amp;nbsp;524''.&lt;ref name=rfc524&gt;{{cite web|url=http://www.faqs.org/rfcs/rfc524.html |title=RFC 524 (rfc524) &#8211; A Proposed Mail Protocol |publisher=Faqs.org |date=1973-06-13 |accessdate=2016-11-18}}&lt;/ref&gt; The service is referred to as ''mail'', and a single piece of electronic mail is called a ''message''.&lt;ref name="11above"&gt;{{cite web|url=http://www.faqs.org/rfcs/rfc1939.html |title=RFC 1939 (rfc1939) &#8211; Post Office Protocol &#8211; Version 3 |publisher=Faqs.org |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref name="12above"&gt;{{cite web|url=http://www.faqs.org/rfcs/rfc3501.html |title=RFC 3501 (rfc3501) &#8211; Internet Message Access Protocol &#8211; version 4rev1 |publisher=Faqs.org |accessdate=2014-01-09}}&lt;/ref&gt;
* ''EMail'' is a traditional form that has been used in RFCs for the "Author's Address"&lt;ref name="11above"/&gt;&lt;ref name="12above"/&gt; and is expressly required "for historical reasons".&lt;ref&gt;{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt |title='&amp;#39;"RFC Style Guide"'&amp;#39;, Table of decisions on consistent usage in RFC |accessdate=2014-01-09}}&lt;/ref&gt;
* ''E-mail'' is sometimes used, capitalizing the initial ''E'' as in similar abbreviations like ''[[E-piano]]'', ''[[E-guitar]]'', ''[[A-bomb]]'', and ''[[H-bomb]]''.&lt;ref&gt;{{cite web|url=http://alt-usage-english.org/excerpts/fxhowdoy.html |title=Excerpt from the FAQ list of the Usenet newsgroup alt.usage.english |publisher=Alt-usage-english.org |accessdate=2014-01-09}}&lt;/ref&gt;

== {{anchor|history}} Origin ==
The [[AUTODIN]] network, first operational in 1962, provided a message service between 1,350 terminals, handling 30 million messages per month, with an average message length of approximately 3,000 characters. Autodin was supported by 18 large computerized switches, and was connected to the [[United States General Services Administration]] Advanced Record System, which provided similar services to roughly 2,500 terminals.&lt;ref name="NAS USPS"&gt;USPS Support Panel, Louis T Rader, Chair, Chapter IV: Systems, [https://books.google.com/books?id=5TQrAAAAYAAJ&amp;pg=PA27 Electronic Message Systems for the U.S. Postal Service], National Academy of Sciences, Washington, D.C., 1976; pages 27&#8211;35.&lt;/ref&gt; By 1968, AUTODIN linked more than 300 sites in several countries.

===Host-based mail systems===
With the introduction of [[Massachusetts Institute of Technology|MIT]]'s [[Compatible Time-Sharing System]] (CTSS) in 1961,&lt;ref&gt;"CTSS, Compatible Time-Sharing System" (September 4, 2006), [[University of South Alabama]], [http://www.cis.usouthal.edu/faculty/daigle/project1/ctss.htm USA-CTSS].&lt;/ref&gt; multiple users could log in to a central system&lt;ref&gt;an [[IBM 7094]]&lt;/ref&gt; from remote dial-up terminals, and store and share files on the central disk.&lt;ref&gt;[[Tom Van Vleck]], "The IBM 7094 and CTSS" (September 10, 2004), ''Multicians.org''
 ([[Multics]]), web: [http://www.multicians.org/thvv/7094.html Multicians-7094].&lt;/ref&gt; Informal methods of using this to pass messages were developed and expanded:
* 1965 &#8211; [[Massachusetts Institute of Technology|MIT]]'s [[Compatible Time-Sharing System|CTSS]] MAIL.&lt;ref name="thvv"&gt;{{cite web|url=http://www.multicians.org/thvv/mail-history.html|title=The History of Electronic Mail|author=Tom Van Vleck}}&lt;/ref&gt;

Developers of other early systems developed similar email applications:
&lt;!-- Please do not delete references without first reading them. I've added a page number. --&gt;
* 1962 &#8211; [[IBM Administrative Terminal System|1440/1460 Administrative Terminal System]]&lt;ref&gt;{{cite book
 |     author = IBM
 |      title = 1440/1460 Administrative Terminal System (1440-CX-07X and 1460-CX-08X) Application Description
 |    section =
 | sectionurl =
 |    version = Second Edition
 |  publisher = IBM
 |       date =
 |        url = http://bitsavers.org/pdf/ibm/144x/H20-0129-1_1440_admTermSys.pdf
 |         id = H20-0129-1
 | accessdate =
 |      quote =
 |       page = 10
 |      pages =
 |        ref =
 |mode=cs2
 }}&lt;/ref&gt;
* 1968 &#8211; [[IBM Administrative Terminal System|ATS/360]]&lt;ref&gt;{{cite book
 |     author = IBM
 |      title = System/36O Administrative Terminal System DOS (ATS/DOS) Program Description Manual
 |    section =
 | sectionurl =
 |    version =
 |  publisher = IBM
 |       date =
 |        url =
 |         id = H20-0508
 | accessdate =
 |      quote =
 |       page =
 |      pages =
 |        ref =
 |mode=cs2
 }}&lt;/ref&gt;&lt;ref&gt;{{cite book
 |     author = IBM
 |      title = System/360 Administrative Terminal System-OS (ATS/OS) Application Description Manual
 |    section =
 | sectionurl =
 |    version =
 |  publisher = IBM
 |       date =
 |        url =
 |         id = H20-0297
 | accessdate =
 |      quote =
 |       page =
 |      pages =
 |        ref =
 |mode=cs2
 }}&lt;/ref&gt;
* 1971 &#8211;  ''[[SNDMSG]]'', a local inter-user mail program incorporating the experimental file transfer program, ''CPYNET'', allowed the first [[Computer network|networked]] electronic mail&lt;ref name="firstnetworkemail"&gt;{{cite web|author=Ray Tomlinson |url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html |title=The First Network Email |publisher=Openmap.bbn.com |accessdate=2014-01-09}}&lt;/ref&gt;
* 1972 &#8211; [[Unix]] [[mail (Unix)|mail]] program&lt;ref&gt;{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V3/man/man1/mail.1 |title=Version 3 Unix mail(1) manual page from 10/25/1972 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6/usr/man/man1/mail.1 |title=Version 6 Unix mail(1) manual page from 2/21/1975 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}&lt;/ref&gt;
* 1972 &#8211; APL Mailbox by [[Lawrence M. Breed|Larry Breed]]&lt;ref&gt;[http://www.jsoftware.com/papers/APLQA.htm APL Quotations and Anecdotes], including [[Leslie H. Goldsmith|Leslie Goldsmith]]'s story of the Mailbox&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx|title=Home &gt; Communications &gt; The Internet &gt; History of the internet &gt; Internet in its infancy|archive-url=https://web.archive.org/web/20110227151622/http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx| archive-date=2011-02-27 |work=actewagl.com.au |accessdate=2016-11-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.youtube.com/watch?v=mjgkhK-nXmk | title=The STSC Story: It's About Time |date=c. 1979 |editor=[http://aprogramminglanguage.com Catherine Lathwell] |publisher=[[Scientific Time Sharing Corporation]] |at=7:08 |accessdate=2017-01-06 |}} Promotional video for Scientific Time Sharing Corporation, which features President [[Jimmy Carter]]'s press secretary [[Jody Powell]] explaining how the company's "APL Mailbox" enabled the 1976 Carter presidential campaign to easily move information around the country to coordinate the campaign. &lt;/ref&gt;
* 1974 &#8211; The [[PLATO (computer system)|PLATO]] IV Notes on-line [[message board]] system was generalized to offer 'personal notes' in August 1974.&lt;ref name="NAS USPS" /&gt;&lt;ref name=wooley&gt;David Wooley, [http://www.thinkofit.com/plato/dwplato.htm#pnotes PLATO: The Emergence of an Online Community], 1994.&lt;/ref&gt;
* 1978 &#8211; ''Mail'' client written by Kurt Shoens for Unix and distributed with the Second Berkeley Software Distribution included support for aliases and distribution lists, forwarding, formatting messages, and accessing different mailboxes.&lt;ref name="shoens"&gt;The Mail Reference Manual, Kurt Shoens, University of California, Berkeley, 1979.&lt;/ref&gt; It used the Unix ''mail'' client to send messages between system users. The concept was extended to communicate remotely over the Berkeley Network.&lt;ref name="berknet"&gt;An Introduction to the Berkeley Network, Eric Schmidt, University of California, Berkeley, 1979.&lt;/ref&gt;
* 1979 &#8211; ''EMAIL'', an application written by [[Shiva Ayyadurai]]. He has been associated with controversial, self-made claims that he had "invented" email due to its presence of certain functionality. These claims have been disputed by various parties.&lt;ref&gt;{{cite web|url=http://www.bizjournals.com/boston/news/2016/05/10/cambridge-man-who-claims-he-invented-email-sues.html|title=Cambridge man who claims he invented email sues Gawker for $35M - Boston Business Journal|last=Harris|first=David L.|date=May 10, 2016|website=Boston Business Journal|access-date=2016-05-16}}&lt;/ref&gt;&lt;ref&gt;[https://assets.documentcloud.org/documents/2829697/Gov-Uscourts-Mad-180248-1-0.pdf ''Shiva Ayyadurai v. Gawker Media, et. al''., Complaint] (D. Mass, filed May 10, 2016)&lt;/ref&gt;&lt;ref name=DavidCrockerWaPo&gt;{{cite news|last=Crocker|first=David|title=A history of e-mail: Collaboration, innovation and the birth of a system|url=http://www.washingtonpost.com/national/on-innovations/a-history-of-e-mail-collaboration-innovation-and-the-birth-of-a-system/2012/03/19/gIQAOeFEPS_story.html|accessdate=10 June 2012|newspaper=Washington Post|date=20 March 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://blogs.smithsonianmag.com/aroundthemall/2012/02/a-piece-of-email-history-comes-to-the-american-history-museum/|title=A Piece of Email History Comes to the American History Museum|date=22 February 2012|accessdate=11 June 2012|first=Joseph|last=Stromberg|publisher=[[Smithsonian Institution]]}}&lt;/ref&gt;&lt;ref name=SmithsonianStatement&gt;{{Cite press release|url=http://americanhistory.si.edu/press/releases/statement-national-museum-american-history-collection-materials-va-shiva-ayyudurai|title=Statement from the National Museum of American History: Collection of Materials from V.A. Shiva Ayyadurai|date=23 February 2012|accessdate=19 February 2013|publisher=National Museum of American History}}&lt;/ref&gt;
* 1979 &#8211; [[MH Message Handling System]] developed at RAND provided several tools for managing electronic mail on Unix.&lt;ref name="borden-mh"&gt;A Mail Handling System, Bruce Borden, The Rand Corporation, 1979.&lt;/ref&gt;
* 1981 &#8211; [[IBM OfficeVision|PROFS]] by IBM&lt;ref&gt;[http://www.ibm.com/ibm100/us/en/icons/networkbus/ ''"...PROFS changed the way organizations communicated, collaborated and approached work when it was introduced by IBM's Data Processing Division in 1981..."''], IBM.com&lt;/ref&gt;&lt;ref&gt;[https://fas.org/spp/starwars/offdocs/reagan/chron.txt ''"1982 &#8211; The National Security Council (NSC) staff at the White House acquires a prototype electronic mail system, from IBM, called the Professional Office System (PROFs)...."''], fas.org&lt;/ref&gt;
* 1982 &#8211; [[ALL-IN-1]]&lt;ref&gt;{{cite web|url=https://research.microsoft.com/en-us/um/people/gbell/Digital/timeline/1982.htm |title=Gordon Bell's timeline of Digital Equipment Corporation |publisher=Research.microsoft.com |date=1998-01-30 |accessdate=2014-01-09}}&lt;/ref&gt; by [[Digital Equipment Corporation]]
* 1982 &#8211; HP Mail (later HP DeskManager) by [[Hewlett-Packard]]&lt;ref&gt;{{cite web|url=http://www.hpmuseum.net/divisions.php?did=10|title=HP Computer Museum}}&lt;/ref&gt;

These original messaging systems had widely different features and ran on systems that were incompatible with each other. Most of them only allowed communication between users logged into the same host or "mainframe", although there might be hundreds or thousands of users within an organization.

===LAN email systems===
In the early 1980s, networked [[personal computer]]s on [[LAN]]s became increasingly important. Server-based systems similar to the earlier mainframe systems were developed. Again, these systems initially allowed communication only between users logged into the same server infrastructure. Examples include:
* [[cc:Mail]]
* [[Lantastic]]
* [[WordPerfect Office]]
* [[Microsoft Mail]]
* [[Banyan VINES]]
* [[Lotus Notes]]
Eventually these systems too could link different organizations as long as they ran the same email system and proprietary protocol.&lt;ref&gt;with various vendors supplying gateway software to link these incompatible systems&lt;/ref&gt;

===Email networks===
To facilitate electronic mail exchange between remote sites and with other organizations, telecommunication links, such as dialup modems or leased lines, provided means to transport email globally, creating local and global networks. This was challenging for a number of reasons, including the widely [[Non-Internet email address|different email address formats]] in use. 
* In 1971 the first [[ARPANET]] email was sent,&lt;ref&gt;{{cite web|title=The First Network Email|url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html|author=Ray Tomlinson}}&lt;/ref&gt; and through RFC 561, RFC 680, RFC 724, and finally 1977's RFC 733, became a standardized working system.
* PLATO IV was networked to individual terminals over leased data lines prior to the implementation of personal notes in 1974.&lt;ref name=wooley/&gt;
* Unix mail was networked by 1978's [[uucp]],&lt;ref&gt;{{cite web|url=http://cm.bell-labs.com/7thEdMan/vol2/uucp.bun |title=Version 7 Unix manual: "UUCP Implementation Description" by D. A. Nowitz, and "A Dial-Up Network of UNIX Systems" by D. A. Nowitz and M. E. Lesk |accessdate=2014-01-09}}&lt;/ref&gt; which was also used for [[USENET]] newsgroup postings, with similar headers.
* BerkNet, the Berkeley Network, was written by [[Eric Schmidt]] in 1978 and included first in the Second Berkeley Software Distribution. It provided support for sending and receiving messages over serial communication links. The Unix mail tool was extended to send messages using BerkNet.&lt;ref name="berknet"/&gt; 
* The [[delivermail]] tool, written by [[Eric Allman]] in 1979 and 1980 (and shipped in 4BSD), provided support for routing mail over dissimilar networks, including Arpanet, UUCP, and BerkNet. (It also provided support for mail user aliases.)&lt;ref name="joy-4bsd"&gt;Setting up the Fourth Berkeley Software Tape, William N. Joy, Ozalp Babaoglu, Keith Sklower, University of California, Berkeley, 1980.&lt;/ref&gt;
* The mail client included in 4BSD (1980) was extended to provide interoperability between a variety of mail systems.&lt;ref name="shoens-mail"&gt;Mail(1), UNIX Programmer's Manual, 4BSD, University of California, Berkeley, 1980.&lt;/ref&gt;
* [[BITNET]] (1981) provided electronic mail services for educational institutions. It was based on the IBM VNET email system.&lt;ref&gt;[http://www.livinginternet.com/u/ui_bitnet.htm "BITNET History"], livinginternet.com&lt;/ref&gt;
* 1983 &#8211; [[MCI Mail]] Operated by MCI Communications Corporation.  This was the first commercial public email service to use the internet. MCI Mail also allowed subscribers to send regular postal mail (overnight) to non-subscribers.&lt;ref&gt;"[[MCI Mail]]", MCI Mail&lt;/ref&gt;
* In 1984, IBM PCs running DOS could link with [[FidoNet]] for email and shared bulletin board posting.

===Email address internationalization===
Globally countries started adopting [[Internationalized domain name|IDN]] registrations for supporting country specific scripts (non-English) for domain names. In 2010  Egypt, the Russian Federation, Saudi Arabia, and the United Arab Emirates started offering IDN registrations. The government of India also registered [[Bh&#257;rat Ga&#7751;ar&#257;jya|.bharat]]&lt;ref&gt;{{Cite web|url=https://registry.in/Internationalized_Domain_Names_IDNs|title=Internationalized Domain Names (IDNs) {{!}} Registry.In|website=registry.in|access-date=2016-10-17}}&lt;/ref&gt; in 8 languages/scripts in 2014. 
In 2016 Data Xgen Technologies was credited as World's first email platform offering EAI in India and [[Russia]].&lt;ref&gt;http://economictimes.indiatimes.com/tech/internet/datamail-worlds-first-free-linguistic-email-service-supports-eight-india-languages/articleshow/54923001.cms&lt;/ref&gt;&lt;ref&gt;http://digitalconqurer.com/gadgets/made-india-datamail-empowers-russia-email-address-russian-language/&lt;/ref&gt;

===Attempts at interoperability===
{{Refimprove section|date=August 2010}}
Early interoperability among independent systems included:
* [[ARPANET]], a forerunner of the Internet, defined protocols for dissimilar computers to exchange email.
* [[uucp]] implementations for Unix systems, and later for other operating systems, that only had dial-up communications available.
* [[CSNET]], which initially used the UUCP protocols via dial-up to provide networking and mail-relay services for non-ARPANET hosts.
* Action Technologies developed the [[Message Handling System]] (MHS) protocol (later bought by [[Novell]],&lt;ref&gt;[https://books.google.com/books?id=vxcEAAAAMBAJ&amp;pg=PA64 "Delivering the Enterprise Message], 19 Sep 1994, Daniel Blum, Network World&lt;/ref&gt;&lt;ref&gt;[http://www.networkworld.com/archive/1994/94-03-07hot_.html ''"...offers improved performance, greater reliability and much more flexibility in everything from communications hardware to scheduling..."''], 03/07/94, Mark Gibbs,
Network World&lt;/ref&gt;&lt;ref&gt;{{cite web | url = http://support.microsoft.com/kb/118859 | title = MHS: Correct Addressing format to DaVinci Email via MHS | work = Microsoft Support Knowledge Base | accessdate = 2007-01-15 }}&lt;/ref&gt; which abandoned it after purchasing the non-MHS WordPerfect Office&amp;mdash;renamed [[Novell GroupWise|Groupwise]]).
* [[HP OpenMail]] was known for its ability to interconnect several other APIs and protocols, including MAPI, cc:Mail, SMTP/MIME, and X.400.
* Soft-Switch released its eponymous email gateway product in 1984, acquired by [[Lotus Software]] ten years later.&lt;ref&gt;https://www.linkedin.com/in/nickshelness&lt;/ref&gt;
* The [[Coloured Book protocols]] ran on [[United Kingdom|UK]] academic networks until 1992.
* [[X.400]] in the 1980s and early 1990s was promoted by major vendors, and mandated for government use under [[GOSIP]], but abandoned by all but a few in favor of [[Internet]] [[Simple Mail Transfer Protocol|SMTP]] by the mid-1990s.

===From SNDMSG to MSG===
In the early 1970s, [[Ray Tomlinson]] updated an existing utility called [[SNDMSG]] so that it could copy messages (as files) over the network. [[Lawrence Roberts (scientist)|Lawrence Roberts]], the project manager for the ARPANET development, took the idea of READMAIL, which dumped all "recent" messages onto the user's terminal, and wrote a programme for [[TOPS-20#TENEX|TENEX]] in [[Text Editor and Corrector|TECO]] macros called ''RD'', which permitted access to individual messages.&lt;ref name="livinginternet1"&gt;{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}&lt;/ref&gt; Barry Wessler then updated RD and called it ''NRD''.&lt;ref&gt;* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April&#8211;June 2008|url=http://www.ir.bbn.com/~craig/email.pdf|format=PDF | doi =  10.1109/mahc.2008.32|pages=3&#8211;29}}&lt;/ref&gt;

Marty Yonke rewrote NRD to include reading, access to SNDMSG for sending, and a help system, and called the utility ''WRD'', which was later known as ''BANANARD''. John Vittal then updated this version to include three important commands: ''Move'' (combined save/delete command), ''Answer'' (determined to whom a reply should be sent) and ''Forward'' (sent an email to a person who was not already a recipient). The system was called ''MSG''. With inclusion of these features, MSG is considered to be the first integrated modern email programme, from which many other applications have descended.&lt;ref name="livinginternet1"/&gt;

===ARPANET mail===
Experimental email transfers between separate computer systems began shortly after the creation of the [[ARPANET]] in 1969.&lt;ref name="thvv" /&gt; [[Ray Tomlinson]] is generally credited as having sent the first email across a network, initiating the use of the "[[At sign|@]]" sign to separate the names of the user and the user's machine in 1971, when he sent a message from one [[Digital Equipment Corporation]] [[DEC-10]] computer to another DEC-10. The two machines were placed next to each other.&lt;ref name="firstnetworkemail"/&gt;&lt;ref&gt;Wave New World,Time Magazine, October 19, 2009, p.48&lt;/ref&gt; Tomlinson's work was quickly adopted across the ARPANET, which significantly increased the popularity of email. Tomlinson is internationally known as the inventor of modern email.&lt;ref&gt;{{cite web|url=http://www.npr.org/2016/03/06/469428062/ray-tomlinson-inventor-of-modern-email-has-died|title=Ray Tomlinson, Inventor Of Modern Email, Dies|date=6 March 2016|work=NPR.org}}&lt;/ref&gt;

Initially addresses were of the form, ''username@hostname''&lt;ref&gt;RFC 805, 8 February 1982, Computer Mail Meeting Notes&lt;/ref&gt; but were extended to "username@host.domain" with the development of the [[Domain Name System]] (DNS).

As the influence of the ARPANET spread across academic communities, [[Gateway (telecommunications)|gateways]] were developed to pass mail to and from other networks such as [[CSNET]], [[JANET NRS|JANET]], [[BITNET]], [[X.400]], and [[FidoNet]]. This often involved addresses such as:
:hubhost!middlehost!edgehost!user@uucpgateway.somedomain.example.com
which routes mail to a user with a "[[UUCP#Mail routing|bang path]]" address at a UUCP host.

==Operation==
The diagram to the right shows a typical sequence of events&lt;ref&gt;{{cite video|title=How E-mail Works|medium=internet video|publisher=howstuffworks.com|year=2008|url=http://www.webcastr.com/videos/informational/how-email-works.html}}&lt;/ref&gt; that takes place when sender [[Placeholder names in cryptography|Alice]] transmits a message using a [[E-mail client|mail user agent]] (MUA) addressed to the [[email address]] of the recipient.
&lt;span style="float:right"&gt;[[File:email.svg|400px|Email operation]]&lt;/span&gt;
# The MUA formats the message in email format and uses the submission protocol, a profile of the [[Simple Mail Transfer Protocol]] (SMTP), to send the message to the local [[mail submission agent]] (MSA), in this case ''smtp.a.org''.
# The MSA determines the destination address provided in the SMTP protocol (not from the message header), in this case ''bob@b.org''. The part before the @ sign is the ''local part'' of the address, often the [[username]] of the recipient, and the part after the @ sign is a [[domain name]]. The MSA resolves a domain name to determine the [[fully qualified domain name]] of the [[Message transfer agent|mail server]] in the [[Domain Name System]] (DNS).
# The [[DNS server]] for the domain ''b.org'' (''ns.b.org'') responds with any [[MX record]]s listing the mail exchange servers for that domain, in this case ''mx.b.org'', a [[message transfer agent]] (MTA) server run by the recipient's ISP.&lt;ref&gt;[https://dnsdb.cit.cornell.edu/explain_mx.html "MX Record Explanation"], it.cornell.edu&lt;/ref&gt;
# smtp.a.org sends the message to mx.b.org using SMTP. This server may need to forward the message to other MTAs before the message reaches the final [[message delivery agent]] (MDA).
# The MDA delivers it to the [[Email Mailbox|mailbox]] of user ''bob''.
# Bob's MUA picks up the message using either the [[Post Office Protocol]] (POP3) or the [[Internet Message Access Protocol]] (IMAP).

In addition to this example, alternatives and complications exist in the email system:
* Alice or Bob may use a client connected to a corporate email system, such as [[IBM]] [[Lotus Notes]] or [[Microsoft]] [[Microsoft Exchange Server|Exchange]]. These systems often have their own internal email format and their clients typically communicate with the email server using a vendor-specific, proprietary protocol. The server sends or receives email via the Internet through the product's Internet mail gateway which also does any necessary reformatting. If Alice and Bob work for the same company, the entire transaction may happen completely within a single corporate email system.
* Alice may not have a MUA on her computer but instead may connect to a [[webmail]] service.
* Alice's computer may run its own MTA, so avoiding the transfer at step 1.
* Bob may pick up his email in many ways, for example logging into mx.b.org and reading it directly, or by using a webmail service.
* Domains usually have several mail exchange servers so that they can continue to accept mail even if the primary is not available.

Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called ''[[open mail relay]]s''. This was very important in the early days of the Internet when network connections were unreliable.{{citation needed|date=July 2015}}  However, this mechanism proved to be exploitable by originators of [[email spam|unsolicited bulk email]] and as a consequence open mail relays have become rare,&lt;ref name="IMCR-016"&gt;{{cite web|url=http://www.imc.org/ube-relay.html |title=Allowing Relaying in SMTP: A Series of Surveys |accessdate=2008-04-13 |last=Hoffman |first=Paul |date=2002-08-20 |work=IMC Reports |publisher=[[Internet Mail Consortium]] |archiveurl=https://web.archive.org/web/20070118121843/http://www.imc.org/ube-relay.html |archivedate=2007-01-18 }}&lt;/ref&gt; and many MTAs do not accept messages from open mail relays.

==Message format {{anchor|Internet Message Format}}==
The Internet email message format is now defined by RFC 5322, with multimedia content attachments being defined in RFC 2045 through RFC 2049, collectively called ''[[Multipurpose Internet Mail Extensions]]'' or ''MIME''. RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 &#8211; which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the [[ARPANET]].&lt;ref&gt;{{cite web|first=Ken|last=Simpson|title=An update to the email standards|date=October 3, 2008|publisher=MailChannels Blog Entry|url= http://blog.mailchannels.com/2008/10/update-to-email-standards.html}}&lt;/ref&gt;

Internet email messages consist of two major sections, the message header and the message body.  The header is structured into [[Field (computer science)|fields]] such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a [[signature block]] at the end. The header is separated from the body by a blank line.

===Message header===
&lt;!-- This section is linked from [[Bracket]] --&gt;
Each message has exactly one [[Header (computing)|header]], which is structured into [[Field (computer science)|fields]]. Each field has a name and a value. RFC 5322 specifies the precise syntax.

Informally, each line of text in the header that begins with a [[Printable characters|printable character]] begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit [[ASCII]] characters. Non-ASCII values may be represented using MIME [[MIME#Encoded-Word|encoded words]].

====Header fields====
Email header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long.&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5322|title=RFC 5322, Internet Message Format|author=P. Resnick, Ed.|date=October 2008|publisher=IETF}}&lt;/ref&gt; Header fields defined by RFC 5322 can only contain [[US-ASCII]] characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used.&lt;ref&gt;{{cite web|last=Moore|first=K|title=MIME (Multipurpose Internet Mail Extensions) Part Three: Message Header Extensions for Non-ASCII Text|url=https://tools.ietf.org/html/rfc2047|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=2012-01-21| date=November 1996 }}&lt;/ref&gt; Recently the IETF EAI working group has defined some standards track extensions,&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc6532|title=RFC 6532, Internationalized Email Headers|author=A Yang, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc6531|title=RFC 6531, SMTP Extension for Internationalized Email Addresses|author=J. Yao, Ed., W. Mao, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}&lt;/ref&gt; replacing previous experimental extensions, to allow [[UTF-8]] encoded [[Unicode]] characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some governments.&lt;ref name="economictimes.indiatimes.com"&gt;{{Cite news|url=http://economictimes.indiatimes.com/tech/internet/now-get-your-email-address-in-hindi/articleshow/53830034.cms|title=Now, get your email address in Hindi - The Economic Times|newspaper=The Economic Times|access-date=2016-10-17}}&lt;/ref&gt;

The message header must include at least the following fields:&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6 |title=RFC 5322, 3.6. Field Definitions |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6.4 |title=RFC 5322, 3.6.4. Identification Fields |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}&lt;/ref&gt;
* ''From'': The [[email address]], and optionally the name of the author(s). In many email clients not changeable except through changing account settings.
* ''Date'': The local time and date when the message was written. Like the ''From:'' field, many email clients fill this in automatically when sending. The recipient's client may then display the time in the format and time zone local to him/her.

RFC 3864 describes registration procedures for message header fields at the [[Internet Assigned Numbers Authority|IANA]]; it provides for [http://www.iana.org/assignments/message-headers/perm-headers.html permanent] and [http://www.iana.org/assignments/message-headers/prov-headers.html provisional] field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5064 |title=RFC 5064 |publisher=Tools.ietf.org|date=December 2007 |accessdate=2014-01-09}}&lt;/ref&gt;

* ''To'': The email address(es), and optionally name(s) of the message's recipient(s). Indicates primary recipients (multiple allowed), for secondary recipients see Cc: and Bcc: below.
* ''Subject'': A brief summary of the topic of the message. [[E-mail subject abbreviations|Certain abbreviations]] are commonly used in the subject, including [[E-mail subject abbreviations|"RE:" and "FW:"]].
* ''Cc'': [[Carbon copy]]; Many email clients will mark email in one's inbox differently depending on whether they are in the To: or Cc: list. (''Bcc'': [[Blind carbon copy]]; addresses are usually only specified during SMTP delivery, and not usually listed in the message header.)
* [[Content-Type]]: Information about how the message is to be displayed, usually a [[MIME]] type.
* ''Precedence'': commonly with values "bulk", "junk", or "list"; used to indicate that automated "vacation" or "out of office" responses should not be returned for this mail, e.g. to prevent vacation notices from being sent to all other subscribers of a mailing list. [[Sendmail]] uses this field to affect prioritization of queued email, with "Precedence: special-delivery" messages delivered sooner. With modern high-bandwidth networks, delivery priority is less of an issue than it once was. [[Microsoft Exchange Server|Microsoft Exchange]] respects a fine-grained automatic response suppression mechanism, the ''X-Auto-Response-Suppress'' field.&lt;ref&gt;Microsoft, Auto Response Suppress, 2010, [http://msdn.microsoft.com/en-us/library/ee219609(v=EXCHG.80).aspx microsoft reference], 2010 Sep 22&lt;/ref&gt;
* ''Message-ID'': Also an automatically generated field; used to prevent multiple delivery and for reference in In-Reply-To: (see below).
* ''In-Reply-To'': [[Message-ID]] of the message that this is a reply to. Used to link related messages together. This field only applies for reply messages.
* ''References'': [[Message-ID]] of the message that this is a reply to, and the message-id of the message the previous reply was a reply to, etc.
* ''Reply-To'': Address that should be used to reply to the message.
* ''Sender'': Address of the actual sender acting on behalf of the author listed in the From: field (secretary, list manager, etc.).
* ''Archived-At'': A direct link to the archived form of an individual email message.

Note that the ''To:'' field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, [[Simple Mail Transfer Protocol|SMTP]], which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply [[email authentication]] systems to messages being relayed. Data pertaining to server's activity is also part of the header, as defined below.

SMTP defines the ''trace information'' of a message, which is also saved in the header using the following two fields:&lt;ref&gt;{{cite IETF|title=Simple Mail Transfer Protocol|rfc=5321|author=[[John Klensin]]|sectionname=Trace Information|section=4.4| date=October 2008 |publisher=[[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt;
* ''Received'': when an SMTP server accepts a message it inserts this trace record at the top of the header (last to first).
* ''Return-Path'': when the delivery SMTP server makes the ''final delivery'' of a message, it inserts this field at the top of the header.

Other fields that are added on top of the header by the receiving server may be called ''trace fields'', in a broader sense.&lt;ref&gt;{{cite web|url=http://www.ietf.org/mail-archive/web/apps-discuss/current/msg04115.html|title=Trace headers|author=John Levine|date=14 January 2012|work=email message|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=16 January 2012|quote=there are many more trace fields than those two}}&lt;/ref&gt;
* ''Authentication-Results'': when a server carries out authentication checks, it can save the results in this field for consumption by downstream agents.&lt;ref&gt;This extensible field is defined by RFC 7001, that also defines an [[Internet Assigned Numbers Authority|IANA]] registry of [http://www.iana.org/assignments/email-auth/ Email Authentication Parameters].&lt;/ref&gt;
* ''Received-SPF'': stores results of [[Sender Policy Framework|SPF]] checks in more detail than Authentication-Results.&lt;ref&gt;RFC 7208.&lt;/ref&gt;
* ''Auto-Submitted'': is used to mark automatically generated messages.&lt;ref&gt;Defined in RFC 3834, and updated by RFC 5436.&lt;/ref&gt;
* ''VBR-Info'': claims [[Vouch by Reference|VBR]] whitelisting&lt;ref&gt;RFC 5518.&lt;/ref&gt;

===Message body===

====Content encoding====
Email was originally designed for 7-bit [[ASCII]].&lt;ref&gt;{{cite book|title=TCP/IP Network Administration|year=2002|isbn=978-0-596-00297-8|author=Craig Hunt|publisher=[[O'Reilly Media]]|page=70}}&lt;/ref&gt; Most email software is [[8-bit clean]] but must assume it will communicate with 7-bit servers and mail readers. The [[MIME]] standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: [[quoted printable]] for mostly 7-bit content with a few characters outside that range and [[base64]] for arbitrary binary data. The [[8BITMIME]] and [[BINARY]] extensions were introduced to allow transmission of mail without the need for these encodings, but many [[mail transport agent]]s still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international [[character set]]s, [[Unicode]] is growing in popularity.{{citation needed|date=September 2014}}

====Plain text and HTML====
Most modern graphic [[email client]]s allow the use of either [[plain text]] or [[HTML#HTML email|HTML]] for the message body at the option of the user. [[HTML email]] messages often include an automatically generated plain text copy as well, for compatibility reasons. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in [[block quote]]s, wrap naturally on any display, use emphasis such as [[underline]]s and [[italics]], and change [[font]] styles. Disadvantages include the increased size of the email, privacy concerns about [[web bug]]s, abuse of HTML email as a vector for [[phishing]] attacks and the spread of [[malware|malicious software]].&lt;ref&gt;{{cite web|title=Email policies that prevent viruses|url=http://advosys.ca/papers/mail-policies.html}}&lt;/ref&gt;

Some web-based [[mailing list]]s recommend that all posts be made in plain-text, with 72 or 80 [[characters per line]]&lt;ref&gt;{{cite web|url=http://helpdesk.rootsweb.com/listadmins/plaintext.html |title=When posting to a RootsWeb mailing list... |publisher=Helpdesk.rootsweb.com |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.openbsd.org/mail.html |title=...Plain text, 72 characters per line... |publisher=Openbsd.org |accessdate=2014-01-09}}&lt;/ref&gt; for all the above reasons, but also because they have a significant number of readers using [[List of email clients#Text-based|text-based email clients]] such as [[Mutt (email client)|Mutt]]. Some [[Microsoft]] [[email client]]s allow rich formatting using their proprietary [[Rich Text Format]] (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible [[email client]].&lt;ref&gt;{{cite web|url=http://support.microsoft.com/kb/138053 |title=How to Prevent the Winmail.dat File from Being Sent to Internet Users |publisher=Support.microsoft.com |date=2010-07-02 |accessdate=2014-01-09}}&lt;/ref&gt;

==Servers and client applications==
[[File:Mozilla Thunderbird 3.1.png|thumb|right|300px|The interface of an email client, [[Mozilla Thunderbird|Thunderbird]].]]
&lt;!-- This section is linked from [[Catch-all (Mail)]]. See [[WP:MOS#Section management]] --&gt;
Messages are exchanged between hosts using the [[Simple Mail Transfer Protocol]] with software programs called [[mail transfer agent]]s (MTAs); and delivered to a mail store by programs called [[mail delivery agent]]s (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it,&lt;ref&gt;In practice, some accepted messages may nowadays not be delivered to the recipient's InBox, but instead to a Spam or Junk folder which, especially in a corporate environment, may be inaccessible to the recipient&lt;/ref&gt; and when a message cannot be delivered, that MTA must send a [[bounce message]] back to the sender, indicating the problem.

Users can retrieve their messages from servers using standard protocols such as [[Post Office Protocol|POP]] or [[IMAP]], or, as is more likely in a large [[corporation|corporate]] environment, with a [[Proprietary software|proprietary]] protocol specific to [[Novell Groupwise]], [[Lotus Notes]] or [[Microsoft Exchange Server]]s.  Programs used by users for retrieving, reading, and managing email are called [[mail user agent]]s (MUAs).

Mail can be stored on the [[client (computing)|client]], on the [[Server (computing)|server]] side, or in both places. Standard formats for mailboxes include [[Maildir]] and [[mbox]]. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as [[IMAP]], moving email from one server to another can be done with any [[Mail user agent|MUA]] supporting the protocol.

Many current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail, Hotmail, or Yahoo! Mail, that performs the same tasks.&lt;ref&gt;http://dir.yahoo.com/business_and_economy/business_to_business/communications_and_networking/internet_and_world_wide_web/email_providers/free_email/&lt;/ref&gt; Such [[webmail]] interfaces allow users to access their mail with any standard [[web browser]], from any computer, rather than relying on an email client.

===Filename extensions===
Upon reception of email messages, [[email client]] applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the ''[[mbox]]'' format. The specific format used is often indicated by special [[filename extension]]s:
;&lt;tt&gt;eml&lt;/tt&gt;
:Used by many email clients including [[Novell GroupWise]], [[Microsoft Outlook Express]], [[Lotus notes]], [[Windows Mail]], [[Mozilla Thunderbird]], and Postbox. The files are [[plain text]] in [[MIME]] format, containing the email header as well as the message contents and attachments in one or more of several formats.
;&lt;tt&gt;emlx&lt;/tt&gt;
:Used by [[Apple Mail]].
;&lt;tt&gt;msg&lt;/tt&gt;
:Used by [[Microsoft Outlook|Microsoft Office Outlook]] and [[OfficeLogic|OfficeLogic Groupware]].
;&lt;tt&gt;mbx&lt;/tt&gt;
:Used by [[Opera Mail]], [[KMail]], and [[Apple Mail]] based on the [[mbox]] format.

Some applications (like [[Apple Mail]]) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.

===URI scheme mailto===
{{main article|mailto}}
The [[URI scheme]], as registered with the [[Internet Assigned Numbers Authority|IANA]], defines the &lt;tt&gt;mailto:&lt;/tt&gt; scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the ''To:'' field.&lt;ref&gt;RFC 2368 section 3 : by Paul Hoffman in 1998 discusses operation of the "mailto" URL.&lt;/ref&gt;

==Types==

===Web-based email===
{{main article|Webmail}}
Many email providers have a web-based email client (e.g. [[AOL Mail]], [[Gmail]], [[Outlook.com]], [[Hotmail]] and [[Yahoo! Mail]]). This allows users to log in to the email account by using any compatible [[web browser]] to send and receive their email. Mail is typically not downloaded to the client, so can't be read without a current Internet connection.

===POP3 email services===
The [[Post Office Protocol]] 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the [[server (computing)|server]]. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).&lt;ref name="Windows to Linux"&gt;{{cite book | last = Allen | first = David | title = Windows to Linux | publisher = Prentice Hall | year = 2004 | location = | page =192 | url = https://books.google.com/books?id=UD0h_GqgbHgC&amp;printsec=frontcover&amp;dq=network%2B+guide+to+networks&amp;hl=en&amp;src=bmrr&amp;ei=hMnATfmmA8j00gGMsOC2Cg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CE8Q6AEwAA#v=onepage&amp;q&amp;f=false}}&lt;/ref&gt;

===IMAP email servers===
The [[Internet Message Access Protocol]] (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like [[smartphone]]s are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.

===MAPI email servers===
[[MAPI|Messaging Application Programming Interface]] (MAPI) is used by [[Microsoft Outlook]] to communicate to [[Microsoft Exchange Server]] - and to a range of other email server products such as [[Axigen|Axigen Mail Server]], [[Kerio Connect]], [[Scalix]], [[Zimbra]], [[HP OpenMail]], [[IBM Lotus Notes]], [[Zarafa (software)|Zarafa]], and [[Bynari]] where vendors have added MAPI support to allow their products to be accessed directly via Outlook.

==Uses==
{{Refimprove section|date=November 2007}}

===Business and organizational use===
Email has been widely accepted by business, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed [[Internet]]). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.&lt;ref name=om&gt;By Om Malik, GigaOm. "[http://gigaom.com/collaboration/is-email-a-curse-or-a-boon/ Is Email a Curse or a Boon?]" September 22, 2010. Retrieved October 11, 2010.&lt;/ref&gt;

It has some key benefits to business and other organizations, including:
; Facilitating logistics
: Much of the business world relies on communications between people who are not physically in the same building, area, or even country; setting up and attending an in-person meeting, [[telephone call]], or [[conference call]] can be inconvenient, time-consuming, and costly. Email provides a method of exchanging information between two or more people with no set-up costs and that is generally far less expensive than a physical meeting or phone call.
; Helping with synchronisation
: With [[Real-time computing|real time]] communication by meetings or phone calls, participants must work on the same schedule, and each participant must spend the same amount of time in the meeting or call. Email allows [[wikt:asynchrony|asynchrony]]: each participant may control their schedule independently.
; Reducing cost
: Sending an email is much less expensive than sending postal mail, or [[long distance telephone call]]s, [[telex]] or [[telegrams]].
; Increasing speed
: Much faster than most of the alternatives.
; Creating a "written" record
: Unlike a telephone or in-person conversation, email by its nature creates a detailed written record of the communication, the identity of the sender(s) and recipient(s) and the date and time the message was sent. In the event of a contract or legal dispute, saved emails can be used to prove that an individual was advised of certain issues, as each email has the date and time recorded on it.

====Email marketing====
Email marketing via "[[opt-in email|opt-in]]" is often successfully used to send special sales offerings and new product information.&lt;ref name=brett&gt;{{cite journal | last1 = Martin | first1 = Brett A. S. | last2 = Van Durme | first2 = Joel | last3 = Raulas | first3 = Mika | last4 = Merisavo | first4 = Marko | year = 2003 | title = E-mail Marketing: Exploratory Insights from Finland | url = http://www.basmartin.com/wp-content/uploads/2010/08/Martin-et-al-2003.pdf | format = PDF | journal = Journal of Advertising Research | volume = 43 | issue = 3| pages = 293&#8211;300 | doi=10.1017/s0021849903030265}}&lt;/ref&gt; Depending on the recipient's culture,&lt;ref&gt;{{cite web|url=http://www.computerworld.com/article/2467778/endpoint-security/spam-culture--part-1--china.html|title=Spam culture, part 1: China|first=Amir|last=Lev|publisher=}}&lt;/ref&gt; email sent without permission&amp;mdash;such as an "opt-in"&amp;mdash;is likely to be viewed as unwelcome "[[email spam]]".

===Personal use===

====Desktop====
Many users access their personal email from friends and family members using a [[desktop computer]] in their house or apartment.

====Mobile====
Email has become widely used on [[smartphone]]s and [[Wi-Fi]]-enabled [[laptop]]s and [[tablet computer]]s. Mobile "apps" for email increase accessibility to the medium for users who are out of their home. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other device to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. Today, there are an estimated 1.4 billion email users worldwide and 50 billion non-spam emails that are sent daily.

Individuals often check email on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their [[Facebook]] accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone.&lt;ref&gt;{{cite web|url=http://marketingland.com/smartphone-activities-study-email-web-facebook-37954|title=Email Is Top Activity On Smartphones, Ahead Of Web Browsing &amp; Facebook [Study]|date=28 March 2013|publisher=}}&lt;/ref&gt; It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.&lt;ref&gt;{{cite web|url=http://www.emailmonday.com/mobile-email-usage-statistics|title=The ultimate mobile email statistics overview|publisher=}}&lt;/ref&gt;

==Issues==
{{Refimprove section|date=October 2016}}

===Attachment size limitation===
{{Main article|Email attachment}}
Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include [[Microsoft Word]] documents, [[pdf]] documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, [[server (computing)|server]]s and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less.&lt;ref&gt;[http://exchangepedia.com/2007/09/exchange-server-2007-setting-message-size-limits.html ''"Setting Message Size Limits in Exchange 2010 and Exchange 2007"''].&lt;/ref&gt;&lt;ref&gt;[http://www.geek.com/articles/news/google-updates-file-size-limits-for-gmail-and-youtube-20090629/#ixzz0oIzFY0Q8 ''"Google updates file size limits for Gmail and YouTube"'', geek.com].&lt;/ref&gt;&lt;ref&gt;[http://mail.google.com/support/bin/answer.py?answer=8770&amp;topic=1517 ''"Maximum attachment size"'', mail.google,com].&lt;/ref&gt; Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ to what the user sees,&lt;ref&gt;{{cite web|url=http://technet.microsoft.com/en-us/magazine/2009.01.exchangeqa.aspx?pr=blog|title=Exchange 2007: Attachment Size Increase,...|date=2010-03-25|publisher=TechNet Magazine, Microsoft.com US}}&lt;/ref&gt; which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, [[file hosting service]]s of various sorts are available; and generally suggested.&lt;ref&gt;[https://support.office.com/en-us/article/Send-large-files-to-other-people-7005da19-607a-47d5-b2c5-8f3982c6cc83 "Send large files to other people"], Microsoft.com&lt;/ref&gt;&lt;ref&gt;[http://www.makeuseof.com/tag/8-ways-to-email-large-attachments/ "8 ways to email large attachments"], Chris Hoffman, December 21, 2012, makeuseof.com&lt;/ref&gt; Some large files, such as digital photos, color presentations and video or music files are too large for some email systems.

===Information overload===
The ubiquity of email for knowledge workers and "white collar" employees has led to concerns that recipients face an "[[information overload]]" in dealing with increasing volumes of email.&lt;ref&gt;{{cite web|last=Radicati|first=Sara|title=Email Statistics Report, 2010|url=http://www.radicati.com/wp/wp-content/uploads/2010/04/Email-Statistics-Report-2010-2014-Executive-Summary2.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Gross|first=Doug|title=Happy Information Overload Day!|url=http://articles.cnn.com/2010-10-20/tech/information.overload.day_1_mails-marsha-egan-rss?_s=PM:TECH|work=CNN|date=July 26, 2011}}&lt;/ref&gt; This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect,&lt;ref&gt;{{cite news|url=http://www.nytimes.com/2008/04/20/technology/20digi.html?_r=2&amp;oref=slogin&amp;oref=slogin|title=Struggling to Evade the E-Mail Tsunami|date=2008-04-20|publisher=The New York Times|first=Randall|last=Stross|accessdate=May 1, 2010}}&lt;/ref&gt; as efforts to read the many emails could reduce [[productivity]].

===Spam===
{{Main article|Email spam}}
Email "spam" is the term used to describe unsolicited bulk email. The low cost of sending such email meant that by 2003 up to 30% of total email traffic was already spam.&lt;ref&gt;[http://visionedgemarketing.com/growth-of-spam-email-2/ "Growth of Spam Email"]&lt;/ref&gt;&lt;ref name="R"&gt;Rich Kawanagh. The top ten email spam list of 2005. ITVibe news, 2006, January 02, [http://itvibe.com/news/3837/ ITvibe.com]&lt;/ref&gt;&lt;ref&gt;How Microsoft is losing the war on spam [http://dir.salon.com/story/tech/feature/2005/01/19/microsoft_spam/index.html Salon.com]&lt;/ref&gt; and was threatening the usefulness of email as a practical tool. The US [[CAN-SPAM Act of 2003]] and similar laws elsewhere&lt;ref&gt;Spam Bill 2003 ([http://www.aph.gov.au/library/pubs/bd/2003-04/04bd045.pdf PDF])&lt;/ref&gt; had some impact, and a number of effective [[anti-spam techniques (email)|anti-spam techniques]] now largely mitigate the impact of spam by filtering or rejecting it for most users,&lt;ref&gt;[http://www.wired.com/2015/07/google-says-ai-catches-99-9-percent-gmail-spam/ "Google Says Its AI Catches 99.9 Percent of Gmail Spam"], Cade Metz, July 09 2015, wired.com&lt;/ref&gt; but the volume sent is still very high&amp;mdash;and increasingly consists not of advertisements for products, but malicious content or links.&lt;ref name="securelist"&gt;[https://securelist.com/analysis/quarterly-spam-reports/74682/spam-and-phishing-in-q1-2016/ "Spam and phishing in Q1 2016"],  May 12, 2016, securelist.com&lt;/ref&gt;

===Malware===
A range of malicious email types exist. These range from [[List of email scams|various types of email scams]], including [[Social engineering (security)|"social engineering"]] scams such as [[advance-fee scam]] "Nigerian letters", to [[phishing]], [[email bomb]]ardment and [[Computer worm|email worms]].

===Email spoofing===
{{Main article|Email spoofing}}
[[Email spoofing]] occurs when the email message header is designed to make the message appear to come from a known or trusted source. [[Email spam]] and [[phishing]] methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email which appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.

===Email bombing===
{{main article|Email bomb}}

[[Email bomb]]ing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.

===Privacy concerns===
{{Main article|Internet privacy}}

Today it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although [[information technology]] personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.

Email privacy, without some security precautions, can be compromised because:
* email messages are generally not encrypted.
* email messages have to go through intermediate computers before reaching their destination, meaning it is relatively easy for others to intercept and read messages.
* many Internet Service Providers (ISP) store copies of email messages on their mail servers before they are delivered. The backups of these can remain for up to several months on their server, despite deletion from the mailbox.
* the "Received:"-fields and other information in the email can often identify the sender, preventing anonymous communication.

There are [[cryptography]] applications that can serve as a remedy to one or more of the above. For example, [[Virtual Private Network]]s or the [[Tor (anonymity network)|Tor anonymity network]] can be used to encrypt traffic from the user machine to a safer network while [[GNU Privacy Guard|GPG]], [[Pretty Good Privacy|PGP]], SMEmail,&lt;ref&gt;[http://www.arxiv.org/pdf/1002.3176 SMEmail &#8211; A New Protocol for the Secure E-mail in Mobile Environments], Proceedings of the Australian Telecommunications Networks and Applications Conference (ATNAC'08), pp. 39&#8211;44, Adelaide, Australia, Dec. 2008.&lt;/ref&gt; or [[S/MIME]] can be used for [[end-to-end principle|end-to-end]] message encryption, and SMTP STARTTLS or SMTP over [[Transport Layer Security]]/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.

Additionally, many [[mail user agent]]s do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as [[Simple Authentication and Security Layer|SASL]] prevent this. Finally, attached files share many of the same hazards as those found in [[Peer-to-peer|peer-to-peer filesharing]]. Attached files may contain [[Trojan horse (computing)|trojans]] or [[Computer virus|viruses]].

===Flaming===
[[Flaming (Internet)|Flaming]] occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word "incendiary" to describe particularly heated email discussions. The ease and impersonality of email communications mean that the [[social norms]] that encourage civility in person or via telephone do not exist and civility may be forgotten.&lt;ref&gt;{{cite journal|author1=S. Kiesler |author2=D. Zubrow |author3=A.M. Moses |author4=V. Geller |title=Affect in computer-mediated communication: an experiment in synchronous terminal-to-terminal discussion|journal=Human-Computer Interaction|volume=1|pages=77&#8211;104|year=1985|doi=10.1207/s15327051hci0101_3}}&lt;/ref&gt;

===Email bankruptcy===
{{main article|Email bankruptcy}}
Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a "boilerplate" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. [[Harvard University]] law professor [[Lawrence Lessig]] is credited with coining this term, but he may only have popularized it.&lt;ref&gt;{{cite news|title=All We Are Saying.|url=http://www.nytimes.com/2007/12/23/weekinreview/23buzzwords.html?ref=weekinreview|publisher=The New York Times|date=December 23, 2007|accessdate=2007-12-24|first=Grant|last=Barrett}}&lt;/ref&gt;

===Tracking of sent mail===
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the [[Internet Engineering Task Force|IETF]] introduced [[Delivery Status Notification]]s (delivery receipts) and [[Return receipt#Email|Message Disposition Notifications]] (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885&lt;ref&gt;RFC 3885, ''SMTP Service Extension for Message Tracking''&lt;/ref&gt; through 3888.&lt;ref&gt;RFC 3888, ''Message Tracking Model and Requirements''&lt;/ref&gt;)

Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:
* Delivery Reports can be used to verify whether an address exists and if so, this indicates to a spammer that it is available to be spammed.
* If the spammer uses a forged sender email address ([[email spoofing]]), then the innocent email address that was used can be flooded with NDRs from the many invalid email addresses the spammer may have attempted to mail. These NDRs then constitute spam from the ISP to the innocent user.

In the absence of standard methods, a range of system based around the use of [[web bug]]s have been developed. However, these are often seen as underhand or raising privacy concerns,&lt;ref&gt;{{cite news|url=http://query.nytimes.com/gst/fullpage.html?res=940CE0D9143AF931A15752C1A9669C8B63&amp;sec=&amp;spon=&amp;pagewanted=print|title=Software That Tracks E-Mail Is Raising Privacy Concerns|author=Amy Harmon|publisher=The New York Times|date=2000-11-22|accessdate=2012-01-13}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://email.about.com/od/emailbehindthescenes/a/html_return_rcp.htm |title=About.com |publisher=Email.about.com |date=2013-12-19 |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.webdevelopersnotes.com/tips/yahoo/notification-when-yahoo-email-is-opened.php |title=Webdevelopersnotes.com |publisher=Webdevelopersnotes.com |accessdate=2014-01-09}}&lt;/ref&gt; and only work with email clients that support rendering of HTML. Many mail clients now default to not showing "web content".&lt;ref&gt;[http://www.slipstick.com/outlook/email/microsoft-outlook-web-bugs-blocked-html-images "Outlook: Web Bugs &amp; Blocked HTML Images"], slipstick.com&lt;/ref&gt; [[Webmail]] providers can also disrupt web bugs by pre-caching images.&lt;ref&gt;[http://arstechnica.com/information-technology/2013/12/gmail-blows-up-e-mail-marketing-by-caching-all-images-on-google-servers/ "Gmail blows up e-mail marketing..."], Ron Amadeo, Dec 13 2013, Ars Technica&lt;/ref&gt;

==U.S. government==
The U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways. Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service.&lt;ref&gt;{{cite web|url=http://www.fastcompany.com/1780716/can-technology-save-us-postal-service|title=Can Technology Save The U.S. Postal Service?|work=Fast Company}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://tech.mit.edu/V131/N60/emaillab.html|title=Can an MIT professor save the USPS? - The Tech|work=mit.edu}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.fedtechmagazine.com/article/2013/01/why-united-states-postal-service-taking-cues-silicon-valley|title=Why the USPS Is Taking Cues from Silicon Valley|work=FedTech}}&lt;/ref&gt; The USPS initiated an experimental email service known as [[E-COM]]. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing.&lt;ref&gt;{{cite web|url=http://cmsw.mit.edu/usps-can-save-itself/|title=Shiva Ayyadurai: USPS can save itself|work=MIT Comparative Media Studies/Writing}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://bostinno.streetwise.co/2012/01/13/could-email-save-snail-mail-or-is-the-internet-too-reliant-on-the-usps/|title=Could Email Save Snail Mail, Or Is The Internet Too Reliant on the USPS?|date=6 March 2012|work=BostInno}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.bostonglobe.com/lifestyle/2012/03/02/dear-usps/V4GJ8w9UCcfV4v0WiVjvmK/story.html|title=&#8216;Dear USPS . . .&#8217;|work=BostonGlobe.com}}&lt;/ref&gt; Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the [[Postal Regulatory Commission]] and the [[Federal Communications Commission]] opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a [[tariff]].&lt;ref&gt;In re Request for declaratory ruling and investigation by Graphnet Systems, Inc., concerning the proposed E-COM service, FCC Docket No. 79-6 (September 4, 1979)&lt;/ref&gt; Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.&lt;ref&gt;Hardy, Ian R; [https://archive.org/web/*/http:/www.ifla.org/documents/internet/hari1.txt The Evolution of ARPANET Email]; 1996-05-13; History Thesis Paper; University of California at Berkeley&lt;/ref&gt;&lt;ref&gt;James Bovard, The Law Dinosaur: The US Postal Service, CATO Policy Analysis (February 1985)&lt;/ref&gt;&lt;ref name="JayAkkad"&gt;{{cite web|url=http://www.cs.ucsb.edu/~almeroth/classes/F04.176A/homework1_good_papers/jay-akkad.html |title=Jay Akkad, The History of Email |publisher=Cs.ucsb.edu |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.gao.gov/archive/2000/gg00188.pdf |title=US Postal Service: Postal Activities and Laws Related to Electronic Commerce, GAO-00-188 |format=PDF |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://govinfo.library.unt.edu/ota/Ota_4/DATA/1982/8214.PDF |title=Implications of Electronic Mail and Message Systems for the U.S. Postal Service , Office of Technology Assessment, Congress of the United States, August 1982 |format=PDF |accessdate=2014-01-09}}&lt;/ref&gt;

The early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the [[Multics]], the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system.&lt;ref name="thvv"/&gt; The [[United States Department of Defense|Department of Defense]] [[DARPA]] desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and [[Austin Henderson]] publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. [[Jon Postel]] recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, led to the release of David Crocker's RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982).&lt;ref&gt;{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History, How Email was Invented, Living Internet |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}&lt;/ref&gt; RFC 822 is a small adaptation of RFC 733's details, notably enhancing the [[host (network)|host]] portion, to use [[Domain Name]]s, that were being developed at the same time.

The [[National Science Foundation]] took over operations of the ARPANET and Internet from the Department of Defense, and initiated [[NSFNet]], a new [[backbone network|backbone]] for the network. A part of the NSFNet AUP forbade commercial traffic.&lt;ref&gt;{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/notes/internet_history80s.htm |title=Internet History |publisher=Cybertelecom |accessdate=2014-01-09}}&lt;/ref&gt; In 1988, [[Vint Cerf]] arranged for an interconnection of [[MCI Mail]] with NSFNET on an experimental basis. The following year [[Compuserve]] email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised. In the late 1990s, the [[Federal Trade Commission]] grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing.&lt;ref&gt;[http://www.cybertelecom.org/spam/Spamref.htm Cybertelecom : SPAM Reference] {{webarchive |url=https://web.archive.org/web/20140919090804/http://www.cybertelecom.org/spam/Spamref.htm |date=September 19, 2014 }}&lt;/ref&gt; In 2004, FTC jurisdiction over spam was codified into law in the form of the [[Can Spam Act|CAN SPAM Act.]]&lt;ref&gt;{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/spam/canspam.htm |title=Can Spam Act |publisher=Cybertelecom |accessdate=2014-01-09}}&lt;/ref&gt; Several other U.S. federal agencies have also exercised jurisdiction including the [[United States Department of Justice|Department of Justice]] and the [[United States Secret Service|Secret Service]]. NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a [[Macintosh Portable]] was used aboard [[Space Shuttle]] mission [[STS-43]] to send the first email via [[AppleLink]].&lt;ref&gt;{{cite web|last=Cowing |first=Keith |url=http://www.spaceref.com/news/viewnews.html?id=213 |title=2001: A Space Laptop &amp;#124; SpaceRef &#8211; Your Space Reference |publisher=Spaceref.com |date=2000-09-18 |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.macobserver.com/columns/thisweek/2004/20040831.shtml |title=The Mac Observer &#8211; This Week in Apple History &#8211; August 22&#8211;31: "Welcome, IBM. Seriously," Too Late to License |publisher=Macobserver.com |date=2004-10-31 |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Linzmayer|first=Owen W.|title=Apple confidential 2.0 : the definitive history of the world's most colorful company|year=2004|publisher=No Starch Press|location=San Francisco, Calif.|isbn=1-59327-010-0|edition=[Rev. 2. ed.].}}&lt;/ref&gt; Today astronauts aboard the International Space Station have email capabilities via the [[WI-FI|wireless networking]] throughout the station and are connected to the ground at 10 [[Mbit/s]] Earth to station and 3 Mbit/s station to Earth, comparable to home [[DSL]] connection speeds.&lt;ref name=issit&gt;{{cite news|title=First Tweet from Space|url=http://bits.blogs.nytimes.com/2010/01/22/first-tweet-from-space/|newspaper=The New York Times|first=Nick|last=Bilton|date=January 22, 2010}}&lt;/ref&gt;

==See also==
{{colbegin||22em}}
* [[Anonymous remailer]]
* [[Anti-spam techniques]]
* [[biff]]
* [[Bounce message]]
* [[Comparison of email clients]]
* [[Dark Mail Alliance]]
* [[Disposable email address]]
* [[E-card]]
* [[Electronic mailing list]]
* [[Email art]]
* [[Email authentication]]
* [[Email digest]]
* [[Email encryption]]
* [[Email hosting service]]
* [[Email storm]]
* [[Email tracking]]
* [[HTML email]]
* [[Information overload]]
* [[Internet fax]]
* [[Internet mail standard]]s
* [[List of email subject abbreviations]]
* [[MCI Mail]]
* [[Netiquette]]
* [[Posting style]]
* [[Privacy-enhanced Electronic Mail]]
* [[Push email]]
* [[RSS]]
* [[Telegraphy]]
* [[Unicode and email]]
* [[Usenet quoting]]
* [[Webmail]], [[Comparison of webmail providers]]
* [[X-Originating-IP]]
* [[X.400]]
* [[Yerkish]]
{{colend}}

==References==
{{Reflist|2}}

==Further reading==
{{refbegin}}
* Cemil Betanov, ''Introduction to X.400'', Artech House, ISBN 0-89006-597-7.
* Marsha Egan, "[http://www.inboxdetox.com Inbox Detox and The Habit of Email Excellence]", Acanthus Publishing ISBN 978-0-9815589-8-1
* Lawrence Hughes, ''Internet e-mail Protocols, Standards and Implementation'', Artech House Publishers, ISBN 0-89006-939-5.
* Kevin Johnson, ''Internet Email Protocols: A Developer's Guide'', Addison-Wesley Professional, ISBN 0-201-43288-9.
* Pete Loshin, ''Essential Email Standards: RFCs and Protocols Made Practical'', John Wiley &amp; Sons, ISBN 0-471-34597-0.
* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April&#8211;June 2008|url=http://www.ir.bbn.com/~craig/papers/email.pdf|format=PDF|issn=1934-1547|ref=harv|postscript={{inconsistent citations}} | doi =  10.1109/mahc.2008.32|pages=3&#8211;29}}
* Sara Radicati, ''Electronic Mail: An Introduction to the X.400 Message Handling Standards'', Mcgraw-Hill, ISBN 0-07-051104-7.
* John Rhoton, ''Programmer's Guide to Internet Mail: SMTP, POP, IMAP, and LDAP'', Elsevier, ISBN 1-55558-212-5.
* John Rhoton, ''X.400 and SMTP: Battle of the E-mail Protocols'', Elsevier, ISBN 1-55558-165-X.
* David Wood, ''Programming Internet Mail'', O'Reilly, ISBN 1-56592-479-7.
{{refend}}

==External links==
{{Wiktionary|email|outbox}}
* [http://www.iana.org/assignments/message-headers/perm-headers.html IANA's list of standard header fields]
* [http://emailhistory.org/ The History of Email] is Dave Crocker's attempt at capturing the sequence of 'significant' occurrences in the evolution of email; a collaborative effort that also cites this page.
* [http://www.multicians.org/thvv/mail-history.html The History of Electronic Mail] is a personal memoir by the implementer of an early email system
* [http://www.circleid.com/posts/20140903_a_look_at_the_origins_of_network_email/ A Look at the Origins of Network Email] is a short, yet vivid recap of the key historical facts
* [https://www.fbi.gov/news/stories/2015/august/business-e-mail-compromise Business E-Mail Compromise - An Emerging Global Threat], [[FBI]]
&lt;!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links --&gt;
{{Computer-mediated communication}}
{{E-mail clients}}

{{Authority control}}

{{DEFAULTSORT:Email}}
[[Category:Email| ]]
[[Category:Internet terminology]]
[[Category:Electronic documents]]
[[Category:History of the Internet]]
[[Category:1971 introductions]]</text>
      <sha1>onsiytwc6yry2e1kntvpayw6jnutlbp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Legal citators</title>
    <ns>14</ns>
    <id>24447352</id>
    <revision>
      <id>389225528</id>
      <parentid>315734059</parentid>
      <timestamp>2010-10-07T02:14:01Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Merging catmore1/catmore2 per [[Wikipedia:Templates for discussion/Log/2010 September 10|TFD]], and renaming catmore/catmore2 per [[Template talk:cat main|discussion]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="107" xml:space="preserve">{{Cat main|Citator}}

[[Category:Citation indices]]
[[Category:Legal research]]
[[Category:Legal citation]]</text>
      <sha1>rtwjcp6ezchgu1mcxoud4orozv4mdjv</sha1>
    </revision>
  </page>
  <page>
    <title>Scopus</title>
    <ns>0</ns>
    <id>582311</id>
    <revision>
      <id>760515008</id>
      <parentid>760514950</parentid>
      <timestamp>2017-01-17T13:45:56Z</timestamp>
      <contributor>
        <username>Sandbergja</username>
        <id>15752169</id>
      </contributor>
      <minor />
      <comment>/* Overview */ removing dead link as a result of my last edit -- Sorry!</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4165" xml:space="preserve">{{Use dmy dates|date=August 2013}}
{{other uses}}
{{infobox bibliographic database
| title = Scopus
| image = [[File:Scopus_type_logo.jpg]]
| caption = 
| producer = [[Elsevier]]
| country = 
| history = 
| languages = English
| providers = 
| cost = Subscription
| disciplines= 
| depth = 
| formats = 
| temporal = 1995-present
| geospatial = Worldwide
| number = 55 million
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://www.scopus.com
| titles = 
}}
'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).&lt;ref&gt;{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}&lt;/ref&gt; It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.&lt;ref&gt;{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092&#8211;6 |pmid=19738094}}&lt;/ref&gt;

==Overview==

Since Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.&lt;ref&gt;{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}&lt;/ref&gt; The board consists of scientists and subject librarians.

Evaluating ease of use and coverage of Scopus and the [[Web of Science]] (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. ... The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive."&lt;ref&gt;{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}&lt;/ref&gt;

Scopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].

Scopus IDs for individual authors can be integrated with the nonproprietary digital identifier [[ORCID]].&lt;ref name="Scopus"&gt;{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}&lt;/ref&gt;

==See also==
*[[Source Normalized Impact per Paper]]
*[[Web of Science]]

== References ==
{{reflist|30em}}

== External links ==
{{Wikidata property|P1153}}
{{Wikidata property|P1154}}
{{Wikidata property|P1155}}
{{Wikidata property|P1156}}
* {{Official website|http://www.scopus.com/}}

{{Reed Elsevier}}

[[Category:Bibliographic databases and indexes]]
[[Category:Elsevier]]
[[Category:Citation indices]]
[[Category:Library cataloging and classification]]</text>
      <sha1>ar5hsr1oqzyai5np4kltdlfcnz9x5x4</sha1>
    </revision>
  </page>
  <page>
    <title>Islamic World Science Citation Database</title>
    <ns>0</ns>
    <id>24783829</id>
    <revision>
      <id>735707083</id>
      <parentid>678205790</parentid>
      <timestamp>2016-08-22T15:38:05Z</timestamp>
      <contributor>
        <username>TheStrayDog</username>
        <id>19920863</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1769" xml:space="preserve">'''Islamic World Science Citation Database''' ('''ISC''') is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].

It was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.&lt;ref&gt;{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}&lt;/ref&gt;  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].

In 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.&lt;ref&gt;{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}&lt;/ref&gt;

== References ==
{{Reflist}}

==See also==
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Impact factor]]

== External links ==
* {{Official website|http://isc.gov.ir/Default.aspx?lan=en}}

[[Category:Bibliographic databases and indexes]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Research management]]
[[Category:Databases in Iran]]
[[Category:Science and technology in Iran]]


{{science-journal-stub}}
{{islam-stub}}
{{iran-stub}}</text>
      <sha1>ih612n610hm4psztpcn7slqnhurofh1</sha1>
    </revision>
  </page>
  <page>
    <title>Russian Science Citation Index</title>
    <ns>0</ns>
    <id>35108736</id>
    <revision>
      <id>670537328</id>
      <parentid>564703231</parentid>
      <timestamp>2015-07-08T16:30:29Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>+[[Category:Russian-language journals]]; +[[Category:Science and technology in Russia]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2476" xml:space="preserve">{{primary sources|date=March 2012}}
'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.

== Purpose ==
From 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.

== Functionality ==
In Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: 
* Scientific capacity and effectiveness of research, and
* Publication activity
through the following indicators:
* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,
* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and
* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.

== See also ==
*[[List of academic databases and search engines]]
*[[Science Citation Index]]
*[[Scopus]]

==External links==
* [http://elibrary.ru/ Scientific Electronic Library]


[[Category:Citation indices]]
[[Category:Russian-language journals| ]]
[[Category:Science and technology in Russia]]</text>
      <sha1>b31c48gt29qy8vxs9hrrhchxc9l72jx</sha1>
    </revision>
  </page>
  <page>
    <title>Chinese Science Citation Database</title>
    <ns>0</ns>
    <id>47180611</id>
    <revision>
      <id>678867458</id>
      <parentid>678205635</parentid>
      <timestamp>2015-09-01T03:14:21Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>CSCD Journal List</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="815" xml:space="preserve">{{Infobox bibliographic database 
 |title=Chinese Science Citation Database
}}
The '''Chinese Science Citation Database''' (CSCD) is a [[bibliographic database]] and [[citation index]] produced by the [[Chinese Academy of Sciences]].

It is hosted by [[Thomson Reuters]], and it was the first database in its [[Web of Science]] product in a language other than English.&lt;ref&gt;[http://wokinfo.com/products_tools/multidisciplinary/cscd/]&lt;/ref&gt;

==See also==
*[[Chinese Social Sciences Citation Index]]

==References==
{{reflist}}

==External links==
*[http://thomsonreuters.com/content/dam/openweb/documents/pdf/scholarly-scientific-research/methodology/cscd-journal-list.pdf CSCD Journal List]

[[Category:Bibliographic databases and indexes]]
[[Category:Citation indices]]
[[Category:Science and technology in China]]</text>
      <sha1>917guhtk4x47wuwp5gxhxh37kg4ac36</sha1>
    </revision>
  </page>
  <page>
    <title>Kelly's Directory</title>
    <ns>0</ns>
    <id>3119155</id>
    <revision>
      <id>745141262</id>
      <parentid>745140482</parentid>
      <timestamp>2016-10-19T13:50:50Z</timestamp>
      <contributor>
        <username>Nick Cooper</username>
        <id>1982981</id>
      </contributor>
      <comment>Rolling back to pre-vandalism version.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6758" xml:space="preserve">'''Kelly's Directory''' (or more formally, the '''Kelly's, Post Office and Harrod &amp; Co Directory''') was a [[trade directory]] in the United Kingdom that listed all businesses and tradespeople in a particular city or town, as well as a general directory of postal addresses of local [[gentry]], landowners, charities, and other facilities.  In effect, it was a Victorian version of today's [[Yellow Pages]].&lt;ref&gt;{{cite web|url=http://www.cottinghamhistory.co.uk/Directories.htm|title=Cottingham History|accessdate=11 July 2010}}&lt;/ref&gt;  Many reference libraries still keep their copies of these directories, which are now an important source for historical research.

==Origins==
The eponymous originator of the directory was [[Frederic Festus Kelly]].  In 1835 or 1836 he became chief inspector of letter-carriers for the inland or general post office, and took over publication of the Post Office London Directory, whose copyright was in private hands despite its semi-official association with the post office, and which Kelly had to purchase from the widow of his predecessor.

He founded Kelly &amp; Co. and he and various family members gradually expanded the company over the next several decades, producing directories for an increasing number of UK [[county|counties]] and buying out or putting out of business various competing publishers of directories.&lt;ref name="pollard"/&gt;&lt;ref&gt;http://www.huthwaite-online.net/hucknall/gazetteers/&lt;/ref&gt;

Other publications followed, including the ''Handbook to the Titled, Landed and Official Classes'' (1875) and ''Merchants, Manufacturers and Shippers'' (1877). In 1897, Kelly &amp; Co Ltd became '''Kelly&#8217;s Directories Ltd.'''&lt;ref name="lg"/&gt;  This name stuck for another 106 years before being renamed Kellysearch in 2003 to reflect its focus away from hard copy directories and towards an Internet-based product search engine.

The front cover of a Kelly's Directory sometimes stated "Kelly's Directories Ltd., established 1799",&lt;ref&gt;{{cite web|title=Trade Directories|work=Stella &amp; Rose's Books|url=http://www.stellabooks.com/articles/trade_directories.php|accessdate=2011-03-28}}&lt;/ref&gt; however this was based on the date of issue of the first Post Office London Directory by an earlier inspector of letter carriers several decades before Kelly's involvement with that publication.&lt;ref name="jenorton" /&gt;

== Kellysearch ==
For a short time, Kelly's existed online as [http://www.kellysearch.co.uk/ Kellysearch (broken link)], a directory similar to the online [[Yellow Pages]]. Kellysearch.com was established in Boston in 2004. It was in many different languages and introduced a fully searchable online-catalogue library and product [[News release|press release]] section.

The old editions of the Kelly&#8217;s Directories are seen as highly collectable by many and have also become a useful reference tool for people tracing the history of local areas (with the ancient data now available to buy on CD Rom from many entrepreneurial sources for this purpose.)  Every edition of the Kelly&#8217;s Directory ever published is held in the [[Guildhall Library]]&lt;ref&gt;[http://www.cityoflondon.gov.uk/things-to-do/archives-and-city-history/guildhall-library/Documents/8-trade-directories-at-guildhall-library.pdf Trade directories and telephone books at Guildhall Library]&lt;/ref&gt; in [[London]].

==References==
{{reflist | refs=

&lt;ref name="jenorton"&gt;{{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | author=Jane Elizabeth Norton | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293&#8211;299 | quote=The Post Office London Directory was started by two inspectors of the Inland letter-carriers called Ferguson and Sparkes&#8230; A third inspector, called B. Critchett, joined the enterprise in 1803 and later it was carried on by Critchett alone, then by Critchett and Woods, and then again by Critchett alone until his death in 1835. ''[sic; he died 18 September 1836]''}}&lt;!-- Library (1966) s5-XXI(4): 293-299 --&gt;&lt;/ref&gt;

&lt;ref name="pollard"&gt;{{cite book | title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83&#8211;84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785 | quote=The first directories of counties outside London were published by Kelly in 1845; and during the next sixteen years the series was extended throughout England. In 1892 Kelly's Directories Ltd. acquired the majority of shares in [[Isaac Slater]] Ltd. [...]; and the firm of [[White's Directories|William White]] of Sheffield [...] was absorbed in 1898.}}&lt;/ref&gt;

&lt;ref name="lg"&gt;{{LondonGazette|issue=26876|date=23 July 1897|startpage=4149}}&lt;/ref&gt;

}}

==Bibliography==
*{{cite book| title=Guide to the national and provincial directories of England and Wales, excluding London, published before 1856 | author=Jane Elizabeth Norton | year=1950 | edition=1984 reprint | publisher=Offices of the Royal Historical Society | isbn=0-86193-102-5}} (original edition: ISBN 0-901050-15-6)
*{{cite book| title=The development and growth of city directories| author=A. V. Williams| year=1913 | publisher=Williams directory co.}}
*{{cite book| title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83&#8211;84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785}}
*{{cite book| title=The directories of London, 1677-1977| author=Peter J. Atkins | publisher=Cassell and Mansell | year=1990
| isbn=0-7201-2063-2}}

==External links==
{{Commons category|Kelly's Directory}}
* [http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4/hd/ Historical Directories] has extensive online versions of old editions for England and Wales
* [http://forebears.co.uk/news/kellys-directories-project-complete#kellys Forebears] has transcriptions of one edition for each county
* {{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | last=Norton | first=Jane Elizabeth | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293&#8211;299}}
* {{cite journal | journal=The London Journal | title=The Compilation and Reliability of London Directories | last=Atkins | first=Peter J. | volume=14 | issue=1 |date=May 1989 | pages=17&#8211;28 | publisher=Maney Publishing | issn=0305-8034 | url=http://www.ingentaconnect.com/content/maney/ldn/1989/00000014/00000001/art00002 | doi=10.1179/ldn.1989.14.1.17}}

{{Reed Elsevier}}

[[Category:Directories]]
[[Category:Waltham, Massachusetts]]</text>
      <sha1>gjgessssei1djzcb3x3cgkg1755x2lf</sha1>
    </revision>
  </page>
  <page>
    <title>CBD Media</title>
    <ns>0</ns>
    <id>4859787</id>
    <revision>
      <id>762771687</id>
      <parentid>743397455</parentid>
      <timestamp>2017-01-30T17:27:10Z</timestamp>
      <contributor>
        <username>ColumnInch</username>
        <id>29554328</id>
      </contributor>
      <comment>removed dead links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1940" xml:space="preserve">{{Infobox company |
 name = CBD Media LLC|
 logo = [[Image:CBD Media logo.jpg]]|
 type = Subsidiary of [[The Berry Company]]|
 slogan = |
 foundation = |
 location = |
 industry = [[Telephone directory]]|
 parent = [[Spectrum Equity]], etc. (2002-2007)&lt;br&gt;[[The Berry Company|Local Insight Media/Berry]] (2007-present)|
 products = Print Yellow Pages, Online Yellow Pages ads|
}}
'''CBD Media LLC''' (formerly '''Cincinnati Bell Directory''') is a division of Local Insight Media that publishes telephone directories under the [[Cincinnati Bell]] name. The company was created in 2002 following the sale of Cincinnati Bell Directory to a consortium led by [[Spectrum Equity]].

CBD Media publishes the '''Cincinnati Bell Yellow Pages''', which consists of 15 directories, published under the "Real Pages" name. CBD Media also operates [http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com], the [[electronic yellow pages]] directory for [[Cincinnati Bell]].

The company was acquired by Local Insight Media Holdings in 2007.&lt;ref&gt;[http://www.spectrumequity.com/investments/investment?Id=1289 Spectrum Equity | Investments]&lt;/ref&gt; Local Insight Media owned [[Local Insight Yellow Pages]], the former directory division of [[Windstream]]. In 2009, Local Insight acquired The Berry Company from [[AT&amp;T]], and changed its own name to '''The Berry Company LLC'''.

==See also==
*[[Engels Maps]]

==External links==
*[http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com]

==References==
{{reflist}}

{{Telephone directory publishers in the United States}}
{{Cincinnati Bell}}

[[Category:Advertising agencies of the United States]]
[[Category:Directories]]
[[Category:Media in Cincinnati]]
[[Category:Publishing companies established in 2002]]
[[Category:Cincinnati Bell]]
[[Category:Publishing companies of the United States]]
[[Category:Companies based in Cincinnati]]
[[Category:2002 establishments in Ohio]]</text>
      <sha1>kp5qut5mh3izmjm5txw0pr36y9newhb</sha1>
    </revision>
  </page>
  <page>
    <title>Subcontractors Register</title>
    <ns>0</ns>
    <id>6401934</id>
    <revision>
      <id>442014646</id>
      <parentid>429883439</parentid>
      <timestamp>2011-07-29T09:52:19Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes (category with space) + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (7796)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="931" xml:space="preserve">[[Image:Subcontractors Register.jpg|thumb|1937 edition of the ''Subcontractors Register'']] 

The '''Subcontractors Register for the Allied Building Trades''' was a directory of [[subcontractors]] for the [[New York City]] area, listing companies by their trade. It was published by the "Society of the Allied Building Trades, Inc." and was published by Joseph O'Malley (1893&#8211;1985) who was later joined by his nephew, [[Walter Francis O'Malley]], as editor. The 1942 version calls itself: "A Classified List for the Allied Building Trades of Sub-Contractors, Material Dealers &amp; Manufacturers, General Contractors &amp; Builders, Architects - Engineers, Real Estate Management Firms".

==External links==
*[http://www.findagrave.com/cgi-bin/fg.cgi?page=gr&amp;GSln=o'malley&amp;GSmid=46580804&amp;GRid=7768640&amp; Findagrave: Joseph O'Malley]

[[Category:O'Malley family]]
[[Category:Directories]]
[[Category:History of New York City]]


{{US-stub}}</text>
      <sha1>bvnf9h2fx093jwuypir7trb4wm7usrv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Personal information managers</title>
    <ns>14</ns>
    <id>1444957</id>
    <revision>
      <id>548049631</id>
      <parentid>530413802</parentid>
      <timestamp>2013-03-31T22:09:17Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 11 langlinks, now provided by Wikidata on [[d:Q9073404]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="171" xml:space="preserve">{{Cat main|personal information manager}}

[[Category:Directories]]
[[Category:Planning]]
[[Category:Personal life|Information managers]]
[[Category:Application software]]</text>
      <sha1>r5petieioqas869hwq5t5gql4uuk2pq</sha1>
    </revision>
  </page>
  <page>
    <title>Thacker's Indian Directory</title>
    <ns>0</ns>
    <id>7719940</id>
    <revision>
      <id>741232778</id>
      <parentid>676469378</parentid>
      <timestamp>2016-09-26T06:08:29Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1687" xml:space="preserve">{{italic title}}
{{Refimprove|date=September 2014}}
'''''Thacker, Spink &amp; Co.''''' was a well-known [[Kolkata]] publishing company. ''Thacker's Bengal Directory'' was published from 1864 to 1884 and covered the [[Bengal Presidency]] &#8211; which included the present day [[Myanmar]] and [[Bangladesh]]. From 1885 the ''Directory'' covered the whole of [[British India]] and was renamed '''''Thacker's Indian Directory'''''.  It was later owned by [[Kameshwar Singh|Maharaja of Darbhanga]].&lt;ref&gt;{{cite book|title=Appendices|date=1982|publisher=India. Second Press Commissior Controller of Publications|pages=266, 343|url=https://books.google.com/books?id=tBwuAAAAMAAJ&amp;q=darbhanga+Thacker+Spink&amp;dq=darbhanga+Thacker+Spink&amp;hl=en&amp;sa=X&amp;ei=Ul0sU5_BH46zrgfjsoCYBw&amp;ved=0CEkQ6AEwBQ}}&lt;/ref&gt;   It continued to be published until 1960.

The directory was essentially an [[almanac]] which listed British and Foreign Merchants and Manufacturers, Commercial Industries, Army, railway and government departments and office holders, European residents, and separately, prominent non-European residents.   Earlier editions of ''Thacker'' had street directories of major cities, such as Kolkata and [[Yangon]], together with the name of the residents of each house.

Similar directories published included:
*''Thacker's Bombay Directory'', city and island (together with a directory of the chief industries of Bombay, etc.);
*Thacker's medical directory of India, Burma and Ceylon;
*''Thacker's Directory of the Chief Industries of India, Burma and Ceylon''.

== References ==
&lt;references /&gt;

[[Category:Almanacs]]
[[Category:Directories]]
[[Category:Books about British India]]
[[Category:Bengal Presidency]]</text>
      <sha1>7cu95oxwtc2zktfnjthfu6hxru1rr82</sha1>
    </revision>
  </page>
  <page>
    <title>Boston Directory</title>
    <ns>0</ns>
    <id>25348863</id>
    <revision>
      <id>743306723</id>
      <parentid>743305496</parentid>
      <timestamp>2016-10-09T01:46:49Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category 20th century in Boston, Massachusetts to [[:Category:20th century in Boston]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 September 6]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28517" xml:space="preserve">{{italic title}}
[[Image:1807 BostonDirectory title page.png|100px|thumb|1807 Boston Directory [[title page]]]]
'''''The Boston Directory''''' of  [[Boston]], [[Massachusetts]], was first published in 1789. It contained "a list of the merchants, mechanics, traders, and others, of the town of Boston; in order to enable strangers to find the residence of any person." Also included were listings for public officials, doctors, bank directors, and firemen.&lt;ref&gt;Boston Directory. 1789.&lt;/ref&gt; The directory was issued annually after 1825; previously it had appeared irregularly.

The number of listings in each directory reflected fluctuations in the population size of Boston. In 1789, the directory included some 1,474 listings; by 1875, there were 126,769.&lt;ref name="auto"&gt;Advertisement for Boston Directory. Boston Almanac, 1875.&lt;/ref&gt;

Publishers included John Norman (1789); John West (1796-1803); Edward Cotton (1805-1818); Charles Stimpson (1820-1846); George Adams (1846-1857);&lt;ref&gt;{{citation |url=https://books.google.com/books?id=Ors-AAAAYAAJ&amp;pg=PA87 |year=1866 |title=New England Historical &amp; Genealogical Register }}&lt;/ref&gt; Adams, Sampson &amp; Co. (1858-1865); Sampson, Davenport &amp; Co. (1865-1884); Sampson, Murdock &amp; Co. (1885-1903); Sampson &amp; Murdock Co. (1904-ca.1930); [[R.L. Polk &amp; Co.]] (1944-ca.1980).&lt;ref name="auto"/&gt;&lt;ref&gt;{{cite web|url=http://www.worldcat.org/oclc/228685309|title=The Boston directory ... including all localities within the city limits, as Allston, Brighton, Charlestown, Dorchester, Hyde Park, Roslindale, Roxbury, West Roxbury ...|work=worldcat.org}}&lt;/ref&gt;
{{TOC right}}

==Boston Directories==

===18th century===
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1789
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectory00sampgoog#page/n10/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1796
| [https://books.google.com/books?id=CJFIAAAAYAAJ&amp;client=safari&amp;pg=RA1-PA215#v=onepage&amp;q=&amp;f=false reprint via Google Books, p.&amp;nbsp;215-302]
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/1 via Boston Athen&#230;um]
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1798
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/2 via Boston Athen&#230;um]
|}

===19th century===

====1800-1829====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1800
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/3 via Boston Athen&#230;um]
|-
| Boston Directory
| John West
| 1803
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/4 via Boston Athen&#230;um]
|-
| Boston Directory
| Edward Cotton
| 1805
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| [https://archive.org/stream/bostondirectory00inbost#page/n9/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Edward Cotton
| 1806
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/5 via Boston Athen&#230;um]
|-
| Boston Directory
| Edward Cotton
| 1807
| &lt;!-- GOOGLE --&gt;
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Edward Cotton
| 1809
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/7 via Boston Athen&#230;um]
|-
| Boston Directory
| Edward Cotton
| 1810
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/8 via Boston Athen&#230;um]
|-
| Boston Directory
| Edward Cotton
| 1813
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/9 via Boston Athen&#230;um]
|-
| Boston Directory
| Edward Cotton
| 1816
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/10 via Boston Athen&#230;um]
|-
| Boston Directory
| Edward Cotton
| 1818
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/11 via Boston Athen&#230;um]
|-
| Boston Directory
| Frost and Stimpson
| 1820
| &lt;!-- GOOGLE --&gt;
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Frost and Stimpson
| 1821
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/12 via Boston Athen&#230;um]
|-
| Boston Directory
| Frost and Stimpson
| 1822
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/15 via Boston Athen&#230;um]
|-
| Boston Directory
| Frost and Stimpson
| 1823
| [https://books.google.com/books?id=nY4vAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Frost and Stimpson
| 1825
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| [https://archive.org/stream/bostondirectorys1825bost via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Frost and Stimpson
| 1826
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/17 via Boston Athen&#230;um]
|-
| Boston Directory
| Hunt, Stimpson, and Frost
| 1827
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/18 via Boston Athen&#230;um]
|-
| Boston Directory
| Hunt and Stimpson
| 1828
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/19 via Boston Athen&#230;um]
|-
| Boston Directory
| Charles Stimpson, Jr.
| 1829
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/20 via Boston Athen&#230;um]
|-
|}

====1830-1849====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1830
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/41 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1831
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/stimpsonsbostond3132adam#page/n29/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1832
| [https://books.google.com/books?id=raQtAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1833
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/27 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1834
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| [https://archive.org/stream/bostondirectory01bost#page/n5/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1835
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectory03bost#page/n5/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1836
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| [https://archive.org/stream/stimpsonsbostond1836adam#page/n21/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1837
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/34 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1838
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/35 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1839
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/40 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1840
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/39 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1841
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/37 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1842
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/36 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1843
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/38 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1844
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/45 via Boston Athen&#230;um]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1845
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/44 via Boston Athen&#230;um]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1846
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/43 via Boston Athen&#230;um]
|-
| Adams's New Directory of the City of Boston
| George Adams
| 1846-47
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/25 via Boston Athen&#230;um]
|-
| Adams's Boston Directory
| French and Stimpson
| 1847-48
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/24 via Boston Athen&#230;um]
|-
| Boston Directory
| French and Stimpson
| 1848-49
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| [https://archive.org/stream/bostondirectory4849bost#page/n7/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1849-50
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| [https://archive.org/stream/bostondirectory00bost#page/n7/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
|}

====1850-1869====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Directory of the City of Boston
| George Adams
| 1850
| [https://books.google.com/books?id=UHDPAAAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1851
| [https://books.google.com/books?id=C6UqAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1852
| [https://books.google.com/books?id=2tsCAAAAYAAJ via Google Books]
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1853
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/48 via Boston Athen&#230;um]
|-
| Boston Directory
| George Adams
| 1854
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/81 via Boston Athen&#230;um]
|-
| Boston Directory
| George Adams
| 1855
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/82 via Boston Athen&#230;um]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| George Adams
| 1856
| [https://books.google.com/books?id=zYMqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1857
| [https://books.google.com/books?id=nYIqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1858
| [https://books.google.com/books?id=En4qAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1859
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1860
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/87 via Boston Athen&#230;um]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1861
| [https://books.google.com/books?id=hHwqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/singleitem/collection/p16057coll32/id/93/rec/57 via Boston Athenaeum]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1862
| [https://books.google.com/books?id=tH4qAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/91 via Boston Athen&#230;um]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1863
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/92 via Boston Athen&#230;um]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1864
| [https://books.google.com/books?id=8IEqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1865
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/95 via Boston Athen&#230;um]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1866
| [https://books.google.com/books?id=_A5FAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/96 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1867
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/97 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1868
| [https://books.google.com/books?id=SFwJAQAAIAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1869
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/69 via Boston Athen&#230;um]
|-
|}

====1870-1889====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1870
| [https://books.google.com/books?id=GytFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/98 via Boston Athen&#230;um]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1871
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/99 via Boston Athen&#230;um]
|-
| Boston Commercial Directory
| Wentworth &amp; Co.
| 1871
| [https://books.google.com/books?id=xfACAAAAYAAJ via Google Books]
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1872
| &lt;!-- GOOGLE --&gt;
| [http://babel.hathitrust.org/cgi/pt?id=hvd.32044092998012;view=1up;seq=19 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/65 December] supplement via Boston Athen&#230;um&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1873
| [https://books.google.com/books?id=NqHNAAAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/67 November], [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/66 December] supplements via Boston Athen&#230;um
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1874
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/108 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1875
| [https://books.google.com/books?id=EC5FAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/104 via Boston Athen&#230;um]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1876
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/105 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1877
| [https://books.google.com/books?id=RTBFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/103 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1878
| &lt;!-- GOOGLE --&gt;
| [http://hdl.handle.net/2027/uc1.c047888986 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/110 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1879
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/111 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1880
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/107 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1881
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/109 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1882
| [https://books.google.com/books?id=NSFFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/112 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1883
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/113 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1884
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/114 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1885
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/116 via Boston Athen&#230;um]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1886
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/118 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1887
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/117 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1888
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/119 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1889
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/121 via Boston Athen&#230;um]
|-
|}

====1890-1899====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1890
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/123 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1891
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/124 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1892
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/125 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1893
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/126 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1894
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/127 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1895
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/128 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1896
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/129 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1897
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/130 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1898
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/131 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1899
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/132 via Boston Athen&#230;um]
|-
|}

===20th century===

====1900-1949====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1900
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/133 via Boston Athen&#230;um]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1905
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson &amp; Murdock Co.
| 1916
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostonmassachuse1916112samp#page/n9/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Register and Business Directory
| Sampson &amp; Murdock Co.
| 1922
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostonregisterbu1922bost#page/n13/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Sampson &amp; Murdock Co.
| 1925
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
|}

====1950-1999====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1955
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi00bost#page/n7/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1956
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi56bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1959
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi002bost via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1961
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi11961bost#page/n3/mode/2up v.1], [https://archive.org/details/bostondirectoryi261bost v.2] via Internet Archive
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1962
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi162bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1965
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi11965bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1966
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi1966bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1969
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi169bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1970
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi170bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
|}

==See also==
* ''[[Boston Almanac|Boston Almanac and Business Directory]]''
* ''[[Boston Almanac|Boston Register and Business Directory]]''
* ''[[Massachusetts Register]]''

==References==
{{reflist}}

==Further reading==
* [https://books.google.com/books?id=ALIUAAAAYAAJ New England historical and genealogical register]. Oct. 1862; p.&amp;nbsp;387+
* [https://books.google.com/books?id=CJFIAAAAYAAJ Report of the record commissioners of the city of Boston], Volume 10. Rockwell and Churchill, 1886; p.&amp;nbsp;163+
* {{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |title = The Development and Growth of City Directories |publication-date = 1913 |chapter=Boston, Massachusetts |chapterurl=http://hdl.handle.net/2027/nyp.33433082423645?urlappend=%3Bseq=61 }}

==External links==
* HathiTrust. [https://catalog.hathitrust.org/Record/010363295 1805 etc]; [https://catalog.hathitrust.org/Record/000499337 1849-1883]
* [http://www.damrellsfire.com/cgi-bin/directory_search.pl damrellsfire.com]
* [http://cdm.bostonathenaeum.org/cdm/landingpage/collection/p16057coll32 Boston Athen&#230;um: The Boston Directory 1789-1900 (Ongoing Project), Digital Collection].

[[Category:History of Boston]]
[[Category:Directories]]
[[Category:Publications established in 1789]]
[[Category:18th century in Boston]]
[[Category:19th century in Boston]]
[[Category:20th century in Boston]]
[[Category:1789 establishments in Massachusetts]]</text>
      <sha1>0hgvn6wnx54cj2t87zsuak717u80pli</sha1>
    </revision>
  </page>
  <page>
    <title>Sources (website)</title>
    <ns>0</ns>
    <id>20263150</id>
    <revision>
      <id>708318686</id>
      <parentid>708318646</parentid>
      <timestamp>2016-03-04T22:18:44Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <comment>++</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13982" xml:space="preserve">{{Use dmy dates|date=May 2014}}
{{More footnotes|date=November 2011}}
'''Sources''' is a [[web portal]] for journalists, freelance writers, editors, [[authors]] and [[researchers]], focusing especially on human sources: [[expert]]s and spokespersons who are prepared to answer [[Reporter]]s' questions or make themselves available for on-air [[interview]]s.

==Structure==
The Sources website is built around a [[Controlled vocabulary|controlled-vocabulary]] subject index comprising more than 20,000 topics. This [[Subject indexing|subject index]] is underpinned by an 'Intelligent Search' system which helps reporters focus their searches by suggesting additional subjects related to their search terms. For example, a search for "cancer" will suggest terms such as "chemotherapy", "melanoma", "oncology", "radiation therapy", "tobacco diseases" and "tumours", as well as topics that actually contain the word "cancer".

Each topic reference links in turn to experts and spokespersons on that topic, with profiles describing their expertise and, where relevant, their approach to the issue, along with their phone numbers and other contact information. Sources includes listings for universities and research institutes, non-profit associations and NGOs, government and public sector bodies, businesses, and individuals including academics, public speakers, and consultants.

The subject index and the search menus are being translated into French, Spanish and German to make Sources more of an international resource.

==History==

===Print supplement===
Based in Canada, Sources was founded in 1977 as a print directory for reporters, editors, and story producers. It was first published as a supplement to ''Content'' magazine, an influential and controversial magazine of journalism criticism. ''Content'', founded by Dick MacDonald in 1970 and published by [[Barrie Wallace Zwicker|Barrie Zwicker]] after MacDonald's death in 1974, frequently took journalists to task for always relying on the same narrow range of sources representing the same conventional points of view for their stories. Zwicker and MacDonald argued in ''Content'' and in their book ''The News: Inside the Canadian Media''&lt;ref&gt;MacDonald, Dick; Zwicker, Barrie. ''The News: Inside the Canadian Media''. Deneau. 1982. ISBN 0-88879-053-8&lt;/ref&gt; that there was a &#8220;terrible sameness&#8221; in the media&#8217;s coverage of many important issues, and a shutting out of other, potentially valuable, perspectives and sources of information.

Zwicker decided to do something about the problem, and in summer 1977, ''Content'' published its first directory issue, called Sources. Billed as &#8220;A Directory of Contacts for Editors and Reporters in Canada&#8221;, Sources listed &#8220;information officers, public relations officers, media relations and public affairs people, and other contacts for groups, associations, federations, unions, societies, institutions, foundations, industries and companies and federal, provincial and municipal ministries, departments, agencies and boards.&#8221;&lt;ref&gt;Sources: A Directory of Contacts for Editors and Reporters in Canada. ''Content''. 1977. {{ISSN|0045-835X}}&lt;/ref&gt;

Explaining the rationale behind Sources, Zwicker said that &#8220;It&#8217;s a clich&#233; that every story has two sides. An untrue clich&#233;. Most have several. The reporter&#8217;s challenge is digging out all sides. Sources can help.&#8221;&lt;ref&gt;Sources 50. 2002. ISBN 0-920299-55-5&lt;/ref&gt; From the beginning, Zwicker saw Sources as a public service as well as a tool for journalists. He said that Sources aimed &#8220;to help promote a system of information fairness. Communications resources are equivalent to other basic needs &#8211; shelter, food, health care, for example. Everyone should have reasonable access to all.&#8221;&lt;ref name="ReferenceA"&gt;Sources 36. 1995. ISBN 0-920299-24-5&lt;/ref&gt; Therefore, he said &#8220;we attempt to provide true diversity: access to people in organizations large and small, for-profit and not-for-profit, from low-tech to high-tech, long-established to just-launched.&#8221;&lt;ref name="ReferenceA"/&gt;

Zwicker told users that &#8220;within Sources you will find both mainstream and alternative information. Some may consider alternative as off to one side, not quite up to par, more or less second hand. Here at Sources &#8216;alternative&#8217; is considered differently, considered as authentic and substantial, even if normally less accessible. The surprises, the jarring notes, the flashes of insight, the &#8216;odd takes&#8217;, the pearls of wisdom, the cries de coeur, the avant garde, tomorrow&#8217;s news, the prophesies, the unfiltered, the exciting, the elsewhere-squelched, the memorable, the eccentric, the thought-out-at-length, the unmentionable in polite company, the outrageous, the uncensored ... these are what &#8216;alternative&#8217; media offer. So far as we can, we will include the alternative with Sources. Sources&#8217; driving philosophy is flat-out informational democracy enabled by user-friendly technology. The assumption is that there is a significant fraction of Canadians who want to use and benefit from such an information resource. The assumption is that a significant fraction of Canadians want to expand their search for solutions, and deepen their understandings, rather than chant conventional wisdoms (however freshly minted) to each other.&#8221;&lt;ref name="ReferenceA"/&gt;

===Separate publications===
After a few years, Sources become so big that it could no longer fit into ''Content'' (the print directory eventually grew to more than 500 pages), and in 1981 it became an independent publication. ''Content'' itself eventually folded, but Zwicker continued to devote a substantial editorial section in Sources to coverage of topics of interest to journalists, ranging from practical topics such as grammar, style, [[fact-checking]], [[photojournalism]], [[copyright]], fees for freelancers and [[self-publishing]], to feature articles on the state of journalism and the media, to book reviews. From the early 1990s, Sources began to feature articles about online research, notably the regular feature 'Dean's Digital World'&lt;ref&gt;[Dean's Digital World &#8211; http://www.sources.com/SSR/DeansDigital.htm&lt;/ref&gt; by informatics expert Dean Tudor.

===World Wide Web===

====Content====
Sources went on the Internet in 1995 and has been expanding its online portal ever since. It continues to publish a print edition of the directory, primarily for the benefit of freelancers who use it as a source of story ideas, but is now primarily a Web-based resource.

The Sources website includes not only the Sources directory itself, but a separate government directory, Parliamentary Names &amp; Numbers; a directory of the media, Media Names &amp; Numbers; and The Sources HotLink  [http://www.hotlink.ca (www.hotlink.ca)], which features articles about media relations and public relations. Also on the site is [http://www.sources.com/Fandf/Index.htm Fame and Fortune], a directory of awards, prizes, and scholarships available to writers and journalists, and a portal linked into the online archive of [[Connexions (Information Sharing Services)|Connexions]], a library of documents related to alternatives and social justice.

The site also houses Sources Select Resources,&lt;ref&gt;Sources Select Resources &#8211; http://www.sources.com/SSR.htm&lt;/ref&gt; a large library of articles and reviews about journalism and the media, spanning a period of more than 30 years.

====Controversy====
While much of the editorial content has focused on the nitty-gritty of writing, editing and research, Sources has also regularly published articles that have sparked controversy on topics such as censorship and [[media bias]]. One campaign waged by Zwicker and others challenged the [[journalism ethics|ethics]] of journalists accepting free gifts from the people they are supposed to cover. This campaign eventually led Canadian managing editors to agree among themselves that their newspapers would not accept free tickets from travel agencies, resorts, and hotels.

A series of articles by Zwicker on "War, Peace, and the Media"&lt;ref&gt;Zwicker, Barrie. ''War, Peace and the Media''. Sources. 1983, 1985&lt;/ref&gt; (later collected and published as a booklet) provoked a furor from readers upset by its criticisms of how the media cover [[United States foreign policy|U.S. foreign policy]]. As Zwicker put it in a publisher's letter in the next issue, the "reaction ranged from high praise to angry denunciation." The ''[[Toronto Sun]]'' newspaper devoted three stories to the series. The columnist Claire Hoy was left "trembling with rage" and the editor [[Peter Worthington]] felt "outraged" and a lead editorial denounced Zwicker.

Other controversial articles included one by Wendy Cukier on the public relations battle surrounding proposed [[Gun politics in Canada|gun control]] legislation, which drew the ire of the gun lobby.&lt;ref&gt;Cukier, Wendy. "Anatomy of the Gun Control Debate". Sources. 1996 &#8211; http://www.sources.com/SSR/Docs/PNN5-1-GunControl.htm&lt;/ref&gt; Ulli Diemer, who succeeded Zwicker as publisher in 1999, came under attack from the [[Fraser Institute]] for his article "Ten Health Care Myths: Understanding Canada&#8217;s Medicare Debate&#8221;, in which he argued that opponents of [[public health care]] were spreading [[Misinformation|mis-information]] designed to mislead and frighten the public.&lt;ref&gt;Diemer, Ulli. 'Ten Health Care Myths: Understanding Canada&#8217;s Medicare Debate&#8217;. Sources. 1995. &#8211; http://www.diemer.ca/Docs/Diemer-TenHealthCareMyths.htm&lt;/ref&gt;

====New resources====
In keeping with its mandate of encouraging a wide diversity of points of view in the media, Sources has added extra resources over time to help organizations and individuals to be heard. These include a calendar of events open to the media&lt;ref&gt;Sources Calendar &#8211; http://calendar.sources.com&lt;/ref&gt; and a [[news release]] service which Sources members can use to distribute their statements and communiques via online posting and [[RSS]]. The releases are also subject indexed and integrated into the overall search structure for information on the Sources site.

==See also==
* [[Barrie Wallace Zwicker|Barrie Zwicker]]

== Notes ==
{{reflist|33em}}

==References==
{{refbegin|33em}}
* Basch, Reva. ''Secrets of the Super Net Searchers: The Reflections, Revelations, and Hard-won Wisdom of 35 of the World&#8217;s Top Internet Researchers''. Pemberton Press. 1996. ISBN 0-910965-22-6
* Berkman, Robert. ''The Skeptical Business Searcher: The Information Advisor&#8217;s Guide to Evaluating Web Data, Sites and Sources''. Information Today, 2004. ISBN 0-910965-66-8
* Bonner, Allan. ''Media Relations''. Briston House. 2003. ISBN 1-894921-00-3
* Carney, William Wray. ''In the News The Practice of Media Relations in Canada''. University of Alberta Press', 2002. ISBN 0-88864-382-9
* Comber, Mary Anne; Mayne, Robert S. ''The Newsmongers: How The Media Distort the Political News''. 1987. McClelland &amp; Stewart
* Cormack, Paul G.; Shewchuk, Murphy (eds.) ''The Canadian Writers&#8217; Guide''. 13th Edition. Canadian Authors Association. Fitzhenry &amp; Whiteside, 2003. ISBN 1-55041-740-1
* Hackett, Robert A.; Gruneau, Richard. ''The Missing News: Filters and Blind Spots in Canada&#8217;s Press''. Newswatch Canada. Canadian Centre for Policy Alternatives &amp; Garamond Press, 2000
* Hackett, Robert A. ''News and Dissent: The Press and The Politics of Peace in Canada''. 1993. Ablex.
* Hackett, Robert A.; Zhao, Yuezhi. ''Sustaining Democracy? Journalism and the Politics of Objectivity''. Garamond Press. 1998. ISBN 1-55193-013-7
* Kashmeri, Zuhair. ''The Gulf Within: Canadian Arabs, Racism, &amp; The Gulf War''. James Lorimer. 1991
* MacDonald, Dick; Zwicker, Barrie. ''The News: Inside the Canadian Media''. Deneau. 1982. ISBN 0-88879-053-8
* Mann, Thomas. ''The Oxford Guide to Library Research''. Oxford University Press. 1998. ISBN 0-19-512313-1
* Manson, Katherine; Hackett, Robert; Winter, James; Gutstein, Donald; Gruneau, Richard (eds.) ''Blindspots in the News? Project Censored Canada Yearbook''. Project Censored Canada. 1995.
* McGuire, Mary; Stilborne, Linda; McAdams, Melinda; Hyatt, Laurel. ''The Internet Handbook for Writers, Researchers, and Journalists''. Trifolium Books. 1997, 2002. ISBN 1-895579-17-1
* Miljan, Lydia; Cooper, Barry Cooper. ''Hidden Agendas: How Journalists Influence the News''. University of British Columbia Press. 2003. ISBN 0774810203
* Miller, John. ''Yesterday&#8217;s News: Why Canada&#8217;s Daily Newspapers are Failing Us''. Fernwood Publishing, 1999
* Ouston, Rick. ''Getting the Goods: Information in B.C.: How to Find It, How to Use It''. New Star Books, 1990
* Patriquin, Larry. ''Inventing Tax Rage: Misinformation in the National Post''. Fernwood Publishing, 2004. ISBN 1-55266-146-6
* Soderlund, Walter C.; Hildebrandt, Kai (eds.) ''Canadian Newspaper Ownership in the Era of Convergence: Rediscovering Social Responsibility''. University of Alberta Press. 2005, ISBN 0-88864-439-6
* Tudor, Dean. ''Finding Answers: Approaches to Gathering Information''. McClelland &amp; Stewart Inc., Toronto. 1993.
* Ward, Stephen J.A. ''The Invention of Journalism Ethics: The Path to Objectivity and Beyond''. McGill-Queen&#8217;s University Press. 2004. ISBN 0-7735-2810-5
* Winter, James. ''Media Think''. Black Rose Books. 2002. ISBN 1-55164-054-6
* Zwicker, Barrie. ''War, Peace and the Media''. Sources. 1983, 1985
{{refend}}

==External links==
* {{official website|http://www.sources.com/}}
** [http://www.sources.com/SSR.htm Sources Select Resources]
** [http://www.sources.com/News.htm Sources Select News]
** [http://calendar.sources.com Sources Calendar]
** [http://www.sources.com/Fandf/Index.htm Fame &amp; Fortune]
* [http://www.hotlink.ca The Sources HotLink]
* [http://www.connexions.org Connexions Information Sharing Services]

{{DEFAULTSORT:Sources (Website)}}
[[Category:Directories]]
[[Category:Journalism organizations]]
[[Category:Knowledge markets]]
[[Category:Online databases]]
[[Category:Web directories]]
[[Category:Websites]]</text>
      <sha1>lm7up8nb1jub2qhrz5pdjd5ifva7yqd</sha1>
    </revision>
  </page>
  <page>
    <title>Mobile directory</title>
    <ns>0</ns>
    <id>27409630</id>
    <revision>
      <id>605743830</id>
      <parentid>605743412</parentid>
      <timestamp>2014-04-25T12:57:30Z</timestamp>
      <contributor>
        <username>Gilliam</username>
        <id>506179</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/117.222.194.201|117.222.194.201]] ([[User talk:117.222.194.201|talk]]) to last version by Yobot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="530" xml:space="preserve">{{multiple issues|
{{Unreferenced|date=October 2010}}
{{orphan|date=October 2010}}
}}

A '''mobile directory''' is a collection of subscriber details of a [[mobile phone]] [[Mobile phone companies|operators]]. Generally, the mobile telephony operators do not publish a mobile [[directory (databases)|directory]]. Some third party [[websites]] offer mobile directory facility through [[Reverse telephone directory|reverse search]].

[[Category:Telephone numbers]]
[[Category:Directories]]


{{telephonenumber-stub}}
{{mobile-stub}}</text>
      <sha1>65ljng79q2pcgj44aiyfbtnijj93j79</sha1>
    </revision>
  </page>
  <page>
    <title>Adelskalender (directory)</title>
    <ns>0</ns>
    <id>24177992</id>
    <revision>
      <id>649375493</id>
      <parentid>606836543</parentid>
      <timestamp>2015-03-01T13:30:39Z</timestamp>
      <contributor>
        <username>Nikolaj Christensen</username>
        <id>2089102</id>
      </contributor>
      <minor />
      <comment>interwiki: removed da (which refers to the adelskalender in skating)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="570" xml:space="preserve">'''Adelskalender''' ({{lang-de|Directory of Nobility}}) is a comprehensive directory of the nobility of a country or area. The best known such directory is the German [[Almanach de Gotha]] ("The Gotha") and its successor, the [[Genealogisches Handbuch des Adels]].

[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]
[[Category:Directories]]


{{royal-bio-book-stub}}
{{bio-dict-stub}}

[[cs:Adelskalender]]
[[de:Adelskalender]]
[[nl:Adelskalender]]
[[no:Adelskalender]]
[[fi:Aateliskalenteri]]
[[sv:Adelskalender]]</text>
      <sha1>393vpub7bamsvnlhtw9sp0u1s38nt1l</sha1>
    </revision>
  </page>
  <page>
    <title>Major Information Technology Companies of the World</title>
    <ns>0</ns>
    <id>33232338</id>
    <revision>
      <id>604710562</id>
      <parentid>593209219</parentid>
      <timestamp>2014-04-18T08:48:56Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo/[[WP:AWB/GF|general]] fixing, replaced: published published &#8594; published using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="859" xml:space="preserve">The '''Major Information Technology Companies of the World''' is a directory of [[information technology]] companies published by [[Graham &amp; Whiteside]] annually since 1997. The directory contains over 8000 companies.&lt;ref&gt;{{cite web|url=http://www.gale.cengage.com/servlet/BrowseSeriesServlet?region=9&amp;imprint=000&amp;cf=ps&amp;titleCode=MITCW|title=Major Information Technology Companies of the World|publisher=Cengage|accessdate=27 September 2011}}&lt;/ref&gt; The directory is also available online as part of the Gale Directory Library.&lt;ref&gt;{{cite web|url=http://www.gale.cengage.com/pdf/facts/GML25909_MITCOW_Major_GDL.pdf|title=Major Information Technology Companies of the World|publisher=Gale|accessdate=27 September 2011|year=2009}}&lt;/ref&gt;

==See also==
*[[Corporate Technology Directory]]

==References==
{{reflist}}

[[Category:Directories]]


{{technology-stub}}</text>
      <sha1>jmwahp1oteo1dxevpk1q61w0xm0y8q3</sha1>
    </revision>
  </page>
  <page>
    <title>Clergy List</title>
    <ns>0</ns>
    <id>34259473</id>
    <revision>
      <id>748236582</id>
      <parentid>658979837</parentid>
      <timestamp>2016-11-07T04:07:33Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Publishers and later history */clean up; http&amp;rarr;https for [[Google Books]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4023" xml:space="preserve">The '''Clergy List''' was a professional directory of the [[Church of England]] which appeared between 1841-1917.  From the start it also covered Wales, together with more limited information relating to Scotland, Ireland, and other churches within the [[Anglican Communion]].

==Background and early contents==

An opportunity to compile and issue a new directory had been created by the effective disappearance of the earlier [[Clerical Guide or Ecclesiastical Directory]], edited by '''Richard Gilbert''', and also by the introduction of the much improved system of the [[Penny Post]]. 

The basic contents of the '''Clergy List''''s earlier editions was summarised on their title pages: 
*an alphabetical list of the clergy (or at least of those who held benefices)
*an alphabetical list of the benefices,with their post towns
*lists of the cathedral establishments
*benefices arranged under their ecclesiastical divisions
*lists of ecclesiastical preferments variously under the patronage of the Crown, the bishops, and the deans &amp; chapters, etc.

The directory was always a bit less expensive than its later rival, [[Crockford's Clerical Directory]], but not surprisingly it consequently offered considerably less in the way of biographical detail.  This was especially true in the earlier editions which offered little or no information as to previous appointments, universities attended, or lists of publications by the clergy.

==Publishers and later history==

The directory was initially published by '''Charles Cox''' at the Ecclesiastical Directory Office, [[Southampton Street, London|Southampton Street]], [[Strand, London|Strand]].   Cox &#8211; who in 1839 had taken over a periodical called the '''Ecclesiastical Gazette,''' originating during the previous year &#8211; was able to produce two separate editions during the Clergy List's inaugural year of 1841.&lt;ref name="paflin"&gt;[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article ''Shop-talk and mordant wit'', by Christopher Currie &amp; Glyn Paflin, describing the background to ''Crockford's Clerical Directory'''s first hundred editions, 6&#8211;13 December 2007&lt;/ref&gt;  Thereafter it managed to maintain annual publication right up until adverse trading conditions forced its closure as a separate volume in 1917.

Cox remained as the Clergy List's publisher for many years, but by 1881 the title had been taken over by John Hall of [[Whitehall|Parliament Street]], In 1888 it was further taken over by Hamilton, Adams &amp; Company, of London's [[Paternoster Row]].  They had earlier acquired Thomas Bosworth's '''[[Clerical Guide and Ecclesiastical Directory]]''', merging the two titles in 1889.  During the following year the combined directory was still further transferred to Kelly &amp; Company, the publishers of [[Kelly's Directories]].&lt;ref name="paflin" /&gt; 

The later volumes were considerably expanded to include much greater biographical detail &#8211; broadly comparable with Crockford &#8211; but this was not sufficient to sustain the publication in the longer term.  Over the years the number of pages also increased &#8211; ranging from around 300 in 1841 to around 700 by the 1890s. 

After 1917 the Clergy List finally merged with its long-time rival, '''Crockford's Clerical Directory'''.  At least as late as 1932 the latter continued to advertise on its preliminary pages that it "incorporated the '''Clergy List, the Clerical Guide and the Ecclesiastical Directory'''''".&lt;ref name="paflin" /&gt;

In recent years certain of the earlier editions of the Clergy List (including the first edition &lt;ref&gt;The 1841 first edition of the ''Clergy List'' may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]&lt;/&lt;/ref&gt;) have been reissued by various publishers &#8211; either on CD-ROM or in scanned format on the World Wide Web.

==References==
{{reflist}}

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]</text>
      <sha1>bck3rx49e6yg9qeuxrii56w11ohswvi</sha1>
    </revision>
  </page>
  <page>
    <title>Almanach de Bruxelles</title>
    <ns>0</ns>
    <id>36297227</id>
    <revision>
      <id>704189969</id>
      <parentid>606836690</parentid>
      <timestamp>2016-02-10T02:30:40Z</timestamp>
      <contributor>
        <username>Gioto</username>
        <id>1586414</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="844" xml:space="preserve">{{one source|date=July 2012}}
The '''''Almanach de Bruxelles''''' is a [[Belgian]] website that lists [[royal family|royal]] and [[nobility|noble]] [[dynasties]] out of [[Europe]] in the form of a database. 

It was established in 1996 and lists around 2,690 world dynasties.&lt;ref&gt;cr&#233;&#233; en 1996, est le site de r&#233;f&#233;rence des dynasties en dehors de l'Europe...2.690 dynasties, beaucoup d'entre elles introuvables ailleurs [http://www.almanach.be/about/index.htm About the ''Almanach'']&lt;/ref&gt;

==See also==
* ''[[Almanach de Gotha]]''
* ''[[Almanach de Bruxelles (defunct)]]''

==Sources==
{{reflist}}

==External links==
*{{Official|www.almanach.be}}

[[Category:Directories]]
[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]


{{website-stub}}
{{royal-bio-book-stub}}
{{bio-dict-stub}}</text>
      <sha1>1ryx4ds4e2qnskyggi36tl4hjhkpuh1</sha1>
    </revision>
  </page>
  <page>
    <title>Writers' &amp; Artists' Yearbook</title>
    <ns>0</ns>
    <id>37966541</id>
    <revision>
      <id>722552187</id>
      <parentid>632750182</parentid>
      <timestamp>2016-05-28T22:00:24Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6412" xml:space="preserve">[[File:Writers' &amp; Artists' Yearbook cover 2003.jpg|thumb|150px|2003 edition of ''Writers' &amp; Artists' Yearbook'']]
{{Italic title}}'''''Writers' &amp; Artists' Yearbook''''' is an annual directory for writers, designers, illustrators and photographers. It is published in the UK each July, with a separate version for children's writers and artists published in August. The yearbook contains some 4,500 named industry contacts updated for each edition and includes articles about getting work published.&lt;ref Name=BBC&gt;[http://news.bbc.co.uk/dna/place-lancashire/plain/A16932017 "The Writers' and Artists' Yearbook", BBC]&lt;/ref&gt;&lt;ref name="Irish Times"&gt;{{cite news|title=Essential Reading for Writers|newspaper=Irish Times|date=13 September 2003}}&lt;/ref&gt; In 2007, an associated website, known as Writers&amp;Artists, was launched.&lt;ref name="website launch"&gt;{{cite web|title=New website with free resources for writers and artists|url=http://www.publishers.org.uk/index.php?option=com_content&amp;view=article&amp;id=554:new-website-with-free-resources-for-writers-and-artists&amp;catid=80:general-news&amp;Itemid=1617|publisher=Publishers Association|accessdate=2 March 2014}}&lt;/ref&gt;

== History ==

First published in 1906, by [[A &amp; C Black|Adam &amp; Charles Black]], the original ''Writers&#8217; &amp; Artists&#8217; Yearbook'' was an 80-page booklet, costing one [[shilling]]. It gave details of seven literary agents and 89 publishers.&lt;ref Name=BBC/&gt; It has been published on an annual basis since, expanding over time to include information for illustrators and photographers.&lt;ref Name=BBC/&gt; A &amp; C Black became part of [[Bloomsbury Publishing]] in 2000, and other titles in its reference division include ''[[Who's Who (UK)|Who's Who]]'', ''[[Wisden Cricketers' Almanack|Wisden]]'' and ''[[Black's Medical Dictionary]]''.&lt;ref name="A &amp; C Black"&gt;{{cite news|last=Neill|first=Graeme|title=Coleman to leave A &amp; C Black for Magi|url=http://www.thebookseller.com/news/coleman-leave-c-black-magi.html|accessdate=2 March 2014|newspaper=The Bookseller|date=2 February 2011}}&lt;/ref&gt;
Articles offering advice first appeared in the 1914 yearbook.&lt;ref Name=BBC/&gt; Forewords have been written by, among others, [[William Boyd (writer)|William Boyd]] and [[Kate Mosse]].&lt;ref name=A&amp;U&gt;{{cite web|title=Writers' and Artists' Yearbook 2013|url=https://www.allenandunwin.com/default.aspx?page=305&amp;book=9781408157497|publisher=Allen &amp; Unwin|accessdate=2 March 2014}}&lt;/ref&gt;&lt;ref name=Bibliography&gt;{{cite web|last=Mosse|first=Kate|title=Complete Bibliography|url=http://www.katemosse.co.uk/index.php/kates-books/|publisher=Kate Mosse|accessdate=2 March 2014}}&lt;/ref&gt; Following the success of ''[[Fifty Shades of Grey]]'', a new section on writing erotic fiction &#8211; by an anonymous author &#8211; appeared in the 2014 edition.&lt;ref name="Fifty Shades"&gt;{{cite news|last=Wyatt|first=Daisy|title=Fifty Shades of Grey inspires new chapter on erotic fiction in Bloomsbury Writers' and Artists' Yearbook|url=http://www.independent.co.uk/arts-entertainment/books/news/fifty-shades-of-grey-inspires-new-chapter-on-erotic-fiction-in-bloomsbury-writers-and-artists-yearbook-8685703.html|accessdate=2 March 2014|newspaper=The Independent|date=3 July 2013}}&lt;/ref&gt;

=== Website and competitions ===

In 2007, ''Writers' &amp; Artists' Yearbook'' launched an associated website. Initially this was only accessible to anyone purchasing the print edition.&lt;ref name="website launch"/&gt; In 2009, the website was relaunched and now includes blogs from guest authors and a social networking feature that enables authors and artists to add a public profile.&lt;ref name=Bookseller&gt;{{cite news|last=Gallagher|first=Victoria|title=Writers and Artists Yearbook launches social networking|url=http://www.thebookseller.com/news/writers-and-artists-yearbook-launches-social-networking.html|accessdate=2 March 2014|date=7 August 2009}}&lt;/ref&gt; From 2013, the website featured a section focusing on [[self-publishing]], also hosting a conference on the subject in November of that year in association with [[National Novel Writing Month]].&lt;ref name=self-publish&gt;{{cite news|title=Self-published writers get online resource|url=http://www.thebookseller.com/news/self-published-writers-get-online-resource.html|accessdate=2 March 2014|newspaper=The Bookseller|date=27 September 2013}}&lt;/ref&gt;
''Writers' &amp; Artists' Yearbook'' runs an annual short story competition and has also collaborated with Bloomsbury to run a competition for aspiring crime writers.&lt;ref&gt;{{cite web|title=Writers&#8217; &amp; Artists&#8217; Yearbook 2014 Short Story Competition|url=http://www.commonwealthwriters.org/writers-and-artists-short-story-competition-2014/|publisher=Commonwealth Writers|accessdate=2 March 2014}}&lt;/ref&gt;&lt;ref name="Book Trust"&gt;{{cite web|title=Prizes|url=http://www.booktrust.org.uk/books/adults/short-stories/prizes/|publisher=Book Trust|accessdate=2 March 2014}}&lt;/ref&gt;&lt;ref name="crime competition"&gt;{{cite news|last=Williams|first=Charlotte|title=Bloomsbury launches crime story competition|url=http://www.thebookseller.com/news/bloomsbury-launches-crime-story-competition.html|accessdate=2 March 2014|newspaper=The Bookseller|date=1 March 2012}}&lt;/ref&gt;

== Sections and listings ==

The yearbook is divided into the following sections:&lt;ref Name=BBC/&gt;
* Newspapers and magazines &#8211; regional, national and overseas, [[Print syndication|syndicates]] and [[News agency|news agencies]]
* Books &#8211; regional, national and overseas, audio publishers, [[Book packaging|book packagers]] and [[Book sales club|book clubs]]
* Poetry organisations
* Television, film and radio broadcasters
* Theatre &#8211; producers
* [[Literary agent]]s 
* Art and illustration &#8211; agents, commercial studios and card and stationery publishers 
* Societies, prizes and festivals &#8211; associations and clubs, prizes and awards and [[literary festival]]s
* Digital and self-publishing
* Resources for writers &#8211; courses, libraries and writers' retreats
* Copyright and libel information
* Finance for writers and artists.

== See also ==

* ''[[Writer's Digest]]''
* ''[[Novel &amp; Short Story Writer's Market]]''

== References ==

{{reflist|2}}

== External links ==
*[http://www.writersandartists.co.uk/ Writers&amp;Artists website]

{{DEFAULTSORT:Writers' and Artists' Yearbook}}
[[Category:Directories]]
[[Category:1906 establishments in the United Kingdom]]
[[Category:Handbooks and manuals]]
[[Category:Yearbooks]]
[[Category:Publishing]]
[[Category:A &amp; C Black books]]</text>
      <sha1>66eb8g7xnl2dur54nwfzzp2ogw2ei7m</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Index (publishing)</title>
    <ns>14</ns>
    <id>2198982</id>
    <revision>
      <id>724690645</id>
      <parentid>724690638</parentid>
      <timestamp>2016-06-10T21:13:58Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Bot: Removing CFD templates for completed action</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="77" xml:space="preserve">[[Category:Library science]]
[[Category:Publishing]]
[[Category:Directories]]</text>
      <sha1>stx01bvcpgtawnckh4o3y1xc2og84rv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:File system directories</title>
    <ns>14</ns>
    <id>30139425</id>
    <revision>
      <id>604573322</id>
      <parentid>547741813</parentid>
      <timestamp>2014-04-17T09:47:41Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="59" xml:space="preserve">[[Category:Computer file systems]]
[[Category:Directories]]</text>
      <sha1>bn5mnxftiscf52bq7g5z92oni8jg5tr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Australian directories</title>
    <ns>14</ns>
    <id>45583078</id>
    <revision>
      <id>654629053</id>
      <parentid>654628936</parentid>
      <timestamp>2015-04-02T12:52:24Z</timestamp>
      <contributor>
        <username>JarrahTree</username>
        <id>278097</id>
      </contributor>
      <comment>ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="147" xml:space="preserve">{{portal|Australia}}
''Directories - of businesses, addresses and people in Australia''
[[Category:Directories]]
[[Category:Books about Australia]]</text>
      <sha1>nc6blyrxg9bkldx9utqoksnpv66o6ds</sha1>
    </revision>
  </page>
  <page>
    <title>Whitepages (company)</title>
    <ns>0</ns>
    <id>25901032</id>
    <revision>
      <id>763025316</id>
      <parentid>762822514</parentid>
      <timestamp>2017-01-31T23:21:53Z</timestamp>
      <contributor>
        <username>Lizhpowell</username>
        <id>29411754</id>
      </contributor>
      <comment>link Alex Algard</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24442" xml:space="preserve">{{pp-protect|small=yes}}
{{good article}}
{{Infobox dot-com company
| name             = Whitepages
| logo     = [[File:White-Pages-Logo.png|175px]]
| caption          =
| type             = Private 
| industry         = 
| foundation       = 1997
| founder          = [[Alex Algard]]
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = Seattle, Washington, US
| location_country = US
| locations        = &lt;!-- Number of locations, stores, offices, etc. --&gt;
| area_served      = Worldwide
| key_people       =  Rob Eleveld (CEO)&lt;ref name="newCEO"/&gt;
| products         = People search, contact data, mobile apps
| production       = 
| services         = 
| revenue          = $70 million (2015)&lt;ref name="recentsource"/&gt;
| operating_income = 
| net_income       = 
| aum              = &lt;!-- Only used with financial services companies --&gt;
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 120 (2016)&lt;ref&gt;{{citation|publisher=Whitepages|title=Careers|url=http://whitepagesinc.com/about/careers.html|accessdate=August 19, 2013}}&lt;/ref&gt;
| parent           = 
| divisions        = 
| subsid           = 
| website = {{URL|http://www.whitepages.com}}
| footnotes        = 
| intl             =
| bodystyle        =
| website_type          = Directory
| current_status        = Active
}}'''Whitepages''' is a provider of online directory services, fraud screening and identity verification for businesses, public record background checks, and other products, based on its database of contact information for people and businesses. It has the largest database available of contact information on US residents.&lt;ref name="VB"/&gt;

Whitepages was founded in 1997 as a hobby for then-[[Stanford]] student [[Alex Algard]]. It was incorporated in 2000 and received $45 million in funding in 2005. Investors were later bought-out by Algard in 2013. From 2008 to 2013, Whitepages released several mobile apps, a re-design in 2009, the ability for consumers to control their contact information, and other features. From 2010 to 2016, the company shifted away from advertising revenue and began focusing more on selling business services and subscription products.

==History==
The idea for Whitepages was conceived by Alex Algard, while studying at [[Stanford]] in 1996. Algard was searching for a friend's contact information and the phone company gave him the wrong number.&lt;ref name="ppp"/&gt; He thought of an online email directory as an easier to way to find people.&lt;ref name="seven"/&gt;&lt;ref name="two"&gt;{{cite news|title=WhitePages.com has number for fast growth|url=http://community.seattletimes.nwsource.com/archive/?date=20031013&amp;slug=btinterface13|newspaper=The Seattle Times|accessdate=August 7, 2013|date=October 13, 2003}}&lt;/ref&gt; Algard bought the Whitepages.com domain for nine hundred dollars,&lt;ref name="four"&gt;{{cite news|first=Nicholas|last=Carlson|date=January 24, 2007|url=http://www.internetnews.com/xSP/article.php/3655611|publisher=InternetNews|title=WhitePages.com: Reach out and search someone|accessdate=December 2, 2013}}&lt;/ref&gt;&lt;ref name="recentsource"/&gt; which he says was all of his savings at the time.&lt;ref name="seven"/&gt; He continued operating the website as a hobby while working as an investment banker for [[Goldman Sachs]].&lt;ref name="dakfhukajehf"/&gt; He expanded the database of contact information using data licensed from American Business Information (now a part of Infogroup).&lt;ref name="recentsource"/&gt; Eventually WhitePages was producing more ad-revenue than Algard was earning at Goldman Sachs.&lt;ref name="recentsource"/&gt; In 1998, Algard left his job to focus on the website; he incorporated Whitepages in 2000.&lt;ref name="dakfhukajehf"&gt;{{citation|publisher=Private Equity Growth Capital Council|url=http://www.pegcc.org/wordpress/wp-content/uploads/pec_cs_whitepages_020309a.pdf|title=WhitePages.com: From hobby to number one people search destination|accessdate=August 6, 2013}}&lt;/ref&gt;

The site grew and attracted more advertisers. The company brokered deals with Yellowpages and Superpages, whereby Whitepages earned revenue for sending them referral traffic. By 2005, $15 million in annual revenues was coming from these contracts.&lt;ref name="recentsource"/&gt; In 2003, Algard stepped down as CEO to focus on CarDomain.com, which he had also founded&lt;ref name="ppp"&gt;{{cite news|first=Brad|last=Broberg|title=Founder returns to WhitePages.com|publisher=Puget Sound Business Journal|date=September 30, 2007|url=http://www.bizjournals.com/seattle/stories/2007/10/01/focus10.html|accessdate=August 7, 2013}}&lt;/ref&gt; and Max Bardon took his place as CEO temporarily.&lt;ref name="recentsource"/&gt; In 2005, Technology Crossover Ventures and Providence Equity Partners invested $45 million in the company.&lt;ref name="recentsource"/&gt;&lt;ref name="one"/&gt; That same year, MSN adopted Whitepages' directory data for its "Look it up" feature.&lt;ref&gt;{{cite news|title=MSN Replaces InfoSpace with WhitePages.com|url=http://www.mediapost.com/publications/article/28828/#axzz2bIuB3tM1|first=Shankar|last=Gupta|date=April 5, 2005|accessdate=August 7, 2013|publisher=MediaPost}}&lt;/ref&gt; Algard returned to the company in 2007.&lt;ref name="ppp"/&gt; By the end of that year, the Whitepages database had grown to 180 million records&lt;ref&gt;{{cite news|title=WhitePages.com coverage expands from 40 to 80 percent|url=http://seattletimes.com/html/businesstechnology/2004062675_btbriefs10.html|newspaper=The Seattle Times|date=December 10, 2007|accessdate=August 7, 2013}}&lt;/ref&gt; and the company was listed as one of [[Deloitte]]'s 500 fastest growing technology companies in North America three times.&lt;ref name="seven"/&gt;&lt;ref&gt;{{cite news|title=WhitePages hires new CTO|first=Rebecca|last=Collins|url=http://www.bizjournals.com/seattle/blog/techflash/2010/11/whitepages-taps-new-cto.html|publisher=Puget Sound Business Journal|date=November 17, 2010|accessdate=August 8, 2013}}&lt;/ref&gt; By 2008 the company had $66 million in annual revenues.&lt;ref name="recentsource"/&gt;

In 2008, Whitepages said it would start working on options for users to control their information on the site.&lt;ref&gt;{{cite news|first=Steven|last=Vaughan-Nichols|newspaper=Computerworld|url=http://www.computerworld.com.au/article/216557/whitepages_com_grapples_privacy_web_2_0_world/?|title=WhitePages.com grapples with privacy in Web 2.0 world|date=May 19, 2008|accessdate=August 7, 2013}}&lt;/ref&gt; That same year, it acquired [[Voice over Internet Protocol|VoIP]] developer [[Snapvine]]&lt;ref name="one"&gt;{{cite news|first=Angel|last=Gonzalez|url=http://seattletimes.com/html/businesstechnology/2004458452_whitepages05.html|newspaper=The Seattle Times|title=WhitePages.com to buy Snapvine|accessdate=August 7, 2013|date=June 5, 2008}}&lt;/ref&gt; in order to add features where users could be called through the website without giving out their phone number.&lt;ref&gt;{{cite news|title=WhitePages.com to buy Snapvine for around $20 million|first=Michael|last=Arrington|date=June 4, 2008|accessdate=August 7, 2013|url=http://techcrunch.com/2008/06/04/whitepagescom-to-buy-snapvine-for-around-20-million/|publisher=TechCrunch}}&lt;/ref&gt; It also introduced an [[api]],  which gave third-party developers access to Whitepages' data.&lt;ref&gt;{{cite news|first=Mike|last=Gunderloy|date=March 31, 2008|url=http://gigaom.com/2008/03/31/open-phone-data-whitepages/|accessdate=August 7, 2013|title=Open Phone Data from WhitePages.com|publisher=Giga Om}}&lt;/ref&gt; Whitepages released an iOS app that August, followed by the Whitepages Caller ID app for Android devices  in February 2009&lt;ref&gt;{{cite news|publisher=VentureBeat|first=MG|last=Siegler|date=February 27, 2009|accessdate=August 7, 2013|url=http://venturebeat.com/2009/02/27/caller-id-a-paid-android-app-to-better-screen-my-phone-calls/|title=Caller ID: A paid Android app to better screen my phone calls}}&lt;/ref&gt; and for Blackberry that May.&lt;ref name="plp"&gt;{{cite news|publisher=VentureBeat|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|first=Matt|last=Marshall|date=May 7, 2009|accessdate=August 7, 2013}}&lt;/ref&gt; 

The app displays information on callers, such as their latest social media posts, local weather at the caller's location and the identity of the caller.&lt;ref name="eightlyy"&gt;{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}&lt;/ref&gt;&lt;ref name="twenty"&gt;{{cite news|title=WhitePages' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}&lt;/ref&gt; It originally had the ability to display information on callers, such as their latest social media posts, local weather at the caller's location and the identity of the caller.&lt;ref name="eightlyy"&gt;{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}&lt;/ref&gt;&lt;ref name="twenty"&gt;{{cite news|title=WhitePages' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}&lt;/ref&gt; The ability for consumers to add themselves to the directory was added in the summer of 2009 and being able to edit existing entries was added that October.&lt;ref&gt;{{cite news|title=WhitePages Now Lets you control your own listings|first=Erick|last=Schonfeld|date=October 14, 2009|accessdate=August 8, 2013|url=http://techcrunch.com/2009/10/14/whitepages-now-lets-you-control-your-own-listings/|publisher=TechCrunch}}&lt;/ref&gt;

Whitepages.com underwent a re-design in 2009.&lt;ref name="three"&gt;{{cite news|title=WhitePages launches $2.5 million overhaul|first=Brier|last=Dudley|url=http://seattletimes.com/html/technologybrierdudleysblog/2009467080_whitepagescom_launches_25_mill.html|date=July 14, 2009|accessdate=August 7, 2013|newspaper=The Seattle Times}}&lt;/ref&gt; According to VentureBeat reporter Matt Marshall, the redesign made the advertising "cleaner" and made it more obvious when someone was going to a third-party website like US Search.&lt;ref name="VB"&gt;{{cite news|date=July 14, 2009|first=Matt|last=Marshall|url=http://venturebeat.com/2009/07/14/whitepages-now-the-largest-database-of-american-people-cleans-up-act/|publisher=VentureBeat|title=WhitePages, now the largest database of American people, cleans up act|accessdate=August 7, 2013}}&lt;/ref&gt; Marshall had previously criticized Whitepages, because website users that clicked on US Search ads and purchased data from US Search were sent through perpetual advertisements for other services that made it difficult to access the information they paid for.&lt;ref name="VB"/&gt;&lt;ref&gt;{{cite news|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|date=May 7, 2009|first=Matt|last=Marshall|accessdate=August 7, 2013}}&lt;/ref&gt; A local business lookup feature called "Store Finder" was added in June 2010.&lt;ref&gt;{{cite news|title=WhitePages upgrades business search, adds "store finder"|url=http://seattletimes.com/html/technologybrierdudleysblog/2012197459_whitepages_upgrades_business_s.html|first=Brier|last=Dudley|newspaper=The Seattle Times|date=June 24, 2010}}&lt;/ref&gt; The following month, Whitepages.com launched a deal site, Dealpop.com,&lt;ref&gt;{{cite news|title=Local shops join forces with coupon websites to sweeten sales|first=Melissa|last=Allison|author2=Amy Martinez |url=http://seattletimes.com/html/retailreport/2012259556_retailreport02.html|newspaper=The Seattle Times|date=July 1, 2010|accessdate=August 6, 2013}}&lt;/ref&gt; which differed from [[Groupon]] by offering short-term deals on nationally available products.&lt;ref&gt;{{cite news|first=Amy|last=Martinez|date=October 20, 2010|accessdate=August 7, 2013|newspaper=The Seattle Times|url=http://seattletimes.com/html/businesstechnology/2013209878_dealpopweb21.html|title=WhitePages' DealPop to try national approach as it takes on Groupon, other coupon websites}}&lt;/ref&gt; Dealpop was sold to [[Martin Tobias#Tippr.com|Tippr]] the following year.&lt;ref&gt;{{cite news|title=Tippr Grabs Sales &amp; Tech Talent in DealPop Acquisition, Continuing Daily Deals Dogfight for Third Place|url=http://www.xconomy.com/seattle/2011/06/01/tippr-grabs-sales-tech-talent-in-dealpop-acquisition-continuing-daily-deals-dogfight-for-third-place/|newspaper=Xconomy|date=July 1, 2011|accessdate=August 7, 2013|first=Curt|last=Wooodward}}&lt;/ref&gt;

In 2010, Superpages and Yellowpages cut back spending with Whitepages from $33 million to $7 million, causing a substantial decline in revenues and a tense relationship with investors. Algard spent $50 million in cash the company had on-hand and $30 million from a bank loan, to buyout the investors in 2013. He also used his personal house, savings account and personal belongings as collateral for the loan.&lt;ref name="recentsource"/&gt; Algard began shifting the company's business model to reduce its reliance on advertising and instead focus on business users and paid subscriptions.&lt;ref name="recentsource"/&gt;&lt;ref name="Carlson 2013"&gt;{{cite web | last=Carlson | first=Nicholas | title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million | website=Business Insider | date=October 21, 2013 | url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10 | accessdate=August 18, 2016}}&lt;/ref&gt; 

Whitepages released the Localicious app in July 2011. The app was released on Android first, because Whitepages was frustrated with Apple's approval process for iPhone apps.&lt;ref name="agiu"&gt;{{cite news|title=WhitePages goes Android first with latest app|url=http://news.cnet.com/8301-1023_3-20079150-93/whitepages-goes-android-first-with-latest-app/|date=July 13, 2011|first=Ina|last=Fried|accessdate=August 7, 2013|publisher=All Things Digital}}&lt;/ref&gt; Whitepages PRO was also introduced that same year.&lt;ref name="cardnotpresent"&gt;{{cite news|url=http://pro.whitepages.com/sites/pro.whitepages.com/files/Marketing_Documents/CardNotPresent%20Article%2010.24.12.pdf|publisher=CNP Report|first=D.J.|last=Murphy|date=October 24, 2012|accessdate=September 24, 2013|title=WhitePages PRO Taps Phone Data and More to Identify CNP Fraud}}&lt;/ref&gt; An updated Android app called  Current Caller ID was released in August 2012.&lt;ref name="eightlyy"/&gt; Within a year of its release, 5 billion calls and texts had been transmitted using the app. It was updated in July 2013 with new features, such as the ability to customize the layout of caller information for each caller and the ability to "Like" Facebook posts from within the app.&lt;ref name="fgy"&gt;{{cite news|title=WhitePages' Current Caller ID app powers more than 5B calls &amp; texts, adds new customization features|url=http://venturebeat.com/2013/07/25/whitepages-current-caller-id-app-powers-more-than-5b-calls-texts-adds-new-customization-features/|publisher=VentureBeat|first=Devindra|last=Hardawar|date=July 25, 2013|accessdate=August 7, 2013}}&lt;/ref&gt; In June 2013, Whitepages acquired Mr. Number, an Android app for blocking unwanted callers.&lt;ref&gt;{{cite news|title=WhitePages Scoops up Mr. Number, an Android App for Blocking Unwanted Calls|date=June 1, 2013|first=Ina|last=Fried|url=http://allthingsd.com/20130601/whitepages-scoops-up-mr-number-an-android-app-for-blocking-unwanted-calls/|newspaper=The Wall Street Journal|accessdate=August 7, 2013}}&lt;/ref&gt;

In August 2013 Whitepages purchased all the interests in the company owned by investors for $80 million.&lt;ref name="dafhybniub"&gt;{{cite news|title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million|first=Nicholas|last=Carlson|date=October 21, 2013|url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10#ixzz2qRETXgXX|publisher=Business Insider|accessdate=October 30, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Nextcast: WhitePages CEO Alex Algard on the distraction of outside investors and keeping your startup zeal|first=Jeff|last=Dickey|date=April 5, 2014|accessdate=May 2, 2014|url=http://www.geekwire.com/2014/nextcast-whitepages-ceo-alex-algard-distraction-outside-investors-keep-startup-zeal/|publisher=Geekwire}}&lt;/ref&gt; In 2015, WhitePages acquired San Francisco-based NumberCorp to improve the database of phone numbers used for scams in the Caller ID app.&lt;ref name="Perez 2015"&gt;{{cite web | last=Perez | first=Sarah | title=Whitepages Acquires NumberCop To Improve Its Scam-Detecting Caller ID App | website=TechCrunch | date=June 10, 2015 | url=http://social.techcrunch.com/2015/06/10/whitepages-acquires-numbercop-to-improve-its-scam-detecting-caller-id-app/ | accessdate=August 12, 2016}}&lt;/ref&gt; In April 2016, Whitepages spun-off its caller ID business into a separate company called Hiya&lt;ref name="Lunden 2016"&gt;{{cite web | last=Lunden | first=Ingrid | title=Whitepages spins out its caller-ID business as Hiya to take on TrueCaller | website=TechCrunch | date=April 27, 2016 | url=http://social.techcrunch.com/2016/04/27/whitepages-hiya/ | accessdate=July 8, 2016}}&lt;/ref&gt; with a staff of 40 in Seattle.&lt;ref name="Flynn 2016"&gt;{{cite web | last=Flynn | first=Kerry | title=Meet Hiya: Whitepages Spins Off Caller ID Business With Mission To Fight Robocalls, Spam Texts Worldwide | website=International Business Times | date=April 27, 2016 | url=http://www.ibtimes.com/meet-hiya-whitepages-spins-caller-id-business-mission-fight-robocalls-spam-texts-2360298 | accessdate=July 8, 2016}}&lt;/ref&gt; In September 2016, Alex Algard stepped down as CEO of WhitePages, in order to focus on the mobile spam-blocking spin-off Hiya. He appointed Rob Eleveld as the new WhitePages CEO.&lt;ref name="newCEO"&gt;{{cite web | title=Whitepages Founder Alex Algard Gives Up CEO Slot To Focus On Caller ID Startup Hiya | newspaper=Forbes | date=September 16, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/09/16/whitepages-founder-alex-algard-gives-up-ceo-slot-there-to-focus-on-caller-id-spinoff-hiya/#2db4ba761803 | accessdate=September 20, 2016}}&lt;/ref&gt;

==Services==
Whitepages has the largest database of contact information on Americans.&lt;ref name="VB"/&gt; As of 2008, it had data on about 90 percent of the US adult population,&lt;ref&gt;{{cite news|publisher=IntoMobile|first=Dusan|last=Belic|date=May 8, 2012|accessdate=September 24, 2013|url=http://www.intomobile.com/2012/05/08/whitepages-ios-app-gets-nearby-search-capability/|title=WhitePages' iOS app gets nearby search capability}}&lt;/ref&gt; including 200 million records on people and 15 million business listings.&lt;ref name="seven"&gt;{{cite news|title=A Directory of Success: WhitePages CEO Alex Algard|date=February 2, 2011|newspaper=Examiner|first=Paul|last=Kim}}&lt;/ref&gt; Whitepages' data is collected from property deeds,&lt;ref name="five"/&gt; telecom companies, and public records.&lt;ref name="ll"&gt;{{cite news|title=WhitePages IDs Growth in the Explosion of Personal Data|date=August 20, 2012|first=Curt|last=Woodward|accessdate=August 7, 2013|url=http://www.xconomy.com/seattle/2012/08/20/whitepages/}}&lt;/ref&gt; Privacy is a common concern regarding Whitepages' publishing of personal contact information.&lt;ref name="StairReynolds2008"&gt;{{cite book|author1=Ralph M. Stair|author2=George Reynolds|author3=George Walter Reynolds|title=Fundamentals of Information Systems|url=https://books.google.com/books?id=J85RP4YmBTYC&amp;pg=PA253|accessdate=7 August 2013|date=December 2008|publisher=Cengage Learning|isbn=978-1-4239-2581-1|pages=253&#8211;}}&lt;/ref&gt; The Whitepages.com website has features that allow users to remove themselves from the directory or correct and update information.&lt;ref name="five"&gt;{{cite news|title=Connecticut may let residents remove directory information|url=http://www.scmagazine.com/connecticut-may-let-residents-remove-directory-data/article/100267/#|date=December 28, 2007|first=Dan|last=Kaplan|newspaper=SC Magazine}}&lt;/ref&gt;&lt;ref name="StairReynolds2008"/&gt; WhitePages.com has about 50 million unique visitors per month&lt;ref&gt;{{cite news|publisher=VentureBeat|title=WhitePages acquires Mr. Number, the phone-spam Android app with 7M downloads, to reduce phone spam|url=http://www.reuters.com/article/2013/05/31/idUS27174982720130531|first=John|last=Koetsier|date=May 31, 2013|accessdate=December 2, 2013}}&lt;/ref&gt; and performs two billion searches per month.&lt;ref name="cardnotpresent"/&gt;

WhitePages started developing features for business users around 2010.&lt;ref name="recentsource"/&gt; WhitePages Pro is used for things like verifying the identity of a sales lead, find fake form data in online forms and to check form data from consumers making a purchase against common indicators of fraud, like shipping to a mailbox at an unoccupied building.&lt;ref name="recentsource"&gt;{{cite news| first=Amy|last=Feldman |title=Alex Algard Risked Everything To Turn His Struggling Firm, Whitepages, Into A Growing Tech Company | newspaper=Forbes | date=August 23, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/08/03/alex-algard-risked-everything-to-turn-his-struggling-firm-whitepages-into-a-growing-tech-company/#165b97ae73d0 | accessdate=August 10, 2016}}&lt;/ref&gt;&lt;ref name="cardnotpresent"/&gt;&lt;ref name="Whitepages Pro"&gt;{{cite web | title=Whitepages Pro &#8211; Mobile Identity Data for Businesses | website=Whitepages Pro | url=http://pro.whitepages.com/ | accessdate=August 15, 2016}}&lt;/ref&gt; In 2016, advertising on WhitePages.com was turned off in favor of selling monthly subscriptions that give users unlimited background checks and other records.&lt;ref name="recentsource"/&gt;

As of 2013 Whitepages provides its data and related services through seven web properties, ten mobile apps&lt;ref&gt;{{citation|url=http://whitepagesinc.com/about/|publisher=WhitePages|title=About Us|accessdate=December 2, 2013}}&lt;/ref&gt; and  through multiple web properties, including 411.com and Switchboard.com.&lt;ref name="SuiElwood2012"&gt;{{cite book|author1=Daniel Zhi Sui|author2=Sarah Elwood|author3=Michael F. Goodchild|title=Crowdsourcing Geographic Knowledge: Volunteered Geographic Information (VGI) in Theory and Practice|url=https://books.google.com/books?id=SSbHUpSk2MsC&amp;pg=PA267|accessdate=7 August 2013|date=10 August 2012|publisher=Springer|isbn=978-94-007-4587-2|pages=267&#8211;}}&lt;/ref&gt; The Hiya app (previously known as WhitePages Caller ID) checks incoming calls against a database of phone numbers known for spam or scam calls and helps users report scams to the Federal Trade Commission.&lt;ref name="Stern 2016"&gt;{{cite web | last=Stern | first=Joanna | title=How to Stop Robocalls &#8230; or at Least Fight Back | website=WSJ | date=June 28, 2016 | url=http://www.wsj.com/articles/how-to-stop-robocalls-or-at-least-fight-back-1467138771 | accessdate=July 8, 2016}}&lt;/ref&gt;&lt;ref name="Lerman 2016"&gt;{{cite web | last=Lerman | first=Rachel | title=Whitepages spins out mobile caller-ID startup Hiya | website=The Seattle Times | date=April 27, 2016 | url=http://www.seattletimes.com/business/technology/whitepages-spins-out-mobile-caller-id-startup-ceo-takes-on-dual-roles/ | accessdate=July 8, 2016}}&lt;/ref&gt; Hiya mobile app replaces the Android user interface for making and receiving phone calls.&lt;ref name="fgy"/&gt;

==References==
{{reflist|2}}

==External links==
*[http://www.whitepages.com/ Official website]

{{DEFAULTSORT:Whitepages.Com}}
[[Category:Directories]]
[[Category:Internet properties established in 1997]]
[[Category:Privately held companies based in Washington (state)]]
[[Category:Companies based in Seattle, Washington]]
[[Category:Online person databases]]</text>
      <sha1>i7gkruewobsz0fvf3csi3ixei4q0nhb</sha1>
    </revision>
  </page>
  <page>
    <title>Web query classification</title>
    <ns>0</ns>
    <id>16350490</id>
    <revision>
      <id>720182341</id>
      <parentid>713166845</parentid>
      <timestamp>2016-05-14T06:33:32Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* How to use the unlabeled query logs to help with query classification? */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cl using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9169" xml:space="preserve">{{Cleanup|date=March 2011}}
''' 
A Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query &#8220;''apple''&#8221; might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.

== KDDCUP 2005 ==

KDDCUP 2005 competition&lt;ref&gt;[http://www.kdd.org/kdd-cup/view/kdd-cup-2005 KDDCUP 2005 dataset]&lt;/ref&gt; highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query &#8220;''apple''&#8221;, it should be classified into ranked categories: &#8220;''Computers \ Hardware''; ''Living \ Food &amp; Cooking''&#8221;.

{| class="wikitable"
|-
! Query
! Categories
|-
| apple
| Computers \ Hardware&lt;br /&gt;Living \ Food &amp; Cooking
|-
| FIFA 2006
| Sports \ Soccer&lt;br /&gt;Sports \ Schedules &amp; Tickets&lt;br /&gt;Entertainment \ Games &amp; Toys
|-
| cheesecake recipes
| Living \ Food &amp; Cooking&lt;br /&gt;Information \ Arts &amp; Humanities
|-
| friendships poem
| Information \ Arts &amp; Humanities&lt;br /&gt;Living \ Dating &amp; Relationships
|}

[[Image:Web query length.gif]]
[[Image:Web query meaning.gif]]

== Difficulties ==

Web query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:

=== How to derive an appropriate feature representation for Web queries? ===

Many queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "''apple''" can mean a kind of fruit or a computer company. "''Java''" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.

* Query-enrichment based methods&lt;ref&gt;Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.&lt;/ref&gt;&lt;ref&gt;Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.&lt;/ref&gt; start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).

How about disadvantages and advantages??
give the answers:

=== How to adapt the changes of the queries and categories over time? ===

The meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "''Barcelona''" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.

* Intermediate taxonomy based method&lt;ref&gt;Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. ''ACM SIGIR, 2006''.&lt;/ref&gt; first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.

=== How to use the unlabeled query logs to help with query classification? ===

Since the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.

* Query clustering method&lt;ref&gt;Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], ''ACM TOIS, Volume 20, Issue 1, January 2002''.&lt;/ref&gt; tries to associate related queries by clustering &#8220;session data&#8221;, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.
* Selectional preference based method&lt;ref&gt;Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], ''ACM TOIS, Volume 25, Issue 2, April 2007''.&lt;/ref&gt; tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.

== Applications ==

* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.
* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.
* '''[[Online advertising]]'''&lt;ref&gt;[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007&lt;/ref&gt;&lt;ref&gt;[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008&lt;/ref&gt; aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.
All these services rely on the understanding Web users' search intents through their Web queries.

== See also ==

* [[Document classification]]
* [[Web search query]]
* [[Information retrieval]]
* [[Query expansion]]
* [[Naive Bayes classifier]]
* [[Support vector machines]]
* [[Meta search]]
* [[Vertical search]]
* [[Online advertising]]

== References ==

{{reflist}}

== Further reading ==
* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&amp;skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. ''Phd Thesis'', ''HKUST'', June 2007.
{{Internet search}}

{{DEFAULTSORT:Web Query Classification}}
[[Category:Information retrieval techniques]]
[[Category:Internet search]]</text>
      <sha1>fhmja596w3lbuch2sv0qcbvudiqxy3x</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Real-time web</title>
    <ns>14</ns>
    <id>23686083</id>
    <revision>
      <id>470667521</id>
      <parentid>389252365</parentid>
      <timestamp>2012-01-10T19:53:22Z</timestamp>
      <contributor>
        <username>Gray eyes</username>
        <id>12633548</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="93" xml:space="preserve">{{Cat main|Real-time web}}

[[Category:Internet search]]
[[Category:Real-time computing|web]]</text>
      <sha1>62goeto6e11tzkpku3mefzgzzsok7m7</sha1>
    </revision>
  </page>
  <page>
    <title>Sponsored search auction</title>
    <ns>0</ns>
    <id>23022154</id>
    <revision>
      <id>709294939</id>
      <parentid>663625159</parentid>
      <timestamp>2016-03-10T04:13:09Z</timestamp>
      <contributor>
        <ip>14.139.180.66</ip>
      </contributor>
      <comment>/* Unthruthfulness */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5060" xml:space="preserve">A '''sponsored search auction (SSA)''', also known as a '''keyword auction''', is an indispensable part of the [[business model]] of modern [[web host]]s. It refers to results from a search engine that are not output by the main search algorithm, but
rather clearly separate advertisements paid for by third parties. These advertisements
are typically related to the terms for which the user searched. They come in the form
of a link and some information, with the hope that a search user will click on the link
and buy something from the advertiser.
In sponsored search auctions, there are typically some fixed number of slots for advertisements and more advertisers that want these slots than there are slots. The advertisers
have different valuations for these slots and slots are a scarce resource, so an auction
is done to determine how the slots will be assigned.

==History==
Prior to 1998, many advertisements were charged by impression, as it was the
easiest metric to calculate. In 1998, GoTo.com, Inc debuted a pay-per-click charging
system, with pricing and slot placement determined by an auction. GoTo used a first
price auction, where bidders were placed according to their bids and charged their bids
when they won. GoTo faced bidders who were constantly changing their bid
in response to new information and changing information from other bidders.

Currently, charging per action is a common pricing scheme in affiliate networks,
such as the Amazon Associates Program.

In 2002, [[Google AdWords]] began using a second price auction to sell the single advertisement
slot. Shortly thereafter, pages had multiple advertisements slots, which were allocated
and sold via [[generalized second-price auction]] (GSP) auction, the natural generalization of a second price, single item, multi bidder
auction.&lt;ref&gt;Hal Varian, Christopher Harris. The VCG Auction in Theory and Practice, In The
American Economic Review, Volume 104, Issue 5, pages 442-452. Elsevier B.V.,
2014.&lt;/ref&gt;

==Auction Mechanisms==
===Generalized Second Price Auction===
[[Generalized second-price auction]] (GSP) is the most commonly used auction mechanism for sponsored search.

====Untruthfulness====
An issue with GSP is that it's not a truthful auction and it is not the optimal strategy. To illustrate this, consider the following example.

There are three bidders with only two possible slots. The values of
each bidders 1, 2, and 3 are $10, $5, and $3 respectively. Suppose that the first slot click
through rate (CTR) is 300 and the second slot CTR is 290. If bidder 1 is truthful, he
would have to pay &lt;math&gt;p_1 = $5(300) = $1500&lt;/math&gt; for a utility of &lt;math&gt;u_1 = $10(300)-$1500 = $1500&lt;/math&gt;.
However, if bidder 1 decides to lie and reports a value of $4 instead then his utility
would be &lt;math&gt;u_2 = $10(290) - $3(290) = $2030&lt;/math&gt;. Notice that &lt;math&gt;u_2 &gt; u_1&lt;/math&gt; which makes GSP
untruthful and bidders have an incentive to lie.

====Quality Variant====
Google uses a minor variant of GSP to auction off advertisement slots. Potential
advertisements may be of varying quality. Suppose that there are two advertisements
for eggs. One advertisement simply fills its space with the word &#8220;egg&#8221; repeated over
and over, while the other advertisement shows a picture of eggs, contains branding
information, and mentions positive qualities about their eggs, such as cage-freeness.
The second advertisement may be thought of as having higher quality than the first
advertisement, being more useful to consumers, more likely to be clicked on, and more
likely to generate revenue for both the advertiser and Google. Advertisements that
have a history of high click through rates, are geographically targeted at the user, or
have a high quality landing page may also be thought of as having higher quality.&lt;ref&gt;Google AdWords, Check and understand Quality Score. support.google.com/adwords/answer/2454010&lt;/ref&gt;

Google assigns a numeric &#8220;quality&#8221; score &lt;math&gt;\gamma_i&lt;/math&gt; to each bidder &lt;math&gt;i&lt;/math&gt;. Bidders, rather than
being ordered purely by their bid, are instead ordered by rank, which is the product
of their bid and quality score &lt;math&gt;\gamma_1 b_1 \geq \gamma_2 b_2 \geq \dots \geq \gamma_n b_n&lt;/math&gt; . Slots are still assigned in
decreasing rank order. Bidders are charged, rather than the bid of the bidder one rank
lower (&lt;math&gt;p_i(b_i, b_{-i}) = b_{i+1}&lt;/math&gt;), are charged the minimum price for which, if it was their bid,
would keep them in their current rank: &lt;math&gt;p_i(b_i, b_{-i}) = \frac{\gamma_{i+1}b_{i+1}}{\gamma_i}&lt;/math&gt;

===Vickrey&#8211;Clarke&#8211;Groves Auction===
[[Vickrey&#8211;Clarke&#8211;Groves auction]] (VCG) is a truthful auction optimizing social welfare. VCG is more complicated to explain than GSP and that might deter many websites from using a VCG auction mechanism even though it's truthful. However, some websites use VCG as their auction mechanism, most notably [[Facebook]].

==See also==
*[[Generalized second-price auction]]
*[[Vickrey&#8211;Clarke&#8211;Groves auction]]

==References==
{{reflist}}

[[Category:Internet search]]</text>
      <sha1>s7ajt0123zrqbpup87tdzr4arjmotqj</sha1>
    </revision>
  </page>
  <page>
    <title>SpyFu</title>
    <ns>0</ns>
    <id>25580778</id>
    <revision>
      <id>744129094</id>
      <parentid>683305395</parentid>
      <timestamp>2016-10-13T08:48:41Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3933" xml:space="preserve">'''SpyFu''', originally GoogSpy, is a [[search analytics]] company based out of Scottsdale, AZ. Started in April 2005, SpyFu shows the keywords that websites buy on [[Google Adwords]]&lt;ref&gt;{{Cite web|url=http://searchenginewatch.com/3632613|accessdate=December 28, 2009|title=Advanced Keyword Research Checklist: Using Multiple Datasets}}&lt;/ref&gt; as well as the keywords that websites are showing up for within search results. The service also gives cost per click and search volume statistics on keywords and uses that data to approximate what websites are spending on advertising.&lt;ref&gt;{{Cite web|url=http://www.entrepreneur.com/ebusiness/searchoptimization/searchengineoptimizationcolumnistjonrognerud/article175856.html|accessdate=December 28, 2009|title=Using the Competition to Boost Your SEO Performance }}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.pcworld.com/businesscenter/article/141728/top_11_moneywasting_adwords_mistakes.html|accessdate=December 28, 2009|title=Top 11 Money-Wasting AdWords Mistakes}}&lt;/ref&gt; Historical advertising budgets offered by SpyFu also help advertisers project what an advertising campaign will cost in the future.&lt;ref&gt;{{Cite web|url=http://searchengineland.com/spying-on-your-paid-search-competitors-13235|accessdate=December 28, 2009|title=Spying On Your Paid Search Competitors}}&lt;/ref&gt; The main value proposition is to see or to "spy on" the keywords that competitors use and improve [[Search Engine Marketing|SEM]] and [[Search Engine Optimization|SEO]] strategies based on those.&lt;ref&gt;{{Cite news|url=http://www.wired.com/epicenter/2009/06/coolsearchengines/|accessdate=December 28, 2009|title=Cool Search Engines That Are Not Google | work=Wired|first=Ryan|last=Singel|date=June 30, 2009}}&lt;/ref&gt; SpyFu's data was also used in the [[Washington Post]] during the [[United States presidential election, 2008|2008 Presidential election]] to disclose various keywords that candidates were advertising on.&lt;ref&gt;{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/10/15/AR2008101503574_2.html?sid=ST2008101503923|accessdate=December 28, 2009|title=In Targeting Online Ads, Campaigns Ask: Who's Searching for What? | work=The Washington Post | first=Peter | last=Whoriskey | date=October 16, 2008}}&lt;/ref&gt; SpyFu can also uncover emerging or niche markets.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=DrQtbOroN0UC&amp;pg=PT188&amp;dq=spyfu&amp;ei=oz44S4ehGqmKkATX1KjNAQ&amp;cd=1#v=onepage&amp;q=spyfu&amp;f=false|accessdate=December 28, 2009|title=The Findability Formula | first=Heather F. | last=Lutze | isbn=978-0-470-42090-4 | year=2009 | publisher=John Wiley and Sons}}&lt;/ref&gt; SpyFu has been mentioned in ''[[4_hour_work_week|The 4-Hour Work Week]]'', Oreilly's ''[[Complete Web Monitoring]]'', and ''[[SEO Warrior]]''.

SpyFu's data is obtained via [[web scraping]], based on technology developed by [[Velocityscape]], a company that makes web scraping software. The accuracy of its data, especially advertising budgets, was found to be somewhat dependent on the size of the website in question.&lt;ref&gt;{{Cite web|url=http://www.seoptimise.com/blog/2008/09/the-small-but-great-spyfu-experiment.html|accessdate=December 28, 2009|title=The Small (but Great) SpyFu Experiment}}&lt;/ref&gt; SpyFu refreshes its data on a monthly basis, and as such is used as a guide to what's happening with larger trends in SEM/SEO rather than as a real time tracking engine.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=5G4kfJ3DG2kC&amp;pg=PA113&amp;dq=spyfu&amp;ei=gvg4S-WvLZPIlAT3nuzSAQ&amp;cd=2#v=onepage&amp;q=spyfu&amp;f=false|accessdate=December 28, 2009|title=The complete guide to Google advertising | first=Bruce C. | last=Brown | isbn=978-1-60138-045-6 | year=2007 | publisher=Atlantic Publishing Company}}&lt;/ref&gt;
==References==
{{Reflist}}

==External links==
* [http://spyfu.com/ SpyFu Corporate Website]

[[Category:Internet search]]
[[Category:Companies based in Scottsdale, Arizona]]
[[Category:Companies established in 2005]]</text>
      <sha1>k871ub9igcw2zhk2ev96iagtsysqlvo</sha1>
    </revision>
  </page>
  <page>
    <title>Instant indexing</title>
    <ns>0</ns>
    <id>6111052</id>
    <revision>
      <id>752691331</id>
      <parentid>752690705</parentid>
      <timestamp>2016-12-02T19:06:28Z</timestamp>
      <contributor>
        <username>Kuru</username>
        <id>764407</id>
      </contributor>
      <comment>rmv spam link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3251" xml:space="preserve">{{Orphan|date=February 2009}}

'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].

==Delayed inclusion==
Certain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]].&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=HBDC35Vgj34|title=How to index your domains|accessdate=2015-12-24}}&lt;/ref&gt;

Delayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services, such as [[Ask.com]] only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index.&lt;ref&gt;{{cite web|url=https://www.smartz.com/web-marketing/search-engine-optimization/submit-site-to-search-engines/|title=How to Submit Your Site to Search Engines|accessdate=2015-12-24}}&lt;/ref&gt;

==Criticisms==
A criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}

Instant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.

Select search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.

==External links==
* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html &lt;!-- Bot retrieved archive --&gt; |archivedate = 2006-04-27}}
* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories &#8212; A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}

== See also ==
* [[Search engine]]
* [[Search engine indexing]]
* [[Web crawling]]

== References ==
{{Reflist}}

[[Category:Internet terminology]]
[[Category:Internet search]]


{{website-stub}}</text>
      <sha1>ewd9tyywi119s3j74xqawnx6qspy2no</sha1>
    </revision>
  </page>
  <page>
    <title>Online search</title>
    <ns>0</ns>
    <id>31385661</id>
    <revision>
      <id>758281414</id>
      <parentid>749191378</parentid>
      <timestamp>2017-01-04T14:36:48Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>new key for [[Category:Internet search]]: " " using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="973" xml:space="preserve">'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].&lt;ref name="whatis?"&gt;{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12&#8211;18|id=Eric:EJ214713| accessdate=2011-04-04}}&lt;/ref&gt; Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.&lt;ref name="whatis?"/&gt; In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.&lt;ref name="whatis?"/&gt; Today, searches through [[web search engine]]s constitute the majority of online searches.

Online searches often supplement reference transactions.{{cn|date=June 2015}}

==References==
{{reflist}}

{{Internet search}}

[[Category:Internet terminology]]
[[Category:Information retrieval genres]]
[[Category:Internet search| ]]

{{web-stub}}</text>
      <sha1>idvwujij49z03js03e19alrzazycrkc</sha1>
    </revision>
  </page>
  <page>
    <title>National Centre for Text Mining</title>
    <ns>0</ns>
    <id>10795520</id>
    <revision>
      <id>729685598</id>
      <parentid>729685541</parentid>
      <timestamp>2016-07-13T21:31:31Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Services */ add word</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16006" xml:space="preserve">{{Multiple issues|
{{COI|date=December 2015}}
{{advert|date=December 2015}}
{{external links|date=December 2015}}
}}

{{Infobox academic division
| name           = National Centre for Text Mining (NaCTeM)
| image_name     = 
| image_alt      = National Centre for Text Mining 
| established    = 2004
| type           = 
| parent         = [[School of Computer Science, University of Manchester]] 
| affiliation    = [[University of Manchester]]
| city           = [[Manchester]]
| country        = [[United Kingdom]]
| director       = Prof. Sophia Ananiadou
| website        = {{URL|www.nactem.ac.uk}}
| logo           =&lt;!-- Deleted image removed:  [[File:Nactem hires.tif|300px]] --&gt;
}}

The '''National Centre for Text Mining''' ('''NaCTeM''')&lt;ref name="ariadne"&gt;{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}&lt;/ref&gt; is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community.

The [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest &#8211; examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].

The Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[natural language processing]] and [[information extraction]], including [[named-entity recognition]] and extractions of complex relationships (or events) that hold between named entitites, along with parallel and distributed data mining systems in biomedical and clinical applications.

==Services==
[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them.&lt;ref name="multi-word"&gt;{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117&#8211;132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}&lt;/ref&gt;

[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[disambiguation (metadata)|disambiguates]] them.&lt;ref name="pmid17050571"&gt;{{cite journal|vauthors=Okazaki N, Ananiadou S | title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089&#8211;95 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&amp;tool=sumsearch.org/cite&amp;retmode=ref&amp;cmd=prlinks&amp;id=17050571  }}&lt;/ref&gt;

[http://www.nactem.ac.uk/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts &lt;ref&gt;{{cite conference |author=Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J.|title=Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases |year=2006 |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1017&#8211;1024 |doi=10.3115/1220175.1220303}}&lt;/ref&gt;

[http://www.nactem.ac.uk/facta/ '''Facta+'''] is a [[Medline]]  search engine for finding associations between biomedical concepts.&lt;ref name="pmid18772154"&gt;{{cite journal|vauthors=Tsuruoka Y, Tsujii J, Ananiadou S | title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559&#8211;60 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }}&lt;/ref&gt;

[http://www.nactem.ac.uk/facta-visualizer/ '''Facta+ Visualizer'''] is a web application that aids in understanding FACTA+ search results through intuitive graphical visualisation.&lt;ref&gt;{{Cite journal| last1 = Tsuruoka| first1 = Y
| last2 = Miwa| first2 = M| last3 = Hamamoto| first3 = K| last4 = Tsujii| first4 = J| last5 = Ananiadou| first5 = S
 | year = 2011| title = Discovering and visualizing indirect associations between biomedical concepts
 | journal = Bioinformatics| volume = 27| issue = 13 | pages = i111-9| publisher =  | jstor = | doi = 10.1093/bioinformatics/btr214 }}&lt;/ref&gt;

[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system over [[Medline]] abstracts.

[http://labs.europepmc.org/evf '''Europe PMC EvidenceFinder'''] helps users to explore facts that involve entities of interest within the full text articles of the [[Europe PubMed Central]] database.&lt;ref&gt;{{cite journal| author=The Europe PMC Consortium| title=Europe PMC: a full-text literature database for the life sciences and platform for innovation | journal=Nucleic Acids Research| year= 2014 | volume= 43 | issue=D1 | pages=D1042-D1048 | doi=10.1093/nar/gku1061 | pmid=25378340 | pmc=4383902}}&lt;/ref&gt;

[http://www.nactem.ac.uk/EvidenceFinderAnatomyMK/ '''EUPMC Evidence Finder for Anatomical entities with meta-knowledge'''] &#8211; similar to the Europe PMC EvidenceFinder, allowing exploration of facts involving anatomical entities within the full text articles of the [[Europe PubMed Central]] database.  Facts can be filtered according to various aspects of their interpretation (e.g., negation, certainly level, novelty).

[http://www.nactem.ac.uk/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].

[http://www.nactem.ac.uk/ClinicalTrialProtocols/ '''Clinical Trial Protocols (ASCOT) '''] is an efficient, semantically-enhanced search application, customised for clinical trial documents.&lt;ref&gt;{{cite journal| author=Korkontzelos, I., Mu, T. and Ananiadou, S.| title=ASCOT: a text mining-based web-service for efficient search and assisted creation of clinical trials | journal=BMC Medical Informatics and Decision Making | year= 2012 | volume= 12 | issue= Suppl 1 | pages= S3 | doi=10.1186/1472-6947-12-S1-S3}}&lt;/ref&gt;

[http://www.nactem.ac.uk/hom/ '''History of Medicine (HOM)'''] is a semantic search system over historical medical document archives

==Resources==

[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] &#8211; a large-scale terminological resource for the biomedical domain.&lt;ref&gt;{{cite journal| author=Thompson, P., McNaught, J., Montemagni, S., Calzolari, N., del Gratta, R., Lee, V., Marchi, S., Monachini, M., Pezik, P., Quochi, V., Rupp, C. J., Sasaki, Y., Venturi, G., Rebholz-Schuhmann, D. and Ananiadou, S.| title=The BioLexicon: a large-scale terminological resource for biomedical text mining | journal=BMC Bioinformatics | year= 2011 | volume= 12 | pages=397 | doi=10.1186/1471-2105-12-397}}&lt;/ref&gt;

[http://www.nactem.ac.uk/genia/ '''GENIA'''] &#8211; a collection of reference materials for the development of biomedical text mining systems.

[http://www.nactem.ac.uk/GREC/ '''GREC''']  &#8211; a semantically annotated corpus of [[Medline]] abstracts intended for training IE systems and/or resources which are used to extract events from biomedical literature.&lt;ref&gt;{{cite journal| author=Thompson, P., Iqbal, S. A., McNaught, J. and Ananiadou, S.| title=Construction of an annotated corpus to support biomedical information extraction| journal=BMC Bioinformatics| year= 2009| volume= 10 | pages=349| doi=10.1186/1471-2105-10-349}}&lt;/ref&gt;

[http://www.nactem.ac.uk/metabolite-corpus/ '''Metabolite and Enzyme Corpus''']  &#8211; a corpus of [[Medline]] abstracts annotated by experts with metabolite and enzyme names.

[http://www.nactem.ac.uk/anatomy_corpora/ '''Anatomy Corpora''']  &#8211; A collection of corpora manually annotated with fine-grained, species-independent anatomical entities, to facilitate the development of text mining systems that can carry out detailed and comprehensive analyses of biomedical scientific text.&lt;ref&gt;{{cite journal| author=Pyysalo, S., Ohta, T., Miwa, M., Cho, H. -C., Tsujii, J. and Ananiadou, S.| title=Event extraction across multiple levels of biological organization| journal=Bioinformatics| year= 2012| volume= 28 | issue=18 |pages=i575-i581| doi=10.1093/bioinformatics/bts407}}&lt;/ref&gt;
&lt;ref&gt;{{cite journal|author1=Pyysalo, S.  |author2=Ananiadou, S. |lastauthoramp=yes | title=Anatomical Entity Mention Recognition at Literature Scale| journal=Bioinformatics| year= 2014| volume= 30 | issue=6 |pages=868&#8211;875| doi=10.1093/bioinformatics/btt580}}&lt;/ref&gt;

[http://www.nactem.ac.uk/meta-knowledge/ '''Meta-knowledge corpus''']  &#8211; an enrichment of the [http://www.nactem.ac.uk/genia/ '''GENIA Event corpus'''], in which events are enriched with various levels of information pertaining to their interpretation. The aim is to allow systems to be trained that can distinguish between events that factual information or experimental analyses, definite information from speculated information, etc.&lt;ref&gt;{{cite journal| author=Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S.| title=Enriching a biomedical event corpus with meta-knowledge annotation| journal=BMC Bioinformatics| year= 2011| volume= 12  |pages=393| doi=10.1186/1471-2105-12-393}}&lt;/ref&gt;

==Projects==

[http://nactem.ac.uk/argo/ '''Argo'''] &#8211; The objective of the Argo project is to develop a workbench for analysing (primarily annotating) textual data. The workbench, which is accessed as a web application, supports the combination of elementary text-processing components to form comprehensive processing workflows. It provides functionality to manually intervene in the otherwise automatic process of annotation by correcting or creating new annotations, and facilitates user collaboration by providing sharing capabilities for user-owned resources. Argo benefits users such as text-analysis designers by providing an integrated environment for the development of processing workflows; annotators/curators by providing manual annotation functionalities supported by automatic pre-processing and post-processing; and developers by providing a workbench for testing and evaluating text analytics.

[http://nactem.ac.uk/big_mechanism/ '''Big Mechanism'''] &#8211;  Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by [[DARPA]], the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a 'Robot Scientist' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.

[http://nactem.ac.uk/copious/ '''COPIOUS'''] &#8211; This project aims to produce a knowledge repository of Philippine biodiversity by combining the domain-relevant expertise and resources of Philippine partners with the text mining-based big data analytics of the University of Manchester's National Centre for Text Mining. The repository will be a synergy of different types of information, e.g., taxonomic, occurrence, ecological, biomolecular, biochemical, thus providing users with a comprehensive view on species of interest that will allow them to (1) carry out predictive analysis on species distributions, and (2) investigate potential medicinal applications of natural products derived from Philippine species.

[http://nactem.ac.uk/europepmc/ '''Europe PMC Project'''] &#8211; This is a collaboration with the Text-Mining group at the [[European Bioinformatics Institute]] (EBI) and [[Mimas (data centre)]], forming a work package in the [[Europe PubMed Central]] project (formerly UKPMC) hosted and coordinated by the [[British Library]]. Europe PMC, as a whole, forms a European version of the [[PubMed Central]] paper repository, in collaboration with the [[National Institutes of Health]] (NIH) in the United States. Europe PMC is funded by a consortium of key funding bodies from the biomedical research funders. The contribution to this major project is in the application of text mining solutions to enhance information retrieval and knowledge discovery. As such this is an application of technology developed in other NaCTeM projects on a large scale and in a prominent resource for the Biomedicine community.

[http://nactem.ac.uk/DID-MIBIO/ '''Mining Biodiversity'''] &#8211; This project aims to transform the [[Biodiversity Heritage Library]] (BHL) into a next-generation social digital library resource to facilitate the study and discussion (via social media integration) of legacy science documents on biodiversity by a worldwide community and to raise awareness of the changes in biodiversity over time in the general public. The project integrates novel text mining methods, visualisation, crowdsourcing and social media into the BHL. The resulting digital resource will provide fully interlinked and indexed access to the full content of BHL library documents, via semantically enhanced and interactive browsing and searching capabilities, allowing users to locate precisely the information of interest to them in an easy and efficient manner.

[http://nactem.ac.uk/text-mining-mrc/ '''Mining for Public Health''']  &#8211; This project aims to conduct novel research in text mining and machine learning to transform the way in which evidence-based public health (EBPH) reviews are conducted. The aims of the project are to develop new text mining unsupervised methods for deriving term similarities, to support screening while searching in EBPH reviews and to develop new algorithms for ranking and visualising meaningful associations of multiple types in a dynamic and iterative manner. These newly developed methods will be evaluated in EBPH reviews, based on implementation of a pilot, to ascertain the level of transformation in EBPH reviewing.

==References==
{{Reflist}}

==External links==
* http://www.nactem.ac.uk

[[Category:Computational linguistics]]
[[Category:Computer science organizations]]
[[Category:Information retrieval organizations]]
[[Category:Linguistics organizations]]
[[Category:School of Computer Science, University of Manchester]]</text>
      <sha1>7wh8c64xaeq4arqbrqbvhkinlm0z6jv</sha1>
    </revision>
  </page>
  <page>
    <title>Clearinghouse for Networked Information Discovery and Retrieval</title>
    <ns>0</ns>
    <id>45638306</id>
    <revision>
      <id>666879159</id>
      <parentid>666705116</parentid>
      <timestamp>2015-06-14T09:13:04Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* References */Removed invisible unicode characters + other fixes, removed: &#8206; using [[Project:AWB|AWB]] (11140)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3490" xml:space="preserve">The '''Clearinghouse for Networked Information Discovery and Retrieval''' or '''CNIDR''' was an organization funded by the U.S. [[National Science Foundation]] from 1993 to 1997 and based at the Microelectronics Center of North Carolina (MCNC) in [[Research Triangle Park]].&lt;ref&gt;National Science Foundation, [http://www.nsf.gov/awardsearch/showAward?AWD_ID=9216963 Award Abstract #9216963: Clearinghouse for Network Information Discovery Retrieval]&lt;/ref&gt;&lt;ref&gt;Brett, George. [http://grantome.com/grant/NSF/CNS-9315306 Clearinghouse for Networked Information Discovery and Retrieval (CNIDR)]&lt;/ref&gt;  CNIDR was active in the research and development of open source software and open standards, centered on information discovery and retrieval, in the emerging Internet.

Among the software developed at CNIDR were Isite, an open source [[Z39.50]] implementation and successor to the free version of [[Wide area information server|WAIS]],&lt;ref&gt;Gamiel, Kevin and Nassar, Nassib.  1995.  Structural components of the Isite information system.  In ''Z39.50 Implementation Experiences'', P. Over, R. Denenberg, W. E. Moen, and L. Stovel, Eds.  National Institute of Standards and Technology Special Publication 500-229, US Department of Commerce, Gaithersburg, MD, 71-74.&lt;/ref&gt;&lt;ref&gt;Nebert, Douglas D. and Fullton, James.  [http://www.csdl.tamu.edu/DL95/papers/nebert/nebert.html Use of the ISite Z39.50 software to search and retrieve spatially-referenced data]&lt;/ref&gt;&lt;ref&gt;[http://inkdroid.org/tmp/www-talk/8133.html CNIDR Announces Isite v1.00 Integrated Information System]&lt;/ref&gt;&lt;ref&gt;[http://isite.awcubed.com/Isite.html The Isite Information System]&lt;/ref&gt;&lt;ref&gt;[http://www.loc.gov/z3950/mums.html Library of Congress Search Form]&lt;/ref&gt; and [[Isearch]], an open source text retrieval system.  CNIDR staff were involved in the development of open standards in the [[Internet Engineering Task Force]], the Z39.50 Implementors Group and [[Dublin Core]].&lt;ref&gt;[https://www.ietf.org/meeting/past.html IETF Past Meetings]&lt;/ref&gt;&lt;ref&gt;[http://www.loc.gov/z3950/agency/zig/meetings/output.html ZIG Meeting Output]&lt;/ref&gt;&lt;ref&gt;[http://dublincore.org/workshops/dc1/ DC1: OCLC/NCSA Metadata Workshop: The Essential Elements of Network Object Description]&lt;/ref&gt;

CNIDR collaborated with the [[United States Patent and Trademark Office|U.S. Patent and Trademark Office (USPTO)]] to develop the USPTO's first Internet-based patent search systems.  One of these provided full text searching and images of medical patents related to the research and treatment of HIV/AIDS and issued by the US, Japanese and European patent offices.  Another system, known as the US Patent Bibliographic Database, provided searching of "front page" bibliographic information for all US patents since 1976.&lt;ref&gt;Miller, Annetta.  1994.  [http://www.newsweek.com/new-online-aids-database-186740 "A New Online Aids Database."]  In ''Newsweek'', November 13, 1994.&lt;/ref&gt;&lt;ref&gt;[http://www.pubzpro.com/Pubz/#!search/a/6105 MCNC and U.S. Patent Office Launch Internet AIDS Library]&lt;/ref&gt;&lt;ref&gt;Kawakami, Alice K.  [http://www.istl.org/98-summer/article5.html "Patents and Patent Searching."]&lt;/ref&gt;&lt;ref&gt;[http://www2.iastate.edu/~cyberstacks/hyb_t_7.htm Patents and Trademarks]&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Information retrieval organizations]]
[[Category:Internet Standards]]
[[Category:Internet protocols]]
[[Category:Internet search engines]]
[[Category:Organizations established in 1992]]
[[Category:Computer-related organizations]]</text>
      <sha1>7ta0k9oeyj6ptgo03n32l6tlzifsa6q</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Google</title>
    <ns>14</ns>
    <id>853521</id>
    <revision>
      <id>742382110</id>
      <parentid>742380738</parentid>
      <timestamp>2016-10-03T10:46:12Z</timestamp>
      <contributor>
        <username>Marianna251</username>
        <id>27604025</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/119.82.64.37|119.82.64.37]] ([[User talk:119.82.64.37|talk]]) ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="590" xml:space="preserve">{{Commons category|Google}}
{{Cat main|Google}}

[[Category:Internet search engines]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Internet companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Web portals]]
[[Category:Information retrieval organizations]]
[[Category:Alphabet Inc.]]
[[Category:Wikipedia categories named after information technology companies of the United States]]
[[Category:Wikipedia categories named after websites]]</text>
      <sha1>1bvoyv3r22op39dtt9fextb81e1ypig</sha1>
    </revision>
  </page>
  <page>
    <title>Information Retrieval Facility</title>
    <ns>0</ns>
    <id>16635934</id>
    <revision>
      <id>735904927</id>
      <parentid>731672421</parentid>
      <timestamp>2016-08-23T21:41:54Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>cap, layout</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8860" xml:space="preserve">{{Advert|date=May 2012}}

[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]

The '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.

The IRF had members in the following categories:

* Researchers in [[information retrieval]] (IR) or related scientific areas
* Industrial/corporate information management professionals
* Patent authorities and governmental institutions
* Students of one of the above

==The Scientific Board==
'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]

'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],
Director of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]

'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]

'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]

'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]

'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]

'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]

'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]

'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]

'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]

'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]

'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]

'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]

'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',
[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]

==Scientific goals==

* Modelling innovative and specialised information retrieval systems for global patent document collections.
* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.&lt;
* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.
* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.
* Ability to create different views of patent data depending on the focus of the information need.
* Defining standardised methods for benchmarking the information retrieval process in patent document collections.
* Ability to handle text and non-text parts of a patent in a coherent manner.
* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.
* Integrating the temporal dimension of patent documents in retrieval strategies.
* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.
* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.
* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.
* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.
* Investigating and experimenting with computing architectures for very high-capacity information management.
* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.
* Discovering and investigating novel use cases and business applications deriving from intellectual property information.
* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.
* Development and integration of different information access methods.
* Research on effective methods for interactive information retrieval.

==Semantic supercomputing==
Current technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:

* multi-node clusters (currently 80 cores, up to 1024)
* highest speed interconnect technology
* single system image with large compound memory (currently 320 GB, up to 4 TB)
* fully integrated configurable computing (currently 4 FPGA cores, up to 256)

The combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.

==The World Patent Corpus==
The IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. 
The entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What&#8217;s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.

== Research collections ==
The IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.

The MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.

The ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.

==References==
* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]
* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]

==External links==
* [http://www.ir-facility.org/ Official site: ir-facility.org]
* [https://www.youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] 
* [https://www.youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]

[[Category:Organizations established in 2006]]
[[Category:Computer science organizations]]
[[Category:Information retrieval organizations]]
[[Category:Education in Vienna]]</text>
      <sha1>5u10mb7icqvy0aoyxicwzhrvaxlcvk4</sha1>
    </revision>
  </page>
  <page>
    <title>AUTINDEX</title>
    <ns>0</ns>
    <id>43739701</id>
    <revision>
      <id>666705511</id>
      <parentid>649208076</parentid>
      <timestamp>2015-06-13T01:55:48Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5720" xml:space="preserve">{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
}}

'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.&lt;ref&gt;Ripplinger, B&#228;rbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen&lt;/ref&gt;&lt;ref&gt;Paul Schmidt, Mahmoud Gindiyeh &amp; Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6&#8211;8 October 2009, Portugal&lt;/ref&gt;&lt;ref&gt;Paul Schmidt &amp; Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19&#8211;20 November&lt;/ref&gt;

'''AUTINDEX''' resulting from research in [[information extraction]] &lt;ref&gt;Paul Schmidt, Thomas B&#228;hr &amp; Dr.-Ing. Jens Biesterfeld &amp;Thomas Risse &amp; Kerstin Denecke &amp; Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen f&#252;r die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt&lt;/ref&gt;&lt;ref&gt;Ursula Deriu, J&#246;rn Lehmann &amp; Paul Schmidt, 2009: &#8218;Erstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie&#8217;. In: Proceedings Knowtech, Frankfurt&lt;/ref&gt; is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbr&#252;cken, Germany.

'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX &lt;ref&gt;[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.&lt;/ref&gt;), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch &lt;ref&gt;[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.&lt;/ref&gt; and WISSMER,&lt;ref&gt;[//www.wissmer.info/index.php/de/]. Project Wissmer.&lt;/ref&gt; see also the reference to IAI-Webite.&lt;ref&gt;[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.&lt;/ref&gt;

The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.&lt;ref&gt;Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR &#8211; The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.&#8211;8. Oktober 2009, Portugal. 2009, S. 259 - 262.&lt;/ref&gt; Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.&lt;br&gt; 
AUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems &lt;ref&gt;[//www.wti-frankfurt.de]. WTI Information system.&lt;/ref&gt; and in document management and content management environments. &lt;br&gt; 
 
Together with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] &lt;ref&gt;Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.&lt;/ref&gt; software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].&lt;ref&gt;[//www.wissmer.info]. Electro mobility information system.&lt;/ref&gt;

==See also==

* [[Information retrieval]]
* [[Linguistics]]
* [[Knowledge Management]]
* [[Natural Language Processing]]
* [[Semantics]]

== References ==
{{reflist}}

== Publications ==
* Ripplinger, B&#228;rbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.
* Paul Schmidt, Thomas B&#228;hr &amp; Dr.-Ing. Jens Biesterfeld &amp;Thomas Risse &amp; Kerstin Denecke &amp; Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen f&#252;r die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.
* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR &#8211; The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.&#8211;8. Oktober 2009, Portugal. 2009, S. 259 - 262.
* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.&#8211;20. November 2009.
* R&#246;sener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD f&#252;r Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.
* Svenja Siedle: ''Suchst du noch oder wei&#223;t du schon? Inhaltserschlie&#223;ung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''
* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum f&#252;r Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)
* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.

== External links ==
* http://www.iai-sb.de/ Institute for Applied Information Sciences

[[Category:Natural language processing]]
[[Category:Information retrieval systems]]</text>
      <sha1>oy6cg0i2ew9hmjr9jcompdp8sd6s36w</sha1>
    </revision>
  </page>
  <page>
    <title>Agrep</title>
    <ns>0</ns>
    <id>308939</id>
    <revision>
      <id>739026236</id>
      <parentid>739026203</parentid>
      <timestamp>2016-09-12T10:43:14Z</timestamp>
      <contributor>
        <ip>94.219.155.25</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4209" xml:space="preserve">{{lowercase|title=agrep}}
{{Infobox software
| name                   = agrep
| logo                   = &lt;!-- Image name is enough --&gt;
| logo caption           = 
| logo_size              = 
| logo_alt               = 
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot_size        = 
| screenshot_alt         = 
| collapsible            = 
| developer              = {{Plainlist|
* [[Udi Manber]]
* Sun Wu
}}
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| status                 = 
| programming language   = C
| operating system       = {{Plainlist|
* [[Unix-like]]
* [[OS/2]]
* [[DOS]]
* [[Microsoft Windows|Windows]]
}}
| platform               = 
| size                   = 
| language               = 
| language footnote      = 
| genre                  = [[Pattern matching]]
| license                = [https://raw.githubusercontent.com/Wikinaut/agrep/master/COPYRIGHT ISC open source license]
| standard               = 
| website                = {{URL|http://www.tgries.de/agrep}}
}}

'''agrep''' (approximate [[grep]]) is an [[open-source]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].

It selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.

agrep is also the [[Search engine (computing)|search engine]] in the indexer program [[GLIMPSE]]. agrep is under a free [[ISC License]].&lt;ref&gt;[http://webglimpse.net/sublicensing/licensing.html WebGlimpse, Glimpse and also AGREP license] since 18.09.2014 ([http://opensource.org/licenses/ISC ISC License]).&lt;/ref&gt;

== Alternative implementations ==
A more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.&lt;ref&gt;{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}&lt;/ref&gt; Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].

FREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.&lt;ref&gt;{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}&lt;/ref&gt; However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.

==References==
{{Reflist}}

==External links==
* Wu-Manber agrep
**[http://www.tgries.de/agrep AGREP home page]
**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add &lt;code&gt;-Wno-return-type&lt;/code&gt; to the &lt;code&gt;CFLAGs  = -O&lt;/code&gt; line in the Makefile)
*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph's Personal Wiki]
*See also
**[http://laurikari.net/tre TRE regexp matching package]
**[https://web.archive.org/web/20080513225010/http://www1.bell-labs.com/project/wwexptools/cgrep/ cgrep a defunct command line approximate string matching tool]
**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool
**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]

[[Category:Information retrieval systems]]
[[Category:Unix text processing utilities]]
[[Category:Software using the ISC license]]</text>
      <sha1>hkizr3c9na791kzxmzi3mcnkqsudbu3</sha1>
    </revision>
  </page>
  <page>
    <title>MAREC</title>
    <ns>0</ns>
    <id>24979660</id>
    <revision>
      <id>666715167</id>
      <parentid>525865765</parentid>
      <timestamp>2015-06-13T03:58:02Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3401" xml:space="preserve">{{other uses}}
The '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.&lt;ref&gt;Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland&lt;/ref&gt;&lt;ref&gt;Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition&lt;/ref&gt; It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.

MAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.&lt;ref&gt;Manning, C. D. and Sch&#252;tze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.&lt;/ref&gt; The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.

In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.&lt;ref&gt;European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)&lt;/ref&gt;

MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics &#8211; news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.&lt;ref&gt;J&#228;rvelin A. , Talvensaari T. , J&#228;rvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)&lt;/ref&gt; Since the patent document refers to the same &#8220;invention&#8221; or &#8220;concept of idea&#8221; the text is a translation of the invention, but it does not have to be a direct translation of the text itself &#8211; text parts could have been removed or added for clarification reasons.

The 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.

== Use Cases ==
* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.

== References ==
{{Reflist}}

== External links ==
* [http://www.ir-facility.org/prototypes/marec User guide and statistics]
* [http://ir-facility.org Information Retrieval Facility]

[[Category:Corpora]]
[[Category:Information retrieval systems]]
[[Category:Machine translation]]
[[Category:Natural language processing]]
[[Category:XML]]</text>
      <sha1>n66or125eserscibgq6yq690rl4fw0g</sha1>
    </revision>
  </page>
  <page>
    <title>Phynd</title>
    <ns>0</ns>
    <id>2353165</id>
    <revision>
      <id>749640039</id>
      <parentid>726558084</parentid>
      <timestamp>2016-11-15T13:10:58Z</timestamp>
      <contributor>
        <username>Zyxw</username>
        <id>473593</id>
      </contributor>
      <minor />
      <comment>/* External links */update archive links using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2408" xml:space="preserve">'''Phynd''' (Find) is a LAN-indexing [[search engine]] used to facilitate [[peer-to-peer]] [[file sharing]] over a [[local-area network]].  It was developed by [[Rensselaer Polytechnic Institute]] student researcher Jesse Jordan to solve various problems experienced by Microsoft browsers and networks when trying to index files within a large network.  

One of the results of Jordan's file indexing exercise was that large numbers of downloaded music files were found on other users' local systems.  Jordan was relatively unconcerned with the nature of the content he was indexing.  His objective was enabling a network to index all its files without crashing any elements of the network. &lt;ref&gt;Lessig 2004, p. 48&lt;/ref&gt;

Although Jordan's search engine, Phynd, merely indexed public data that users elected to share through an integrated sharing feature in [[Microsoft Windows]], Jordan was sued by [[RIAA]] for copyright infringement. The original Phynd search engine, rpi.phynd.net (defunct), existing years before and months after Jesse's lawsuit was shut down by the enormous pressure that the [[RIAA]] in November 2003 brought upon Jordan and his family. The RIAA was demanding $15,000,000 to settle.&lt;ref&gt;Lessig 2004, p. 51&lt;/ref&gt;  As a student researcher, Jordan had only modest life savings of approximately $12,000, and his family had only modest assets.  His limited options were to fight the RIAA at enormous personal expense, or to settle. Jordan, chose to settle outside of court for $12,000, his entire life savings from student employment. He subsequently raised $12,005.67 via contributions on a personal web site in July 2003.

==References==
*Lessig, Lawrence (2004) . "Free Culture" . ISBN 1-59420-006-8 . The Penguin Press . New York 

==Notes==
{{reflist}} 


==External links==
* [http://poly.rpi.edu/old/article_view.php3?view=2599&amp;part=1 Phynd server shut down by threat of lawsuit]
* {{webarchive |date=2013-01-20 |url=http://archive.is/20130120043253/http://news.com.com/2100-1027-995429.html?tag=fd_lede1_hed |title=RIAA sues campus file-swappers}}
* [http://www.isp-planet.com/news/2003/riaa_030505.html Students to Pay in RIAA Song-Swapping Suit]
* [http://articles.chicagotribune.com/2003-07-07/news/0307080008_1_recording-industry-donations-settlement $12,005.67: Amount Jesse Jordan, sued by the recording...]

[[Category:Information retrieval systems]]

{{compu-network-stub}}</text>
      <sha1>4dzg7d2qtek3na3okyuiqppm3t7htj9</sha1>
    </revision>
  </page>
  <page>
    <title>RetrievalWare</title>
    <ns>0</ns>
    <id>23327147</id>
    <revision>
      <id>666716823</id>
      <parentid>572116340</parentid>
      <timestamp>2015-06-13T04:23:16Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8795" xml:space="preserve">{{COI|date=June 2009}}
{{Infobox software
| name                   = RetrievalWare
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Fast Search &amp; Transfer]], [[Convera]], Excalibur Technologies, ConQuest Software, Microsoft
| latest release version = 8.2
| latest release date    = {{release date|2006|10|13}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]]
| genre                  = [[Search algorithm|Search]] and [[Index (search engine)|Index]]
| website                =
}}
'''RetrievalWare''' is an [[enterprise search|enterprise search engine]] emphasizing [[natural language processing]] and [[semantic networks]] which was commercially available from 1992 to 2007 and is especially known for its use by government intelligence agencies.&lt;ref&gt;{{cite news
| url = http://www.washingtonpost.com/ac2/wp-dyn/A30161-2004Dec2
| title = Agencies Find What They're Looking For|publisher = The Washington Post
| date = 2004-12-03
| first=David A.
| last=Vise
| accessdate=2010-05-22
}}&lt;/ref&gt;

== History ==

RetrievalWare was initially created by [http://www.linkedin.com/pub/paul-nelson/3/316/146 Paul Nelson], [http://kenclark7.home.comcast.net/~kenclark7/ Kenneth Clark], and [http://www.linkedin.com/in/edaddison Edwin Addison] as part of ConQuest Software. Development began in 1989, but the software was not commercially available on a wide scale until 1992. Early funding was provided by [[Rome Laboratory]] via a [[Small Business Innovation Research]] grant.&lt;ref&gt;
{{citation
| title = FY 1991 SBIR SOLICITATION - PHASE I AWARD ABSTRACTS - AIR FORCE PROJECTS - VOLUME III
| pages = 70&#8211;71
| date = 1992-07-06
| url = http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA252509&amp;Location=U2&amp;doc=GetTRDoc.pdf
}} - Note that "Synchronetics" was the original name for ConQuest Software Incorporated.
&lt;/ref&gt;

On July 6, 1995, ConQuest Software was merged with Excalibur Technologies&lt;ref&gt;{{cite press release
 | title = Excalibur Technologies to merge with ConQuest Software; text and multimedia information retrieval leaders join forces to expand products, channels and markets| publisher = Business Wire
 | date = 1995-07-06
 | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_July_6/ai_17215774/?tag=content;col1
 }}&lt;/ref&gt; and the product was rebranded as RetrievalWare. On December 21, 2000, Excalibur Technologies was combined with [[Intel|Intel Corporation]]'s Interactive Media Services division to form the [[Convera|Convera Corporation]].&lt;ref&gt;{{cite news
 | title = Intel and Excalibur Form Convera Corporation| publisher = Silicon Valley / San Jose Business Journal
 | date = 2000-12-21
 | url = http://sanjose.bizjournals.com/sanjose/stories/2000/12/25/daily5.html
 }}&lt;/ref&gt; Finally, on April 9, 2007, the RetrievalWare software and business was purchased by [[Fast Search &amp; Transfer]] at which point the product was officially retired.&lt;ref name="fastpurchase"&gt;{{cite news
 | title = FAST Acquires Convera&#8217;s RetrievalWare Business| publisher = Information Today, Inc.
 | date = 2007-04-09
 | url = http://newsbreaks.infotoday.com/NewsBreaks/FAST-Acquires-Converas-RetrievalWare-Business-35840.asp
 | quote = While FAST will continue to support the RetrievalWare platform, it will not continue development on it or add new features. RetrievalWare customers will be offered an upgrade path to FAST&#8217;s own offering.
 }}&lt;/ref&gt; [[Microsoft|Microsoft Corporation]] continues to maintain the product for its existing customer base.

Annual revenues for RetrievalWare peaked in 2001 at around $40 million US dollars.&lt;ref&gt;{{citation
| title = Convera Corp &#183; 10-K &#183; For 1/1/01
| date = 2001-01-01
| url = http://www.secinfo.com/d12B5f.4f89a.c.htm
}} - Indicates that Convera products accounted for 85% of the total revenue of $51.5 million.&lt;/ref&gt;

== Use of natural language techniques ==

RetrievalWare is a relevancy ranking text search system with processing enhancements drawn from the fields of [[natural language processing|natural language processing (NLP)]] and [[semantic networks]]. NLP algorithms include dictionary-based [[stemming]] (also known as [[lemmatisation]]) and dictionary-based phrase identification. Semantic networks are used by RetrievalWare to expand the query words entered by the user to related terms with terms weights determined by the distance from the user's original terms. In addition to automatic expansion, a feedback-mode whereby users could choose the meaning of the word before performing the expansion was available. The first semantic networks were built using [[WordNet]].

In addition, RetrievalWare implemented a form of [[n-gram]] search (branded as APRP - Adaptive Pattern Recognition Processing&lt;ref&gt;[http://www.thefreelibrary.com/Excalibur+Announces+Excalibur+RetrievalWare+6.5+Featuring+...-a019849416 Excalibur Announces Excalibur RetrievalWare 6.5 Featuring RetrievalWare FileRoom] - Contains a description of APRP&lt;/ref&gt;), designed to search over documents with [[Optical character recognition|OCR]] errors. Query terms are divided into sets of 2-grams which are used to locate similarly matching terms from the [[inverted index]]. The resulting matches are weighted based on similarly measures and then used to search for documents.

All of these features were available no later than 1993&lt;ref name="trec2"&gt;[http://trec.nist.gov/pubs/trec2/papers/txt/25.txt Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC2)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec2/t2_proceedings.html here]&lt;/ref&gt; and ConQuest software has claimed that it was the first commercial text-search system to implement these techniques.&lt;ref&gt;{{cite press release
 | title = Homework Helper debuts on Prodigy using ConQuest search engine| publisher = Business Wire
 | date = 1995-02-09
 | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_Feb_9/ai_16432681/
 | quote = ConQuest is the only search engine which uses dictionaries, thesauri and other lexical resources to build in a semantic knowledgebase of over 440,000 word meanings, and 1.6 million word relationships.
 }}&lt;/ref&gt;

== Other notable features ==

Other notable features of RetrievalWare include distributed search servers,&lt;ref name="trec2"/&gt; synchronizers for indexing external [[content management system]]s and [[relational database]]s,&lt;ref name="kmref"&gt;{{cite news
| url = http://www.kmworld.com/Articles/Editorial/Feature/Excalibur-RetrievalWare-more-than-information-retrieval--9139.aspx
| title = Excalibur RetrievalWare: more than information retrieval
| publisher = KMWorld
| date = 1999-10-01
}}&lt;/ref&gt; a heterogeneous security model,&lt;ref name="kmref"/&gt; [[document classification|document categorization]],&lt;ref name="kmref"/&gt; real-time document-query matching (profiling),&lt;ref name="trec2"/&gt; multi-lingual searches (queries containing terms from multiple languages searching for documents containing terms from multiple languages), and cross-lingual searches (queries in one language searching for documents in a different language).&lt;ref&gt;{{cite news
| title = Multimedia search, retrieval, categorization
| url = http://www.kmworld.com/Articles/News/Breaking-News/Multimedia-search,-retrieval,-categorization-12763.aspx
| date = 2002-03-25
| publisher = KMWorld
}}&lt;/ref&gt;

== Participation in TREC ==

RetrievalWare participated in the [[Text REtrieval Conference (TREC)|Text REtrieval Conference]] in 1992 (TREC-1), 1993 (TREC-2), and 1995 (TREC-4).

In TREC-1&lt;ref name="trec1"&gt;[http://trec.nist.gov/pubs/trec1/papers/21.txt   Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC-1)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec1/t1_proceedings.html here]&lt;/ref&gt; and TREC-4,&lt;ref&gt;[http://trec.nist.gov/pubs/trec4/papers/excalibur.ps.gz The Excalibur TREC-4 System, Preparations, and Results] - A PDF version of which can be found [http://www.pnelsoncomposer.com/writings/excalibur-trec4.pdf here] and the complete proceedings can be found [http://trec.nist.gov/pubs/trec4/t4_proceedings.html here]&lt;/ref&gt; the RetrievalWare runs for manually entered queries produced the best results based on the 11-point averages over all search engines which participated in the ''ad hoc'' category where search engines are allowed a single opportunity to process previously unknown queries against an existing database.

== References ==
{{Reflist}}

== External links ==

*  [http://www.saoug.org.za/archive/1999/9907.pdf Marketing presentation on RetrievalWare semantic networks and adaptive pattern recognition algorithms]

{{DEFAULTSORT:Retrievalware}}
[[Category:Information retrieval systems]]</text>
      <sha1>e0pjugk02u3pub6ptf4n5cigu4jxfcv</sha1>
    </revision>
  </page>
  <page>
    <title>Database search engine</title>
    <ns>0</ns>
    <id>7330158</id>
    <revision>
      <id>754379935</id>
      <parentid>689538597</parentid>
      <timestamp>2016-12-12T10:08:00Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>/* See also */ lists of related topics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1943" xml:space="preserve">A '''database search engine''' is a [[search engine]] that operates on material stored in a digital [[database]].

== Search engines ==

Categories of search engine software include: 
* Web search or full-text search (e.g. [[Lucene]]).
* [[Database]] or [[structured data]] search (e.g. [[Dieselpoint]]).
* Mixed or [[enterprise search]] (e.g. [[Google Search Appliance]]). 

The largest online directories, such as [[Google]] and [[Yahoo]], utilize thousands of computers to process billions of website documents using [[web crawlers]] or [[spiders (software)]], returning results for thousands of searches per second. Processing high query volumes requires software to run in a distributed environment with redundancy.

== Components ==

Searching for textual content in [[databases]] or [[structured data]] formats (such as [[XML]] and [[Comma-separated values|CSV]]) presents special challenges and opportunities which specialized search engines resolve. [[Databases]] allow logical queries such as the use of multi-field [[Boolean logic]], while full-text searches do not. "Crawling" (a human by-eye search) is not necessary to find information stored in a database because the data is already structured. [[index (datatbase)|Indexing]] the data allows for faster searches.

Database search engines are usually included with major database software products. 

== Applications ==

Database search technology is used by large public and private entities including government database services, e-commerce companies, online advertising platforms, telecommunications service providers and other consumers with a need to access information in large repositories.

==See also==

*[[Outline of search engines]]
*[[List of search engines]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Information retrieval systems]]</text>
      <sha1>ekhagsu3irs74gw0504opjysdgbfhn2</sha1>
    </revision>
  </page>
  <page>
    <title>Search engine (computing)</title>
    <ns>0</ns>
    <id>27804</id>
    <revision>
      <id>744163945</id>
      <parentid>722592416</parentid>
      <timestamp>2016-10-13T14:12:39Z</timestamp>
      <contributor>
        <ip>199.15.104.190</ip>
      </contributor>
      <comment>Lisa wichkoski</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5310" xml:space="preserve">{{multiple issues|
{{more footnotes|date=August 2014}}
{{one source|date=August 2014}}
}}

A '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}

The most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].

==How search engines work==
Search engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[Web search query|search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.&lt;ref&gt;Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.&lt;/ref&gt; There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].

[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]

The list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive nor gate|XOR]]) in a probabilistic context.

To provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[Serpent (album)|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.

Other types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.

==Types of search engines==

; By source

*[[Desktop search]]
*[[Federated search]]
*[[Human search engine]]
*[[Metasearch engine]]
*[[Multisearch]]
*[[Search aggregator]]
*[[Web search engine]]

; By content type

*[[Full text search]]
*[[Image search]]
*[[Video search engine]]

; By interface

*[[Incremental search]]
*[[Instant answer]]
*[[Semantic search]]
*[[Selection-based search]]

; By topic

*[[Bibliographic database]]
*[[Enterprise search]]
*[[Medical literature retrieval]]
*[[Vertical search]]

==See also==
{{Portal|Computer Science}}
{{div col|colwidth=30em}}
*[[Automatic summarization]]
*[[Emanuel Goldberg]] (inventor of early search engine)
*[[Index (search engine)]]
*[[Inverted index]]
*[[List of search engines]]
*[[List of enterprise search vendors]]
*[[Search engine optimization]]
*[[Search suggest drop-down list]]
* [[Solver (computer science)]]
*[[Spamdexing]]
*[[SQL]]
*[[Text mining]]
{{div col end}}

==References==
{{Reflist}}
{{Internet search}}

{{Authority control}}
{{DEFAULTSORT:Search Engine (Computing)}}
[[Category:Information retrieval systems]]</text>
      <sha1>bi2y6wshmxtidx95o72za2ne8whsvpl</sha1>
    </revision>
  </page>
  <page>
    <title>Relevance feedback</title>
    <ns>0</ns>
    <id>5818361</id>
    <revision>
      <id>727842398</id>
      <parentid>703409284</parentid>
      <timestamp>2016-07-01T15:53:37Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* Blind feedback */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 10 to &#8594; between 10 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8071" xml:space="preserve">'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.

== Explicit feedback ==

Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.

Users may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.

The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

A performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].

== Implicit feedback ==

Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf]. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response&lt;ref&gt;Jansen, B. J. and McNeese, M. D. 2005. [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_assistance_jasist2005.pdf Evaluating the effectiveness of and patterns of interactions with automated assistance in IR systems]. Journal of the American Society for Information Science and Technology. 56(14), 1480-1503&lt;/ref&gt;&lt;ref&gt;Kelly, Diane, and Jaime Teevan. "Implicit feedback for inferring user preference: a bibliography." ACM SIGIR Forum. Vol. 37. No. 2. ACM, 2003.&lt;/ref&gt;

The key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:

# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and
# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback

An example of this is [[dwell time (information retrieval)|dwell time]], which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.
Another example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.

== Blind feedback ==

Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:

# Take the results returned by initial query as relevant results (only top k with k being between 10 and 50 in most experiments).
# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.
# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.

Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.

This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.&lt;ref&gt;Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.&lt;/ref&gt; Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.&lt;ref&gt;Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.&lt;/ref&gt; 
Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.

Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.

== Using relevance information ==

Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

==Further reading==
*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's
*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''
*Stefan B&#252;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

== References ==
{{reflist|2}}

[[Category:Internet search algorithms]]
[[Category:Information retrieval evaluation]]
[[zh:&#30456;&#20851;&#21453;&#39304;]]</text>
      <sha1>3px44bcr09bhfa7my6y6xoy5g4c52nk</sha1>
    </revision>
  </page>
  <page>
    <title>Precision and recall</title>
    <ns>0</ns>
    <id>14343887</id>
    <revision>
      <id>762972179</id>
      <parentid>755633573</parentid>
      <timestamp>2017-01-31T18:41:11Z</timestamp>
      <contributor>
        <ip>63.66.64.244</ip>
      </contributor>
      <comment>/* Introduction */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18105" xml:space="preserve">[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]
In [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]].

Suppose a computer program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a [[Search engine (computing)|search engine]] returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.  So, in this case, precision is "how useful the search results are", and recall is "how complete the results are".

In [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &amp;minus; 4 = 3 type I errors and 9 &amp;minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.

In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.

==Introduction==
In an information retrieval scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.

In a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of true positives'' (i.e. the number of items correctly labeled as belonging to the positive class) ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and [[Type I and type II errors|false positives]], which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and [[Type I and type II errors|false negatives]], which are items which were not labeled as belonging to the positive class but should have been).

In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).

In a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).

Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient&#8217;s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).

Usually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s [[Informedness]] (DeltaP') and [[Markedness]] (DeltaP).&lt;ref name="Powers2011" /&gt;&lt;ref&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97&#8211;119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt; [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).&lt;ref name="Powers2011"/&gt; Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.&lt;ref name="Powers2011"/&gt;  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, [[Informedness]] and [[Markedness]] are Kappa-like renormalizations of Recall and Precision,&lt;ref&gt;{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}&lt;/ref&gt; and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.

== Definition (information retrieval context) ==

In [[information retrieval]] contexts, precision and recall are defined in terms of a set of ''retrieved documents'' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of ''relevant documents'' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]]. The measures were defined in {{harvtxt|Perry|Kent|Berry|1955}}.

===Precision===

In the field of [[information retrieval]], precision is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:

&lt;math display="block"&gt; \text{precision}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|} &lt;/math&gt;

Precision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

For example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.

Precision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.

===Recall===

Recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.

&lt;math display="block"&gt; \text{recall}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{relevant documents}\}|} &lt;/math&gt;

For example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned.

In binary classification, recall is called [[Sensitivity and specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

== Definition (classification context) ==
For classification tasks, the terms ''true positives'', ''true negatives'', ''false positives'', and ''false negatives'' (see [[Type I and type II errors]] for definitions) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation'').

Let us define an experiment from ''P'' positive instances and ''N'' negative instances for some condition. The four outcomes can be formulated in a 2&#215;2 [[contingency table]] or [[confusion matrix]], as follows:

{{DiagnosticTesting_Diagram}}
{{Confusion matrix terms}}

&lt;!--
{| border="0" align="center" style="text-align: center; background: #FFFFFF;"
|+
!
! colspan="2" style="background: #ddffdd;"|actual class &lt;br/&gt; (observation)
|-
!
|-----
|+
! rowspan="2" style="background: #ffdddd;"|predicted class &lt;br/&gt; (expectation)
| '''tp''' &lt;br&gt; (true positive) &lt;br/&gt; Correct result
| '''fp''' &lt;br&gt; (false positive) &lt;br/&gt; Unexpected result
|-bgcolor="#EFEFEF"
| '''fn''' &lt;br&gt; (false negative) &lt;br/&gt; Missing result
| '''tn''' &lt;br&gt; (true negative) &lt;br/&gt; Correct absence of result
|+
|}

--&gt;

Precision and recall are then defined as:&lt;ref name="OlsonDelen"&gt;Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1&lt;/ref&gt;

&lt;math display="block"&gt;\text{Precision}=\frac{tp}{tp+fp} \, &lt;/math&gt;

&lt;math display="block"&gt;\text{Recall}=\frac{tp}{tp+fn} \, &lt;/math&gt;

Recall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy and precision#In binary classification|accuracy]].&lt;ref name="OlsonDelen" /&gt; True negative rate is also called [[Specificity (tests)#Specificity|specificity]].

&lt;math display="block"&gt;\text{True negative rate}=\frac{tn}{tn+fp} \, &lt;/math&gt;

&lt;math display="block"&gt;\text{Accuracy}=\frac{tp+tn}{tp+tn+fp+fn} \, &lt;/math&gt;

== Probabilistic interpretation ==

It is possible to interpret precision and recall not as ratios but as probabilities:

* Precision is the probability that a (randomly selected) retrieved document is relevant.
* Recall is the probability that a (randomly selected) relevant document is retrieved in a search.

Note that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by ''randomly selected retrieved document'', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected.

Note that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation).

Another interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.

== F-measure ==
{{main article|F1 score}}
A measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:

&lt;math display="block"&gt;F = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{ \mathrm{precision} + \mathrm{recall}}&lt;/math&gt;

This measure is approximately the average of the two when they are close, and is more generally the [[harmonic mean]], which, for the case of two numbers, coincides with the square of the [[geometric mean]] divided by the [[arithmetic mean]]. There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric.&lt;ref name="Powers2011" /&gt; This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

It is a special case of the general &lt;math&gt;F_\beta&lt;/math&gt; measure (for non-negative real values of&amp;nbsp;&lt;math&gt;\beta&lt;/math&gt;):

&lt;math display="block"&gt;F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}&lt;/math&gt;

Two other commonly used &lt;math&gt;F&lt;/math&gt; measures are the &lt;math&gt;F_2&lt;/math&gt; measure, which weights recall higher than precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which puts more emphasis on precision than recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; "measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E_{\alpha} = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;, the second term being the weighted harmonic mean of precision and recall with weights &lt;math&gt;(\alpha, 1-\alpha)&lt;/math&gt;.  Their relationship is &lt;math&gt;F_\beta = 1 - E_{\alpha}&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;.

==Limitations as goals==
There are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).&lt;ref&gt;Zygmunt Zaj&#261;c. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/&lt;/ref&gt;

For [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,&lt;ref&gt;Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']&lt;/ref&gt;
{{quote|[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).
* Search results don't have to be very good.
* Recall?    Not important (as long as you get at least some good hits).
* Precision? Not important (as long as at least some of the hits on the first page you return are good).}}

==See also==
* [[Uncertainty coefficient]], also called ''proficiency''
* [[Sensitivity and specificity]]

== References ==
{{Reflist}}
{{refbegin}}
* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X
* Hj&#248;rland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237
* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''
* {{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |year=1955 |doi=10.1002/asi.5090060411}}
* van Rijsbergen, Cornelis Joost "Keith" (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4
{{refend}}

== External links ==
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval &#8211; C. J. van Rijsbergen 1979]
* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]

[[Category:Information retrieval evaluation]]
[[Category:Information science]]
[[Category:Bioinformatics]]

[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]</text>
      <sha1>1o065lccur4zt25gka29nsba8m7kosd</sha1>
    </revision>
  </page>
  <page>
    <title>Mooers' law</title>
    <ns>0</ns>
    <id>11373842</id>
    <revision>
      <id>755331442</id>
      <parentid>747593630</parentid>
      <timestamp>2016-12-17T11:05:53Z</timestamp>
      <contributor>
        <username>Furrykef</username>
        <id>17163</id>
      </contributor>
      <minor />
      <comment>/* Original interpretation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3622" xml:space="preserve">{{For|the observation regarding integrated circuits|Moore's law}}
{{Refimprove|date=September 2011}}

'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.

{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]&lt;ref name="morville"&gt;{{cite book|url=https://books.google.com/books?id=xJNLJXXbhusC&amp;printsec=frontcover&amp;dq=isbn:9780596007652&amp;hl=en&amp;sa=X&amp;ei=qvWhT5DfHITs2QX1rNzPCA&amp;ved=0CDAQ6AEwAA#v=onepage&amp;q=mooers'%20law&amp;f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology &amp; Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}&lt;/ref&gt;}}

==Original interpretation==

Mooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the user's personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.&lt;ref&gt;{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}&lt;/ref&gt;

==Out-of-context interpretation==

The more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:

{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}
{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit &lt;ref name="morville"/&gt;}}

In this interpretation, "painful and troublesome" comes from ''using'' the retrieval system.

==References==
{{reflist}}

*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=607&#8211;609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}

==External links==
* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.
* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.
[[Category:Empirical laws]]
[[Category:Information retrieval evaluation]]</text>
      <sha1>9py7o3ehrqar0r7k3ra8xcmibf9u8mt</sha1>
    </revision>
  </page>
  <page>
    <title>Queries per second</title>
    <ns>0</ns>
    <id>26039201</id>
    <revision>
      <id>715385301</id>
      <parentid>705017813</parentid>
      <timestamp>2016-04-15T13:08:34Z</timestamp>
      <contributor>
        <username>Rich Farmbrough</username>
        <id>82835</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="714" xml:space="preserve">{{distinguish-redirect|Query rate|Query throughput}}
'''Queries per second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.&lt;ref&gt;[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]&lt;/ref&gt; The term is used more broadly for any [[request&#8211;response]] system, more correctly called [[requests per second]] (RPS).

High-traffic systems must watch their QPS in order to know when to scale the system to handle more load.

== References ==
{{reflist}}

[[Category:Units of frequency]]
[[Category:Information retrieval evaluation]]

{{computer-stub}}</text>
      <sha1>sataj5lgtlryp2ngmkol7cthkdt7gu0</sha1>
    </revision>
  </page>
  <page>
    <title>Matthews correlation coefficient</title>
    <ns>0</ns>
    <id>12306500</id>
    <revision>
      <id>757297687</id>
      <parentid>757183084</parentid>
      <timestamp>2016-12-29T22:14:22Z</timestamp>
      <contributor>
        <username>Erotemic</username>
        <id>18610581</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7489" xml:space="preserve">The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]], introduced by biochemist [[Brian Matthews (biochemist)|Brian W. Matthews]] in 1975.&lt;ref name="Matthews1975"&gt;{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442&#8211;451|doi=10.1016/0005-2795(75)90109-9}}&lt;/ref&gt; It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &amp;minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &amp;minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 2&#215;2 [[contingency table]]

: &lt;math&gt;|\text{MCC}| = \sqrt{\frac{\chi^2}{n}}&lt;/math&gt;

where ''n'' is the total number of observations.

While there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures.&lt;ref name="Powers2011"/&gt; Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.

The MCC can be calculated directly from the [[confusion matrix]] using the formula:

: &lt;math&gt;
\text{MCC} = \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;

In this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.

The original formula as given by Matthews was:&lt;ref name=Matthews1975 /&gt;
: &lt;math&gt;
\text{N} = TN + TP + FN + FP
&lt;/math&gt;
: &lt;math&gt;
\text{S} = \frac{ TP + FN } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{P} = \frac{ TP + FP } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{MCC} = \frac{ TP / N - S \times P } {\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }
&lt;/math&gt;

This is equal to the formula given above. As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[Markedness]] (&#916;p) and [[Youden's J statistic]] ([[Informedness]] or &#916;p').&lt;ref name="Powers2011"&gt;{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37&#8211;63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}&lt;/ref&gt;&lt;ref name="Perruchet2004"&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97&#8211;119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt; [[Markedness]] and [[Informedness]] correspond to different directions of information flow and generalize [[Youden's J statistic]], the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.&lt;ref name="Powers2011"/&gt;

== Confusion Matrix ==
{{main article|Confusion matrix}}

{{Confusion matrix terms|recall=}}

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2&#215;2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

== Multiclass Case ==
The Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the  &lt;math&gt;R_K&lt;/math&gt; statistic (for K different classes) by the author, and defined in terms of a &lt;math&gt;K\times K&lt;/math&gt; confusion matrix &lt;math&gt;C&lt;/math&gt;
&lt;ref name="gorodkin2004comparing"&gt;{{cite journal|last=Gorodkin|first=Jan|title=Comparing two K-category assignments by a K-category correlation coefficient|journal=Computational biology and chemistry|date=2004|volume=28|number=5|pages=367&#8211;374|publisher=Elsevier}}&lt;/ref&gt;
.&lt;ref name="GorodkinRk2006"&gt;{{cite web|last1=Gorodkin|first1=Jan|title=The Rk Page|url=http://rk.kvl.dk/introduction/index.html|website=The Rk Page|accessdate=28 December 2016}}&lt;/ref&gt;

:&lt;math&gt;
\text{MCC} = \frac{\sum_{k}\sum_{l}\sum_{m} C_{kk}C_{lm} - C_{kl}C_{mk}}{
\sqrt{
\sum_{k}(\sum_l C_{kl} )(\sum_{k' | k' \neq k}\sum_{l'} C_{k'l'})
}
\sqrt{
\sum_{k}(\sum_l C_{lk} )(\sum_{k' | k' \neq k}\sum_{l'} C_{l'k'})
}
}
&lt;/math&gt;

When there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1. 

&lt;!-- 
TODO: potentially un-comment later, for now just stick with referenced version

This formula can be more easily understood by defining intermediate variables: 
* &lt;math&gt;t_k=\sum_i C_{ik}&lt;/math&gt; the number of times class k truly occurred, 
* &lt;math&gt;p_k=\sum_i C_{ki}&lt;/math&gt; the number of times class k was predicted, 
* &lt;math&gt;c=\sum_{k} C_{kk}&lt;/math&gt; the total number of samples correctly predicted, 
* &lt;math&gt;s=\sum_i \sum_j C_{ij}&lt;/math&gt; the total number of samples. This allows the formula to be expressed as:

:&lt;math&gt;
\text{MCC} = \frac{cs - \vec{t} \cdot \vec{p}}{
\sqrt{s^2 - \vec{p} \cdot \vec{p}}
\sqrt{s^2 - \vec{t} \cdot \vec{t}}
}
&lt;/math&gt;
--&gt;

== See also ==
* [[Phi coefficient]]
* [[F1 score]]
* [[Cram&#233;r's V (statistics)|Cram&#233;r's V]], a similar measure of association between nominal variables.
* [[Cohen's kappa]]

== References ==

{{Reflist}}

&lt;!--should reference in the main text  === General References ===
* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" ''Bioinformatics'' 2000, 16, 412&amp;ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]
* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]
--&gt;

{{DEFAULTSORT:Matthews Correlation Coefficient}}
[[Category:Machine learning]]
[[Category:Information retrieval evaluation]]
[[Category:Statistical classification]]
[[Category:Computational chemistry]]
[[Category:Cheminformatics]]
[[Category:Bioinformatics]]
[[Category:Statistical ratios]]
[[Category:Summary statistics for contingency tables]]</text>
      <sha1>tw7trbdj0pd9182rxy5aap9zj8uu3vt</sha1>
    </revision>
  </page>
  <page>
    <title>Type-1 OWA operators</title>
    <ns>0</ns>
    <id>33591382</id>
    <revision>
      <id>730987765</id>
      <parentid>730928859</parentid>
      <timestamp>2016-07-22T04:55:21Z</timestamp>
      <contributor>
        <username>Alvin Seville</username>
        <id>8629244</id>
      </contributor>
      <comment>removing spaces</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10595" xml:space="preserve">{{Underlinked|date=July 2016}}

The [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]{{R|yagerOWA}} are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and  multi-criteria/multi-expert decision making).{{R|Yager|YagerBeliakov}} It is widely accepted that [[Fuzzy set]]s{{R|Zadeh}} are more suitable for representing preferences of criteria in decision making.

The type-1 OWA operators{{R|fssT1OWA|kdeT1OWA}} have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and [[data mining]], where these uncertain objects are modelled by fuzzy sets.

The two definitions for type-1 OWA operators are based on Zadeh's Extension Principle and &lt;math&gt;\alpha&lt;/math&gt;-cuts of fuzzy sets. The two definitions lead to equivalent results.

==Definitions==

===Definition 1===
Let &lt;math&gt;F(X)&lt;/math&gt; be the set of fuzzy sets with domain of discourse &lt;math&gt;X&lt;/math&gt;, a type-1 OWA operator is defined as follows:{{R|kdeT1OWA}}

Given n linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i = 1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,1]&lt;/math&gt;, a type-1 OWA operator is a mapping, &lt;math&gt;\Phi&lt;/math&gt;,

:&lt;math&gt;\Phi \colon F(X)\times \cdots \times F(X)  \longrightarrow  F(X)&lt;/math&gt;
:&lt;math&gt;(A^1 , \cdots ,A^n)  \mapsto   Y&lt;/math&gt;

such that

:&lt;math&gt;\mu _{Y} (y) =\displaystyle \sup_{\displaystyle \sum_{k =1}^n \bar {w}_i a_{\sigma (i)}  = y }\left({\begin{array}{*{1}l}\mu _{W^1 } (w_1 )\wedge \cdots \wedge \mu_{W^n } (w_n )\wedge \mu _{A^1 } (a_1 )\wedge \cdots \wedge \mu _{A^n } (a_n )\end{array}}\right)&lt;/math&gt;

where &lt;math&gt;\bar {w}_i = \frac{w_i }{\sum_{i = 1}^n {w_i } }&lt;/math&gt;,and &lt;math&gt;\sigma \colon \{1, \cdots ,n\} \longrightarrow \{1, \cdots ,n\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \geq a_{\sigma (i + 1)},\ \forall i = 1, \cdots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma(i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th highest element in the set &lt;math&gt;\left\{ {a_1 , \cdots ,a_n } \right\}&lt;/math&gt;.

===Definition 2===
Using the alpha-cuts of fuzzy sets:{{R|kdeT1OWA}}

Given the n linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, then for each &lt;math&gt;\alpha \in [0,\;1]&lt;/math&gt;, an &lt;math&gt;\alpha &lt;/math&gt;-level type-1 OWA operator with &lt;math&gt;\alpha &lt;/math&gt;-level sets &lt;math&gt;\left\{ {W_\alpha ^i } \right\}_{i = 1}^n &lt;/math&gt; to aggregate the &lt;math&gt;\alpha &lt;/math&gt;-cuts of fuzzy sets &lt;math&gt;\left\{ {A^i} \right\}_{i =1}^n &lt;/math&gt; is:

: &lt;math&gt;
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}&lt;/math&gt;

where  &lt;math&gt;W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}&lt;/math&gt;, and &lt;math&gt;\sigma :\{\;1, \cdots ,n\;\} \to \{\;1, \cdots ,n\;\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \cdots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma (i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th largest
element in the set &lt;math&gt;\left\{ {a_1 , \cdots ,a_n } \right\}&lt;/math&gt;.

== Representation theorem of Type-1 OWA operators==

Given the ''n'' linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, and the fuzzy sets &lt;math&gt;A^1, \cdots ,A^n&lt;/math&gt;, then we have that{{R|kdeT1OWA}}
:&lt;math&gt;Y=G&lt;/math&gt;

where &lt;math&gt;Y&lt;/math&gt; is the aggregation result obtained by Definition 1, and &lt;math&gt;G&lt;/math&gt; is the result obtained by in Definition 2.

==Programming problems for Type-1 OWA operators==

According to the Representation Theorem of Type-1 OWA Operators, a general type-1 OWA operator can be decomposed into a series of &lt;math&gt;\alpha&lt;/math&gt;-level type-1 OWA operators. In practice, this series of  &lt;math&gt;\alpha&lt;/math&gt;-level type-1 OWA operators is used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals &lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)&lt;/math&gt;. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:

:&lt;math&gt;\mu _{G} (x) = \operatorname{ \bigvee} \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha &lt;/math&gt;

For the left end-points, we need to solve the following programming problem:
:&lt;math&gt; \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \operatorname {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } &lt;/math&gt;

while for the right end-points, we need to solve the following programming problem:
:&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \operatorname {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } &lt;/math&gt;

A fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.{{R|kdeT1OWA}}

==Alpha-level approach to Type-1 OWA operation==

Three-step process:{{R|kdeT1OWA}}
* Step 1&amp;mdash;To set up the &lt;math&gt;\alpha &lt;/math&gt;- level resolution in [0, 1].
* Step 2&amp;mdash;For each &lt;math&gt;\alpha \in [0,1]&lt;/math&gt;,
:*Step 2.1&amp;mdash;To calculate &lt;math&gt;\rho _{\alpha +} ^{i_0^\ast } &lt;/math&gt;
# Let &lt;math&gt;i_0 = 1&lt;/math&gt;;
# If &lt;math&gt;\rho _{\alpha +} ^{i_0 } \ge A_{\alpha + }^{\sigma (i_0 )} &lt;/math&gt;, stop, &lt;math&gt;\rho _{\alpha +} ^{i_0 } &lt;/math&gt; is the solution; otherwise go to Step 2.1-3.
# &lt;math&gt;i_0 \leftarrow i_0 + 1&lt;/math&gt;, go to Step 2.1-2.

:*Step 2.2 To calculate&lt;math&gt;\rho _{\alpha -} ^{i_0^\ast } &lt;/math&gt;
# Let &lt;math&gt;i_0 = 1&lt;/math&gt;;
# If &lt;math&gt;\rho _{\alpha -} ^{i_0 } \ge A_{\alpha - }^{\sigma (i_0 )} &lt;/math&gt;, stop, &lt;math&gt;\rho _{\alpha -} ^{i_0 } &lt;/math&gt; is the solution; otherwise go to Step 2.2-3.
#&lt;math&gt;i_0 \leftarrow i_0 + 1&lt;/math&gt;, go to step Step 2.2-2.

* Step 3&amp;mdash;To construct the aggregation resulting fuzzy set &lt;math&gt;G&lt;/math&gt; based on all the available intervals &lt;math&gt;\left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]&lt;/math&gt;:

:&lt;math&gt;\mu _{G} (x) = \operatorname \bigvee \limits_{\alpha :x \in \left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]} \alpha &lt;/math&gt;

==Special cases==
* Any OWA operators, like maximum, minimum, mean operators;&lt;ref name="yagerOWA" /&gt;
* Join operators of (type-1) fuzzy sets,{{R|MT}} i.e., fuzzy maximum operators;
* Meet operators of (type-1) fuzzy sets,{{R|MT|zadehJ}} i.e., fuzzy minimum operators;
* Join-like operators of (type-1) fuzzy sets;{{R|kdeT1OWA|bookT1OWA}}
* Meet-like operators of (type-1) fuzzy sets.{{R|kdeT1OWA|bookT1OWA}}

==Generalizations==
Type-2 OWA operators{{R|Zhou}} have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.

==References==
{{reflist|30em|refs=

&lt;ref name="yagerOWA"&gt;{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183&#8211;190|doi=10.1109/21.87068}}&lt;/ref&gt;

&lt;ref name=Yager&gt;{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}&lt;/ref&gt;

&lt;ref name=YagerBeliakov&gt;{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}&lt;/ref&gt;

&lt;ref name=Zadeh&gt;{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338&#8211;353|doi=10.1016/S0019-9958(65)90241-X}}&lt;/ref&gt;

&lt;ref name="kdeT1OWA"&gt;{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455&#8211;1468|doi=10.1109/TKDE.2010.191}}&lt;/ref&gt;

&lt;ref name="fssT1OWA"&gt;{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281&#8211;3296|doi=10.1016/j.fss.2008.06.018}}&lt;/ref&gt;

&lt;ref name="MT"&gt;{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312&#8211;40|doi=10.1016/s0019-9958(76)80011-3}}&lt;/ref&gt;&lt;ref name="zadehJ"&gt;{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199&#8211;249|doi=10.1016/0020-0255(75)90036-5}}&lt;/ref&gt;

&lt;ref name="bookT1OWA"&gt;{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91&#8211;109|doi=10.1007/978-3-642-17910-5_5}}&lt;/ref&gt;

&lt;ref name=Zhou&gt;{{cite journal|last=Zhou|first=S.M. |author2=R. I. John |author3=F. Chiclana |author4=J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540&#8211;558|doi=10.1002/int.20420}}&lt;/ref&gt;
}}

[[Category:Artificial intelligence]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
[[Category:Logic in computer science]]</text>
      <sha1>6cbnswid9iqo62j67052btmyserl2dy</sha1>
    </revision>
  </page>
  <page>
    <title>Category:String similarity measures</title>
    <ns>14</ns>
    <id>9833053</id>
    <revision>
      <id>666717206</id>
      <parentid>544714325</parentid>
      <timestamp>2015-06-13T04:29:13Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="220" xml:space="preserve">{{Cat main|String metrics}}

[[Category:Algorithms on strings|Similarity]]
[[Category:Information retrieval techniques]]
[[Category:Metric geometry]]
[[Category:Information theory]]
[[Category:String (computer science)]]</text>
      <sha1>b0zfcvc850esdc5ft6zvz838xqljlvr</sha1>
    </revision>
  </page>
  <page>
    <title>Binary Independence Model</title>
    <ns>0</ns>
    <id>25957127</id>
    <revision>
      <id>689664883</id>
      <parentid>679093728</parentid>
      <timestamp>2015-11-08T17:53:11Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>/* Further reading */Update deprecated cite parameter (coauthors) and genfixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5603" xml:space="preserve">{{context|date=June 2012}}
The '''Binary Independence Model''' (BIM)&lt;ref name="cyu76" /&gt;&lt;ref name="jones77"/&gt; is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.

==Definitions==
The Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.
The representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x&lt;sub&gt;1&lt;/sub&gt;, ..., x&lt;sub&gt;m&lt;/sub&gt;)'' where ''x&lt;sub&gt;t&lt;/sub&gt;=1'' if term ''t'' is present in the document ''d'' and ''x&lt;sub&gt;t&lt;/sub&gt;=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.
"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.

The probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:

&lt;math&gt;P(R|x,q) = \frac{P(x|R,q)*P(R|q)}{P(x|q)}&lt;/math&gt;

where ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.
The exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.

''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.
Since a document is either relevant or nonrelevant to a query we have that:

&lt;math&gt;P(R=1|x,q) + P(R=0|x,q) = 1&lt;/math&gt;

=== Query Terms Weighting ===
Given a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the
terms in the query such that the retrieval effectiveness will be high. Let &lt;math&gt;p_i&lt;/math&gt; and &lt;math&gt;q_i&lt;/math&gt; be the probability that a relevant document and an irrelevant document has the &lt;math&gt;i^{th}&lt;/math&gt; term respectively. Yu and [[Gerard Salton|Salton]],&lt;ref name="cyu76" /&gt; who first introduce BIM, propose that the weight of the &lt;math&gt;i^{th}&lt;/math&gt; term is an increasing function of &lt;math&gt;Y_i =  \frac{p_i *(1-q_i)}{(1-p_i)*q_i}&lt;/math&gt;. Thus, if &lt;math&gt;Y_i&lt;/math&gt; is higher than &lt;math&gt;Y_j&lt;/math&gt;, the weight
of term &lt;math&gt;i&lt;/math&gt; will be higher than that of term &lt;math&gt;j&lt;/math&gt;. Yu and Salton&lt;ref name="cyu76" /&gt; showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Sp&#228;rck Jones|Sp&#228;rck Jones]]&lt;ref name="jones77"/&gt; later showed that if the &lt;math&gt;i^{th}&lt;/math&gt; term is assigned the weight of &lt;math&gt;log Y_i&lt;/math&gt;, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.

The Binary Independence Model was introduced by Yu and Salton.&lt;ref name="cyu76" /&gt; The name Binary Independence Model was coined by Robertson and Sp&#228;rck Jones.&lt;ref name="jones77"/&gt;

== See also ==

* [[Bag of words model]]

==Further reading==
* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | author2=Prabhakar Raghavan|author3=Hinrich Sch&#252;tze | publisher=Cambridge University Press | year=2008}}
* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&amp;uuml;ttcher | author2=Charles L. A. Clarke |author3= Gordon V. Cormack | publisher=MIT Press | year=2010}}

==References==
{{Reflist|refs=
&lt;ref name="cyu76"&gt;{{Cite journal | doi = 10.1145/321921.321930| title = Precision Weighting &#8211; An Effective Automatic Indexing Method| journal = Journal of the ACM| volume = 23| pages = 76| year = 1976| last1 = Yu | first1 = C. T.| last2 = Salton | first2 = G. | authorlink2 = Gerard Salton}}&lt;/ref&gt;
&lt;ref name="jones77"&gt;{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Sp&#228;rck Jones | first2 = K. | authorlink2 = Karen Sp&#228;rck Jones}}&lt;/ref&gt; 
}}

[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]</text>
      <sha1>tqoxpd505n5o03bamz7p9l1r0mgp0hf</sha1>
    </revision>
  </page>
  <page>
    <title>Contextual searching</title>
    <ns>0</ns>
    <id>44571310</id>
    <revision>
      <id>760143005</id>
      <parentid>749432317</parentid>
      <timestamp>2017-01-15T06:16:44Z</timestamp>
      <contributor>
        <ip>74.177.99.126</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8655" xml:space="preserve">{{Orphan|date=May 2015}}

'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.&lt;ref&gt;{{cite journal | first = Susan E. | last = Feldman | title = The Answer Machine | journal = Synthesis Lectures on Information Concepts, Retrieval, and Services | doi = 10.2200/S00442ED1V01Y201208ICR023 }}&lt;/ref&gt; Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.&lt;ref&gt;{{cite journal | last1 = Pitokow | first1 = James | first2 = Hinrich | last2 = Sch&#252;tze | first3 = Todd | last3 = Cass | first4 = Rob | last4 = Cooley | first5 = Don | last5 = Turnbull | first6 = Andy | last6 = Edmonds | first7 = Eytan | last7 = Adar | first8 = Thomas | last8 = Breuel | date = 2002 | title = Personalized search | url = http://www.cond.org/p50-pitkow.pdf | journal = Communications of the ACM (CACM) | volume = 45 | issue = 9 | pages = 50&#8211;55 }}&lt;/ref&gt;

== Basic Contextual Search ==
The basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are not rated higher. Users have limited control over the context of their query based on the words they use to search with.&lt;ref&gt;Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.&lt;/ref&gt;  For example, users looking for the menu portion of a website can add &#8220;menu&#8221; to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.

== Explicitly Supplied Context ==
Certain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.&lt;ref&gt;Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.&lt;/ref&gt; For example, a user looking for research papers can specify documents with &#8220;references&#8221; or &#8220;abstracts&#8221; to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.&lt;ref&gt;Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]&lt;/ref&gt;

Explicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.&lt;ref&gt;[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results&lt;/ref&gt; Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.&lt;ref&gt;[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators&lt;/ref&gt; Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.&lt;ref&gt;[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks&lt;/ref&gt;

== Automatically Inferred Context ==
There are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.&lt;ref&gt;[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM&lt;/ref&gt; Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.

Major search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example, if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.&lt;ref&gt;{{cite journal | first1 = Eric J. | last1 = Glover | first2 = Steve | last2 = Lawrence | first3 = Michael D. | last3 = Gordon | first4 = William P. | last4 = Birmingham | first5 = C. Lee | last5 = Giles | title = Web Search - Your Way | publisher = NEC Research Institution | citeseerx = 10.1.1.41.7499 }}&lt;/ref&gt; Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&amp;go=Submit&amp;qs=n&amp;form=GEOMA1&amp;pq=pizza&amp;sc=8-1&amp;sp=-1&amp;sk=&amp;cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.

=== Contextual Mobile Search ===
The drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".&lt;ref&gt;[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop&lt;/ref&gt; Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.&lt;ref&gt;[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices&lt;/ref&gt;

== References ==
{{reflist}}

{{Internet search}}

{{DEFAULTSORT:Contextual Searching}}
[[Category:Internet search engines]]
[[Category:Semantic Web]]
[[Category:Information retrieval techniques]]
[[Category:Internet terminology]]</text>
      <sha1>2peqn69i802rwv7kafcb0uwnwhnrtb3</sha1>
    </revision>
  </page>
  <page>
    <title>Natural language user interface</title>
    <ns>0</ns>
    <id>18863997</id>
    <revision>
      <id>758322270</id>
      <parentid>758322160</parentid>
      <timestamp>2017-01-04T19:24:00Z</timestamp>
      <contributor>
        <username>Michal.on</username>
        <id>17441124</id>
      </contributor>
      <minor />
      <comment>/* Others */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17215" xml:space="preserve">'''Natural language user interfaces''' ('''LUI''' or '''NLUI''') are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.

In [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.&lt;ref&gt;Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.&lt;/ref&gt;
Natural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general natural language interface is one of the active goals of the [[Semantic Web]].

Text interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a "shallow" natural language user interface.

==Overview==
A natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.{{Citation needed|date=October 2015}}

==History==

Prototype Nl interfaces had already appeared in the late sixties and early seventies.&lt;ref name="edin"&gt;Natural Language Interfaces to Databases &#8211; An Introduction,
I. Androutsopoulos,
G.D. Ritchie,
P. Thanisch,
Department of Artificial Intelligence, University of Edinburgh&lt;/ref&gt;

*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"
*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].
*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.&lt;ref&gt;[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]&lt;/ref&gt;
*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.&lt;ref&gt;[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]&lt;/ref&gt;
* ''Janus'' is also one of the few systems to support temporal questions.
* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).
* BBN&#8217;s ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.
* [[IBM]] ''Languageaccess''
* [[Q&amp;A (software)|Q&amp;A]] from [[Symantec]].
* ''Datatalker'' from Natural Language Inc.
* ''Loqui''  from [[Bim]].
* ''English Wizard'' from [[Linguistic Technology Corporation]].
* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001&lt;ref&gt;{{cite book | last = Galitsky
 | first = Boris
 | title = Natural Language Question Answering: technique of semantic headers
 | publisher = Advance Knowledge International
 | date = 2003
 | location = Adelaide, Australia
 | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799
 | isbn = 0868039799
  }}&lt;/ref&gt;

==Challenges==
Natural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.

A [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases &#8211; An Introduction', describes some challenges:&lt;ref name="edin"/&gt;
;Modifier attachment
:The request "List all employees in the company with a driving licence" is ambiguous unless you know that companies can't have driving licences.
;Conjunction and disjunction
:"List all applicants who live in California and Arizona" is ambiguous unless you know that a person can't live in two places at once.
;[[Anaphora resolution]]
:resolve what a user means by 'he', 'she' or 'it', in a self-referential query.

Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.

Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.

==Uses and applications==

The natural language interface gives rise to technology used for many different applications.

Some of the main uses are:

* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.
* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.
* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.
* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.
* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.
* ''Embedded applications'', some new cellular phones include C&amp;C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].

Below are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.

===Ubiquity===
{{main|Ubiquity (Firefox)}}
Ubiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.

===Wolfram Alpha===
{{main|Wolfram Alpha}}
Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.&lt;ref&gt;{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}&lt;/ref&gt; It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.&lt;ref name="launch date"&gt;{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}&lt;/ref&gt;

===Siri===
{{main|Siri (software)}}
Siri is an [[intelligent personal assistant]] application integrated with operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations.

Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.&lt;ref&gt;[https://www.apple.com/iphone/features/siri.html Siri webpage]&lt;/ref&gt;

===Others===
* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.
* [[Braina]]&lt;ref&gt;[http://www.brainasoft.com/braina/ Braina]&lt;/ref&gt; - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.
* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.
* [http://minock.github.io/c-phrase/ C-Phrase] - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English.
* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.
* [https://friendlydata.io/ FriendlyData] is a natural language interface for relational databases.
[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]
* [http://yagadi.com/ Enguage] - this is an open source text understanding interface for web/mobile devices, using publicly available speech-to-text and text-to-speech facilities. This is directed at controlling apps, rather than as a front-end database query or web search. The interpretation of utterances is programmed, and programmable, in natural language utterances; thus, it is (or at least asserts that language is) an [[autopoiesis|autopoietic]] system.&lt;ref&gt;http://www.academia.edu/10177437/An_Autopoietic_Repertoire&lt;/ref&gt; It can achieve a deep understanding of text.&lt;ref&gt;http://cit.srce.unizg.hr/index.php/CIT/article/view/2278/1658 if we are holding hands whose hand am i holding&lt;/ref&gt; A reference app is available on [https://play.google.com/store/apps/details?id=com.yagadi.iNeed Google Play]
* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).&lt;ref&gt;Ubuntu 10.04 Add/Remove Applications description for GNOME Do&lt;/ref&gt;
* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.
* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.
* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.
* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in S&#227;o Paulo when it is 6pm on the 2nd of June in Detroit'.
* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.
* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.
* [[Powerset (company)|Powerset]] &#8212; On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.&lt;ref&gt;{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}&lt;/ref&gt; On July 1, 2008, it was purchased by [[Microsoft]].&lt;ref&gt;{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=https://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}&lt;/ref&gt;
* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company&#8217;s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011
* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.
* [https://www.statmuse.com/ StatMuse] - Natural language analytics platform, currently in private beta with NBA data. Ask natural questions and get rich visualizations and raw data.
* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).
* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.&lt;ref&gt;Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] ''Geek.com''. 31 July 2009.&lt;/ref&gt;

==See also==
*[[Attempto Controlled English]]
*[[Natural user interface]]
*[[Natural language programming]]
**[[xTalk]], a family of English-like programming languages
*[[Chatterbot]], a computer program that simulates human conversations
*[[Noisy text]]
*[[Question answering]]
*[[Selection-based search]]
*[[Semantic search]]
*[[Semantic query]]
*[[Semantic Web]]

==References==
{{reflist}}

{{Internet search}}
{{Computable knowledge}}

{{DEFAULTSORT:Natural language user interface}}
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]</text>
      <sha1>3gubgk0g4bzbev2zi5cim11f8g2zr7i</sha1>
    </revision>
  </page>
  <page>
    <title>Proximity search (text)</title>
    <ns>0</ns>
    <id>1934622</id>
    <revision>
      <id>755821189</id>
      <parentid>694535384</parentid>
      <timestamp>2016-12-20T11:23:12Z</timestamp>
      <contributor>
        <username>Feminist</username>
        <id>25530780</id>
      </contributor>
      <minor />
      <comment>/* Usage in commercial search engines */[[Talk:Bing (search engine)#Requested move 13 December 2016]], replaced: [[Bing]] &#8594; [[Bing (search engine)|Bing]] (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7226" xml:space="preserve">In [[natural language processing|text processing]], a '''proximity search''' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.

For example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.

== Rationale ==
The basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.

Commercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].

== Boolean syntax and operators ==
Note that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".

== Usage in commercial search engines ==
In regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).

[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]], [[Altavista]], and [[Bing (search engine)|Bing]]:
* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.&lt;ref&gt;[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009&lt;/ref&gt;
* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is &lt;tt&gt;(keyword1 NEAR/n keyword2)&lt;/tt&gt; where n is the number of words.&lt;ref&gt;[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009&lt;/ref&gt;
* [[Yandex]] uses the syntax &lt;tt&gt;keyword1 /n keyword2&lt;/tt&gt; to search for two keywords separated by at most &lt;math&gt;n - 1&lt;/math&gt; words, and supports a few other variations of this syntax.&lt;ref&gt;[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)&lt;/ref&gt;
* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.&lt;ref&gt;[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)&lt;/ref&gt;&lt;ref&gt;[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)&lt;/ref&gt; The syntax is &lt;tt&gt;keyword1 NEAR keyword2&lt;/tt&gt;.
* [[Google Search]] supports AROUND(#).&lt;ref&gt;[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search's Little Known AROUND Operator"]&lt;/ref&gt;&lt;ref&gt;[http://www.netforlawyers.com/content/google-offers-proximity-search-around-connector-0015/ "Google Offers Proximity Search"] (8 Feb 2011)&lt;/ref&gt;
* [[Bing (search engine)|Bing]] supports NEAR.&lt;ref&gt;[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ "How to Use Bing&#8217;s Advanced Search Operators"]&lt;/ref&gt; The syntax is &lt;tt&gt;keyword1 near:n keyword2&lt;/tt&gt; where n=the number of maximum separating words.

Ordered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,&lt;ref&gt;[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]&lt;/ref&gt; and an in Yahoo! Search this matches exactly one word.&lt;ref&gt;[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]&lt;/ref&gt;  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)

To emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".

== See also ==
* [[Compound term processing]]
* [[Edit distance]]
* [[Information retrieval]]
* [[Search engine]]
* [[Search engine indexing]] - how texts are indexed to support proximity search
* [[Semantic proximity]]

== Notes ==
{{Reflist}}

[[Category:Information retrieval techniques]]
[[Category:Internet search algorithms]]</text>
      <sha1>i3qh3jimn39edq902cn23uoo1g1ycd3</sha1>
    </revision>
  </page>
  <page>
    <title>Voice search</title>
    <ns>0</ns>
    <id>13667706</id>
    <revision>
      <id>749657486</id>
      <parentid>681513453</parentid>
      <timestamp>2016-11-15T15:11:58Z</timestamp>
      <contributor>
        <username>HB2016</username>
        <id>29664607</id>
      </contributor>
      <minor />
      <comment>Added Siri</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1501" xml:space="preserve">'''Voice search''', also called voice-enabled search, allows the user to use a voice command to search the Internet, or a portable device.  Currently, voice search is commonly used in (in a narrow sense) "directory assistance", or local search. Examples include [[Google 411]], [[Tellme]] directory assistance and [[Yellowpages.com]]'s 1-800-YellowPages. 

In a broader definition, voice search include open-domain keyword query on any information on the Internet, for example in [[Google Voice Search]], [[Cortana (software)|Cortana]], [[Siri]] and [[Amazon Echo]]. Given that voice-based systems are interactive, such systems are also called open-domain [[question answering]] systems. 

Voice search is often interactive, involving several rounds of interaction that allows a system to ask for clarification. Voice search is a type of [[Dialog systems|dialog system]].

== References ==
{{No footnotes|date=April 2009}}
*Ye-Yi Wang, Dong Yu, Yun-Cheng Ju, Alex Acero, An Introduction to Voice Search, IEEE Signal Processing Magazine (Special Issue on Spoken Language Technology), Institute of Electrical and Electronics Engineers, Inc., May 2008
*J. Sherwani, Dong Yu, Tim Paek, Mary Czerwinski, Yun-Cheng Ju, and Alex Acero. 'VoicePedia: Towards Speech-Based Access to Unstructured Information', Proceedings of the 8th Annual Conference of the International Communication Association (Interspeech 2007). Antwerp, Belgium, August, 2007
{{Internet search}}

[[Category:Information retrieval genres]]</text>
      <sha1>pzj9uywjeuw6a3wzu2w0m56dqwxucfs</sha1>
    </revision>
  </page>
  <page>
    <title>Multimodal search</title>
    <ns>0</ns>
    <id>34005384</id>
    <revision>
      <id>755822143</id>
      <parentid>715964389</parentid>
      <timestamp>2016-12-20T11:32:09Z</timestamp>
      <contributor>
        <username>Feminist</username>
        <id>25530780</id>
      </contributor>
      <minor />
      <comment>[[Talk:Bing (search engine)#Requested move 13 December 2016]], replaced: [[Bing]] &#8594; [[Bing (search engine)|Bing]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6407" xml:space="preserve">'''Multimodal search''' is a type of [[search engines|search]] that uses different methods to get relevant results. They can use any kind of search, [[keyword search|search by keyword]], [[concept search|search by concept]], [[Query by Example|search by example]],etc.

== Introduction ==

A multimodal search engine is designed to imitate the flexibility and agility of how the [[mind|human mind]] works to create, process and refuse irrelevant ideas. So, the more elements you have in the input of the search engine to can compare, the more [[Arithmetic precision|accurate]] the results can be.
Multimodal search engines use different inputs of different nature and methods of search at the same time with the possibility of combining the results by merging all of the input elements of the search. There are also engines that can use a feedback of the results with the evaluation of the user to perform a more appropriate and relevant search.

[[File:Schema of a simple search.jpg|thumb|Schema of a Simple Search]]

Nowadays, mobile devices have been developed to a point that they can perform infinite functions from any place at any time, thanks to the [[internet]] and [[GPS]] connections. Touch screens, motion sensors and voice recognition are now featured on mobile devices called [[smartphone]]s. All the features and functions make possible to can execute multimodal searches from any place of the world at any time.

=== Search elements ===

The use of text is an option, as well as [[multimedia search]]ing, [[image]]s, [[video]]s, [[Content (media)|audio]], [[voice]], [[document]]s. Even the location of the user can help the search engine to perform a more effective search, adaptable to every situation.
Nowadays, different ways to [[Human&#8211;computer interaction|interact]] with a search engine are being discovered, in terms of input elements of the search and in the variety of results obtained.

=== Personal context ===

Many queries from mobiles are [[location-based service|location-based]] (LBS), that use the location of the user to interact with the applications. If available, the browser uses the device GPS, or computes an approximate location based on cell tower triangulation, with the permission of the user, who must be agree to share his/her location with the application in the download.
Therefore, multimodal searches use not only audiovisual content that the user provides directly, but also the context where the user is, like his/her location, language, time at the moment, web site or document where the user is surfing, or other elements that can help to improve of a search in every situation.
[[File:Contextual query.jpg|Example of Contextual Query]]

== Classification of the results ==

The multimodal search engine works in parallel, whilst at the same time, performs a search of more to less relevance of every element introduced directly or indirectly (personal context). Afterwards, it provides a combination of all the results, merging every element with its associated weight for every descriptor.

The engine analyzes every element and tags them, so a comparison of the tags can be made with existent indexed information in databases. A classification of the results proceeds, to show them from more to less relevance.

[[File:Framework of Multimodal Search.jpg|thumb|Framework of a Multimodal Search]]

It&#8217;s necessary to define the importance of every input element. There are search engines that do this automatically, however there are also engines where the user can do it manually, giving more or less weight to every element of the search.
It&#8217;s also important that the user provides the appropriate and essential information for the search; too much information can confuse the system and provide unsatisfactory results.
With multimodal searches users can get better results than with a simple search, but multimodal searches must process more input information. It can also spend more time to process it and require more memory space.

An efficient search engine interprets the query of the users, realizes his/her intention and applies a strategy to use an appropriate search, i.e. the engine adapts to every input query and also to the combination of the elements and methods.

== Applications ==

Nowadays, existing multimodal search engines are not very complex, and some of them are in an experimental phase. Some of the more simple engines are [[Google Images]] [http://images.google.es/] or [[Bing (search engine)|Bing]] [http://www.bing.com], web interfaces that use text and images as inputs to find images in the output.

MMRetrieval [http://www.aviarampatzis.com/publications/p117-zagoris.pdf] is a multimodal experimental search engine that uses multilingual and multimedia information through a web interface. The engine searches the different inputs in parallel and merges all the results by different chosen methods. The engine also provides different multistage retrieval, as well as a single text index baseline to be able to compare all the different phases of search.

There are a lot of applications for mobile devices, using the context of the user, like based-location services, and using also text, images, audios or videos that the user provides at the moment or with saved files, or even interacting with the voice.

== References ==

* Query-Adaptive Fusion for Multimodal Search,Lyndon Kennedy, Student Member IEEE, Shih-Fu Chang, Fellow IEEE, and Apostol Natsev [http://www.ee.columbia.edu/~lyndon/pubs/pieee2008-queryadaptive.pdf]
* Context-aware Querying for Multimodal Search Engines, Jonas Etzold, Arnaud Brousseau, Paul Grimm and Thomas Steiner [http://www.lsi.upc.edu/~tsteiner/papers/2012/context-aware-querying-mmm2012.pdf]
* Apply Multimodal Search and Relevance Feedback In a Digital Video Library, Thesis of Yu Zhong [http://www.informedia.cs.cmu.edu/documents/zhong_thesis_may00.pdf]
* Aplicaci&#243; rica d&#8217;internet per a la consulta amb text i imatge al repositori de v&#237;deos de la Corporaci&#243; Catalana de Mitjans Audiovisuals, Ramon Salla, Universitat Polit&#232;cnica de Catalunya [http://upcommons.upc.edu/pfc/bitstream/2099.1/8766/1/PFC.pdf]

== External links ==
* MMRetrieval [http://www.mmretrieval.net]
* Google Images [http://images.google.es/]
* Bing [http://www.bing.com]

&lt;!--- Categories ---&gt;
[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
[[Category:Multimedia]]</text>
      <sha1>8zsv59pmvhjv87436fqffx3cj0thyxx</sha1>
    </revision>
  </page>
  <page>
    <title>Visual search engine</title>
    <ns>0</ns>
    <id>17813833</id>
    <revision>
      <id>754255852</id>
      <parentid>747891104</parentid>
      <timestamp>2016-12-11T18:09:51Z</timestamp>
      <contributor>
        <ip>18.111.117.62</ip>
      </contributor>
      <comment>/* Applications */ Someone is defacing wikipedia by inserting blatant advertising</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9028" xml:space="preserve">{{multiple issues|
{{original research|date=February 2012}}
{{refimprove|date=February 2012}}
}}

A '''Visual Search Engine''' is a [[search engine (computing)|search engine]] designed to search for information on the [[World Wide Web]] through the input of an image or a search engine with a visual display of the search results. Information may consist of [[web page]]s, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query). Examples are buildings in a foreign city. These search engines often use techniques for [[CBIR|Content Based Image Retrieval]].

A visual search engine searches images, patterns based on an algorithm which it could recognize and gives relative information based on the selective or apply pattern match technique.

== Classification ==
Depending on the nature of the search engine there are two main groups, those which aim to find visual information  and those with a visual display of results.

== Visual information searchers ==
[[File:Imatge cercadors 1.jpg|thumb|Screenshot of results shown by the image searcher through example GOS]]

=== Image search ===
An image search is a search engine that is designed to find an image. The search can be based on keywords, a picture, or a web [[Hyperlink|link]] to a picture. The results depend on the search criterion, such as [[metadata]], distribution of color, shape, etc., and the search technique which the browser uses.
[[File:Imatge wiki 2.png|thumb|Diagram of a search realized through example based on detectable regions from an image]]

==== Image search techniques ====
Two techniques currently used in image search:

'''Search by metadata:''' Image search is based on comparison of metadata associated with the image as keywords, text, etc.. and it is obtained a set of images sorted by relevance. The metadata associated with each image can reference the title of the image, format, color, etc.. and can be generated manually or automatically. This metadata generation process is called audiovisual indexing.

'''Search by example:''' In this technique, also called [[content-based image retrieval]], the search results are obtained through the comparison between images using computer vision techniques. During the search it is examined the content of the image such as color, shape, texture or any visual information that can be extracted from the image. This system requires a higher [[Computational complexity theory|computational complexity]], but is more efficient and reliable than search by metadata.

There are image searchers that combine both search techniques, as the first search is done by entering a text, and then, from the images obtained can refine the search using as search parameters the images which appear as a result.

=== Video search ===
A video search is a [[search engine (computing)|search engine]] designed to search video on the net. Some video searchers process the search directly in the Internet, while others shelter the videos from which the search is done. Some searchers also enable to use as search parameters the [[File format|format]] or the length of the video. Usually the results come with a miniature capture of the video.

==== Video search techniques ====
Currently, almost all video searchers are based on keywords (search by metadata) to perform searches. These keywords can be found in the title of the video, text accompanying the video or can be defined by the author. An example of this type of search is [[YouTube]].

Some searchers generate keywords manually, while others use [[algorithms]] to analyze the audiovisual content of the video and to generate labels. The combination of these two processes improves the reliability of the search.

=== 3D Models searcher ===
A searcher of 3D models aims to find the file of a 3D modeling object from a [[database]] or network. At first glance the implementation of this type of searchers may seem unnecessary, but due to the continuous documentary inflation of the Internet, every day it becomes more necessary indexing information.

==== 3D Models search techniques ====
These have been used with traditional text-based searchers (keywords / tags), where the authors of the indexed material, or Internet users, have contributed these tags or keywords.  Because it is not always effective, it has recently been investigated in the implementation of search engines that combine the search using text with the search compared to 2D drawings, 3D drawings and 3D models.

[[Princeton University]] has developed a search engine that combines all these parameters to perform the search, thus increasing the efficiency of search.&lt;ref name= funk&gt;{{cite journal | last= Funkhouser | first= Thomas  | first2 = Patrick | last2 = Min | first3 = Michael | last3 = Kazhdan | first4 = Joyce | last4 = Chen | first5 = Alex | last5 = Halderman | first6 = David | last6 = Dobkin | first7 = David | last7 = Jacobs  | year= 2002 | title=  A Search Engine for 3D Models | journal= ACM Transactions on Graphics |url=https://www.cs.princeton.edu/~funk/tog03.pdf | volume= 22 | issue =1  | pages= 83-105  |doi = 10.1145/588272.588279 }}&lt;/ref&gt;

Imaginestics LLC created the world's first online shape search engine in the fall of 2005.&lt;ref&gt;{{cite web|url=http://www.purdue.edu/uns/html3month/2006/060824.Imaginestics.grant.html|title=Purdue Research Park's Imaginestics wins grant for research on search engines|work=purdue.edu}}&lt;/ref&gt; They currently use VizSeek search engine technology in the industrial and manufacturing settings to help discover parts using shape as the matching criteria.

=== Mobile visual search ===
A mobile image searcher is a type of [[search engine]] designed exclusively for mobile phones, through which you can find any information on [[Internet]], through an image made with the own [[mobile phone]] or using certain words ([[Keyword (computer programming)|keywords]]).

==== Introduction ====
Mobile phones have evolved into powerful image and video processing devices equipped with high-resolution cameras, color displays, and hardware-accelerated graphics. They are also increasingly equipped with a global positioning system and connected to broadband wireless networks. All this enables a new class of applications that use the camera phone to initiate search queries about objects in visual proximity to the user (Figure 1). Such applications can be used, e.g., for identifying products, comparison shopping, finding information about movies, compact disks (CDs), real estate, print media, or artworks.

==== Process ====
Typically, this type of search engine uses techniques of [[query by example]] or [[Content-based image retrieval|Image query by example]], which use the content, shape, texture and color of the image to compare them in a [[database]] and then deliver the approximate results from the query.

The process used in these searches in the [[mobile phone]]s is as follows:

First, the image is sent to the server application. Already on the server, the image will be analyzed by different analytical teams, as each one is specialized in different fields that make up an image. Then, each team will decide if the submitted image contains the fields of their speciality or not.

Once this whole procedure is done, a central computer will analyze the data and create a page of the results sorted with the efficiency of each team, to eventually be sent to the [[mobile phone]].

==== Applications ====
[[Google Goggles]] is the most popular application of image search engines{{citation required|date=November 2016}}, developed by [[Google labs|Google Labs]]. Available for [[Android (operating system)|Android]] only today. [[CamFind]] is a similar application available for both [[Android (operating system)|Android]] and [[iOS]].

JustVisual.com (formerly known as 'Superfish') and its LikeThat showcase apps are API for developers to create their own visual-search mobile app.

Other companies in the image recognition space are the [[reverse image search]]-engines [[TinEye]] and [[Google]]'s [[Google Images#Search by image|"search by image" feature of Google Images]].

== Visual display searchers ==
Another type of visual search is a search engine that shows results with a visual display image. This is an alternative to the traditional results of a sequence of links. Through some kind of image display, such as graphs, diagrams, previews of the websites, etc., it presents the results visually so that it is easier to find the desired material.
Such search engines like [http://www.kiddle.co/ Kiddle] and Manzia&lt;ref&gt;{{cite web|title=Manzia Search|website=http://www.manzia.com|accessdate=8 August 2014}}&lt;/ref&gt; present a new concept in the presentation of results, but the search techniques used are the same as in other search engines.

==References==
{{reflist}}

[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
[[Category:Multimedia]]</text>
      <sha1>n6gbyyuv0mm2lbrapnpuhiy2rrkz4dg</sha1>
    </revision>
  </page>
  <page>
    <title>Concept search</title>
    <ns>0</ns>
    <id>17785794</id>
    <revision>
      <id>751208074</id>
      <parentid>751208015</parentid>
      <timestamp>2016-11-24T01:58:28Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* See also */ linked already</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24726" xml:space="preserve">A '''concept search''' (or '''conceptual search''') is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a [[concept]] search query are relevant to the ideas contained in the text of the query.

__TOC__

==Development==
Concept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.

Polysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.&lt;ref&gt;Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.&lt;/ref&gt;

In addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.

A concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),&lt;ref&gt;R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.&lt;/ref&gt; and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.

==Approaches==
In general, [[information retrieval]] research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.&lt;ref&gt;Greengrass, E., Information Retrieval: A Survey, 2000.&lt;/ref&gt;

Efforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:

* Auxiliary structures
* Local [[co-occurrence]] statistics
* Transform techniques (particularly [[matrix decomposition]]s)

===Auxiliary structures===
A variety of techniques based on [[artificial intelligence]] (AI) and [[natural language processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.&lt;ref&gt;Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.&lt;/ref&gt; Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.&lt;ref&gt;Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.&lt;/ref&gt;  It was shown that concept search that is based on auxiliary structures, such as WordNet, can be efficiently implemented by reusing retrieval models and data structures of classical information retrieval.&lt;ref&gt;Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.&lt;/ref&gt;  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.

Handcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.&lt;ref name="Bradford, R. B. 2008"&gt;Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.&lt;/ref&gt;

===Local co-occurrence statistics===
Information retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example, &#177; 5 sentences or &#177; 50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.

This approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only &#188; of the information contained in text is local in nature.&lt;ref&gt;Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.&lt;/ref&gt;   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.&lt;ref name="Bradford, R. B. 2008"/&gt;

===Transform techniques===
Some of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:&lt;ref&gt;Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.&lt;/ref&gt;

* [[Independent component analysis]]
* Semi-discrete decomposition
* [[Non-negative matrix factorization]]
* [[Singular value decomposition]]

Matrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.&lt;ref name="Bradford, R. B. 2008"/&gt;

Independent component analysis is a technique that creates sparse representations in an automated fashion,&lt;ref&gt;Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010&lt;/ref&gt; and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.&lt;ref name="Bradford, R. B. 2008"/&gt;

Singular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[latent semantic indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.&lt;ref&gt;Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.&lt;/ref&gt;

==Uses==
* '''[[eDiscovery]]''' &#8211; Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.&lt;ref&gt;Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.
Disability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul &amp; Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. &amp; Tech. 10 (2007).&lt;/ref&gt;

* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' &#8211; Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.&lt;ref name="Laplanche, R. 2004"&gt;Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.&lt;/ref&gt;  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.&lt;ref name="Laplanche, R. 2004"/&gt;
* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' &#8211; Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.&lt;ref name="Lew, M. S. 2006"&gt;Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.&lt;/ref&gt;  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.&lt;ref&gt;Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.&lt;/ref&gt;  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.
* '''Multimedia and Publishing''' &#8211; Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.&lt;ref name="Lew, M. S. 2006"/&gt;
* '''Digital Libraries and Archives''' &#8211; Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.
* '''Genomic Information Retrieval (GIR)''' &#8211; Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.
* '''Human Resources Staffing and Recruiting''' &#8211; Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.

==Effective searching==
The effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:

* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.
* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''
* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.

As with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.&lt;ref&gt;[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Sp&#228;rck Jones|Sp&#228;rck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.&lt;/ref&gt; The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.&lt;ref&gt;Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&amp;nbsp;42&#8211;49&lt;/ref&gt;

==Relevance feedback==
[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.&lt;ref name="Manning, C. D. 2008"&gt;Manning, C. D., Raghavan P., Sch&#252;tze H., Introduction to Information Retrieval, Cambridge University Press, 2008.&lt;/ref&gt;   It is a way to involve users in the retrieval process in order to improve the final result set.&lt;ref name="Manning, C. D. 2008"/&gt; Users can refine their queries based on their initial results to improve the quality of their final results.

In general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.

Relevance feedback has been shown to be very effective at improving the relevance of results.&lt;ref name="Manning, C. D. 2008"/&gt;   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.&lt;ref name="Laplanche, R. 2004"/&gt;

[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.&lt;ref name="Callan, J. 2007"&gt;Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.&lt;/ref&gt;

==Guidelines for evaluating a concept search engine==
# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.
# Result items should be sorted and ranked by relevance.
# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.
# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.
# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.
# Combined queries using concepts, keywords, and metadata should be allowed.
# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.
# Query-ready indexes should be created relatively quickly.
# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.
# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.

==Conferences and forums==
Formalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.&lt;ref&gt;Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.&lt;/ref&gt;

In 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.

Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.&lt;ref name="Manning, C. D. 2008"/&gt;

Although the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.&lt;ref name="Callan, J. 2007"/&gt;   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven&#8217;t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.&lt;ref name="Callan, J. 2007"/&gt;

==See also==
{{div col|3}}
* [[Approximate string matching]]
* [[Compound term processing]]
* [[Concept mining]]
* [[Information extraction]]
* [[Latent semantic analysis]]
* [[Semantic network]]
* [[Semantic search]]
* [[Semantic Web]]
* [[Statistical semantics]]
* [[Text mining]]
{{div col end}}

==References==
{{Reflist|2}}

==External links==
* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]
* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]
* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]
* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]

[[Category:Information retrieval genres]]</text>
      <sha1>dk7wu21bdfwvjikhnvf1koqctfxtl83</sha1>
    </revision>
  </page>
  <page>
    <title>Stephen Robertson (computer scientist)</title>
    <ns>0</ns>
    <id>24019253</id>
    <revision>
      <id>741150362</id>
      <parentid>717032186</parentid>
      <timestamp>2016-09-25T18:34:15Z</timestamp>
      <contributor>
        <username>Timrollpickering</username>
        <id>32005</id>
      </contributor>
      <minor />
      <comment>/* External links */rename cat per [[Wikipedia:Categories for discussion/Log/2016 September 2]], replaced: Category:Alumni of City University London &#8594; Category:Alumni of City, University of London, Category:Academics of C using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4019" xml:space="preserve">{{More footnotes|date=September 2012}}
{{Infobox scientist
| name = Stephen Robertson
| image =
| image_size = 100px
| residence = United Kingdom
| nationality = British
| field = Computer science
| alma_mater = Cambridge, City University, University College London
| doctoral_advisor = B.C (Bertie) Brookes
| doctoral_students = Ayse G&#246;ker, Andrew MacFarlane, Xiangji (Jimmy) Huang, Olga Vechtomova, Murat Karamuftuoglu, Micheline Beaulieu, Efthimis Efthimiadis, Anna Ritchie, Jagadeesh Gorla
| known_for  = Work on information retrieval and inverse document frequency
| prizes = [[Gerard Salton Award]] (2000), [[Tony Kent Strix award]] (1998), [[ACM Fellow]] (2013)
| website = {{URL|http://staff.city.ac.uk/~sb317}}
}}

'''Stephen Robertson''' is a [[United Kingdom|British]] computer scientist. He is known for his work on [[information retrieval]]&lt;ref&gt;{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Sp&#228;rck Jones | first2 = K. | authorlink2 = Karen Sp&#228;rck Jones}}&lt;/ref&gt; and the [[Okapi BM25]] weighting model.&lt;ref&gt;{{Cite journal | doi = 10.1016/S0306-4573(00)00015-7| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 1| journal = Information Processing &amp; Management| volume = 36| issue = 6| pages = 779&#8211;808| year = 2000| last1 = Sp&#228;rck Jones | first1 = K. | authorlink1 = Karen Sp&#228;rck Jones| last2 = Walker | first2 = S.| last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | doi = 10.1016/S0306-4573(00)00016-9| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 2| journal = Information Processing &amp; Management| volume = 36| issue = 6| pages = 809&#8211;840| year = 2000| last1 = Sp&#228;rck Jones | first1 = K. | authorlink1 = Karen Sp&#228;rck Jones| last2 = Walker | first2 = S. | last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}&lt;/ref&gt;

After completing his undergraduate degree in mathematics at [[Cambridge university|Cambridge University]], he took an MS at [[City University, London|City University]], and then worked for [[ASLIB]]. He then studied for his PhD at [[University College London]] under the renowned statistician and scholar B. C. Brookes. He then returned to City University working there from 1978 until 1998 in the Department of [[Information Science]], continuing as a part-time professor and subsequently as professor emeritus. He is also a fellow of [[Girton College, Cambridge|Girton College]], Cambridge University.

From 1998 to 2013 he worked in the Cambridge laboratory of [[Microsoft Research]], where he led a group investigating core search processes such as term weighting, document scoring and ranking algorithms, combining evidence from different sources, and metrics and methods for the evaluation and optimisation of search. Much of his work has contributed to the [[Microsoft]] [[Web search engine|search engine]] [[Bing (search engine)|Bing]]. He participated a number of times in the [[Text Retrieval Conference|TREC conference]].

==References==
{{Reflist}}

== External links ==
* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}

{{DEFAULTSORT:Robertson, Stephen}}
[[Category:British computer scientists]]
[[Category:Alumni of University College London]]
[[Category:Fellows of Girton College, Cambridge]]
[[Category:Living people]]
[[Category:Alumni of City, University of London]]
[[Category:Academics of City, University of London]]
[[Category:Information retrieval researchers]]</text>
      <sha1>coqwj4mcnpl0qh4d62zf50s00r8ir3t</sha1>
    </revision>
  </page>
  <page>
    <title>Norbert Fuhr</title>
    <ns>0</ns>
    <id>37022703</id>
    <revision>
      <id>722307345</id>
      <parentid>668400217</parentid>
      <timestamp>2016-05-27T07:18:12Z</timestamp>
      <contributor>
        <username>KasparBot</username>
        <id>24420788</id>
      </contributor>
      <comment>migrating [[Wikipedia:Persondata|Persondata]] to Wikidata, [[toollabs:kasparbot/persondata/|please help]], see [[toollabs:kasparbot/persondata/challenge.php/article/Norbert Fuhr|challenges for this article]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1975" xml:space="preserve">'''Norbert Fuhr''' (born 1956) is a professor of computer science
and the leader of the Duisburg Information Engineering Group based at
the [[University of Duisburg-Essen]], Germany.

==Education==
His first  degree is in  technical computer science, which he got  from the Electrical
Engineering Department of  the [[Technical University of Darmstadt]] in 1980, and in 1986 he finished his PhD
(Dr.-Ing) in the Computer Science Department of the same university on "Probabilistic
Indexing and Retrieval".&lt;ref name="Fuhr1986"&gt;{{citation
 | author=Fuhr, Norbert
 | publisher=Fachinformationszentrum Karlsruhe
 | title=Probabilistisches Indexing und Retrieval
 | year=1986
}}&lt;/ref&gt;

==Profession==
He held a PostDoc position in Darmstadt until
1991, when he was appointed Associate Professor in the  Computer
Science Department  of  the [[Technical University of Dortmund]]. Since 2002, he is a full professor at the
[[University of Duisburg-Essen]].

==Honors and awards==
Fuhr's dissertation was awarded the  "Gerhard Pietsch Award" of the German Society
of Documentation in 1987. In 2012, he received the  
[[Gerard Salton Award]].&lt;ref name="Fuhr2012"&gt;{{citation
 | author=Fuhr, Norbert
 | journal=SIGIR '12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval
 | title=Salton award lecture: information retrieval as engineering science
 | pages=1&#8211;2
 | year=2012
 | doi=10.1145/2348283.2348285
}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://www.is.inf.uni-due.de/staff/fuhr.html Norbert Fuhr - University of Duisburg-Essen]

{{Authority control}}
{{DEFAULTSORT:Fuhr, Norbert}}
[[Category:German computer scientists]]
[[Category:1956 births]]
[[Category:Living people]]
[[Category:University of Duisburg-Essen faculty]]
[[Category:Technical University of Dortmund faculty]]
[[Category:Technische Universit&#228;t Darmstadt alumni]]
[[Category:Information retrieval researchers]]


{{Compu-bio-stub}}</text>
      <sha1>che7bny1vjcooclnbx8jn2isuw7k8qv</sha1>
    </revision>
  </page>
  <page>
    <title>KM programming language</title>
    <ns>0</ns>
    <id>525334</id>
    <revision>
      <id>743296284</id>
      <parentid>654949221</parentid>
      <timestamp>2016-10-09T00:57:26Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Undid revision 654949221 by [[Special:Contributions/182.66.2.206|182.66.2.206]] ([[User talk:182.66.2.206|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1100" xml:space="preserve">{{Infobox programming language
| name = KM
| paradigm = [[knowledge representation]]
| generation =
| year = 
| designer =
| developer = 
| latest_release_version = 
| latest_release_date = 
| turing-complete = 
| typing = 
| implementations = 
| dialects = 
| influenced_by = [[KRL (programming language)|KRL]]
| influenced = 
}}

'''KM''', the '''Knowledge Machine''', is a [[Knowledge frame|frame]]-based language used for [[knowledge representation]] work.

It has first-order logic semantics, and includes machinery for reasoning, including selection by description, unification, classification, and reasoning about actions. Its origins were the Theo language and [[KRL (programming language)|KRL]], and is implemented in [[Lisp (programming language)|Lisp]].

==External links==
* [http://www.cs.utexas.edu/users/mfkb/RKF/km.html KM: The Knowledge Machine]. 
* An Ontology editor for the KM language: [http://www.algo.be/ref-projects.htm#KMgen/ KMgen].

[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]
[[Category:Common Lisp software]]


{{compu-lang-stub}}</text>
      <sha1>seqv6s20syh9zrer9y7b9fvytkhxv3b</sha1>
    </revision>
  </page>
  <page>
    <title>Colon classification</title>
    <ns>0</ns>
    <id>6888</id>
    <revision>
      <id>749094918</id>
      <parentid>749094878</parentid>
      <timestamp>2016-11-12T10:38:48Z</timestamp>
      <contributor>
        <username>Jim1138</username>
        <id>7695475</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/14.139.243.34|14.139.243.34]] ([[User talk:14.139.243.34|talk]]): Editing tests ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5042" xml:space="preserve">'''Colon classification''' ('''CC''') is a system of [[library classification]] developed by [[S. R. Ranganathan]]. It was the first ever [[Faceted classification|faceted]] (or analytico-synthetic) [[Taxonomic classification|classification]]. The first edition was published in 1933. Since then six more editions have been published. It is especially used in [[library|libraries]] in [[India]].

Its name "colon classification" comes from the use of [[Colon (punctuation)|colons]] to separate facets in class numbers. However, many other classification schemes, some of which are completely unrelated, also use colons and other [[punctuation]] in various functions. They should not be confused with colon classification.

In CC, facets describe "personality" (the most specific subject), matter, energy, space, and time (PMEST).  These facets are generally associated with every item in a library, and so form a reasonably universal sorting system.&lt;ref&gt;GOPINATH (M A). Colon classification: Its theory and practice.
Library Herald
.
26, 1
-
2; 1987; 1
-
3.&lt;/ref&gt;

As an example, the subject "research in the cure of tuberculosis of lungs by x-ray conducted in India in 1950" would be categorized as:

:Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India'1950

This is summarized in a specific call number:

:L,45;421:6;253:f.44'N5

== Organization ==

The colon classification uses 42 main classes that are combined with other letters, numbers and marks in a manner resembling the [[Library of Congress Classification]] to sort a publication.

=== Facets ===

CC uses five primary categories, or facets, to further specify the sorting of a publication. Collectively, they are called ''PMEST'':

:&lt;nowiki&gt;,&lt;/nowiki&gt; Personality, the most specific or focal subject.
:&lt;nowiki&gt;;&lt;/nowiki&gt; Matter or property, the substance, properties or materials of the subject.
:&lt;nowiki&gt;:&lt;/nowiki&gt; Energy, including the processes, operations and activities.
:&lt;nowiki&gt;.&lt;/nowiki&gt; Space, which relates to the geographic location of the subject.
:&lt;nowiki&gt;'&lt;/nowiki&gt; Time, which refers to the dates or seasons of the subject.

=== Classes ===

The following are the main classes of CC, with some subclasses, the main method used to sort the subclass using the PMEST scheme and examples showing application of PMEST.

:z Generalia
:1 Universe of Knowledge
:2 [[Library Science]]
:3 Book science
:4 [[Journalism]]
:B [[Mathematics]]
::B2 [[Algebra]]
:C [[Physics]]
:D [[Engineering]]
:E [[Chemistry]]
:F [[Technology]]
:G [[Biology]]
:H [[Geology]]
::HX [[Mining]]
:I [[Botany]]
:J [[Agriculture]]
::J1 [[Horticulture]]
::J2 Feed
::J3 Food
::J4 Stimulant
::J5 Oil
::J6 Drug
::J7 Fabric
::J8 Dye
:K [[Zoology]] 
::KZ Animal Husbandry 
:L Medicine
::LZ3 [[Pharmacology]]
::LZ5 [[Pharmacopoeia]]
:M [[Useful arts]]
::M7 Textiles ''[material]:[work]''
:&#916; Spiritual experience and [[mysticism]] ''[religion],[entity]:[problem]''
:N [[Fine arts]]
::ND Sculpture
::NN Engraving
::NQ Painting
::NR Music
:O Literature
:P Linguistics
:Q [[Religion]]
:R [[Philosophy]]
:S [[Psychology]]
:T [[Education]]
:U [[Geography]]
:V [[History]]
:W [[Political science]]
:X [[Economics]]
:Y [[Sociology]]
:YZ [[Social Work]]
:Z [[Law]]

== Example ==

A common example of the colon classification is:

* "Research in the cure of the tuberculosis of lungs by x-ray conducted in India in 1950s":
* Main classification is Medicine
** (Medicine)
* Within Medicine, the Lungs are the main concern
** (Medicine,Lungs)
* The property of the Lungs is that they are afflicted with Tuberculosis
** (Medicine,Lungs;Tuberculosis)
* The Tuberculosis is being performed (:) on, that is the intent is to cure (Treatment)
** (Medicine,Lungs;Tuberculosis:Treatment)
* The matter that we are treating the Tuberculosis with are X-Rays
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray)
* And this discussion of treatment is regarding the Research phase
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research)
* This Research is performed within a geographical space (.) namely India
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India)
* During the time (') of 1950
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India'1950)
* And translating into the codes listed for each subject and facet the classification becomes
** L,45;421:6;253:f.44'N5

==See also==
*[[Bliss bibliographic classification]]
*[[Subject (documents)]]
*[[Universal Decimal Classification]]

== References ==
{{Reflist|2}}
* [http://www.essessreference.com/servlet/esGetBiblio?bno=000374 ''Colon Classification'' (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Ess Publications, Delhi, India
* Chan, Lois Mai. ''Cataloging and Classification: An Introduction''. 2nd ed. New York: McGraw-Hill, c1994. ISBN 0-07-010506-5.

==External links==
* [http://www.iskoi.org/doc/colon.htm More Detail about the Colon Classification at ISKO Italia]

{{Library classification systems}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>r8ncyrbwmp80w1hg7f5zd35d2cyi0o1</sha1>
    </revision>
  </page>
  <page>
    <title>Ontic</title>
    <ns>0</ns>
    <id>2788896</id>
    <revision>
      <id>704554614</id>
      <parentid>692489657</parentid>
      <timestamp>2016-02-12T05:05:29Z</timestamp>
      <contributor>
        <username>Philebritite</username>
        <id>9149149</id>
      </contributor>
      <minor />
      <comment>/* Usage in philosophy of critical realism */  the archive cited is inaccessible.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6532" xml:space="preserve">{{The Works of Aristotle}}
In [[philosophy]], '''ontic''' (from the [[Greek language|Greek]] {{lang|grc|&#8004;&#957;}}, genitive {{lang|grc|&#8004;&#957;&#964;&#959;&#962;}}: "of that which is") is physical, real, or factual existence.

"Ontic" describes what is there, as opposed to the nature or properties of that being. To illustrate:

*[[Roger Bacon]], observing that all languages are built upon a common grammar, stated that they share a foundation of ontically anchored linguistic structures.
*[[Martin Heidegger]] posited the concept of ''Sorge'', or caring, as the fundamental concept of the [[intentionality|intentional being]], and presupposed an ontological significance that distinguishes [[ontology|ontological]] being from mere "thinghood" of an ontic being. He uses the [[German language|German]] word "[[Dasein]]" for a being that is capable of ontology, that is, [[recursivity|recursively]] comprehending [[property (philosophy)|properties]] of the very fact of its own Being. For Heidegger, "ontical" signifies concrete, specific realities, whereas "ontological" signifies deeper underlying structures of reality. Ontological objects or subjects have an ontical dimension, but they also include aspects of being like self-awareness, evolutionary vestiges, future potentialities, and networks of relationship.&lt;ref&gt;{{cite web|title=Ontico-Ontological Distinction|url=http://www.blackwellreference.com/public/tocnode?id=g9781405106795_chunk_g978140510679516_ss1-33|publisher=Blackwell Reference|accessdate=26 February 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Duffy|first1=Michael|title=The Ontological and the Ontic|url=http://mikejohnduff.blogspot.com/2007/08/ontological.html|accessdate=26 February 2015}}&lt;/ref&gt;
* [[Nicolai Hartmann]] distinguishes among ontology, ontics, and metaphysics: (i) ontology concerns the categorical analysis of entities by means of the knowledge categories able to classify them, (ii) ontics refers to a pre-categorical and pre-objectual connection which is best expressed in the relation to transcendent acts, and (iii) metaphysics is that part of ontics or that part of ontology which concerns the residue of being that cannot be rationalized further according to categories.

== Usage in philosophy of science ==
[[Harald Atmanspacher]] writes extensively about the philosophy of science, especially as it relates to [[Chaos theory]], [[determinism]], [[Causality|causation]], and [[stochastic process|stochasticity]]. He explains that "''ontic'' states describe all properties of a physical system exhaustively. ('Exhaustive' in this context means that an ''ontic'' state is 'precisely the way it is,' without any reference to [[epistemic]] knowledge or ignorance.)"{{ref|autonumber}}

In an earlier paper, Atmanspacher portrays the difference between an [[epistemic]] perspective of a [[system]], and an ontic perspective:

:Philosophical [[discourse]] traditionally distinguishes between [[ontology]] and [[epistemology]] and generally enforces this distinction by keeping the two subject areas separated. However, the relationship between the two areas is of central importance to [[physics]] and [[philosophy of physics]]. For instance, many [[measurement]]-related problems force us to consider both our [[knowledge]] of the [[Classical mechanics|states]] and [[observables]] of a [[system]] ([[epistemic perspective]]) and its states and observables, independent of such knowledge (ontic perspective). This applies to [[quantum|quantum systems]] in particular.{{ref|autonumber}}

== Usage in philosophy of critical realism ==
The [[United Kingdom|British]] [[philosopher]] [[Roy Bhaskar]], who is closely associated with the philosophical [[Cultural movement|movement]] of [[Critical realism (philosophy of the social sciences)|Critical Realism]] writes:
:"I differentiate the 'ontic' ('ontical' etc.) from the 'ontological'. I employ the former to refer to

:# whatever pertains to being generally, rather than some distinctively philosophical (or scientific) theory of it (ontology), so that in this sense, that of the '''ontic&lt;sub&gt;1&lt;/sub&gt;''', we can speak of the ontic presuppositions of a work of art, a [[joke]] or a strike as much as a [[epistemology|theory of knowledge]]; and, within this [[rubric]], to
:# the [[intransitive]] [[object (philosophy)|object]]s of some specific, [[historically determinate]], [[scientific investigation]] (or set of such investigations), the '''ontic&lt;sub&gt;2&lt;/sub&gt;'''.

:"The ontic&lt;sub&gt;2&lt;/sub&gt; is always specified, and only identified, by its relation, as the intransitive object(s) of some or other (denumerable set of) particular transitive process(es) of enquiry. It is cognitive process-, and level-specific; whereas the ontological (like the ontic&lt;sub&gt;1&lt;/sub&gt;) is not."{{ref|autonumber}}

[[Ruth Groff]] offers this expansion of Bhaskar's note above:
:"'ontic&lt;sub&gt;2&lt;/sub&gt;' is an abstract way of denoting the [[object-domain]] of a particular [[scientific]] area, field, or inquiry. E.g.: [[molecules]] feature in the ontic&lt;sub&gt;2&lt;/sub&gt; of chemistry. He's just saying that the scientific undertaking ITSELF is not one of the objects of said, most narrowly construed, immediate object-domain. So [[chemistry]] itself is not part of the ontic&lt;sub&gt;2&lt;/sub&gt; of chemistry."

==See also==
*[[Ding an sich#Noumenon and the thing-in-itself|Ding an sich]]
*[[Ontologism]]
*[[Physical ontology]]
*[[Substance theory]]

==References==
{{Reflist}}
# {{Note|autonumber}} Atmanspacher, Dr. H., and Primas, H., 2003 [2005], "Epistemic and Ontic [[Quantum]] [[Reality|Realities]]", in Khrennikov, A (Ed.), ''Foundations of Probability and Physics'' ([[American Institute of Physics]] 2005, pp 49&amp;ndash;61, Originally published in ''Time, Quantum and Information'', edited by Lutz Castell and Otfried Ischebeck, Springer, Berlin, 2003, pp 301&amp;ndash;321
# {{Note|autonumber}} Atmanspacher, Harald (2001) ''[[Determinism]] Is Ontic, Determinability is [[Epistemic]]'' ([http://philsci-archive.pitt.edu/archive/00000939/00/determ.pdf [[University of Pittsburgh]] Archives])
# {{Note|autonumber}} Bhaskar, R.A., 1986, ''Scientific Realism and Human Emancipation'' (London: Verso), pp 36 and 37, as quoted by [[Howard Engelskirchen]] in the [http://archives.econ.utah.edu/archives/bhaskar/2001m11/msg00015.htm Bhaskar mailing list archive]
{{Continental philosophy}}
{{wiktionary}}

[[Category:Concepts in metaphysics]]
[[Category:Knowledge representation]]
[[Category:Martin Heidegger]]
[[Category:Modal logic]]
[[Category:Ontology]]
[[Category:Philosophy of science]]
[[Category:Reality]]</text>
      <sha1>f69zw6ifb5drs5quo5r5n2xtksj32uc</sha1>
    </revision>
  </page>
  <page>
    <title>CDS ISIS</title>
    <ns>0</ns>
    <id>2889648</id>
    <revision>
      <id>683750557</id>
      <parentid>606951351</parentid>
      <timestamp>2015-10-02T07:48:58Z</timestamp>
      <contributor>
        <username>Ymblanter</username>
        <id>14596827</id>
      </contributor>
      <comment>/* See also */  rm redlinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3577" xml:space="preserve">{{Use dmy dates|date=May 2014}}
'''CDS/ISIS''' is a [[software]] package for generalised ''Information Storage and Retrieval systems'' developed, maintained and disseminated by [[UNESCO]]. It was first released in 1985 and since then over 20,000 [[license|licences]] have been issued by UNESCO and a worldwide network of distributors. It is particularly suited to bibliographical applications and is used for the [[library catalog|catalogues]] of many small and medium-sized [[library|libraries]]. Versions have been produced in Arabic, Chinese, English, French, German, Portuguese, Russian and Spanish amongst other languages. UNESCO makes the software available free for non-commercial purposes, though distributors are allowed to charge for their expenses.

CDS/ISIS is an acronym which stands for Computerised Documentation Service / Integrated Set of Information Systems. In 2003 it was stated that "This package is accepted by libraries in the developing countries as a standard software for information system development".&lt;ref&gt;National Science Foundation of Sri Lanka. "CDS ISIS Library Software" [Last Update 10 January 2003.] http://www.nsf.ac.lk/slstic/isis.htm  Accessed 20 June 2007.&lt;/ref&gt;

The original CDS/ISIS ran on an [[IBM]] [[mainframe computer|mainframe]] and was designed in the mid-1970s under Mr Giampaolo Del Bigio for UNESCO's Computerized Documentation System (CDS). It was based on the internal ISIS (Integrated Set of Information Systems) at the [[International Labour Organization]] in Geneva.

In 1985 a version was produced for mini- and microcomputers programmed in Pascal. It ran on an [[IBM PC]] under [[MS-DOS]]&lt;ref&gt;Buxton, Andrew and Hopkinson, Alan. ''The CDS/ISIS handbook''. London: Library Association, 1994&lt;/ref&gt;
. ''Winisis'', the [[Microsoft Windows|Windows]] version,  first demonstrated in 1995, may run on a single [[computer]] or in a [[local area network]]. A ''JavaISIS'' client/server component was designed in 2000, allowing remote [[database management system|database management]] over the [[Internet]] from [[Microsoft Windows|Windows]], [[Linux]] and [[Apple Macintosh|Macintosh]] computers. Furthermore, ''GenISIS'' allows the user to produce [[HTML]] Web forms for CDS/ISIS database searching. The ''ISIS_DLL'' provides an [[API]] for developing CDS/ISIS based applications. The [[OpenIsis]] library, developed independently from 2002 to 2004, provided another [[API]] for developing CDS/ISIS-like applications.

The most recent effort towards a completely renewed [[Free and open-source software|FOSS]], [[Unicode]] implementation of CDS/ISIS is the J-Isis project, developed by UNESCO since 2005 and currently maintained by Mr Jean Claude Dauphin.

== See also ==
* [[IDIS (software)|IDIS]] is a tool for direct data exchange between CDS/ISIS and IDAMS.

== External links ==
* [http://kenai.com/projects/j-isis J-ISIS New UNESCO Java CDS/ISIS Software]
* [http://portal.unesco.org/ci/en/ev.php-URL_ID=2071&amp;URL_DO=DO_TOPIC&amp;URL_SECTION=201.html CDS/ISIS database software (UNESCO)]
* [http://lists.iccisis.org International list hosted from 2010 by the ICCIsis (International Coordination Committee on ISIS)]
* [https://listserv.surfnet.nl/archives/cds-isis.html Archives of CDS-ISIS@NIC.SURFNET.NL (discontinued in 2010)]
* http://openisis.org/ (discontinued)
* http://sourceforge.net/projects/isis (discontinued)
* [http://pecl.php.net/package/isis PHP extension for reading CDS/ISIS databases]

== References ==
{{Reflist}}

[[Category:Knowledge representation]]
[[Category:Proprietary database management systems]]</text>
      <sha1>r96j1m0jielydootm3a3l2m458qk4hh</sha1>
    </revision>
  </page>
  <page>
    <title>Conceptual graph</title>
    <ns>0</ns>
    <id>346755</id>
    <revision>
      <id>761060488</id>
      <parentid>751303674</parentid>
      <timestamp>2017-01-20T17:15:30Z</timestamp>
      <contributor>
        <username>Jefferythomas</username>
        <id>13631841</id>
      </contributor>
      <comment>/* External links */ added john sowa link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7368" xml:space="preserve">'''Conceptual graphs''' ('''CGs''') are a formalism for [[knowledge representation]]. In the first published paper  on CGs, [[John F. Sowa]] {{harv|Sowa|1976}} used them to represent the [[conceptual schema]]s used in [[database system]]s. The first book on CGs {{harv|Sowa|1984}} applied them to a wide range of topics in [[artificial intelligence]], [[computer science]], and [[cognitive science]].

Since 1984, the model has been developed along three main directions.

== A graphical interface for first-order logic ==
In this approach, a formula in [[first-order logic]] (predicate calculus) is represented by a labeled graph.

A linear notation, called the '''Conceptual Graph Interchange Format (CGIF)''', has been standardized in the ISO standard for [[common logic]].

[[Image:Cat-on-mat.svg|thumb|250px|Elsie the cat is sitting on a mat]]
The diagram on the right is an example of the '''display form''' for a conceptual graph.  Each box is called a '''concept node''', and each oval is called a '''relation node'''.  In CGIF, this CG would be represented by the following statement:

    [Cat Elsie] [Sitting *x] [Mat *y] (agent ?x Elsie) (location ?x ?y)
In CGIF, brackets enclose the information inside the concept nodes, and parentheses enclose the information inside the relation nodes.  The letters x and y, which are called '''coreference labels''', show how the concept and relation nodes are connected.  In the '''Common Logic Interchange Format (CLIF)''', those letters are mapped to variables, as in the following statement:

    (exists ((x Sitting) (y Mat)) (and (Cat Elsie) (agent x Elsie) (location x y)))

As this example shows, the asterisks on the coreference labels *x and *y in CGIF map to existentially quantified variables in CLIF, and the question marks on ?x and ?y map to bound variables in CLIF.  A universal quantifier, represented '''@every*z''' in CGIF, would be represented '''forall (z)''' in CLIF.

Reasoning can be done by translating graphs into logical formulas, then applying a logical inference engine.

== Diagrammatic calculus of logics ==
Another research branch continues the work on [[existential graph]]s of [[Charles Sanders Peirce]], which were one of the origins of conceptual graphs as proposed by Sowa.
In this approach, developed in particular by Dau {{harv|Dau|2003}}, conceptual graphs are conceptual [[diagram]]s rather than graphs in the sense of [[graph theory]], and reasoning operations are performed by operations on these diagrams.

== Graph-based knowledge representation and reasoning model ==
Key features of '''GBKR''', the graph-based knowledge representation and reasoning model developed by Chein and Mugnier and the Montpellier group {{harv|Chein|Mugnier|2009}},
can be summarized as follows:

* all kinds of knowledge (ontology, rules, constraints and facts) are labeled graphs, which provide an intuitive and easily understandable means to represent knowledge,
* reasoning mechanisms are based on graph notions, basically the classical notion of graph homomorphism; this allows, in particular, to link basic reasoning problems to other fundamental problems in computer science (problems concerning conjunctive queries in relational databases, constraint satisfaction problem, ...),
* the formalism is logically founded, i.e., it has a semantics in first-order logic and the inference mechanisms are sound and complete with respect to deduction in first-order logic,
* from a computational viewpoint, the graph homomorphism notion was recognized in the 1990s as a central notion, and complexity results and efficient algorithms have been obtained in several domains.

COGITANT and COGUI are tools that implement the '''GBKR''' model. COGITANT is a library of C++ classes that implement most of the GBKR notions and reasoning mechanisms. COGUI  is a graphical user interface dedicated to the construction of a GBKR knowledge base (it integrates COGITANT and, among numerous functionalities, it contains a translator from GBKR to RDF/S and conversely).

== Sentence generalization and generalization diagrams ==
Sentence [[generalization]] and generalization diagrams can be defined as a special sort of conceptual graphs which can be constructed automatically from syntactic [[parse tree]]s and support semantic classification task {{harv|Galitsky et al|2010}}. Similarity measure between syntactic parse trees can be done as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the [[syntax]] generalization level and [[semantics]] generalization level (anti-unification of [[logic forms]]). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at semantic level.

==See also==
*[[Resource Description Framework]] (RDF)
*[[SPARQL]] (Graph Query Language)
*[[Semantic network]]
*[[Knowledge representation]]
*[[Chunking (psychology)]]
*[[Concept map]]
*[[Conceptual schema]]
*[[Diagrammatic reasoning]]

==References==
* {{cite book |last=Chein |first=Michel |last2=Mugnier |first2=Marie-Laure |year=2009 |title=Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs |publisher=Springer |url=http://www.lirmm.fr/gbkrbook/ |isbn=978-1-84800-285-2 |ref=harv | doi=10.1007/978-1-84800-286-9}}
* {{cite journal |last=Dau |first=F. |year=2003 |title=The Logic System of Concept Graphs with Negation and Its Relationship to Predicate Logic |journal=[[Lecture Notes in Computer Science]] |volume=2892 |publisher=Springer |isbn= |ref=harv }}
* {{cite journal |last=Sowa |authorlink = John Sowa |first=John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336&#8211;357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}
* {{cite book |last=Sowa |first=John F. |year=1984 |title=Conceptual Structures:  Information Processing in Mind and Machine |location=Reading, MA |publisher=Addison-Wesley |isbn=978-0-201-14472-7 |ref=harv }}
* {{cite journal | last=Galitsky | first=Boris | last2=Dobrocsi |first2=Gabor | last3=de la Rosa |first3=Josep Lluis | last4=Kuznetsov |first4=Sergei O. |year=2010 |title=From Generalization of Syntactic Parse Trees to Conceptual Graphs |journal=Lecture Notes in Computer Science |volume=6208 |publisher=Springer |isbn= |url=http://dl.acm.org/citation.cfm?id=1881190|ref=harv }}
* {{cite journal|title=Conceptual graphs for the analysis and generation of sentences
|first1=Paola |last1=Velardi |first2=Maria Teresa |last2=Pazienza |first3=Mario |last3=De' Giovanetti |journal=IBM Journal of Research and Development |volume=32 |number=2 |date=March 1988 |pages=251&#8211;267 |publisher=IBM Corp. Riverton, NJ, USA |doi=10.1147/rd.322.0251}}

==External links==
* [http://conceptualstructures.org Conceptual Structures Home Page]. (Old site:  [http://conceptualgraphs.org Conceptual Graphs Home Page])
* [http://www.informatik.uni-trier.de/~ley/db/conf/iccs/index.html Yearly international conferences (ICCS)]
* [http://www.jfsowa.com/cg/index.htm Conceptual Graphs on John F. Sowa's Website]

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Application-specific graphs]]</text>
      <sha1>dcj1fcdgy0lnqgloe3a1wb9jc40to0k</sha1>
    </revision>
  </page>
  <page>
    <title>Brinkler classification</title>
    <ns>0</ns>
    <id>3388492</id>
    <revision>
      <id>680103537</id>
      <parentid>517350881</parentid>
      <timestamp>2015-09-08T18:54:43Z</timestamp>
      <contributor>
        <ip>69.135.193.98</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2666" xml:space="preserve">'''Brinkler Classification''' is the [[library classification]] system  of [[Bartol Brinkler]] described in his article "The Geographical Approach to Materials in the Library of Congress Subject Headings".&lt;ref&gt;Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings: report of a study project. s.l.: s.n., 1960. [Accession No: {{OCLC|3853830}}].&lt;/ref&gt; The geographical aspect of a subject may be conveyed through three types of headings labeled A, B, and C. Heading A uses a primary topical description with geographical subdivisions (e.g. Art&#8212;Paris).  Type B uses a place-name for the main heading with a topical subdivision (e.g. Paris&#8212;Description). C headings use a geographical description of a phrase (e.g. Paris Literature).  

Brinkler explores what type of heading is more useful to a [[patron]], and he finds that it depends on the level of familiarity a patron has with a topic and what approach they take when searching for resources on their topic. Ideally readers will either be looking for everything on a particular topic, or everything regarding a particular place. Bartol Brinkler investigates a system of classification that will best serve these two ideal types of patrons.  He finds working with Type A headings will best assist a patron who is more topic oriented, while using Type B headings is preferable for those who are primarily interested in one place. 

However this is problematic in practice. One possibility is to assign Type A and Type B headings to every resource, but the cataloguing cost would be high.  A system that aids readers regardless of their approach to a topic involves using cross-references (e.g. Canada&#8212;Botany, See Botany&#8212;Canada).  Admitting that see and see also references would require more work on the part of librarians, Bartol Brinkler notes that librarians must keep in mind "...readers do not have the same knowledge [of classification] and do need all the help they can get..."{{Citation needed|date=March 2008}}

==References==
{{reflist}}
*Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings. Library Resources &amp; Technical Services 6, no. 1 (Winter 1962): 49-64.

==External links==
*[http://hcl.harvard.edu/libraries/#widener  Harvard University. Widener Library.]
*[http://www.loc.gov/catdir/cpso/lcco/lcco.html Library of Congress Classification Outline.]
*[http://www.princeton.edu/~paw/memorials/memorials_1930s/memorials_1937.html Princeton Alumni Weekly: Memorials 1937.]

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]</text>
      <sha1>531yu33u8zo74zu2q8olqx0kpt8wyix</sha1>
    </revision>
  </page>
  <page>
    <title>Simple Knowledge Organization System</title>
    <ns>0</ns>
    <id>4916685</id>
    <revision>
      <id>753634092</id>
      <parentid>749208144</parentid>
      <timestamp>2016-12-08T09:40:15Z</timestamp>
      <contributor>
        <ip>134.2.65.26</ip>
      </contributor>
      <comment>/* Tools */ added ThesauRex</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27082" xml:space="preserve">'''Simple Knowledge Organization System''' ('''SKOS''') is a [[W3C recommendation]] designed for representation of [[Thesaurus (information retrieval)|thesauri]], [[classification scheme]]s, [[Taxonomy (general)|taxonomies]], [[Authority control|subject-heading systems]], or any other type of structured [[controlled vocabulary]]. SKOS is part of the [[Semantic Web]] family of standards built upon [[Resource Description Framework|RDF]] and [[RDF Schema|RDFS]], and its main objective is to enable easy publication and use of such vocabularies as [[linked data]].

== History ==

=== DESIRE II project (1997&amp;ndash;2000) ===

The most direct ancestor to SKOS was the RDF Thesaurus work undertaken in the second phase of the EU DESIRE project &lt;ref name="Desire Project"&gt;{{Citation |publication-date=August 7, 2000 |title=Desire: Development of a European Service for Information on Research and Education |publisher=Desire Consortium |url=http://www.desire.org/ |archiveurl=https://web.archive.org/web/20110725230823/http://www.desire.org/ |archivedate=July 25, 2011 }}&lt;/ref&gt;{{Citation needed|reason=The Desire Project reference does not appear to directly address the SKOS ancestry statement made here.|date=August 2012}}.  Motivated by the need to improve the user interface and usability of multi-service browsing and searching,&lt;ref name="Desire Deliverable D.36b"&gt;{{Citation |title=Desire: Research Deliverables: D3.1 |publisher=Desire Consortium |url=http://www.desire.org/docs/research/deliverables/D3.6/d36b.html |archiveurl=https://web.archive.org/web/20080509135041/http://www.desire.org/html/research/deliverables/D3.6/#d36b |archivedate=May 9, 2008 }}&lt;/ref&gt; a basic RDF vocabulary for Thesauri was [http://www.desire.org/results/discovery/rdfthesschema.html produced]. As noted later in the [http://www.w3.org/2001/sw/Europe/plan/workpackages/live/esw-wp-8.html SWAD-Europe workplan], the DESIRE work was adopted and further developed in the SOSIG and LIMBER projects. A version of the DESIRE/SOSIG implementation was described in W3C's QL'98 workshop, motivating early work on RDF rule and query languages: [http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF].&lt;ref&gt;[http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF]&lt;/ref&gt;

=== LIMBER (1999&amp;ndash;2001) ===

SKOS built upon the output of the Language Independent Metadata Browsing of European Resources (LIMBER) project funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. In the LIMBER project [[CCLRC]] further developed an [[Resource Description Framework|RDF]]  thesaurus interchange format&lt;ref&gt;[http://journals.tdl.org/jodi/article/viewArticle/34/35 Miller, K. &amp; Matthews, B. (2001). Having the right connections: the LIMBER Project. Journal of Digital Information, 1 (8), 5 February. ]&lt;/ref&gt; which was demonstrated on the European Language Social Science Thesaurus ([http://www.cessda.org/results.html?query=elsst ELSST]) at the [[UK Data Archive]] as a multilingual version of the English language Humanities and Social Science Electronic Thesaurus (HASSET) which was planned to be used by the Council of European Social Science Data Archives [http://www.cessda.org/ CESSDA].

=== SWAD-Europe (2002&amp;ndash;2004) ===

SKOS as a distinct initiative began in the SWAD-Europe project, bringing together partners from both DESIRE, SOSIG (ILRT) and LIMBER (CCLRC) who had worked with earlier versions of the schema. It was developed in the Thesaurus Activity Work Package, in the Semantic Web Advanced Development for Europe (SWAD-Europe) project.&lt;ref&gt;[http://www.w3.org/2001/sw/Europe/ SWAD-Europe]&lt;/ref&gt; SWAD-Europe was funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. The project was designed to support W3C's Semantic Web Activity through research, demonstrators and outreach efforts conducted by the five project partners, [[ERCIM]], the [http://www.ilrt.bris.ac.uk/ ILRT] at [[Bristol University]], [[HP Labs]], [[CCLRC]] and Stilo.&lt;ref&gt;[http://www.stilo.com Stilo Home Page]&lt;/ref&gt;
The first release of SKOS Core and SKOS Mapping were published at the end of 2003, along with other deliverables on RDF encoding of multilingual thesauri&lt;ref&gt;[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.3.html SWAD-Europe Deliverable 8.3 : RDF Encoding of Multilingual Thesauri]&lt;/ref&gt; and thesaurus mapping.&lt;ref&gt;[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.4.html SWAD-Europe Deliverable 8.4 : Inter-Thesaurus Mapping]&lt;/ref&gt;

=== Semantic web activity (2004&amp;ndash;2005) ===

Following the termination of SWAD-Europe, SKOS effort was supported by the W3C Semantic Web Activity&lt;ref&gt;[http://www.w3.org/2001/sw/ W3C Semantic Web Activity]&lt;/ref&gt; in the framework of the Best Practice and Deployment Working Group.&lt;ref&gt;[http://www.w3.org/2004/03/thes-tf/mission W3C Semantic Web Best Practice and Deployment Working Group : Porting Thesauri Task Force]&lt;/ref&gt; During this period, focus was put both on consolidation of SKOS Core, and development of practical guidelines for porting and publishing thesauri for the Semantic Web.

=== Development as W3C Recommendation (2006&amp;ndash;2009)===

The SKOS main published documents &#8212; the SKOS Core Guide,&lt;ref&gt;[http://www.w3.org/TR/swbp-skos-core-guide SKOS Core Guide] W3C Working Draft 2 November 2005&lt;/ref&gt; the SKOS Core Vocabulary Specification,&lt;ref&gt;[http://www.w3.org/TR/swbp-skos-core-spec SKOS Core Vocabulary Specification] W3C Working Draft 2 November 2005&lt;/ref&gt; and the Quick Guide to Publishing a Thesaurus on the Semantic Web&lt;ref&gt;[http://www.w3.org/TR/swbp-thesaurus-pubguide Quick Guide to Publishing a Thesaurus on the Semantic Web] W3C Working Draft 17 May 2005&lt;/ref&gt; &#8212; were developed through the W3C Working Draft process. Principal editors of SKOS were Alistair Miles,&lt;ref&gt;[http://purl.org/net/aliman Alistair Miles Home Page]&lt;/ref&gt; initially Dan Brickley,&lt;ref&gt;[http://danbri.org/ Dan Brickley Home Page]&lt;/ref&gt; and Sean Bechhofer.&lt;ref&gt;[http://www.cs.man.ac.uk/~seanb/#me Sean Bechhofer Home Page]&lt;/ref&gt;

The Semantic Web Deployment Working Group,&lt;ref&gt;[http://www.w3.org/2006/07/SWD/ W3C Semantic Web Deployment Working Group]&lt;/ref&gt; chartered for two years (May 2006 - April 2008), has put in its charter to push SKOS forward on the [[W3C Recommendation]] track. The roadmap projects SKOS as a Candidate Recommendation by the end of 2007, and as a Proposed Recommendation in the first quarter of 2008. The main issues to solve are determining its precise scope of use, and its articulation with other RDF languages and standards used in libraries (such as [[Dublin Core]]).&lt;ref&gt;[http://isegserv.itd.rl.ac.uk/public/skos/press/dc2006/camera-ready-paper.pdf SKOS: Requirements for Standardization]. The paper by Alistair Miles presented in October 2006 at the International Conference on Dublin Core and Metadata Applications.&lt;/ref&gt;&lt;ref&gt;[http://purl.org/net/retrieval Retrieval and the Semantic Web, incorporating a Theory of Retrieval Using Structured Vocabularies]. Dissertation on the theory of retrieval using structured vocabularies by Alistair Miles.&lt;/ref&gt;

=== Formal release (2009) ===
On August 18, 2009, [[W3C]] released the new standard that builds a bridge between the world of knowledge organization systems - including thesauri, classifications, subject headings, taxonomies, and [[folksonomy|folksonomies]] - and the [[linked data]] community, bringing benefits to both. Libraries, museums, newspapers, government portals, enterprises, social networking applications, and other communities that manage large collections of books, historical artifacts, news reports, business glossaries, blog entries, and other items can now use SKOS&lt;ref&gt;[http://www.w3.org/TR/2009/REC-skos-reference-20090818/ Simple Knowledge Organization System (SKOS)]&lt;/ref&gt; to leverage the power of linked data.

=== Historical view of components ===

SKOS was originally designed as a modular and extensible family of languages, organized as SKOS Core, SKOS Mapping, and SKOS Extensions, and a Metamodel. The entire specification is now complete within the namespace [http://www.w3.org/2004/02/skos/core# http://www.w3.org/2004/02/skos/core#].

== Overview ==

In addition to the reference itself, the [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/ SKOS Primer] (a W3C Working Group Note) summarizes the Simple Knowledge Organization System.

The SKOS&lt;ref&gt;[http://www.w3.org/TR/skos-reference SKOS Reference]&lt;/ref&gt; defines the classes and properties sufficient to represent the common features found in a standard thesaurus. It is based on a concept-centric view of the vocabulary, where primitive objects are not terms, but abstract notions represented by terms. Each SKOS concept is defined as an [[web resource|RDF resource]]. Each concept can have RDF properties attached, including:
* one or more preferred [[index term]]s (at most one in each natural language)
* alternative terms or [[synonym]]s
* definitions and notes, with specification of their language

Concepts can be organized in [[hierarchy|hierarchies]] using broader-narrower relationships, or linked by non-hierarchical (associative) relationships.
Concepts can be gathered in concept schemes, to provide consistent and structured sets of concepts, representing whole or part of a controlled vocabulary.

=== Element categories ===

The principal element categories of SKOS are concepts, labels, notations, semantic relations, mapping properties, and collections. The associated concepts are listed in the table below.

{| border="1" class="wikitable"
|+ SKOS Vocabulary Organized by Theme
! Concepts
! Labels &amp; Notation
! Documentation
! Semantic Relations
! Mapping Properties
! Collections
|-
| Concept || prefLabel  || note || broader || broadMatch || Collection 
|-
| ConceptScheme || altLabel  || changeNote || narrower || narrowMatch || orderedCollection 
|-
| inScheme || hiddenLabel || definition || related || relatedMatch || member
|-
| hasTopConcept || notation  || editorialNote || broaderTransitive || closeMatch || memberList
|-
| topConceptOf ||   || example || narrowerTransitive || exactMatch ||  
|-
|   ||   || historyNote || semanticRelation || mappingRelation || 
|-
|   ||   || scopeNote ||  ||  || 
|-

|}

=== Concepts ===

The SKOS vocabulary is based on concepts. Concepts are the units of thought&#8212;ideas, meanings, or objects and events (instances or categories)&#8212;which underlie many knowledge organization systems. As such, concepts exist in the mind as abstract entities which are independent of the terms used to label them. In SKOS, a &lt;code&gt;Concept&lt;/code&gt; (based on the OWL &lt;code&gt;Class&lt;/code&gt;) is used to represent items in a knowledge organization system (terms, ideas, meanings, etc.) or such a system's conceptual or organizational structure.

A &lt;code&gt;ConceptScheme&lt;/code&gt; is analogous to a vocabulary, thesaurus, or other way of organizing concepts. SKOS does not constrain a concept to be within a particular scheme, nor does it provide any way to declare a complete scheme&#8212;there is no way to say the scheme consists only of certain members. A topConcept is (one of) the upper concept(s) in a hierarchical scheme.

=== Labels and notations ===

Each SKOS &lt;code&gt;label&lt;/code&gt; is a string of [[Unicode]] characters, optionally with language tags, that are associated with a concept. The &lt;code&gt;prefLabel&lt;/code&gt; is the preferred human-readable string (maximum one per language tag), while &lt;code&gt;altLabel&lt;/code&gt; can be used for alternative strings, and &lt;code&gt;hiddenLabel&lt;/code&gt; can be used for strings that are useful to associate, but not meant for humans to read.

A SKOS &lt;code&gt;notation&lt;/code&gt; is similar to a label, but the literal string has a datatype, like integer, float, or date; the datatype can even be made up (see [http://www.w3.org/TR/skos-reference/#L2613 6.5.1 Notations, Typed Literals and Datatypes] in the SKOS Reference). The notation is useful for classification codes and other strings not recognizable as words.

=== Documentation ===

The Documentation or Note properties provide basic information about SKOS concepts. All the concepts are considered a type of &lt;code&gt;skos:note&lt;/code&gt;; they just provide more specific kinds of information. The property &lt;code&gt;definition&lt;/code&gt;, for example, should contain a full description of the subject resource.  More specific note types can be defined in a SKOS extension, if desired. A query for &lt;code&gt;&amp;lt;A&amp;gt; skos:note ?&lt;/code&gt; will obtain all the notes about &amp;lt;A&amp;gt;, including definitions, examples, and scope, history and change, and editorial documentation.

Any of these SKOS Documentation properties can refer to several object types: a literal (e.g., a string); a resource node that has its own properties; or a reference to another document, for example using a URI. This enables the documentation to have its own [[metadata]], like creator and creation date.

Specific guidance on SKOS documentation properties can be found in the SKOS Primer [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/#secdocumentation Documentary Notes].

=== Semantic relations ===

SKOS semantic relations are intended to provide ways to declare relationships between concepts within a concept scheme. While there are no restrictions precluding their use with two concepts from separate schemes, this is discouraged because it is likely to overstate what can be known about the two schemes, and perhaps link them inappropriately.

The property &lt;code&gt;related&lt;/code&gt; simply makes an association relationship between two concepts; no hierarchy or generality relation is implied. The properties &lt;code&gt;broader&lt;/code&gt; and &lt;code&gt;narrower&lt;/code&gt; are used to assert a direct hierarchical link between two concepts. The meaning may be unexpected; the relation &lt;code&gt;&amp;lt;A&amp;gt; broader &amp;lt;B&amp;gt;&lt;/code&gt; means that A has a broader concept called B&#8212;hence that B is broader than A. Narrower follows in the same pattern.

While the casual reader might expect broader and narrower to be transitive properties, SKOS does not declare them as such. Rather, the properties &lt;code&gt;broaderTransitive&lt;/code&gt; and &lt;code&gt;narrowerTransitive&lt;/code&gt; are defined as transitive super-properties of broader and narrower. These super-properties are (by convention) not used in declarative SKOS statements. Instead, when a broader or narrower relation is used in a triple, the corresponding transitive super-property also holds; and transitive relations can be inferred (and queried) using these super-properties.

=== Mapping ===

SKOS mapping properties are intended to express matching (exact or fuzzy) of concepts from one concept scheme to another, and by convention are used only to connect concepts from different schemes. The concepts &lt;code&gt;relatedMatch&lt;/code&gt;, &lt;code&gt;broadMatch&lt;/code&gt;, and &lt;code&gt;narrowMatch&lt;/code&gt; are a convenience, with the same meaning as the semantic properties &lt;code&gt;related&lt;/code&gt;, &lt;code&gt;broader&lt;/code&gt;, and &lt;code&gt;narrower&lt;/code&gt;. (See previous section regarding the meanings of broader and narrower.)

The property relatedMatch makes a simple associative relationship between two concepts. When concepts are so closely related that they can generally be used interchangeably, &lt;code&gt;exactMatch&lt;/code&gt; is the appropriate property (exactMatch relations are transitive, unlike any of the other Match relations). The &lt;code&gt;closeMatch&lt;/code&gt; property that indicates concepts that only sometimes can be used interchangeably, and so it is not a transitive property.

=== Concept collections ===

The concept collections (&lt;code&gt;Collection&lt;/code&gt;, &lt;code&gt;orderedCollection&lt;/code&gt;) are labeled and/or ordered (&lt;code&gt;orderedCollection&lt;/code&gt;) groups of SKOS concepts. Collections can be nested, and can have defined URIs or not (which is known as a blank node). Neither a SKOS &lt;code&gt;Concept&lt;/code&gt; nor a &lt;code&gt;ConceptScheme&lt;/code&gt; may be a Collection, nor vice versa; and SKOS semantic relations can only be used with a Concept (not a Collection). The items in a Collection can not be connected to other SKOS Concepts through the Collection node; individual relations must be defined to each Concept in the Collection.

== Community and participation ==

All development work is carried out via the mailing list which is a completely open and publicly archived&lt;ref&gt;[http://lists.w3.org/Archives/Public/public-esw-thes/ public-esw-thes@w3.org online archive]. Archives of mailing list used for SKOS development.&lt;/ref&gt; mailing list devoted to discussion of issues relating to knowledge organisation systems, information retrieval and the Semantic Web. Anyone may participate informally in the development of SKOS by joining the discussions on public-esw-thes@w3.org - informal participation is warmly welcomed. Anyone who works for a [http://www.w3.org/Consortium/join W3C member] organisation may formally participate in the development process by joining the [http://www.w3.org/2006/07/SWD/ Semantic Web Deployment Working Group] - this entitles individuals to edit specifications and to vote on publication decisions.

== Applications ==

*Some important vocabularies have been migrated into SKOS format and are available in the public domain, including [[EuroVoc]], [[AGROVOC]] and [[GEMET]]. [[Library of Congress Subject Headings]] (LCSH) also support the SKOS format.&lt;ref&gt;[http://id.loc.gov/authorities/about.html About the Library of Congress Authorities]&lt;/ref&gt;
*SKOS has been used as the language for the thesauri used in the [[SWED Environmental Directory]]&lt;ref&gt;[http://www.swed.org.uk/swed Semantic Web Environmental Directory]&lt;/ref&gt; developed in the SWAD-Europe project framework.
*A way to convert thesauri to SKOS,&lt;ref&gt;[http://thesauri.cs.vu.nl/eswc06/ A Method to Convert Thesauri to SKOS]&lt;/ref&gt; with examples including the [[Medical Subject Headings|MeSH]] thesaurus, has been outlined by the [[Vrije Universiteit Amsterdam]].
*Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS has been developed by [[IBM]].&lt;ref&gt;[http://www-128.ibm.com/developerworks/xml/library/x-dita10/ Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS] by IBM developerWorks.&lt;/ref&gt;
*SKOS is used to represent geographical feature types in the [[GeoNames]] ontology.

== Tools ==
* [https://github.com/eScienceCenter/ThesauRex ThesauRex] is an open-source, web-based SKOS editor. It is limited to broader/narrower relations among concepts and offers tree-based interaction and with thesauri and drag&amp;drop creation of new thesauri based on a master thesaurus.
* Mondeca's [http://www.mondeca.com/Products/ITM Intelligent Topic Manager] (ITM) is a full-featured SKOS-compliant solution for managing taxonomies, thesauri, and other controlled vocabularies.
*[http://pactols.frantiq.fr/opentheso/ Opentheso] is an open source web-based thesaurus management system compliant with ISO 25964:2011 and ISO 25964-2:2012 standards (Information and Documentation. Thesauri and Interoperability with other vocabularies). It offers SKOS and csv exports and imports, REST and SOAP web services and manages persistent identifiers (ARK). It has been developed at the French National Center for Scientific Research since 2007. It is currently used by the French archaeological libraries network [http://www.frantiq.fr Frantiq] and by research teams  and by the Hospices Civils de Lyon as a collaborative thesaurus management tool. It can be dowloaded on [https://github.com/frantiq/opentheso github]. 
* [http://openskos.org OpenSKOS] is a web service-based approach to publication, management and use of vocabulary data that can be mapped to SKOS. Its source code is available on [https://github.com/CatchPlus/OpenSKOS GitHub]. It includes [[CRUD]] like [[RESTful]] operations on SKOS concepts and a web-based editor for searching and editing concepts. It was developed by [http://picturae.com Picturae] and funded by the Dutch heritage fond [http://www.catchplus.nl/ CATCHPlus].
* TemaTres Vocabulary Server&lt;ref&gt;[http://www.vocabularyserver.com/ TemaTres] is a Web tool to manage formal and linguistic representations of knowledge.&lt;/ref&gt; is an open source web-based vocabulary server for managing controlled vocabularies, taxonomies and thesauruses. [http://sourceforge.net/projects/tematres Tematres] provides complete export of vocabularies into SKOS-core in addition to Zthes, TopicMaps, MADS, Dublin Core,VDEX, BS 8723, SiteMap, SQL and text.
* ThManager&lt;ref&gt;[http://thmanager.sourceforge.net/ ThManager] an Open Source Tool for creating and visualizing SKOS RDF vocabularies.&lt;/ref&gt; is a [[Java (programming language)|Java]] [[Open-source software|open-source]] application for creating and visualizing SKOS vocabularies.
* The W3C provides an experimental on-line validation service.&lt;ref&gt;[http://www.w3.org/2004/02/skos/core/validation SKOS Core Validation Service]&lt;/ref&gt;
* SKOS files can also be imported and edited in RDF-OWL editors such as [[Protege (software)|Prot&#233;g&#233;]] and [[SWOOP]] developed by Maryland Information and Network Dynamics Lab Semantic Web Agents Project [[Mindswap]].&lt;ref&gt;[http://www.mindswap.org/2004/SWOOP/ SWOOP] A Hypermedia-based Featherweight OWL Ontology Editor, developed by [[Mindswap]] - Maryland Information and Network Dynamics Lab Semantic Web Agents Project&lt;/ref&gt;
* SKOS synonyms can be transformed from [[WordNet]] RDF format using an [[XSLT]] style sheet; see [http://www.w3.org/TR/wordnet-rdf W3C RDF]
* PoolParty&lt;ref&gt;[http://www.poolparty.biz/ PoolParty] is a thesaurus management system and a SKOS editor for the Semantic Web.&lt;/ref&gt; is a commercial-quality thesaurus management system and a SKOS editor for the Semantic Web including text analysis functionalities and [[Linked Data]] capabilities.
* qSKOS&lt;ref&gt;[https://github.com/cmader/qSKOS/ qSKOS] is an open-source tool for SKOS vocabulary quality assessment.&lt;/ref&gt; is an open-source tool for performing quality assessment of SKOS vocabularies by checking against a quality issue catalog.
* SKOSEd&lt;ref&gt;[http://code.google.com/p/skoseditor/ SKOSEd] SKOS plugin for Protege 4&lt;/ref&gt; is an open source plug-in for the Prot&#233;g&#233; 4&lt;ref&gt;[http://www.co-ode.org/downloads/protege-x/ Prot&#233;g&#233; 4] Prot&#233;g&#233; 4 OWL editor&lt;/ref&gt; [[Web Ontology Language|OWL]] ontology editor that supports authoring SKOS vocabularies. SKOSEd has an accompanying SKOS API&lt;ref&gt;[http://skosapi.sourceforge.net/ SKOS Java API] Java API for SKOS&lt;/ref&gt; written in Java that can be used to build SKOS-based applications.
* Model Futures SKOS Exporter&lt;ref&gt;[http://www.modelfutures.com/software Model Futures Excel SKOS Exporter]&lt;/ref&gt; for [[Microsoft Excel]] allows simple vocabularies to be developed as indented Excel spreadsheets and exported as SKOS RDF. BETA version.
* Lexaurus&lt;ref&gt;[http://www.vocman.com/ Lexaurus] is an enterprise thesaurus management system and multi-format editor.&lt;/ref&gt; is an enterprise thesaurus management system and multi-format editor. Its extensive API includes full revision management. SKOS is one of its many supported formats.
* TopBraid Enterprise Vocabulary Net (EVN) &lt;ref&gt;[http://www.topquadrant.com/solutions/ent_vocab_net.html TopBraid EVN]&lt;/ref&gt; is a web-based solution for simplified development and management of interconnected controlled vocabularies. It supports collaboration on defining and linking enterprise vocabularies, taxonomies, thesauri and ontologies used for information integration, customization and search.
* [http://www.dataharmony.com/products/thesaurus_master.html Thesaurus Master], for creating, developing, and maintaining taxonomies and thesauri, is part of Access Innovations' [http://www.dataharmony.com/ Data Harmony] knowledge management software line. It offers SKOS-compliant export.
* [http://www.cognitum.eu/semantics/FluentEditor/ Fluent Editor 2014] - an ontology editor which allows to work and edit directly OWL annotations and SKOS. Annotations will processed also for referenced ontologies as well as imported/exported to OWL/RDF and can be processed on the server.
* [https://trial.smartlogic.com/S4Trials/ Smartlogic Semaphore Ontology Editor] - a SKOS and SKOS-XL based ontology editor which allows creating models based strictly on the SKOS standards.

== Data ==
There are publicly available SKOS data sources.
* SKOS Datasets wiki&lt;ref&gt;[http://www.w3.org/2001/sw/wiki/SKOS/Datasets SKOS/Datasets]&lt;/ref&gt; The W3C recommends using this list of publicly available SKOS data sources. Most data found in this wiki can be used for commercial and research applications.

== Relationships with other standards ==

=== Metamodel ===
The SKOS metamodel is broadly compatible with the data model of [[ISO 25964-1]] - Thesauri for Information Retrieval. This data model can be viewed and downloaded from the website for [[ISO 25964]].&lt;ref name="niso.org"&gt;[http://www.niso.org/schemas/iso25964 ''ISO 25964 &#8211; the international standard for thesauri and interoperability with other vocabularies'']&lt;/ref&gt;
[[File:Skos metamodel.png|thumb|alt=Alt text|Semantic model of the information elements of SKOS]]

=== SKOS and thesaurus standards ===
SKOS development has involved experts from both RDF and library community, and SKOS intends to allow easy migration of thesauri defined by standards such as [[NISO]] Z39.19 - 2005&lt;ref&gt;[http://www.niso.org/standards/ NISO Standards] Z39.19 - 2005 : Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies&lt;/ref&gt; or [[ISO 25964]].&lt;ref name="niso.org"/&gt;

=== SKOS and other semantic web standards ===
SKOS is intended to provide a way to make a legacy of concept schemes available to Semantic Web applications, simpler than the more complex ontology language, [[Web Ontology Language|OWL]]. OWL is intended to express complex conceptual structures, which can be used to generate rich metadata and support inference tools. However, constructing useful web ontologies is demanding in terms of expertise, effort, and cost. In many cases, this type of effort might be superfluous or unsuited to requirements, and SKOS might be a better choice. The extensibility of RDF makes possible further incorporation or extension of SKOS vocabularies into more complex vocabularies, including OWL ontologies.

== See also ==
* [[Glossary]]
* [[Knowledge representation]]
* [[Metadata registry]]

== References ==
{{Reflist|2}}

==External links==
* [http://www.w3.org/TR/skos-reference/ SKOS Simple Knowledge Organization System Reference]
* [http://www.w3.org/2004/02/skos/ W3C SKOS Home Page]
* [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818 W3C Simple Knowledge Organization System Primer]
* [http://www.idealliance.org/proceedings/xtech05/papers/03-04-01/ Presentation of SKOS at XTech 2005 Conference]
* [http://www.w3.org/News/2009#item35 W3C Invites Implementations of SKOS (Simple Knowledge Organization System) Reference; Primer Also Published]
* [http://demo.semantic-web.at:8080/SkosServices/index SKOS Validator and Zthes Converter]

{{Semantic Web}}
{{W3C standards}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:School of Computer Science, University of Manchester]]</text>
      <sha1>stpq50op4c92g15mz2xumixsctvk4n5</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic interoperability</title>
    <ns>0</ns>
    <id>7233280</id>
    <revision>
      <id>746132961</id>
      <parentid>739686551</parentid>
      <timestamp>2016-10-25T12:51:59Z</timestamp>
      <contributor>
        <username>Padawan ch</username>
        <id>2036094</id>
      </contributor>
      <minor />
      <comment>Add link to "Syntax"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16024" xml:space="preserve">{{multiple issues|
{{underlinked|date=January 2013}}
{{more footnotes|date=February 2011}}
}}

'''Semantic interoperability''' is the ability of [[computer]] systems to exchange [[data]] with unambiguous, shared meaning. [[Semantic]] interoperability is a requirement to enable machine computable logic, inferencing, knowledge discovery, and data federation between information systems.&lt;ref&gt;NCOIC, [https://www.ncoic.org/technology/deliverables/scope/ "SCOPE"], [https://www.ncoic.org/home ''Network Centric Operations Industry Consortium''], 2008&lt;/ref&gt;

Semantic interoperability is therefore concerned not just with the packaging of data ([[syntax]]), but the simultaneous transmission of the meaning with the data (semantics).  This is accomplished by adding data about the data (metadata), linking each data element to a controlled, shared vocabulary.  The meaning of the data is transmitted with the data itself, in one self-describing "information package" that is independent of any information system.  It is this shared vocabulary, and its associated links to an ontology, which provides the foundation and capability of machine interpretation, inferencing, and logic.

Syntactic interoperability is a prerequisite for semantic interoperability.  Syntactic interoperability refers to the packaging and transmission mechanisms for data.  In healthcare, HL7 has been in use for over thirty years (which predates the internet and web technology), and uses the pipe character (|) as a data delimiter. The current internet standard for document markup is XML, which uses "&lt; &gt;" as a data delimiter.  The data delimiters convey no meaning to the data other than to structure the data.  Without a data dictionary to translate the contents of the delimiters, the data remains meaningless.  While there are many attempts at creating data dictionaries and information models to associate with these data packaging mechanisms, none have been practical to implement.  This has only perpetuated the ongoing "babelization" of data and inability to exchange of data with meaning.

Since the introduction of the Semantic Web concept by [[Tim Berners-Lee]] in 1999,&lt;ref&gt;{{cite book |last=Berners-Lee |first=Tim |authorlink=Tim Berners-Lee |author2=Fischetti, Mark |title=[[Tim Berners Lee#Weaving the Web|Weaving the Web]] |publisher=[[HarperSanFrancisco]] |year=1999 |pages=chapter 12 |isbn=978-0-06-251587-2 |nopp=true }}&lt;/ref&gt; there has been growing interest and application of the W3C (World Wide Web Consortium, [[WWWC]]) standards to provide web-scale semantic data exchange, federation, and inferencing capabilities.

== Semantic as a function of syntactic interoperability ==

Syntactic interoperability, provided by for instance [[XML]] or the [[SQL]] standards, is a pre-requisite to semantic.  It involves a common data format and common protocol to structure any data so that the manner of processing the information will be interpretable from the structure.  It also allows detection of syntactic errors, thus allowing receiving systems to request resending of any message that appears to be garbled or incomplete.  No semantic communication is possible if the syntax is garbled or unable to represent the data.  However, information represented in one syntax may in some cases be accurately translated into a different syntax.  Where accurate translation of syntaxes is possible, systems using different syntaxes may also interoperate accurately.  In some cases the ability to accurately translate information among systems using different syntaxes may be limited to one direction, when the formalisms used have different levels of ''expressivity'' (ability to express information).

A single ontology containing representations of every term used in every application is generally considered impossible, because of the rapid creation of new terms or assignments of new meanings to old terms.  However, though it is impossible to anticipate ''every'' concept that a user may wish to represent in a computer, there is the possibility of finding some finite set of "primitive" concept representations that can be combined to create any of the more specific concepts that users may need for any given set of applications or ontologies.  Having a foundation ontology (also called ''[[upper ontology]]'') that contains all those primitive elements would provide a sound basis for general semantic interoperability, and allow users to define any new terms they need by using the basic inventory of ontology elements, and still have those newly defined terms properly interpreted by any other computer system that can interpret the basic foundation ontology.  Whether the number of such primitive concept representations is in fact finite, or will expand indefinitely, is a question under active investigation.  If it is finite, then a stable foundation ontology suitable to support accurate and general semantic interoperability can evolve after some initial foundation ontology has been tested and used by a wide variety of users.  At the present time, no foundation ontology has been adopted by a wide community, so such a stable foundation ontology is still in the future.

== Words and Meanings ==

One persistent misunderstanding recurs in discussion of semantics - the confusion of words and meanings.  The meanings of words change, sometimes rapidly. But a formal language such as used in an ontology can encode the meanings (semantics) of concepts in a form that does not change.  In order to determine what is the meaning of a particular word (or term in a database, for example) it is necessary to label each fixed concept representation in an ontology with the word(s) or term(s) that may refer to that concept.  When multiple words refer to the same (fixed) concept, in language this is called synonymy; when one word is used to refer to more than one concept, that is called ambiguity.  Ambiguity and synonymy are among the factors that make computer understanding of language very difficult.  The use of words to refer to concepts (the meanings of the words used)is very sensitive to the context and the purpose of any use for many human-readable terms.  The use of ontologies in supporting semantic interoperability is to provide a fixed set of concepts whose meanings and relations are stable and can be agreed to by users.  The task of determining which terms in which contexts (each database is a different context) then is separated from the task of creating the ontology, and must be taken up by the designer of a database, or the designer of a form for data entry, or the developer of a program for language understanding.  When a word used in some interoperability context changes its meaning, then to preserve interoperability it is necessary to change the pointer to the ontology element(s) that specifies the meaning of that word.

== Knowledge representation requirements and languages ==

A knowledge representation language may be sufficiently expressive to describe nuances of meaning in well understood fields.  There are at least five levels of complexity of these{{specify|date=June 2014}}.

For general [[semi-structured data]] one may use a general purpose language such as XML.&lt;ref&gt;[http://www.cs.umd.edu/projects/plus/SHOE/pubs/extreme2000.pdf XML as a tool for Semantic Interoperability] Semantic Interoperability on the Web, Jeff Heflin and James Hendler&lt;/ref&gt;

Languages with the full power of first-order predicate logic may be required for many tasks.

Human languages are highly expressive, but are considered too ambiguous to allow the accurate interpretation desired, given the current level of human language technology.   In human languages the same word may be used to refer to different concepts (ambiguity), and the same concept may be referred to by different words (synonymy).

== Prior agreement not required ==
{{confusing|section|date=February 2016}}

Semantic interoperability may be distinguished from other forms of interoperability by considering whether the information transferred has, in its communicated form, all of the meaning required for the receiving system to interpret it correctly, even when the algorithms used by the receiving system are unknown to the sending system.  Consider sending one number:

If that number is intended to be the sum of money owed by one company to another, it implies some action or lack of action on the part of both those who send it and those who receive it.

It may be correctly interpreted if sent in response to a specific request, and received at the time and in the form expected.  This correct interpretation does not depend only on the number itself, which could represent almost any of millions of types of quantitative measure, rather it depends strictly on the circumstances of transmission.  That is, the interpretation depends on both systems expecting that the algorithms in the other system use the number in exactly the same sense, and it depends further on the entire envelope of transmissions that preceded the actual transmission of the bare number.  By contrast, if the transmitting system does not know how the information will be used by other systems, it is necessary to have a shared agreement on how information with some specific meaning (out of many possible meanings) will appear in a communication.  For a particular task, one solution is to standardize a form, such as a request for payment; that request would have to encode, in standardized fashion, all of the information needed to evaluate it, such as: the agent owing the money, the agent owed the money, the nature of the action giving rise to the debt, the agents, goods, services, and other participants in that action; the time of the action; the amount owed and currency in which the debt is reckoned; the time allowed for payment; the form of payment demanded; and other information.  When two or more systems have agreed on how to interpret the information in such a request, they can achieve semantic interoperability ''for that specific type of transaction''.  For semantic interoperability generally, it is necessary to provide standardized ways to describe the meanings of many more things than just commercial transactions, and the number of concepts whose representation needs to be agreed upon are at a minimum several thousand.

== Ontology research ==

How to achieve semantic interoperability for more than a few restricted scenarios is currently a matter of research and discussion.  For the problem of General Semantic Interoperability, some form of foundation ontology ('[[upper ontology]]') is required that is sufficiently comprehensive to provide the defining concepts for more specialized ontologies in multiple domains.  Over the past decade more than ten foundation ontologies have been developed, but none have as yet been adopted by a wide user base.

The need for a single comprehensive all-inclusive ontology to support Semantic Interoperability can be avoided by designing the common foundation ontology as a set of basic ("primitive") concepts that can be combined to create the logical descriptions of the meanings of terms used in local domain ontologies or local databases.  This tactic is based on the principle that:

'''If:'''
&lt;pre style="white-space:pre-wrap;"&gt;
(1) the meanings and usage of the primitive ontology elements in the foundation ontology are agreed on, and 
(2) the ontology elements in the  domain ontologies are constructed as logical
combinations of the elements in the foundation ontology,
&lt;/pre&gt;
'''Then:'''
&lt;pre style="white-space:pre-wrap;"&gt;
The intended meanings of the domain ontology elements can be computed automatically using an FOL reasoner, by any system that accepts the meanings of the elements in the foundation ontology, and has both the foundation ontology and the logical specifications of the elements in the domain ontology.
&lt;/pre&gt;
'''Therefore:'''
&lt;pre style="white-space:pre-wrap;"&gt;
Any system wishing to interoperate accurately with another system need transmit only the data to be communicated, plus any logical descriptions of terms used in that data that were created locally and are not already in the common foundation ontology.
&lt;/pre&gt;

This tactic then limits the need for prior agreement on meanings to only those ontology elements in the common Foundation Ontology (FO).  Based on several considerations, this is likely to be fewer than 10,000 elements (types and relations).

In practice, together with the FO focused on representations of the primitive concepts, a set of domain extension ontologies to the FO with elements specified using the FO elements will likely also be used.  Such pre-existing extensions will ease the cost of creating domain ontologies by providing existing elements with the intended meaning, and will reduce the chance of error by using elements that have already been tested.  Domain extension ontologies may be logically inconsistent with each other, and that needs to be determined if different domain extensions are used in any communication.

Whether use of such a single foundation ontology can itself be avoided by sophisticated mapping techniques among independently developed ontologies is also under investigation.

== Importance==

The practical significance of semantic interoperability has been measured by several studies that estimate the cost (in lost efficiency) due to lack of semantic interoperability.  One study,&lt;ref&gt;[http://content.healthaffairs.org/cgi/content/full/hlthaff.w5.10/DC1 Jan Walker, Eric Pan, Douglas Johnston, Julia Adler-Milstein, David W. Bates and Blackford Middleton, ''The Value of Healthcare Information Exchange and Interoperability'' Health Affairs, 19 January 2005]&lt;/ref&gt; focusing on the lost efficiency in the communication of healthcare information, estimated that US$77.8 billion per year could be saved by implementing an effective interoperability standard in that area.  Other studies, of the construction industry&lt;ref&gt;[http://www.bfrl.nist.gov/oae/publications/gcrs/04867.pdf Microsoft Word - 08657 Final Rpt_8-2-04.doc&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and of the automobile manufacturing supply chain,&lt;ref&gt;http://www.nist.gov/director/prog-ofc/report99-1.pdf&lt;/ref&gt; estimate costs of over US$10 billion per year due to lack of semantic interoperability in those industries.  In total these numbers can be extrapolated to indicate that well over US$100 billion per year is lost because of the lack of a widely used semantic interoperability standard in the US alone.

There has not yet been a study about each policy field that might offer big cost savings applying semantic interoperability standards. But to see which policy fields are capable of profiting from semantic interoperability see '[[Interoperability]]' in general. Such policy fields are [[eGovernment]], health, security and many more. The EU also set up the [[Semantic Interoperability Centre Europe]] in June 2007.

==See also==
*[[Interoperability]], Interoperability generally
*[[Semantic Computing]]
*[[Upper ontology (computer science)]], Discussion of using an ''upper ontology''.
*[[Conceptual interoperability|Levels of Conceptual Interoperability]], A discussion describing an interoperability spectrum in the context of exchange of Modeling and Simulation information, in which ''semantic interoperability '' is not defined as fully independent of context, as described here.
*[[UDEF]], Universal Data Element Framework

==External links==
*[http://colab.cim3.net/cgi-bin/wiki.pl?OntologyTaxonomyCoordinatingWG/OntacGlossary the ONTACWG Glossary Other definitions of Semantic Interoperability]
*[http://marinemetadata.org/guides/vocabs/cvchooseimplement/cvsemint MMI Guide: Achieving Semantic Interoperability]

==References==
{{reflist|2}}

[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Computing terminology]]
[[Category:Telecommunication theory]]
[[Category:Interoperability]]</text>
      <sha1>jucsfxeolzoubx60bjddy4izlrttf7u</sha1>
    </revision>
  </page>
  <page>
    <title>Futures wheel</title>
    <ns>0</ns>
    <id>8612764</id>
    <revision>
      <id>738482537</id>
      <parentid>705943857</parentid>
      <timestamp>2016-09-09T06:09:27Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2866" xml:space="preserve">[[Image:Futures wheel 01.svg|thumb|right|250px|A futures wheel as described by Jerome C. Glenn.]]
'''The Futures wheel''' is a method for graphical [[visualization (graphic)|visualisation]] of direct and indirect [[future]] '''consequences''' of a particular change or development. It was invented by [[Jerome C. Glenn]] in 1971, when he was a [[student]] at the Antioch Graduate School of Education (now [[Antioch University New England]]).
&lt;blockquote&gt;The Futures Wheel is a way of organizing thinking and questioning about the future &#8211; a kind of structured brainstorming. (Jerome C. Glenn (1994) The Futures Wheel)&lt;/blockquote&gt;

==Description==

To start a Futures wheel the central [[terminology|term]] describing the change to evaluate is positioned in the center of the page (or drawing area). Then, events or consequences following directly from that development are positioned around it. Next, the (indirect) consequences of the direct consequences are positioned around the first level consequences. The terms may be connected as nodes in a tree (or even a web). The levels will often be marked by concentric circles.

==Usage==

The Futures wheel is usually used to organize [[thought]]s about a future development or trend. With it, possible impacts can be collected and put down in a structured way. The use of interconnecting lines makes it possible to visualize interrelationships of the causes and resulting changes. Thus, Futures wheels can assist in developing multi-concepts about possible future development by offering a futures-conscious perspective and aiding in group [[brainstorming]].

==See also==

* [[Mind Mapping]]

==Bibliography==

* Glenn, Jerome C. ''Futurizing Teaching vs Futures Course'', Social Science Record, Syracuse University, Volume IX, No. 3 Spring 1972.
* Snyder, David Pearce. Monograph: ''The Futures Wheel: A Strategic Thinking Exercise'', The Snyder Family Enterprise, Bethesda, Maryland 1993.
* Glenn, Jerome C. ''Futures Wheel'', Futures Research Methodology Version 3.0, The Millennium Project, Washington, D.C. 2009.

==External links==
* [https://web.archive.org/web/20080612175450/http://www.ltag.education.tas.gov.au/glossary.htm Learning, Teaching and Assessment Guide Glossary] at Tasmania's [[Department of Education (Tasmania)|Department of Education]]'s homepage.
* Downloadable template of a [https://web.archive.org/web/20070927143447/http://www.globaleducation.edna.edu.au/globaled/go/cache/offonce/pid/1835;jsessionid=050A14CB101EAF863AE979C80461FCB3 Futures wheel] at the [[Australia]]n [http://www.globaleducation.edna.edu.au/ Global Education] website.
* Futures Wheel, Futures Research Methodology Version 3.0, The Millennium Project, Washington, DC 2009 [http://millennium-project.org/millennium/FRM-V3.html] 

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Futurology]]</text>
      <sha1>bomytieprjmu7nvh48m8btwcbbog9iq</sha1>
    </revision>
  </page>
  <page>
    <title>Fuzzy cognitive map</title>
    <ns>0</ns>
    <id>11270885</id>
    <revision>
      <id>759295748</id>
      <parentid>759295093</parentid>
      <timestamp>2017-01-10T09:47:22Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>/* Details */ lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14903" xml:space="preserve">[[File:FCMdrug520.png|thumb|right|Rod Tabers FCM depicting eleven factors of the American drug market]]
A '''fuzzy cognitive map''' is a [[cognitive map]] within which the relations between the elements (e.g. concepts, events, project resources) of a "mental landscape" can be used to compute the "strength of impact" of these elements.  Fuzzy cognitive maps were introduced by [[Bart Kosko]].&lt;ref&gt;{{cite journal|author=[[Bart Kosko]]|title=''Fuzzy Cognitive Maps''|journal=International Journal of Man-Machine Studies|volume=24|date=1986|pages=65-75|url=http://sipi.usc.edu/~kosko/FCM.pdf|format=PDF}}&lt;/ref&gt;&lt;ref&gt;[http://sipi.usc.edu/~kosko/Virtual_Worlds_FCM.pdf] {{dead link|date=January 2017}}&lt;/ref&gt;  Ron Axelord introduced Cognitive Maps as a formal way of representing social scientific knowledge and modeling [[decision making]] in social and political systems. Then brought in the computation [[fuzzy logic]].

==Details==
Fuzzy cognitive maps are signed fuzzy [[directed graph|digraph]]s.  They may look at [[first blush]] like [[Hasse diagrams]] but they are not.
[[Spreadsheet]]s or tables are used to map FCMs into [[matrix (Mathematics)|matric]]es for further computation.&lt;ref&gt;{{cite web|url=http://www.FCMappers.net/joomla/index.php?option=com_content&amp;view=article&amp;id=52&amp;Itemid=53 |title=FCMapper - our Fuzzy Cognitive Mapping Software Solution |website=Fcmappers.net |date=2016-01-27 |accessdate=2017-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.ochoadeaspuru.com/fuzcogmap/index.php |title=Fuzzy Cognitive Maps |website=Ochoadeaspuru.com |date= |accessdate=2017-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://jfcm.megadix.it/ |title=JFCM - Java Fuzzy Cognitive Maps |website=Jfcm.megadix.it |date= |accessdate=2017-01-09}}&lt;/ref&gt;
FCM is a technique used for causal knowledge acquisition and representation, it supports causal knowledge reasoning process and belong to the neuro-fuzzy system that aim at solving decision making problems, modeling and simulate [[complex system]]s. 
Learning algorithms  have been proposed for training and updating FCMs weights mostly based on ideas coming from the field of [[Artificial Neural Network]]s. Adaptation and learning methodologies used to adapt the FCM model and adjust its weights.  Kosko and Dickerson (Dickerson &amp; Kosko, 1994) suggested the Differential [[Hebbian Learning]] (DHL) to train FCM.&lt;ref&gt;{{cite web|url=http://home.eng.iastate.edu/~julied/publications/FCM96.pdf |title=IEEEBook8.dvi |website=Home.eng.iastate.edu |format=PDF |date= |accessdate=2017-01-09}}&lt;/ref&gt; There have been proposed algorithms based on the initial Hebbian algorithm;&lt;ref&gt;{{cite journal |doi=10.1016/j.ijar.2004.01.001 |title=Active Hebbian learning algorithm to train fuzzy cognitive maps |journal=International Journal of Approximate Reasoning |volume=37 |issue=3 |page=219 |year=2004 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}&lt;/ref&gt; others algorithms come from the field of [[genetic algorithms]], [[swarm intelligence]]&lt;ref&gt;{{cite journal |doi=10.1007/s10844-005-0864-9 |title=Fuzzy Cognitive Maps Learning Using Particle Swarm Optimization |journal=Journal of Intelligent Information Systems |volume=25 |page=95 |year=2005 |last1=Papageorgiou |first1=Elpiniki I. |last2=Parsopoulos |first2=Konstantinos E. |last3=Stylios |first3=Chrysostomos S. |last4=Groumpos |first4=Petros P. |last5=Vrahatis |first5=Michael N. }}&lt;/ref&gt; and  [[evolutionary computation]].&lt;ref&gt;{{cite book |doi=10.1109/FUZZY.2005.1452465 |chapter=Evolutionary Development of Fuzzy Cognitive Maps |title=The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05 |pages=619&#8211; |year=2005 |last1=Stach |first1=W. |last2=Kurgan |first2=L. |last3=Pedrycz |first3=W. |last4=Reformat |first4=M. |isbn=0-7803-9159-4 }}&lt;/ref&gt; [[Learning algorithms]] are used to overcome the shortcomings that the traditional FCM present i.e. decreasing the human intervention by suggested automated FCM candidates; or by activating only the most relevant concepts every execution time; or by making models more transparent and dynamic.&lt;ref&gt;{{cite journal |doi=10.1016/j.ijhcs.2006.02.009 |title=Unsupervised learning techniques for fine-tuning fuzzy cognitive map causal links |journal=International Journal of Human-Computer Studies |volume=64 |issue=8 |page=727 |year=2006 |last1=Papageorgiou |first1=Elpiniki I. |last2=Stylios |first2=Chrysostomos |last3=Groumpos |first3=Peter P. }}&lt;/ref&gt;

Fuzzy cognitive maps (FCMs) have gained considerable research interest due to their ability in representing structured knowledge and model complex systems in various fields. This growing interest led to the need for enhancement and making more reliable models that can better represent real situations.
A first simple application of FCMs is described in a book&lt;ref name="confusion"&gt;William R. Taylor: ''[http://www.americanconfusion.com/?p=122 Lethal American Confusion] (How Bush and the Pacifists Each Failed in the War on Terrorism)'', 2006, ISBN 0-595-40655-6 (FCM application in chapter 14) {{webarchive |url=https://web.archive.org/web/20070930103802/http://www.americanconfusion.com/?p=122 |date=September 30, 2007 }}&lt;/ref&gt; of William R. Taylor, where the war in Afghanistan and Iraq is analyzed. And in [[Bart Kosko]]'s book ''Fuzzy Thinking'',&lt;ref name="FuzzyThinking"&gt;Bart Kosko: ''Fuzzy Thinking'', 1993/1995, ISBN 0-7868-8021-X (Chapter 12: Adaptive Fuzzy Systems)''&lt;/ref&gt; several Hasse diagrams illustrate the use of FCMs. As an example, one FCM quoted from Rod Taber&lt;ref name="Drugs"&gt;Rod Taber: ''Knowledge Processing with Fuzzy Cognitive Maps'', Expert Systems with Applications, vol. 2, no. 1, 83-87, 1991 ([[:de:Bild:FCMdrug520.png|Hasse diagram]] in German Wikipedia)&lt;/ref&gt; describes 11 factors of the American cocaine market and the relations between these factors. For computations, Taylor uses pentavalent logic (scalar values out of {-1,-0.5,0,+0.5,+1}). That particular map of Taber uses [[trivalent logic]] (scalar values out of {-1,0,+1}). Taber et al.  also illustrate the dynamics of map fusion and give a theorem on the convergence of combination in a related article &lt;ref name='Medical'&gt;{{cite journal |doi=10.1002/int.20185 |title=Quantization effects on the equilibrium behavior of combined fuzzy cognitive maps |journal=International Journal of Intelligent Systems |volume=22 |issue=2 |page=181 |year=2007 |last1=Taber |first1=Rod |last2=Yager |first2=Ronald R. |last3=Helgason |first3=Cathy M. }}&lt;/ref&gt;

While applications in social sciences&lt;ref name="confusion"/&gt;&lt;ref name="FuzzyThinking"/&gt;&lt;ref name="Drugs"/&gt;&lt;ref&gt;Costas Neocleous, Christos Schizas, Costas Yenethlis: ''[http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf Fuzzy Cognitive Models in Studying Political Dynamics  - The case of the Cyprus problem]'' {{webarchive |url=https://web.archive.org/web/20070929055849/http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf |date=September 29, 2007 }}&lt;/ref&gt; introduced FCMs to the public, they are used in a much wider range of applications, which all have to deal with creating and using models&lt;ref&gt;Chrysostomos D. Stylios, Voula C. Georgopoulos, Peter P. Groumpos: ''[http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF The Use of Fuzzy Cognitive Maps in Modeling Systems]'' {{webarchive |url=https://web.archive.org/web/20110720011915/http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF |date=July 20, 2011 }}&lt;/ref&gt; of uncertainty and complex processes and systems. Examples:
*In business FCMs can be used for product planning.&lt;ref&gt;Antonie Jetter: ''Produktplanung im Fuzzy Front End'', 2005, ISBN 3-8350-0144-2&lt;/ref&gt;
*In economics, FCMs support the use of [[game theory]] in more complex settings.&lt;ref&gt;Vesa A. Niskanen: ''[http://www.ijicic.org/fic04-20.pdf Application of Fuzzy Linguistic Cognitive Maps to Prisoner's Dilemma]'', 2005, ICIC International pp. 139-152, ISSN 1349-4198 {{webarchive |url=https://web.archive.org/web/20070929040145/http://www.ijicic.org/fic04-20.pdf |date=September 29, 2007 }}&lt;/ref&gt;
* In Medical applications to model systems, provide diagnosis,&lt;ref&gt;{{cite journal |doi=10.1016/S0933-3657(02)00076-3 |pmid=14656490 |title=A fuzzy cognitive map approach to differential diagnosis of specific language impairment |journal=Artificial Intelligence in Medicine |volume=29 |issue=3 |pages=261&#8211;78 |year=2003 |last1=Georgopoulos |first1=Voula C |last2=Malandraki |first2=Georgia A |last3=Stylios |first3=Chrysostomos D }}&lt;/ref&gt; develop [[decision support systems]]&lt;ref&gt;{{cite journal |doi=10.1109/TBME.2003.819845 |pmid=14656062 |title=An integrated two-level hierarchical system for decision making in radiation therapy based on fuzzy cognitive maps |journal=IEEE Transactions on Biomedical Engineering |volume=50 |issue=12 |pages=1326&#8211;39 |year=2003 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}&lt;/ref&gt; and [[medical assessment]].&lt;ref&gt;{{cite book |doi=10.1007/978-3-319-11457-6_18 |chapter=Supervisory Fuzzy Cognitive Map Structure for Triage Assessment and Decision Support in the Emergency Department |title=Simulation and Modeling Methodologies, Technologies and Applications |volume=319 |pages=255&#8211;69 |series=Advances in Intelligent Systems and Computing |year=2015 |last1=Georgopoulos |first1=Voula C. |last2=Stylios |first2=Chrysostomos D. |isbn=978-3-319-11456-9 }}&lt;/ref&gt;
* In Engineering for [[process modeling|modeling]] and [[Process control|control]]&lt;ref&gt;{{cite web|url=http://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs089?resultNumber=0&amp;totalResults=11&amp;start=0&amp;q=stylios&amp;resultsPageSize=10&amp;rows=10 |title=Fuzzy Cognitive Maps in modeling supervisory control systems - IOS Press |website=Content.iospress.com |date= |accessdate=2017-01-09}}&lt;/ref&gt; mainly of complex systems&lt;ref&gt;{{cite journal |doi=10.1109/TSMCA.2003.818878 |title=Modeling Complex Systems Using Fuzzy Cognitive Maps |journal=IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans |volume=34 |page=155 |year=2004 |last1=Stylios |first1=C.D. |last2=Groumpos |first2=P.P. }}&lt;/ref&gt;
*In project planning FCMs help to analyze the mutual dependencies between project resources.
*In robotics&lt;ref name="FuzzyThinking"/&gt;&lt;ref&gt;Marc B&#246;hlen: ''[http://www.realtechsupport.org/pdf/SpaceRobotics2000.pdf More Robots in Cages]'',&lt;/ref&gt; FCMs support machines to develop fuzzy models of their environments and to use these models to make crisp decisions.
*In computer assisted learning FCMs enable computers to check whether students understand their lessons.&lt;ref&gt;Benjoe A. Juliano, Wylis Bandler: ''Tracing Chains-of-Thought (Fuzzy Methods in Cognitive Diagnosis)'', Physica-Verlag Heidelberg 1996, ISBN 3-7908-0922-5&lt;/ref&gt;
*In [[expert system]]s&lt;ref name="Drugs"/&gt; a few or many FCMs can be aggregated into one FCM in order to process estimates of knowledgeable persons.&lt;ref&gt;W. B. Vasantha Kandasamy, Florentin Smarandache: ''[http://www.gallup.unm.edu/~smarandache/NCMs.pdf Fuzzy Cognitive Maps and Neutrosophic Cognitive Maps]'', 2003, ISBN 1-931233-76-4&lt;/ref&gt;
*In IT project management, a FCM-based methodology helps to success modelling.&lt;ref&gt;{{cite journal |doi=10.1016/j.eswa.2006.01.032 |title=Modelling IT projects success with Fuzzy Cognitive Maps |journal=Expert Systems with Applications |volume=32 |issue=2 |page=543 |year=2007 |last1=Rodriguez-Repiso |first1=Luis |last2=Setchi |first2=Rossitza |last3=Salmeron |first3=Jose L. }}&lt;/ref&gt;

FCMappers&lt;ref&gt;FCMappers - international community for fuzzy cognitive mapping: http://www.FCMappers.net/&lt;/ref&gt; - an international online community for the analysis and the visualization of fuzzy cognitive maps offer support for starting with FCM and also provide an MS-Excel-based tool that is able to check and analyse FCMs. The output is saved as [[Pajek]] file and can be visualized within 3rd party software like Pajek, Visone,... . They also offer to adapt the software to specific research needs. On their webpage you also will find a linklist for interesting scientific articles, related software, institutes, people and projects. The FCMappers have about one thousand registered members worldwide.

Additional FCM software tools, such as Mental Modeler,&lt;ref&gt;{{cite book |doi=10.1109/HICSS.2013.399 |chapter=Mental Modeler: A Fuzzy-Logic Cognitive Mapping Modeling Tool for Adaptive Environmental Management |title=2013 46th Hawaii International Conference on System Sciences |pages=965&#8211; |year=2013 |last1=Gray |first1=Steven A. |last2=Gray |first2=Stefan |last3=Cox |first3=Linda J. |last4=Henly-Shepard |first4=Sarah |isbn=978-1-4673-5933-7 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.mentalmodeler.com/ |title=Fuzzy Logic Cognitive Mapping |publisher=Mental Modeler |date= |accessdate=2017-01-09}}&lt;/ref&gt; have recently been developed as a decision-support tool for use in [[social science]] research, [[collaborative decision-making]], and [[Natural resource management|natural resource planning]].

==Bipolar Fuzzy Cognitive Maps==
Fuzzy cognitive maps have been further extended to bipolar fuzzy cognitive maps based on bipolar fuzzy sets &lt;ref&gt;Wen-Ran Zhang, 1998, (Yin)(Yang) Bipolar Fuzzy Sets. Proceedings of IEEE World Congress on Computational Intelligence &#8211; Fuzz-IEEE, Anchorage, AK, 835-840 &lt;/ref&gt; and bipolar cognitive mapping.&lt;ref&gt;{{cite journal |doi=10.1109/21.24529 |title=Pool2: A generic system for cognitive map development and decision analysis |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=19 |page=31 |year=1989 |last1=Zhang |first1=W.R. |last2=Chen |first2=S.S. |last3=Bezdek |first3=J.C. }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |doi=10.1109/21.141315 |title=A cognitive-map-based approach to the coordination of distributed cooperative agents |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=22 |page=103 |year=1992 |last1=Zhang |first1=W.-R. |last2=Chen |first2=S.-S. |last3=Wang |first3=W. |last4=King |first4=R.S. }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |doi=10.1109/TSMCB.2003.810444 |title=Equilibrium relations and bipolar cognitive mapping for online analytical processing with applications in international relations and strategic decision support |journal=IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) |volume=33 |issue=2 |page=295 |year=2003 |last1=Wen-Ran Zhang }}&lt;/ref&gt;&lt;ref&gt;Wen-Ran Zhang,  2003b, Equilibrium Energy and Stability Measures for Bipolar Decision and Global Regulation. Int&#8217;l J. of Fuzzy Sys. Vol. 5, No. 2, 2003, 114-122&lt;/ref&gt; Bipolar fuzzy set theory as an equilibrium-based extension to fuzzy sets is recognized by [[L. A. Zadeh]]. &lt;ref&gt;L. A. Zadeh, 2008, Fuzzy logic. Scholarpedia, 3(3):1766, Created: 10 July 2006, reviewed: 27 March 2007, accepted: 31 March 2008.&lt;/ref&gt;

==See also==
[[Soft Computing]]   

==References==
{{Reflist|30em}}

{{Commons category|Cognitive maps}}

[[Category:Knowledge representation]]
[[Category:Fuzzy logic]]</text>
      <sha1>5gwyovw8cc74i8e1qagkgd27j5hu1at</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Grouping</title>
    <ns>14</ns>
    <id>11284359</id>
    <revision>
      <id>131699395</id>
      <timestamp>2007-05-18T03:10:28Z</timestamp>
      <contributor>
        <username>Grumpyyoungman01</username>
        <id>846078</id>
      </contributor>
      <comment>[[WP:AES|&#8592;]]Created page with '[[Category:Knowledge representation]]'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37" xml:space="preserve">[[Category:Knowledge representation]]</text>
      <sha1>c7ub5uwhswlhnrrfyh07qu4qoi2x1xv</sha1>
    </revision>
  </page>
  <page>
    <title>Historical Thesaurus of the Oxford English Dictionary</title>
    <ns>0</ns>
    <id>12612212</id>
    <revision>
      <id>752664885</id>
      <parentid>747479544</parentid>
      <timestamp>2016-12-02T15:51:31Z</timestamp>
      <contributor>
        <username>Marcalexander</username>
        <id>29801393</id>
      </contributor>
      <comment>Update logo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7272" xml:space="preserve">{{italic title}}
{{Infobox website
| name            = ''Historical Thesaurus of English''
| logo            = Historical Thesaurus of English logo.png
| logo_size       = &lt;!-- default is 250px --&gt;
| logo_alt        =
| url             = {{URL|www.glasgow.ac.uk/thesaurus}}
| commercial      = No
| type            = Academic
| registration    = None
| content_licence = Free for personal and non-commercial research&lt;ref name=hte-using&gt;{{cite web|title=Using ''Historical Thesaurus'' Data|url=http://historicalthesaurus.arts.gla.ac.uk/using-data/|website=The Historical Thesaurus of English|publisher=University of Glasgow|accessdate=25 October 2014}}&lt;/ref&gt;
| programming_language = 
| owner           = [[University of Glasgow]]
| author          = Marc Alexander and Christian Kay&lt;ref name=hte-cite&gt;{{cite web|last1=Alexander|first1=Marc|last2=Kay|first2=Christian|title=How to Cite|url=http://historicalthesaurus.arts.gla.ac.uk/how-to-cite/|website=The Historical Thesaurus of English, version 4.2|publisher=University of Glasgow|accessdate=25 October 2014}}&lt;/ref&gt;
| editor          = [[Christian Kay]], Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], Iren&#233; Wotherspoon, and Marc Alexander (editors)
| current_status  = Version 4.2, since September, 2014&lt;ref name=hte-versions /&gt;
| footnotes       = 
}}
{{Infobox book
|name           = Historical Thesaurus of the Oxford English Dictionary : with additional material from "A Thesaurus of Old English"
|image          = Historical Thesaurus.jpg
|caption        = Print edition of version 1.0 of the ''Historical Thesaurus of English''&lt;ref name="hte-versions" /&gt;
|alt            = Printed boxed set
|author         = Christian Kay, Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], and Iren&#233; Wotherspoon (editors)
|title_working  = Historical Thesaurus of English
|country        = Great Britain
|language       = English
|subject        = [[History of the English language]]
|genre          = [[Thesaurus|Thesauri]] 
|published      = 2009 ([[Oxford University Press]])
|pages          = 4,448
|awards         = Scottish Research Book of the Year Award, [[Saltire Society Literary Awards]], 2009
|isbn           = 978-0199208999
|oclc           = 318409912
|dewey          = 
|congress       =  PE1591 .H55 2009
}}

The '''''Historical Thesaurus of the Oxford English Dictionary''''' ('''''HTOED''''') is the print edition of the largest [[thesaurus]] in the world, the '''''Historical Thesaurus of English''''' ('''''HTE'''''), conceived and compiled by the English Language Department of the [[University of Glasgow]]. The ''HTE'' is a complete database of all the words in the second edition of [[Oxford English Dictionary|''The Oxford English Dictionary'']], arranged by [[semantic field]] and date. In this way, the ''HTE'' arranges the whole vocabulary of [[English language|English]], from the earliest written records in [[Old English language|Old English]] to the present, alongside types and dates of use. It is the first historical thesaurus to be compiled for any of the world's languages and contains 800,000 meanings for 600,000 words, within 230,000 categories, covering more than 920,000 words and meanings.&lt;ref name="Woolcock"&gt;{{cite news| url=http://entertainment.timesonline.co.uk/tol/arts_and_entertainment/books/article6644646.ece | location=London | work=The Times | first=Nicola | last=Woolcock | title=After a 44-year labour of love worlds biggest thesaurus is born | date=2009-07-06}}{{subscription required}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last1=Hitchings|first1=Henry|authorlink1=Henry Hitchings|title=Historical Thesaurus is a masterpiece worth waiting 40 years for|url=http://www.telegraph.co.uk/comment/personal-view/6413166/Historical-Thesaurus-is-a-masterpiece-worth-waiting-40-years-for.html|accessdate=25 October 2014|publisher=The Telegraph|date=23 October 2009|ref=Hitchings-2009|location=London}}&lt;/ref&gt;  As the ''HTE'' website states, "in addition to providing hitherto unavailable information for linguistic and textual scholars, the ''Historical Thesaurus'' online is a rich resource for students of social and cultural history, showing how concepts developed through the words that refer to them."&lt;ref name="hte"&gt;{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/ |title=Home page |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}&lt;/ref&gt;

The ambitious project was announced at a 1965 meeting of the [[Philological Society]] by its originator, [[Michael Samuels (academic)|Michael Samuels]].&lt;ref name=Crystal-2014&gt;{{cite book|last1=Crystal|first1=David|authorlink1=David Crystal|title=Words in Time and Place: Exploring Language Through the ''Historical Thesaurus of the Oxford English Dictionary''|date=2014|publisher=Oxford University Press|location=Oxford|isbn=0199680477|page=vii}}&lt;/ref&gt;  Work on the ''HTE'' started in 1965.

On 22 October 2009, after 44 years of work, version 1.0 was published as a two-volume set as ''HTOED''.&lt;ref&gt;{{cite news|url=http://news.bbc.co.uk/1/hi/england/oxfordshire/8136122.stm |title=UK &amp;#124; England &amp;#124; Oxfordshire &amp;#124; Forty-year wait for new thesaurus |publisher=BBC News |date=2009-07-06 |accessdate=2010-04-15}}&lt;/ref&gt; It consists of two slipcased hardcover volumes, totaling nearly 4,000 pages. The ''HTE'', released as version 4.2 in September 2014, is freely available online from the University of Glasgow.&lt;ref name="hte-versions"&gt;{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/versions-and-changes/ |title=Versions of the Thesaurus |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}&lt;/ref&gt;

==Main sections==
The work is divided into three main sections: the External World, the Mind, and Society. These are broken down into successively narrower domains. The text eventually discriminates more than 236,000 categories.
The second order categories are:&lt;ref&gt;{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/classification/ |title=Classification |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-22}} An oversize, one-page listing of all categories in top three tiers is available for download here.&lt;/ref&gt;
{{col-begin-small}}
{{col-break}}
;I. The External World
 
# The Earth
# Life
# Physical sensibility
# Matter
# Existence
# Relative properties
# The Supernatural
{{col-break}} 
;II. The Mind
 
# Soul, spirit, mind
# Emotion/feeling
# Judgement, opinion
# Aesthetics
# Will/faculty of will
# Expectation
# Having/possession
# Languages
{{col-break}}
;III. Society
 
# Society/life in association with others
# Inhabiting/dwelling
# Relations between social groups
# Authority
# Law
# Education
# Religion
# Communications
# Travel/travelling
# Work / Serious occupation
# Leisure/The Arts
{{col-end}}

==References==
{{reflist}}

==External links==
* {{cite web|title=Search|url=http://historicalthesaurus.arts.gla.ac.uk/search/|website=The Historical Thesaurus of English|publisher=University of Glasgow}} {{open access}}
{{Dictionaries of English}}

[[Category:Thesauri]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Language histories]]
[[Category:History of the English language]]</text>
      <sha1>qhgui4h1sxxjw9ce74o94i8izwic2pr</sha1>
    </revision>
  </page>
  <page>
    <title>Default logic</title>
    <ns>0</ns>
    <id>889639</id>
    <revision>
      <id>708046143</id>
      <parentid>708045952</parentid>
      <timestamp>2016-03-03T08:20:35Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor />
      <comment>/* Entailment */ &amp;not;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20570" xml:space="preserve">
'''Default logic''' is a [[non-monotonic logic]] proposed by [[Raymond Reiter]] to formalize reasoning with default assumptions.

Default logic can express facts like &#8220;by default, something is true&#8221;; by contrast, standard logic can only express that something is true or that something is false. This is a problem because reasoning often involves facts that are true in the majority of cases but not always. A classical example is: &#8220;birds typically fly&#8221;. This rule can be expressed in standard logic either by &#8220;all birds fly&#8221;, which is inconsistent with the fact that penguins do not fly, or by &#8220;all birds that are not penguins and not ostriches and ... fly&#8221;, which requires all exceptions to the rule to be specified. Default logic aims at formalizing inference rules like this one without explicitly mentioning all their exceptions.

==Syntax of default logic==
A default theory is a pair &lt;math&gt;\langle W, D \rangle&lt;/math&gt;. {{mvar|W}} is a set of logical formulae, called ''the background theory'', that formalize the facts that are known for sure. {{mvar|D}} is a set of ''default rules'', each one being of the form:

: &lt;math&gt;\frac{\mathrm{Prerequisite : Justification}_1, \dots , \mathrm{Justification}_n}{\mathrm{Conclusion}}&lt;/math&gt;

According to this default, if we believe that {{math|Prerequisite}} is true, and each of &lt;math&gt;\mathrm{Justification}_i&lt;/math&gt; is consistent with our current beliefs, we are led to believe that {{math|Conclusion}} is true.

The logical formulae in {{mvar|W}} and all formulae in a default were originally assumed to be [[first-order logic]] formulae, but they can potentially be formulae in an arbitrary formal logic. The case in which they are formulae in [[propositional logic]] is one of the most studied.

===Examples===
The default rule &#8220;birds typically fly&#8221; is formalized by the following default:

:&lt;math&gt;D = \left\{ \frac{\mathrm{Bird}(X) : \mathrm{Flies}(X)}{\mathrm{Flies}(X)} \right\}&lt;/math&gt;

This rule means that, if {{mvar|X}} is a bird, and it can be assumed that it flies, then we can conclude that it flies. A background theory containing some facts about birds is the following one:

:&lt;math&gt;W = \{ \mathrm{Bird}(\mathrm{Condor}), \mathrm{Bird}(\mathrm{Penguin}), \neg \mathrm{Flies}(\mathrm{Penguin}), \mathrm{Flies}(\mathrm{Bee}) \}&lt;/math&gt;.

According to this default rule, a condor flies because the precondition {{math|Bird(Condor)}} is true and the justification {{math|Flies(Condor)}} is not inconsistent with what is currently known. On the contrary, {{math|Bird(Penguin)}} does not allow concluding {{math|Flies(Penguin)}}: even if the precondition of the default {{math|Bird(Penguin)}} is true, the justification {{math|Flies(Penguin)}} is inconsistent with what is known.
From this background theory and this default, {{math|Bird(Bee)}} cannot be concluded because the default rule only allows deriving
{{math|Flies(''X'')}} from {{math|Bird(''X'')}}, but not vice versa. Deriving the antecedents of an inference rule from the consequences is a form of explanation of the consequences, and is the aim of [[abductive reasoning]].

A common default assumption is that what is not known to be true is believed to be false. This is known as the [[Closed World Assumption]], and is formalized in default logic using a default like the following one for every fact {{mvar|F}}.

: &lt;math&gt;\frac{:{\neg}F}{{\neg}F}&lt;/math&gt;

For example, the computer language [[Prolog]] uses a sort of default assumption when dealing with negation: if a negative atom cannot be proved to be true, then it is assumed to be false.
Note, however, that Prolog uses the so-called [[negation as failure]]: when the interpreter has to evaluate the atom &lt;math&gt;\neg F&lt;/math&gt;, it tries to prove that {{mvar|F}} is true, and conclude that &lt;math&gt;\neg F&lt;/math&gt; is true if it fails. In default logic, instead, a default having &lt;math&gt;\neg F&lt;/math&gt; as a justification can only be applied if &lt;math&gt;\neg F&lt;/math&gt; is consistent with the current knowledge.

===Restrictions===

A default is categorical or prerequisite-free if it has no prerequisite (or, equivalently, its prerequisite is [[tautology (logic)|tautological]]). A default is normal if it has a single justification that is equivalent to its conclusion. A default is supernormal if it is both categorical and normal. A default is seminormal if all its justifications entail its conclusion. A default theory is called categorical, normal, supernormal, or seminormal if all defaults it contains are categorical, normal, supernormal, or seminormal, respectively.

==Semantics of default logic==

A default rule can be applied to a theory if its precondition is entailed by the theory and its justifications are all '''''consistent with''''' the theory.  The application of a default rule leads to the addition of its consequence to the theory.  Other default rules may then be applied to the resulting theory.  '''When the theory is such that no other default can be applied, the theory is called an extension of the default theory.'''  The default rules may be applied in different order, and this may lead to different extensions. The [[Nixon diamond]] example is a default theory with two extensions:

:&lt;math&gt;
\left\langle
\left\{
\frac{\mathrm{Republican}(X):\neg \mathrm{Pacifist}(X)}{\neg \mathrm{Pacifist}(X)},
\frac{\mathrm{Quaker}(X):\mathrm{Pacifist}(X)}{\mathrm{Pacifist}(X)}
\right\},
\left\{\mathrm{Republican}(\mathrm{Nixon}), \mathrm{Quaker}(\mathrm{Nixon})\right\}
\right\rangle
&lt;/math&gt;

Since [[Richard Nixon|Nixon]] is both a [[American Republican|Republican]] and a [[Quaker]], both defaults can be applied. However, applying the first default leads to the conclusion that Nixon is not a pacifist, which makes the second default not applicable. In the same way, applying the second default we obtain that Nixon is a pacifist, thus making the first default not applicable. This particular default theory has therefore two extensions, one in which {{math|Pacifist(Nixon)}} is true, and one 
in which {{math|Pacifist(Nixon)}} is false. 

The original semantics of default logic was based on the [[Fixed point (mathematics)|fixed point]] of a function. The following is an equivalent algorithmic definition. If a default contains formulae with free variables, it is considered to represent the set of all defaults obtained by giving a value to all these variables. A default &lt;math&gt;\frac{\alpha:\beta_1,\ldots,\beta_n}{\gamma}&lt;/math&gt; is applicable to a propositional theory {{mvar|T}} if &lt;math&gt;T \models \alpha&lt;/math&gt; and
all theories &lt;math&gt;T \cup \{\beta_i\}&lt;/math&gt; are consistent. The application of this default to {{mvar|T}} leads to the theory &lt;math&gt;T \cup \{\gamma\}&lt;/math&gt;. An extension can be generated by applying the following algorithm:

 T=W           /* current theory */
 A=0           /* set of defaults applied so far */
 &amp;nbsp;
               /* apply a sequence of defaults */
 '''while''' there is a default d that is not in A and is applicable to T
   add the consequence of d to T
   add d to A
 &amp;nbsp;
               /* final consistency check */
 '''if''' 
   for every default d in A
     T is consistent with all justifications of d
 '''then'''
   output T

This algorithm is [[nondeterministic algorithm|non-deterministic]], as several defaults can alternatively be applied to a given theory {{mvar|T}}. In the Nixon diamond example, the application of the first default leads to a theory to which the second default cannot be applied and vice versa. As a result, two extensions are generated: one in which Nixon is a pacifist and one in which Nixon is not a pacifist.

The final check of consistency of the justifications of all defaults that have been applied implies that some theories do not have any extensions. In particular, this happens whenever this check fails for every possible sequence of applicable defaults. The following default theory has no extension:

:&lt;math&gt;
\left\langle 
\left\{
\frac{:A(b)}{\neg A(b)}
\right\},
\emptyset
\right\rangle
&lt;/math&gt;

Since &lt;math&gt;A(b)&lt;/math&gt; is consistent with the background theory, the default can be applied, thus leading to the conclusion that &lt;math&gt;A(b)&lt;/math&gt; is false. This result however undermines the assumption that has been made for applying the first default. Consequently, this theory has no extensions.

In a normal default theory, all defaults are normal: each default has the form &lt;math&gt;\frac{\phi : \psi}{\psi}&lt;/math&gt;. A normal default theory is guaranteed to have at least one extension. Furthermore, the extensions of a normal default theory are mutually inconsistent, i.e., inconsistent with each other.

===Entailment===

A default theory can have zero, one, or more extensions. [[Entailment]] of a formula from a default theory can be defined in two ways:

; Skeptical : a formula is entailed by a default theory if it is entailed by all its extensions;

; Credulous : a formula is entailed by a default theory if it is entailed by at least one of its extensions.

Thus, the Nixon diamond example theory has two extensions, one in which Nixon is a pacifist and one in which he is not a pacifist. Consequently, neither {{math|Pacifist(Nixon)}} nor {{math|&amp;not;Pacifist(Nixon)}} are skeptically entailed, while both of them are credulously entailed. As this example shows, the credulous consequences of a default theory may be inconsistent with each other.

===Alternative default inference rules===
&lt;!-- these are the alternative default inference rules that are based on the same original syntax of default logic --&gt;

The following alternative inference rules for default logic are all based on the same syntax as the original system.

; Justified: differs from the original one in that a default is not applied if thereby the set {{mvar|T}} becomes [[inconsistent]] with a justification of an applied default;

; Concise: a default is applied only if its consequence is not already entailed by {{mvar|T}} (the exact definition is more complicated than this one; this is only the main idea behind it);

; Constrained: a default is applied only if the set composed of the background theory, the justifications of all applied defaults, and the consequences of all applied defaults (including this one) is consistent;

; Rational: similar to constrained default logic, but the consequence of the default to add is not considered in the consistency check;

; Cautious: defaults that can be applied but are conflicting with each other (like the ones of the Nixon diamond example) are not applied.

The justified and constrained versions of the inference rule assign at least an extension to every default theory.

==Variants of default logic==
&lt;!-- these are the variants of default logic that differ from the original one both in syntax and semantics --&gt;

The following variants of default logic differ from the original one on both syntax and semantics.

; Assertional variants : An assertion is a pair &lt;math&gt;\langle p: \{r_1,\ldots,r_n\} \rangle&lt;/math&gt; composed of a formula and a set of formulae. Such a pair indicates that {{mvar|p}} is true while the formulae &lt;math&gt;r_1,\ldots,r_n&lt;/math&gt; have been assumed consistent to prove that {{mvar|p}} is true. An assertional default theory is composed of an assertional theory (a set of assertional formulae) called the background theory and a set of defaults defined as in the original syntax. Whenever a default is applied to an assertional theory, the pair composed of its consequence and its set of justifications is added to the theory. The following semantics use assertional theories:

*Cumulative default logic
*Commitment to assumptions default logic
*Quasi-default logic

; Weak extensions : rather than checking whether the preconditions are valid in the theory composed of the background theory and the consequences of the applied defaults, the preconditions are checked for validity in the extension that will be generated; in other words, the algorithm for generating extensions starts by guessing a theory and using it in place of the background theory; what results from the process of extension generation is actually an extension only if it is equivalent to the theory guessed at the beginning. This variant of default logic is related in principle to [[autoepistemic logic]], where a theory &lt;math&gt;\Box x \rightarrow x&lt;/math&gt; has the model in which {{mvar|x}} is true just because, assuming &lt;math&gt;\Box x&lt;/math&gt; true, the formula &lt;math&gt;\Box x \rightarrow x&lt;/math&gt; supports the initial assumption.

; Disjunctive default logic : the consequence of a default is a set of formulae instead of a single formula. Whenever the default is applied, at least one of its consequences is nondeterministically chosen and made true.

; Priorities on defaults : the relative priority of defaults can be explicitly specified; among the defaults that are applicable to a theory, only one of the most preferred ones can be applied. Some semantics of default logic do not require priorities to be explicitly specified; rather, more specific defaults (those that are applicable in fewer cases) are preferred over less specific ones.

; Statistical variant : a statistical default is a default with an attached upper bound on its frequency of error; in other words, the default is assumed to be an incorrect inference rule in at most that fraction of times it is applied.

==Translations==

Default theories can be translated into theories in other logics and vice versa. The following conditions on translations have been considered:

; Consequence-Preserving : the original and the translated theories have the same (propositional) consequences;

; Faithful : this condition only makes sense when translating between two variants of default logic or between default logic and a logic in which a concept similar to extension exists, e.g., models in modal logic; a translation is faithful if there exists a mapping (typically, a bijection) between the extensions (or models) of the original and translated theories;

; Modular : a translation from default logic to another logic is modular if the defaults and the background theory can be translated separately; moreover, the addition of formulae to the background theory only leads to adding the new formulae to the result of the translation;

; Same-Alphabet : the original and translated theories are built on the same alphabet;

; Polynomial : the running time of the translation or the size of the generated theory are required to be polynomial in the size of the original theory.

Translations are typically required to be faithful or at
least consequence-preserving, while the conditions of
modularity and same alphabet are sometimes ignored.

The translatability between propositional default logic and
the following logics have been studied:

* classical propositional logic;
* autoepistemic logic;
* propositional default logic restricted to seminormal theories;
* alternative semantics of default logic;
* circumscription.

Translations exist or not depending on which conditions are imposed. Translations from propositional default logic to classical propositional logic cannot always generate a polynomially sized propositional theory, unless the [[polynomial hierarchy]] collapses. Translations to autoepistemic logic exists or not depending on whether modularity or the use of the same alphabet is required.

==Complexity==

The [[Analysis of algorithms|computational complexity]] of the following problems about default logic is known:

; Existence of extensions : deciding whether a propositional default theory has at least one extension is &lt;math&gt;\Sigma^P_2&lt;/math&gt;-complete;

; Skeptical entailment : deciding whether a propositional default theory skeptically entails a [[propositional formula]] is &lt;math&gt;\Pi^P_2&lt;/math&gt;-complete;

; Credulous entailment : deciding whether a propositional default theory credulously entails a propositional formula is &lt;math&gt;\Sigma^P_2&lt;/math&gt;-complete;

; Extension checking : deciding whether a propositional formula is equivalent to an extension of a propositional default theory is &lt;math&gt;\Delta^{P[log]}_2&lt;/math&gt;-complete;

; Model checking : deciding whether a propositional interpretation is a model of an extension of a propositional default theory is &lt;math&gt;\Sigma^P_2&lt;/math&gt;-complete.

==Implementations==

Three systems implementing default logics are 
[ftp://www.cs.engr.uky.edu/cs/manuscripts/deres.ps DeReS],
[http://www.cs.uni-potsdam.de/wv/xray/ XRay] and
[http://www.info.univ-angers.fr/pub/stephan/Research/GADEL/GADEL_prolog.html GADeL]
&lt;!-- algorithms? other implemented systems? --&gt;

==See also==
* [[Answer set programming]]
* [[Defeasible logic]]
* [[Non-monotonic logic]]

==References==
* G. Antoniou (1999). A tutorial on default logics. ''ACM Computing Surveys'', 31(4):337-359.
* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (2000). Space efficiency of propositional knowledge representation formalisms. ''Journal of Artificial Intelligence Research'', 13:1-31.
* P. Cholewinski, V. Marek, and M. Truszczynski (1996). Default reasoning system DeReS. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 518-528.
* J. Delgrande and T. Schaub (2003). On the relation between Reiter's default logic and its (major) variants. In ''Seventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2003)'', pages 452-463.
* J. P. Delgrande, T. Schaub, and W. K. Jackson (1994). Alternative approaches to default logic. ''Artificial Intelligence'', 70:167-237.
* G. Gottlob (1992). Complexity results for nonmonotonic logics. ''Journal of Logic and Computation'', 2:397-425.
* G. Gottlob (1995). Translating default logic into standard autoepistemic logic. ''Journal of the ACM'', 42:711-740.
* T. Imielinski (1987). Results on translating defaults to circumscription. ''Artificial Intelligence'', 32:131-146.
* T. Janhunen (1998). On the intertranslatability of autoepistemic, default and priority logics, and parallel circumscription. In ''Proceedings of the Sixth European Workshop on Logics in Artificial Intelligence (JELIA'98)'', pages 216-232.
* T. Janhunen (2003). Evaluating the effect of semi-normality on the expressiveness of defaults. ''Artificial Intelligence'', 144:233-250.
* H. E. Kyburg and C-M. Teng (2006). Nonmonotonic Logic and Statistical Inference. ''Computational Intelligence'', 22(1): 26-51.
* P. Liberatore and M. Schaerf (1998). The complexity of model checking for propositional default logics. In ''Proceedings of the Thirteenth European Conference on Artificial Intelligence (ECAI'98)'', pages 18&#8211;22.
* W. Lukaszewicz (1988). Considerations on default logic: an alternative approach. ''Computational Intelligence'', 4(1):1-16.
* W. Marek and M. Truszczynski (1993). ''Nonmonotonic Logics: Context-Dependent Reasoning''. Springer.
* A. Mikitiuk and M. Truszczynski (1995). Constrained and rational default logics. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1509-1517.
* P. Nicolas, F. Saubion and I. St&#233;phan (2001). Heuristics for a Default Logic Reasoning System. ''International Journal on Artificial Intelligence Tools'', 10(4):503-523.
* R. Reiter (1980). A logic for default reasoning. ''Artificial Intelligence'', 13:81-132.
* T. Schaub, S. Br&#252;ning, and P. Nicolas (1996). XRay: A prolog technology theorem prover for default reasoning: A system description. In ''Proceedings of the Thirteenth International Conference on Automated Deduction (CADE'96)'', pages 293-297.
* G. Wheeler (2004). A resource bounded default logic. In ''Proceedings of the 10th International Workshop on Non-Monotonic Reasoning (NMR-04)'', Whistler, British Columbia, 416-422.
* G. Wheeler and C. Damasio (2004). An Implementation of Statistical Default Logic. In ''Proceedings of the 9th European Conference on Logics in Artificial Intelligence (JELIA 2004)'', LNCS Series, Springer, pages 121-133.

==External links==
* Schmidt, Charles F. [http://www.rci.rutgers.edu/~cfs/472_html/Logic_KR/DefaultTheory.html RCI.Rutgers.edu], Default Logic. Retrieved August 10, 2004.
* Ramsay, Allan (1999). [http://www.ccl.umist.ac.uk/teaching/material/5005/node33.html UMIST.ac.uk], Default Logic. Retrieved August 10, 2004.
* [http://plato.stanford.edu/entries/reasoning-defeasible/ Stanford.edu], Defeasible reasoning, [[Stanford Encyclopedia of Philosophy]].

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Logical calculi]]
[[Category:Non-classical logic]]</text>
      <sha1>eypulo2qp3a7g9s03ruggy91aj0tg3z</sha1>
    </revision>
  </page>
  <page>
    <title>DogmaModeler</title>
    <ns>0</ns>
    <id>15967932</id>
    <revision>
      <id>664929732</id>
      <parentid>599383412</parentid>
      <timestamp>2015-05-31T22:32:46Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]] (11023)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1726" xml:space="preserve">{{unreferenced|date=March 2014}}
[[Image:DogmaModeler1.jpg|thumb|DogmaModeler Screenshot]]
'''DogmaModeler''' is a free and open source ([[GNU GPL]]) [[Ontology (computer science)|ontology]] modeling tool based on [[object-role modeling]] (ORM). The philosophy of DogmaModeler is to enable non-IT experts to model ontologies with a little or no involvement of an ontology engineer. This challenge is tackled in DogmaModeler through well-defined methodological principles: the (1) [[Ontology double articulation|double-articulation]] and the (2) [[modularization]] principles. Other important features are: (3) the use of ORM as a graphical notation for ontology modeling; (4) the verbalization of ORM diagrams into pseudo natural language (supporting flexible verbalization templates for 11 human languages, including English, Dutch, German, French, Spanish, Arabic, Russian, etc.) that allows non-experts to check, validate, or build ontologies; (5)the automatic composition of ontology modules, through a well-defined composition operator; (6) the incorporation of linguistic resources in [[ontology engineering]]; (7) the automatic mapping of ORM diagrams into the DIG [[description logic]] interface and reasoning using [[RACER system|Racer]]; and many other functionalities.

The first version of DogmaModeler was developed at the [[Vrije Universiteit Brussel]].

== See also ==
* [[DOGMA]] 
* [[NORMA (software modeling tool)]]
* [[Prot&#233;g&#233; (software)|Prot&#233;g&#233;]]

==References==
{{reflist}}

== External links ==
* {{Official website|http://www.jarrar.info/Dogmamodeler/index.htm}}

[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]</text>
      <sha1>nmrg7uv113wspgctaf4m3etx0oa344i</sha1>
    </revision>
  </page>
  <page>
    <title>BCM Classification</title>
    <ns>0</ns>
    <id>17952329</id>
    <revision>
      <id>745787482</id>
      <parentid>742924097</parentid>
      <timestamp>2016-10-23T08:49:12Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.5)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3855" xml:space="preserve">The '''British Catalogue of Music Classification''' (BCM Classification)&lt;ref&gt;The British Catalogue of Music Classification / compiled for the Council of the British National Bibliography Ltd. by E. J. Coates, F.L.A.  London : Council of the British National Bibliography, 1960&lt;/ref&gt; is a [[faceted classification]] that was commissioned from E. J. Coates by the Council of the British National Bibliography to organize the content of the British Catalogue of Music.&lt;ref&gt;{{cite web|url=https://archive.org/details/britishcatalogue001781mbp |title=Internet Archive: Details: The British Catalogue Of Music 1960 |publisher=Archive.org |date= |accessdate=2016-10-06}}&lt;/ref&gt; The published schedule (1960) was considerably expanded by Patrick Mills of the British Library up until its use was abandoned in 1998. Entries in the catalogue were organized by BCM classmark from the catalogue's inception in 1957 until 1982. From that year the British Catalogue of Music (which from 1974 onward was published by [[The British Library]]) was organized instead by [[Dewey Decimal Classification]] number, though BCM classmarks continued to be added to entries up to the 1998 annual cumulation.

The schedule is divided into two main parts: A-B representing Musical literature and C-Z representing Music &#8212; Scores and Parts.  There are also seven auxiliary tables dealing with various sub-arrangements, sets of ethnic/locality subdivisions and chronological reference points.

The notation is retroactive using uppercase alphabetic characters omitting I and O, with the addition of slash / and parentheses ( ) which have specific anteriorizing functions.  Retroactive notation requires that the classifier combines terms in reverse schedule order. This has the benefit of producing a compact notation by removing the need for facet indicators.

The schedule at A (Music Literature) parallels that from the Scores and Parts schedules thus Choral Music is at D while books about Choral Music are at AD; Harp Music is at TQ so books on harp music are at ATQ.  The schedule at B accommodates books about specific composers and music in non-European traditions.

As a fully faceted scheme after the ideas of [[S. R. Ranganathan]], BCM class numbers are capable of being chain-indexed, allowing index access to each step of the hierarchy.

BCM classification had a strong influence on Russell Sweeney's so-called Phoenix Dewey 780 schedule&lt;ref&gt;DDC Dewey Decimal Classification : proposed revision of 780 music / prepared under the direction of Russell Sweeney and John Clews with assistance from Winton E. Mathews, Jr. Albany N.Y. : Forest Press, 1980. ISBN 0-910608-25-3&lt;/ref&gt; which in turn influenced the 780 Music schedule in the 20th edition of [[Dewey Decimal Classification]].  The music schedule of the second edition of the Bliss Classification&lt;ref&gt;{{cite web|url=http://library.music.indiana.edu/tech_s/mla/facacc.rev |title=Archived copy |accessdate=2008-06-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20080512000152/http://library.music.indiana.edu:80/tech_s/mla/facacc.rev |archivedate=2008-05-12 |df= }}&lt;/ref&gt; is also strongly influenced by BCM.

This classification system is still in use at a number of libraries, including the [[State Library of Western Australia]]&lt;ref&gt;{{cite web|url=http://www.slwa.wa.gov.au/find/guides/music/general_information/british_catalogue_of_music_classification_scheme |title=(accessed 2015-12-17) |publisher=Slwa.wa.gov.au |date=2013-08-20 |accessdate=2016-10-06}}&lt;/ref&gt; and the Library at [[Edith Cowan University]].&lt;ref&gt;{{cite web|url=http://ecu.au.libguides.com/c.php?g=410622&amp;p=2797056 |title=(accessed 2015-12-17) |publisher=Ecu.au.libguides.com |date=2016-08-05 |accessdate=2016-10-06}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>thhham0uws0efjceg0xja3z3fs7yi03</sha1>
    </revision>
  </page>
  <page>
    <title>Darwin Core</title>
    <ns>0</ns>
    <id>21195116</id>
    <revision>
      <id>753442765</id>
      <parentid>744814321</parentid>
      <timestamp>2016-12-07T05:08:48Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 5 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8006" xml:space="preserve">'''Darwin Core''' (often abbreviated to '''DwC''') is an extension of [[Dublin Core]] for [[biodiversity informatics]]. It is meant to provide a stable standard reference for sharing information on biological diversity.&lt;ref&gt;{{cite journal|last=Wieczorek|first=John|author2=D. Bloom |author3=R. Guralnick |author4=S. Blum |author5=M. D&#246;ring |author6=R. De Giovanni |author7=T. Robertson |author8=D. Vieglais |title=Darwin Core: An Evolving Community-developed Biodiversity Data Standard.|journal=[[PLoS ONE]] |year=2012|volume=7|issue=1|doi=10.1371/journal.pone.0029715|pmid=22238640|pmc=3253084}}&lt;/ref&gt; The terms described in this standard are a part of a larger set of vocabularies and technical specifications under development and maintained by [[Biodiversity Information Standards (TDWG)]] (formerly known as the Taxonomic Databases Working Group (TDWG)).

== Description ==
The Darwin Core is a body of standards. It includes a glossary of terms (in other contexts these might be called properties, elements, fields, columns, attributes, or concepts) intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on [[taxon|taxa]], their occurrence in nature as documented by observations, specimens, and samples, and related information. Included in the standard are documents describing how these terms are managed, how the set of terms can be extended for new purposes, and how the terms can be used. The '''Simple Darwin Core''' &lt;ref name="simpledwc"&gt;[http://rs.tdwg.org/dwc/terms/simple/index.htm  The Simple Darwin Core]&lt;/ref&gt; is a specification for one particular way to use the terms and to share data about taxa and their occurrences in a simply-structured way. It is likely what is meant if someone were to suggest "formatting your data according to the Darwin Core".

Each '''term''' has a definition and commentaries that are meant to promote the consistent use of the terms across applications and disciplines. Evolving commentaries that discuss, refine, expand, or translate the definitions and examples are referred to through links in the Comments attribute of each term. This approach to documentation allows the standard to adapt to new purposes without disrupting existing applications. There is meant to be a clear separation between the terms defined in the standard and the applications that make use of them. For example, though the data types and constraints are not provided in the term definitions, recommendations are made about how to restrict the values where appropriate.

In practice, Darwin Core decouples the definition and semantics of individual terms from application of these terms in different technologies such as [[XML]], [[Resource Description Framework|RDF]] or simple [[Comma-separated values|CSV]] text files. Darwin Core provides separate guidelines on how to encode the terms as XML&lt;ref name="dwc-xml" &gt;[http://rs.tdwg.org/dwc/terms/guides/xml/index.htm Darwin Core XML Guide]&lt;/ref&gt; or text files.&lt;ref name="dwc-text"&gt;[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guide]&lt;/ref&gt;

== History ==
Darwin Core was originally created as a [[Z39.50]] profile by the Z39.50 Biology Implementers Group (ZBIG), supported by funding from a USA National Science Foundation award.&lt;ref name="zbig"&gt;An Experimental Z39.50 Information Retrieval Protocol Test Bed for Biological Collection and Taxonomic Data, #9811443 [http://nsf.gov/awardsearch/showAward.do?AwardNumber=9811443]&lt;/ref&gt;  The name "Darwin Core" was first coined by Allen Allison at the first meeting of the ZBIG held at the University of Kansas in 1998 while commenting on the profile's conceptual similarity with Dublin Core. The Darwin Core profile was later expressed as an XML Schema document for use by the Distributed Generic Information Retrieval (DiGIR) protocol. A [[TDWG]] task group was created to revise the Darwin Core, and a ratified metadata standard was officially released on 9 October 2009.

Though ratified as a TDWG/[[Biodiversity Information Standards]] standard since then, Darwin Core has had numerous previous versions in production usage. The published standard contains a history&lt;ref name="history"&gt;[http://rs.tdwg.org/dwc/terms/history/index.htm Darwin Core History]&lt;/ref&gt; with details of the versions leading to the current standard.

{| class="wikitable" style="text-align:left"
|+Darwin Core Versions
|-
! Name !! Namespace !! Number of terms !! XML Schema !! Date Issued
|-
! Darwin Core 1.0
| Not Applicable || 24 || (Z39.50 GRS-1) || 1998
|-
! Darwin Core 1.2 (Classic)
| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 46 || [http://digir.net/schema/conceptual/darwin/2003/1.0/darwin2.xsd] || 2001-09-11
|-
! Darwin Core 1.21 (MaNIS/HerpNet/ORNIS/FishNet2)
| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 63 || [http://digir.net/schema/conceptual/darwin/manis/1.21/darwin2.xsd] || 2003-03-15
|-
! Darwin Core OBIS
| http://www.iobis.org/obis  {{dead link|date=October 2016}} || 27 || [http://iobis.org/obis/obis.xsd] || 2005-07-10
|-
! Darwin Core 1.4 (Draft Standard)
| http://rs.tdwg.org/dwc/dwcore/  {{dead link|date=October 2016}} || 45 || [http://rs.tdwg.org/dwc/tdwg_dw_core.xsd] || 2005-07-10
|-
! Darwin Core Terms (properties)
| http://rs.tdwg.org/dwc/terms/ || 172 || [http://rs.tdwg.org/dwc/xsd/tdwg_dwcterms.xsd] || 2009-10-09
|-
|}

== Key Projects Using Darwin Core ==
* The [[Global Biodiversity Information Facility]] (GBIF)&lt;ref&gt;{{cite web|url=http://www.gbif.org/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives/ |title=Darwin Core |publisher=[[Global Biodiversity Information Facility]] |accessdate=April 12, 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110412022331/http://www.gbif.org:80/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives |archivedate=April 12, 2011 |df= }}&lt;/ref&gt;
* The [[Ocean Biogeographic Information System]] (OBIS)&lt;ref&gt;{{cite web|url=http://www.iobis.org/data/schema-and-metadata|title=Data Schema and metadata|publisher=[[Ocean Biogeographic Information System]]|accessdate=April 12, 2011}}&lt;/ref&gt;
*[http://www.ala.org.au/datastandards.htm The Atlas of Living Australia (ALA)]
*[http://www3.interscience.wiley.com/cgi-bin/fulltext/120713092/PDFSTART Online Zoological Collections of Australian Museums (OZCAM)]
*[http://manisnet.org Mammal Networked Information System (MaNIS)]
*[http://ornisnet.org Ornithological Information System (ORNIS)]
*[http://www.fishnet2.net/index.html FishNet 2]
*[http://vertnet.org VertNet]
*[http://www.canadensys.net/ Canadensys]
*[http://w3.ufsm.br/herbarioflorestal/nature/site/ Sistema Nature 3.0]
*[http://eol.org Encyclopedia of Life]
*[https://www.idigbio.org Integrated Digitized Biocollections (iDigBio)] &lt;ref&gt;{{cite web|title=Data Ingestion Guidance|url=https://www.idigbio.org/wiki/index.php/Data_Ingestion_Guidance|publisher=[[iDigBio]]|accessdate=26 September 2016}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=Getting your data out there: Data publishing &amp; data standards with iDigBio|url=https://www.idigbio.org/content/getting-your-data-out-there-data-publishing-data-standards-idigbio|publisher=[[iDigBio]]|accessdate=26 September 2016}}&lt;/ref&gt;

== See also ==
* [[Darwin Core Archive]]
* [[Biodiversity Information Standards]] (TDWG)
* [[Biodiversity]]
* [[Biodiversity informatics]]
* [[Metadata standards]]

==References==
{{reflist}}

==External links==
*[http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]
*[https://github.com/tdwg/dwc/ Darwin Core Development Site]
*[http://www.tdwg.org/activities/darwincore/ Official Darwin Core Website]
*[http://www.tdwg.org/fileadmin/subgroups/dwc/exec_summary_dwc.doc Executive Summary of Darwin Core]

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
[[Category:Metadata standards]]</text>
      <sha1>7an2l6iix6p09vpc675wf64tkl4uy3v</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge representation languages</title>
    <ns>14</ns>
    <id>23890667</id>
    <revision>
      <id>733574189</id>
      <parentid>722189610</parentid>
      <timestamp>2016-08-08T19:14:16Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Another addition to the list</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="221" xml:space="preserve">== See also ==

* [[:Category:Constraint programming languages]]
* [[:Category:Domain-specific programming languages]]

[[Category:Knowledge representation]]
[[Category:Engineered languages]]
[[Category:Markup languages]]</text>
      <sha1>44y3tklg8zhfl7wsyu8n9na341tm8sj</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology alignment</title>
    <ns>0</ns>
    <id>4696039</id>
    <revision>
      <id>757961504</id>
      <parentid>757958751</parentid>
      <timestamp>2017-01-02T18:34:32Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Clarify}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11736" xml:space="preserve">'''Ontology alignment''', or '''ontology matching''', is the process of determining correspondences between [[concept]]s in [[ontologies]]. A set of correspondences is also called an alignment. The phrase takes on a slightly different meaning, in [[computer science]], [[cognitive science]] or [[philosophy]].

==Computer Science==

For [[computer scientist]]s, concepts are expressed as labels for data.  Historically, the need for ontology alignment arose out of the need to [[data integration|integrate]] heterogeneous [[database]]s, ones developed independently and thus each having their own data vocabulary.  In the [[Semantic Web]] context involving many actors providing their own [[ontology (information science)|ontologies]], ontology matching has taken a critical place for helping heterogeneous resources to interoperate. Ontology alignment tools find classes of data that are "[[semantic equivalence|semantically equivalent]]," for example, "Truck" and "Lorry."  The classes are not necessarily logically identical.  According to Euzenat and Shvaiko (2007),&lt;ref name="Euzenat Shvaiko"&gt;J&#233;r&#244;me Euzenat and Pavel Shvaiko. 2007. [http://book.ontologymatching.org Ontology matching], Springer-Verlag, 978-3-540-49611-3.&lt;/ref&gt; there are three major dimensions for similarity: syntactic, external, and semantic.  Coincidentally, they roughly correspond to the dimensions identified by Cognitive Scientists below.  A number of tools and frameworks have been developed for aligning ontologies, some with inspiration from Cognitive Science and some independently.

Ontology alignment tools have generally been developed to operate on [[database schema]]s,&lt;ref&gt;J. Berlin and A. Motro. 2002. [http://www.dit.unitn.it/~accord/RelatedWork/Matching/Berlin_caise02.pdf Database Schema Matching Using Machine Learning with Feature Selection]. Proc. of the 14th International Conference on Advanced Information Systems Engineering, pp. 452-466&lt;/ref&gt; [[XML schema]]s,&lt;ref name="coma"&gt;D. Aumueller, H. Do, S. Massmann, E. Rahm. 2005. [http://www.dit.unitn.it/~p2p/RelatedWork/Matching/COMA++-SIGMOD05.pdf Schema and ontology matching with COMA++]. Proc. of the 2005 International Conference on Management of Data, pp. 906-908&lt;/ref&gt; [[Taxonomy (general)|taxonomies]],&lt;ref&gt;S. Ponzetto, R. Navigli. 2009. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf "Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia"]. Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009), Pasadena, California, pp. 2083-2088.&lt;/ref&gt; [[formal language]]s, [[entity-relationship model]]s,&lt;ref&gt;A. H. Doan, A. Y. Halevy. [http://pages.cs.wisc.edu/~anhai/papers/si-survey-db-community.pdf Semantic integration research in the database community: A brief survey]. AI magazine, 26(1), 2005&lt;/ref&gt; [[dictionary|dictionaries]], and other label frameworks. They are usually converted to a graph representation before being matched. 
Since the emergence of the Semantic Web, such graphs can be represented in the [[Resource Description Framework]] line of languages by triples of the form &lt;subject, predicate, object&gt;, as illustrated in the [[Notation 3]] syntax.
In this context, aligning ontologies is sometimes referred to as "ontology matching".

The problem of Ontology Alignment has been tackled recently by trying to compute matching first and mapping (based on the matching) in an automatic fashion. Systems like [[DSSim]], X-SOM&lt;ref name="curino-xsom2007"&gt;{{cite journal|author=Carlo A. Curino and Giorgio Orsi and Letizia Tanca |title=X-SOM: A Flexible Ontology Mapper |url=http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |journal=International Workshop on Semantic Web Architectures for Enterprises (SWAE'07) in conjunction with the 18th International Conference on Database and Expert Systems Applications (DEXA'07) |year=2007 |format= |deadurl=yes |archiveurl=https://web.archive.org/web/20120213104823/http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |archivedate=February 13, 2012 }}&lt;/ref&gt; or COMA++ obtained at the moment very high precision and recall.&lt;ref name="coma" /&gt; The [http://oaei.ontologymatching.org Ontology Alignment Evaluation Initiative] aims to evaluate, compare and improve the different approaches.

More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

=== Formal Definition ===
Given two ontologies &lt;math&gt;i=\langle C_{i}, R_{i}, I_{i}, A_{i}\rangle&lt;/math&gt; and &lt;math&gt;j=\langle C_{j}, R_{j}, I_{j}, A_{j}\rangle&lt;/math&gt;{{clarify|Give a (link to a) formal definition of 'ontology' first, such that the meaning of C, R, I, and A is explained.|date=January 2017}} we can define different type of (inter-ontology) relationships among their terms. 
Such relationships will be called, all together, alignments and can be categorized among different dimensions:

* similarity vs logic: this is the difference between matchings (predicating about the [[semantic similarity|similarity]] of ontology terms), and mappings ([[logical axiom]]s, typically expressing [[logical equivalence]] or inclusion among ontology terms)
* atomic vs complex: whether the alignments we considered are [[bijection|one-to-one]], or can involve more terms in a query-like formulation (e.g., [[data integration|LAV/GAV]] mapping)
* homogeneous vs heterogeneous: do the alignments predicate on terms of the same type (e.g., classes are related only to classes, individuals to individuals, etc.) or we allow heterogeneity in the relationship?
* type of alignment: the semantics associated to an alignment. It can be [[Hierarchy#Subsumptive containment hierarchy|subsumption]], [[logical equivalence|equivalence]], [[disjointness]], [[part-of]] or any user-specified relationship.

Subsumption, atomic, homogeneous alignments are the building blocks to obtain richer alignments, and have a well defined semantics in every Description Logic. 
Let's now introduce more formally ontology matching and mapping.

An atomic homogeneous '''matching''' is an alignment that carries a similarity degree &lt;math&gt;s\in [0,1]&lt;/math&gt;, describing the similarity of two terms of the input ontologies &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;.
Matching can be either ''computed'', by means of heuristic algorithms, or ''[[inference|inferred]]'' from other matchings.

Formally we can say that, a matching is a quadruple &lt;math&gt;m=\langle id, t_{i}, t_{j}, s\rangle&lt;/math&gt;, where &lt;math&gt;t_{i}&lt;/math&gt; and &lt;math&gt;t_{j}&lt;/math&gt; are homogeneous ontology terms, &lt;math&gt;s&lt;/math&gt; is the similarity degree of &lt;math&gt;m&lt;/math&gt;. 
A (subsumption, homogeneous, atomic) mapping is defined as a pair &lt;math&gt;\mu=\langle t_{i}, t_{j}\rangle&lt;/math&gt;, where &lt;math&gt;t_{i}&lt;/math&gt; and &lt;math&gt;t_{j}&lt;/math&gt; are homogeneous ontology terms.

==Cognitive Science==

For [[cognitive scientist]]s interested in ontology alignment, the "concepts" are nodes in a [[semantic network]] that reside in brains as "conceptual systems."  The focal question is: if everyone has unique experiences and thus different semantic networks, then how can we ever understand each other?  This question has been addressed by a model called ABSURDIST (Aligning Between Systems Using Relations Derived Inside Systems for Translation). Three major dimensions have been identified for similarity as equations for "internal similarity, external similarity, and mutual inhibition."&lt;ref&gt;R. Goldstone and B. Rogosky. 2002. [http://courses.media.mit.edu/2003spring/mas963/goldstone.pdf Using relations within conceptual systems to translate across conceptual systems]. Cognition 84, pp. 295&#8211;320.&lt;/ref&gt;

Ontology alignment is closely related to [[analogy formation]], where "concepts" are variables in logic expressions.

==Ontology alignment methods==
Two sub research fields have emerged in recent years in ontology mapping, namely monolingual ontology mapping and cross-lingual ontology mapping. The former refers to the mapping of ontologies in the same natural language, whereas the latter refers to "the process of establishing relationships among ontological resources from two or more independent ontologies where each ontology is labelled in a different natural language".&lt;ref&gt;Bo Fu, Rob Brennan, Declan O'Sullivan, A Configurable Translation-Based Cross-Lingual Ontology Mapping System to adjust Mapping Outcomes. Journal of Web Semantics, Volume 15, 15-36, ISSN 1570-8268, 2012 [http://www.sciencedirect.com/science/article/pii/S1570826812000704].&lt;/ref&gt; Existing matching methods in monolingual ontology mapping are discussed in Euzenat and Shvaiko (2007).&lt;ref name="Euzenat Shvaiko"/&gt; Current approaches to cross-lingual ontology mapping are presented in Fu et al. (2011).&lt;ref&gt;Fu B., Brennan R., O'Sullivan D., Using Pseudo Feedback to Improve Cross-Lingual Ontology Mapping [http://www.springerlink.com/content/a214858426kgm750/]. In Proceedings of the 8th Extended Semantic Web Conference (ESWC 2011), LNCS 6643, pp.336-351, Heraklion, Greece, May 2011.&lt;/ref&gt;

==Philosophy==

For philosophers, much like cognitive scientists, the interest is in the nature of "understanding."  The roots of discourse, however, may be traced to [[radical interpretation]].

==Visualization Tools (links obsolete)==
*[http://www.mondeca.com/content/download/718/6964/file/ITM_ALIGN_en.pdf ITM Align: semi-automated ontology alignment]
*[http://cs.uga.edu/~uthayasa/Optima/Optima.html Optima: A Visual Ontology Alignment Tool]
*[http://www.stanford.edu/~sfalc/cogz/cogz.html CogZ: Cognitive Support and Visualization for Human-Guided Mapping Systems]
*[http://agreementmaker.org AgreementMaker: Efficient Matching for Large Real-World Schemas and Ontologies]
*[http://bio-mixer.appspot.com/ Biomixer]: A Web-based Collaborative Ontology Visualization Tool.
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6882028 SDI(Semantic Data Integration) Tool]: A Semantic Mapping Representation and Generation Tool Using UML for System Engineers

==See also==
* [[Ontology (computer science)]]
* [[Rule Interchange Format]] (RIF)
* [[Data conversion]]
* [[Semantic Integration]]
* [[Semantic matching]]
* [[Minimal mappings|Minimal Mappings]]
* [[Semantics|Interpretation]] "An interpretation can be the part of a presentation or portrayal of information altered in order to conform to a specific set of symbols."
* [[Graph isomorphism]]
* [[Unification (computer science)]] (as [[Semantic unification]])
* [[Semantic integration]]

==References==
{{Reflist|colwidth=35em}}

== Further reading ==
*[http://www.ontologymatching.org/publications.html Collection of surveys and research papers related to ontology mapping, matching, and alignment]
* [http://www.atl.external.lmco.com/projects/ontology/ The Ontology Alignment Source]
* [http://cognitrn.psych.indiana.edu/rgoldsto/pdfs/cogsci2002.pdf ABSURDIST]
* [http://ontologymatching.org/publications.html Ontologymatching.org: Surveys, Approaches, and Themes]
* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data]
* [http://sites.google.com/site/bschopman/master-thesis/master.pdf Instance-based Ontology Matching by Instance Enrichment]
* Noy, N. F. (2004). "Semantic integration: a survey of ontology-based approaches." SIGMOD Rec. 33(4): 65-70.

[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Knowledge representation]]</text>
      <sha1>rtk9ywu7w3r0ahtwds0pxy5mq8jamwu</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic data model</title>
    <ns>0</ns>
    <id>19558680</id>
    <revision>
      <id>741600527</id>
      <parentid>700444944</parentid>
      <timestamp>2016-09-28T14:36:01Z</timestamp>
      <contributor>
        <ip>168.215.73.114</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8765" xml:space="preserve">[[File:A2 4 Semantic Data Models.svg|thumb|320px|Semantic data models.&lt;ref name ="FIPS184"&gt;[http://www.itl.nist.gov/fipspubs/idef1x.doc FIPS Publication 184] released of IDEF1X by the Computer Systems Laboratory of the National Institute of Standards and Technology (NIST). 21 December 1993.&lt;/ref&gt;]]
A '''semantic data model''' in [[software engineering]] has various meanings: 
# It is a [[conceptual data model]] in which semantic information is included. This means that the model describes the meaning of its instances. Such a semantic [[data model]] is an abstraction that defines how the stored [[symbol]]s (the instance data) relate to the real world.&lt;ref name ="FIPS184"/&gt;
# It is a [[conceptual data model]] that includes the capability to express information that enables parties to the information exchange to interpret meaning (semantics) from the instances, without the need to know the meta-model. Such semantic models are fact oriented (as opposed to object oriented). Facts are typically expressed by [[binary relations]] between [[data]] elements, whereas higher order relations are expressed as collections of binary relations. Typically binary relations have the form of triples: Object-RelationType-Object. For example: the Eiffel Tower &lt;is located in&gt; Paris.
Typically the instance data of semantic data models explicitly include the kinds of relationships between the various data elements, such as &lt;is located in&gt;. To interpret the meaning of the facts from the instances it is required that the meaning of the kinds of relations (relation types) be known. Therefore, semantic data models typically standardise such relation types. This means that the second kind of semantic data models enable that the instances express facts that include their own meaning. 
The second kind of semantic data models are usually meant to create semantic databases. The ability to include meaning in semantic databases facilitates building [[distributed database]]s that enable applications to interpret the meaning from the content. This implies that semantic databases can be integrated when they use the same (standard) relation types. This also implies that in general they have a wider applicability than relational or object oriented databases.

== Overview ==
The logical data structure of a [[database management system]] (DBMS), whether [[Hierarchical model|hierarchical]], [[Network model|network]], or [[Relational model|relational]], cannot totally satisfy the [[Requirements analysis|requirements]] for a conceptual definition of data, because it is limited in scope and biased toward the implementation strategy employed by the DBMS. Therefore, the need to define data from a [[Three schema approach|conceptual view]] has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure. The real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.&lt;ref name ="FIPS184"/&gt;

According to Klas and Schrefl (1995), the "overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the [[Artificial Intelligence]] field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations".&lt;ref&gt;Wolfgang Klas, Michael Schrefl (1995). "Semantic data modeling" In: ''Metaclasses and Their Application''. Book Series Lecture Notes in Computer Science. Publisher Springer Berlin / Heidelberg. Volume Volume 943/1995.&lt;/ref&gt;

== History ==
The need for semantic data models was first recognized by the U.S. Air Force in the mid-1970s as a result of the [[Integrated Computer-Aided Manufacturing]] (ICAM) Program. The objective of this program was to increase manufacturing productivity through the systematic application of computer technology. The ICAM Program identified a need for better analysis and communication techniques for people involved in improving manufacturing productivity. As a result, the ICAM Program developed a series of techniques known as the IDEF (ICAM Definition) Methods which included the following:&lt;ref name ="FIPS184"/&gt;
* [[IDEF0]] used to produce a &#8220;function model&#8221; which is a structured representation of the activities or processes within the environment or system.
* [[IDEF1]] used to produce an &#8220;information model&#8221; which represents the structure and semantics of information within the environment or system.
** [[IDEF1X]] is a semantic data modeling technique. It is used to produce a graphical information model which represents the structure and semantics of information within an environment or system. Use of this standard permits the construction of semantic data models which may serve to support the management of data as a resource, the integration of information systems, and the building of computer databases.
* [[IDEF2]] used to produce a &#8220;dynamics model&#8221; which represents the time varying behavioral characteristics of the environment or system.

During the 1990s the application of semantic modelling techniques resulted in the semantic data models of the second kind. An example of such is the semantic data model that is standardised as [[ISO 15926]]-2 (2002), which is further developed into the semantic modelling language [[Gellish]] (2005). The definition of the Gellish language is documented in the form of a semantic data model. Gellish itself is a semantic modelling language, that can be used to create other semantic models. Those semantic models can be stored in Gellish Databases, being semantic databases.

== Applications ==
A semantic data model can be used to serve many purposes. Some key objectives include:&lt;ref name ="FIPS184"/&gt;
* Planning of Data Resources: A preliminary data model can be used to provide an overall view of the data required to run an enterprise. The model can then be analyzed to identify and scope projects to build shared data resources.
* Building of Shareable Databases: A fully developed model can be used to define an application independent view of data which can be validated by users and then transformed into a physical database design for any of the various DBMS technologies. In addition to generating databases which are consistent and shareable, development costs can be drastically reduced through data modeling.
* Evaluation of Vendor Software: Since a data model actually represents the infrastructure of an organization, vendor software can be evaluated against a company&#8217;s data model in order to identify possible inconsistencies between the infrastructure implied by the software and the way the company actually does business.
* Integration of Existing Databases: By defining the contents of existing databases with semantic data models, an integrated data definition can be derived. With the proper technology, the resulting conceptual schema can be used to control transaction processing in a distributed database environment. The U.S. Air Force Integrated Information Support System (I2S2) is an experimental development and demonstration of this type of technology applied to a heterogeneous DBMS environment.

== See also ==
* [[Conceptual schema]]
* [[Object-role modeling]]
* [[Entity-relationship model]]
* [[Information model]]
* [[Relational Model/Tasmania]]
* [[Three schema approach]]
* [[QuakeSim]]

== References ==
{{NIST-PD}}
{{reflist}}

== Further reading ==
* [http://hpdrc.cs.fiu.edu/library/books/datades-book/ Database Design - The Semantic Modelling Approach]
* Johan ter Bekke (1992). ''Semantic Data Modeling''. Prentice Hall.
* Alfonso F. Cardenas and Dennis McLeod (1990). ''Research Foundations in Object-Oriented and Semantic Database Systems''. Prentice Hall.
* Peter Gray, Krishnarao G. Kulkarni and, Norman W. Paton (1992). ''Object-Oriented Databases: A Semantic Data Model Approach''. Prentice-Hall International Series in Computer Science.
* Michael Hammer and Dennis McLeod (1978). "The Semantic Data Model: a Modeling Mechanism for Data Base Applications." In: ''Proc. ACM SIGMOD Int&#8217;l. Conf. on Management of Data''. Austin, Texas, May 31 - June 2, 1978, pp.&amp;nbsp;26&#8211;36.

== External links ==
* [http://www.jhterbekke.net/SemanticDataModeling.html Semantic Data Modeling] Johan ter Bekke tribute site.

{{Data model}}

{{DEFAULTSORT:Semantic Data Model}}
[[Category:Data modeling]]
[[Category:Systems analysis]]
[[Category:Knowledge representation]]</text>
      <sha1>hsepseqn59qia362hscd2phj4wf8az5</sha1>
    </revision>
  </page>
  <page>
    <title>Procedural reasoning system</title>
    <ns>0</ns>
    <id>8233911</id>
    <revision>
      <id>753455537</id>
      <parentid>749301501</parentid>
      <timestamp>2016-12-07T07:17:16Z</timestamp>
      <contributor>
        <username>Jessicapierce</username>
        <id>2003421</id>
      </contributor>
      <minor />
      <comment>minor copy edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9990" xml:space="preserve">In [[artificial intelligence]], a '''procedural reasoning system''' ('''PRS''') is a framework for constructing real-time [[Reasoning system|reasoning systems]] that can perform complex tasks in dynamic environments. It is based on the notion of a [[rational agent]] or [[intelligent agent]] using the [[belief&#8211;desire&#8211;intention software model]].

A user application is predominately defined, and provided to a PRS system is a set of ''knowledge areas''.  Each knowledge area is a piece of [[procedural knowledge]] that specifies how to do something, e.g., how to navigate down a corridor, or how to plan a path (in contrast with [[robotic architectures]] where the [[computer programmer|programmer]] just provides a model of what the states of the world are and how the agent's primitive actions affect them).  Such a program, together with a PRS [[interpreter (computing)|interpreter]], is used to control the agent.

The interpreter is responsible for maintaining beliefs about the world state, choosing which goals to attempt to achieve next, and choosing which knowledge area to apply in the current situation.  How exactly these operations are performed might depend on domain-specific [[metaknowledge|meta-level]] knowledge areas.  Unlike traditional [[computer planning|AI planning]] systems that generate a complete plan at the beginning, and replan if unexpected things happen, PRS interleaves planning and doing actions in the world.  At any point, the system might only have a partially specified plan for the future.

PRS is based on the [[BDI software agent|BDI]] or belief&#8211;desire&#8211;intention framework for intelligent agents.  Beliefs consist of what the agent believes to be true about the current state of the world, desires consist of the agent's goals, and intentions consist of the agent's current plans for achieving those goals.  Furthermore, each of these three components is typically ''explicitly'' represented somewhere within the memory of the PRS agent at runtime, which is in contrast to purely reactive systems, such as the [[subsumption architecture]].

== History ==
The PRS concept was developed by the [[Artificial Intelligence Center]] at [[SRI International]] during the 1980s, by many workers including [[Michael Georgeff]], [[Amy L. Lansky]], and [[Fran&#231;ois F&#233;lix Ingrand]]. Their framework was responsible for exploiting and popularizing the BDI model in software for control of an [[intelligent agent]]. The seminal application of the framework was a fault detection system for the reaction control system of the [[NASA]] [[Space Shuttle Discovery]]. Development on this PRS continued at the [[Australian Artificial Intelligence Institute]] through to the late 1990s, which lead to the development of a [[C++]] implementation and extension called [[distributed multi-agent reasoning system|dMARS]].

== Architecture ==
[[Image:PRS.gif|thumb|Depiction of the PRS architecture]]
The system architecture of SRI's PRS includes the following components:
* '''Database''' for beliefs about the world, represented using first order predicate calculus.
* '''Goals''' to be realized by the system as conditions over an interval of time on internal and external state descriptions (desires).
* '''Knowledge areas''' (KAs) or plans that define sequences of low-level actions toward achieving a goal in specific situations.
* '''Intentions''' that include those KAs that have been selected for current and eventual execution.
* '''Interpreter''' or inference mechanism that manages the system.

== Features ==
SRI's PRS was developed for embedded application in dynamic and real-time environments. As such it specifically addressed the limitations of other contemporary control and reasoning architectures like [[expert system]]s and the [[blackboard system]]. The following define the general requirements for the development of their PRS:&lt;ref&gt;
{{cite journal
 | doi = 10.1109/64.180407
 | last = Ingrand
 | first = F.
 |author2=M. Georgeff |author3=A Rao
  | title = An architecture for real-time reasoning and system control
 | journal = IEEE Expert: Intelligent Systems and Their Applications
 | volume = 7
 | issue = 6
 | year = 1992
 | pages = 34&#8211;44 
 | publisher = IEEE Press
 | url = http://portal.acm.org/citation.cfm?id=629535.629890 }}
&lt;/ref&gt;

* asynchronous event handling
* guaranteed reaction and response types
* procedural representation of knowledge
* handling of multiple problems
* reactive and goal-directed behavior
* focus of attention
* reflective reasoning capabilities
* continuous embedded operation
* handling of incomplete or inaccurate data
* handling of transients
* modeling delayed feedback
* operator control

== Applications ==
The seminal application of SRI's PRS was a monitoring and fault detection system for the reaction control system (RCS) on the NASA space shuttle.&lt;ref&gt;
{{cite conference
  | last = Georgeff
  | first = M. P.
  |author2=F. F. Ingrand
  | title = Real-time reasoning: the monitoring and control of spacecraft systems
  | booktitle = Proceedings of the sixth conference on Artificial intelligence applications
  | year = 1990
  | pages = 198&#8211;204
  | url = http://portal.acm.org/citation.cfm?id=96782 }}
&lt;/ref&gt; The RCS provides propulsive forces from a collection of jet thrusters and controls altitude of the space shuttle. A PRS-based fault diagnostic system was developed and tested using a simulator. It included over 100 KAs and over 25 meta level KAs. RCS specific KAs were written by space shuttle mission controllers. It was implemented on the [[Symbolics]] 3600 Series [[LISP]] machine and used multiple communicating instances of PRS. The system maintained over 1000 facts about the RCS, over 650 facts for the forward RCS alone and half of which are updated continuously during the mission. A version of the PRS was used to monitor the reaction control system on the [[NASA]] [[Space Shuttle Discovery]].

PRS was tested on [[Shakey the robot]] including navigational and simulated jet malfunction scenarios based on the space shuttle.&lt;ref&gt;
{{cite conference
  | last = Georgeff
  | first = M. P.
  |author2=A. L. Lansky
  | title = Reactive reasoning and planning
  | booktitle = Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87)
  | year = 1987
  | pages = 198&#8211;204
  | url = http://www.ai.sri.com/pubs/files/1364.pdf 
  | work = [[Artificial Intelligence Center]]
  | publisher = [[SRI International]] }}
&lt;/ref&gt; Later applications included a network management monitor called the Interactive Real-time Telecommunications Network Management System (IRTNMS) for [[Telecom Australia]].&lt;ref&gt;
{{cite conference
  | last = Rao
  | first = Anand S.
  |author2=Michael P. Georgeff 
  | title = Intelligent Real-Time Network Management
  | booktitle = Australian Artificial Intelligence Institute, Technical Note 15
  | year = 1991
  | citeseerx = 10.1.1.48.3297 }}
&lt;/ref&gt;

== Extensions ==
The following list the major implementations and extensions of the PRS architecture.&lt;ref&gt;
{{cite conference
  | last = Wobcke
  | first = W. R.
  | title = Reasoning about BDI Agents from a Programming Languages Perspective
  | booktitle = Proceedings of the AAAI 2007 Spring Symposium on Intentions in Intelligent Systems
  | year = 2007
  | url = http://www.cse.unsw.edu.au/~wobcke/papers/ss.07.pdf }}
&lt;/ref&gt;
* UM-PRS &lt;ref&gt;[http://www.marcush.net/IRS/irs_downloads.html]&lt;/ref&gt;
* OpenPRS (formerly C-PRS and Propice) &lt;ref&gt;[http://www.laas.fr/~felix/PRS]&lt;/ref&gt; &lt;ref&gt;[https://softs.laas.fr/openrobots/wiki/openprs]&lt;/ref&gt;
* [[AgentSpeak]]
* [[Distributed Multi-Agent Reasoning System]] (dMARS)
* JAM &lt;ref&gt;[http://www.marcush.net/IRS/irs_downloads.html]&lt;/ref&gt;
* [[JACK Intelligent Agents]]
* SRI Procedural Agent Realization Kit (SPARK) &lt;ref&gt;[http://www.ai.sri.com/~spark/]&lt;/ref&gt;
* PRS-CL &lt;ref&gt;[http://www.ai.sri.com/~prs/]&lt;/ref&gt;

== See also ==
* [[Distributed multi-agent reasoning system]]
* [[JACK Intelligent Agents]]
* [[Belief-desire-intention software model]]
* [[Intelligent agent]]

== References ==
{{reflist}}

==Further reading==
* M.P. Georgeff and A.L. Lansky. "A system for reasoning in dynamic domains: Fault diagnosis on the space shuttle" Technical Note 375, Artificial Intelligence Center, SRI International, 1986.
* Michael P. Georgeff, Amy L. Lansky, Marcel J. Schoppers. "[http://www.ai.sri.com/pubs/files/579.pdf Reasoning and Planning in Dynamic Domains: An Experiment with a Mobile Robot]" Technical Note 380, Artificial Intelligence Center, SRI International, 1987.
* M. Georgeff, and A. L. Lansky (1987). [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1457907 Procedural knowledge].  Proceedings of the IEEE 74(10):1383&#8211;1398, IEEE Press.
* Georgeff, Michael P.; Ingrand, Francois Felix. "[http://ntrs.nasa.gov/search.jsp?R=124384&amp;id=4&amp;as=false&amp;or=false&amp;qs=Ns%3DArchiveName%257c0%26N%3D4294823185 Research on procedural reasoning systems]" Final Report &#8211; Phase 1, Artificial Intelligence Center, SRI International, 1988.
* Michael P. Georgeff and Fran&#231;ois F&#233;lix Ingrand "[http://www.laas.fr/~felix/download.php/ijcai89.pdf Decision-Making in an Embedded Reasoning System]" Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit (Michigan), August 1989.
* K. L. Myers, [http://www.ai.sri.com/~prs/prs-manual.pdf User Guide for the Procedural Reasoning System] Technical Report, Artificial Intelligence Center, Technical Report, SRI International, Menlo Park, CA, 1997
* [http://www.sti.nasa.gov/tto/Spinoff2006/ch_2.html A Match Made in Space] Spinoff, NASA, 2006

== External links ==
* [http://www.ai.sri.com/~prs/ PRS-CL: A Procedural Reasoning System] An extension to PRS maintained by SRI International

[[Category:Knowledge representation]]
[[Category:Cognitive architecture]]
[[Category:Agent-based software]]
[[Category:Multi-agent systems]]
[[Category:Agent-oriented programming languages]]
[[Category:Agent-based programming languages]]
[[Category:SRI International software]]</text>
      <sha1>8m09j43ht5k9rllvgecnj3x1mpkgue0</sha1>
    </revision>
  </page>
  <page>
    <title>Figurative system of human knowledge</title>
    <ns>0</ns>
    <id>464119</id>
    <revision>
      <id>755233275</id>
      <parentid>737857487</parentid>
      <timestamp>2016-12-17T00:17:14Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>/* External links */ English link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12594" xml:space="preserve">[[File:ENC SYSTEME FIGURE.jpeg|right|200px|thumb|[[Classification chart]] with the original "figurative system of human knowledge" tree, in French.]]

The '''"figurative system of human knowledge"''', sometimes known as '''the tree of Diderot and d'Alembert''', was a tree developed to represent the structure of [[knowledge]] itself, produced for the ''[[Encyclop&#233;die]]'' by [[Jean le Rond d'Alembert]] and [[Denis Diderot]].

The tree was a [[Taxonomy (general)|taxonomy]] of human knowledge, inspired by [[Francis Bacon]]'s ''[[The Advancement of Learning]]''. The three main branches of knowledge in the tree are: "Memory"/[[History]], "Reason"/[[Philosophy]], and "Imagination"/[[Poetry]].

Notable is the fact that [[theology]] is ordered under 'Philosophy'. The historian [[Robert Darnton]] has argued that this categorization of [[religion]] as being subject to human reason, and not a source of knowledge in and of itself ([[revelation]]), was a significant factor in the controversy surrounding the work.&lt;ref&gt;Robert Darnton, "Philosophers Trim the Tree of Knowledge: The Epistemological Strategy of the ''Encyclopedie''," ''The Great Cat Massacre and Other Episodes in French Cultural History'' (New York: Basic Books, Inc., 1984), 191-213.&lt;/ref&gt;  Additionally notice that 'Knowledge of God' is only a few nodes away from 'Divination' and 'Black Magic'.

The original version, in [[French (language)|French]], can be seen in the graphic on the right. An [http://www.hti.umich.edu/d/did/tree.html image of the diagram with English translations superimposed over the French text] is available. Another example of English translation of the tree is available in literature (see the reference by Schwab). Below is a version of it rendered in [[English (language)|English]] as a bulleted outline.

== ''The Tree of Diderot and d'Alembert'' ==
'''"Detailed System of Human Knowledge"'''
from the [[Encyclop&#233;die]].
* [[Understanding]]
:* [[Memory]].
::* [[History]].
:::* [[Sacred history|Sacred]] (History of [[Prophet]]s).
:::* [[History of Christianity|Ecclesiastical]].
:::* [[Civilization#History|Civil]], [[Ancient history|Ancient]] and [[Modern history|Modern]].
::::* [[Civilization#History|Civil History]], properly said. ''(See also: [[Civil society#History|History of civil society]])''
::::* [[Literary History]].
:::::* [[Memoirs]].
:::::* [[Antiquities]]. ''(See also: [[Classical antiquity]])''
:::::* Complete Histories.
:::* [[Natural history|Natural]].
::::* Uniformity of Nature. ''(See: [[Uniformitarianism]])''
:::::* [[Cosmology|Celestial History]].
:::::* History...
::::::* of [[Meteoroid#History|Meteors]].
::::::* of the [[History of the Earth|Earth]] and the [[World Ocean|Sea]] ''(See also: [[Origin of water on Earth]])''
::::::* of [[Minerals]]. ''(See also:  [[Geological history of Earth]])''
::::::* of [[Vegetable]]s. ''(See also: [[History of agriculture]])''
::::::* of [[Animal]]s.  ''(See also: [[Evolutionary history of life]])''
::::::* of the [[Chemical element|Elements]]. ''(See also: [[Classical element]], [[History of alchemy]], and [[History of chemistry]])''
::::* Deviations of Nature.
:::::* [[Celestial object|Celestial Wonders]].
:::::* [[Meteoroid#Frequency of large meteors|Large Meteors]]. ''(See also: [[Asteroid]]s)''
:::::* Wonders of Land and Sea. ''(See: [[Wonders of the World]])''
:::::* [[Mineral#Other properties|Monstrous Mineral]]s.
:::::* Monstrous Vegetables. ''(See: [[Largest organisms#Plants|Largest plants]], [[Poisonous plant]]s, and [[Carnivorous plant]]s)''
:::::* Monstrous Animals. (See: ''[[Largest organisms#Animals|Largest animals]] and [[Predator]]s)''
:::::* Wonders of the Elements. ''(See: [[Natural disaster]]s)''
::::* Uses of Nature (See ''[[Technology]] and [[Applied science]]s)''
:::::* Arts, [[Craft]]s, Manufactures.
::::::* Work and Uses of [[Gold]] and [[Silver]].
:::::::* [[Mint (coin)|Minting]].
:::::::* [[Goldsmith]].
:::::::* Gold Spinning.
:::::::* Gold Drawing.
:::::::* [[Silversmith]]
:::::::* [[Planishing|Planisher]], etc.
::::::* Work and Uses of Precious Stones.
:::::::* [[Lapidary]].
:::::::* [[Diamond cutting]].
:::::::* [[Jewellery|Jeweler]], etc.
::::::* Work and Uses of [[Iron]].
:::::::* Large [[Forge|Forges]].
:::::::* [[Locksmithing|Locksmith]].
:::::::* Tool Making.
:::::::* Armorer.
:::::::* Gun Making, etc.
::::::* Work and Uses of [[Glass]].
:::::::* [[Glass|Glassmaking]].
:::::::* [[Plate-Glass|Plate-Glassmaking]].
:::::::* [[Mirror#Manufacture|Mirror Making]].
:::::::* [[Optician]].
:::::::* [[Glazier]], etc.
::::::* Work and Uses of Skin.
:::::::* [[Tanner (occupation)|Tanner]].
:::::::* [[Chamois leather|Chamois Maker]].
:::::::* Leather Merchant.
:::::::* [[Glove]] Making, etc.
::::::* Work and Uses of [[Stonemasonry|Stone]], [[Plaster#Uses|Plaster]], [[Slate#Uses|Slate]], etc.
:::::::* Practical [[Architecture]].
:::::::* Practical [[Sculpture]].
:::::::* [[Masonry|Mason]].
:::::::* [[Tiler]], etc.
::::::* Work and Uses of [[Silk#Uses|Silk]].
:::::::* Spinning.
:::::::* Milling.
:::::::* Work like.
:::::::* [[Velvet]].
:::::::* Brocaded Fabrics, etc.
::::::* Work and Uses of [[Wool]].
:::::::* Cloth-Making.
:::::::* Bonnet-Making, etc.
::::::* Working and Uses, etc.
:* [[Reason]]
::* [[Philosophy]]
:::* General [[Metaphysics]], or [[Ontology]], or Science of Being in General, of Possibility, of [[Existence]], of Duration, etc.
:::* Science of [[God]].
::::* [[Natural Theology]].
::::* Revealed [[Theology]].
::::* Science of Good and Evil Spirits.
:::::* [[Divination]].
:::::* [[Black Magic]].
:::* Science of Man.
::::* [[Pneumatology]] or Science of the [[Soul]].
:::::* Reasonable.
:::::* Sensible.
::::* [[Logic]].
:::::* Art of [[Outline of thought|Thinking]].
::::::* [[Apprehension (understanding)|Apprehension]].
:::::::* Science of [[Idea]]s
::::::* [[Judgement]].
:::::::* Science of [[Proposition]]s.
::::::* [[Reasoning]].
:::::::* [[Inductive reasoning|Induction]].
::::::* [[Reasoning#Logical_reasoning_methods_and_argumentation|Method]].
:::::::* Demonstration.
::::::::* [[Analysis]].
::::::::* [[:wikt:-synthesis|Synthesis]].
:::::* Art of Remembering.
::::::* [[Memory]].
:::::::* Natural.
:::::::* [[Art of memory|Artificial]].
::::::::* Prenotion.
::::::::* Emblem.
::::::* Supplement to Memory.
:::::::* [[Writing]].
:::::::* [[Printing]].
::::::::* [[Alphabet]].
::::::::* Cipher.
:::::::::* Arts of [[Writing]], Printing, [[Reading (process)|Reading]], Deciphering.
::::::::::* [[Orthography]].
:::::* Art of [[Communication]]
::::::* Science of the Instrument of [[Discourse]].
:::::::* [[Grammar]].
::::::::* [[Sign]]s.
:::::::::* [[Gesture]].
::::::::::* [[Mime|Pantomime]].
::::::::::* Declamation.
:::::::::* Characters.
::::::::::* [[Ideogram]]s.
::::::::::* [[Hieroglyphics]].
::::::::::* [[Heraldry]] or Blazonry.
::::::::* [[Prosody (linguistics)|Prosody]].
::::::::* Construction.
::::::::* [[Syntax]].
::::::::* [[Philology]].
::::::::* Critique.
:::::::* [[Pedagogy]].
::::::::* [[Curriculum|Choice of Studies]].
::::::::* [[Teaching method|Manner of Teaching]].
::::::* Science of Qualities of [[Discourse]].
:::::::* [[Rhetoric]].
:::::::* Mechanics of [[Poetry]].
::::* [[Outline of ethics|Ethics]].
:::::* [[Contemporary ethics|General]].
::::::* General Science of [[Good and evil|Good and Evil]], of duties in general, of [[Virtue]], of the necessity of being Virtuous, etc.
:::::* [[Outline of ethics#Branches of ethics|Particular]].
::::::* Science of [[Law]]s or [[Jurisprudence]].
:::::::* [[Natural law|Natural]].
:::::::* [[Economic forces|Economic]]. ''(See also [[commercial law]])''
:::::::* [[Politics|Political]]. ''(See also [[political law]])''
::::::::* [[Domestic politics|Internal]] and [[International politics|External]]. ''(See also [[foreign policy]])''
::::::::* [[Commerce]] on Land and [[Maritime industry|Sea]].
:::* [[Natural science|Science of Nature]]
::::* [[Metaphysics]] of Bodies or, General Physics, of Extent, of Impenetrability, of Movement, of Word, etc.
::::* [[Outline of mathematics|Mathematics]].
:::::* [[Pure mathematics|Pure]].
::::::* [[Outline of arithmetic|Arithmetic]].
:::::::* [[Number|Numeric]].
:::::::* [[Algebra]].
::::::::* [[Elementary algebra|Elementary]].
::::::::* [[Infinitesimal]].
:::::::::* [[Differential algebra|Differential]].
:::::::::* [[Integral]].
::::::* [[Outline of geometry|Geometry]].
:::::::* Elementary (Military Architecture, Tactics).
:::::::* Transcendental (Theory of Courses).
:::::* Mixed.
::::::* [[Mechanics]].
::::::::* [[Statics]].
:::::::::* Statics, properly said.
:::::::::* [[Hydrostatics]].
::::::::* [[Dynamics (mechanics)|Dynamics]].
:::::::::* Dynamics, properly said.
:::::::::* [[Ballistics]].
:::::::::* [[Hydrodynamics]].
::::::::::* [[Hydraulics]].
::::::::::* [[Navigation]], Naval Architecture.
::::::* Geometric [[Astronomy]].
:::::::* [[Cosmography]].
::::::::* [[Celestial cartography|Uranography]].
::::::::* [[Geography]].
::::::::* [[Hydrography]].
:::::::* [[Chronology]].
:::::::* [[Gnomon]]ics.
::::::* [[Optics]].
:::::::* Optics, properly said.
:::::::* [[Dioptrics]], Perspective.
:::::::* [[Catoptrics]].
::::::* [[Acoustics]].
::::::* [[Pneumatics]].
::::::* Art of Conjecture. [[probability|Analysis of Chance]].
:::::* Physicomathematics.
::::* Particular Physics.
:::::* [[Outline of zoology|Zoology]].
::::::* [[Anatomy]].
:::::::* Simple.
:::::::* [[Comparative anatomy|Comparative]].
::::::* [[Physiology]].
::::::* [[Outline of medicine|Medicine]].
:::::::* Hygiene.
::::::::* [[Hygiene]], properly said.
::::::::* Cosmetics (Orthopedics).
::::::::* Athletics (Gymnastics).
:::::::* Pathology.
:::::::* Semiotics.
:::::::* Treatment.
::::::::* Diete.
::::::::* [[Surgery]].
::::::::* Pharmacy.
::::::* [[Veterinary medicine|Veterinary Medicine]].
::::::* [[Horse care|Horse Management]].
::::::* [[Hunting]].
::::::* [[Outline of fishing|Fishing]].
::::::* [[Falconry]].
:::::* Physical [[Astronomy]].
::::::* [[Astrology]].
:::::::* Judiciary Astrology.
:::::::* Physical Astrology.
:::::* [[Meteorology]].
:::::* [[Cosmology]].
::::::* Uranology.
::::::* [[Aerology]].
::::::* [[Geology]].
::::::* [[Hydrology]].
:::::* [[Botany]].
::::::* [[Agriculture]].
::::::* [[Gardening]].
:::::* [[Mineralogy]].
:::::* [[Chemistry]].
::::::* Chemistry, properly said, ([[Pyrotechnics]], Dyeing, etc.).
::::::* [[Metallurgy]].
::::::* [[Alchemy]].
::::::* Natural Magic.
:* Imagination.
::* [[Poetry]].
:::* Sacred, Profane.
::::* Narrative.
:::::* [[Epic poetry|Epic Poem]]
:::::* [[Madrigal (poetry)|Madrigal]]
:::::* [[Epigram]]
:::::* [[Novel]], etc.
::::* [[Drama|Dramatic]]
:::::* [[Tragedy]]
:::::* [[Comedy]]
:::::* [[Pastoral]], etc.
::::* Parable
:::::* [[Allegory]]
(NOTE: THIS NEXT BRANCH SEEMS TO BELONG TO BOTH THE NARRATIVE AND DRAMATIC TREE AS DEPICTED BY THE LINE DRAWN CONNECTING THE TWO.)
::::* [[Outline of music|Music]]
:::::* [[Music theory|Theoretical]]
:::::* Practical ''(see also [[musical technique]])''
:::::** [[Instrumental]]
:::::** [[vocal music|Vocal]]
::::* [[Outline of painting|Painting]]
::::* [[Outline of sculpture|Sculpture]]
::::* [[Engraving]]

== See also ==
* [[Classification chart]]
* [[Instauratio magna]]
* [[Prop&#230;dia]]
* [[Pierre Mouchon]]

== References ==
{{reflist}}

== Further reading ==
* Robert Darnton, "Epistemological angst: From encyclopedism to advertising," in Tore Fr&#228;ngsmyr, ed., ''The structure of knowledge: classifications of science and learning since the Renaissance'' (Berkeley, CA: Office for the History of Science and Technology, University of California, Berkeley, 2001).
* Adams, David (2006) 'The Syst&#232;me figur&#233; des Connaissances humaines and the structure of Knowledge in the Encyclop&#233;die',  in Ordering the World, ed. Diana Donald and Frank O'Gorman, London: Macmillan, p.&amp;nbsp;190-215. 
* ''Preliminary discourse to the Encyclopedia of Diderot'', Jean Le Rond d'Alembert, translated by Richard N. Schwab, 1995. ISBN 0-226-13476-8

==External links==

* [http://quod.lib.umich.edu/d/did/tree.html The ''Tree'' translated into English]
* [http://artfl.uchicago.edu/cactus/ ESSAI D'UNE DISTRIBUTION G&#201;N&#201;ALOGIQUE DES SCIENCES ET DES ARTS PRINCIPAUX, published as a fold-out frontispiece in volume 1 of Pierre Mouchon, ''Table analytique et raisonn&#233;e des matieres contenues dans les XXXIII volumes in-folio du Dictionnaire des sciences, des arts et des m&#233;tiers, et dans son suppl&#233;ment'', Paris, Panckoucke 1780.]

{{DEFAULTSORT:Figurative System Of Human Knowledge}}
[[Category:Taxonomy]]
[[Category:Age of Enlightenment]]
[[Category:Trees (data structures)]]
[[Category:Knowledge representation]]</text>
      <sha1>2ni1slkgn6gv6qjg3a08pb6samuqoem</sha1>
    </revision>
  </page>
  <page>
    <title>Mental mapping</title>
    <ns>0</ns>
    <id>8090717</id>
    <revision>
      <id>756921340</id>
      <parentid>739537802</parentid>
      <timestamp>2016-12-27T17:40:40Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor />
      <comment>/* Background */ cite repair;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9052" xml:space="preserve">{{about|the geographical concept|the diagram|Mind map}}
In [[behavioral geography]], a '''mental map''' is a person's [[Perspective (cognitive)|point-of-view]] perception of their area of interaction. Although this kind of subject matter would seem most likely to be studied by fields in the [[social sciences]], this particular subject is most often studied by modern day [[geographers]]. They study it to determine [[Subjectivity|subjective]] qualities from the public such as personal preference and practical uses of geography like driving directions. [[Mass media]] also have a virtually direct effect on a person's mental map of the geographical world.&lt;ref&gt;[http://mentalmaps.info Mental Maps Resource Site]&lt;/ref&gt; The perceived geographical dimensions of a foreign nation (relative to one's own nation) may often be heavily influenced by the amount of time and relative news coverage that the news media may spend covering news events from that foreign region. For instance, a person might perceive a small island to be nearly the size of a continent, merely based on the amount of news coverage that he or she is exposed to on a regular basis.&lt;ref&gt;[http://geography.about.com/cs/culturalgeography/a/mentalmaps.htm Mental Maps on About.com]&lt;/ref&gt;

In [[Experimental psychology|psychology]], the term names the information maintained in the mind of an organism by means of which it may plan activities, select routes over previously traveled territories, etc. The rapid traversal of a familiar [[maze]] depends on this kind of mental map if scents or other markers laid down by the subject are eliminated before the maze is re-run.

==Background==
Mental maps are an outcome of the field of behavioral geography. The imagined maps are considered one of the first studies that intersected geographical settings with human action.&lt;ref name="Gregory"&gt;{{cite book|last=Gregory|first=Derek|title=Dictionary of Human Geography: Mental maps/Cognitive Maps|year=2009|publisher=Wiley-Blackwell|location=Hoboken|edition=5th |author2=Johnston, Rom |author3=Pratt, Geraldine |page=455}}&lt;/ref&gt;  The most prominent contribution and study of mental maps was in the writings of [[Kevin A. Lynch|Kevin Lynch]]. In ''[[The Image of the City]]'', Lynch used simple sketches of maps created from memory of an urban area to reveal five elements of the city; nodes, edges, districts, paths and landmarks.&lt;ref&gt;{{cite book|last=Lynch|first=Kevin|title=The Image of the City|year=1960|publisher=MIT Press|location=Cambridge MA}}&lt;/ref&gt;  Lynch claimed that &#8220;Most often our perception of the city is not sustained, but rather partial, fragmentary, mixed with other concerns. Nearly every sense is in operation, and the image is the composite of them all.&#8221; (Lynch, 1960, p 2.) The creation of a mental map relies on memory as opposed to being copied from a preexisting map or image. In ''The Image of the City'', Lynch asks a participant to create a map as follows: &#8220;Make it just as if you were making a rapid description of the city to a stranger, covering all the main features. We don&#8217;t expect an accurate drawing- just a rough sketch.&#8221; (Lynch 1960, p 141) In the field of human geography mental maps have led to an emphasizing of social factors and the use of social methods versus quantitative or positivist methods.&lt;ref name="Gregory" /&gt; Mental maps have often led to revelations regarding social conditions of a particular space or area. Haken and Portugali (2003) developed an information view, which 
argued that the face of the city is its information &lt;ref&gt;{{cite journal|last=Haken|first=Herman|author2=Portugali, Juval|title=The face of the city is its information|journal=Journal of Environmental Psychology|date=August 2003|volume=23|pages=385&#8211;408|doi=10.1016/s0272-4944(03)00003-3}}&lt;/ref&gt;
. Bin Jiang (2012) argued that the image of the city (or mental map) arises out of the scaling of city artifacts and locations.&lt;ref&gt;{{cite journal|last=Jiang|first=Bin|title=The image of the city out of the underlying scaling of city artifacts or locations|journal=Annals of the Association of American Geographers|year=2012|volume=103|pages= 1552&#8211;1566| doi = 10.1080/00045608.2013.779503 |arxiv=1209.1112}}&lt;/ref&gt; He addressed that why the image of city can be formed 
&lt;ref&gt;{{cite journal|last=Jiang|first=Bin|title=Why can the image of the city be formed| arxiv=1212.3703}}&lt;/ref&gt;
, and he even suggested ways of computing the image of the city, or more precisely the kind of collective image of the city, using increasingly available geographic information such as Flickr and Twitter
&lt;ref&gt;{{cite journal|last=Jiang|first=Bin|title=Computing the image of the city| arxiv=1212.0940}}&lt;/ref&gt;
.

==Research applications==
Mental maps have been used in a collection of spatial research. Many studies have been performed that focus on the quality of an environment in terms of feelings such as fear, desire and stress. A study by Matei et al. in 2001 used mental maps to reveal the role of media in shaping urban space in Los Angeles. The study used Geographic Information Systems (GIS) to process 215 mental maps taken from seven neighborhoods across the city. The results showed that people&#8217;s fear perceptions in Los Angeles are not associated with high crime rates but are instead associated with a concentration of certain ethnicities in a given area.&lt;ref&gt;{{cite journal|last=Matei|first=Sorin |author2=Ball-Rokeach, Sandra |author3=Qiu Linchuan, Jack|title=Fear and Misperception of Los Angeles Urban Space: A Spatial-Statistical Study of Communication-Shaped Mental Maps|journal=Communication Research|date=August 2001|volume=28|issue=4|pages=429&#8211;463|accessdate=4 November 2012|url=http://mentalmap.org/files/matei_fear_CR.pdf|doi=10.1177/009365001028004004}}&lt;/ref&gt;  The mental maps recorded in the study draw attention to these areas of concentrated ethnicities as parts of the urban space to avoid or stay away from. 
	
Mental maps have also been used to describe the urban experience of children. In a 2008 study by Olga den Besten mental maps were used to map out the fears and dislikes of children in Berlin and Paris. The study looked into the absence of children in today&#8217;s cities and the urban environment from a child&#8217;s perspective of safety, stress and fear.&lt;ref&gt;{{cite journal|last=Den Besten|first=Olga den|title=Local belonging and &#8216;geographies of emotions&#8217;: Immigrant children&#8217;s experience of their neighbourhoods in Paris and Berlin|journal=Childhood|date=May 2010|volume=17|issue=2|pages=181&#8211;195|url=http://chd.sagepub.com/content/17/2/181|accessdate=4 November 2012|doi=10.1177/0907568210365649}}&lt;/ref&gt;

Peter Gould and Rodney White have performed prominent analyses in the book &#8220;Mental Maps.&#8221; The book is an investigation into people&#8217;s spatial desires. The book asks of its participants: &#8220;Suppose you were suddenly given the chance to choose where you would like to live- an entirely free choice that you could make quite independently of the usual constraints of income or job availability. Where would you choose to go?&#8221; (Gould, 1974, p 15) Gould and White use their findings to create a surface of desire for various areas of the world. The surface of desire is meant to show people&#8217;s environmental preferences and regional biases.&lt;ref&gt;{{cite book|last=Gould|first=Peter|title=Mental Maps|year=1993|publisher=Rutledge|location=New York|author2=White, Rodney|page=93}}&lt;/ref&gt;

In an experiment done by [[Edward C. Tolman]], the development of a mental map was seen in rats.&lt;ref&gt;Goldstein, B. (2011). ''Cognitive Psychology: Connecting Mind, Research, and Everyday Experience--with coglab manual. (3rd ed.).'' Belmont, CA: Wadsworth.&lt;/ref&gt; A rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial mental map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.

The idea of mental maps is also used in strategic analysis. David Brewster, an Australian strategic analyst, has applied the concept to strategic conceptions of South Asia and Southeast Asia.  He argues that popular mental maps of where regions begin and end can have a significant impact on the strategic behaviour of states.&lt;ref&gt;{{cite web|author=David Brewster|url=https:// www.academia.edu/7697999/Dividing_Lines_Evolving_Mental_Maps_of_the_Bay_of_Bengal|title=Dividing Lines: Evolving Mental Maps of the Bay of Bengal. Retrieved 21 September 2014}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Knowledge representation]]
[[Category:Cognitive psychology]]
[[Category:Psychology terminology]]</text>
      <sha1>8x393tlyfzwqzcihnthgol145mgvm6g</sha1>
    </revision>
  </page>
  <page>
    <title>Integrated Operations in the High North</title>
    <ns>0</ns>
    <id>22713707</id>
    <revision>
      <id>641067747</id>
      <parentid>623363904</parentid>
      <timestamp>2015-01-05T10:11:53Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>per [[MOS:BOLDSYN]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8436" xml:space="preserve">{{Infobox Organization
|name         = Integrated Operations in the High North
|image        = IOHN logo small.gif
|size         = 200
|alt          = Logo for Integrated Operations in the High North.
|caption      = Logo for Integrated Operations in the High North.
|abbreviation = IOHN or IO High North
|formation    = 2008-05-06
|status       = Project at [[Det Norske Veritas|Det Norske Veritas (DNV)]]
|purpose      = Designing, implementing and testing a Digital Platform for the next generation of [[Integrated Operations]]
|location     = B&#230;rum, Norway
|region_served = Worldwide
|membership   = 22
|language     = English
|leader_title = Project Manager
|leader_name  = [http://www.linkedin.com/in/fredericverhelst Fr&#233;d&#233;ric Verhelst]
|main_organ   = Steering Committee
|affiliations = &lt;!-- if any --&gt;
|num_staff    = 
|num_volunteers =
|budget       = 
|website      = http://www.IOHN.org/
}}
'''Integrated Operations in the High North''' ('''IOHN''', '''IO High North or IO in the High North''') is a unique collaboration project that during a four-year period starting May 2008 is working on designing, implementing and testing a Digital Platform for what in the [[Upstream (oil industry)|Upstream Oil and Gas Industry]] is called the next or second generation of [[Integrated Operations]].&lt;ref&gt;
{{cite web 
|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf 
|title=Integrated Operations and the Oil and Gas Ontology 
|author=The [[Norwegian Oil Industry Association]] (OLF) and POSC Caesar Association (PCA) 
|accessdate=2009-05-06 
|date=2007-09-19
}}&lt;/ref&gt; 
The work on the Digital platform is focussed on capture, transfer and integration of [[Real-time data]] from the remote production installations to the decision makers. A risk evaluation across the whole chain is also included. The platform is based on [[open standards]] and enables a higher degree of [[interoperability]]. Requirements for the digital platform come from use cases defined within the [[Oil_and_gas_well_drilling#Drilling|Drilling]] and [[Oil_and_gas_well_drilling#Completion|Completion]], Reservoir and Production and Operations and Maintenance domains. The platform will subsequently be demonstrated through pilots within these three domains.&lt;ref name="IOHNsite"&gt;
{{cite web 
|url=http://trac.posccaesar.org/wiki/IOHN
|title=Short introduction to the Integrated Operations in the High North (IOHN) project
|author=Integrated Operations in the High North (IOHN) 
|accessdate=2009-05-07 
}}&lt;/ref&gt; 

This new platform is considered an important enabler for safe and sustainable operations in remote, vulnerable and hazardous areas such as the [[Arctic|High North]],&lt;ref&gt;
{{cite web 
|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html 
|title=Norway takes a leading role in next generation Integrated Operations 
|author=The [[Norwegian Oil Industry Association]] (OLF) 
|accessdate=2009-05-07 
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="Rigzone"&gt;
{{cite web 
|url=http://www.rigzone.com/news/article.asp?a_id=65883 
|title=Norway Takes Reign to Provide Next Generation Integrated Operations 
|author=Rigzone E&amp;P News
|accessdate=2009-05-08 
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="DEJ"&gt;
{{cite web 
|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&amp;PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7
|title=Norway developing next generation Integrated Operations 
|author=Digital Energy Journal
|accessdate=2009-05-08 
|date=2008-08-27
}}&lt;/ref&gt;&lt;ref name="EPMag"&gt;
{{cite web 
|url=http://www.epmag.com/Magazine/2008/12/item24047.php 
|title=Offshore R&amp;D pushes the limits 
|author=E&amp;P Magazine
|accessdate=2009-05-08 
|date=2008-12-02
}}&lt;/ref&gt; but the technology is clearly also applicable in more general applications.

The IOHN project consortium consists of 23 participants,&lt;ref name="IOHNmembers"&gt;
{{cite web 
|url=http://www.posccaesar.org/wiki/IOHN/participants 
|title=List of participating companies in the IOHN project 
|author=[[IOHN]] 
|accessdate=2010-03-10 
|date=2010-03-10
}}&lt;/ref&gt; including operators, service providers, software vendors, technology providers, research institutions and universities. In addition, the [[Norwegian Defence Force]] is working with the project to resolve common infrastructural and [[interoperability]] challenges.&lt;ref name="IOHNsite"/&gt;

The project is managed by [[DNV|Det Norske Veritas (DNV)]].&lt;ref&gt;
{{cite web 
|url=http://www.dnv.com/news_events/news/2008/dnvleadsintegratedoperationsdevelopment.asp
|title=DNV leads Integrated Operations development 
|author=[[Det Norske Veritas]] (DNV) 
|accessdate=2009-05-07 
|date=2008-08-26
}}&lt;/ref&gt; Nils Sandsmark was the project manager during the initiation and start-up phase. Fr&#233;d&#233;ric Verhelst took over as project manager from the beginning of 2009.&lt;ref&gt;
{{cite web 
|url=http://www.linkedin.com/in/fredericverhelst
|title=Profile of Fr&#233;d&#233;ric Verhelst 
|author=[[LinkedIn]]
|accessdate=2009-09-28 
}}&lt;/ref&gt;

Financing comes from the participants and the [[Research Council of Norway]] (RCN) for parts of the project (GOICT&lt;ref&gt;
{{cite web 
|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&amp;cid=1207296035860&amp;pagename=verdikt/Hovedsidemal&amp;p=1226993814962 
|title=Dependable ICT for the Energy Sector (GOICT, RCN proj.no. 183235, VERDIKT-programme)
|author=The [[Research Council of Norway]] (RCN) 
|accessdate=2009-05-07 
}}&lt;/ref&gt;
and AutoConRig&lt;ref&gt;
{{cite web 
|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&amp;cid=1198060412649&amp;pagename=ForskningsradetNorsk/Hovedsidemal&amp;p=1181730334233 
|title=Semi-autonomous control system for unmanned drilling rigs (AutoConRig, RCN proj.no. 187473, PETROMAKS-programme)
|author=The [[Research Council of Norway]] (RCN) 
|accessdate=2009-05-07 
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
|url=http://www.posccaesar.org/wiki/IOHN/AutoConRig 
|title=RCN/NFR project "AutoConRig"
|author=Jens Orn&#230;s (NOV)
|accessdate=2009-07-02 
}}&lt;/ref&gt;).

== Participants ==
The consortium consists of the following 22 participants&lt;ref name="IOHNmembers"/&gt; (in alphabetical order):&lt;br /&gt;
{| class="wikitable"
|-
| [[ABB Group|ABB]]
| [http://www.abelia.no Abelia]
| [[Baker Hughes]]
| [[Cisco]]
| [http://www.computas.no Computas]
|-
| [[Det Norske Veritas]]
| [[Eni|ENI]]
| [http://www.epsis.no Epsis]
| [[FMC Technologies]]
| [http://www.fsi.no FSI]
|-
| [http://www.ntnu.no/iocenter IO Center]
| [http://www.iris.no IRIS]
| [[National Oilwell Varco]]
| [[Norwegian University of Science and Technology|NTNU]]
| [http://www.olf.no OLF]
|-
| [[POSC Caesar Association]]
| [http://www.ptil.no Petroleum Safety Authority Norway]
| [[Siemens]]
| [[Statoil]]
| [[Norwegian Defence]]
|-
| [[University of Oslo]]
| [[University of Stavanger]]
|}

== See also ==
* [[Integrated Operations]]
* [[Semantic Web]]
* [[ISO 15926]] aka [[Oil and Gas Ontology]], an enabler for the next or second generation of [[Integrated Operations]] by integrating data across disciplines and business domains.
* [[Petroleum exploration in the Arctic]]
* [[POSC Caesar Association]], the custodian of [[ISO 15926]], the [[Oil and Gas Ontology]].

== References ==
{{reflist|2}}

== External links ==
* [http://www.IOHN.org/ Integrated Operations in the High North] website
* [[W3C]] workshop on [http://www.w3.org/2008/12/ogws-report.html Semantic Web in Oil and Gas industry], Houston, December 9&#8211;10, 2008. [http://www.w3.org/2008/12/ogws-report#papers Position papers] from several participants in IOHN.
* [http://www.posccaesar.org/wiki/PCA/SemanticDays2009/AboutSemanticDays Semantic Days 2009] conference, Stavanger, May 18&#8211;20, 2009. One [http://www.posccaesar.org/wiki/PCA/SemanticDays2009#Session6:SemantictechnologyforIOGeneration2 session] is devoted to IOHN.
* [http://www.ioconf.no/2009/ IO 09 Science and Practice] conference, Trondheim, September 29&#8211;30, 2009. One [http://ioconf.no/2009/parallel6 session] is devoted to IOHN.
* [http://www.oilit.com/2journal/2article/1003_16.htm#IOHN Integrated Operations in the High North&#8212;mid term report], ''Oil IT Journal'', March 2010.

{{DEFAULTSORT:Integrated Operations In The High North}}
[[Category:Petroleum organizations]]
[[Category:Petroleum engineering]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]</text>
      <sha1>89kga6t8z3v8iuyxvjja8a1em9kr6cq</sha1>
    </revision>
  </page>
  <page>
    <title>Ishikawa diagram</title>
    <ns>0</ns>
    <id>57535</id>
    <revision>
      <id>762972655</id>
      <parentid>762972614</parentid>
      <timestamp>2017-01-31T18:43:56Z</timestamp>
      <contributor>
        <ip>130.44.210.14</ip>
      </contributor>
      <comment>/* The 5 Ss (used in service industry) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5855" xml:space="preserve">{{Cleanup|date=March 2012}}
{{Infobox quality tool
| image =     Cause and effect diagram for defect XXX.svg
| category =  One of the [[Seven Basic Tools of Quality]]
| describer = [[Kaoru Ishikawa]]
| purpose =   To break down (in successive layers of detail) root causes that potentially contribute to a particular effect
}}
'''Ishikawa diagrams''' (also called '''fishbone diagrams''', '''herringbone diagrams''', '''cause-and-effect diagrams''', or '''Fishikawa''') are [[causal diagram]]s created by [[Kaoru Ishikawa]] (1968) that show the [[cause]]s of a specific [[wikt:event|event]].&lt;ref&gt;{{cite book | last = Ishikawa |first = Kaoru | title= Guide to Quality Control | year = 1968 | publisher = JUSE | location = Tokyo
}}&lt;/ref&gt;&lt;ref&gt;{{cite book | last = Ishikawa | first = Kaoru | title = Guide to Quality Control | publisher = Asian Productivity Organization | year =  1976 | isbn = 92-833-1036-5}}&lt;/ref&gt; Common uses of the Ishikawa diagram are [[product design]] and quality defect prevention to identify potential factors causing an overall effect. Each cause or reason for imperfection is a source of variation. Causes are usually grouped into major categories to identify these sources of variation. The categories typically include
*People: Anyone involved with the process
*Methods: How the process is performed and the specific requirements for doing it, such as policies, procedures, rules, regulations and laws
*Machines: Any equipment, computers, tools, etc. required to accomplish the job
*Materials: Raw materials, parts, pens, paper, etc. used to produce the final product
*Measurements: Data generated from the process that are used to evaluate its quality
*Environment: The conditions, such as location, time, temperature, and culture in which the process operates

==Overview==
[[File:Ishikawa Fishbone Diagram.svg|280px|thumb|left|Ishikawa diagram, in fishbone shape, showing factors of Equipment, Process, People, Materials, Environment and Management, all affecting the overall problem. Smaller arrows connect the sub-causes to major causes.]]
Ishikawa diagrams were popularized in the 1960s by [[Kaoru Ishikawa]],&lt;ref&gt;{{cite book |year=2001 |title=Infusion Therapy in Clinical Practice |first=Judy |last=Hankins |pages=42}}&lt;/ref&gt; who pioneered quality management processes in the [[Kawasaki Heavy Industries|Kawasaki]] shipyards, and in the process became one of the founding fathers of modern management.

The basic concept was first used in the 1920s, and is considered one of the [[Seven Basic Tools of Quality|seven basic tools]] of [[quality control]].&lt;ref&gt;{{cite web | url = http://www.asq.org/learn-about-quality/seven-basic-quality-tools/overview/overview.html |first=Nancy R. | last=Tague | title = Seven Basic Quality Tools | year = 2004 | work = The Quality Toolbox | publisher = American Society for Quality | location = Milwaukee, Wisconsin | page = 15 | accessdate = 2010-02-05}}&lt;/ref&gt; It is known as a fishbone diagram because of its shape, similar to the side view of a fish skeleton.

[[Mazda]] Motors famously used an Ishikawa diagram in the development of the [[Miata]] sports car, where the required result was "Jinba Ittai" (Horse and Rider as One &#8212; jap. &#20154;&#39340;&#19968;&#20307;). The main causes included such aspects as "touch" and "braking" with the lesser causes including highly granular factors such as "50/50 weight distribution" and "able to rest elbow on top of driver's door". Every factor identified in the diagram was included in the final design.{{citation needed|date=November 2015}}

==Causes==
Causes in the diagram are often categorized, such as to the 5 M's, described below. Cause-and-effect diagrams can reveal key relationships among various variables, and the possible causes provide additional insight into process behavior.

Causes can be derived from brainstorming sessions. These groups can then be labeled as categories of the fishbone.  They will typically be one of the traditional categories mentioned above but may be something unique to the application in a specific case.  Causes can be traced back to root causes with the [[5 Whys]] technique.

Typical categories are

===The 5 Ms (used in manufacturing industry)===
*Machine (technology)
*Method (process)
*Material (Includes Raw Material, Consumables and Information.)
*Man Power (physical work)/Mind Power (brain work): [[Kaizen]]s, Suggestions
*Measurement (Inspection)
The original 5 Ms used by the Toyota Production System have been expanded by some to include the following and are referred to as the 8 Ms. However, this is not globally recognized. It has been suggested to return to the roots of the tools and to keep the teaching simple while recognizing the original intent; most programs do not address the 8Ms.
*Milieu/Mother Nature(Environment)
*Management/Money Power
*Maintenance

"Milieu" is also used as the 6th M by industries for investigations taking the environment into account.

===The 8 Ps (used in marketing industry)===
*Product/Service
*Price 
*Place
*Promotion
*People/personnel
*Process
*Physical Evidence
*Packaging
The 8 Ps are primarily used in service marketing.

===The 5 Ss (used in service industry)===
*Surroundings
*Suppliers
*Systems
*Standard documentation skills
*Scope of work

==See also==
{{Portal|Thinking}}
* [[Seven Basic Tools of Quality]]
* [[Five whys]]

== References ==

=== Citations ===
{{Reflist|30em}}

=== Sources ===
* Ishikawa, Kaoru (1990); (Translator: J. H. Loftus); ''Introduction to Quality Control''; 448 p; ISBN 4-906224-61-X {{OCLC|61341428}}
* Dale, Barrie G. et al. (2007); ''Managing Quality 5th ed''; ISBN 978-1-4051-4279-3 {{OCLC|288977828}}

==External links==
{{Commons category|Ishikawa diagrams}}

{{DEFAULTSORT:Ishikawa Diagram}}
[[Category:Causal diagrams]]
[[Category:Causality]]
[[Category:Knowledge representation]]
[[Category:Quality control tools]]</text>
      <sha1>7om8evk4zqz0cqydaoc684elmde4smx</sha1>
    </revision>
  </page>
  <page>
    <title>Logico-linguistic modeling</title>
    <ns>0</ns>
    <id>30109665</id>
    <revision>
      <id>739519313</id>
      <parentid>708600196</parentid>
      <timestamp>2016-09-15T04:58:42Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <comment>Fix ISSN</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11545" xml:space="preserve">'''Logico-linguistic modeling''' is a method for building knowledge-based systems with a learning capability using [[Conceptual model|Conceptual Models]] from [[Soft systems methods]], modal predicate logic and the Prolog artificial intelligence language.

== Overview==
Logico-linguistic modeling is a six stage method developed primarily for building [[knowledge-based systems]] (KBS), but it also has application in manual decision support systems and information source analysis. Logico-linguistic models have a superficial similarity to Sowa's&lt;ref&gt;Sowa, John F. (1984), ''Conceptual Structures:  Information Processing in Mind and Machine'', Addison-Wesley, Reading, MA, USA.&lt;/ref&gt; [[Conceptual Graphs]], both use bubble style diagrams, both are concerned with concepts, both can be expressed in logic and both can be used in artificial intelligence. However, logico-linguistic models are very different in both logical form and in their method of construction.
 
Logico-linguistic modeling was developed in order to solve theoretical problems found in the Soft Systems method for information system design. The main thrust of the research into has been to show how [[Soft Systems Methodology]] (SSM), a method of systems analysis, can be extended into artificial intelligence.

== Background ==

SSM employs three modeling devices i.e. rich pictures, root definitions, and Conceptual Models of human activity systems. The root definitions and conceptual models are built by stakeholders themselves in an iterative debate organized by a facilitator. The strengths of this method lie, firstly, in its flexibility, the fact that it can address any problem situation, and, secondly, in the fact that the solution belongs to the people in the organization and is not imposed by an outside analyst.&lt;ref name =  "source"&gt;Gregory, Frank Hutson and Lau, Sui Pong (1999) [http://logicalgregory.jimdo.com/publications/logical-ssm-for-isa/ Logical Soft Systems Modelling for Information Source Analysis - The Case of Hong Kong Telecom], Journal of the Operational Research Society, vol. 50 (2).&lt;/ref&gt;

Information Requirements Analysis (IRA)&lt;ref name="Wilson"&gt;Wilson, Brian ''Systems: Concepts, Methodologies and Applications'', John Wiley &amp; Sons Ltd. 1984, 1990. ISBN 0-471-92716-3&lt;/ref&gt;  took the basic SSM method a stage further and showed how the Conceptual Models could be developed into a detailed information system design. IRA calls for the addition of two modeling devices: "Information Categories" which show the required information inputs and outputs from the activities identified in an expanded conceptual model; and the "Maltese Cross" a matrix which shows the inputs and outputs from the information categories and shows where new information processing procedures are required. A completed Maltese Cross is sufficient for the detailed design of a transaction processing system.

The initial impetus to the development of logico-linguistic modeling was a concern with the theoretical problem of how an information system can have a connection to the physical world.&lt;ref&gt;Gregory, Frank Hutson (1995) [[s:Mapping Information Systems onto the Real World|Mapping Information Systems onto the Real World]]. Working Paper Series No. WP95/01. Dept. of Information Systems, City University of Hong Kong.&lt;/ref&gt; This is a problem in both IRA and more established methods (such as [[SSADM]]) because none base their information system design on models of the physical world. IRA designs are based on a notional conceptual model and SSADM is based on models of the movement of documents.

The solution to these problems provided a formula that was not limited to the design of transaction processing systems but could be used for the design of KBS with learning capability.&lt;ref name="know"&gt;Gregory, Frank Hutson (1993) SSM for Knowledge Elicitation &amp; Representation, Warwick Business School Research Paper No. 98 ({{ISSN|0265-5976}}). Later published as Soft Systems Models for Knowledge Elicitation and Representation in Journal of the Operational Research Society (1995) 46, 562-578.&lt;/ref&gt;

== The Six Stages of logico-linguistic modeling==
[[File:Fig 1. SSM model abstracted from Wilson.jpg|thumb|Fig 1. SSM Conceptual Model]]
The logico-linguistic modeling method comprises six stages.&lt;ref name="know"/&gt;

=== 1. Systems Analysis ===

In the first stage logico-linguistic modeling uses SSM for [[systems analysis]]. This stage seeks to structure the problem in the client organization by identifying stakeholders, modelling organizational objectives and discussing possible solutions. At this stage it not assumed that a KBS will be a solution and logico-linguistic modeling often produces solutions that do not require a computerized KBS.

[[Expert systems]] tend to capture the expertise, of individuals in different organizations, on the same topic. By contrast a KBS, produced by logico-linguistic modeling, seeks to capture the expertise of individuals in the same organization on different topics. The emphasis is on the [[elicitation technique|elicitation]] of organizational or group knowledge rather than individual experts. In logico-linguistic modeling the stakeholders become the experts.

The end point of this stage is an SSM style conceptual models such as figure 1.

=== 2. Language Creation ===
[[File:Fig 2. Logico-linguistic Model.jpeg|thumb|Fig 2. Logico-linguistic Model]]

According to the theory behind logico-linguistic modeling the SSM conceptual model building process is a Wittgensteinian [[language-game]] in which the stakeholders build a language to describe the problem situation.&lt;ref&gt;Gregory, Frank Hutson (1992) [[s:SSM to Information Systems: A Wittengsteinian Approach|SSM to Information Systems: A Wittengsteinian Approach. Warwick Business School Research Paper No. 65.]] With revisions and additions this paper was published in Journal of Information Systems (1993) 3, pp.&amp;nbsp;149&#8211;168.&lt;/ref&gt; The logico-linguistic model expresses this language as a set of definitions, see figure 2.

=== 3. Knowledge Elicitation===
After the model of the language has been built putative knowledge about the real world can be added by the stakeholders. Traditional SSM conceptual models contain only one logical connective (a necessary condition). In order to represent causal sequences, &#8220;[[sufficient condition]]&#8221; and &#8220;[[necessary and sufficient condition|necessary &amp; sufficient conditions]]&#8221; are also required.&lt;ref name="cause2"&gt;Gregory, Frank Hutson (1992) [[s:Cause, Effect, Efficiency &amp; Soft Systems Models|Cause, Effect, Efficiency &amp; Soft Systems Models. Warwick Business School Research Paper No. 42]]. Later published in Journal of the Operational Research Society (1993) 44 (4), pp 149-168&lt;/ref&gt; In logico-linguistic modeling this deficiency is remedied by two addition types of connective. The outcome of stage three is an empirical model, see figure 3.

=== 4. Knowledge Representation ===
[[File:Fig 3. Empirical Model.jpeg|thumb|Fig 3. Empirical Model]]

Modal predicate logic (a combination of [[modal logic]] and [[predicate logic]]) is used as the formal method of knowledge representation. The connectives from the language model are logically true (indicated by the &#8220;''L''&#8221; modal operator) and connective added at the knowledge elicitation stage are possibility true (indicated by the &#8220;''M''&#8221; modal operator). Before proceeding to stage 5, the models are expressed in logical formulae.

=== 5. Computer code ===

Formulae in predicate logic translate easily into the [[Prolog]] artificial intelligence language. The modality is expressed by two different types of Prolog rules. Rules taken from the language creation stage of  model building process are treated as incorrigible. While rules from the knowledge elicitation stage are marked as hypothetical rules. The system is not confined to decision support but has a built in learning capability.

=== 6. Verification ===

A knowledge based system built using this method verifies itself. [[Verification and Validation (software)|Verification]] takes place when the KBS is used by the clients. It is an ongoing process that continues throughout the life of the system. If the stakeholder beliefs about the real world are mistaken this will be brought out by the addition of Prolog facts that conflict with  the hypothetical rules. It operates in accordance to the classic principle of [[falsifiability]] found in the philosophy of science&lt;ref&gt;Gregory, Frank Hutson (1996) "The need for "Scientific" Information Systems" Proceedings of the Americas Conference on Information Systems, Aug 1996, Association for Information Systems, 1996. pp. 534-536.&lt;/ref&gt;

== Applications ==
* '''Knowledge-based computer systems'''
Logico-linguistic modeling has been used to produce fully operational computerized knowledge based systems, such as one for the management of diabetes patients in a hospital out-patients department.&lt;ref&gt;Choi, Mei Yee Sarah (1997) Logico-linguistic Modelling for building a Diabetes Mellitus Patient Management Knowledge Based System. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.&lt;/ref&gt;

*'''Manual decision support'''
In other projects the need to move into Prolog was considered unnecessary because the printed logico-linguistic models provided an easy to use guide to decision making. For example, a system for mortgage loan approval&lt;ref&gt;Lee, Kam Shing Clive (1997) The Development of a Knowledge Based System on Mortgage Loan Approval. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.&lt;/ref&gt;

*'''Information source analysis'''
In some cases a KBS could not be built because the organization did not have all the knowledge needed to support all their activities. In these cases logico-linguistic modeling showed shortcomings in the supply of information and where more was needed. For example, a planning department in a telecoms company&lt;ref name =  "source"/&gt;

== Criticism ==
While logico-linguistic modeling overcomes the problems found in SSM's transition from conceptual model to computer code, it does so at the expense of increased stakeholder constructed model complexity. The benefits of this complexity are questionable&lt;ref&gt;Klein, J. H. (1994) Cognitive processes and operational research: a human information processing perspective. Journal of the Operational Research Society. Vol. 45, No. 8.&lt;/ref&gt;
and this modeling method may be much harder to use than other methods.&lt;ref&gt;Klein, J. H. (1995) Over-simplistic cognitive science: A response.  Journal of the Operational Research Society. Vol. 46, No. 4. pp. 275-6.&lt;/ref&gt;

This contention has been exemplified by subsequent research. An attempt by researchers to model buying decisions across twelve companies using logico-linguistic modeling required simplification of the models and removal of the modal elements.&lt;ref&gt;Nakswasdi, Suravut (2004) [http://arrow.unisa.edu.au:8080/vital/access/manager/Repository/unisa:44235 Logical Soft Systems for Modeling Industrial Machinery Buying Decisions in Thailand]. Doctor of Business Administration thesis, University of South Australia.&lt;/ref&gt;

== References ==
{{Reflist}}

== Further reading ==
{{commons category}}
* Gregory, Frank Hutson  (1993) "[http://wrap.warwick.ac.uk/2888/ A logical analysis of soft systems modelling: implications for information system design and knowledge based system design]''. PhD thesis, University of Warwick.

[[Category:Knowledge representation]]
[[Category:Systems analysis]]
[[Category:Modal logic]]</text>
      <sha1>5xwgrr5pzmwoy6327rid64l1cx7bpor</sha1>
    </revision>
  </page>
  <page>
    <title>Issue trees</title>
    <ns>0</ns>
    <id>30713569</id>
    <revision>
      <id>752347555</id>
      <parentid>734051901</parentid>
      <timestamp>2016-11-30T19:36:02Z</timestamp>
      <contributor>
        <username>Joie67</username>
        <id>13011716</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2397" xml:space="preserve">{{Orphan|date=August 2012}}

'''An issue tree''', also called "logic tree" or "issue map", is a graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.&lt;ref&gt;[https://global.oup.com/academic/product/strategic-thinking-in-complex-problem-solving-9780190463908?q=chevallier&amp;lang=en&amp;cc=us  Chevallier, Arnaud (2016). Strategic Thinking in Complex Problem Solving. Oxford, UK, Oxford University Press. p.47]&lt;/ref&gt;

Issue trees are useful in [[problem solving]] to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem.&lt;ref&gt;http://webarchive.nationalarchives.gov.uk/20060213205515/http://strategy.gov.uk/downloads/survivalguide/downloads/ssg_v2.1.pdf&lt;/ref&gt;

There are two types of issue trees: diagnostic ones and solution ones.

Diagnostic trees breakdown a "why" key question, identifying all the possible root causes for the problem.
Solution tree breakdown a "how" key question, identifying all the possible alternatives to fix the problem.

To be effective, an issue tree needs to obey four basic rules:&lt;ref&gt;http://powerful-problem-solving.com/build-logic-trees&lt;/ref&gt;
# Consistently answer a &#8220;why&#8221; or a &#8220;how&#8221; question
# Progress from the key question to the analysis as it moves to the right
# Have branches that are mutually exclusive and collectively exhaustive ([[MECE]])
# Use an insightful breakdown

The requirement for issue trees to be collectively exhaustive implies that [[divergent thinking]] is a critical skill.

A profitability tree is an example of an issue tree. It looks at different ways in which a company can increase its profitability. Starting from the key question on the right, it breaks it down between revenues and costs, and break these down into further details.
[[File:An issue tree showing how a company can increase profitability.png|thumb|An issue tree showing how a company can increase profitability]]

==References==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

[[Category:Articles created via the Article Wizard]]
[[Category:Knowledge representation]]
[[Category:Problem solving methods]]


{{logic-stub}}</text>
      <sha1>qd6z2gk8mviaahvfru8sd992s5gw4zp</sha1>
    </revision>
  </page>
  <page>
    <title>Concept map</title>
    <ns>0</ns>
    <id>698226</id>
    <revision>
      <id>757802557</id>
      <parentid>755974258</parentid>
      <timestamp>2017-01-01T21:04:44Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2016 July 2|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11682" xml:space="preserve">{{for|concept maps in [[generic programming]]|Concept (generic programming)}}
[[File:Electricity Concept Map.gif|thumb|An Electricity Concept Map, an example of a concept map]]
{{InfoMaps}}
A '''concept map''' or '''conceptual diagram''' is a [[diagram]] that depicts suggested relationships between [[concept]]s.&lt;ref&gt;Peter J. Hager,Nancy C. Corbin. ''Designing &amp; Delivering: Scientific, Technical, and Managerial Presentations,'' 1997, . 163.&lt;/ref&gt; It is a graphical tool that [[instructional designer]]s, [[engineer]]s, [[Technical communication|technical writers]], and others use to organize and structure [[knowledge]].

A concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as ''causes'', ''requires'', or ''contributes to''.&lt;ref name=theory&gt;[[Joseph D. Novak]] &amp; Alberto J. Ca&#241;as (2006). [http://cmap.ihmc.us/Publications/ResearchPapers/TheoryCmaps/TheoryUnderlyingConceptMaps.htm "The Theory Underlying Concept Maps and How To Construct and Use Them"], [[Institute for Human and Machine Cognition]]. Accessed 24 Nov 2008.&lt;/ref&gt;

The technique for [[Visualization (graphic)|visualizing]] these relationships among different concepts is called ''concept mapping''. Concept maps define the [[Ontology (information science)|ontology]] of computer systems, for example with the [[object-role modeling]] or  [[Unified Modeling Language]] formalism.

== Overview ==
A concept map is a way of representing relationships between [[idea]]s, [[image]]s, or [[word]]s in the same way that a [[sentence diagram]] represents the grammar of a sentence, a road map represents the locations of highways and towns, and a [[circuit diagram]] represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.{{clarify|why is this noteworthy?|date=November 2016}}&lt;ref name="CMP"&gt;[http://www.energyeducation.tx.gov/pdf/223_inv.pdf CONCEPT MAPPING FUELS]. Accessed 24 Nov 2008.&lt;/ref&gt;

Concept maps were developed{{whom|date=November 2016}} to enhance meaningful learning in the sciences{{fact|date=November 2016}}. A well-made concept map grows within a ''context frame'' defined by an explicit "focus question", while a [[mind map]] often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on [[declarative memory]] content, which is also referred to as chunks or propositions.&lt;ref&gt;Anderson, J. R., &amp; Lebiere, C. (1998). The atomic components of thought. Mahwah, NJ: Erlbaum.&lt;/ref&gt;&lt;ref&gt;Anderson, J. R., Byrne, M. D., Douglass, S., Lebiere, C., &amp; Qin, Y. (2004). An Integrated Theory of the Mind. Psychological Review, 111(4), 1036&amp;ndash;1050.&lt;/ref&gt; Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.

== Differences from other visualizations ==

'''[[Topic map]]s:'''  Concept maps are rather similar to topic maps in that both allow to concepts or topics via [[graph (data structure)|graphs]]. Among the various schema and techniques for visualizing ideas, processes, and organizations, concept mapping, as developed by [[Joseph D. Novak|Joseph Novak]] is unique in its philosophical basis, which "makes concepts, and propositions composed of concepts, the central elements in the structure of knowledge and construction of meaning."&lt;ref&gt;Novak, J.D. &amp; Gowin, D.B. (1996). Learning How To Learn, Cambridge University Press: New York, p. 7.&lt;/ref&gt;

'''[[mind mapping|Mind maps]]:'''  Both concept maps and topic maps can be contrasted with [[mind mapping]], which is often restricted to radial hierarchies and [[tree structure]]s. Another contrast between concept mapping and mind mapping is the speed and spontaneity when a mind map is created. A mind map reflects what you think about a single topic, which can focus group brainstorming. A concept map can be a map, a system view, of a real (abstract) system or set of concepts. Concept maps are more free form, as multiple hubs and clusters can be created, unlike mind maps, which fix on a single two centered approach.

== History ==
The technique of concept mapping was developed by [[Joseph D. Novak]] and his research team at [[Cornell University]] in the 1970s as a means of representing the emerging science knowledge of students.&lt;ref&gt;{{cite web|url=http://www.ihmc.us/users/user.php?UserID=jnovak|title=Joseph D. Novak|publisher=Institute for Human and Machine Cognition (IHMC)|accessdate=2008-04-06}}&lt;/ref&gt; It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin  in the learning movement called [[constructivism (learning theory)|constructivism]]. In particular, constructivists hold that learners actively construct knowledge.

Novak's work is based on the cognitive theories of [[David Ausubel]], who stressed the importance of prior knowledge in being able to learn (or ''assimilate'') new concepts: "The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly."&lt;ref&gt;Ausubel, D. (1968) Educational Psychology: A Cognitive View. Holt, Rinehart &amp; Winston, New York.&lt;/ref&gt; Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as "What is water?" "What causes the seasons?" In his book ''Learning How to Learn'', Novak states that a "meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures."

Various attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of ''off-loading''. In this 1998 paper, McAleese draws on the work of Sowa&lt;ref&gt;Sowa, J.F., 1983. ''Conceptual structures: information processing in mind and machine'', Addison-Wesley.&lt;/ref&gt; and a paper by Sweller &amp; Chandler.&lt;ref&gt;Sweller, J. &amp; Chandler, P., 1991. Evidence for Cognitive Load Theory. ''Cognition and Instruction'', 8(4), p.351-362.&lt;/ref&gt; In essence, McAleese suggests that the process of making knowledge explicit, using ''nodes'' and ''relationships'', allows the individual to become aware of what they know and as a result to be able to modify what they know.&lt;ref&gt;McAleese,R (1998) '''The Knowledge Arena''' as an Extension to the Concept Map: Reflection in Action, ''Interactive Learning Environments'', '''6,3,p.251-272'''.&lt;/ref&gt; Maria Birbili applies that same idea to helping young children learn to think about what they know.&lt;ref&gt;Birbili, M. (2006) [http://ecrp.uiuc.edu/v8n2/birbili.html "Mapping Knowledge: Concept Maps in Early Childhood Education"], ''Early Childhood Research &amp; Practice'', ''8(2)'', Fall 2006&lt;/ref&gt; The concept of the ''knowledge arena'' is suggestive of a virtual space where learners may explore what they know and what they do not know.

==Use==
[[Image:Conceptmap.png|thumb|450px|Example concept map created using the IHMC CmapTools computer program.]]
Concept maps are used to stimulate the generation of ideas, and are believed to aid [[creativity]].&lt;ref name="theory"/&gt; Concept mapping is also sometimes used for [[brain-storming]]. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.

Formalized concept maps are used in [[software design]], where a common usage is [[Unified Modeling Language]] diagramming amongst similar conventions and development methodologies.

Concept mapping can also be seen as a first step in [[ontology (computer science)|ontology]]-building, and can also be used flexibly to represent formal argument.

Concept maps are widely used in education and business. Uses include:
*[[Note taking]] and summarizing gleaning key concepts, their relationships and hierarchy from documents and source materials
*New knowledge creation: e.g., transforming [[tacit knowledge]] into an organizational resource, mapping team knowledge
*Institutional knowledge preservation (retention), e.g., eliciting and mapping expert knowledge of employees prior to retirement
*Collaborative knowledge modeling and the transfer of expert knowledge
*Facilitating the creation of shared vision and shared understanding within a team or organization
*Instructional design: concept maps used as [[David Ausubel|Ausubelian]] "advance organizers" that provide an initial conceptual frame for subsequent information and learning.
*Training: concept maps used as [[David Ausubel|Ausubelian]] "advanced organizers" to represent the training context and its relationship to their jobs, to the organization's strategic objectives, to training goals.
*Communicating complex ideas and arguments
*Examining the symmetry of complex ideas and arguments and associated terminology
*Detailing the entire structure of an idea, [[train of thought]], or line of argument (with the specific goal of exposing faults, errors, or gaps in one's own reasoning) for the scrutiny of others.
*Enhancing [[metacognition]] (learning to learn, and thinking about knowledge)
*Improving language ability
*Assessing learner understanding of learning objectives, concepts, and the relationship among those concepts
*Lexicon development

==See also==
{{list|date=November 2016}}
{{colbegin}}
* [[Argument map]]
* [[Cognitive map]]
* [[Conceptual graphs]]
* [[Conceptual framework]]
* [[Idea networking]]
* [[Knowledge visualization]]
* [[List of concept- and mind-mapping software]]
* [[Mental model]]
* [[Mind map]]
* [[Radial tree]]
* [[Entity-relationship model]]
* [[Nomological network]]
* [[Semantic web]]
* [[Topic Maps]]
* [[Educational psychology]]
* [[Educational technology]]
* [[Morphological analysis (problem-solving)|Morphological analysis]]
* [[Wicked problem]]
* [[Object role modeling]]
* [[Personal knowledge base]]
* [[Semantic network]]
* [[Olog]]
* [[Pathfinder network]]
* [[Sensemaking]]
{{colend}}

==References==
{{reflist|2}}

== Further reading ==
* {{cite book |last= Novak |first= J.D. |title= Learning, Creating, and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations |publisher= Routledge |edition= 2nd |date= 2009 |isbn= 9780415991858 }}
&lt;!-- |publisher= Lawrence Erlbaum Associates |location= Mahwah |date= 1998 |edition= 1st --&gt;
* {{cite book |last1= Novak |first1= J.D. |last2= Gowin |first2= D.B. |title= Learning How to Learn |publisher= Cambridge University Press |location= Cambridge |date= 1984 |isbn= 9780521319263 }}

== External links ==
{{commons|Concept map}}
{{wikiversity|Concept mapping}}
* [http://www.mind-mapping.org/images/walt-disney-business-map.png Example of a concept map from 1957] by Walt Disney.

{{Mindmaps}}

{{DEFAULTSORT:Concept Map}}
[[Category:Concepts]]
[[Category:Constructivism (psychological school)]]
[[Category:Diagrams]]
[[Category:Educational technology]]
[[Category:Graph drawing]]
[[Category:Knowledge representation]]
[[Category:Note-taking]]
[[Category:Visual thinking]]</text>
      <sha1>ec7zlqfjgs2bnpx0wgct2nqhg1nu5ub</sha1>
    </revision>
  </page>
  <page>
    <title>Folksonomy</title>
    <ns>0</ns>
    <id>23219749</id>
    <revision>
      <id>756231871</id>
      <parentid>756218811</parentid>
      <timestamp>2016-12-22T21:29:11Z</timestamp>
      <contributor>
        <username>McFarlandDana</username>
        <id>27259913</id>
      </contributor>
      <minor />
      <comment>/* Social tagging for knowledge acquisition */ applied hdl=free</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24865" xml:space="preserve">A '''folksonomy''' is a system in which users apply public [[Tag (metadata)|tags]]  to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a [[Taxonomy (general)|taxonomic]] classification specified by the owners of the content when it is published.&lt;ref&gt;{{cite news
 | title = Folksonomies. Indexing and Retrieval in Web 2.0.
 | url = https://books.google.com/books?id=Aeib_wy18gkC&amp;printsec=frontcover&amp;dq=folksonomies.+Indexing+and+Retrieval+in+Web+2.0#v=onepage&amp;q&amp;f=false
 | first = Isabella
 | last = Peters
 | work = Berlin: De Gruyter Saur
 | year = 2009
 }}&lt;/ref&gt;&lt;ref&gt;{{cite news
 | title = Folksonomy
 | first = Daniel H.
 | last = Pink
 | authorlink =
 | url = http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html
 | work = New York Times
 | date = 11 December 2005
 | accessdate = 14 July 2009
 }}&lt;/ref&gt; This practice is also known as '''collaborative tagging''',&lt;ref&gt;Lambiotte, R, and M Ausloos. 2005. Collaborative tagging as a tripartite network. http://arxiv.org/abs/cs.DS/0512090.&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of Association for Information Science and Technology|publisher=ASIS&amp;T|accessdate=26 May 2016}}&lt;/ref&gt; '''social classification''', '''social indexing''', and '''social tagging'''. However, these terms have slightly different meanings than folksonomy. Folksonomy was originally &#8220;the result of personal free tagging of information [...] for one&#8217;s own retrieval.&#8221;.&lt;ref&gt;Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from &lt;nowiki&gt;http://www.vanderwal.net/folksonomy.html&lt;/nowiki&gt;&lt;/ref&gt; '''Social tagging''' is the application of tags in an open online environment where the tags of other users are available to others. '''Collaborative tagging''' (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.

The term was coined by [[Thomas Vander Wal]] in 2004&lt;ref&gt;{{cite news
 | title = Folksonomy Coinage and Definition
 | url = http://www.vanderwal.net/folksonomy.html
 | first = Tomas
 | last = Vander Wal
 | date = 11 December 2005
 }}&lt;/ref&gt;&lt;ref&gt;Vander Wal, T. (2005). "[http://www.vanderwal.net/random/category.php?cat=153 Off the Top: Folksonomy Entries]." Visited November 5, 2005. See also: Smith, Gene. "[https://web.archive.org/web/20040828035712/http://atomiq.org/archives/2004/08/folksonomy_social_classification.html Atomiq: Folksonomy: social classification]." Aug 3, 2004. Retrieved January 1, 2007.&lt;/ref&gt;&lt;ref&gt;http://vanderwal.net/folksonomy.html Origin of the term&lt;/ref&gt; as a [[portmanteau]] of ''[[Volk (German word)|folk]]'' and ''[[Taxonomy (general)|taxonomy]]''. Folksonomies became popular as part of [[social software]] applications such as [[social bookmarking]] and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include [[tag cloud]]s as a way to visualize tags in a folksonomy.&lt;ref&gt;{{Cite journal
 | last1 = Lamere | first1 = Paul
 | title = Social Tagging And Music Information Retrieval
 | journal = Journal of New Music Research
 | volume = 37
 | issue = 2
 | pages = 101&#8211;114
 | date = June 2008
 | url = http://www.informaworld.com/smpp/content~db=all~content=a906001732
 | doi = 10.1080/09298210802479284 }}&lt;/ref&gt;

Folksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.

==Benefits and disadvantages==
Folksonomies are a trade-off between traditional centralized classification and no classification at all,&lt;ref&gt;Gupta, M., et al., ''An Overview of Social Tagging and Applications, in Social Network Data Analytics'', C.C. Aggarwal, Editor. 2011, Springer. p. 447-497.&lt;/ref&gt; and have several advantages:&lt;ref&gt;Quintarelli, E., ''Folksonomies: power to the people''. 2005.&lt;/ref&gt;&lt;ref&gt;Mathes, A., ''Folksonomies - Cooperative Classification and Communication Through Shared Metadata''. 2004.&lt;/ref&gt;&lt;ref&gt;Wal, T.V. ''Folksonomy''. 2007&lt;/ref&gt;
* tagging is easy to understand and do, even without training and previous knowledge in classification or indexing
* the vocabulary in a folksonomy directly reflects the user&#8217;s vocabulary
* folksonomies are flexible, in the sense that the user can add or remove tags
* tags consist of both popular content and long-tail content, enabling users to browse and discover new content even in narrow topics
* tags reflect the user&#8217;s conceptual model without cultural, social, or political bias
* enable the creation of communities, in the sense that users who apply the same tag have a common interest
* folksonomies are multi-dimensional, in the sense that users can assign any number and combination of tags to express a concept

There are several disadvantages with the use of tags and folksonomies as well,&lt;ref&gt;Kipp, M. and D.G. Campbell, ''Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices''. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.&lt;/ref&gt; and some of the advantages (see above) can lead to problems. For example, the simplicity in tagging can result in poorly applied tags.&lt;ref&gt;Hayman, S., ''Folksonomies and Tagging: New developments in social bookmarking'', in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.&lt;/ref&gt; Further, while controlled vocabularies are exclusionary by nature,&lt;ref&gt;Kroski, E., The Hive Mind: ''Folksonomies and User-Based Tagging. 2005''&lt;/ref&gt; tags are often ambiguous and overly personalized.&lt;ref&gt;Guy, M. and E. Tonkin, ''Folksonomies: Tidying up Tags?'' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.&lt;/ref&gt; Users apply tags to documents in many different ways and tagging systems also often lack mechanisms for handling synonyms, acronyms and homonyms, and they also often lack mechanisms for handling spelling variations such as misspellings, singular/plural form, conjugated and compound words. Some tagging systems do not support tags consisting of multiple words, resulting in tags like &#8220;viewfrommywindow&#8221;. Sometimes users choose specialized tags or tags without meaning to others.

==Elements and types==
A folksonomy emerges when users tag content or information, such web pages, photos, videos, podcasts, tweets, scientific papers and others. Strohmaier et al.&lt;ref&gt;Strohmaier, M., C. K&#246;rner, and R. Kern, ''Understanding why users tag: A survey of tagging motivation literature and results from an empirical study''. Web Semantics: Science, Services and Agents on the World Wide Web, 2012. 17: p. 1-11.&lt;/ref&gt; elaborate the concept: the term &#8220;tagging&#8221; refers to a "voluntary activity of users who are annotating resources with term-so-called 'tags' &#8211; freely chosen from an unbounded and uncontrolled vocabulary". Others explain tags as an unstructured textual label &lt;ref&gt;Ames, M.N.M., ''Why We Tag: Motivations for Annotation in Mobile and Online Media'', in SIGCHI conference on Human factors in computing systems. 2007, ACM Press: New York, NY, USA. p. 971-980.&lt;/ref&gt; or keywords,&lt;ref&gt;Guy, M. and E. Tonkin, Folksonomies: ''Tidying up Tags?'' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.&lt;/ref&gt; and that they appear as a simple form of metadata.&lt;ref&gt;Brooks, C.H. and N. Montanez, ''Improved annotation of the blogosphere via autotagging and hierarchical clustering'', in WWW '06: Proceedings of the 15th international conference on World Wide Web. 2006, ACM Press: New York, NY, USA. p. 625-632.&lt;/ref&gt;

Folksonomies consist of three basic entities: users, tags, and resource. Users create tags to mark resources such as: web pages, photos, videos, and podcasts. These tags are used to manage, categorize and summarize online content. This collaborative tagging system also uses these tags as a way to index information, facilitate searches and navigate resources. Folksonomy also includes a set of URLs that are used to identify resources that have been referred to by users of different websites. These systems also include category schemes that have the ability to organize tags at different levels of granularity.&lt;ref name="Berlin, B. 1992"&gt;Berlin, B. (1992). Ethnobiological Classification. Princeton: Princeton University Press.&lt;/ref&gt;

Vander Wal identifies two types of folksonomy: broad and narrow.&lt;ref name="Vander Wal"&gt;{{cite web |title=Explaining and Showing Broad and Narrow Folksonomies |url=http://www.vanderwal.net/random/entrysel.php?blog=1635 | last = Vander Wal | first=Thomas |accessdate= 2013-03-05}}&lt;/ref&gt;  A broad folksonomy arises when multiple users can apply the same tag to an item, providing information about which tags are the most popular. A narrow folksonomy occurs when users, typically fewer in number and often including the item's creator, tag an item with tags that can each be applied only once.  While both broad and narrow folksonomies enable the searchability of content by adding an associated word or phrase to an object, a broad folksonomy allows for sorting based on the popularity of each tag, as well as the tracking of emerging trends in tag usage and developing vocabularies.&lt;ref name="Vander Wal"/&gt;

An example of a broad folksonomy is [[Delicious (website)|del.icio.us]],  a website where users can tag any online resource they find relevant with their own personal tags. The photo-sharing website [[Flickr]] is an oft-cited example of a narrow folksonomy.

==Folksonomy vs. taxonomy==
'Taxonomy' refers to a hierarchical categorization in which relatively well-defined classes are nested under broader categories. A ''folksonomy'' establishes categories (each tag is a category) without stipulating or necessarily deriving a hierarchical structure of parent-child relations among different tags. (Work has been done on techniques for deriving at least loose hierarchies from clusters of tags.&lt;ref&gt;{{cite journal|last1=Laniado|first1=David|title=Using WordNet to turn a folksonomy into a hierarchy of concepts|journal=CEUR Workshop Proceedings|volume=314|issue=51|url=http://ceur-ws.org/Vol-314/51.pdf|accessdate=7 August 2015}}&lt;/ref&gt;)

Supporters of folksonomies claim that they are often preferable to taxonomies because folksonomies democratize the way information is organized, they are more useful to users because they reflect current ways of thinking about domains, and they express more information about domains.&lt;ref&gt;{{cite web|last1=Weinberger|first1=David|title=Folksonomy as Symbol|url=http://www.hyperorg.com/blogger/?p=6254|website=Joho the Blog|accessdate=7 August 2015}}&lt;/ref&gt; Critics claim that folksonomies are messy and thus harder to use, and can reflect transient trends that may misrepresent what is known about a field.

An empirical analysis of the complex dynamics of tagging systems, published in 2007,&lt;ref name="WWW07-ref" &gt;Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proc. International Conference on World Wide Web, ACM Press, 2007.&lt;/ref&gt; has shown that consensus around stable distributions and shared vocabularies does emerge, even in the absence of a central [[controlled vocabulary]]. For content to be searchable, it should be categorized and grouped. While this was believed to require commonly agreed on sets of content describing tags (much like keywords of a journal article), some research has found that in large folksonomies common structures also emerge on the level of categorizations.&lt;ref name="TWEB-ref" &gt;V. Robu, H. Halpin, H. Shepherd [http://portal.acm.org/citation.cfm?id=1594173.1594176 Emergence of consensus and shared vocabularies in collaborative tagging systems], ACM Transactions on the Web (TWEB), Vol. 3(4), art. 14, 2009.&lt;/ref&gt;
Accordingly, it is possible to devise mathematical [[models of collaborative tagging]] that allow for translating from personal tag vocabularies (personomies) to the vocabulary shared by most users.&lt;ref&gt;Robert Wetzker, Carsten Zimmermann, Christian Bauckhage, and Sahin Albayrak [http://portal.acm.org/citation.cfm?id=1718487.1718497 I tag, you tag: translating tags for advanced user models], Proc. International Conference on Web Search and Data Mining, ACM Press, 2010.&lt;/ref&gt;

Folksonomy is unrelated to [[folk taxonomy]], a cultural practice that has been widely documented in anthropological and [[folkloristics|folkloristic]] work. Folk taxonomies are culturally supplied, intergenerationally transmitted, and relatively stable classification systems that people in a given culture use to make sense of the entire world around them (not just the [[Internet]]).&lt;ref name="Berlin, B. 1992"/&gt;

The study of the structuring or classification of folksonomy is termed ''folksontology''.&lt;ref&gt;{{cite web | url=http://www.heppnetz.de/files/vandammeheppsiorpaes-folksontology-semnet2007-crc.pdf | title=FolksOntology: An Integrated Approach for Turning Folksonomies into Ontologies | accessdate=April 20, 2012 | author=Van Damme, C&#233;line|display-authors=etal}}&lt;/ref&gt; This branch of [[ontology (information science)|ontology]] deals with the intersection between highly structured taxonomies or hierarchies and loosely structured folksonomy, asking what best features can be taken by both for a system of classification. The strength of flat-tagging schemes is their ability to relate one item to others like it. Folksonomy allows large disparate groups of users to collaboratively label massive, dynamic information systems. The strength of taxonomies are their browsability: users can easily start from more generalized knowledge and target their queries towards more specific and detailed knowledge.&lt;ref&gt;Trattner, C., K&#246;rner, C., Helic, D.: [http://www.christophtrattner.info/pubs/iknow2011.pdf Enhancing the Navigability of Social Tagging Systems with Tag Taxonomies]. In Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies, ACM, New York, NY, USA, 2011&lt;/ref&gt; Folksonomy looks to categorize tags and thus create browsable spaces of information that are easy to maintain and expand.

== Social tagging for knowledge acquisition ==
Social tagging for knowledge acquisition is the specific use of tagging for finding and re-finding specific content for an individual or group. Social tagging systems differ from traditional taxonomies in that they are community-based systems lacking the traditional hierarchy of taxonomies. Rather than a top-down approach, social tagging relies on users to create the folksonomy from the bottom up.&lt;ref name=":0"&gt;Held, C., &amp; Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.&lt;/ref&gt;

Common uses of social tagging for knowledge acquisition include personal development for individual use and collaborative projects. Social tagging is used for knowledge acquisition in secondary, post-secondary, and graduate education as well as personal and business research. The benefits of finding/re-finding source information are applicable to a wide spectrum of users. Tagged resources are located through search queries rather than searching through a more traditional file folder system.&lt;ref&gt;Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229&#8211;238. ACM, New York.&lt;/ref&gt; The social aspect of tagging also allows users to take advantage of metadata from thousands of other users.&lt;ref name=":0" /&gt;

Users choose individual tags for stored resources. These tags reflect personal associations, categories, and concepts. All of which are individual representations based on meaning and relevance to that individual. The tags, or keywords, are designated by users. Consequently, tags represent a user&#8217;s associations corresponding to the resource. Commonly tagged resources include videos, photos, articles, websites, and email.&lt;ref name=":1"&gt;Kimmerle, J., Cress, U., &amp; Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research &amp; Practice, 8(1), 33-44.&lt;/ref&gt; Tags are beneficial for a couple of reasons. First, they help to structure and organize large amounts of digital resources in a manner that makes them easily accessible when users attempt to locate the resource at a later time. The second aspect is social in nature, that is to say that users may search for new resources and content based on the tags of other users. Even the act of browsing through common tags may lead to further resources for knowledge acquisition.&lt;ref name=":0" /&gt;

Tags that occur more frequently with specific resources are said to be more strongly connected. Furthermore, tags may be connected to each other. This may be seen in the frequency in which they co-occur. The more often they co-occur, the stronger the connection. Tag clouds are often utilized to visualize connectivity between resources and tags. Font size increases as the strength of association increases.&lt;ref name=":1" /&gt;

Tags show interconnections of concepts that were formerly unknown to a user. Therefore, a user&#8217;s current cognitive constructs may be modified or augmented by the metadata information found in aggregated social tags. This process promotes knowledge acquisition through cognitive irritation and equilibration. This theoretical framework is known as the co-evolution model of individual and collective knowledge.&lt;ref name=":1" /&gt;

The co-evolution model focuses on cognitive conflict in which a learner&#8217;s prior knowledge and the information received from the environment are dissimilar to some degree.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt; When this incongruence occurs, the learner must work through a process cognitive equilibration in order to make personal cognitive constructs and outside information congruent. According to the coevolution model, this may require the learner to modify existing constructs or simply add to them.&lt;ref name=":0" /&gt; The additional cognitive effort promotes information processing which in turn allows individual learning to occur.&lt;ref name=":1" /&gt;

A Canadian university study of instructors' use of folksonomy tools in a learning objects repository identified critical success factors, and affirmed the applicability of Zipf's [[Principle of least effort|Principle of Least Effort]], concluding that a major benefit of "the folksonomical approach to knowledge management... is the fact that it is developed and maintained by the users of that body of knowledge," fostering "the dual outcome of creating a more viable knowledge management tool while strengthening the bonds of the user community."&lt;ref&gt;{{Cite book|url=http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-368-5.ch045|title=Critical Success Factors in the Development of Folksonomy-Based Knowledge Management Tools|last=Owen|first=Kenneth|last2=Willis|first2=Robert|date=2010|publisher=IGI Global|year=|isbn=|location=|pages=509&#8211;518|language=English|doi=10.4018/978-1-60566-368-5.ch045|hdl=http://hdl.handle.net/10613/3176|quote=|via=VIUSpace|hdl-access=free}}&lt;/ref&gt;

==Examples of folksonomies==

* [[Twitter]] [[hashtag]]s
* Many libraries' online catalogs&lt;ref&gt;Steele, T. (2009).  The new cooperative cataloging.  Library Hi Tech, 27 (1), 68-77&lt;/ref&gt;&lt;ref&gt;Corey A. Harper and [[Barbara B. Tillett]], [https://scholarsbank.uoregon.edu/dspace/bitstream/1794/3269/1/ccq_s Library of Congress controlled vocabularies and their application to the Semantic Web]&lt;/ref&gt;
* [[Delicious (website)|del.icio.us]]: public tagging service
* [[Flickr]]: shared photos
* [[Steam (software)|Steam]] video game store
* [[Mendeley]]: social reference management software
* [[StumbleUpon]]: content discovery engine
* [[Diigo]]: [[social bookmarking]] website
*  The [[World Wide Web Consortium]]'s [[Annotea]] project with user-generated tags in 2002.
* [[Instagram]]: online photo-sharing and social networking service
* [[WordPress]]: blogging tool and Content Management System
* [[Pinterest]]: photosharing and publishing website

==See also==
{{div col|colwidth=30em}}
* [[Collective intelligence]]
* [[Enterprise bookmarking]]
* [[Faceted classification]]
* [[Semantic similarity]]
* [[Thesaurus]]
* [[Weak ontology]]
* [[Wiki]]
{{div col end}}

==References==
{{Reflist|30em|refs = Bateman, S., Brooks, C., McCalla, G., &amp; Brusilovsky, P. (2007, May). Applying collaborative tagging to e-learning. In Proceedings of the 16th International World Wide Web Conference (WWW2007).

Civan, A., Jones, W., Klasnja, P., &amp; Bruce, H. (2008). Better to organize personal information by folders or by tags?: The devil is in the details.Proceedings of the American Society for Information Science and Technology,45(1), 1-13.
 
Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229&#8211;238. ACM, New York.

Guy, M. and E. Tonkin, Folksonomies: Tidying up Tags? D-Lib Magazine, 2006. 12(Number 1): p. 1-15.

Gupta, M., et al., An Overview of Social Tagging and Applications, in Social Network Data Analytics, C.C. Aggarwal, Editor. 2011, Springer. p. 447-497
 
Held, C., &amp; Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.

Hayman, S., Folksonomies and Tagging: New developments in social bookmarking, in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.
 
Kimmerle, J., Cress, U., &amp; Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research &amp; Practice, 8(1), 33-44.

Kipp, M. and D.G. Campbell, Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.

Kroski, E., The Hive Mind: Folksonomies and User-Based Tagging. 2005.
 
Lavou&#233;, &#201;. (2011). Social tagging to enhance collaborative learning. In Advances in Web-Based Learning-ICWL 2011 (pp. 92-101). Springer Berlin Heidelberg.

Mathes, A., Folksonomies - Cooperative Classification and Communication Through Shared Metadata. 2004.

Quintarelli, E., Folksonomies: power to the people. 2005.

Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from http://www.vanderwal.net/folksonomy.html

Weinberger, D. (2007). Everything is miscellaneous: The power of the new digital disorder. Times Books, New York.
 
}}

==Additional references==
* [http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html "Folksonomy"], [[The New York Times]], 2005-12-11
* [http://www.wired.com/science/discoveries/news/2005/02/66456 "Folksonomies Tap People Power"], [[Wired News]], 2005-02-01
* {{cite journal | journal = [[Information Services &amp; Use]] | title = Folksonomy and science communication | issue = 27 | year = 2007 | pages = 97&#8211;103 | url = http://wwwalt.phil-fak.uni-duesseldorf.de/infowiss/admin/public_dateien/files/1/1194272247inf_servic.pdf }}{{spaced ndash}} Folksonomies as a tool for professional scientific databases
* [http://www.hyperorg.com/blogger/misc/taxonomies_and_tags.html "The Three Orders"]: 2005 explanation of tagging and folksonomies

==External links==
* [http://www.socialtagging.org/ SocialTagging.org] provides short definitions of key terms related to tagging and folksonomies
* [http://www.vanderwal.net/folksonomy.html Vanderwal's definition of folksonomy]
* [http://www.vanderwal.net/random/entrysel.php?blog=1750 Vanderwal's take on Wikipedia's definition of folksonomy]
*[http://er.educause.edu/articles/2011/9/classroom-collaboration-using-social-bookmarking-service-diigo Classroom Collaboration Using Social Bookmarking Service Diigo]

{{Web syndication}}
{{Semantic Web}}

[[Category:Collective intelligence]]
[[Category:Folksonomy| ]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Social bookmarking]]
[[Category:Taxonomy]]
[[Category:Web 2.0 neologisms]]
[[Category:Sociology of knowledge]]</text>
      <sha1>b4pt0gknezyskcl56r4mrh3gu6u0fa6</sha1>
    </revision>
  </page>
  <page>
    <title>Brand page</title>
    <ns>0</ns>
    <id>34215536</id>
    <revision>
      <id>741997145</id>
      <parentid>685866571</parentid>
      <timestamp>2016-10-01T01:03:30Z</timestamp>
      <contributor>
        <username>Trivialist</username>
        <id>5360838</id>
      </contributor>
      <comment>Copyedit (minor)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7678" xml:space="preserve">A '''brand page''' (also known as a '''page''' or '''fan page'''), in online social networking parlance, is a profile on a social networking website which is considered distinct from an actual [[user profile]] in that it is created and managed by at least one other registered user as a representation of a non-personal [[online identity]]. This feature is most used to represent the brands of organizations associated with, properties owned by, or general interests favored by a user of the hosting network.

While also being potentially manageable by more than one registered user, pages are distinguished from [[Group (online social networking)|groups]] in that pages are usually designed for the managers to direct messages and posts to subscribing users (akin to a [[newsletter]] or [[blog]]) and promote a brand, while groups are usually and historically formed for discussion purposes.

==History==
Prior to 2007, only a few websites made use of non-personal profile pages. [[Last.fm]], established in 2002, used its music recommendation service to automatically generate "artist pages" which serve as portals for biographies, events and artist-related playlists. This approach, however, is not explicitly controlled by artists or music groups because of the automatic nature of artist pages; pages, for example, could be created from erroneous misspellings and miscredits of works which are accepted as-is by the Audioscrobbler recommendation service used by Last.fm. Furthermore, Last.fm has never advertised itself as a social networking service, despite accruing myriad social features since 2002.

The most high-profile usage of this model is [[Facebook]]'s Pages (formerly known as "Fan Page" until 2010) feature, launched in 2007; one could "be a fan of" a page until April 2010, when the parlance was replaced with "Like".&lt;ref&gt;{{cite web|url=http://www.allfacebook.com/2010/04/facebooks-become-a-fan-officially-switches-to-like/ | title=Facebook&#8217;s "Become A Fan" Officially Switches To "Like" | author=[[Nick O'Neill]]|publisher = AllFacebook.com|date = April 19, 2010 &lt;!-- 4:37 PM --&gt; }}&lt;/ref&gt; [[Foursquare]], a location-oriented social networking site, launched its "Brands" feature allowing for the creation of specialized brand pages in January 2010 (with [[Intel]] being the first user), but they did not become "self-serve" (controllable by individuals employed by page brand owners) until August 2011.&lt;ref&gt;{{cite web|url = http://blog.foursquare.com/2011/08/02/pages-are-now-self-serve-a-new-home-for-brands-and-organizations-on-foursquare/|title = Pages are now self-serve! A new home for brands and organizations on foursquare.|date = Aug 2, 2011|publisher = Foursquare}}&lt;/ref&gt; [[LinkedIn]], an enterprise-oriented social networking service, launched "Company Pages" in November 2010.&lt;ref&gt;{{cite web|url = http://blog.linkedin.com/2010/11/01/linkedin-company-pages/|title = Recommend your favorite products and services on LinkedIn Company Pages|author = Ryan Roslansky|publisher = LinkedIn|date = November 1, 2010}}&lt;/ref&gt; [[Google+]], the current social networking service operated by [[Google]], launched its own "Pages" feature in October 2011.&lt;ref&gt;{{cite web|url = http://googleblog.blogspot.com/2011/11/google-pages-connect-with-all-things.html|title = Google+ Pages: connect with all the things you care about|publisher = Google|date = 11-07-2011 &lt;!-- 10:01:00 AM --&gt; }}&lt;/ref&gt; On November 19th, 2012, [[Amazon.com|Amazon]] announced Amazon Pages giving brands self-service control over their presence on the site.&lt;ref&gt;{{cite web|url = http://techcrunch.com/2012/11/20/amazon-offers-amazon-pages-for-brands-to-customize-with-their-own-urls-and-amazon-posts-for-social-media-marketing/|title = Amazon Offers &#8216;Amazon Pages&#8217; For Brands To Customize With Their Own URLs, And &#8216;Amazon Posts&#8217; For Social Media Marketing|author = TechCrunch|date = November 20, 2012}}&lt;/ref&gt; On 8 December, [[Twitter]] announced that it would roll out "brand pages" as part of a major user interface redesign in 2012.&lt;ref&gt;{{cite web|url = http://advertising.twitter.com/2011/12/let-your-brand-take-flight-on-twitter.html|title = Let your brand take flight on Twitter with enhanced profile pages|publisher = Twitter Advertising Blog|author = TwitterAds|date = December 8, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url = http://adage.com/article/digital/twitter-joins-facebook-google-launches-brand-pages-marketers/231448/|title = Twitter Joins Facebook, Google, Launches 'Brand Pages' for Marketers|author = Cotton Delo|date = December 8, 2011}}&lt;/ref&gt;

==Features==
Increasingly, brand pages make use of the following features: 
* Header banners
* The ability to post blogs or replies on the brand page in the name of the brand page
* The ability to administer multiple pages
* Photos
* Video
* Maps (including physical location of the page)
* Subscribers
* Other apps

Twitter made use of header banners in their launch of brand pages, and Facebook made use of "cover photos" in their re-design of brand pages in March 2011.

==Uses==
Organizations and brands regularly make use of pages in order to syndicate news and upcoming events, especially off-site blog posts, to subscribing users. Page subscription numbers can also be used as a metric of trust or interest in the associated brand.

Interests can also be indexed as pages, and are often the basis for the formation of mass social movements (i.e., the [[Arab Spring]], [[Occupy Wall Street]]).

===Newsroom accounts===
Pages are also used as newsroom accounts.

A '''[[newsroom]] account''' refers to any microblogging or social networking account branded by or owned by a publishing or broadcasting organization which is dedicated solely to syndicating content from a particular category of content as published on the original website of the organization. Such accounts have come into increased usage by news organizations as means by which:
# A news organization's presence on a social networking or microblogging website is increased
# A news organization can specialize content syndication to selective users who wish to subscribe

News organizations who make use of multiple newsroom accounts typically allow for either online editors or multiple employed authors to edit and update the syndications of newsroom content. Such accounts are typically marked by graphic icons which make use of the brand symbol combined with distinct colors assigned to each account.

Examples of newsroom accounts and pages include the Facebook pages for both ''[[The Guardian]]'' and the newspaper's Technology newsroom.

==Impact==

===Pseudonyms===
The usage of pseudonyms on social networking services, long considered a preserve of user privacy, has been partially affected by the promotion of pseudonyms, as social networking services have encouraged users to create pages for pseudonyms and implemented legal name requirements for user profile registration (i.e., New York resident Stefani Germanotta keeping a separate personal user profile under her legal name while maintaining a fan page under her stage name and pseudonym [[Lady Gaga]]).

===Interest-based connections===
As pages can be created to represent interests, the number of attempts to create vertical social networking services (i.e., [[Ning (website)|Ning]]) has leveled off in the 2010s. [[Social network advertising]] can also be targeted to users based upon their page subscriptions.

==See also==
* [[Fansite]]
* [[Landing page]]

==References==
{{reflist}}

{{Online social networking}}
{{Microblogging}}

{{DEFAULTSORT:Page (online social networking)}}
[[Category:Software features]]
[[Category:Knowledge representation]]
[[Category:Identity management]]</text>
      <sha1>1oqe7q9qll5in86qhrv4iozzhi7kt7w</sha1>
    </revision>
  </page>
  <page>
    <title>Prot&#233;g&#233; (software)</title>
    <ns>0</ns>
    <id>5007318</id>
    <revision>
      <id>747620753</id>
      <parentid>737605140</parentid>
      <timestamp>2016-11-03T11:43:24Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4058" xml:space="preserve">{{Infobox software
| name                   = Prot&#233;g&#233;
| logo                   = &lt;!-- Image name is enough --&gt;
| logo alt               = 
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot alt         = 
| collapsible            = 
| author                 = 
| developer              = [[Stanford University School of Medicine|Stanford]] Center for Biomedical Informatics Research
| released               = {{Start date and age|1999|11|11|df=yes}}&lt;ref name=versions&gt;{{cite web |title=Protege Desktop Older Versions |website=Protege Wiki |date=24 May 2016 |url=http://protegewiki.stanford.edu/wiki/Protege_Desktop_Old_Versions }}&lt;/ref&gt;
| discontinued           = 
| latest release version = 5.0.0
| latest release date    = {{Start date and age|2016|05|24|df=yes}}&lt;ref name=versions/&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| status                 = Active
| programming language   = [[Java (programming language)|Java]]
| operating system       = Linux, Mac OS X &amp; Windows&lt;ref name=install5&gt;{{cite web |title=Protege Desktop 5.0 Installation Instructions |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Install_Protege5 }}&lt;/ref&gt;
| platform               = Java {{abbr|VM|Virtual Machine}}&lt;ref&gt;{{cite web |title= Protege Desktop Frequently Asked Questions  |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Protege-OWL_4_FAQ }}&lt;/ref&gt;
| size                   = 
| language               = 
| language count         = &lt;!-- Number only --&gt;
| language footnote      = 
| genre                  = [[Ontology (information science)#Editor|Ontology editor]]
| license                = [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause]]
| alexa                  = 
| website                = {{URL|protege.stanford.edu}}
| repo                   = {{URL|https://github.com/protegeproject/protege}}
| standard               = 
| AsOf                   = 
}}

'''Prot&#233;g&#233;''' is a free, open source [[Ontology (computer science)|ontology]] editor and a [[knowledge management]] system. Prot&#233;g&#233; provides a graphic user interface to define ontologies. It also includes [[deductive classifier]]s to validate that models are consistent and to infer new information based on the analysis of an ontology. Like [[Eclipse (software)|Eclipse]], Prot&#233;g&#233; is a framework for which various other projects suggest plugins. This application is written in [[Java (programming language)|Java]] and heavily uses [[Swing (Java)|Swing]] to create the user interface. Prot&#233;g&#233; recently has over 300,000 registered users.&lt;ref&gt;[http://protege.stanford.edu/community.php Prot&#233;g&#233; Community]&lt;/ref&gt; According to a 2009 book it is "the leading ontological engineering tool".&lt;ref name="SelicGa&#353;evic2009"&gt;{{cite book|author1=Dragan Ga&#353;evi&#263;|author2=Dragan Djuri&#263;|author3=Vladan Deved&#382;i&#263;|title=Model Driven Engineering and Ontology Development|url=https://books.google.com/books?id=s-9yu7ubSykC&amp;pg=PA194|year= 2009|publisher=Springer|isbn=978-3-642-00282-3|pages=194|edition=2nd}}&lt;/ref&gt;

Prot&#233;g&#233; is being developed at [[Stanford University]] and is made available under the [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause license]].&lt;ref&gt;{{cite web |title=protege/license.txt |website=GitHub |url=https://github.com/protegeproject/protege/blob/master/license.txt }}&lt;/ref&gt; Earlier versions of the tool were developed in collaboration with the [[University of Manchester]].

== References ==
&lt;references/&gt;

== External links ==
* {{Official website|http://protege.stanford.edu/}}
* [http://protegewiki.stanford.edu/wiki/Main_Page Prot&#233;g&#233; wiki]

{{DEFAULTSORT:Protege (software)}}
[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Free software programmed in Java (programming language)]]


{{programming-software-stub}}</text>
      <sha1>s43hvvok1qqkywvr5fr2skv75uvb4zn</sha1>
    </revision>
  </page>
  <page>
    <title>Conceptualization (information science)</title>
    <ns>0</ns>
    <id>38982174</id>
    <revision>
      <id>725295976</id>
      <parentid>722584061</parentid>
      <timestamp>2016-06-14T19:50:31Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <comment>refs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11584" xml:space="preserve">[[File:Ontological commitments.png|thumb|200px|Chart showing the relation between a conceptualization in information science, its various ontologies (each with its own specialized language), and their shared ontological commitment.&lt;ref name=CZ&gt;
This figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&amp;pg=PA7&amp;lpg=PA7#v=onepage&amp;q&amp;f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].
&lt;/ref&gt;]]
In [[information science]] a '''conceptualization''' is an abstract simplified view of some selected part of the world, containing the objects, concepts, and other entities that are presumed of interest for some particular purpose and the relationships between them.&lt;ref name=Gruber/&gt;&lt;ref name=Smith/&gt; An explicit specification of a conceptualization is an [[ontology (information science)|ontology]], and it may occur that a conceptualization can be realized by several distinct ontologies.&lt;ref name=Gruber/&gt;  An ''[[ontological commitment]]'' in describing ontological comparisons is taken to refer to that subset of elements of an ontology shared with all the others.&lt;ref name=Audi/&gt;&lt;ref name=Ceccaroni1/&gt;  "An ontology is ''language-dependent''", its objects and interrelations described within the language it uses,  while a conceptualization is always the same, more general, its concepts existing "independently of the language used to describe it".&lt;ref name=Guarino/&gt; The relation between these terms is shown in the figure to the right.

Not all workers in [[knowledge engineering]] use the term &#8216;conceptualization&#8217;, but instead refer to the conceptualization itself, or to the ontological commitment of all its realizations, as an overarching ontology.&lt;ref name=Ceccaroni/&gt;

==Purpose and implementation==
As a higher level abstraction, a conceptualization facilitates the discussion and comparison of its various ontologies, facilitating knowledge sharing and reuse.&lt;ref name=Ceccaroni/&gt;&lt;ref name=Harmelen/&gt; Each ontology based upon the same overarching conceptualization maps the conceptualization into specific elements and their relationships.

The question then arises as to how to describe the 'conceptualization' in terms that can encompass multiple ontologies. This issue has been called the '[[Tower of Babel]]' problem, that is, how can persons used to one ontology talk with others using a different ontology?&lt;ref name=Smith/&gt;&lt;ref name=Harmelen/&gt; This problem is easily grasped, but a general resolution is not at hand. It can be a 'bottom-up' or a 'top-down' approach, or something in between.&lt;ref name=Alignment/&gt;

However, in more artificial situations, such as information systems, the idea of a 'conceptualization' and the 'ontological commitment' of various ontologies that realize the 'conceptualization' is possible.&lt;ref name=Guarino/&gt;&lt;ref name=Guarino1/&gt; The formation of a conceptualization and its ontologies involves these steps:&lt;ref name=Hadzic/&gt;
* specification of the conceptualization
* ontology concepts: every definition involves the definitions of other terms
* relationships between the concepts: this step maps conceptual relationships onto the ontology structure
* groups of concepts: this step may lead to the creation of sub-ontologies
* formal description of ontology commitments, for example, to make them computer readable

An example of moving conception into a language leading to a variety of ontologies is the expression of a process in [[pseudocode]] (a strictly structured form of ordinary language) leading to implementation in several different formal computer languages like [[Lisp (programming language)|Lisp]] or [[Fortran]]. The pseudocode makes it easier to understand the instructions and compare implementations, but the formal languages make possible the compilation of the ideas as computer instructions. {{citation needed|date=August 2013}}

Another example is mathematics, where a very general formulation (the analog of a conceptualization) is illustrated with 'applications' that are more specialized examples. For instance, aspects of a [[function space]] can be illustrated using a [[vector space]] or a [[topological space]] that introduce interpretations of the 'elements' of the conceptualization and additional relationships between them but preserve the connections required in the [[function space]]. {{citation needed|date=August 2013}}

==See also==
*[[Knowledge representation and reasoning]]
*[[Ontology alignment]]
*[[Ontology (computer science)]]
*[[Semantic integration]]
*[[Semantic matching]]
*[[Semantic translation]]

==References==
{{reflist |30em|refs=
&lt;ref name=Alignment&gt;
In information science, one approach to finding a conceptualization (or avoiding it and using an automated comparison) is called 'ontology alignment' or 'ontology matching'. See for example, {{cite book |title=Ontology Matching |url=https://books.google.com/books?id=qYVpA2t2EtQC&amp;printsec=frontcover  |author1=J&#233;r&#244;me. Euzenat |author2=Pavel Shvaiko |isbn=3540496122 |year=2007 |publisher=Springer}}
&lt;/ref&gt;

&lt;ref name=Audi&gt;
{{cite book |title=The Cambridge Dictionary of Philosophy |edition=Paperback 2nd |page= 631 |chapter=Ontological commitment |isbn=0521637228 |author= Roger F. Gibson |editor=Robert Audi  |year=1999 |url=https://books.google.com/books?id=kQQNBTW_hoAC&amp;pg=PT1537}} A shortened version of that definition is as follows:
:The ''ontological commitments'' of a theory are those things which occur in all the ''ontologies'' of that theory. To explain further, the [[ontology]] of a theory consists of the objects the theory makes use of. A dependence of a theory upon an object is indicated if the theory fails when the object is omitted. However, the ontology of a theory is not necessarily unique. A theory is ''ontologically committed'' to an object only if that object occurs in ''all'' the ontologies of that theory. A theory also can be ''ontologically committed'' to a class of objects if that class is populated (not necessarily by the same objects) in all its ontologies. [italics added]
&lt;/ref&gt;

&lt;!-- Unused ref &lt;ref name=Aydede&gt;
{{cite web |title=The language of thought hypothesis |first=Murat|last=Aydede |work= The Stanford Encyclopedia of Philosophy (Fall 2010 Edition) |editor=Edward N. Zalta |url= http://plato.stanford.edu/archives/fall2010/entries/language-thought/  |date=September 17, 2010}}
&lt;/ref&gt; --&gt;

&lt;ref name=Ceccaroni&gt;
For example, see {{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf |journal=Proceedings of the workshop AAMAS |year=2002}}
&lt;/ref&gt;

&lt;ref name=Ceccaroni1&gt;
{{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf  |journal=Proceedings of the workshop AAMAS |year=2002}} A quotation follows:
:&#8220;Researchers...come from different areas of study and have different perspectives on modeling, but significantly they pledged to adopt the same ''ontological commitment''. That is, they agree to adopt common, predefined ontologies...to express general categories, even if they do not completely agree on the modeling behind the ontological representations. Where ontological commitment is lacking, it is difficult to converse clearly about a domain and to benefit from knowledge representations developed by others... Ontological commitment is thus an integral aspect of ontological engineering.&#8221; [italics added]
&lt;/ref&gt;

&lt;!--ref name=CZ&gt;
This figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&amp;pg=PA7&amp;lpg=PA7#v=onepage&amp;q&amp;f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].
&lt;/ref--&gt;

&lt;ref name=Guarino&gt;
{{cite book |title=Formal Ontology in Information Systems (Proceedings of FOIS '98, Trento, Italy) |first=Nicola|last=Guarino |pages=3 ''ff'' |chapter=Formal Ontology in Information Systems |editor=Nicola Guarino |isbn=978-90-5199-399-8 |year=1998 |publisher=IOS Press |url=http://books.google.ca/books?id=Wf5p3_fUxacC&amp;pg=PA7&amp;lpg=PA7}}
&lt;/ref&gt;

&lt;ref name=Gruber&gt;
{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199&#8211;220 |doi=10.1006/knac.1993.1008}}
&lt;/ref&gt;

&lt;ref name=Guarino1&gt;
{{cite journal |title=Formalizing ontological commitments |author1=Nicola Guarino |author2=Massimiliano Carrara |author3=Pierdaniele Giaretta |journal=AAAI |volume=94 |pages=560&#8211;567 |year=1994 |url=http://www.mit.bme.hu/system/files/oktatas/targyak/7412/Formalizing_Ontological_Commitments.pdf}} 
&lt;/ref&gt;

&lt;ref name=Hadzic&gt;
{{cite book |title=Ontology-Based Multi-Agent Systems |chapter=Chapter 7: Design methodology for integrated systems - Part I (Ontology design) |pages=111 ''ff'' |author1=Maja Hadzic |author2=Pornpit Wongthongtham |author3=Elizabeth Chang |author4=Tharam Dillon |isbn=364201903X |year=2009 |publisher=Springer |url=https://books.google.com/books?id=kRoA_vxUwvQC&amp;pg=PA111}}
&lt;/ref&gt;

&lt;ref name=Harmelen&gt;
{{cite web |title=Ontology mapping: a way out of the medical tower of babel |author=Frank van Harmelen |url=http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf}}
&lt;/ref&gt;

&lt;ref name=Smith&gt;
{{cite book |chapter= Chapter 11: Ontology |first=Barry|last=Smith |url=http://ontology.buffalo.edu/smith/articles/ontology_PIC.pdf |title=Blackwell Guide to the Philosophy of Computing and Information |publisher=Blackwell |year=2003 |pages=155&#8211;166 |editor=Luciano Floridi |isbn=0631229183 }}
&lt;/ref&gt;

}}

==Further reading==

*{{cite book |chapter=On Ontology, ontologies, conceptualizations, modeling languages and (meta)models |first=G|last=Guizzardi |title=Frontiers in artificial intelligence and applications, databases and information systems IV |editor=Olegas Vaselicas |editor2=Johan Edler |editor3=Albertas Caplinskas, eds |isbn=978-1-58603-715-4|publisher=IOS Press |year=2007 |url=http://www.loa.istc.cnr.it/Guizzardi/FAIA.pdf }}
*{{cite book |title=Applied ontology: an introduction |editor1=Katherine Munn |editor2=Barry Smith |url=https://books.google.com/books?id=vuYLID7IfqEC&amp;printsec=frontcover |isbn=3938793988 |publisher=Ontos Verlag |year=2008}}

==External links==
*{{cite web |url=http://www.obitko.com/tutorials/ontologies-semantic-web/specification-of-conceptualization.html |title=Specification of conceptualization |work=Ontologies and the semantic web |first=Marek|last=Obitko |year=2006&#8211;2007}}
{{Citizendium|Ontological commitment#Conceptualization}}

[[Category:Information science]]
[[Category:Ontology]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)| ]]
[[Category:Semantic Web]]
[[Category:Technical communication]]</text>
      <sha1>1x4vnd56vz4mdptz5lj381q5ygbsfpk</sha1>
    </revision>
  </page>
  <page>
    <title>Type&#8211;token distinction</title>
    <ns>0</ns>
    <id>14934822</id>
    <revision>
      <id>761239431</id>
      <parentid>760006896</parentid>
      <timestamp>2017-01-21T20:51:54Z</timestamp>
      <contributor>
        <username>KevinHaller</username>
        <id>30158544</id>
      </contributor>
      <minor />
      <comment>I think there was a typo.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9477" xml:space="preserve">[[File:Flock of birds at Rome.ogg|right|300px|thumb|Although this flock is made of the same ''type'' of bird, each individual bird is a different ''token''. (50 MB video of a [[Flock (birds)|flock of birds]] in Rome)|thumbtime=6]]
The '''type&#8211;token distinction''' is used in disciplines such as [[logic]], [[linguistics]], [[metalogic]], [[typography]], and [[computer programming]] to clarify what words mean.

The sentence "''they drive the same car''" is ambiguous. Do they drive the same ''type'' of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type&#8211;token distinction separates types (representing abstract descriptive concepts) from tokens (representing objects that instantiate concepts).

For example: "bicycle" is a type that represents the concept of a bicycle; whereas "my bicycle" is a token that represents an object that instantiates that type. In the sentence "the bicycle is becoming more popular" the word "bicycle" is a type representing a concept; whereas in the sentence "the bicycle is in the garage" the word "bicycle" is a token representing a particular object.

(The distinction in [[computer programming]] between [[class (software)|classes]] and [[object (computer science)|objects]] is related, though in this context, "class" sometimes refers to a set of objects (with class-level attribute or operations) rather than a description of an object in the set.)

The words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is "thorny", "flowering" and "bushy". You might say a rose bush ''instantiates'' these three types, or ''embodies'' these three concepts, or ''exhibits'' these three properties, or ''possesses'' these three qualities, features or attributes.

Property types (e.g "height in metres" or "thorny") are often understood [[ontology|ontologically]] as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.

Some say types exist in descriptions of objects, but not as tangible [[physical object]]s. They say one can show someone a particular bicycle, but cannot show someone the type "bicycle", as in "''the bicycle'' is popular.". However types do exist in sense that they appear in mental and documented models.

Some say tokens represent objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can represent intangible objects of types such as "thought", "tennis match", "government" and "act of kindness".

== Occurrences ==
There is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an '''''occurrence''''' of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: "[[A rose is a rose is a rose]]". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: "rose", "is" and "a". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, 'a', 'is' and 'rose', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type 'a', two of 'is' and three of 'rose'.

The need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them.&lt;ref&gt;Stanford Encyclopedia of Philosophy, ''[http://plato.stanford.edu/entries/types-tokens Types and Tokens]''&lt;/ref&gt; Reflection on the simple case of occurrences of [[numeral]]s is often helpful.{{citation needed|date=January 2017|reason=Claim made with no support or citation, term used without definition.}}

== Typography ==
In [[typography]], the type&#8211;token distinction is used to determine the presence of a text printed by [[movable type]]:&lt;ref&gt;[[Herbert E. Brekle|Brekle, Herbert E.]]: ''Die Pr&#252;feninger Weiheinschrift von 1119. Eine pal&#228;ographisch-typographische Untersuchung'', Scriptorium Verlag f&#252;r Kultur und Wissenschaft, Regensburg 2005, ISBN 3-937527-06-0, p.&amp;nbsp;23&lt;/ref&gt;

{{quote|The defining criteria which a typographic print has to fulfill is that of the type identity of the various [[letter form]]s which make up the printed text. In other words: each letter form which appears in the text has to be shown as a particular instance ("token") of one and the same type which contains a reverse image of the printed [[Letter (alphabet)|letter]].}}

== Charles Sanders Peirce ==
:''There are only 26 letters in the [[English alphabet]] and yet there are more than 26 letters in this [[sentence (linguistics)|sentence]]. Moreover, every time a child writes the alphabet 26 new letters have been created.''

The word 'letters' was used three times in the above paragraph, each time in a different meaning. The word 'letters' is one of many words having "type&#8211;token ambiguity". This section disambiguates 'letters' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher [[Charles Sanders Peirce]] in 1906 using terminology that he established.&lt;ref&gt;Charles Sanders Peirce, Prolegomena to an apology for pragmaticism, Monist, vol.16 (1906), pp. 492&#8211;546.&lt;/ref&gt;

The letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.

'''Peirce's type&#8211;token distinction''', also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or [[concatenation theory]]. There is only one word type spelled el-ee-tee-tee-ee-ar,&lt;ref&gt;Using a variant of [[Alfred Tarski]]'s structural-descriptive naming found in [[John Corcoran (logician)|John Corcoran]] , Schemata: the Concept of Schema in the History of Logic, Bulletin of Symbolic Logic, vol. 12 (2006), pp. 219&#8211;40.&lt;/ref&gt; namely, 'letter'; but every time that word type is written, a new word token has been created.

Some logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.

The word type 'letter' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type 'letter' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type 'letter'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.

Peirce's original words are the following.
"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty 'thes' on a page, and, of course, they count as twenty words. In another sense of the word 'word,' however, there is but one word 'the' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice .... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. .... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies." &#8211; Peirce 1906, Ogden-Richards, 1923, 280-1.

These distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.

:''There are 26 letter types in the English alphabet and yet there are more than 26 letter occurrences in this sentence type. Moreover, every time a child writes the alphabet 26 new letter tokens have been created.''

== See also ==
* [[Formalism (philosophy)]]
* [[Is-a]]
* [[Class (philosophy)]]
* [[Type theory]]
* [[Type physicalism]]
* [[Mental model]]
* [[Map&#8211;territory relation]]
* [[Problem of universals#Peirce]]

==References==
{{reflist}}

===Sources===
*Baggin J., and Fosl, P. (2003) ''The Philosopher's Toolkit''. Blackwell: 171-73. ISBN 978-0-631-22874-5.
*Peper F., Lee J., Adachi S.,Isokawa T. (2004) ''Token-Based Computing on Nanometer Scales'', Proceeding of the ToBaCo 2004 Workshop on Token Based Computing, Vol.1 pp.&amp;nbsp;1&#8211;18.

== External links ==
*[[The Stanford Encyclopedia of Philosophy]]: "[http://plato.stanford.edu/entries/types-tokens/ Types and Tokens]" by Linda Wetzel.

{{Metalogic}}
{{Metaphysics}}

{{DEFAULTSORT:Type-token distinction}}
[[Category:Metalogic]]
[[Category:Conceptual distinctions]]
[[Category:Knowledge representation]]
[[Category:Abstraction]]
[[Category:Concepts in metaphysics]]
[[Category:Articles containing video clips]]
[[Category:Philosophy of logic]]
[[Category:Philosophy of language]]
[[Category:Linguistics]]</text>
      <sha1>tp6fnz64fyz3bikxh0oj18zkc7t9u1w</sha1>
    </revision>
  </page>
  <page>
    <title>Retrievability</title>
    <ns>0</ns>
    <id>39585214</id>
    <revision>
      <id>757295412</id>
      <parentid>753458935</parentid>
      <timestamp>2016-12-29T21:59:33Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility/LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2050" xml:space="preserve">'''Retrievability''' is a term associated with the ease with which information can be found or retrieved using an information system, specifically a [[search engine]] or [[information retrieval]] system.

A document (or information object) has high retrievability if there are many queries which retrieve the document via the search engine, and the document is ranked sufficiently high that a user would encounter the document. Conversely, if there are few queries that retrieve the document, or when the document is retrieved the documents are not high enough in the ranked list, then the document has low retrievability.

Retrievability can be considered as one aspect of [[findability]].

Applications of retrievability include detecting [[Web search engine#Search engine bias|search engine bias]], measuring algorithmic bias, evaluating the influence of search technology, tuning information retrieval systems and evaluating the quality of documents in a [[text corpus|collection]].

==See also==
* [[Information retrieval]]
* [[Knowledge mining]]
* [[Search engine optimization]]
* [[Findability]]

==References==
*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Retrievability: an evaluation measure for higher order information access tasks| title=Proceedings of the 17th ACM conference on Information and knowledge management| year=2008| pages=561&#8211;570| publisher=ACM| location=Napa Valley, California, USA| series=CIKM '08| doi=10.1145/1458082.1458157| url=http://doi.acm.org/10.1145/1458082.1458157| accessdate=5 June 2013}}
*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Accessibility in information retrieval| title=Proceedings of the IR research, 30th European conference on Advances in information retrieval| year=2008| pages=482&#8211;489| publisher=Springer| location=Glasgow,UK| series=ECIR '08| url=http://dl.acm.org/citation.cfm?id=1793333| accessdate=7 Dec 2016}}

[[Category:Web design]]
[[Category:Knowledge representation]]
[[Category:Information science]]</text>
      <sha1>r5seqt4bw65ssmbyouvswhffhqzb4tl</sha1>
    </revision>
  </page>
  <page>
    <title>Transaction logic</title>
    <ns>0</ns>
    <id>12817496</id>
    <revision>
      <id>747984977</id>
      <parentid>668546553</parentid>
      <timestamp>2016-11-05T16:55:17Z</timestamp>
      <contributor>
        <username>Narky Blert</username>
        <id>22041646</id>
      </contributor>
      <comment>DN tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6103" xml:space="preserve">'''Transaction Logic''' is an extension of [[predicate logic]] that accounts in a clean and declarative way for the phenomenon of state changes in [[logic program]]s and [[database]]s. This extension adds connectives specifically designed for combining simple actions into complex transactions and for providing control over their execution. The logic has a natural [[model theory]] and a sound and complete [[proof theory]]. Transaction Logic has a [[Horn clause]] subset, which has a procedural as well as a declarative semantics. The important features of the logic include hypothetical and committed updates, dynamic constraints on transaction execution, non-determinism, and bulk updates.  In this way, Transaction Logic is able to declaratively capture a number of non-logical phenomena, including [[procedural knowledge]] in [[artificial intelligence]], [[active database]]s, and methods with side effects in [[object database]]s.

Transaction Logic was originally proposed in &lt;ref name="tr-iclp1993"&gt;A.J. Bonner and M. Kifer (1993), ''Transaction Logic Programming'', International Conference on Logic Programming (ICLP), 1993.&lt;/ref&gt; by [http://www.cs.toronto.edu/~bonner/ Anthony Bonner] and [http://www.cs.stonybrook.edu/~kifer/ Michael Kifer] and later described in more detail in &lt;ref&gt;A.J. Bonner and M. Kifer (1994), ''An Overview of Transaction Logic'', Theoretical Computer Science, 133:2, 1994.&lt;/ref&gt; and.&lt;ref&gt;A.J. Bonner and M. Kifer (1998), [http://www.cs.sunysb.edu/~kifer/TechReports/tr-chomicki.pdf ''Logic Programming for Database Transactions''] in Logics for Databases and Information Systems, J. Chomicki and G. Saake (eds.), Kluwer Academic Publ., 1998.&lt;/ref&gt; The most comprehensive description appears in.&lt;ref&gt;A.J. Bonner and M. Kifer (1995), [http://www.cs.sunysb.edu/~kifer/TechReports/transaction-logic.pdf ''Transaction Logic Programming (or A Logic of Declarative and Procedural Knowledge)'']. Technical Report CSRI-323, November 1995, Computer Science Research Institute, University of Toronto.&lt;/ref&gt;

In later years, Transaction Logic was extended in various ways, including [[concurrency]]{{dn|date=November 2016}},&lt;ref name="concurrentTR"&gt;A.J. Bonner and M. Kifer (1996), [http://www.cs.sunysb.edu/~kifer/TechReports/concurrent-trans-logic.pdf ''Concurrency and communication in Transaction Logic''], Joint Intl. Conference and Symposium on Logic Programming, Bonn, Germany, September 1996&lt;/ref&gt; [[defeasible reasoning]],&lt;ref&gt;P. Fodor and M. Kifer (2011), [http://drops.dagstuhl.de/opus/volltexte/2011/3159/ ''Transaction Logic with Defaults and Argumentation Theories'']. In Technical communications of the 27th International Conference on Logic Programming (ICLP), July 2011.&lt;/ref&gt; partially defined actions,&lt;ref&gt;M. Rezk and M. Kifer (2012), [http://link.springer.com/article/10.1007%2Fs13740-012-0007-8 ''Transaction Logic with Partially Defined Actions'']. Journal on Data Semantics, August 2012, vol. 1, no. 2, Springer.&lt;/ref&gt; and other features.&lt;ref&gt;H. Davulcu, M. Kifer and I.V. Ramakrishnan (2004), [http://www.www2004.org/proceedings/docs/2p144.pdf CTR-S: A Logic for Specifying Contracts in Semantic Web Services'']. Proceedings of the 13-th World Wide Web Conference (WWW2004), May 2004.&lt;/ref&gt;&lt;ref&gt;P. Fodor and M. Kifer (2010), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.6968 ''Tabling for Transaction Logic'']. In Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming (PPDP), July 2010.&lt;/ref&gt;

In 2013, the original paper on Transaction Logic &lt;ref name="tr-iclp1993"/&gt; has won the 20-year Test of Time Award as the most influential paper from the proceedings of [http://www.informatik.uni-trier.de/~ley/db/conf/iclp/iclp93.html ICLP 1993 conference] in the preceding 20 years.{{citation needed|date=February 2014}}

== Examples ==

Graph coloring. Here &lt;tt&gt;tinsert&lt;/tt&gt; denotes the elementary update operation of ''transactional insert''. The connective &#8855; is called ''serial conjunction''.
 colorNode &lt;-  // color one node correctly
     node(N) &#8855; &amp;neg; colored(N,_) &#8855; color(C)
     &#8855; &#172;(adjacent(N,N2) &#8743; colored(N2,C))
     &#8855; tinsert(colored(N,C)).
 colorGraph &lt;- &#172;uncoloredNodesLeft.
 colorGraph &lt;- colorNode &#8855; colorGraph.

Pyramid stacking. The elementary update &lt;tt&gt;tdelete&lt;/tt&gt; represents the ''transactional delete'' operation.
 stack(N,X) &lt;- N&gt;0 &#8855; move(Y,X) &#8855; stack(N-1,Y).
 stack(0,X).
 move(X,Y) &lt;- pickup(X) &#8855; putdown(X,Y).
 pickup(X) &lt;- clear(X) &#8855; on(X,Y) &#8855;
              &#8855; tdelete(on(X,Y)) &#8855; tinsert(clear(Y)).
 putdown(X,Y) &lt;-  wider(Y,X) &#8855; clear(Y) 
                  &#8855; tinsert(on(X,Y)) &#8855; tdelete(clear(Y)).

Hypothetical execution. Here &lt;tt&gt;&amp;lt;&amp;gt;&lt;/tt&gt; is the modal operator of possibility: If both &lt;tt&gt;action1&lt;/tt&gt; and &lt;tt&gt;action2&lt;/tt&gt; are possible, execute &lt;tt&gt;action1&lt;/tt&gt;. Otherwise, if only &lt;tt&gt;action2&lt;/tt&gt; is possible, then execute it.
  execute &lt;- &lt;&gt;action1 &#8855; &lt;&gt;action2 &#8855; action1.
  execute &lt;- &#172;&lt;&gt;action1 &#8855; &lt;&gt;action2 &#8855; action2.

Dining philosophers. Here | is the logical connective of parallel conjunction of Concurrent Transaction Logic.&lt;ref name="concurrentTR"/&gt;
 diningPhilosophers &lt;- phil(1) | phil(2) | phil(3) | phil(4).

== Implementations ==

A number of implementations of Transaction Logic exist. The original implementation is available [http://www.cs.toronto.edu/~bonner/transaction-logic.html here]. An implementation of Concurrent Transaction Logic is available [http://www.cs.toronto.edu/~bonner/ctr/index.html here]. Transaction Logic enhanced with [[tabling]] is available [http://flora.sourceforge.net/tr-interpreter-suite.tar.gz here]. An implementation of Transaction Logic has also been incorporated as part of the [[Flora-2]] knowledge representation and reasoning system. All these implementations are [[open source]].

Additional papers on Transaction Logic can be found on the [http://flora.sourceforge.net Flora-2 Web site].

== References ==
{{Reflist}}

[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]</text>
      <sha1>sjzstulfw3hl9xm8hbdhan3lc78u29k</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge Engineering Environment</title>
    <ns>0</ns>
    <id>11856314</id>
    <revision>
      <id>674425454</id>
      <parentid>661542141</parentid>
      <timestamp>2015-08-03T21:08:17Z</timestamp>
      <contributor>
        <username>Hampton11235</username>
        <id>22860730</id>
      </contributor>
      <minor />
      <comment>/* External links */Typo fix, replaced: External references &#8594; External links using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2774" xml:space="preserve">'''KEE''' (Knowledge Engineering Environment) is a [[Frame language|frame-based]] development tool for [[Expert system|Expert Systems]].&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 An evaluation of expert system development tools]&lt;/ref&gt; KEE was developed and sold by [[IntelliCorp (Software)|IntelliCorp]]. It was first released in 1983 and ran on [[Lisp Machine]]s. KEE was later ported to Lucid [[Common Lisp]] with [[CLX (Common Lisp)|CLX]] (X11 interface for Common Lisp). This version was available on various Workstations.

On top of KEE several extensions were offered:

* Simkit,&lt;ref&gt;[http://doi.acm.org/10.1145/76738.76766 The SimKit system: knowledge-based simulation and modeling tools in KEE]&lt;/ref&gt;&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 SimKit: a model-building simulation toolkit]&lt;/ref&gt; a frame-based simulation library
* KEEconnection,&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 KEEConnection: a bridge between databases and knowledge bases]&lt;/ref&gt; [[database connection]] between the frame system and relational databases.

Frames are called ''Units'' in KEE. Units are used for both individual instances and classes. Frames have ''slots'' and slots have ''facets''. Facets for example describe the expected values of a slot, the inheritance rule for the slot or the value of a slot. Slots can have multiple values. Behavior can be implemented using the message-passing paradigm.

KEE provides an extensive graphical user interface to create, browse and manipulate frames.

KEE also includes a frame-based [[Production system (computer science)|rule system]]. Rules themselves are frames in the KEE knowledge base. Both forward and backward chaining inference is available.

KEE supports non-monotonic reasoning through the concepts of ''worlds''. Worlds allow provide alternative slot-values of frames. Through an assumption-based [[Truth maintenance system]] inconsistencies can be detected and analyzed.&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 Reasoning with worlds and truth maintenance]&lt;/ref&gt;

''ActiveImages'' allows graphical displays to be attached to slots of Units. Typical examples are buttons, dials, graphs and histograms. The graphics are also implemented as Units via ''KEEPictures'' - a frame-based graphics library.

==See also==
* [[Expert system]]
* [[Frame language]]
* [[Inference engine]]
* [[IntelliCorp (software)|IntelliCorp]]
* [[Knowledge base]]
* [[Knowledge-based system]]
* [[Knowledge representation]]

==References==
&lt;references/&gt;

==External links==
* [http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625 An Assessment of Tools for Building Large Knowledge-Based Systems]

[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
[[Category:Common Lisp software]]</text>
      <sha1>4levqolrpi3i0253usxau1t2znskt3z</sha1>
    </revision>
  </page>
  <page>
    <title>PowToon</title>
    <ns>0</ns>
    <id>38027627</id>
    <revision>
      <id>758471365</id>
      <parentid>758306928</parentid>
      <timestamp>2017-01-05T17:05:06Z</timestamp>
      <contributor>
        <username>Pinkbeast</username>
        <id>11291690</id>
      </contributor>
      <comment>/* History */ Please actually check when adding wikilinks that they go to the right place</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3054" xml:space="preserve">{{Underlinked|date=March 2014}}
{{Infobox company
| name = PowToon
| type = [[Private company|Private]]
| foundation = {{Start date and age|2012}}
| location_city =  28 Church Rd&lt;br /&gt;[[London, UK]]
| location_country = [[United Kingdom]]
| key_people = Ilya Spitalnik (Co-Founder and CEO), Daniel Zaturansky (Co-Founder and COO), Sven Hoffman (Co-Founder and CTO)
| industry = [[Internet Marketing]]
| products = PowToon Web-based animation software
| homepage = {{URL|www.powtoon.com}}
}}
'''PowToon''' is a company which sells cloud-based software [[Software as a service|(SaaS)]] for creating animated presentations and animated explainer videos.&lt;ref&gt;Perez, Sarah. [http://techcrunch.com/2012/06/26/now-everyone-can-make-marketing-videos-powtoon-launches-diy-presentation-tool/ TechCrunch], June 26th, 2012, "Now Everyone Can Make Marketing Videos: PowToon Launches DIY Presentation Tool"&lt;/ref&gt;

== History ==
PowToon was founded in January 2012. The company released a [[Software_release_life_cycle#BETA|beta]] version in August 2012 and has seen fast subscriber growth since.&lt;ref name="powtoon"&gt;[http://www.powtoon.com Powtoon Website]&lt;/ref&gt; In December 2012 PowToon secured $600,000 investment from LA based Venture Capital firm Startup Minds.&lt;ref&gt;Perez, Sarah, [http://techcrunch.com/2012/12/14/diy-animation-platform-powtoon-grabs-600k-for-its-video-creation-software/ TechCrunch] , Dec 14, 2012, "DIY Animation Platform PowToon Grabs $600K For Its Video Creation Software"&lt;/ref&gt; In February 2013 PowToon introduced a free account option allowing users to create animated videos that can be exported to [[YouTube]]. The free videos include the PowToon branding.

== Product ==
PowToon is Web-based animation software that allows users to create animated presentations  by manipulating pre-created objects, imported images, provided music and user created voice-overs.&lt;ref&gt;{{cite web|last=Mersand |first=Shannon |title=Product Review: PowToon|url=http://www.techlearning.com/product-reviews/0072/product-review-powtoon-/54971|publisher=''Tech and Learning''|accessdate=12 May 2014|date=May 2014}}&lt;/ref&gt; 
Powtoon uses an [[Apache Flex]] engine to generate an XML file that can be played in the Powtoon online viewer, exported to YouTube or downloaded as an MP4 file.&lt;ref name="powtoon" /&gt;

PowToon is also available on the Google Chrome Store&lt;ref&gt;{{citation |title=PowToon - Chrome Web Store|url=https://chrome.google.com/webstore/detail/powtoon/aomfhbjiekjcbeefclbidjgnikfbooem?hl=en|accessdate=25 February 2015|date=Feb 2015}}&lt;/ref&gt;  and has an application on Edmodo.com.&lt;ref&gt;{{citation |title=PowToon by PowToon Ltd|url=https://www.edmodo.com/store/app/powtoon-1|accessdate=25 February 2015|date=Feb 2015}}&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* {{official website|http://www.powtoon.com/}}

[[Category:Animation software]]
[[Category:Companies established in 2012]]
[[Category:Cloud applications]]
[[Category:Computer animation]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]

{{animation-stub}}</text>
      <sha1>cedlou6fgmuj7yhnweqsede3vdhdb2y</sha1>
    </revision>
  </page>
  <page>
    <title>Flail space model</title>
    <ns>0</ns>
    <id>47369663</id>
    <revision>
      <id>747918838</id>
      <parentid>747918816</parentid>
      <timestamp>2016-11-05T05:16:35Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/104.173.100.38|104.173.100.38]] ([[User talk:104.173.100.38|talk]]): Unexplained removal of content ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4767" xml:space="preserve">The '''flail space model (FSM)''' is a [[Physical model|model]] of how a [[passenger|car passenger]] moves in a [[vehicle]] that collides with a roadside feature such as a [[Traffic barrier|guardrail]] or a [[crash cushion]]. Its principal purpose is to assess the potential risk of harm to the hypothetical occupant as he or she impacts the interior of the passenger compartment and, ultimately, the efficacy of an experimental roadside feature undergoing full-scale vehicle crash testing.

The FSM eliminates the complexity and expense of using instrumented [[Crash test dummy|anthropometric dummies]] during the crash test experiments. Furthermore, while crash test dummies were developed to model collisions between vehicles, they are not accurate when used for the sorts of collision angles that occur when a vehicle collides with a roadside feature; by contrast, the FSM was designed for such collisions.&lt;ref name="gabauer"&gt;Gabauer, Douglas, "A methodology to evaluate the flail space model using event data recorder technology", Department of Mechanical Engineering, Rowan University, Glassboro, NJ, 2004.&lt;/ref&gt;

== History ==
The FSM is based on research performed at [[Southwest Research Institute]] in 1980&lt;ref&gt;Michie, J. D., "Development of improved criteria for evaluating safety performance of highway appurtenances", Final Report of  Internal Research Project No. 03-9254, Southwest Research Institute, San Antonio, Texas, June 1980.&lt;/ref&gt; and published in 1981 in the paper entitled "Collision Risk Assessment Based on Occupant Flail-Space Model" by Jarvis D. Michie.&lt;ref name=":0"&gt;Michie, J. D., "Collision risk assessment based on occupant flail space model," in Transportation Research Record 796, 1981, pp. 1&#8211;9.&lt;/ref&gt; The FSM (coined by Michie) was accepted by the highway community and published as a key part of the "Recommended Procedures for the Safety Evaluation of Highway Appurtenances" published in 1981 in [[National Cooperative Highway Research Program]] (NCHRP) Report 230.&lt;ref&gt;Michie, J. D.  National Cooperative Highway Research Program Report 230: Recommended Procedures for the Safety Performance Evaluation of Highway Appurtenances.  NCHRP Transportation Research Board, Washington, DC, March 1981.&lt;/ref&gt; In 1993, the NCHRP Report was updated and presented as NCHRP Report 350;&lt;ref&gt;Ross, H. E., Jr. et al.  National Cooperative Highway Research Program Report 350:  Recommended Procedures for the Safety Evaluation of Highway Features.  NCHRP Transportation Research Board, Washington, DC, 1993.&lt;/ref&gt; in this research effort performed by the [[Texas A&amp;M Transportation Institute|Texas Transportation Research Institute]], the FSM was reexamined and was unmodified in the new publication. In 2004, Douglas Gabauer further examined the efficacy of the FSM in his [[PhD thesis]].&lt;ref name="gabauer" /&gt; The [[American Association of State Highway and Transportation Officials]] (AASHTO) retained the FSM as the method of assessing the risk of harm to vehicle occupants in the 2009 "Manual for Assessing Safety Hardware" that replaced NCHRP Report 350, stating that the FSM had "served its intended purpose well".&lt;ref&gt;Manual for Assessing Safety Hardware.  American Association of State Highway and Transportation Officials, Washington, DC,  2009.&lt;/ref&gt;

== Details ==
The FSM hypothesis divides the collision into two stages.  In stage one, the unrestrained occupant is propelled forward and sideways in the compartment space due to vehicle collision [[Acceleration|accelerations]] and then impacts one or more surfaces (including the steering wheel) with velocity "V". According to the model, the vehicle (instead of the occupant) is the object that is accelerating. The occupant experiences no injury-producing force prior to contact with the compartment surfaces.&lt;ref name=":0" /&gt;

In stage two, the occupant is assumed to remain in contact with the compartment surface and experiences the same accelerations as the vehicle for the rest of the collision.  The occupant may sustain [[Blunt trauma|injury]] at the end of stage one based on the velocity of impact with the compartment surfaces and due to vehicle accelerations during stage two.  The occupant impact velocity and acceleration are computed from the vehicle collision acceleration history and the compartment geometry.  Finally, the hypothetical occupant impact velocity and acceleration are then compared to threshold values of [[Engineering tolerance|human tolerance]] to these forces.&lt;ref name=":0" /&gt;

==References==
{{reflist|colwidth=30em}}

[[Category:Articles created via the Article Wizard]]
[[Category:Scientific modeling]]
[[Category:Applied mathematics]]
[[Category:Knowledge representation]]
[[Category:Mathematical modeling]]
[[Category:Transport safety]]</text>
      <sha1>i20l5qnty5ur8n1rljkfu169uj0rmat</sha1>
    </revision>
  </page>
  <page>
    <title>Unified Modeling Language</title>
    <ns>0</ns>
    <id>32169</id>
    <revision>
      <id>760575574</id>
      <parentid>760106171</parentid>
      <timestamp>2017-01-17T21:02:30Z</timestamp>
      <contributor>
        <ip>139.102.14.206</ip>
      </contributor>
      <comment>Internal link to "Silver Bullet" goes to page about actual bullets made of silver.  Changed link to point to article about paper "No Silver Bullet".</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20621" xml:space="preserve">{{Use American English|date=January 2012}}
[[File:UML logo.gif|thumb|UML logo]]
The '''Unified Modeling Language''' ('''UML''') is a general-purpose, developmental,  [[modeling language]] in the field of [[software engineering]], that is intended to provide a standard way to visualize the design of a system.&lt;ref name=":1" /&gt;

UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design developed by [[Grady Booch]], [[Ivar Jacobson]] and [[James Rumbaugh]] at [[Rational Software]] in 1994&#8211;1995, with further development led by them through 1996.&lt;ref name=":1" /&gt;

In 1997 UML was adopted as a standard by the [[Object Management Group]] (OMG), and has been managed by this organization ever since. In 2005 UML was also published by the [[International Organization for Standardization]] (ISO) as an approved ISO standard.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32620 |title=ISO/IEC 19501:2005 - Information technology - Open Distributed Processing - Unified Modeling Language (UML) Version 1.4.2 |publisher=Iso.org |date=2005-04-01 |accessdate=2015-05-07}}&lt;/ref&gt; Since then it has been periodically revised to cover the latest revision of UML.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32624 |title=ISO/IEC 19505-1:2012 - Information technology - Object Management Group Unified Modeling Language (OMG UML) - Part 1: Infrastructure |publisher=Iso.org |date=2012-04-20 |accessdate=2014-04-10}}&lt;/ref&gt;

== History ==

[[File:OO Modeling languages history.jpg|thumb|320px|History of object-oriented methods and notation]]

=== Before UML 1.x ===

UML has been evolving since the second half of the 1990s and has its roots in the object-oriented methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.

It is originally based on the notations of the [[Booch method]], the [[object-modeling technique]] (OMT) and [[object-oriented software engineering]] (OOSE), which it has integrated into a single language.&lt;ref name=":0" /&gt;

[[Rational Software Corporation]] hired [[James Rumbaugh]] from [[General Electric]] in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day:&lt;ref&gt;Andreas Zendler (1997) ''Advanced Concepts, Life Cycle Models and Tools for Objeckt-Oriented Software Development''. p.122&lt;/ref&gt; Rumbaugh's [[object-modeling technique]] (OMT) and [[Grady Booch]]'s method. They were soon assisted in their efforts by [[Ivar Jacobson]], the creator of the [[object-oriented software engineering]] (OOSE) method, who joined them at Rational in 1995.&lt;ref name=":1"&gt;{{cite book
 | title = Unified Modeling Language User Guide, The
 | publisher = Addison-Wesley
 | edition = 2
 | year = 2005
 | page = 496
 | url = http://www.informit.com/store/unified-modeling-language-user-guide-9780321267979
 | isbn = 0321267974
}}
, See the sample content, look for history&lt;/ref&gt;

=== UML 1.x ===

Under the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the [[UML Partners]] was organized in 1996 to complete the ''Unified Modeling Language (UML)'' specification, and propose it to the Object Management Group (OMG) for standardisation. The partnership also contained additional interested parties (for example [[Hewlett-Packard|HP]], [[Digital Equipment Corporation|DEC]], [[IBM]] and [[Microsoft]]). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by [[Cris Kobryn]] and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.&lt;ref name=":1" /&gt;&lt;ref&gt;{{cite web|url=http://www.omg.org/cgi-bin/doc?ad/97-08-11 |title=UML Specification version 1.1 (OMG document ad/97-08-11) |publisher=Omg.org |accessdate=2011-09-22}}&lt;/ref&gt;

After the first release a task force was formed&lt;ref name=":1" /&gt; to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.&lt;ref&gt;{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2014-04-10}}&lt;/ref&gt;

The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.&lt;ref&gt;G&#233;nova et alia 2004 "Open Issues in Industrial Use Case Modeling"&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.uml-forum.com/docs/papers/CACM_Jan02_p107_Kobryn.pdf |title=Will UML 2.0 Be Agile or Awkward? |format=PDF |accessdate=2011-09-22}}&lt;/ref&gt;

=== UML 2.x ===

UML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.&lt;ref&gt;{{cite web|url=http://www.omg.org/spec/UML/2.0/ |title=UML 2.0 |publisher=Omg.org |accessdate=2011-09-22}}&lt;/ref&gt;

Although UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010.&lt;ref name="spec"&gt;{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2011-09-22}}&lt;/ref&gt; UML 2.4.1 was formally released in August 2011.&lt;ref name="spec"/&gt; UML 2.5 was released in October 2012 as an "In process" version and was officially released in June 2015.&lt;ref name="spec"/&gt;

There are four parts to the UML 2.x specification:

# The Superstructure that defines the notation and semantics for diagrams and their model elements
# The Infrastructure that defines the core metamodel on which the Superstructure is based
# The [[Object Constraint Language]] (OCL) for defining rules for model elements
# The UML Diagram Interchange that defines how UML 2 diagram layouts are exchanged

The current versions of these standards follow: UML Superstructure version 2.4.1, UML Infrastructure version 2.4.1, OCL version 2.3.1, and UML Diagram Interchange version 1.0.&lt;ref name="Versions"&gt;{{cite web|author=OMG|title=OMG Formal Specifications (Modeling and Metadata paragraph)|url=http://www.omg.org/spec/#M&amp;M|accessdate = 2016-02-12}}&lt;/ref&gt; It continues to be updated and improved by the revision task force, who resolve any issues with the language.&lt;ref&gt;{{cite web|url=http://www.omg.org/issues/uml2-rtf.open.html |title=Issues for UML 2.6 Revision task Force mailing list |publisher=Omg.org |accessdate=2014-04-10}}&lt;/ref&gt;

== Design ==

UML offers a way to visualize a system's architectural blueprints in a diagram (see image), including elements such as:&lt;ref name=":0"&gt;{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Superstructure/PDF |title=OMG Unified Modeling Language (OMG UML), Superstructure. Version 2.4.1 |publisher=Object Management Group |accessdate=9 April 2014}}&lt;/ref&gt;

* any [[Activity (UML)|activities]] (jobs);
* individual [[Component (UML)|components]] of the system;
** and how they can interact with other [[Component-based software engineering|software components]];
* how the system will run;
* how entities interact with others (components and interfaces);
* external [[user interface]].

Although originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above),&lt;ref&gt;Satish Mishra (1997). [http://www2.informatik.hu-berlin.de/~hs/Lehre/2004-WS_SWQS/20050107_Ex_UML.ppt "Visual Modeling &amp; Unified Modeling Language (UML): Introduction to UML"]. Rational Software Corporation. Accessed 9 November 2008.&lt;/ref&gt; and been found useful in many contexts.&lt;ref name="UML, Success Stories"&gt;{{cite web|url=http://www.uml.org/uml_success_stories/index.htm|title=UML, Success Stories|accessdate=9 April 2014}}&lt;/ref&gt;

=== Software development methods ===

UML is not a development method by itself;&lt;ref&gt;John Hunt (2000). ''The Unified Process for Practitioners: Object-oriented Design, UML and Java''. Springer, 2000. ISBN 1-85233-275-1. p.5.door&lt;/ref&gt; however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example [[Object-modeling technique|OMT]], [[Booch method]], [[Objectory]] and especially [[Rational Unified Process|RUP]] that it was originally intended to be used with when work began at Rational Software.

=== Modeling ===

It is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system's model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).

UML diagrams represent two different views of a system model:&lt;ref&gt;Jon Holt Institution of Electrical Engineers (2004). ''UML for Systems Engineering: Watching the Wheels'' IET, 2004, ISBN 0-86341-354-4. p.58&lt;/ref&gt;

* Static (or ''structural'') view: emphasizes the static structure of the system using objects, attributes, operations and relationships. It includes [[class diagram]]s and [[composite structure diagram]]s.
* Dynamic (or ''behavioral'') view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes [[sequence diagram]]s, [[activity diagram]]s and [[UML state machine|state machine diagrams]].

UML models can be exchanged among UML tools by using the [[XML Metadata Interchange]] (XMI) format.

== Diagrams ==
{{UML diagram types}}

UML 2 has many types of diagrams, which are divided into two categories.&lt;ref name=":0" /&gt; Some types represent ''structural'' information, and the rest represent general types of ''behavior'', including a few that represent different aspects of ''interactions''. These diagrams can be categorized hierarchically as shown in the following class diagram:&lt;ref name=":0" /&gt;

[[File:UML diagrams overview.svg|center|600px|Hierarchy of UML 2.2 Diagrams, shown as a [[class diagram]]]]

These diagrams may all contain comments or notes explaining usage, constraint, or intent.

=== Structure diagrams ===

[[Structure diagram]]s emphasize the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the [[software architecture]] of software systems. For example, the [[component diagram]] describes how a software system is split up into components and shows the dependencies among these components.

&lt;gallery class="center"&gt;
Policy Admin Component Diagram.PNG|[[Component diagram]]
BankAccount1.svg|[[Class diagram]]
&lt;/gallery&gt;

=== Behavior diagrams ===

Behavior diagrams emphasize what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the [[activity diagram]] describes the business and operational step-by-step activities of the components in a system.

&lt;gallery class="center"&gt;
Activity conducting.svg|[[Activity diagram]]
UML Use Case diagram.svg|[[Use case diagram]]
&lt;/gallery&gt;

==== Interaction diagrams ====

Interaction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the [[sequence diagram]] shows how objects communicate with each other in terms of a sequence of messages.

&lt;gallery class="center"&gt;
CheckEmail.svg|[[Sequence diagram]]
UML Communication diagram.svg|[[Communication diagram]]
&lt;/gallery&gt;

== Meta modeling ==
{{Main article|Meta-Object Facility}}

[[File:M0-m3.png|thumb|320px|Illustration of the Meta-Object Facility]]

The Object Management Group (OMG) has developed a [[metamodeling]] architecture to define the UML, called the [[Meta-Object Facility]].&lt;ref&gt;Iman Poernomo (2006) "[http://calcium.dcs.kcl.ac.uk/1259/1/acm-paper.pdf The Meta-Object Facility Typed]" in: ''Proceeding SAC '06 Proceedings of the 2006 ACM symposium on Applied computing''. pp. 1845-1849&lt;/ref&gt; MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.

The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.&lt;ref&gt;{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Infrastructure/PDF/ |title=UML 2.4.1 Infrastructure |publisher=Omg.org |date=2011-08-05 |accessdate=2014-04-10}}&lt;/ref&gt;

The meta-model can be extended using a mechanism called [[stereotype (UML)|stereotyping]]. This has been criticised as being insufficient/untenable by [[Brian Henderson-Sellers]] and Cesar Gonzalez-Perez in "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0".&lt;ref name="UsesAbusesStereotype"&gt;B. Henderson-Sellers; C. Gonzalez-Perez (2006). "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0". in: ''Model Driven Engineering Languages and Systems''. Springer Berlin / Heidelberg.&lt;/ref&gt;

== Adoption ==

UML has been found useful in many design contexts.&lt;ref name="UML, Success Stories"/&gt;&lt;ref&gt;{{Cite web|url = http://www.drdobbs.com/architecture-and-design/uml-25-do-you-even-care/240163702?queryText=uml|title = UML 2.5: Do you even care?}} "UML truly is ubiquitous"&lt;/ref&gt;

It has been treated, at times, as a design [[no silver bullet|silver bullet]], which has led to problems in its usage. Misuse of it includes excessive usage of it (design every little part of the system's [[Programming code|code]] with it, which is unnecessary) and assuming that anyone can design anything with it (even those who haven't [[Programmer|programmed]]).&lt;ref&gt;{{Cite web|url = http://queue.acm.org/detail.cfm?id=984495|title = Death by UML Fever}}&lt;/ref&gt;

It is seen to be a large language, with many [[Syntax (programming languages)|constructs]] in it. Some (including [[Ivar Jacobson|Jacobson]]) feel that there are too many and that this hinders the learning (and therefore usage) of it.&lt;ref&gt;{{Cite web|url = http://www.infoq.com/interviews/Ivar_Jacobson|title = Ivar Jacobson on UML, MDA, and the future of methodologies}}&lt;/ref&gt;

== Criticisms ==
{{Criticism section|date=December 2010}}

Common criticisms of UML from industry include:&lt;ref name="petre"&gt;{{Cite conference| quote=The majority of those interviewed simply do not use UML, and those who do use it tend to do so selectively and often informally|conference=35th International Conference on Software Engineering 18&#8211;26 May 2013 |url=http://oro.open.ac.uk/35805/8/UML%20in%20practice%208.pdf|title=UML in practice|first=Marian|last=Petre| date=2013|pages=722&#8211;731}}&lt;/ref&gt;

* not useful: "[does] not offer them advantages over their current, evolved practices and representations"
* too complex, particularly for communication with clients: "unnecessarily complex" and "The best reason not to use UML is that it is not &#8216;readable&#8217; for all stakeholders. How much is UML worth if a business user (the customer) can not understand the result of your modelling effort?"
* need to keep UML and code in sync, as with documentation generally

=== Critique of UML 1.x ===

; Cardinality notation: As with database Chen, Bachman, and ISO [[ER diagram]]s, class models are specified to use "look-across" [[Cardinality (data modeling)|cardinalities]], even though several authors ([[Merise]],&lt;ref&gt;Hubert Tardieu, Arnold Rochfeld and Ren&#233; Colletti La methode MERISE: Principes et outils (Paperback - 1983)&lt;/ref&gt; Elmasri &amp; Navathe&lt;ref&gt;Elmasri, Ramez, B. Shamkant, Navathe, Fundamentals of Database Systems, third ed., Addison-Wesley, Menlo Park, CA, USA, 2000.&lt;/ref&gt; amongst others&lt;ref&gt;[https://books.google.com/books?id=odZK99osY1EC&amp;pg=PA52&amp;img=1&amp;pgis=1&amp;dq=genova&amp;sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&amp;edge=0 ER 2004 : 23rd International Conference on Conceptual Modeling, Shanghai, China, 8-12 November 2004] {{webarchive |url=https://web.archive.org/web/20130527133330/https://books.google.com/books?id=odZK99osY1EC&amp;pg=PA52&amp;img=1&amp;pgis=1&amp;dq=genova&amp;sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&amp;edge=0 |date=27 May 2013 }}&lt;/ref&gt;) prefer same-side or "look-here" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer,&lt;ref&gt;{{cite web|url=http://publik.tuwien.ac.at/files/pub-inf_4582.pdf |title=A Formal Treatment of UML Class Diagrams as an Efficient Method for Configuration Management 2007 |format=PDF |accessdate=2011-09-22}}&lt;/ref&gt; Dullea et. alia&lt;ref&gt;{{cite web|url=http://www.ischool.drexel.edu/faculty/song/publications/p_DKE_03_Validity.pdf |title=James Dullea, Il-Yeol Song, Ioanna Lamprou - An analysis of structural validity in entity-relationship modeling 2002 |format=PDF |accessdate=2011-09-22}}&lt;/ref&gt;) have shown that the "look-across" technique used by UML and ER diagrams is less effective and less coherent when applied to ''n''-ary relationships of order strictly greater than 2.

: Feinerer says: "Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann&lt;ref&gt;{{cite web|url=http://crpit.com/confpapers/CRPITV17Hartmann.pdf |title="Reasoning about participation constraints and Chen's constraints" S Hartmann - 2003 |format=PDF |accessdate=2013-08-17}}&lt;/ref&gt; investigates this situation and shows how and why different transformations fail.", and: "As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to ''n''-ary associations."

== See also ==
{{Portal|Software}}

* [[Object Oriented Role Analysis and Modeling]]
* [[Model-based testing]]
* [[Model-driven engineering]]
* [[Applications of UML]]
* [[List of Unified Modeling Language tools]]

== References ==
{{FOLDOC}}

{{reflist|colwidth=30em}}

== Further reading ==

* {{cite book

 | first= Scott William
 | last = Ambler
 | year = 2004
 | url = http://www.ambysoft.com/books/theObjectPrimer.html
 | title = The Object Primer: Agile Model Driven Development with UML 2
 | publisher = Cambridge University Press
 | isbn=0-521-54018-6
}}

* {{cite book

 | first= Michael Jesse
 | last = Chonoles
 | author2=James A. Schardt
 | year = 2003
 | title = UML 2 for Dummies
 | publisher = Wiley Publishing
 | isbn=0-7645-2614-6
}}

* {{cite book

 | first = Martin
 | last = Fowler
 | authorlink = Martin Fowler
 | title = UML Distilled: A Brief Guide to the Standard Object Modeling Language
 | edition = 3rd
 | publisher = Addison-Wesley
 | isbn = 0-321-19368-7
}}

* {{cite book

 | first= Ivar
 | last = Jacobson |author2=Grady Booch |author3=James Rumbaugh
 | authorlink = Ivar Jacobson
 | year = 1998
 | title = The Unified Software Development Process
 | publisher = Addison Wesley Longman
 | isbn=0-201-57169-2
}}

* {{cite book

 | first = Robert Cecil
 | last = Martin
 | authorlink = Robert Cecil Martin
 | year = 2003
 | title = UML for Java Programmers
 | publisher = Prentice Hall
 | isbn = 0-13-142848-9
}}

* {{cite web

 | author = Noran, Ovidiu S.
 | url = http://www.cit.gu.edu.au/~noran/Docs/UMLvsIDEF.pdf
 | title = Business Modelling: UML vs. IDEF
 | format = PDF
 | accessdate = 2005-12-28
}}

* {{cite web

 | author = Horst Kargl
 | url = http://umlnotation.sparxsystems.eu/
 | title = Interactive UML Metamodel with additional Examples
 }}

* {{cite book

 | first = Magnus
 | last = Penker
 | author2=Hans-Erik Eriksson
 | author-link2= Hans-Erik Eriksson
 | year = 2000
 | title = Business Modeling with UML
 | publisher = John Wiley &amp; Sons
 | isbn = 0-471-29551-5
}}

== External links ==
{{Commons}}

{{Wikiversity|UML}}

* {{Official website}}

{{UML}}

{{Software engineering}}

{{ISO standards}}

{{Use dmy dates|date=July 2011}}

{{Authority control}}

[[Category:Architecture description language]]
[[Category:Data modeling languages]]
[[Category:Data modeling diagrams]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:ISO standards]]
[[Category:Specification languages]]
[[Category:Unified Modeling Language| ]]
[[Category:Software modeling language]]</text>
      <sha1>pxj03bgrblz0zh00cravcupivradz38</sha1>
    </revision>
  </page>
  <page>
    <title>Categorization</title>
    <ns>0</ns>
    <id>72717</id>
    <revision>
      <id>759688568</id>
      <parentid>759685697</parentid>
      <timestamp>2017-01-12T17:02:42Z</timestamp>
      <contributor>
        <username>Fyrael</username>
        <id>6675779</id>
      </contributor>
      <comment>This currently reads as though it's an entire article of original research</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8344" xml:space="preserve">{{Refimprove|date=January 2017}}
{{selfref|For information about Wikipedia's article categories, see [[Help:Category]].}}
{{for|particular uses|Category (disambiguation)}}
{{Information science}}
'''Categorization''' is the process in which ideas and objects are recognized, differentiated, and understood.&lt;ref&gt;Cohen, H., &amp; Lefebvre, C. (Eds.). (2005).[https://books.google.com/books?id=5WDfl14RgKMC ''Handbook of Categorization in Cognitive Science'']. Elsevier.&lt;/ref&gt; Categorization implies that objects are grouped into categories, usually for some specific purpose. Ideally, a category illuminates a [[Binary relation|relationship]] between the [[subject (philosophy)|subject]]s and [[object (philosophy)|object]]s of knowledge. Categorization is fundamental in language, prediction, [[inference]], decision making and in all kinds of environmental interaction. It is indicated that categorization plays a major role in [[computer programming]].&lt;ref&gt;Frey, T., Gelhausen, M., &amp; Saake (2011).[http://ecs.victoria.ac.nz/twiki/pub/Events/PLATEAU/Program/plateau2011-frey.pdf '' Categorization of Concerns &#8211; A Categorical Program Comprehension Model. In Proceedings of the Workshop on Evaluation and Usability of Programming Languages and Tools (PLATEAU) at the ACM Onward! and SPLASH Conferences. October, 2011. Portland, Oregon, USA''].&lt;/ref&gt;

There are many categorization theories and techniques. In a broader historical view, however, three general approaches to categorization may be identified:
* Classical categorization
* Conceptual clustering
* Prototype theory

==The classical view==
{{main|Categories (Aristotle)}}
'''Classical categorization''' first appears in the context of [[Western Philosophy]] in the work of [[Plato]], who, in his [[Statesman (dialogue)|Statesman]] dialogue, introduces the approach of grouping objects based on their similar [[Property (philosophy)|properties]]. This approach was further explored and systematized by [[Aristotle]] in his [[Categories (Aristotle)|Categories]] treatise, where he analyzes the differences between [[Class (philosophy)|class]]es and [[Object (philosophy)|object]]s. Aristotle also applied intensively the classical categorization scheme in his approach to the classification of living beings (which uses the technique of applying successive narrowing questions such as "Is it an animal or vegetable?", "How many feet does it have?", "Does it have fur or feathers?", "Can it fly?"...), establishing this way the basis for natural [[Taxonomy (biology)|taxonomy]].

The classical [[Aristotelianism|Aristotelian]] view claims that categories are discrete entities characterized by a set of properties which are shared by their members. In [[analytic philosophy]], these properties are assumed to establish the conditions which are both [[necessary and sufficient condition]]s to capture meaning. 

According to the classical view, categories should be clearly defined, mutually exclusive and collectively exhaustive. This way, any entity of the given classification universe belongs unequivocally to one, and only one, of the proposed categories.

==Conceptual clustering==
{{main|Conceptual clustering}}
'''Conceptual clustering''' is a modern variation of the classical approach, and derives from attempts to explain how knowledge is represented. In this approach, [[Class (philosophy)|class]]es (clusters or entities) are generated by first formulating their conceptual descriptions and then classifying the entities according to the descriptions. 

Conceptual clustering developed mainly during the 1980s, as a machine paradigm for [[unsupervised learning]]. It is distinguished from ordinary [[Cluster analysis|data clustering]] by generating a concept description for each generated category. 

Categorization tasks in which category labels are provided to the learner for certain objects are referred to as supervised classification, [[supervised learning]], or [[concept learning]]. Categorization tasks in which no labels are supplied are referred to as unsupervised classification, [[unsupervised learning]], or [[Cluster analysis|data clustering]]. The task of supervised classification involves extracting information from the labeled examples that allows accurate prediction of class labels of future examples. This may involve the [[abstraction]] of a rule or concept relating observed object features to category labels, or it may not involve abstraction (e.g., [[Exemplar theory|exemplar model]]s). The task of clustering involves recognizing inherent structure in a data set and grouping objects together by similarity into classes. It is thus a process of ''generating'' a classification structure. 

Conceptual clustering is closely related to [[fuzzy set]] theory, in which objects may belong to one or more groups, in varying degrees of fitness.

==Prototype theory==
{{main|Prototype theory}}
Since the research by [[Eleanor Rosch]] and [[George Lakoff]] in the 1970s, categorization can also be viewed as the process of grouping things based on [[prototype]]s&#8212;the idea of necessary and sufficient conditions is almost never met in categories of naturally occurring things. It has also been suggested that categorization based on prototypes is the basis for human development, and that this learning relies on learning about the world via [[embodied cognition|embodiment]].

A [[cognition|cognitive]] approach accepts that natural categories are graded (they tend to be fuzzy at their boundaries) and inconsistent in the status of their constituent members.

Systems of categories are not objectively "out there" in the world but are rooted in people's experience. Conceptual categories are not identical for different cultures, or indeed, for every individual in the same culture. 

Categories form part of a hierarchical structure when applied to such subjects as [[Taxonomy (biology)|taxonomy]] in [[biological classification]]: higher level: life-form level, middle level: generic or [[genus]] level, and lower level: the [[species]] level. These can be distinguished by certain traits that put an item in its distinctive category. But even these can be arbitrary and are subject to revision.

Categories at the middle level are perceptually and conceptually the more salient. The generic level of a category tends to elicit the most responses and richest images and seems to be the psychologically basic level. Typical taxonomies in zoology for example exhibit categorization at the [[embodied cognition|embodied]] level, with similarities leading to formulation of "higher" categories, and differences leading to differentiation within categories.

== Miscategorization ==
Miscategorization can be a [[Fallacy|logical fallacy]] in which diverse and dissimilar objects, concepts, entities, etc. are grouped together based upon illogical common denominators, or common denominators that virtually any concept, object or entity have in common. A common way miscategorization occurs is through an over-categorization of concepts, objects or entities, and then miscategorization based upon characters that virtually all things have in common.

== See also ==
{{too many see alsos|date=October 2013}}
{{columns-list|3| 
* [[Lumpers and splitters]]
* [[Artificial neural network]]
* [[Category learning]]
* [[Categorical perception]]
* [[Classification in machine learning]]
* [[Family resemblance]]
* [[Fuzzy concept]]
* [[Language acquisition]]
* [[Library classification]]
* [[Machine learning]]
* [[Multi-label classification]]
* [[Natural kind]]
* [[Ontology]]
* [[Pattern recognition]]
* [[Perceptual learning]]
* [[Semantics]]
* [[Socrates]]
* [[Sortal]]
* [[Structuralism]]
* [[Symbol grounding]]
* [[Taxonomy (general)]]
}}

== References ==
{{Reflist}}

==External links==
{{Wiktionary}}
* [http://eprints.ecs.soton.ac.uk/11725/ To Cognize is to Categorize: Cognition is Categorization]
*[http://toolserver.org/~dapete/catgraph/ Wikipedia Categories Visualizer]
*[http://www.revue-emulations.net/archives/n8/categentretien Interdisciplinary Introduction to Categorization: Interview with Dvora Yanov (political sciences), Amie Thomasson (philosophy) and Thomas Serre (artificial intelligence)]

{{philosophy of language}}

[[Category:Knowledge representation]]
[[Category:Concepts in epistemology]]
[[Category:Semantics]]
[[Category:Cognition]]</text>
      <sha1>d6up0gikqv0q2346b0kmulxecpn4zyu</sha1>
    </revision>
  </page>
  <page>
    <title>Brian Deer Classification System</title>
    <ns>0</ns>
    <id>49726563</id>
    <revision>
      <id>718070242</id>
      <parentid>709686234</parentid>
      <timestamp>2016-05-01T10:30:29Z</timestamp>
      <contributor>
        <username>Themightyquill</username>
        <id>1212157</id>
      </contributor>
      <comment>removed [[Category:Library science]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2521" xml:space="preserve">The '''Brian Deer Classification System''' is a [[library classification]] system created for use in Indigenous contexts by Canadian [[Mohawk people|Kahnawake Mohawk]] librarian A. Brian Deer.&lt;ref name="auto"&gt;{{cite journal|last1=Doyle|first1=Ann M.|last2=Lawson|first2=Kimberley|last3=Dupont|first3=Sarah|title=Indigenization of Knowledge Organization at the Xwi7xwa Library|journal=Journal of Library and Information Studies|date=December 2015|volume=13|issue=2|page=112|doi=10.6182/jlis.2015.13(2).107|url=https://open.library.ubc.ca/cIRcle/collections/ubclibraryandarchives/29962/items/1.0103204|accessdate=11 March 2016}}&lt;/ref&gt;

== History ==
Deer designed his classification system while working in the library of the [[National Indian Brotherhood]] from 1974-1976, with the goal of reflecting indigenous viewpoints and values in knowledge organization. Between 1978 and 1980, the system was adapted for use in [[British Columbia]] by Gene Joseph and Keltie McCall while working at the [[Union of British Columbia Indian Chiefs]].&lt;ref name="auto"/&gt;

Variations of the Brian Deer Classification are in use at the [[University of British Columbia Library|Xwi7xwa Library]] at the [[University of British Columbia]];&lt;ref name="auto"/&gt; the [[Union of British Columbia Indian Chiefs]] Resource Centre;&lt;ref&gt;{{cite journal|last1=Cherry|first1=Alissa|last2=Mukunda|first2=Keshav|title=A Case Study in Indigenous Classification: Revisiting and Reviving the Brian Deer Scheme|journal=Cataloging &amp; Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=pages 548&#8211;567|doi=10.1080/01639374.2015.1008717|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1008717?journalCode=wccq20|accessdate=11 March 2016}}&lt;/ref&gt; and the  Aanischaaukamikw Cree Cultural Institute in [[Ouj&#233;-Bougoumou, Quebec]].&lt;ref&gt;{{cite journal|last1=Swanson|first1=Raegan|title=Adapting the Brian Deer Classification System for Aanischaaukamikw Cree Cultural Institute|journal=Cataloging &amp; Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=568&#8211;579|doi=10.1080/01639374.2015.1009669|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1009669|accessdate=11 March 2016}}&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://xwi7xwa.library.ubc.ca/files/2011/09/deer.pdf Brian Deer Classification System]

&lt;!--- Categories ---&gt;
[[Category:Library cataloging and classification]]
[[Category:Information science]]
[[Category:Knowledge representation]]
[[Category:Aboriginal peoples in Canada]]</text>
      <sha1>1no085plym3xfjr1t18gxgyaci3nt92</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computational fields of study</title>
    <ns>14</ns>
    <id>52242291</id>
    <revision>
      <id>750534442</id>
      <parentid>750259546</parentid>
      <timestamp>2016-11-20T10:06:25Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Inductive reasoning]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="706" xml:space="preserve">Computational fields of study are areas of research in an existing field using the power of computation, and which are usually named for that. Computational fields of study as a group are sometimes also be referred to as [[Computational X]]&lt;ref&gt;[http://blog.stephenwolfram.com/2016/09/how-to-teach-computational-thinking/ How to Teach Computational Thinking] by [[Stephen Wolfram]], Stephen Wolfram Blog, September 7, 2016.&lt;/ref&gt;.

[[Category:Computer science]]
[[Category:Knowledge representation]]
[[Category:Applied mathematics]]
[[Category:Big data]]
[[Category:Systems theory]]
[[Category:Computing and society]]
[[Category:Systems thinking]]
[[Category:Futurology]]
[[Category:Theories of deduction]]</text>
      <sha1>23fsujue2h4bd7m7z462hf67b493etd</sha1>
    </revision>
  </page>
  <page>
    <title>Hallin's spheres</title>
    <ns>0</ns>
    <id>31244175</id>
    <revision>
      <id>761583402</id>
      <parentid>752889039</parentid>
      <timestamp>2017-01-23T19:17:47Z</timestamp>
      <contributor>
        <ip>149.63.0.134</ip>
      </contributor>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7239" xml:space="preserve">'''Hallin's spheres''' is a theory of media objectivity posited by journalism historian [[Daniel C. Hallin]] in his book ''The Uncensored War'' to explain the coverage of the Vietnam war.&lt;ref name="DH"&gt;{{cite book|last=Hallin|first=Daniel|title=The Uncensored War: The Media and Vietnam.|year=1986|publisher=Oxford University press|location=New York|pages=116&#8211;118|isbn=978-0-19-503814-9}}&lt;/ref&gt; Hallin divides the world of political discourse into three concentric spheres: consensus, legitimate controversy, and deviance. In the sphere of consensus, journalists assume everyone agrees. The sphere of legitimate controversy includes the standard political debates, and journalists are expected to remain neutral. The sphere of deviance falls outside the bounds of legitimate debate, and journalists can ignore it. These boundaries shift, as public opinion shifts.&lt;ref&gt;[http://www.cjr.org/analysis/trump_inspires_murrow_moment_for_journalism.php For journalists covering Trump, a Murrow moment], By David Mindich, Columbia Journalism Review, July 15, 2016&lt;/ref&gt;

Hallin's spheres, which deals with the media, are similar to the [[Overton window]], which deals with public opinion generally, and posits a sliding scale of public opinion on any given issue ranging from conventional wisdom to unacceptable. 

Hallin used the concept of [[Framing (social sciences)|framing]] to describe the presentation and reception of issues in public. For example, framing the use of drugs as criminal activity can encourage the public to consider that behavior anti-social. Hallin also used the concept of an [[opinion corridor]], in which the range of public opinion narrows, and opinion outside that corridor moves from legitimate controversy into deviance. 

== Description ==

=== Sphere of consensus ===
This sphere contains those topics on which there is widespread agreement, or at least the perception thereof. Within the sphere of consensus, 'journalists feel free to invoke a generalized "we" and to take for granted shared values and shared assumptions'&lt;ref&gt;Schudson 2002, p. 40&lt;/ref&gt; Example include such things as free speech, the abolition of slavery, or human rights.  For topic in this sphere "journalists do not feel compelled to present an opposing view point or to remain disinterested observers."&lt;ref name="DH"/&gt;

=== Sphere of legitimate controversy ===
For topics in this sphere rational and informed people hold differing views. These topics are therefore the most important to cover, and also ones upon which journalists are obliged to remain disinterested reporters, rather than advocating for or against a particular view.&lt;ref&gt;Hallin, 1986, p. 116;&lt;/ref&gt; Schudson notes that Hallin, in his influential study of the US media during the Vietnam War, argues that journalism's commitment to objectivity has always been compartmentalized. That is, within a certain sphere&#8212;the sphere of legitimate controversy&#8212;journalists seek conscientiously to be balanced and objective.&lt;ref&gt;Schudson, M (2002) 'What's unusual about covering politics as usual', in Zelizer, B., &amp; Allan, S. (Eds.). Journalism after September 11. London: Routledge, p. 40&lt;/ref&gt;

=== Sphere of deviance ===
Topics in this sphere are rejected by journalists as being unworthy of general consideration.  Such views are perceived as being either unfounded, taboo, or of such minor consequence that they are not news worthy.  Hallin argues that in the sphere of deviance, 'journalists also depart from standard norms of objective reporting and feel authorized to treat as marginal, laughable, dangerous, or ridiculous individuals and groups who fall far outside a range of variation taken as legitimate.'&lt;ref&gt;Schudson 2002, 40&lt;/ref&gt; For example, a person claiming that aliens are manipulating college basketball scores might have difficulty finding media coverage for such a claim.&lt;ref&gt;Hallin, 1986, p. 117&lt;/ref&gt;

== Uses of the terms ==
Craig Watkins (2001, pp.&amp;nbsp;92&#8211;4) makes use of the Hallin's spheres in a paper examining ABC, CBS, and NBC television network television news coverage of the "Million Man March", a demonstration that took place in Washington, DC on October 16, 1995. Watkins analyzes the dominant framing practices-problem definition, rhetorical devices, use of sources, and images-employed by journalists to make sense of this particular expression of political protest. He argues that Hallins three spheres are a way for media framing practices to develop specific reportorial contexts, each sphere develops its own distinct style of news reporting resources by different rhetorical tropes and discourses.&lt;ref&gt;Watkins, S. C. (2001). Framing protest: News media frames of the Million Man March. Critical Studies in Media Communication, 18(1), 83-101.&lt;/ref&gt;

Piers Robinson (2001, p.&amp;nbsp;536) uses the concept in relation to debate that have emerged over the extent to which the mass media serves elite interests or, alternatively, plays a powerful role in shaping political outcomes. His articles reviews Hallin's spheres as an example of media-state relations, that highlights theoretical and empirical shortcomings in the 'manufacturing consent' thesis (Chomsky McChesney).&lt;ref&gt;Herman, E. S., &amp; Chomsky, N. (2010). Manufacturing consent: The political economy of the mass media. Random House.&lt;/ref&gt; Robinson argues that a more nuanced and bi-directional understanding is needed of the direction of influence between media and the state that builds upon, rather than rejecting, existing theoretical accounts.&lt;ref&gt;Robinson, P. (2001). Theorizing the Influence of Media on World Politics Models of Media Influence on Foreign Policy. European Journal of Communication, 16(4), 523-544.&lt;/ref&gt;

Hallin's theory assumed a relatively homogenized media environment, where most producers were trying to reach most consumers. A more fractured media landscape can challenge this assumption.&lt;ref&gt;{{cite web|title=Does NPR Have A Liberal Bias?|url=http://www.onthemedia.org/2012/sep/14/|work=On The Media from NPR|publisher=WNYC|accessdate=11 February 2013}}&lt;/ref&gt; because different audiences may place topics in different spheres, a concept related to the [[filter bubble]], which posits that many members of the public choose to limit their media consumption to the areas of consensus and deviance that they personally prefer.

==See also==
* [[Ambit claim]]
* [[Argument to moderation]]
* [[Creeping normality]]
* [[Cultural hegemony]]
* [[Door-in-the-face technique]]
* [[Political suicide]]
* [[Slippery slope]]
* [[Spiral of silence]]
* [[Third rail of politics]]

== References ==
{{reflist}}

==External links==
* [http://archive.pressthink.org/2009/01/12/atomization.html Audience Atomization Overcome: Why the Internet Weakens the Authority of the Press], Rosen, Jay. PressThink.org, January 12, 2009
* [http://wnymedia.net/smith/2009/03/the-sphere-of-deviance/ The Sphere of Deviance] Smith, Christopher. WNYMedia.net, 2009.
* [http://www.onthemedia.org/2012/sep/14/ Does NPR have a Liberal Bias?], On The Media from NPR. Retrieved 11 February 2013

{{Media culture}}
{{Media manipulation}}
{{Propaganda}}

[[Category:Framing (social sciences)]]
[[Category:Knowledge representation]]
[[Category:Propaganda techniques]]
[[Category:Journalism]]</text>
      <sha1>pduoifdz93dvxozjp6mzsd0o2mfuznr</sha1>
    </revision>
  </page>
  </mediawiki>